One of the most ubiquitous discourses is advertisements. When we watch TV in the comfort of our living rooms, we are bombarded with ads; when we read a newspaper or magazine, somehow our attention is distracted by one form of an ad or another. On our way to school or office, we come across ads in various shapes or colors. Indeed, advertising, whether print, broadcast, or any other type, is part of our everyday lives. It is no wonder then that advertising discourse has attracted the attention of scholars in over two decades. Simpson (2001) acknowledges that there has been “an enormous upsurge of interest in the linguistic and discoursal characteristics of advertising” (p. 589), adding that the studies conducted have been anchored on different traditions and perspectives, such as cognitive, cultural and anthropological, genre and register analysis, critical discourse analysis, and linguistic pragmatics (Simpson, 2001, p. 590). In recent years, research has focused on reader effects of poetic and rhetorical elements in ads from a relevance-theoretic perspective. For instance, van Mulken, van Enschot-van Dijk, and Hoeken (2005) aimed to find out whether slogans in ads are appreciated more than slogans without a pun, and whether puns containing two relevant interpretations are appreciated more than puns containing only one relevant interpretation (p. 707). To do this, 68 participants rated their appreciation of 24 slogans. The results showed that the presence or absence of puns had a significant impact on the respondents’ appreciation of the slogans. Furthermore, whether the pun contained two relevant interpretations or only one did not influence the extent to which they were considered funny, but the former were considered a better choice than the latter (van Mulken, van Enschot-van Dijk, and Hoeken, 2005). Lagerwerf (2007), on the other hand, examined the effects on audiences of irony in ads and of sarcasm in public information announcements. Two studies were conducted. Sixty students took part in the first study, with stimuli consisting of 12 magazine ads, six of which were positively formulated and six negatively. In the second, there were 40 students who participated in the experiment, with stimuli consisting of ads that were partly based on the researcher’s own designs and partly on actual ads. In advertisements for commercial products and services, irony was found in the use of negative captions where positive captions were expected. Sarcasm was used by placing a positive caption against a background displaying a harrowing picture. Such departures from common practice in the use of negative and positive wordings were regarded as inappropriate. It turned out that advertisements with ironic intent were appreciated more when the inappropriateness was re-interpreted correctly as irony (Study 1). Even so, irony and 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 1–15 Copyright 2008 by Danilo T. Dayag.  sarcasm may impede a proper understanding of the advertisements’ informative intention. This has a negative impact on the assessment by an audience of the importance of the societal issues emphasized in sarcastic announcements (Study 2) (Lagerwerf, 2007). Working within the pragmatic construct of metadiscourse, Fuertes-Olivera, et al. (2001) analyzed the metadiscourse devices typically used by ad copywriters to construct their slogans and/or headlines. The researchers’ analysis proceeded from the assumption that advertising English should be represented as a continuum of text functions fluctuating between “informing” and “manipulating” in accordance with the idea that advertising is an example of covert communication. Based on an examination of ads from a women’s magazine, they concluded that both textual and interpersonal metadiscourse devices help copywriters to convey a persuasive message under an informative mask (Fuertes-Olivera, et al., 2001). The present study seeks to contribute to the ongoing interest in describing the discourse of advertising. In particular, it aims to describe magazine ads in the Philippines in terms of their generic structures and linguistic properties, including speech acts performed by utterances. The study is anchored on Simpson’s (2001) ‘reason’-‘tickle’ binary distinction between types of advertising discourse, which expands Bernstein’s (1974) proposal. In Simpson’s view, ‘reason’ ads are those which suggest a motive or reason for purchase. Furthermore, these ads follow a basic discourse pattern and the corresponding conjunctive adjuncts – conditional, causal, and purposive – which realize the pattern. This pattern parallels the notion of generic structure) (see discussion of generic structure below). ‘Tickle’ ads, by contrast, are those which appeal to humor, emotion and mood. Unlike reason ads which are stable in terms of structure and whose language is straightforward, ‘tickle’ ads are indirect, and therefore need the reader’s inferencing strategies to figure out what they convey (Simpson, 2001). Figure 1 locates the ‘reason’-‘tickle’ construct within three well-known pragmatic models:  Grice (1975)  Brown & Levinson (1987)  Sperber & Wilson (2004)  Reason  Direct  ‘maximal’ efficiency bald-onrecord  strong relevance  Tickle  Oblique  implicature  off-record  weak relevance  Figure 1. ‘Reason’-‘tickle’ distinction and pragmatic models (Simpson, 2001, p. 593) In looking at the generic structure of print ads as texts, the study proceeds from the assumption that an ad is a genre, defined here as “a class of communicative events, the members of which share some set of communicative purposes. These purposes are recognized by the expert members of the parent discourse community, and thereby constitute the rationale for the genre” (Swales, 1990, pp. 45-58). The unit of genre analysis is rhetorical move (or simply move), a functional unit (Halleck and Connor, 2006, p. 72) or a semantic unit related to the writer’s purpose (Hassan, 2008, p. 39), which has been used by several genre-analytic studies. These include research that has focused on academic texts like research article introductions (e.g. Swales, 1990) and on professional texts such as sales letters (e.g. Bhatia, 1993). Taking its cue from Simpson (2001), the study uses modified generic structures consisting of the following moves: Giving Reason/s for Buying + Citing Positive Benefits (or Cause + Effect), and Creating Need/Purpose + Recommending Course of Action. In addition, five other structures have been used in the study. They are Identifying Product Name/Features + Citing Positive Benefits, Creating Need + Identifying Product Name, Describing Company/Product + Identifying Product Name, Identifying Product Name + Slogan, Conditional Constructions  2  (Antecedent + Consequent), and Combination of structures (e.g. Creating Need + Identifying Product Name + Citing Positive Benefits). The linguistic properties used in this paper are based on features which were found by Lakoff (1982) and Geis (1982) to have been possessed by advertisements, which, in turn, were used by Schmidt and Kess (1986) in characterizing televangelism. Among these properties are linguistic novelty, repetition of names, adjectivalization processes, and imperative structures. In addition, the description of noun phrases, especially their pre-modification structures, has been influenced by Rush’s (1998) characterization of the complexity of NPs in the ads. Finally, codeswitching patterns in the ads are described in terms of Dayag (2002), which differentiates between inter-sentential code-switching, intra-sentential code-switching, and tag switching, using the terminology of Poplack (1980). Drawing upon the works of Searle (1979), the present study also analyzes the speech acts performed by utterances in the ads. Specifically, it uses Searle’s (1979) taxonomy of illocutionary acts consisting of five general types, namely, assertives (representatives), directives, commissives, declarations, and expressives. Huang (2007) defines assertives as “those kinds of speech act that commit the speaker to the truth of the expressed proposition, and thus carry a truth-value. They express the speaker’s belief” (Huang, 2007, p. 106). Examples are “asserting, claiming, concluding, reporting, and stating” (Huang, 2007, p. 106). In this study, an assertive generally takes the form of a claim, which is “an assertion, statement or implication (as of value, effectiveness, qualification, eligibility) which predicates a past or present event and whose justification is not readily verifiable” (Schmidt and Kess, 1986, p. 49). Directives, on the other hand, are “those kinds of speech act that represent attempts by the speaker to get the addressee to do something,” and they include “advice, commands, orders, questions, and requests” (Huang, 2007, p. 107). Commissives “commit the speaker to some future course of action,” and “offers, pledges, promises, refusals, and threats” are some examples (Huang, 2007, p. 107). Declarations (or declaratives) “effect immediate changes in some current state of affairs” (Huang, 2007, p. 108). Also called institutionalized performatives, declarations include “bidding in bridge, declaring war, excommunicating, firing from employment, and nominating a candidate” (Huang, 2007, p. 108). Lastly, expressives “express a psychological attitude or state in the speaker such as joy, sorrow, and likes/dislikes” (Huang, 2007, p. 107). Examples include “apologizing, blaming, congratulating, praising, and thanking” (Huang, 2007, p. 107). In Austin’s (1962) view, certain conditions must be fulfilled for a speech act to be felicitous. Searle (1969) elaborated on this by proposing felicity conditions of speech acts, i.e. constitutive rules (or rules that create the activity itself) (Huang, 2007, 104). The felicity conditions for assertive and directives are shown below:  Propositional content p  Preparatory condition 1. S has evidence (reasons etc.) for the truth of p 2. It is not obvious to both S and H that H knows (does not need to be reminded of, etc.) p  Sincerity condition S believes p  Essential condition  The utterance of e counts as an undertaking to the effect that p represents an actual state of affairs  (Searle, 1969, p. 67, cited in Allan, 2008) Figure 2. Felicity conditions for assertives  3  Propositional content Future act A of H  Preparatory condition (a) S believes H can do A (b) It is not obvious that H would do A  without being asked  Sincerity condition  S wants H to do A  Essential condition  The utterance of e counts as an attempt to get H to do A  where: A = act  S = speaker  H = hearer  e = linguistic expression  (Searle (1969) quoted in Huang, 2007, p. 105)  Figure 3. Felicity conditions for directives  2. Methodology The corpus of the study consists of 74 ads for non-consumer durables, broken down as follows: 28 (medicines), 25 (vitamins and food supplements), and 21 (cosmetics/beauty/ personal hygiene products). They were taken from health and entertainment magazines published from 2005 to 2007, which were sub-components of the Corpus of Asian Magazine Advertising (CAMA): The Philippine database (Dayag, 2008). The CAMA corpus building project “seeks to compile a corpus of advertisements from various East and Southeast Asian magazines. The completed corpus as well as a catalogued database of the advertisements will be made available to each research team working to complete the corpus” (“CAMA sampling guidelines,” p. 1). For the purposes of this study, only the verbal elements of the headline and body text of each ad were considered for analysis. These were coded in terms of generic structures and linguistic features including speech acts, following the discussion above.  3. Generic Structures of Print Ads  In this section, I describe the preferred generic structures of print ads as well as speech acts performed by utterances in the ads. Table 1 shows the preferred generic structures of the 74 magazine ads included in the corpus of the study.  4  Table 1. Preferred generic structures of print ads  Vitamins &  Generic  Medicines  Food  Structure  Supplements  Conditional  (Antecedent + 1  0  Consequent)  (1.35%)  Reason  +  Benefits (Cause 1  
Automatic spoken language recognition (SLR) is a process of determining the identity of the language in a spoken document. As multilingual applications are demanded by the emerging need for globalization and the growing international business interflow, SLR has become an enabling technology in many applications such as multilingual conversational systems (Zue and Glass, 2000), multilingual speech recognition and translation (Waibel et al., 2000), and spoken document retrieval (Dai et al. 2003). It is also a topic of great importance in the areas of intelligence and security, where the language identities of recorded messages and archived materials need to be established before any information can be extracted. SLR technology also facilitates massive on-line language routing for voice surveillance over telephone network. The National Institute of Standards and Technology (NIST) has conducted a series of evaluations of SLR technology in 1996, 2003, 2005 and 2007 (NIST, 2007). The language recognition evaluations (LREs) focus on language and dialect detection in the context of conversational telephony speech. They are conducted to foster research progress, with the goals of exploring promising new ideas in language recognition, developing advanced technology incorporating these ideas, and measuring the performance of this technology. The Institute for * Copyright 2008 by Haizhou Li, Bin Ma, Kong-Aik Lee, Khe-Chai Sim, Hanwu Sun, Rong Tong, Donglai Zhu, and Changhuai You 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 46–57 46  Infocomm Research (IIR) team has participated in the 2005 and 2007 NIST LREs and demonstrated the state-of-the-art technologies. One of the fundamental issues in SLR is to explore the discriminative cues for spoken languages. In the state-of-the-art language recognition systems, these cues mainly come from the acoustic features (Sugiyama, 1991; Torres-Carassquilo et al., 2002; Burget et al., 2006; Campbell et al., 2006) and phonotactic representations (Hazen and Zue, 1994; Zissman, 1996; Berkling and Barnard, 1994; Corredor-Ardoy et al., 1997; Li and Ma, 2005; Ma, Li, and Tong, 2007), which reflect different aspects of spoken language characteristics. Another issue is how to effectively organize and exploit these language cues obtained from multiple sources in the recognition system design for the best performance. Significant improvements in automatic speech recognition (ASR) have been achieved through exploiting the acoustic features representing the temporal properties of speech spectrum. These acoustic features, such as Mel-frequency Cepstral Coefficients (MFCCs), are also good choices to be the front-ends in language recognition systems. Gaussian mixture model (GMM), which can be seen as a one-state hidden Markov model (HMM) (Rabiner, 1989), is a simple modeling method to provide a multimodal density and is reasonably accurate when speech data are generated from a set of Gaussian distributions. It has demonstrated a great success in textindependent speaker recognition (Reynolds, Quatieri, and Dunn, 2000). In language recognition, GMM is also an effective method to model the unique characteristics among languages (Torres-Carassquilo et al., 2002). The support vector machine (SVM) has proven to be a powerful classifier in many pattern classification tasks. It is a discriminative classifier to separate two classes with a hyperplane in a high-dimensional space. The generalized linear discriminant sequence kernel (GLDS) has been proposed to apply SVM for speaker and language recognition (Campbell et al., 2006). The cepstral feature vectors extracted from an utterance are expanded to a high-dimensional space by calculating all the monomials. In recent years, phonotactic features have been shown to provide effective cues for language recognition. The phonotactic features are extracted from an utterance to represent phonetic constraints in a language. Although common sounds are shared considerably across spoken languages, the statistics of these sounds, such as phone n-gram, can differ considerably from one language to another. Parallel Phone Recognizers followed by Language Models (PPR-LM) (Zissman, 1996) uses multiple parallel phone recognizers to convert the input utterance into a phone token sequence. It is followed by a set of n-gram phone language models that imposes constraints on phone decoding and provides language scores. Instead of n-gram phone language models, vector space modeling (VSM) was proposed as the classifier (Li, Ma, and Lee, 2007), called PPR-VSM. For each phone sequence generated from the multiple phone recognizers, the occurrences of phone n-grams are counted. A phone sequence is then represented as a highdimensional vector of n-gram occurrence. SVM is used as the classifier on the concatenated ngram occurrence vectors. It is generally agreed upon that the integration with different cues of discriminative information can improve the performance of language recognition (Adda-Decker et al., 2003). The information extraction and organization of multiple sources has been critical to a successful language recognition system (Singer et al., 2003; Tong et al., 2006). In this paper, we will report our language recognition system submitted to the 2007 NIST LRE. The system is based on the fusion of multiple classifiers, each providing unique discriminative cue for language classification. In order to avoid a spoiled classifier in the submitted fusion system, we have designed a pseudo key analysis approach to check the integrity of each individual classifier before the system fusion. The remainder of this paper is organized as follows. The evaluation data and evaluation metric of the 2007 NIST LRE will be introduced in Section 2. The system structure together with the phonotactic and acoustic language classifiers will be presented in Section 3. The fusion of multiple language classifiers and language recognition results on the 2007 NIST LRE evaluation data will be described in Section 4. The pseudo key analysis will be shown in Section 5. Finally in Section 6, we summarize our findings in language recognition. 47  2. Data and Metric  2.1. Evaluation Data There are six test categories in the 2007 NIST LRE involving 26 target languages and dialects: • General Language Recognition (LR) including 14 languages, Arabic, Bengali, Chinese, English, Hindustani, Spanish, Farsi, German, Japanese, Korean, Russian, Tamil, Thai and Vietnamese. • Chinese LR including four Chinese dialects, Cantonese, Mandarin, Min and Wu. • Mandarin Dialect Recognition (DR) including Mainland Mandarin and Taiwan Mandarin. • English DR including American English and India English. • Hindustani DR including Hindi and Urdu. • Spanish DR including Caribbean Spanish and non-Caribbean Spanish. Both closed-set and open-set tests in the six categories were conducted. For the closed-set tests, the non-target languages will be limited to those languages and dialects known to the system. For the open-set test the non-target languages will also include all other unknown languages such as Italian, Punjabi, Tagalog, Indonesian, and French. These unknown languages were not disclosed to participants, and the training data for these languages were not made available. There are three test conditions to evaluate the system performance under different test segment durations: • 3 seconds of speech (2-4 seconds actual) • 10 seconds of speech (7-13 seconds actual) • 30 seconds of speech (25-35 seconds actual) The silence was not removed from speech so a segment could be much longer. There are 2510 segments for each of the three durations.  2.2. Training and Development Data All the phonotactic and acoustic classifiers were trained with the LDC CallFriend corpus1 and the LRE 2007 development databases released by NIST to all the participants. The phone recognizers used for phonotactic features were trained with OGI Multilingual database (Muthusamy, Cole, and Oshika, 1992) and IIR-LID database (Tong et al., 2006). The weights of fusion system were tuned on the LRE 1996, 2003, 2005 databases as well as the LRE 2007 development database.  2.3. Evaluation Metric  The primary evaluation metric is taken as the average cost performance Cavg (NIST LRE, 2007), which indicates the pair-wise language recognition performance, represented in terms of detection miss and false alarm probabilities, for all target/non-target language pairs. For the case of closed-set test condition, the Cavg is given by  ∑ ∑ Cavg  =  
1. Introduction Talks that state the obvious, or review well-known research are boring and risk losing an audience. Talks that give controversial positions often have the same result. But I'd rather take the dangerous route, in hopes of spurring some new ideas and new research. A further risk is that I'm a computer scientist by training, not a linguist. I may have substantial blind spots in computational linguistics, and there are undoubtedly people I'm not aware of already working in the direction I will advocate, but that provides a big opportunity for me to learn from your feedback on this talk. I'll focus on a broad goal of language understanding or language processing in Artificial Intelligence. I'll define this as a set of techniques for processing human language that show evidence of the same competencies or behaviors as human language processing. Fundamental to this is the ability to accept statements in language that affect future responses, and the ability to respond to questions that demonstrates prior assimilation of knowledge. I don't believe that a “tabula rasa” approach is feasible in this context; I will take it as a given that a great deal of knowledge must already reside in a practical language understanding system. There has been considerable research in computational linguistics that takes particular linguistic features and subjects them to semantic analysis, with the goal of specifying a formal semantic interpretation derived from syntactic features. This entire area of research has waned however, in part because it was so difficult to combine these sorts of analyses into a single * While this paper discusses nearly a decade of research, which preludes mentioning all the sponsors and collaborators who have contributed, we hope that we will not slight those not mentioned by listing some of the major supporters of this work, who include, US Army CECOM, Army Research Institute, ARDA and DARPA. Copyright 2008 by Adam Pease and released under the terms of GNU Free Documentation License 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 58–64 58  semantic theory. The need for providing some interpretation of all text has moved the computational linguistics field to robust shallow interpretations, rather than brittle and deep ones. A contributing factor has been the need for some very large resources to make it possible to have non-toy implementations of deep linguistic semantic processing. Another issue is that because the scope of linguistic semantics is so large, it's hard to tell if different component theories result in a harmonious total interpretation. We're now at the threshold of being able to address these issues. This is why I call for a new direction of Executable Linguistics Research. Note that I'm not proposing an exclusive alternative to statistical linguistic methods. There are portions of this general problem that benefit from the marriage of statistical and logical approaches, most notably, word sense disambiguation. I'm proposing a shift in emphasis, recommending that more people concentrate on an approach to linguistics that has been somewhat neglected – a shift to focusing on a more difficult and longer term approach, because the utility of current robust and arguably shallow approaches to understanding are yielding less substantial incremental improvements as time goes on. I'll first sketch an outline of the products that I think are needed. Then I'll discuss existing resources that can meet some of those needs. Next I'll provide some concrete examples to show how this all might work. 2. What's Needed A fundamental component of any practical and large scale language understanding system is a large vocabulary. Any system that understands language must be able to identify words. It should know basic relationships among words that form some of the building blocks of meaning – synonyms, antonyms, and which words subsume others' meanings or entail them, at a minimum. Harder to acquire, though no less necessary, is a corpus of groups of words that are “tokens”, which have a single and collective meaning that is more than the sum of their component words, and which repeatedly appear in group form. For English at least, polysemy is a significant issue in language understanding. While better models and algorithms are certainly needed to handle word sense disambiguation the foundation of most algorithms of this sort is the availability of data on which to train the algorithms. Specifically, there is a need for both balanced and domain specific corpora where words have been manually disambiguated with respect to a lexicon. All this may be relatively uncontroversial so far. Now for the more controversial part. While word meanings change over time, the vast majority of meanings are constant. While meanings can't be legislated, they can be discovered and described, and will largely remain fixed. Linguists often study language at the margins, where there are changes and differences and scientifically interesting features, but much of language is certainly stable, at least over decades. If our goal is machine understanding, it's not enough to know that one word is more specific than another, we must know in what way it is more specific, and what knowledge logically follows from using the more specific word instead of the more general one. If we agree that this sort of specific information is needed about word meanings, then the next question is in what form it should be represented. There are broadly two options: statistical representations that specify approximate relationships learned automatically, and logical ones that at least at the moment must largely be crafted by humans. Each general approach has advantages. Statistical approaches require human effort to create a good algorithm, but then can be run automatically on large data sets without human intervention. Such approaches are robust in terms of coverage, but decidedly not robust in terms of the precision of the data. We may be able to learn entailment relationships one inference deep, but I would venture that truthpreserving inferences from automatically acquired data are a long way away. The combinatorics of inference dictate that even if only 10% of the entailments learned are wrong (and the state of art is more like >50% even for simple entailments (TAC 2008), even a simple five step deduction will usually be wrong. 59  Logical representations can be truth-preserving, and the consistency of any logical theory can be automatically tested (subject to time limitations of course). The main problem is that the effort to craft theories requires human effort, and that effort is specialized and often expensive. I'll return to the issue of open source development later, but for now, let me just state as a given that the scope of such an effort requires open collaboration with many entities and individuals involved. One issue with logical representations is that words are not logical terms with mathematically precise definitions (at least in most cases). If we treat words as logical terms, or logical terms as though they were words, we'll have an inaccurate model. I'd suggest that we need both a lexicon and a logical model, and relations between them. It is not enough simply to classify words with a small number of formal terms. Knowing that “water” is a “substance” is not sufficient. Any system for understanding must know the implications, uses and properties of water. It must know that water dissolves some other substances, that people can both swim and drown in it, that it can become ice or steam, and many more facts. So, the logical model must ultimately be as large as a dictionary, and it must be subject to continuous evolution as language changes and expands. Language does not just consist of isolated words. English has many standard phrases in which the meaning of a phrase is more than the sum of its constituent words. A simple case of this is light or “helper” verbs like “take” as in “take a walk” in which the noun functions to modify the mostly meaningless verb. Note that one cannot simply replace the verb with the verbal form of the noun, since “take a hit” is not the same as “hit”. Other examples such as “pay attention” are pairs in which the word choices are specific to a given phrase. We cannot simply make these multi-word lexicon entries since we also have examples like “take a long walk” and “a walk was taken”. So we must have a corpus of phrases that have logical templates that are filled in by the remaining context of a given sentence.1 Beyond a corpus of words and phrases we must have a way of interpreting the overall semantics of a sentence. The building block of a lexicon, phrase corpus, and logical definitions for each is not sufficient. We must be able to interpret the appearance of subject, object, negation, word morphology, conjunction and disjunction, conditionals, modals and many other features. We must handle a host of possibly more mundane linguistic elements like statements about metric time and numbers with units. There is a wealth of research in this area, but not to my knowledge much effort (with Fuch's ACE (Fuchs, 1999) and Kamp& Reyle's DRS work (Kamp&Reyle,1993) as notable exceptions) in systematizing interpretations of all of these different linguistic elements. The next challenge once we have a system for converting language into logic is to do something useful with the logic. A key enabler is the availability of the same large logical theory that helped us to define individual words in the first place. Those definitions become the corpus of facts and rules that tie together individual logical assertions, and allow us to deduce new consequences. To cover a useful space of knowledge, this theory must be very large. The larger the theory however, the harder it is for a deductive system to process queries on that theory. We must have deductive theorem provers that use smart and adaptive techniques to order the relevance of knowledge, learn how to segment it, and process queries with great efficiency. 3. What We Have The preceding discussion is not just an abstract exercise, but a guide to research that exists and research that is needed. For each proposed component we have a solution, but all components would benefit from significant focused, collaborative and open research and development. 
Question answering (QA) is an area that operates on top of search engines (such as Google, Exalead, Yahoo, etc.) in order to provide users with more accurate and elaborated responses where search engine response outputs remain punctual, difficult to understand, and sometimes incoherent. QA builds on top of search engine responses via language processing tools, reasoning mechanisms and response production techniques. QA has been recognized as an essential component of man-machine communication since it greatly improves communication with the Web or textual databases, allowing users to communicate in their own language, in order to get much more precise, accurate and user-tailored responses. Factoid questions (questions about facts, e.g.: what is the capital of X ?) have been largely investigated (witness, e.g. the TREC competition), but the other types of questions, frequently encountered such as : procedural (how to), causal (why), evaluative and comparative questions (what is the cheapest air ticket to go to Cebu from Singapore ?) have received little attention so far. However, they correspond to the majority of the 'real' questions asked either by the large public or by professionals to the web or to textual databases. Questions cover a large number of everyday life (health, employment, administrative life, tourism, etc.) as well as technical areas (investments, competitive intelligence, etc.). Complex questions require language processing (to process the question and on the outputs of search engines), reasoning (e.g. data fusion, incoherence solving, summarizing, etc.) and response production in natural language (in a way accessible and user-friendly). These complex questions are in fact those users ask most of the time, these users being professionals or from the large public. Complex questions are traditionally classified according to the type of response which is induced, e.g.: - definition questions, * Acknowledgments: we are grateful to a number of MA students who contributed to this work, among which: Estelle DELPECH, Lionel FONTAN, Isabelle DAUTRICHE and Clementine ADAM. We also thank the ANR-RNTL French program for supporting part of this research. Copyright 2008 by Patrick Saint-Dizier. 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 65–73 65  - how-to questions for procedures, - why questions for causes/consequence (possibly involving chains of events), - evaluative and comparative questions (asking for comparisons between two or more elements such as fares). Considering the technology which is required, it is clear that search engines will not be able in the near future or even in the mid-term future to correctly process these questions due to the complexity of the task, the need of some domain knowledge and the necessity to provide natural language responses, beyond text extracts. The current trend in TREC Question Answering is towards the processing of large volumes of open-domain texts (e.g. documents extracted from the World Wide Web). Open domain QA is a hard task because no restriction is imposed either on the question type or on the user's vocabulary. This is why most of the efforts (in evaluation campaigns such as TREC) are focused on answering factoid style questions, and, to a little extend, definition questions, using shallow text processing which is roughly based on pattern extraction or information retrieval techniques. However, QA should integrate more complex techniques, in order to provide, for example, deep semantic analysis of NL questions such as anaphora resolution, context and ambiguity detection, complex question processing, better answer ranking and answer justification, responses to unanticipated questions, response fusion or integration, and response summarization to cite just the main topics. It is also important to be able to resolve situations in which no answer is found or when the answer is incomplete or fuzzy in the data sources. This can be resolved for example via dialogue or interactive QA scenarios. In order to address these future challenges and to guide research in this field, a number of QA roadmaps have been proposed: in 2001, Advanced Question and Answering for Intelligence, the (AQUAINT) program initiative, was created by ARDA. It was then revised in 2002 by the participants of the LREC (Language Resources and Evaluation Conference) QA workshop and further refined at the AAAI (American Association of Artificial Intelligence) Spring Symposium in 2004 and by the KRAQ series 2004-2008. In these roadmaps, the research community states that QA systems should support the integration of deeper modes of language understanding as well as more elaborated reasoning schemas in order to boost the performances of current QA systems as well as the quality and the relevance of the produced answers. Advanced QA systems can be viewed as an enhancement, rather than a rival to retrieval based approaches. In this paper, we present the main principles that we followed to identify titles, instructions, instructional compounds and arguments. From this point of view, our work is a contribution to the semantics of texts, including the recognition of some rhetorical relations, which is known to be a hard problem. This work is applied to French; we are developing a similar study for Thai. A priori, it should be possible to transpose it to a number of other languages. It is interesting to note how useful text semantics can be for question-answering. 2. How-To questions and the Structure of Procedural Texts The main goal of the work presented here is to be able to answer procedural questions, which are questions whose induced response is typically a fragment, more or less large, of a procedure, i.e., a set of coherent instructions designed to reach a goal. Recent informal observations from queries to Web search engines show that procedural questions is the second largest set of queries after factoid questions (de Rijke, 2005). The current text emerges from (Delpech et al 2007, 2008). Answering procedural questions thus requires being able to extract not simply a word in a text fragment, as for factoid questions, but a well-formed text structure which may be quite large. Analyzing a procedural text requires a dedicated discourse analysis, e.g. by means of a grammar. Such grammars (Webber 2004) are not very common yet due to the complex intertwining of lexical, syntactic, semantic and pragmatic factors they require to get a correct 66  analysis. Discourse grammars have basically a top-down organization, they take discourse acts as their basic units, instead of just words, they account for the structure and for the interactions between these acts and they require a relatively elaborated conceptual representation as output. Such a grammar must capture the discourse cohesion, possibly the communicative intentions, as well as the discourse organization, e.g. in terms of plans. Procedural texts are organized sets of instructions (Adam 2001); they may also be sets of advices, as in social behavior texts. In our perspective, procedural texts range from apparently simple cooking recipes to large maintenance manuals. They also include documents as diverse as teaching texts, medical notices, social behavior recommendations, directions for use, assembly notices, do-it-yourself notices, itinerary guides, advice texts, savoir-faire guides etc. Even if procedural texts adhere more or less to a number of structural criteria, which may depend on the author's writing abilities and on traditions associated with a given domain, we observed a very large variety of realizations, which makes identifying the structure of such texts quite challenging. Procedural texts explain how to realize a certain goal by means of actions which may be temporally organized. Procedural texts can indeed be a simple, ordered list of instructions to reach a goal, but they can also be less linear, outlining different ways to realize something, with arguments, advices, conditions, hypothesis, and preferences. They also often contain a number of recommendations, warnings, and comments of various sorts. The organization of a procedural text is in general made visible by means of linguistic and typographic marks. Another feature is that procedural texts tend to minimize the distance between language and action. Plans to realize a goal are made as immediate and explicit as necessary, the objective being to reduce the inferences that the user will have to make before acting. Texts are thus oriented towards action; they therefore combine instructions with icons, images, graphics, summaries, warnings, advices, etc. Research on procedural texts was initiated by works in psychology, cognitive ergonomics, and didactics. Several facets, such as temporal and argumentative structures have then been subject to general purpose investigations in linguistics, but they need to be customized to this type of text. There is however very little work done in Computational Linguistics circles. The present work is based on a preliminary experiment we carried out (Delpech et ali. 07), (Aouladomar 2005) where a preliminary structure was proposed. From a methodological point of view, our approach is based on (1) a conceptual and linguistic analysis of the notion of procedure and (2) a mainly manual corpus-based analysis, whose aim is to validate and enrich the former. Other works on this topic include (Delin et al. 1994), (Takechi et al. 2003) and (Yin, 2004). In terms of research, analyzing procedural texts is of much interest. Indeed this is a ‘genre’ which is quite well restricted, but that is rich enough to allow investigations in semantics and pragmatics: rhetorical relations, temporal relations, illocutionary force, etc. It makes relatively feasible studies that would be impossible in the open language. However, results remain proper to those texts. In fact, our approach, at several levels is to try to focus on prototypical structures describing a phenomenon, and then to investigate the conditions its extension to other areas. For example, titles in procedural texts on the web are very diverse in form and contents; they nevertheless share some properties that make their tagging relatively possible. This would not be the case on open texts. 3. Basic Analysis: Recognizing Titles and Instructions Let us now develop in more depth the different phases of our system. Preliminary steps include cleaning and labeling text objects. Then title and instructions can be recognized and tagged. 3.1. Cleaning Web texts and tagging The inputs of our system are raw Web pages. From these pages, we need: (1) to extract relevant text, that is, any kind of text that is not navigation help, advertisements or comments posted by cybernauts 67  (2) to select and simplify the html tags so as to keep the main typo-dispositional information (paragraph breaks, subdivisions of paragraphs into lines, lists and their subdivision into elements, emphasis). Although (2) was quite an easy task, we had some difficulties achieving (1). We designed an algorithm that returns, for each paragraph, if its text can be considered relevant or not. It mainly uses paragraph length and proportion of close-class words criteria. We evaluated it on 100 Web pages, coming from 12 different web sites. The results were: 0,95 precision and 0,76 recall. We evaluate the recognition of titles and instruction on a hand-cleaned corpus. The next stage is to tag the different lexical objects of the text, so that the segmentation of titles and instructions can be done properly. For that purpose, we use the Treetagger that labels all the objects (syntactic category, morphological features). We also add some semantic considerations about action verbs. Of particular interest to us is the recognition of verbs, some nouns and adjectives, modals and connectors of various kinds. 3.2 Recognizing Titles For answering How-to questions it is obviously of much importance to recognize titles and possibly hierarchies of titles in complex texts since, in general, titles express the main goals of procedures. However, for some domains (like health) additional information should be used to index a text, in addition to titles. A first observation is that html encodings are, by far, not homogeneous. Titles are coded with the tag <hi> in only 20% of the cases over the 600 titles observed. In most cases the tag <b> is used, possibly also <emp>, <u> and a few others (macros...). Encodings may be quite homogeneous within a given web site, but heterogeneity prevails over different sites, even in the same domain. We identify titles in two steps. First, an algorithm processes the paragraphs of the text one by one, and gives them one of these tags: title, text or ambiguous. This first step processes easy cases. For example, an easy case for a title is a paragraph composed of a unique sequence of words, less than 12 words long and bearing emphasis. The tag text will be given without doubt if the paragraph is subdivided into smaller units or is longer than 12 words. Ambiguous paragraphs are mainly short sequences of words (12 words or less) with no emphasis. The second step disambiguates the ambiguous paragraphs one by one, using the tags given by the first step to its surrounding paragraphs. For example, an ambiguous paragraph between two paragraphs tagged as text will be considered a title. Similarly, an ambiguous paragraph followed by a title is labeled text. We have elaborated about 7 such rules that deal with ambiguities. This second step also operates some repairs on the tags yielded by the first step. For example, any sequence marked title at the end of the text will be repaired as text. Each disambiguation/repair rule is applied sequentially and in a specific order on the list of tags. The title hierarchy is very difficult to identify without content analysis. However, standard procedural texts are not very long and tend to be relatively linear. This means that, besides the page title, we observed in 80% of our texts not more than 2 levels of titles (excluding the main title). We observed two regular types of titles that can be correlated to some form of hierarchy. Type 1 is a title separated from its following paragraph by a <p> tag. Type 2 is a title separated from its following paragraph by a <br> tag. Although we still have no means to tell the exact level titles, we can quite confidently say that a type 2 title will be at a lower level than a type 1 title, whatever the website or the domain. This information may be useful for question-title matching: type 2 titles are expected to introduce paragraphs that deal with more specific aspects of the procedure than paragraphs introduced by a type 1 title. Type 2 titles could help answering specific questions. One remaining difficulty for question-title matching is that titles have often a very elliptic structure. 68  
The usages of some linguistic expressions and constructions are constrained such that certain conditions must be satisfied by the context. When such a linguistic item is used, the hearer simply assumes that the required condition has been satisfied. In other words, some information is delivered to the hearer because of the presence of the linguistic item. In this paper, the information that is delivered to the hearer in this process is referred as ‘presupposition’. This notion of presupposition is closest to Stalnaker’s (1973) notion of presupposition. The difference is that while Stalnaker’s notion refers to the relation between a person and a proposition, I would like to think it as one between a sentence containing a presuppositiontrigger and a presupposed proposition, following Karttunen (1974). Although the notion of presupposition is so ubiquitous, how to identify it has not been clear. The dominant approach to identify a presupposition is to try to isolate the non-truth-conditional meaning in a given sentence that includes the presupposition-trigger. In other words, what we often assume to be a presupposition triggered by a linguistic item is discussed rather subjectively. In this study, following the approach employed in Shudo (1998, 2002) and Shudo and Harada (2008), I will try to identify a presupposition by reconstructing a condition that the context must satisfy in order for the speaker to use the presupposition-trigger. In particular, I will investigate the presupposition of even. The basic explanation for the assumed equation between the presupposition of a linguistic item and the contextual constraint on the usage of the item is simple: if a meaning is generated because of the usage of a presupposition-trigger, that is because the linguistic item is constrained such that it cannot be used unless the condition generating the meaning is satisfied. Shudo (1998, 2002), following Kay’s (1990) analysis on even, contended that the Japanese particle mo, which is equivalent to too or also and sometimes is used to generate the even-like meaning, is different from the English even. However, in this paper, I will argue that the meaning of even is indeed quite similar to the Japanese mo. * An earlier version of this paper was presented at the Pragmatic Society of Japan’s annual meeting in 2007. I would like to thank Yasunari Harada as well as Harumi Sawada for their comments. Copyright 2008 by Sachiko Shudo 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 74–86 74  2. Problem While there are disagreements on details, most works on even agree that even triggers two types of presupposition, existential presupposition and scalar presupposition (Horn 1969, 1972, Karttunen and Karttunen (K&K) 1977, Karttunen and Peters (K&P) 1979, Rooth 1985, Kay 1990, Wilkinson 1996, Schwarz 2005, Nakanishi 2006, inter alia). 1 (1a) gives a rise to an existential presupposition, shown in (1b), and a scalar presupposition, shown in (1c). (1) a. Even Emily smiled. b. Someone other than Emily smiled. c. Emily is a less likely (or the least likely) candidate to smile.2 In this paper, as mentioned earlier, whatever meaning is generated by the usage of even is assumed to be the result of the constraint on the usage such that it cannot be used unless the condition is satisfied. This paper contends that existing accounts for presupposition triggered by even are correct but not sufficient to explain the presupposition because even may not be used although the context satisfies the above mentioned two types of presupposition. Let us start with a scenario. Amelia (A) teaches German in a college. Brenda (B) is her colleague who also teaches the same class. Harry is one of the least proficient students. One day Brenda gave students a quiz. When Brenda comes back from the class, the following conversation begins. (2) a. A: Hi. How was the quiz?3 b. B: I am afraid it was too easy. c. Even Harry got seven right answers. Now let us observe the following in which Amelia starts differently. She has been lately quite concerned with Harry’s academic performance. (3) a. A: Hi. How did Harry do on the quiz? b. B: #Even he got seven right answers. (2c) and (3b) offer exactly the same semantic content. As for the performances of students on the quiz, not only Harry’s but also those of others, the context of (2) and (3) should be identical. In other words, what applies to the usage of even in (2), the existential presupposition, the scalar presupposition, etc, is expected to be present in (3). However, the usage of even in (3b) is problematic. It is obvious that the inappropriateness of (3b) has something to do with Amelia’s question. However, we can easily come up with a slightly different context in which (3b) is not problematic such as the following. (4) a. A: Hi. How was the quiz? b. B: Everyone did very well. c. A: How did Harry do? d. B: Even he got seven right answers. 
Gary F. Simons and Steven Bird  a SIL International and Graduate Institute of Applied Linguistics 7500 W. Camp Wisdom Road, Dallas, TX 75236, USA gary_simons@sil.org b Dept of Computer Science and Software Engineering, University of Melbourne, Victoria 3010, Australia and Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104, USA sb@csse.unimelb.edu.au  Abstract. This paper describes work the Open Language Archives Community (OLAC) is doing to contribute to a global infrastructure for the sustainability of language resources. After offering a definition of language resource, it addresses the issue of what makes language resources sustainable by defining six necessary and sufficient conditions for their sustained use, then discusses what it takes to make such sustainability a reality by describing the roles of four key sets of players—creators, archives, aggregators, users. With this background, the paper describes the community infrastructure OLAC has developed for allowing its members to express consensus about best practices for digital archiving, plus the technical infrastructure it has developed to provide aggregation and search for the language resources community. The concluding section probes the broader issue of sustainable development to consider the sustainability of language resources in the context of the sustainability of language development and of languages themselves. Keywords: Language documentation, metadata, digital archives, sustainable development 1. Introduction Sustainability has become a byword of our times. In fact, The Global Language Monitor recognized sustainable as the Top Word of 2006.1 Behind all the buzz there is an important concept that has significance even for our language resources community. Focus on the sustainability of the planet in the news media is making us increasingly aware that unless we mend our wasteful ways, we could squander the world’s natural resources along with the opportunity of future generations to enjoy the same quality of life that we do. The language resources community is no stranger to waste. Waste happens when the resources resulting from prior work are no longer available due to the deterioration of the media * This work is supported by the NSF Project OLAC: Accessing the World’s Language Resources, awards 0723357 and 0723864 to the University of Pennsylvania and the Graduate Institute of Applied Linguistics. Components of the infrastructure have been developed with our research assistants Haejoong Lee and Debbie Chang and archiving consultant Joan Spanne. We are grateful to members of the OLAC community for their collaboration in developing and implementing OLAC’s standards and recommendations. 1 http://www.languagemonitor.com/top_word_lists Copyright © 2008 by Gary Simons and Steven Bird 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 87–100 87  
1. Introduction The purpose of the current morpho-semantic analyzer is to understand the semantic structures of Japanese verb phrases, and to contribute to the semantic understandings of sentences. At present the system understands the morpho-semantic structures of the verb phrases of simple sentences. It employs the algorithms based on linguistic analyses and several lists of carefully grouped verbs. Being able to understand verb phrases semantically, it differs from other Japanese morphological analyzers, notably Juman (Kurohashi and Nagao, 2003) and Chasen (Matsumoto et al., 2000), which focus primarily on the segmentation of sentences into morphophonemes such as prefixes, suffixes, inflections, Case particles and the components of compound words, and on labeling them with syntactic classifications. Another major difference from other morphological analyzers is that the current system is exclusively rule-based while most analyzers, whether morphological or syntactic, are based on statistics, such as Uchimoto et al. (2003) and Murata et al. (2005), to name a few. The third difference is that the present system is able to identify the syntactic category, such as the adjective and the verb, of the head word of a verb phrase, and, if it is a verb, the conjugation group, even when it is not listed in the dictionaries of the system. The current system, unlike finite state models, is easy to trace the problems and extend itself according to needs, because each step in the algorithms is linguistically accountable and * Copyright 2008 by Yukiko Sasaki Alam 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 101–112 101  intuitively comprehensible. It parses the Japanese verb phrases of simple sentences, but not those with modal and honorific expressions and those in complex sentences. It can be embedded in a larger parsing system such as a sentence phrase analyzer (Alam, 2007). The system is encoded in Java programming language on an object-oriented design, consisting of several packages including those named phrases, words, suffixes, vp_parsers, verb_lists and utils. In the following, I will explain the components of each suffix class and the algorithms of their main methods all called check, with the Causative class in most detail, and then discuss future work, together with concluding remarks. 2. The Package of Suffixes The package of suffixes is composed of Causative, Passive, Desiderative, Negative and Inflection classes. Each class represents a verbal suffix having the same name. The suffixes appear in verb phrases in the fixed order in which the classes are listed above. Each class has a static method named check, and the check methods are called for in that order by the controller class of the system. In this section, I will explain each class, with the Causative class in more detail, because the ideas underlying many of the algorithmic steps in each class are similar. 2.1. The Causative Class and the check Method Among the Japanese verbal suffixes, the causative suffix, if any contained in the verb phrase, is the first that follows the stem of a verb. Thus, the parser must check first if it immediately appears after the stem, and the checking is performed by the static method of the Causative class entitled check. This method is called for from the class that embodies the main algorithm of the parser. The check method takes as the argument an object of the verb_phrase class, the components of which are to be identified in the process of parsing. It checks if the verb_phrase object contains an input string that matches a causative suffix, instantiates, when a matching string is found, a causative suffix object in the Suffix class, and registers the suffix object as a component of the verb phrase object, before returning to the system the verb phrase object with the added information. 2.1.1. The Causative Suffix and Regular Verbs The most important function of the check method is to check if the initial portion of the unprocessed string of the verb phrase instance matches a string that forms a causative suffix. For that purpose, the class provides two sets of patterns which are in the form of regular expressions, one to identify a causative suffix following a consonant verb1, and the other for a vowel verb, as illustrated below: (1) (a) Causative suffix pattern for a consonant verb static final String CAUSATIVE_CV = “[kgsmnbtwr]ase”2; (b) Causative suffix pattern for a vowel verb static final String CAUSATIVE_VV = “[kgsmnbtwr][ei]sase”; Given the two sets of causative suffix patterns, the parser is able to find out if the head word of the verb phrase is a consonant verb or a vowel verb. As shown in (1a) and (1b), the instances in the String class used for matching are in the form of regular expressions. Using the two string matching sets, the stem of the head verb of the verb phrase is determined in the following way. When one of the string sequences in the two sets matches the initial portion of the unprocessed string of the verb phrase, that means that a causative suffix is found, and that the stem of the head verb is ready to be determined. When the matched string is among the set for a consonant verb, the stem should be the one of a consonant verb, and it is 
1. Introduction This paper discusses the algorithms used for the automatic induction of grammar for the Filipino language. The rationale for the study stems on the minimal work done on the development of a computational grammar for the Filipino language for the development of robust and industrialstrength natural language analysis and technologies. Existing Filipino grammars can only handle a subset of declarative type sentences. Considering the difficulty to manually construct a robust grammar capable of parsing a broad scope of sentences, automatic grammar induction is a consideration that can be used for learning language structure. Automated grammar induction systems deals with the generation of a grammar based from input corpora. Existing work for grammar induction fall under two categories based on their input constraints: Supervised and Unsupervised. Slightly supervised systems generate grammar rules from bracketed corpora or tree banks. Bracketed corpora are text documents that have been bracketed by a linguist to represent the skeletal syntactic structure of the sentences. Treebanks are large corpora that have been annotated with the part of speech tags, syntactic structure, and other functional attributes necessary. Unsupervised systems make use of nonbracketed corpora while applying searching and clustering algorithms to attempt to learn the language rules. Works on slightly supervised grammar induction, such as the works of Lari and Young(1991), Brill(1993), Sekine and Grishman(1995), and Charniak(1996), present different output formalisms to represent the grammar. However, Filipino is currently a resource-limited language and does not have the computational resources necessary for the algorithms presented. There are existing corpora available for the language, but these have not yet been bracketed. * The authors wish to thank Dr. Shirley Dita for providing the initial gold standard. Copyright 2008 by Danniel Alcantara and Allan Borra 113 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 113–122  Osborne and Briscoe(1997), Clark(2001), Klein and Manning,(2001) attempt to learn the grammar formalism for the language through the use of statistical analysis methods applied to a tagged corpus. Klein and Manning(2002) applies a context-constituency model and achieved promising results, precision rate of 55% and recall rate of 48%. Existing systems are designed for the English language and have had modifications applied that are specific to the right-biased nature and Subject-Verb-Object structure. The Filipino language does not follow these structures; rather it has free word order patterns, and Predicate-Topic phenomena, wherein the focus of a sentence is referred to as Topic rather than as a Subject. Klein(2005) experimented with a combination of the said context-constituency model and a dependency model Klein and Manning(2004), which was then applied to English, German, and Chinese. The English language reached recall of 88% and precision of 69%. The German language had a corresponding recall value of 90%, but a considerably lower precision of 50%, caused by relatively flat gold standard corpora. The flat structure of the German language is attributed to free word order, a phenomena also identified in Filipino. The aim is to develop an automated grammar induction system using an unsupervised approach. The approach chosen is influenced by the limitation of computational grammar resources in Filipino. Existing induction algorithms, usually for the English language, are modified to handle Filipino language phenomena. 2. Unsupervised Grammar Induction Approaches Unsupervised grammar induction systems apply statistical or probabilistic analysis to estimate the necessary grammar formalism. Most unsupervised approaches apply language specific heuristics to improve computational reliability. 2.1.Current Solutions in Other Languages Clark(2001) developed an unsupervised approach for extracting the phrase-structure from an input English corpus. Contiguous tag subsequences are identified from an input tagged corpus, and are considered as constituent candidates. Tag sequences that occur at least 5000 times are clustered together based on their context, the part of speech tag immediately preceding and following. The clustering process identified sequences with clear syntactic correspondence. Accordingly, the procedure identified clusters with poor syntactic quality. Mutual Information criterion is applied to justify the valid clusters and filter out spurious candidates. Mutual Information indicates the importance of a relationship between the prior and subsequent part of speech tags. This intuitively implies that a correct constituent structure is highly sensitive on the context of usage and can appear in different contexts. The presented algorithm introduces the possibility to induce grammatical structure from an unsupervised approach. The initial results are filtered through Mutual Information heuristics to remove spurious candidates. The implementation of the algorithm minimizes the requirement on input corpora, but is computationally expensive and requires sufficient memory to handle clustering. Klein and Manning(2001) describes two systems for learning linguistic constituency in natural language grammars. Radford’s study, as cited in Klein and Manning(2001) discussed two linguistic criteria for constituency identified that serves as the primary basis for the work: 1. External distribution: A constituent is a sequence of words which appears in various structural positions with larger constituents; and 2. Substitutability: A constituent is a sequence of words with (simple) variants which can be substituted for that sequence. Klein and Manning take a tagged corpus and apply statistical analysis to identify most commonly occurring contiguous tag sequences. The tag sequences that reach the target criteria, based on a combination of entropy and divergence, are identified as constituent structures. Another work by Klein and Manning(2002) develops a “Constituent Context Model” describing contiguous subsequences within a sentence. Each sentence is described by a list of 114  all possible subsequences, identified by their span, enclosed terminals, and context, terminals before and after. Bracketing of the sentence is identified through probabilistic analysis of a class conditional independence model based on the input corpora. Subsequences are estimated as constituent structures or non-constituents, referred to as distituents. Initial bracketing is approximated based on a random split mechanism that promotes generation of unbalanced binary trees. The conditional completion likelihood of the current state is computed based on the specified Expectation parameters. Bracketing is adjusted and Maximized to further improve the generated constituents. This model focuses on identifying hidden bracketing structures, based on observable sentence and tag structures. A precision rate of 55% and recall rate of 48% was achieved through experimentation of the identified model, the best published unsupervised results at the time. The implementation of the EM algorithm maximizes the combinational likelihood of generating a correct bracket span. Although currently beyond the scope of the study of grammar induction for Filipino, it is noteworthy to discuss the work by Klein and Manning(2004) which made use of dependency structure and extraction to model the language. Unlike standard approaches, where the tree or phrase structure is being identified, the work identifies dependencies between words. Each word is directly dependent on another word, and a single word dependent on the ROOT component of the sentence.  2.2.Probabilistic Induction for Filipino: Architectural Design Figure 1 provides the overall architectural design for the system. The first module is the training module. In the process of training, the training corpora are tokenized into sentences. Quotations found within sentences, words enclosed within double quotes, are separated from their host sentence and are tokenized further. Tokenized sentences are then tagged through the use of an external part of speech tagger. The tagged sentences are passed through statistical analysis, in order to retrieve the necessary data used for probabilistic computations in rule generation.  TRAINING  TESTING  Training Corpora  Sentence Tokenizer PartofSpeech Tagger  Statistical Analyzer  Rule Generator Bracketed Corpora  Statistics Repository  Rule Repository  Input Corpora Sentence Tokenizer PartofSpeech Tagger Parser Parsed Corpora  Figure 1: Overall Architectural Design of the Probabilistic Constituent Structure Induction System for Filipino  115  The statistical analyzer identifies all part of speech tag sequences that occurred in the training corpus. The symbol α will denote a specific part of speech tag sequence. The occurrence count of each α is scored and this value is one of the primary measurements used in rule generation. All occurrences of α is identified within a context of two adjacent tags or sentence boundaries: x α y. x denotes the symbol directly before α, and y directly after. These two contexts may be merged together to form a local linear context, x-y. The distribution of contexts in relation to α will be denoted by σ(α, x), σ(α, y), and σ(α, x-y) for pre, post, and linear respectively. According to Klein and Manning(2001), a study by Radford states that “a constituent is a sequence of words which appears in various structural positions within larger constituents”. The phrase “various structural positions” suggests that the constituency of α can be identified by the entropy of its linear context, H(σ(α, x-y)). However, entropy alone is not enough to recognize a constituent. This is due to uncommon but possible linear contexts having little bearing on the entropy. A scaled entropy, (listing 1) takes into consideration the uniform distribution of contexts in relation to α, σu(α, x-y), and the uniform distribution of all possible contexts u. Hs(σ(α, x-y)) = H(σ(α, x-y)) [H(σu(α, x-y)) / H(u) ] (1) Clark(2001) discussed that if one were to consider the tag previous of a true constituent, one should be able to presume the tag directly after. However for non-constituents, possibly referred to as distituents, the two contexts form independent distributions. Therefore, one measure of constituency is identified by the Mutual Information between the pre and post contexts, MI(σ(α, x), σ(α, y)). Towards the end of the training phase, the resulting probabilities are applied to the input sentences and an estimated parse is proposed. Two models are implemented for bracket estimation and rule generation. The first is a Greedy Selection approach that brackets the highest ranked sequence per iteration based on the measurement identified. Algorithm Skeleton for the Greedy Selection Model. Rules GreedySelection (Corpus corpus) apply statistical analysis of corpus while (at least one sequence meets filter criteria) maxSeq = filtered sequence with maximum measurement generate production rule based on maxSeq for (all sentences in Corpus) parse sentence with generated production rule apply statistical analysis based on new parse return Rules end. The selection and filtering criterion is based on available measurements of the sequence. The simplest filtering criterion is requiring the sequence size to be at least two. Different types of measurements can be applied for selection and filtering, such as requiring Mutual Information of 0.2 while obtaining the maxSeq based on occurrence. The second model designed for training is a modification of the “Constituent Context Model” of Klein and Manning(2002). Sequences σ and linear contexts x-y are assigned probabilities of being a constituent or a distituent. Initial probabilities are derived from enumerating all possible valid bracketings of all sentences of the input corpora. Valid bracketing requires the bracket result in a binary tree. To minimize execution time, a three-dimensional dynamic matrix was implemented for said completions. 116  Overview of the Constituent Context Model Induction. Rules CCMInduction (Corpus corpus) apply statistical analysis of corpus Collection inside = sequence probabilities Collection outside = context probabilities do for (all sentences in Corpus) bracketSentence = InsideOutside(sentence, inside, outside, 0, sentence.size-1) apply statistical analysis base on new parse inside = bracketed sequence probabilities outside = bracketed context probabilities while inside changes and outside changes combine inside and outside to generate Rules return Rules end The Inside Outside algorithm is utilized to dynamically estimate the optimum binary bracketing of each sentence. Sequence probabilities σ represent the inside computations and linear contexts x-y represent the outside. The process applies a divide and conquer approach to identifying the bracketing that will maximize the resulting probability. The estimated probabilities can also be weighted based on the previously identified measurements. Skeleton of the Inside Outside Algorithm [bracketSentence,prob] InsideOutside (Sentence sentence, Collection inside, Collection outside, int nStart, //starting index token or tag int nEnd) //end index token or tag String yield = sentence[nStart..nEnd] String context = sentence[nStart-1] + sentence[nEnd+1] double probC = inside[yield] * outside[context] double probD = (1-inside[yield]) * (1-outside[context] double prob = weight(yield) * probC / (probC + probD) if(nStart != nEnd) for( k = nStart ; k < nEnd ; k++) left = InsideOutside(sentence,inside,outside,nStart,k) right = InsideOutside(sentence,inside,outside,k+1,end) maxProb = max( left.prob * right.prob ) prob *= maxProb bSentence = merge left.bracket with right.bracket based on max return [bSentence, prob] end After initial bracketing of all sentences, probabilities are recomputed and the process iterates in the form of an Expectation-Maximization algorithm. This will induce structure to the corpora and create a globally higher score as compared to the previous model. The second module parses the input sentences utilizing the rules estimated during training. Prior to the actual parsing process, the input corpora is processed by the sentence tokenizer and part of speech tagger, equivalent to the learning phase. The architecture makes use of three data sources, the Training Corpora, the Statistics Repository, and the Rule Repository. The Training Corpora is a non-bracketed data source that contains sentences for Grammar generation. It is possible that the training corpora be pretagged for correctness, but that is the only amount of preprocessing necessary for the unsupervised approach. The Statistics Repository will contain data relating to the training corpora, this includes the occurrence of tag sequences, the contexts of these sequences, and the corresponding occurrence. The Rule Repository will be the storage facility for grammar rules in the learning phase and the source of these rules for the testing phase. 117  3. Linguistic Data The system presented takes as input a collection of sentences. The part of speech tags for each word are then identified and the algorithms only take into consideration the tags rather than their surface form. Sentences were manually verified, to minimize tagging error; however this is the only amount of preprocessing for both learning and parsing phases. A bracketed version of the corpus is available, and this is used for evaluation purposes only. The part of speech tagset used contains 66 word classes, each falling into one of 9 supersets (noun, pronoun, determiner, conjunction, verb, adjective, adverb, cardinal, punctuation), and one added word class used to signify a quotation. The tagset is based on the updated tag set of TPOST. Punctuation marks are not removed from the input corpora, however statistical data is not gathered for sequences that contain punctuation marks. The punctuations are part of the final grammar rules, but they do not directly contribute to the estimation of constituents. The corpus used for training was the Filipino translation of The Little Prince by Antoine de Saint – Exupéry. The selected corpus contains 1,685 sentences composed of approximately 15,000 words, and can be categorized under the domain of literature. The dataset contains a range of sentences, mostly simple and declarative sentences. Due to the fictional narrative nature of the input corpora, a high percentage of the sentences serve a declarative purpose. There are blocks of dialogue found inside the story; this introduces the other types of sentence structures. “The Little Prince” is generally a children’s book, the reason for the majority of simple sentences, but compound and complex sentences are also identified in the narration. The experiments presented are done using sentence lengths of 1-10. Majority of the simple sentence structures can be found within this range. Compound and complex sentences are variations of the simple sentence construct and experimentation focused on simple sentences only is imperative. 4. Preliminary Results and Analysis This section discusses the results and analysis from statistical data as well as evaluating proposed bracketings produced by the system against a linguistically verified gold standard. 4.1.Analysis of Statistical Data Preliminary analysis is focused on the statistical analyzer, and identifies the appropriateness of the identified measurements. Table 1 lists the top ten most frequently found constituents in the Gold corpus.  Table 1. Selected constituent sequences found in the gold corpus with corresponding statistics.  Gold  Gold  Raw  Scaled Mutual  Sequence  Probability Count Occurrence Entropy Entropy Information  CCB NNC  87.16% 129  148 4.495 2.376  0.760  JJD NNC  94.16% 129  137 3.956 1.976  0.691  DTCP NNC  83.00%  83  100 4.670 2.381  0.839  CCB JJD NNC  100.00%  71  71 2.944 1.168  0.375  DTC NNC  52.46%  64  122 5.787 3.440  1.435  CCT PRSP  76.25%  61  80 4.908 2.534  1.132  NNC PRS  71.21%  47  66 4.875 2.452  1.696  PRSP NNC  86.79%  46  53 3.466 1.375  0.548  CCT NNC  67.74%  42  62 4.930 2.514  1.498  VBW CCB JJDNNC  93.18%  41  44 0.994 0.150  0.000  RBI PRS  3.57%  2  56 5.222 2.680  2.292  CCB JJD  2.63%  2  76 2.158 0.782  0.307  RBF PRS  1.43%  
Keywords: cosubordination, arguments, control construction, nexus, core. 1. Introduction Cosubordination is defined as one type of nexus relation in which units of equivalent sizes are strung together in a coordinate-like relation with no marker of syntactic dependency is found between and among units (Olson, 1981). These units share some grammatical categories such as arguments, aspects, negation, and other operators. Argument sharing is a process in which one argument in the matrix core is the same with another argument found from the linked core. However, the problem lies in the syntactic representation of a cosubordinate clause in which there is a missing argument from the linked core. That seems to be a violation of a theory known as Completeness Constraint as it states that all arguments overtly expressed in the semantic representation of a clause must be realised in the syntax (Van Valin, 2005). Before we move on to our discussion of argument sharing, let us first clarify some terms in the literature. Control construction, as one theory applied in analysing argument sharing in cosubordination, refers to how the controller of the missing NP in the linked core is to be determined (Van Valin, 2005). In an ergative language like Filipino, subject control and object control cannot be used, for these terms will lead to some problems. Thus, non-subject actor, controller, or syntactic pivot will be used to avoid confusion. In sentence (1), the single argument of the matrix core realised by ko ‘I’ is in ergative case functioning as a non-subject actor in the construction and cannot be called either subject or object controller. The semantic role of the argument ko ‘I’ is assigned by its nucleus pinilit ‘tried’, a patient verb. The argument in the linked core, on the other hand, is missing but although this argument is not overtly expressed in the syntax, it is clear that this argument is the same with the syntactic argument in the matrix core making this non-subject actor the controller by default. * Copyright 2008 - Aquiles P. Bazar III 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 123–130 123  (1) Pinilit kong matulog ng maaga -in-pilit ko-ng ma-tulog ng ma-aga PAT.PERF-try.hard 1SG.ERG-LNK ACT-sleep OBL ADJ-early ‘I tried (hard) to sleep early.’ Even in transitive clauses like in sentence (2), the overt syntactic argument in the matrix core realised by Nanay ‘mother’ is the same with the missing argument from the linked core. It is apparent, however, that the matrix argument Nanay ‘mother’ is in ergative case and therefore functioning as a non-subject actor. (2) Iniisip ni Nanay na lutuin ang pansit -in-iisip ni=Nanay na luto-in ang=pansit PAT-IMPRF-think ERG=mother LNK cook-PAT ABS=noodle ‘Mother is thinking of cooking the noodles’ On the other hand, in sentence (3), the second argument of the matrix core realised by ako ‘I’ is the controller of the construction, for the missing argument of the linked core is the same with this argument. Though this NP takes the role of a patient because of the semantic role of its verb, the term object control cannot be used, for this argument is in absolutive case and therefore functioning as a subject. The first argument, on the other hand, realised by niya ‘he or she’ is an ergative pronoun but not functioning as the subject of the construction. (3) Pinilit niya akong matulog ng maaga -in-pilit niya ako-ng ma-tulog ng maga PAT.PERF-force 3SG.ERG 1SG.ABS ACT-sleep OBL ADJ-early ‘He/she forced me to sleep early.’ 2. Discussion In nonsubordinate constructions, the semantic role of the overt syntactic argument is determined by the nucleus of the first core which also assigns the nexus relations and levels of junctures of the units involved, whereas the nucleus of the second core assigns the syntactic structure of the sentence (attransitive, single argument, transitive) and the grammatical relations of its arguments. This is the case of a number of verbs in Filipino like pinilit ‘tried or forced’ that may be interpreted either as cosubordinate or coordinate depending on their function and meaning. In sentence (4), the verb pinilit ‘tried’ requires a single argument in the matrix core which is and must be the same with the missing argument from the linked core. This construction is interpreted as cosubordinate. (4) Pinilit niyang buksan ang pinto in-pilit niya-ng bukas-an ang pinto PAT.PERF-try 3SG.ERG-LNK open-PAT ABS door ‘He/She tried to open the door.’ If the verb pinilit ‘forced’, on the other hand, functions as a causative verb requiring two arguments in the matrix core, one as an actor and the other as undergoer, the controller will therefore be realised by the undergoer and not by the actor. This is interpreted as coordinate construction, as shown in Pinilit niya akong ligawan ang kapatid niya -in-pilit niya ako-ng ligaw-an ang kapatid niya PAT.PERF-try 3SG.ERG 1SG.ABS-LNK court-PAT ABS sister 3SG.GEN ‘He/she forced me to court his/her sister.’ 124  Having one overt syntactic argument in the matrix core is not an indication that this construction is cosubordinate. An example of this is the coordinate verb gusto ‘want’ in sentence (5) in which the matrix argument may be the same with the missing argument from the linked core, as shown in (5) Gusto ni Lisang basahin ang libro gusto ni=Lisa-ng basa-in ang=libro want ERG=Lisa-LNK read-PAT ABS=book ‘Lisa wants to read the book’ However, the same overt syntactic argument in the matrix core may not also be the same with the actor argument in the linked core, as shown in (6) Gusto ni Lisang basahin ko ang libro gusto ni=Lisa-ng basa-in=ko ang=libro want ERG=Lisa-LNK read-PAT=1SG.ERG ABS=book ‘Lisa wants me to read the book’ Levels of junctures are also assigned by the nucleus of the matrix core. This will be realised when the matrix core takes a ma-prefix. Sentence (7) is in core juncture, for the two cores have their own sets of arguments. The non-subject actor Marta is an argument of the matrix verb sumubok ‘tried’ as it assigns its semantic role, whereas the absolutive NP puto ‘rice cake’ is an argument of the linked verb iluto ‘cook’. The missing argument from the linked core, however, is the same with the matrix core Marta. (7) Sumubok si Martang iluto ang puto -um-subok si=Marta-ng i-luto ang=puto ACT.PERF-try ABS=Marta-LNK PAT-cook ABS=rice.cake ‘Marta tried to cook (the) rice cake’ On the other hand, when the two verbs occur closer to each other, the argument Marta is no longer an overt sole argument of the verb sumubok ‘tried’ as seen in its morphosyntactic coding. In sentence (8), the role of the argument Marta which is in ergative case is no longer assigned by the actor verb sumubok ‘tried’ nor by the patient verb iluto ‘cook’ but these verbs merge as one requiring one set of arguments. There seems to be no argument sharing in this construction anymore because this sentence is in nuclear juncture. (8) Sumubok iluto ni Marta ang puto -um-subok i-luto ni=Marta ang=puto ACT.PERF-try PAT-cook ERG-Marta ABS=rice.cake ‘Marta tried to cook (the) rice cake’ One important thing to realise about single-argument cosubordinate constructions is that if there is any argument in the clause, it belongs to the matrix core and not to the linked core. Sentence (9) shows control construction where the single overt syntactic argument realised by Pedro found in the matrix core is the controller by default; that is, the missing core argument of the actor verb tumakas ‘escape’ is also the same with this core argument. (9) Tinagka ni Pedrong tumakas -in-tangka ni=Pedro-ng –um-takas PAT.PERF-try ERG=Pedro-LNK ACT-escape ‘Pedro tried to escape’ 125  Even when the two nuclei or verbs occur closer to each other, the presence of one overt syntactic argument will suffice the need for the second one, as shown in (10) Sinubok umawit ni Lisa -in-subok um-awit ni=Lisa PAT.PERF-try ACT-sing ERG=Lisa ‘Lisa tried to sing’ What is interesting about a single-argument clause is when the two verbs take two different focus affixes to assign two different morphosyntactic codes to one syntactic argument. In the sentence below, the verb binalak ‘try’ takes the semantic role of a patient or undergoer, whereas the verb sumayaw ‘dance’ takes the role of an actor. The pronominal niya ‘he or she’ which is in ergative case agrees with the verb binalak and not with the verb sumayaw which is apparent for this pronominal clitic is an argument of this matrix core, as shown in (11) Binalak niyang sumayaw sa plasa -in-balak niya-ng –um-sayaw sa plasa PAT.PERF-plan 3SG.ABS-LNK ACT.AGT-dance DAT park ‘She was planning to dance in the park.’ In sentence (12), the first core pinilit ‘try’ takes the role of a patient, whereas the linked core realised by kumalma ‘to be calm’ takes the role of an actor. It is apparent, however, that the missing argument from the linked core is the same with the overt core argument in the matrix core whose semantic role is determined by its nucleus. (12) Pinilit niyang kumalma -in-pilit=niyang um-kalma PAT.PERF-try=3SG.ERG-LNK ACT-calm ‘She tried to (be) calm’ Even when the two verbs occur closer to each, it is clear that the semantic role of the overt matrix argument is determined by the matrix core or verb and not of the verb closer to it. In sentence (13), the role of the non-subject actor Lisa is determined by the matrix verb binalak ‘planned’ although the actor verb sumayaw ‘dance’ occurs closer to this argument. (13) Binalak sumayaw ni Lisa sa plasa. -in-balak –um-sayaw ni Lisa sa plasa PAT.PERF-plan ACT.AGT-dance ERG Lisa DAT park ‘Lisa was planning to dance in the park.’ When the verb of the matrix core takes the role of an actor, it is still apparent that the morphosyntactic coding of the overt syntactic argument is determined by the semantic role of the matrix core, as shown in (14) Nagbabalak siyang sumayaw nag-babalak=siya-ng –um-sayaw ACT-IMPRF-plan=3SG.ABS-LNK ACT-dance ‘He/she is planning to dance.’ This is different from another structure very much closer to sentence (13) as discussed above. When the matrix verb takes the role of an actor and the second verb occurs closer to it, it is quite 126  apparent that the argument Lisa is no longer determined by any single verb as seen in sentence (15), but the two verbs nagbabalak ‘planning’ and isayaw ‘dance with’ merge as one forming one semantically complex verb having one set of arguments. Having said that, this construction is interpreted as a serial verb construction in nuclear lever. (15) Nagbabalak isayaw ni Lisa ang bata nag-ba-balak i-sayaw ni=Lisa ang=bata ACT-IMPRF-plan PAT-dance ERG=Lisa ABS=child ‘Lisa planned to dance with the child’ Looking into the missing syntactic argument of the linked core, on the other hand, will make us realise that although this argument is the same with the overt syntactic argument of the matrix core, it takes a different morphosyntactic coding assigned by its nucleus. As shown in sentence (16), the matrix core binalak ‘planned’ assigns a patient role to its syntactic argument niya ‘he or she’. However, although the argument of the linked core ‘sumayaw ‘dance’ does not appear in the clause syntactically, we can conclude that the semantic role of this argument will be absolutive as assigned by its verb. (16) Binalak niyang sumayaw (siya) -in-balak niya-ng –um-sayaw siya PAT.PERF-plan 3SG.ERG-LNK ACT-dance 3SG.ABS If the argument of a single-argument cosubordinate clause takes the role of an actor in order to agree with the second verb, the result is ungrammaticality of the sentence, as shown in, (17) *Binalak sumayaw si Lisa sa plasa. -in-balak –um-sayaw si Lisa sa plasa PAT.PERF-plan ACT.AGT-dance ABS Lisa DAT park Cosubordinate construction at core level becomes more complicated in transitive clauses. In a single-argument clause, the controller is the overt syntactic argument of the matrix core by default and there is no other overt syntactic core argument in the linked core. In transitive clauses, on the other hand, the controller in the construction is always realised by a non-subject actor as stated in La Polla and Van Valin (1997). Theory of obligatory control 1. Causative and jussive verbs have undergoer control. 2. All other (M-)transitive verbs have actor control. In transitive cosubordinate clauses, the relationship between the controller in the matrix core and the controllee (missing argument) in the linked core, which are both actor arguments, is not affected by the number of arguments present in the linked core and the semantic role taken by the linked verb. Core chain illustrates control construction in which the overt core argument in the matrix core realised by Leo is the same with the missing argument from the linked core despite the presence of an absolutive argument pinto ‘door’. (18) Pinilit ni Leong buksan ang pinto -in-pilit ni=Leo-ng bukas-an ang=pinto PAT.PERF-try ERG=Leo-LNK open-PAT ABS=door ‘Leo tried to open the door.’ Even if the linked core takes different semantic roles- e.g. instrumental, benefactive, locativethe missing argument from the linked core will still be the same with the non-subject argument 127  in the matrix core. In sentence (19), although the linked core realised by the verb ipansayaw ‘dance with’ takes the role of an instrument, the missing argument is still the same with the nonsubject actor in the matrix core. The presence of an absolutive argument sapatos ‘shoes’ does not affect the argument sharing of the two cores making the nonsubject actor niya ‘he or she’ the controller. (19) Pinilit niyang ipansayaw ang bagong sapatos -in-pilit=niya-ng ipan-sayaw ang=bagong=sapatos PAT.PERF-try=3SG.ERG-LNK INSTRM-dance ABS=new=shoe ‘He/she tried to dance with her new shoes’ In sentence (20), the linked core realised by the benefactive verb iluto ‘cook’ requires to have an absolutive argument realised by Ana. This does not affect the argument sharing of the two cores because the missing argument from the linked core is still the non-subject actor found in the matrix core. The semantic argument that is absent syntactically from the linked is recoverable from the matrix core. (20) Sinubok ni Nanay na iluto si Ana ng paborito niyang meryenda -in-subok ni=Nanay na i-luto si Ana ng paborito niya-ng meryenda PAT.PERF-try ERG=Nanay LNK BEN-cook ABS Ana OBL favourite 3SG.GEN snack ‘Mother tried to cook Ana her favourite snacks’ Even if the two nuclei or verbs occur closer to each, the controller in the matrix core is the same with the missing argument from the linked core which is also an actor argument. The presence of other arguments even the absolutive arguments does not affect the controllercontrollee relationship of the two cores, as seen in (21) Pinilit kunin ni Alex ang pera kay May -in-pilit kuha-in ni=Alex ang=pera kay=May PAT.PERF-try get-PAT ERG=Alex ABS=money LOC=May ‘alex tried to get the money from May’ (22) Sinubok ipansayaw ng babae ang bago niyang sapatos -in-subok ipan-sayaw ng=babae ang=bago=niya-ng=sapatos PAT.PERF-try INSTRM-dance ERG=woman ABS=new=3SG.GEN-LNK=shoe ‘The woman tried to dance with her new shoes’ (23) Iniisip niyang pagkulahan ng damit ang bubong namin -in-i-isip=niya-ng pag-…-an-kula ng=damit ang=bubong=namin PAT-IMPRF-think=3SG.ERG-LNK LOC-bleach OBL=clothes ABS=roof=2PL.GEN.EXCL ‘He/she is thinking to bleach their clothes on our roof’ When a matrix verb takes a ma-prefix, the argument sharing of the two cores is not affected, because although the matrix core argument is in absolutive case, the missing argument from the linked core is still the same with it, as seen in (24) Nagplano si Tatay na ibenta ang bukid nag-plano si Tatay na i-benta ang bukid ACT.PERF-plan ABS Father LNK PAT-sell ABS farm ‘Father planned to sell the farm’ 128  (25) Nagsimula na si Lisang ipansayaw ang bagong damit nag-simula=na si=Lisa-ng ipan-sayaw ang=bagong=damit ACT.PERF-start=already ABS=Lisa INSTRM-dance ABS=new=dress ‘Lisa has started to dance with her new dress’ In sentence (26), there seems to have two absolutive arguments: one realised by the matrix core argument Rodel and the other realised by the linked core argument libro ‘book’. Although the linked verb sulatin ‘write’ has an absolutive verb, the fact remains that its missing argument is the same with the absolutive argument found in the matrix core and therefore making argument sharing possible in this construction. (26) Sumubok si Rodel na sulatin ang libro -um-subok si Rodel na sulat-in ang libro ACT.PERF-try ABS Rodel LNK write-PAT ABS book ‘Rodel tried to write the book’ In the theory of obligatory control, it state that causative verbs should have undergoer control, whereas all other (M-)transitive verbs actor control. The verb nahirapan ‘have a hard time’ seems to be a violation of this theory. In control construction, only causative verbs must have undergoer control. But in sentence (27), the argument estudyante ‘student’ in absolutive case seems to be the controller. However, looking into structure will tell us that the verb nahirapan ‘having a hard time’ is uses a ma-prefix which predicts that its sole argument estudyante ‘student’ should be in absolutive case. And since nahirapan ‘have a hard time’ functions as an actor verb in the construction requiring only a single actor argument in the matrix core, its single argument will be its controller by default, and therefore not violating the theory. (27) Nahirapan ang mga estudyanteng sagutin ang test na-…-an ang mga estudyante-ng sagot-in ang test ACT.PERF-have.a.hard.time ABS PL student-LNK answer-PAT ABS test ‘The students had a hard time answering the test.’ 3. Conclusion 1. Non-subject actor is a better term to use in identifying the controller in cosubordinate construction. The use of subject control and object will lead the readers to confusion, because the language is interpreted to exhibit an ergative system. 2. In nonsubordinate constructions, the semantic role of the overt syntactic argument is determined by the nucleus of the first core which also assigns the nexus relations and levels of junctures of the units involved, whereas the nucleus of the second core assigns the syntactic structure of the sentence (attransitive, single argument, transitive) and the grammatical relations of its arguments. 3. In a single-argument clause, the overt syntactic argument found in the matrix core is the controller by default. 4. One important thing to realise about single-argument cosubordinate constructions is that if there is any argument overtly expressed in the clause, it belongs to the matrix core and not to the linked core. 5. In transitive cosubordinate clauses, the relationship between the controller in the matrix core and the controllee (missing argument) in the linked core, which are both actor arguments, is not affected by the number of arguments present in the linked core and the semantic role taken by the linked verb. 129  ABBREVIATIONS  
Paul You-Jun Chang and Kathleen Ahrens  a Graduate Institute of Linguistics, National Taiwan University, Taipei, Taiwan R. O. C. r95142005@ntu.edu.tw b Hong Kong Baptist University, Kowloon Tong, Hong Kong ahrens@hkbu.edu.hk  Abstract. As previous word adoption models, though proposing potential factors that influence the survival of neologisms, receive little empirical examination, this corpus-based study compares the performance of two such models by providing clear operational criteria for each factor in the models and, consequently, proposes a hybrid model that improves the previous results. We focus on seventy-seven Chinese novel verbs that appeared about ten years ago, defining their survival/failure in the real world, and examine the accurate prediction ratio of the two models. Both models display an overall accuracy of about 60 percent. However, as certain factors, e.g., unobtrusiveness, appear to be invalid predictors for the Chinese data, we attempt to improve the results by deleting inappropriate factors and by adjusting the weightings. As the overall accuracy was improved to about 70 percent, we suggest that this study would shed light on the potential factors that influence the survival of Chinese novel verbs. Keywords: Neologism, Word Adoption Model, Unobtrusiveness. 1. Introduction One interesting aspect about the human cognition is its ability to create. And creativity in language has doubtlessly inspired numerous studies. Among others, one phenomenon that commonly occurs would be the emergence of neologisms. For one thing, all words, together with their senses, that exist in our current vocabulary were once new (Klein and Murphy, 2001) and must have undergone a certain developmental process to finally remain in the vocabulary. For another, we still see this process going on in everyday life as can be seen from the coinage of new words, such as blog or Y2K. Previous studies on neologisms, therefore, mostly focused on the collection of new words (e.g., Algeo and Algeo, 1991), the analysis of their inherent features (Hsu, 1999; Rey, 1995), or their relationship with the society (Hsu, 1999). Metcalf (2002; cf. Chang, 2008) and Kjellmer (2000), among the few, nevertheless attempted to observe the factors that influence the adoption of neologisms in a language and, respectively, proposed a scoring system to assess the possibility for novel words to survive or to fail. 1 Interestingly, these two models are similar regarding certain factors as important, e.g., morphological productivity, * We appreciate comments from two anonymous reviewers and Siaw-Fong Chung, as well as comments from the audience in CLDC-2 for a previous, smaller-scale paper. We also appreciate Professor Janice Fon for her comments on the statistics and Yu-You Chen, Yao-Zhu Zhang, and Yi-Wei Lin for help in collecting the frequencies. The remaining errors are our own. 1 Part of this paper (mainly 2.1 and parts of Sections 3 and 4, about Metcalf’s model,) were presented at The Second Conference on Language, Discourse, and Cognition (CLDC-2). As an extension, this study contrasts Metcalf’s model with Kjellmer’s and proposes a hybrid model for improvement of the results. Copyright © 2008 by Paul You-Jun Chang and Kathleen Ahrens 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 131–140 131  and yet are contrastive in their evaluation of linguistic gaps and borrowings. Following their studies, Sabino (2005) used the two models to evaluate the possibility of the word gameday to survive and found that both models yield a roughly 0.7 probability. While this interesting study displayed the possibility to test Metcalf’s and Kjellmer’s models empirically, the fact that the study looked at only one word, and that gameday itself was a new word in development, leaves the accuracy of prediction, i.e., the performance of the two models, unaddressed. Moreover, although Chang (2008) empirically examined Metcalf’s model, no such study was done for Kjellmer’s model or for the comparison of the two models. Therefore, for a more adequate evaluation, comparison, and improvement of the performance of both of the models, an empirical study with a set of clear criteria for each factor in the two models would seem necessary. This study, therefore, attempts to examine the models proposed by Metcalf (2002) and Kjellmer (2000) by focusing on Chinese novel verbs that appeared around ten years ago. In doing so, we propose, for each factor in the models, a set of criteria for the scoring of the neologisms by using corpus tools. We ask the following questions: (1) Can we predict the survival of Chinese novel verbs from 1996 to 2006 according to Metcalf’s and Kjellmer’s models? (2) If yes, which provides better prediction? (3) If no, how can we better predict the survival of these words by modifying their conditions? And, as these two models contrast in their notion of linguistic gaps and borrowings, we hypothesize that (1) in Chinese, unlike the situation observed in English, borrowings and translated words take no disadvantage, while gapfilling is beneficial, if not essential, for the survival of novel verbs. We demonstrate, by setting up clear criteria, that although the two models perform similarly in predicting successful words, Metcalf’s model performs better at predicting failure words. While the results suggest room for improvement of the models, we further analyze the factors, deleting inappropriate ones or adjusting the weightings; consequently, we improved the overall accuracy to about 70 percent. 2. Word Adoption Models 2.1. Metcalf’s Fudge Scale Metcalf (2002) proposed the FUDGE scale to measure the probability of a word’s survival based on his observation of English neologisms. The crucial factors, as Metcalf stated, involve (1) frequency of the words; (2) unobtrusiveness, i.e., a successful word should not be exotic or too cleverly coined; (3) diversity of users and situations, i.e., the range of their usage; (4) generation of other forms and meanings, namely the productivity of the word; and (5) endurance of the concept, related to the concept’s reference to a historical event. The method of assessing the probability is to rank the new words from level 0 to level 2 in each factor and sum up the total scores in the end. The higher the scores are, (7, as proposed,) the more likely the new words are to survive. Metcalf downplays cleverness, exoticness, and linguistic gaps. He states that obtrusive new words cannot last long, and that words such as skycap, scofflaw, agnostic, etc., caught on because they do not look exotic, or they do root from a certain source language, which English has opened to all along. In addition, Metcalf claims that lexical gaps do not necessarily provoke new words. For example, although the Hebrew word hesed implies God’s mercy, tenderness, and love, it was inadequately translated as mercy in English, thus losing certain original meanings. Metcalf states that gaps such as these do not seem likely to be filled in the near future; communication can be sustained without a proper word for everything. As will be noticed later, this model differs from Kjellmer’s model in terms of humor, gap filling, and exoticness. 2.2. Kjellmer’s Model Kjellmer (2000) presents thirteen conditions to assess potential words, which can be divided into five categories: semantic, phonological, morphological, and graphematic conditions, and others, such as prestige. Firstly, semantic conditions include the existence of a semantic parallel, namely a pre-existing semantic pattern in the language (e.g., –able for adjectives meaning “capable of being V-ed”) as 132  well as semantic transparency, the straightforwardness of the new word’s meaning. Secondly, phonological parallels, i.e., well-established sound combinations in the language, and ease of pronunciation are considered influential factors. Thirdly, a word is more likely to survive if it has a morphological parallel, filling up a gap in a pre-existing morphological pattern in the language; e.g., the potential word pensivity would fill up a gap where the suffix pattern –ive would turn into its nominal form –ivity. In addition to this, Kjellmer states that a new word is also likely to survive if it does not fill up a morphological gap and yet follows the general morphological rules in the language. Besides, Kjellmer states that highly productive affixes (e.g., -ness) may facilitate the survival of new words. Furthermore, the more compatible the etymological roots of the new word and its affixes are, the more likely the word will be accepted. In terms of graphemes, a word following the graphematic customs in the language and a word whose spelling agrees with the pronunciation would possibly succeed. Finally, words that carry a prestigious, exotic, or humorous connotation, together with words that are concise, would also possibly survive.2 In the following sections, we therefore seek to evaluate the performance of the two models. 3. Methodology 3.1. Real World Cases: Evaluating the Words’ Survival in UDN Database 3.1.1. Defining Neologisms and Narrowing Down the Scope In this study, we gather our data from the collection of neologisms from July 1996 to December 1997 in Taiwan (National Languages Committee, 1998). Containing 5,711 new words, this collection selects new words according to their appearance in the Revised Dictionary of Mandarin Chinese based on a day-by-day examination of major newspapers. For our current study, we narrowed down our scope to the category of fashion words (cf. Hsu, 1999) and focused only on non-sense-neologism verbs (cf. Rey, 1995; Hsu, 1999), since we could not obtain accurate frequencies for sense-neologisms from our current corpus, and verbs and nouns may behave differently (Ahrens, 1999). Excluding inappropriate words, such as English acronyms, monosyllabic words, and foul language, we obtained seventy-seven verbs in the end.3 3.1.2. Normalizing the Frequencies and Defining Their Actual Success As researchers working on LIVAC (Linguistic Variations in Chinese Speech Communities) synchronic corpus (http://www.livac.org/) and Fischer (1998) have conducted similar studies, we would currently collect the year-by-year frequencies of these neologisms in the UDN (a major newspaper in Taiwan) database4 from 1996 to 2006, since newspapers might capture the emergence and fading away of new words. These frequencies were further normalized to the frequencies per 10,000 characters. Thus, we obtained the normalized ratios of the words. We further set up criteria to define the words’ actual survival or failure by looking into the words’ normalized ratios in 2006. As the words’ appearance in print would indicate a wide usage, we set the threshold at a low level, in this case 0.3 (about 10 tokens) and 3 (about 100 tokens). Namely, a word having a normalized ratio less than or equal to 0.3 in 2006 (e.g., 哈草, ha1 cao3, ‘to smoke,’ normalized ratio=0.11) would be counted as a failure, and a word having a normalized ratio greater than 3 (e.g., 抓包 zhua1 bao1, ‘(to be) caught doing something,’ normalized ratio=4.24) would be counted as a survival. If a word has a normalized ratio greater than 0.3 but less than or equal to 3 (e.g., 哈啦, ha1 la1, ‘to chat,’ normalized ratio=2.67), we would further look at its slope of the normalized ratios throughout the years to observe its 2 Kjellmer actually discusses three more factors: semantic needs, prompting of media, and fashion; however, since no actual scores are assigned to these factors, they are currently not included in the analysis. Please visit http://graftedlife.googlepages.com for details about the factors and scores in Metcalf’s and Kjellmer’s models (appendices I and II) as well as for all the other appendices of this paper. 3 For the list of verbs and related data, please refer to appendix III. 4 For a description of the corpus, please refer to appendix IV. 133  developmental tendency, since we cannot be sure whether such words would really survive; in this case, only words with a slope of less than -0.06 would be counted as failures. All of these criteria were set up through our observation and comparison in the data. The results of our definition are summarized as follows: Table 1. Survival and Failure Thresholds  NR≦0.3 23  Failure  Survival  0.3<NR ≦3  Slope ≦-0.06  Slope>-0.06  
1. Introduction With the emergence of the Internet, lack of information is no longer a major problem most users face. The vast amount of information available is overwhelming and it is important to hide information that is irrelevant to the users. One example that provides vast information to the people is museums. A museum is an institution providing services—acquiring, conserving, researching, communicating, and exhibiting objects for education and enjoyment (ICOM, 2001). Some museums are made available online through virtual museums so that more people can have access to the information provided by the museum.In order to improve the services that virtual museums provide, they should be able to determine which information is most relevant to a particular user and object information should be presented in such a way that the user can understand (Bandelli, 1999). Similar to internet users, visitors of museums have different expectations, needs and behaviors, and it is important to address these differences. Ambeth Ocampo, a columnist at a notable Philippine newspaper, says that, “While I appreciate the educational task of museums, I would like to think that all our best efforts are still not enough to get the youth into museums and keep them returning… The problem lies not with a museum but a child’s first encounter with it” (Ocampo, 2007). Ocampo also says that for many college students who had to endure a grade school trip to the museum, going there a second or third time is considered a cruel and unusual punishment. This mind-set is not the fault of the museum; it is the fault of the teacher or museum guide who did not infect the students with a sense of discovery and appreciation of our 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 141–150 141  past. Thus, improving the encounter with a museum through a web-based interactive virtual museum that generates descriptions based on user profile is the motivation for this research. The next section introduces the virtual museum we have created. Section 3 expounds more on the components of the virtual museum. In Section 4, we discuss how we modeled the user preference and how these are updated. Section 5 presents the rules created to generate the different types of description. Samples of generated descriptions based on user profile is also discussed in this section. In Section 6, we discuss the testing done for the system. Lastly, we give our conclusion and indicate some future work in Section 7. 2. Our Virtual Museum We have chosen to create a virtual museum for three (3) well-known musuems in Vigan, Philippines, namely: Padre Jose Burgos Museum, Crisologo Museum and Syquia Museum. These musuems are quite small, but contain a substantial collection of objects that represent the culture of the Southern Ilocos Region. We call this interactive virtual museum VIGAN. As VIGAN controls the amount of information generated depending on age group, the user is first asked to register upon first use of the virtual museum. Refer to Section 3 for a more detailed discussion on the information requested in the registration page. On succeeding sessions, the user is tasked to log in his user name and to choose which of the three-mentioned museums he wants to visit first. Once the user has chosen a particular museum, he has to choose a room from a list that is provided to him. After selecting a room, a 2D panoramic image of the actual museum is displayed. Hotspots in green represent another room to enter, while those in red represent objects. Clicking on the green hotspot will introduce the user to the next room with another 2D panoramic view consisting of one or more objects. On the other hand, clicking on the red hotspots indicates that the user wants to view the description for that particular object. Aside from generating the description of that object, each click on objects updates the user model. Updates in the user model involve increasing the weight of preference for that particular type of object, as well as storing the information that was already produced. Storing the information would allow the program to determine which new information can be given to the user, should he request for it. Moreover, storing the actual statements used will provide consistency in the generated text should the user choose to revisit the same object. The preference weight and the items that were visited is also used in producing a list of related objects that the user might be interested in viewing. It should be noted that the 2D panoramic view is done via the Panorama Tool Viewer (PTViewer 2.8 of Senore, 2006). The pictures were personally taken by the researchers and the information stored in the database where from consultations with locals or the curators of the museums. The information stored in the database is called the Knowledge Base. The objects from the knowledge base are divided into four categories: customs, economic life, history and household items. Customs refer to rituals, practices, traditions and way of life of the Ilocanos, such as musical instruments used in traditional dances and way of life of the Syquia, Crisologo and Burgos families during the Spanish era. Economic life refers to how the Ilocanos made a living, such as cottage industries, farming, fishing, hunting. History refers to events and personalities that had happened in Ilocos that has historical importance. Household items refers to items that are seen at home, such as cooking and gardening equipments, paintings and antiques. The knowledge base is hand-constructed and contains keywords that will be used by the NLG to generate descriptions. The type and amount of information given to the user is analyzed from the information given by the curators and from the consultation with a psychologist. The generation of the descriptions in natural language is through SimpleNLG 3.5 (Reiter, 2007), but the various sentence structures for each type of description is defined by the researchers. The types of descriptions for the objects are in the form of messages. Aside from the name of the object, descriptions generated can be a combination of the following: 142  • basicMsg(<noun>basic_name,<noun>basic_what-is, <adjective>basic_description) The basicMsg includes the name of the object, what the object is and the description of the object. It is applicable to all object categories and subcategories. • purposeMsg(<verb>purpose_action,<noun>purpose_object) The purposeMsg includes the name of the object, what the object is and the description of the object. It is applicable to all object categories and subcategories. • make-upMsg(<noun>make-up,<adjective>make-up_description, <noun>make-up_location,<noun>make-up_alternative-make-up) The make-upMsg includes what the object is made up of, the description of the makeup of the object, where the makeup of the object is found, and alternative makeup of the object. It is applicable to the household item and economic item category. It is applicable to the some subcategories. • people-involvedMsg(<noun>people-involved_relation,<noun>peopleinvolved) The people-involvedMsg includes the people involved with the object and the relation of the people involved with the object. It is applicable to all categories and subcategories. For example, for the object bolo, the people-involvedMsg will be peopleinvolvedMsg(<noun>owner, <noun>Elpidio Quirino). • akaMsg(<noun>aka) The akaMsg includes what the object is also known as. It is applicable to all categories and subcategories. • used-inMsg(<noun>used-in) The used-inMsg includes where the object is used in. It is applicable to household item, customs and economic item categories. It is also applicable to some subcategories. • consist-of(<noun>consist-of) The consist-ofMsg includes what the object consists of, because some objects are made up of more than one parts. It is applicable to household item, customs and economic item categories. It is applicable to some subcategories: • personMsg(<noun>person_name,<noun>person_bdate,<noun>person_bplace ,<noun>person_ddate,<noun>person_gender) The personMsg includes the name of the person, his/her birth date birth place, death date and gender. It is applicable to the history category and the personality subcategory. • paintingMsg(<noun>painting_name,<noun>painting_theme,<adjective> painting_description, <noun>painting_age) The paintingMsg includes the name, the theme, description and age of the painting. It is applicable to the household item category and art subcategory. • parentsMsg(<noun>parents_fathersName,<noun>parents_mothersName) The parentsMsg includes the father’s name and the mother’s name. It is applicable to the history category and personality subcategory. • educationMsg(<noun>education_degree,<noun>education_year, <noun>education_school) The educationMsg includes the degree, year and school. It is applicable to the history category and personality subcategory. 143  • siblingMsg(<noun>sibling_siblingsName) The siblingMsg includes the sibling’s name. It is applicable to the history category and personality subcategory. • original-paintingMsg(<adjective>original-painting_description, <noun>original-painting_artist,<noun>original-painting_location) The original-paintingMsg includes the description, artist and location of the original painting. It is applicable to the household item category and art subcategory. • sizeMsg(<noun>size_unit-of-measure, <noun>size_height, <noun>size_ width) The sizeMsg includes the unit of measure, height and width of the object. It is applicable to all categories and all subcategories. • award(<noun>award_ prize,<noun>award_ sponsor,<noun>award_ year) The awardMsg includes the prize, sponsor and year of the award. It is applicable to the household item category and art and personality subcategory. 3. VIGAN’s Architectural Design The architecture of VIGAN can be seen in Figure 1. There are basically two (2) main processes in VIGAN: the Natural Language Generator (NLG) and the Virtual Environment (VE). These components access and update data from the databases User Model, Plan Library, Knowledge Base, Discourse History, Lexicon, and the text file Grammar. Figure 1: The Architecture of the VIGAN System The user chooses an object from the 2D panoramic view of the museum in the Virtual Environment. The chosen object’s identification number is then passed on to the NLG component where the object description will be generated. The NLG component starts with the Discourse Goal, in order to provide a description for the selected object. The Discourse Goal is represented as the ID number of the object selected. Based on the Discourse Goal, the Text Planning component will decide which information will be included and how it will be ordered in text that will be generated. The Text Planning 144  component will get the User Model (UM), which is used to tailor the description according to the user, Discourse History, a database that contains the facts about the objects that have been presented to the user, a message (1), which is a pre-configuration of domain elements grouped basicMsg(name, what-is, description) (1) in categories and arranged according to level of importance, from the Plan Library, and facts about the object stored in the database. The Text Planning component will decide based on the age group of the user (as obtained from the UM) how much information will be included. The UM is also used to make suggestion of objects more appropriate to the user. UM is a database which contains non-decision and decision properties. Non-decision properties about the user which includes username, password, name and gender are gathered from the user explicitly during registration. The user model is dynamic or always changing because new information about the user is gathered each time the user visits an object in the museum. The new information gathered as the session progresses are decision properties that will aid the system in generating descriptions for the objects in the museum. In VIGAN, decision properties are interest rating on each category of objects, importance rating on each available fact and age, because the amount of facts to be presented to the user for each description will depend on it. The Text Planning component will fill in messages with information stored in the Knowledge Base, such as that message in (1) will be (2). basicMsg(burnay, Ilocano earthen jar, null) (2) Once the messages have been filled, it will include it in the Discourse Plan, which is an ordered and structured set of messages that will be included in the generated text. The Discourse Plan is passed to the Surface Realization component, and the facts used in the Discourse Plan are stored in the user’s Discourse History so that the system will know which information has already been presented to the user to avoid redundancy for future presentation of object description. VIGAN uses SimpleNLG (Reiter, 2007), a Java class library developed by the University of Aberdeen, which performs NLG lexicalisation and realisation, as its Surface Realization component. The Surface Realization component makes use of the Grammar and Lexicon to produce Natural Language expressions based on the Discourse Plan. For the Grammar, VIGAN uses a set of grammar rules used to form English sentences (see Example below). Example of grammar rules for English language: <Simple Sentence> = <Declarative Sentence> <Declarative Sentence> = <subject> <predicate> <subject> = <simple subject> | <compound subject> <simple subject> = <noun phrase> | <nominative personal pronoun> <noun phrase> = "the" <specific proper noun> | <proper noun> | <non-personal pronoun> | <article> [<adverb>* <adjective>] <noun> | [<adverb>* <adjective>] <noun-plural> <noun> = <noun> [<prep phr>*] The system reads the Grammar file using the bottom up approach, where it will start with filling the grammar rules at the bottom with facts, before working its way up. Grammar rules that have not been filled up will be removed, while grammar rules that have been filled will be used to generate the description. A message can be applied to more than one grammar rule. A message can result in more than one type of sentences. The selection of sentences to use for the description is done randomly. 145  VIGAN uses WordNet 2.0 (Miller, 2005) as its Lexicon, along with Java WordNet Library (JWNL), a Java API that is used to access WordNet. It will be used to map the messages in the Discourse Plan into words and phrases. Words and phrases that are used frequently in the sentence generation are stored in a Word Bank along with its senses, will be replaced with different lexicalizations, including incorporating synonyms, based on the senses automatically. The building of the Word Bank is done manually before runtime. Natural Language (NL) Text will be the output of the Surface Realization component. The NL Text will be the description of the museum object selected by the user. The NL text will be passed to the WWW Viewer. The WWW Viewer is the website which also links the user to the VE. It is in the VE that user can explore the virtual museum, select objects from the rooms in the virtual museum, and view descriptions of an object. Every time the user selects a uniqueobject, the user model is updated by increasing the interest rating of the user based on which category the object belongs to, refer to Section 4 for a more detailed discussion. For example, the user selects burnay which belongs to Household Items category, therefore the system will increase the percentage of the user’s interest on household items. While exploring the virtual museum, WWW Viewer also suggest objects that might be of interest to the user by determining which category the user is most interested with and getting random objects with that category from the database. 4. User Modeling User’s preferences are modeled in the UM. Each user will have a rating of interest per category of object and rating of importance per facts. Interest rating is the rate of how interested the user is to each category of object (i.e., history, economic life, customs, household items). Interest rating of each category of object will initially be 0.0. During the sign-up phase, the users will be asked to select the categories of object that they are interested in. The rating of the categories is computed using (3). So when the user selects Household Items and Customs as his interests, then his initial interest rating will be 50.0/50.0/0.0/0.0 where the rating corresponds to Household Items, Customs, History, and Economics, respectively. 100/number_of_selected_category (3) UM will also be updated whenever the user clicks on a new object and whenever the user clicks on “more information”. The interest rating of each category will be updated using (4) 
1. Introduction Language allows us to express different perspectives towards things in the world. For example, a single object, like a wooden table, can be described both as a table (i.e., a kind of object), and as some wood (i.e., a kind of material). The ways in which these perspectives are expressed, however, differs from language to language, leading some to claim that speakers of different language may think differently about objects in the world (Lucy, 1992; Imai & Gentner, 1997; Quine 1960). This paper contributes to this debate by probing the representation and development of syntactic cues to individuation in Mandarin Chinese. In English, a distinction can be made between count nouns and mass nouns. Typically, words like dog, table and idea are used as count nouns, and refer to kinds of things that have “atomic structure”, with “atoms” or “individuals” that come in natural units for counting. When hearing one of these words (e.g., dogs), we know that it refers to a quantity of discrete, naturally bounded individuals, and not some arbitrary portions thereof (e.g., pieces of dog). In contrast, mass syntax does not specify individuation (see Bloom 1994; Gordon 1988; Link 1983). Mass nouns can refer to unindividuated stuff like water, wood, and fun, or to sets of individuals like * Thank you to Frankie Chen for his help with preparing the word list in Experiment 1. Thank you also to Becky Huang and Xiaowen Xu for their help with data collection in Taipei, Taiwan. We are grateful to the parents and children who participated, and to the daycares and preschools in Taipei for their support and help with recruitment. This work was supported by a Connaught Grant awarded to D.B. Copyright 2008 by Pierina Cheung, Peggy Li, and David Barner 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 151–160 151  footware, furniture, and ammunition. Ususally, words used in mass syntax do not refer to individuals. In English count nouns can occur directly with numerals (e.g., one dog), in singular or plural forms (a dog, some dogs), or with quasi-cardinal determiners (these dogs) that signal reference to sets of individuals. Mass nouns, in contrast, usually1 cannot be used directly with numerals, with singular or plural morphology, or with quasi-cardinal determiners. Also, mass and count constructions selectively specify different quantifiers (many/*much table vs. *many wood/much wood). Not all languages, however, have such transparent syntactic cues to individuation. In classifier languages like Chinese and Japanese, there is no mass-count distinction at the level of the noun, regardless of what the noun refers to. Instead, nouns in classifier languages syntactically resemble mass nouns in English. For example, in Mandarin Chinese, nouns cannot co-occur directly with numerals, but require a discretizing unit (i.e., a “classifier”) for counting, like English mass nouns (e.g. two pieces of toast). Classifiers encode information such as the shape, animacy, functionality, or the unit of measure of the referent noun. For example, to label three pens in Mandarin requires both the numeral san (three) and the classifier zhi (stick) as in san zhi bi (or “three stick pen”). Also, unlike English count nouns, which obligatorily specify number via singular-plural marking (e.g., a cat vs. some cats), classifier languages normally lack obligatory plural marking. As a result, bare nouns in classifier languages are unspecified for number. If a classifier language has a plural marker, its use is often optional, infrequent, and restricted (e.g., to animates). Finally, nouns in classifier languages, irrespective of whether they denote countable individuals or unindividuated stuff, typically permit the same quantifiers.2 If count syntax specifies individuation in English, are there equivalent syntactic structures to encode individuation in languages that lack a mass-count distinction? According to Cheng and Sybesma (1998, 1999) Mandarin Chinese may make an analogous distinction at the level of the classifier. They argue that Mandarin features two types of classifier, which they call “count classifiers” and “mass classifiers”. Count classifiers form a closed-class and mark reference to individuals, whereas mass classifiers form an open-class and function as measure words that are used to denote portions of unindividuated stuff (e.g., a cup of sugar) or portions of objects (e.g., a cup of marbles). To test the hypothesis that classifiers are semantically analogous to mass-count syntax, Li, Barner and Huang (in press) examined how Mandarin speakers interpret them in a word extension task. Typically, when speakers of English learn new count nouns (e.g., “Look, this is a wug”), they assume that these words refer to kinds of objects that share a common form. When they learn mass nouns (“Look, this is some wug”), in contrast, they are less likely to assume that the word denotes a solid thing. Based on this, Li et al asked if Mandarin speakers would extend count classifiers, like gen (rod) and pian (slice) to solid things with matching shape (e.g., rod shapes or slice shapes), and whether they would do so less for mass classifiers like dui (pile) and tuan (wad). When asked to find “one CL something” (CL = classifier) among several choices, adults selected solid, shape-matched objects when presented count classifiers, but rejected things that were non-solid or that didn’t match in shape. However, this distinction was less available to young children, who were willing at age four to accept portions of non-solid stuff when the experimenter requested something with a count classifier, so long as the substance matched the shape specified by the classifier (e.g., toothpaste that was shaped like a rod when the experimenter asked for gen). It was not until approximately six years of age that children 
a  Sung-Kwon Choi , Ki-Young Lee , Yoon-Hyung Roh ,  a  a  Oh-Woog Kwon , and Young-Gil Kim  a Natural Language Processing Team, Electronics and Telecommunications Research Institute, 161 Gajeong-dong, Youseong-gu, Daejon, Korea {choisk, leeky, yhroh, ohwoog, kimyk}@etri.re.kr 
1. Introduction Named Entity Recognition (NER) is an important tool in almost all Natural Language Processing (NLP) application areas including machine translation, question answering, information retrieval, information extraction, automatic summarization etc. The current trend in NER is to use the machine-learning (ML) approach, which is more attractive in that it is trainable and adoptable and the maintenance of a ML based system is much cheaper than that of a rule-based one. The representative ML approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), ME (New York University’s MENE in (Borthwick, 1999)), CRFs (Lafferty et al., 2001) and SVM (Yamada et al., 2002). The process of stacking and voting method for combining strong classifiers like boosting, SVM and TBL, on NER task can be found in (Wu et al., 2003). Florian et al. (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. The work reported in this paper differs from the existing works in the sense that here, we have conducted a number of experiments to improve the performance of the classifiers with the lexical context patterns, which are generated in a semi-automatic way from an unlabeled corpus of 3 million wordforms, and used several post-processing techniques to improve the performance of each classifier before applying weighted voting. Named Entity (NE) identification in Indian languages in general and in Bengali in particular is difficult and challenging as:  Unlike English and most of the European languages, Bengali lacks capitalization information, which plays a very important role in identifying NEs. * This work is partially supported by the CLIA Project, funded by DIT, MCIT, Govt. of India, Vide Sanction letter no-14 (5)/2006- HCC (TDIL)/29.08.2006. Copyright 2008 by Asif Ekbal and Sivaji Bandyopadhyay 1http://ltrc.iiit.ac.in/ner-ssea-08 169   Indian person names are more diverse and a lot of these words can be found in the dictionary with specific meanings.  Bengali is a highly inflectional language providing one of the richest and most challenging sets of linguistic and statistical features resulting in long and complex wordforms.  Bengali is a relatively free order language.  Bengali, like other Indian languages, is a resource poor language - annotated corpora, name dictionaries, good morphological analyzers, Part of Speech (POS) taggers etc. are not yet available in the required measure.  Although Indian languages have a very old and rich literary history, technological developments are of recent origin. 
Kathleen L. Go and Solomon L. See  a De La Salle University – Manila 2401 Taft Avenue, Malate 1004 Manila, Philippines go.kathleen@gmail.com, sees@dlsu.edu.ph  Abstract. n-gram language modeling is a popular technique used to improve performance of various NLP applications. However, it still faces the “curse of dimensionality” issue wherein word sequences on which the model will be tested are likely to be different from those seen during training (Bengio et al., 2003). An approach that incorporates WordNet to a trigram language modeler has been developed to address this issue. WordNet was used to generate proxy trigrams that may be used to reinforce the fluency of the given trigrams. Evaluation results reported a significant decrease in model perplexity showing that the new method, evaluated using the English language in the business news domain, is capable of addressing the issue. The modeler was also used as a tool to rank parallel translations produced by multiple Machine Translation systems. Results showed a 6-7% improvement over the base approach (Callison-Burch and Flournoy, 2001) in correctly ranking parallel translations. Keywords: language modeling, statistical methods, translation quality, WordNet 1. Introduction n-gram language modeling is a popular statistical language modeling (SLM) technique used to improve performance of various natural language processing (NLP) applications such as speech recognition (SR), machine translation (MT), and information retrieval (IR). However, it still faces the “curse of dimensionality” issue wherein word sequences (i.e. n-grams) on which the model will be tested are likely to be different from those seen during training (Bengio et al., 2003). This means that these sequences would always be assigned low probabilities. There have already been attempts to address this problem. A number of works made use of smoothing techniques such as the one presented in (Callison-Burch and Flournoy, 2001) which made use of linear interpolation of trigram, bigram and unigram probabilities. (Bengio et al., 2003) presents an approach that combines neural networks and smoothing techniques to a trigram model while the work in (Brockett et al., 2001) made use of additional linguistic features, such as syntax trees, in order to construct a decision tree that will classify sentences. Another possible solution is to make use of other linguistic resources such as WordNet (Fellbaum et al., 2006) and devise an approach that combines WordNet features (i.e. synsets and their relations) with the n-grams used in language modelers. By incorporating WordNet features, the language modeler can generate related sequences which can possibly give it a higher score even if there are no exact matches seen during training. (Hoberman and Rosenfeld, 2002) presents a study on the integration of WordNet features to address the data sparseness of nouns in bigrams. The study covered the IS-A relationship of nouns and reported * Copyright 2008 by Kathleen L. Go and Solomon L. See 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 179–188 179  improvement in the language model perplexity, although the improvement was below expectation. This paper presents an extension of the study in (Hoberman and Rosenfeld, 2002). A trigram language modeler has been developed that considers other parts of speech (i.e adjective, adverb, verb) and relationships (i.e. HAS-A, Synonymy/Antonymy), in addition to nouns and the IS-A relationship, to address the “curse of dimensionality” issue. Since there are many existing MT systems with different ways of producing translations, the language modeler was used as a tool to automatically rank parallel translations (i.e. translations produced by multiple MT systems). The ranking of translations was based on the method found in (Callison-Burch and Flournoy, 2001) where sentences are ranked based on their fluency, which are computed using the probabilities of their trigram components. Trigrams were used for the n-gram language modeler since the study in (Callison-Burch and Flournoy, 2001) showed that trigrams are effective when used in ranking parallel translations. Section 2 discusses the architecture of the language modeler while Section 3 presents an example on how the new approach works. Section 4 presents the evaluation results both in terms of addressing the “curse of dimensionality” issue and when applied to the ranking of parallel translations. Lastly, Section 5 contains the conclusions and the discussion of possible future works for this research. 2. Architecture The language modeler has two modules: the Training Module and the Language Evaluation Module. The latter, in turn, consists of the Base Language Evaluation Submodule and the WordNet-Integrated Language Evaluation Submodule. 2.1.Training Module The Training Module, shown in Figure 1, is based on the language modeler presented in (Callison-Burch and Flournoy, 2001). This module handles the extraction of trigrams, bigrams, and unigrams from an input document. Since WordNet only contains lemma and general representations of parts of speech (e.g. “noun” instead of “NNP” for proper nouns), the general representations are needed for the evaluation of trigrams.  Corpus  Training Module  Sentence Segmentation, Tokenization, and Tagging  General POS and Lemma Extraction  n-gram Extraction  WordNet (Stemmer)  n-gram DB  Figure 1: Training Module Architecture. 2.2.Language Evaluation Module The Language Evaluation Module, shown in Figure 2, consists of the Base Language Evaluation Submodule and the WordNet-Integrated Language Evaluation Submodule.  180  Corpus Sentence Segmentation, Tokenization, and Tagging  Language Evaluation Module Base Language Evaluation Submodule  WordNet-Integrated Language Evaluation Submodule  Rank Sentences  Sentence Score Computation  Figure 2: Language Evaluation Module Architecture. 2.2.1. Base Language Evaluation Submodule The Base Language Evaluation Submodule, shown in Figure 3, is a modified version of the evaluation process in (Callison-Burch and Flournoy, 2001).  Tokens with POS Tags  Base Language Evaluation Submodule  General POS and Lemma Extraction  Trigram Extraction  n-gram Look-Up and Trigram Score Computation  WordNet (Stemmer)  n-gram DB  Figure 3: Base Language Evaluation Submodule Architecture.  After the Sentence Segmentation, Tokenization, and Tagging process, the input is passed to the General POS and Lemma Extraction process for the extraction of the generalized representation of its tokens and POS tags. Generalized parts of speech are extracted from the POS tags generated by MontyTagger (Hugo, 2004) while the lemma representations of the tokens are extracted by using WordNet’s stemmer. The Trigram Extraction process then extracts the trigrams that will be evaluated, including their POS sequence and general representation. These trigrams are passed to the n-gram Look-Up and Trigram Score Computation process that handles the matching of trigrams and their components in the n-gram DB to get their frequencies and compute their probabilities. This process performs a Lemma and General POS Look-Up whereas the original version performs a Surface Level without POS Look-Up. In the former, the n-gram Look-Up process matches base on the general representation of the n-grams while in the latter, the process matches base only on the surface level (i.e. actual word sequences) of the n-grams. The formula used to compute trigram probabilities, which was adapted from (Callison-Burch and Flournoy, 2001), is shown below.  P(z|xy) = (0.80 * frequency of xyz) / (frequency of xy) +  (1)  (0.14 * frequency of yz) / (frequency of y) +  (0.099 * frequency of z) / (total # of words seen) +  0.001  181  2.2.2. WordNet-Integrated Language Evaluation Submodule The WordNet-Integrated Language Evaluation Submodule, shown in Figure 4, contains the additional processes that correspond to the new approach. In summary, the process starts with using the contents of WordNet to generate proxy trigrams for the trigrams that were not seen during training. Depending on the presence of the proxy trigrams or their components in the ngram DB, they may be used to reinforce the fluency of the given trigrams. This is done by using the scores of the proxy trigrams to reinforce the score of the given trigram. Originally, only one proxy trigram was used to reinforce trigram scores. However, initial results showed that using only one proxy trigram, either the highest scoring or the most similar, does not give significant increases in trigram scores. Since proxy trigrams are also faced with the “curse of dimensionality” issue, multiple proxy trigrams were used. If a proxy trigram has matches that contain at least one proxy word, it can be used to reinforce the fluency of the given trigram or its lower order components (i.e. bigram and unigram components). This approach also addresses the data sparseness issue of the given trigram’s components wherein even if they have matches in the n-gram DB, they are still assigned low scores.  Trigrams  WordNet-Integrated Language Evaluation Submodule  Proxy Trigram Generation  n-gram Look-Up and Trigram Score Computation  Proxy Trigram Elimination  WordNet  n-gram DB  Trigram Score Reinforcement  Rank Proxy Trigrams  Figure 4: WordNet-Integrated Language Evaluation Submodule Architecture. In detail, an unseen trigram first goes through the Proxy Trigram Generation process. This process generates proxy trigrams by first looking for partially matching trigrams in the n-gram DB (e.g. only the first two words are matched). Words having no position matches (i.e. words that caused the trigrams to not exactly match) are replaced with proxy words (i.e. related words from WordNet). Proxy words are related to the original word through the IS-A and Synonymy/Antonymy relationship. Originally, words related through the HAS-A relationship were also retrieved for nouns. However, initial results showed that the increase in reinforced scores caused by proxy trigrams containing HAS-A related proxy words are insignificant. The resulting proxy trigrams are then passed to the n-gram Look-Up and Trigram Score Computation process, which is the same with the one in the Base Language Evaluation Submodule. In this process, the probabilities of the proxy trigrams are computed. The Proxy Trigram Elimination process would then remove the proxy trigrams that would not aid in the reinforcement of the given trigram’s fluency. These are the proxy trigrams with no matches (i.e. exact, bigram or unigram) in the n-gram DB or proxy trigrams with matches but do not contain at least one proxy word. In the initial study, proxy trigrams undergo the Similarity Score Computation process before n-gram Look-Up and Trigram Score Computation. In the said process, the similarity between a word and its proxy word is computed using WordNet::Similarity (Banarjee et al., 2006). These scores were used in Proxy Trigram Elimination to further filter the proxy trigrams. This is done by using pre-computed similarity score thresholds for each POS such that if the similarity scores of the proxy words in a proxy trigram do not meet the thresholds, the proxy trigram is assumed to be non-fluent and is  182  removed. The thresholds were derived from the range of the similarity scores of the proxy words belonging to proxy trigrams that produced fluent proxy sentences (i.e. sentences with proxy trigram replacements). The initial results showed that the thresholds were not effective in its purpose such that only an insignificant amount of proxy trigrams were being filtered out. Therefore, the Similarity Score Computation process and the use of similarity score thresholds are removed. After the Proxy Trigram Elimination process, the remaining proxy trigrams are passed to the Rank Proxy Trigrams process to be ranked based on the probabilities assigned to them from highest to lowest. The Trigram Score Reinforcement process then makes use of the remaining proxy trigrams to reinforce the fluency of the given trigram. This is done by integrating the original score of the given with the scores of the remaining proxy trigrams. The formula used to compute reinforced trigram scores is shown below.  l  m  n  ∑ ∑ ∑ P(xyz)reinforced = (1 − λ)P(xyz) + λ( P(xyzproxyTriMatch)+ P(xyzproxyBiMatch) + P(xyzproxyUniMatch)) (2)  i =1  i =1  i =1  where: λ = 0.9 ,  l  m  n  ∑ ∑ ∑ P(xyzproxyTriMatch) <= 0.9 , P(xyzproxyBiMatch) <= 0.01 , P(xyzproxyUniMatch) <= 0.006  i =1  i =1  i =1  The purpose of λ is to prevent non-fluent trigrams from being reinforced too much in case it has a proxy trigram that is fluent. If a trigram has proxy trigrams that passed the Proxy Trigram Elimination process, the trigram would most likely have a higher reinforced score. This cannot be prevented even for non-fluent trigrams. Therefore, part of the reinforced trigram score must still be influenced by the original score. The value of λ was derived by getting the value with the highest performance in the ranking of parallel translations. As seen in the formula, the total proxy score for each trigram is derived by getting the sum of: 1) total score of the proxy trigrams with exact matches, 2) total score of the proxy trigrams with bigram matches, and 3) total score of the proxy trigrams with unigram matches. The total scores for each match level are separate since they are assigned different score limits in order to prevent lower level matches from causing a high increase in the reinforced score. If there is only one threshold that covers all the match levels, the reinforced score of a trigram having many proxy trigrams with lower level matches may equal or exceed the score of a trigram that has a proxy trigram with an exact match. This is possible because of the data sparseness issue wherein even if a trigram or proxy trigram has an exact match, the scores assigned to them may still be low. Also, there is a greater chance of a trigram having proxy trigrams with lower level matches than having proxy trigrams with exact matches. Therefore, in order to prevent the scores of the lower level matches to consume the portion of the score that is supposed to be for higher level matches, different score limits are assigned to each match level. These values were derived through multiple tests conducted wherein different values were used to compute reinforced scores of corresponding fluent and non-fluent sets. The results were compared and the set of threshold values that gave a high perplexity reduction while at the same time maintained the perplexity distance was chosen. 2.2.3. Sentence Score Computation After the scores and reinforced scores of the trigrams are computed, the Sentence Score Computation process computes the fluency scores of the sentences. The fluency score of a  183  sentence is determined by getting the product of the probabilities assigned to all its trigram components. If a trigram has a reinforced score, it is used instead of the original score. 2.2.4. Rank Sentences Each of the sentence sets (i.e. parallel translation sets) are then passed to the Rank Sentences process to be ranked. The ranking of sentences is based on fluency score from highest to lowest. 3. Example This section presents an example of the new approach. In this example, the given trigram “to local capitalists” – “TO JJ NNS” with a general representation of “to local capitalist” – “TO adjective noun”, only has a unigram match in the n-gram DB with a score of 0.008. This section shows how the reinforced trigram score is computed. In the Proxy Trigram Generation process, the first step is to look for trigrams with partial matches in the n-gram DB. In this case, assume that the n-gram DB contains two trigrams having partial matches with the given: 1) “a foreign capitalist” – “DT JJ NN” (“a foreign capitalist” – “DT adjective noun”) and 2) “to local investors” – “TO JJ NNS” (“to local investor” – “TO adjective noun”). But since the second trigram has more position matches compared to the first (i.e. 2 position matches vs. 1 position match), only the second is used as a pattern (i.e. “to local {x}”). There can be more than one pattern used as long as they have the same number of matching words (e.g. “to {x} investors”). WordNet is then accessed to retrieve words related to the words with no position matches. Table 1 shows a list of some of the proxy words for the word “capitalists” including their relationships. Table 1: List of Sample Proxy Words  Proxy Word conservative conservativist person individual someone  Relationship hypernymy hypernymy hypernymy hypernymy hypernymy  Proxy Word holder materialist businessperson investor financier  Relationship hyponymy hyponymy hyponymy hyponymy hyponymy  The proxy words are used to generate proxy trigrams by using the patterns (i.e. “to local {x}”) and replacing their counterparts. Table 2 shows the generated proxy trigrams for the example. Table 2: Results of the Proxy Trigram Generation Process  ID  Proxy Trigram  Proxy Trigram  POS  
It is well-known that many oceanic languages have no adjective class which is distinguishable from noun and verb classes (Wetzer, 1996). The only criteria which distinguish adjectives from nouns and verbs could be comparative constructions (Dixon, 2004). However, comparative construction is not universal. Some languages in Papua New Guinea do not have any comparative morpheme or any comparative-specific structure. In this paper I focus on the comparative forms of various languages in the Pacific Asia region and suggest a new typology of comparatives in the framework of formal semantics. The structure of the paper is as follows: in section 2, I summarize the previous studies on semantics of comparatives and define the denotation of adjectives; in section 3, I show the data from various languages in the Pacific Asia region and classify them into four groups depending on the morphological/syntactic markedness; in section 4, I show how to analyze each type of comparative construction in the framework presented in section 2; in section 5, I show that two parameters are relevant to the classification of adjectival systems. 2. Semantics of Comparatives The denotation of an adjective is property, i.e. an expression of type <e,t>. In comparative construction, however, adjectives need to be abstracted over degrees, because what is compared is the degree of property of more than one object. I call this process degree abstraction and define the degree abstraction operator as follows: 1) Degree Abstraction operator Π is a function from expressions of type <e,t> to <e <d,t>> Π<<e,t>, <e,<d,t>>> = λA<e,t>λx<e>λd<d> [RA(d)(x)] where the domain of type <d> is a set of degrees and RA is a function which relate objects to degrees on the scale of A. 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 197–206 197  Ex. λx<e> [tall(x)] → λx<e>λd<d> [Rtall(d)(x)] After an adjective undergoes the process of degree abstraction, we obtain a function from individuals to a set of degrees. An adjective tall now means a set of degrees which the individual x has on the scale of tallness. However, this procedure is not enough for an adjective to be comparative. When we compare two sets of degrees, we need to pick up the maximal degree in each degree set. This process is conducted by the maximization operator ‘MAX’ (Rullmann, 1995). 2) MAX (λd<d> [RA(d)(x)]) = ιd [RA(d)(x)] Degree abstraction and maximization work together with comparative morphemes to construct comparatives. For example, English comparative morpheme –er is defined as (3) (Kennedy, 1999)1. Each argument of –er has to be abstracted over degrees and maximized. 3) Definition of comparative morpheme ||MORE(MAX(D2))(MAX(D1))|| = 1 iff MAX(D1) > MAX(D2) Ex. ||Bill is taller than John|| = MORE (MAX(λd<d> [Rtall(d)(John)]))(MAX(λd<d> [Rtall(d)(Bill)])) = ιd<d> [Rtall(d)(Bill)] > ιd<d> [Rtall(d)(John)]  3. Four Types of Comparative Forms Investigation on a wide variety of languages reveals that there are at least four types of comparative constructions: Enlish-type, Chinese-type, Japanese-type and Dom-type. The former three languages have adpositions to introduce standards of comparison, while Dom-type languages has none. In other words, Dom-type languages cannot describe comparison in one sentence, as there is no way to introduce a standard of comparison into adjectival construction. The sense of comparison is described by coordination of positive and negative adjectives. Other three languages are different in morphological markedness of adjectives. English-type languages mark comparative form and Chinese-type languages absolute form, while Japanesetype languages do not have any morpheme or word to distinguish aboslute and comparative.  Table 1: Typology of Comparative Forms  The standard of comparison morphological  is introduced by...  markedness  English-type  adpositional phrase  comparative  Chinese-type  adpositional phrase  absolute  Japanese-type  adpositional phrase  none  Dom-type  coordination  none  A large number of languages belong to English-type or Japanese-type, while only a few appear  as examples of Chinese and Dom-type languages.  4) English-type: absolute form as basic English, Karo Batak, Madurese, Sundanese, Semelai, Chamorro, ... 5) Chinese-type: comparative form as basic 
 1. Introduction Conceptual Metaphor Theory (Lakoff and Johnson, 1980) considers metaphor as a mapping from the concrete source domain to the abstract target domain. Abstractions and enormously complex situations are routinely understood via metaphors. Metaphorical expressions are pervasive in human languages and must be treated for Natural Language Understanding (NLU) (Carbonell, 1982). As an important figure of speech, metaphor processing has interesting applications in many Natural Language Processing (NLP) tasks like machine translation, paraphrasing, information retrieval and question answering. Metaphor processing can be divided into three tasks, recognition, comprehension and generation, among which recognition is the basic step. Metaphor recognition is to decide whether a sentence contains metaphorical expressions (a word, phrase or the whole sentence). This paper focuses on verb metaphor, to decide whether a verb is in metaphorical usage or literal usage. In selectional preference violation view, a satisfied preference indicates a literal semantic relation, while a violated preference indicates a metaphorical one. Take the following two sentences as examples.  (1) 农民 在 精心 培植 幼苗。  Nong2min2 zai4 jing1xin1 pei2zhi2 you4miao3  Farmer  at carefully cultivate young plants  “Farmers are cultivating young plants carefully.”  * This research is funded by National Basic Research Program of China (No.2004CB318102). The authors are grateful to the two anonymous reviewers for their helpful comments and suggestions. Copyright 2008 by Yuxiang Jia and Shiwen Yu 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 207–214 207  (2) 我们 要 大力 培植 人才。  Wo3men2 yao4 da4li4 pei2zhi2 ren2cai2  We  should devote great effort train talents  “We should devote great effort to train talents.”  Sentence 1 is a literal usage while sentence 2 is a metaphorical one. The fact that literally 培植 ‘cultivate’ requires the object to denote some plants suggests that selectional preferences offer a cue to the presence of a metaphor. But the selectional preferences automatically induced by conventional computational models may not reflect semantics in the literal usage. On the other hand, concept concreteness or abstractness is an important indicator of literal usage, where concrete concepts usually indicate literal usage while abstract concepts correspond to non-literal usage. This paper makes use of concept concreteness based on automatically acquired selectional preferences for verb metaphor recognition. Though metaphorical usage could be considered as a different sense of the target word, but when performing inference, it is beneficial to differentiate literal usage from metaphorical usage, because they share inferential structure. For example, the aspectual structure of 培植 ‘cultivate’ is the same in either domain whether it is literal or metaphorical. Further, this sharing of inferential structure between the source and target domains simplifies the representational mechanisms used for inference making it easier to build the world models necessary for knowledge-intensive tasks like question answering. The rest of this paper is organized as follows. Section 2 is a review of related work. Section 3 describes the details of selectional preferences acquisition. Section 4 shows the method of source domain determination based on selectional preferences. Section 5 uses this source domain knowledge for metaphor recognition. Experiments and conclusions are given in section 6 and 7 respectively. 2. Related Work Previous work on automatic metaphor recognition using selectional preferences idea includes (Martin, 1990), (Fass, 1991), (Mason, 2004) and (Krishnakumaran and Zhu, 2007). (Martin, 1990) detects metaphors by comparing new sentences with an empirically collected metaphor knowledge base and gives some interpretation of metaphorical sentences. (Fass, 1991) uses collative semantics to identify metaphors and distinguish metaphor from metonymy. But they both require hand-coded knowledge bases and thus have limited coverage. (Mason, 2004) develops a corpus-based system CorMet for discovering metaphorical mappings between concepts. It finds selectional preferences of given verbs from automatically compiled domain-specific corpora, and then identifies metaphorical mappings between concepts in two domains based on differences in selectional preferences. (Krishnakumaran and Zhu, 2007) uses lexical resources like WordNet and bigram counts generated from a large scale corpus to classify sentences into metaphorical or normal usages. It does not compute selectional preferences explicitly and the bigram counts omit grammatical relations. Later researches treat metaphor recognition task as a classification problem between normal and metaphorical usage. (Gedigian et al., 2006) uses a maximum entropy classifier to identify metaphors and takes verb arguments as features. (Wang et al., 2006) also uses a maximum entropy approach to recognize Chinese noun phrase metaphors. However, both need manually annotated corpus to train the classifier. In order to reduce manual work on annotation, (Birke and Sarkar, 2006) use a clustering approach with a smaller seed corpus to classify verb usages. One advantage of selectional preferences based method is that it does not need training. One thing that sets our work apart is that all previous selectional preferences based methods do not make use of concept concreteness information. In contrast, we use it and show that it is effective information for metaphor processing.  208  3. Selectional Preference Acquisition The automatic corpus-based induction of selectional preferences was first proposed by (Resnik, 1993). All later approaches have followed the same two-step procedure, first collecting argument head words from a corpus, then generalizing to other similar words. They are different mainly in the generalization step, some using manual semantic taxonomy like WordNet, while others using clustering methods. Different from previous approaches, the first step in this approach is based on grammatical collocations. It makes use of various statistical measures for computing collocations or combination of some of them, not just word frequency used in previous approaches. For generalization, a semantic lexicon containing synonym and hypernym relations is employed. 3.1. Grammatical Collocation Grammatical collocation means that the target word and its collocation are in a certain grammatical relation, such as subject-verb, verb-object or modifier-noun. In order to obtain grammatical collocations for the target word, this paper uses Sketch Engine (Kigarriff and Tugwell, 2001), a query system extracting collocations of different grammatical relations from a large scale corpus. Collocations are sorted in descending order according to the salience value, which is estimated as the product of Mutual Information and log frequency. However, (Kilgarriff and Tugwell, 2001) modify the Mutual Information value by considering of the overall frequency of the grammatical relation as compared to other relations. The purpose of doing so is to avoid cases of low frequency collocations such as those which occur once but have high mutual information values because it is the only time they appear together with the target word. Therefore, the salience value is a reliable calculator instead of the frequency value. The corpus for grammatical collocation extraction is the Simple Chinese Gigaword corpus, which has 706,427,624 tokens. The input parameters for Sketch Engine are as follows: the minimum frequency is 5; the minimum salience value is 0.0; the maximum number of items in a grammatical relation is 999, which is the upper bound due to licensing limitation. As an example, table1 shows the top 20 collocations of the target verb 培 植 pei2zhi2 ‘cultivate’ in the verb-object relation.  Table 1: Top 20 collocations, object of 培植 ‘cultivate’  Collocation Frequency Salience Collocation 
1. Introduction In this paper, we propose a new approach to Categorial Grammars by introducing Curry’s Combinatory Logic (Curry & Feys 1958) in order to improve the parsing of Korean texts from a computational point of view. Since the introduction of simple Categorial Grammars, different propositions were made to improve this formalism by adopting applicative languages such as the calculus of syntactic types proposed by J. Lambek (1961), the lambda-calculus proposed by A. Church, the combinatory logic created by the mathematician H.-B. Curry (1958), some attempts by the logician W. V. O. Quine, etc. These works are based on the mechanism of the application of an operator to an operand. Combinatory logic and lambda-calculus were applied to the analysis of grammatical and lexical meaning in natural languages by S. K. Shaumyan (1987) with his model of the Universal Applicational Grammar using Curry’s combinatory logic, which extends the simple Categorial Grammars: this model is easily implementable on computational tools using functional programming languages such as CAML, HASKELL and SCHEME. In the 80’s, important extensions were given by R. Montague, M. Moortgat (1988), J. Lambek and M. Steedman (1989). Combinatory Categorial Grammar (CCG) developed by Steedman (1989, 2001) was most often quoted and studied for the analysis of Korean sentences. There exist several studies on Korean parsing based on the Categorial Grammar formalism. For example, the Korean Combinatory Categorial Grammar (KCCG) was developed by (Cha 2001 and Cha & Lee 2002) by extending the CCG of Steedman for the Korean parsing. The KCCG, having a purely computational approach, shows the ability to handle important linguistic phenomena of the Korean such as coordination, long distance scrambling, free word Copyright 2008 by Juyeon Kang, Jean-Pierre Desclés 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 215–224 215  order, etc. Cho and Park (2000) tried also to improve the complexity in the coordination, and Lee and Park (2003) proposed a morphological analysis of the irregular conjugation of Korean in order to conceive a morphological parser. The studies presented above and most of the related works are based exclusively on the CCG formalism of Steedman and developed in the purpose of a computational realization. Thus they often ignore the linguistic aspect of language and cannot capture some fine points such as morphological cases in Korean. Compared to these works based on the CCG formalism, the ACCG formalism that we develop in this paper, is not only a computational but also a linguistic approach, namely it better reflects the linguistic aspect in the Korean natural language processing. Consequently, this advantage allows us to parse the Korean language in a more explicit way and to show clearly the morphosyntactic structure of the Korean through our calculations. Thus, the ACCG formalism is a new approach which is both linguistic and computational. This formalism allows us to scope the difficult characteristics of the Korean that we can often find during automatic processing. In particular, we are interested in the problem of cases in Korean including the phenomenon of double case. Despite of their importance in parsing texts, cases have not been well studied from a computational point of view. Once we analyze the cases in the ACCG formalism, we will use some of the results of these analyses to handle the problem of free word order structure and coordination structure. This formalism leads us to easily analyze the free word order structure by a simple application of the combinatory rules we developed. This approach allows us to handle even long distance scrambling in the coordination structure, which is one of the most difficult problems in Korean parsing and has not been completely analyzed in other works (e.g. Cha 2001). 2. Applicative Combinatory Categorial Grammar The Applicative Combinatory Categorial Grammar formalism is an extension of the Combinatory Categorial Grammar developed by Steedman. This ACCG formalism was originally developed by J-P. Desclés and I. Biskri (1995, 1996) for the analysis of coordination and subordination structure in French with the tools of Combinatory Logic by introducing canonical associations between some rules and the combinators. The purpose of this work is the automatic analysis of Korean sentences in which there exist the problems of case, free word order structure and coordination structure. Firstly, the ACCG provides the possibility to go beyond the well-known limits (such as the processing of a coordination, etc.) of simple Categorial Grammars. Secondly, this formalism allows the construction of logico-grammtical representations that provide a way to building semanticocognitive representations in the general model of Applicative and Cognitive Grammar developed by J-P. Desclés (1990, 2003) with the three following levels: 1) morpho-syntactic configurations, 2) logico-grammatical representations, 3) semantico-cognitive representations. The ACCG builds applicative representations on the second level from the concatenated expressions given on the first level. We present here the rules3 of the ACCG, for the analysis of Korean sentences. Table 1: ACCG’s rules. Application rules 3 B is a composition combinator. Its β-reduction is: Bfgx→f(gx). It is joined to the functional composition rule. This combinator allows us in particular to handle the free word order structure in the Korean sentence. C* is a type raising combinator joined to the type raising rules. Its β-reduction is: C*fg→gf. This combinator transforms the operand (argument) to operator (function). It is used essentially to analyze nouns of the Korean as the operators. 216  [X/Y : u1]-[Y : u2] -----------------------> [X : (u1 u2)] Type raising rules [X : u] ----------------------->T [Y/(Y\X) : (C* u)]  [Y : u1]-[X\Y : u2] -----------------------< [X : (u2 u1)] [X : u] -----------------------<T [Y\(Y/X) : (C* u)]  Functional composition rules  [X/Y : u1]-[Y/Z : u2]  [Y\Z : u1]-[X\Y : u2]  -------------------------->B  --------------------------<B  [X/Z : (B u1 u2)]  [X\Z : (B u2 u1 )]  Consider the following analysis of a Korean sentence in the ACCG.  Sumi-ga  Minju-lil man-ass-da. (Sumi met Minju.)  Sumi-NOM Minju-ACC meet-PS-DC.  1.[N*4: Sumi-ga] -5 [(N*: Minju-lil] - [(S\N*)\N*: man-ass-da ] 2.[S/(S\N*):(C*Sumi-ga)]-[(N*:Minju-lil]-[(S\N*)\N*:man-ass-da ] 3.[S/(S\N*):(C*Sumi-ga)]-[(S\N*)/((S\N*)\N*):(C*Minju-lil)]-[(S\N*)\N*:man-ass-da ] 4.[S/((S\N*)\N*): (B(C*Sumi-ga)(C*Minju-lil))]-[(S\N*)\N*:man-ass-da] 5. [S: ((B(C*Sumi-ga)(C*Minju-lil))(man-ass-da))] 6. [S: ((C*Sumi-ga)((C*Minju-lil)(man-ass-da)))] 7. [S: ((C*Minju-lil)(man-ass-da))Sumi-ga] 8. [S: (((man-ass-da)Minju-lil)Sumi-ga)]  (>T) (>T) (>B) (>) (B) (C*) (C*)  We start from the concatenated sentence with assigned syntactic types. Then, we apply consecutively the type raising rules to “Sumi-ga” and “Minju-lil” which are operands, by introducing the combinator C*. This operation allows us to transform an operand into an operator. Then, we apply the functional composition rule to form a new operator “(B(C*Sumiga)(C*Minju-lil))” that will be applied to the operand “man-ass-da” at step 5. We reduce (in the Combinatory Logic formalism) consecutively the combinators B and C* to build a well-formed applicative expression at step 8. This expression gives a formal interpretation in terms of predicates, arguments and cases.  3. ACCG and the Korean Parsing 
1. Introduction Spoken words can be characterized in terms of suprasegmental or prosodic features, markedly stress, determined by acoustic frequency, intensity, and/or duration. Such a stressed-syllable is relatively louder and longer than other syllables in the same word or phrase (Ladefoged, 2001). In some languages such as Korean, French, and Czech, stress pattern of words is somehow predictable and syllable-based. For example, in Korean, stress mostly falls on the first syllable, otherwise on the second syllable, displaying no significant linguistic difference (Lee, 1990; Park, 2004). In French, stress dominantly falls on the final syllable with a full vowel, with no distinctive minimal pairs of words differed by its stress pattern and in Czech, stress almost always falls on the first syllable of a word (Jannedy, Poletto and Weldon, 1994). These syllablebased languages do not show meaning and grammatical differences influenced by their stress patterns. In other languages such as English, although the placement of stress is less predictable, stress can involve in lexical contrasts, causing a difference in meaning (i.e., TRUsty-truSTEE). In addition, it can change grammatical functions of words. For example, the word progress functions as a noun when the stress is placed on the first syllable, whereas as a verb when the stress is placed on the second syllable. The other stress-based languages in which speech sounds is controlled by stress are Spanish and Dutch (Goetry, Wade-Woolley, Kolnsky and Mousty, 2006). These languages do also illustrate lexical contrasts defined by their stress patterns. Current understanding of the stress pattern across languages suggests that the linguistic role of stress pattern widely differs with respect to lexical and grammatical functions and the rule of stress assignment is language specific. Recent studies on first-language (L1) stress sensitivity have demonstrated that L1 speakers of stress-based languages (i.e., English, Spanish, and * First author: Yongsoon Kang, Correspondence Author: Seunghyun Baek. This research was supported by Sungkyunkwan University Brain Korea 21 Project in 2008. 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 225–234 225  Dutch) are more sensitive to stress patterns than those of syllable-based languages (i.e, Korean and French). For example, given the discrepant lexical function of stress between Spanish and French, adult L1 speakers of French produced more errors in judging whether a string of pseudo-words differing in stress pattern display the same stress pattern or not than adult L1 speakers of Spanish (Dupoux, Pallier, Sebastian-Galles and Mehler, 1997). In another follow-up study of phoneme-and stress-contrast sensitivity, Dupoux and colleagues (Dupoux, Peperkamp and Sebastian-Galles, 2001) found that adult L1 speakers of Spanish and French performed similarly on the phoneme contrast judgment (i.e., /kypi/-/kyti/). However, the Spanish-native monolinguals significantly outperformed the French-native monolinguals on the stress contrast judgment (i.e, /’kipi/-/ki’pi/). In the meantime, the sensitivity to stress patterns has not received much attention in L2 reading research until recently because suprasegmental information is not manifested in most written systems. In a recent pioneering study of sensitivity of prosodic features among monolingual and bilingual first graders, Goetry and associates (Goetry, Wade-Woolley, Kolinsky and Mousty, 2006) compared the stress and phonemic awareness of French-native, Dutch-native monolingual, French-native children taught in Dutch, and Dutch-native children taught in French. The results showed that the four groups of first-graders performed similarly on phonemic awareness judgment (i.e., /’tepy/-/’tapy/) but differed on stress sensitivity judgment (i.e., /’tipy/-/ti’py/). The Dutch monolinguals notably outperformed the French monolinguals. The other bilingual first-graders had the intermediary performances. More importantly, Goetry et al. (2006) also suggest that early literacy development in a second stress-based language can be influenced by stress sensitivity by observing the correlation between stress awareness and word reading in the French monolinguals schooled in Dutch, not in the Dutch monolinguals schooled in French. Based on the findings of existing research on cross-linguistic comparisons of L1 and L2 stress awareness, stress processing ability may play a crucial role in learning to read in a second stress-based language such as English, Dutch, or Spanish, especially when students’ L1 is not stress-based. Moreover, spoken word processing is largely related to written word identification (Morais, 2003). At this point, it is significant to explore the stress assignment sensitivity of word reading in Korean, a syllable-based language, compared with that in English, a stress-based language as there has been little exploitation of learning to concurrently speak and read in an L1 with transparent stress rule and an L2 with opaque stress rule. In addition, previous studies have mainly focused on the perceptive sensitivity of stress contrasts. Consequently, more attention needs to be raised in interpreting the productive sensitivity of stress assignment. As discussed earlier, the stress pattern of Korean is somewhat predictable and regular without distinguishing meanings and grammatical functions of words. On the other hand, because the stress rule in English varies depending on word classes (Roca and Johnson, 1999), to some degree, it is less predictable and irregular with lexical and grammatical contrasts. Indeed, if Korean-speaking English language learners (ELLs) face exclusively distinctive stress patterns of English, which is not relevant to their L1, they may confront drastic restructuring of their interlanguage stress assignments and deal with unstable prosodic representations across the two languages. As a result, they would be at risk for difficulties in learning to speak and read in English as a stress-sensitive L2 and displaying prosodic information of English words. Given the discrepant stress assignment between Korean and English, the primary goal of this current study is to investigate the stress processing sensitivity of Korean-speaking ELLs in terms of number of syllables in a word and syllable structure in the word. The three research questions addressed in this study are as follows: (1) Given the distinctive stress assignment between Korean and English, what are the stress processing abilities of Korean-speaking ELLs in the L1 (Korean) and in the L2 (English)? (2) Depending on the number of syllables, how does students’ stress sensitivity differ between the two languages? (3) Within a syllable structure, how sensitive are they to stress patterns across the two languages? 226  Investigating language-specific stress awareness engaged in L1 and L2 word reading can shed light on the degree of cross-language transfer and can furthermore suggest that dissimilar L2specific prosodic features increase particular difficulties in L2 stress processing due to overgeneralized stress rules gained from L1. 2. An Overview of Stress Assignment Rule Differences In Korean, regardless of word classes (i.e., noun or verb), stress is typically assigned depending on number of syllables. In disyllabic words, for example, stress is almost always placed on the first syllable and in Korean polysyllabic words, depending on the weight of the first syllable, stress patterns of the words are divided into two categories. In other words, if the first syllable is heavy1, stress is almost always placed on that syllable. If not, either on the first or on the second syllable, displaying no significant linguistic meaning and function changes (Lee, 1990). The stress assignment rules in Korean are represented as follows: (a) Two syllable morphemes: Stress falls on the first syllable. (b) Three or more syllable morphemes: If the first syllable is heavy, stress falls on that syllable. Otherwise, either on the first or on the second syllable, with no important linguistic difference implied (Lee, 1990, pp. 50-51). However, stress rules in English are varying with classes of words, playing a role of meaningful and grammatical contrasts. Stress of nouns and verbs is governed by different rules, respectively (cf. Roca and Johnson, 1999). (a) Nouns and suffixed adjectives: The penultimate syllable is stressed if it is heavy; otherwise stress falls on the antepenultimate syllable. (b) Verbs and unsuffixed adjectives: The ultimate syllable is stressed if it is heavy; otherwise stress falls on the penultimate syllable. Based on the three research questions addressed earlier, this present study would predict that depending on number of syllables in a word and syllable structure in the word, the sensitivity to stress patterns of Korean-speaking ELLs differs between Korean and English. More specifically, the performance on stress processing across the two languages would noticeably differ on dissimilar and non-overlapping stress patterns than similar and overlapping ones because unfamiliar stress-sensitive L2 (i.e., stress assignment rule) may present additional challenges to bilinguals whose L1 is less stress-sensitive. 3. Methods 3.1. Participants In the context of English as a foreign language (EFL), sixty-four ninth-graders learning to speak and read in Korean and English simultaneously were recruited to voluntarily participate in this study (mean age: 15.97 years; 32 boys, 32 girls). All of the participants were native speakers of Korean with similar socio-cultural backgrounds, attending the same middle school located in Kyunggi-do (province). Based on the results of a demographic questionnaire the participants were individually asked to fill out, the mean stay of English-speaking countries was 0.04 years and all of them have been staying at Seoul metropolitan area and Kyunggi-do in which standard Korean is spoken. All of their family members including the subjects spoke Korean at home. In short, all of the subjects had limited exposure to English as an L2 and spoke standard Korean. 3.2. Testing Items 11 A heavy syllable is one with a branching rhyme (VC) or a branching nucleus (VV), contrasted with V, which` is a light syllable. The number of segments on onset does not matter with regard to the weight of syllables (Spencer, 1996). 227  In order to examine how Korean-speaking ELLs process a stress in a Korean word, twenty-five Korean real words were selected and split into 5 subcategories with respect to their syllable structures, the number of syllables, the placement of stress in a word (see Appendix A). In particular, nouns are dominantly employed for Korean (L1) testing items to match comparability of English (L2) items in terms of their syllable structures and stress patterns. That is, the Korean verbs and adjectives corresponding to a syllable structure and stress pattern in English do exist but rarely. Additionally, because there exist no Korean words stressed on the second syllable, only the syllable structure of ‘CV.CVC under the two syllable category in Korean was used. In the same way, thirty English real words were selected to observe Korean-speaking ELLs’ stress awareness of English (see Appendix B) and fall into 6 subcategories. Twenty items for each subcategory, total of 120 words, were initially field-tested with the participants and then the items with which more than 50% of the subjects were familiar were excluded. Equally important, the selected thirty unfamiliar real words are manipulated by changing the onset of stressed syllable and considering place of articulation and, if it does not work, then manner of articulation (i.e., ‘magic -> ‘nagic). 3.3. Procedure The participants were individually assessed in reading Korean real words, English unfamiliar real words, and English pseudo-words. The order of the three language tasks was counterbalanced and the randomized items were visually presented on the screen of a laptop. Within a language task, the students were asked to read target words one by one. Prior to the administrations of each of the two English real and pseudo-word tasks, two trial items per each syllable structure were given to the students. All of the three tasks were conducted by a fluent Korean-English bilingual experimenter, who recorded the students’ responses over the three tasks in a quiet room. Each session was audio-taped for later coding of accuracy via a MP3 player. 3.4. Coding The recorded responses of the participants’ production of each Korean and English stimulus were transferred onto a computer. In order to identify a placement of stress in a word, Praat program 2 , commonly used for acoustic analysis (Ladefoged, 2003; Yang, 2000) was downloaded from the following link at http://www.fon.hum.uva.nl/praat/. Because a stressed syllable has a longer vowel than the other vowels in a word (Ladefoged. 2001), the waveforms retrieved from the recorded files were edited to measure the durations of vowels in a word. That is, a stress is typically assigned to the longer vowels than the neighboring vowels in the word. Each item within a syllable structure was scored as 1 when each participant has a correct placement of stress in a word. The score of the Korean syllable structure of ‘CV.CVC was used twice because there are no Korean words stressed on the second syllable in two-syllable words. Thus, total score of each language task was 30. Accuracy of stress placement of each item was calculated per subject, then summed up, and averaged in terms of syllable structure, number of syllable, and language task. The mean of stress awareness was converted into correct percentage of stress sensitivity. 4. Results and Discussion In investigating stress processing sensitivities of Korean-speaking ELLs, a series of repeatedmeasures ANOVAs was conducted to measure differences between language tasks, number of syllables, and syllable structures separately. In addition, Bonferroni multiple comparisons were 2 The program was developed by Paul Boersma and David Weenink at the Institute of Phonetic Sciences, University of Amsterdam. 228  carried out to compare specific language tasks, number of syllables, and syllable structures respectively. Table 1 shows the means observed for the three language tasks depending on number of syllables and stress placement within a syllable structure. Inspection of Table 1 indicates that overall, Korean-Speaking ELLs performed better on the Korean words than on the English real/pseudo-words and regardless of language task, the mean accuracy of stress awareness generally decreases when number of syllable increases. More important, their productive sensitivity of stress placement was more accurate in responding to similar stress patterns between Korean and English than dissimilar ones.  Table 1: Means observed for the three language tasks depending on number of syllable and stress placement within a syllable structure (N = 64). Language Task  Korean Real Word  English Real Word  English Pseudo Word  Number of Syllables  
1. Introduction 1.1.Ontology ‘Ontology’ has recently become one of the most attractive research areas. Even though the notion originates from philosophy, there have been many researches within AI or Computer Science. Ontology is usually defined as “an explicit specification of a conceptualization,” where a “conceptualization is an abstract, simplified view of the world that we wish to represent for some purpose” (Gruber 1993: 199). But many constructed ontologies are not consistent, because the conceptualization is different according to the interests of researchers. Therefore most ontologists make use of languages which are considered to reflect the objects of conceptualization objectively. If we consider that an element of conceptualization is a concept, it is followed that the notion of ontology is related closely with languages. As we know, the modern linguistics is based on the ‘meaning triangle’ (Ogden and Richards 1923). At the triangle, the connection of a symbol with an object is mediated by a concept, and each element constructs its own system. So the system of symbols, i.e. a network of words, is similar to the system of concepts, i.e. an ontology. Therefore we can assume, that an ontology can be constructed indirectly by means of words. The network of words is often called ‘Language Ontology’. Nickles et al. (2007) considers Language Ontology as an answer to the following question: “What kinds of things do people talk as if there are?” Further the notion of Language Ontology is defined as “a conceptualization or categorization of what normal everyday human language can talk about” (Zaefferer 2002). * Copyright 2008 by Hae-Yun Lee and Sueun Jun 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 245–252 245  1.2. Goals The goal of this paper is to present the methodology of constructing the language ontology about ‘coherent relations’. Some previous language ontologies such as WordNet, etc. have been restricted to the lexical categories such as nouns, verbs, adjective, and adverbs. There were no ontologies about abstract notions such as coherent relations which are realized mostly as minor categories such as suffixes or connectives. For the construction of ontology of coherent relations, we will adopt the following strategy: At the upper-level, we will construct the ontology in a top-down manner, reflecting the theoretical considerations which are relatively language-independent. But at the middle-level, we will proceed with the work in a bottom-up manner. That is we will construct the middle part of the ontology by investigating lexical items. Since the construction of all the coherent relations is an enormous project, we will focus on the ‘causal relations’ in this paper. With the ontology to be constructed we can make a typological analysis about coherence relations. For example we can compare the Korean ontology with the English or the Dutch ontology presented in Knott (1996). The trend of typological researches is based on the socalled ‘ontolinguistical’ approach.1 2. Coherence 2.1. Coherence relations Discourse is more than a random set of utterances, and shows connectedness. The connectedness is captured by the concept of ‘cohesion’ and ‘coherence’. In comparison with cohesion, coherence is an abstract concept which language users establish by relating the different information units in the text. Coherence is divided into ‘referential coherence’ and ‘ relational coherence’ (Sanders and Maat 2006). The latter has been investigated under the theme ‘coherence relations’.2 That is, under the assumption, that the interpretation of the related segments needs to provide more information than is provided by the sum of the segments taken in isolation, text grammarians adopt the view that text segments are connected by coherence relations like CAUSECONSEQUENCE between them. Such coherence relations can be made explicit by linguistic markers, so-called connectives or cue phrases, but not always, as we see in (1). (1) (a) The buzzard was looking for prey. The bird was soaring in the air for hours. (b) Gareth grew up during the 1970s, so he loves disco music. It has been disputed, what coherence relations are. Some have insisted that coherence relations should be considered as cognitive entities (Hobbs 1979, Mann and Thompson 1988, Sanders et al. 1992, 1993). In addition to that, we can find a similar view in Lyons (1977). He divided entities into 3 subtypes: first-order entities, second-order entities, and third-order entities. He took the third-order entities to be “such abstract entities as proposition, which are outside space and time” (ibid. 443p.). Furthermore, he mentioned the possibility of distinction in those entities, for example between psychological and non-psychological entities. We think that coherence relations in the sense of Sanders et al. (1993) can correspond to those psychological third-order entities. 
1. Introduction Automatic term recognition (ATR) plays an important role in many natural language processing applications, e.g., information retrieval (IR) (Chowdhury, 1999; Zhou and Nie, 2005), information extraction (Yangarber et al., 2000), domain specific lexicon construction (Hull, 2001), and topic extraction (Lin, 2004). Its technological advancements can facilitate all these applications for performance enhancement. Despite the large volume of literature on ATR, further significant success in the field still relies heavily on a sound resolution of two basic issues, namely, the unithood and termhood of a term candidate, as identified in Kageura and Umino (1996). The former quantifies the unity of a candidate (especially, a multi-word candidate), indicating how likely a candidate is to be an atomic linguistic unit. It works more like a filter to exclude non-atomic (thus unqualified) term candidates but has little authority to determine which atomic language unit is a true term. The latter measures how likely a qualified candidate is to be a true term in a subject field. It plays a decisive role in licensing a term. Regardless of the previous progress in term extraction, termhood measurement remains the most critical problem to be solved. Novel technologies and methodologies are needed in order to bring up new insights into our understanding of this problem. This paper is intended to pre- * The research described in this paper was supported by the Research Grants Council of HKSAR, China, through the CERG grant 9040861 (CityU 1318/03H) and by the City University of Hong Kong through the Strategic Research Grants 7002037 and 7001879. Correspondence concerning this research should be addressed to Dr. Chunyu Kit. Copyright 2008 by Xiaoyue Liu, and Chunyu Kit 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 253–261 253  sent a novel statistical approach to domain specific term extraction from a collection of thematic documents following the basic ideas of corpus comparison and emerging pattern. It measures the termhood of a term candidate in a subject domain in terms of its peculiarity to this subject domain via comparison to several background domains. In order to avoid unexpected interference from unithood issues, our study focuses on investigating the termhood measurement for mono-word terms. Nevertheless, this is by no means to imply that mono-word ATR can be any easier than multi-word ATR in any sense. It is pointed out in Daille (1994) that the automatic identification of mono-word terms is possibly more complex than that of multi-word ones. One of the reasons is that all structural information that can be utilized for multi-word ATR is not available for mono-word ATR. In this sense, the latter needs to tackle a fundamental issue, that is, how to differentiate between terms and non-terms without resorting to structure information. The rest of the paper is organized as follows. Section 2 presents a brief review of previous work on ATR, to give a background for our research. Section 3 formulates our approach and the working procedure involved. A series of experiments are then reported in Section 4 for the purpose of evaluation. Section 5 concludes the paper with a highlight on the advantages of our approach. 2. Previous Work Various approaches to ATR were developed in the past. From a methodological point of view, the existing approaches can be classified into the following categories. Linguistic A linguistic approach played a dominant role in the early research on ATR. As early as twenty years ago, Ananiadou (1988) studied the effectiveness of theoretically motivated linguistic knowledge (e.g. morphology) in term recognition. A common procedure involved in this kind of approach is to carry out part-of-speech tagging first and then some pre-defined syntactic patterns, e.g., noun-noun compounds (Dagan and Church, 1994; Wu and Hsu, 2002) and base noun phrases (Justeson and Katz, 1995), can be applied to identify term candidates. All word combinations that match none of the predefined patterns are filtered out. This approach was reported to achieve good results on small scale corpora. However, its disadvantages include inadequate coverage of pre-defined syntactic patterns, low transplantability to other domains or languages, and incapability of excluding non-term candidates consistent with the pre-defined patterns. Statistical Various kinds of statistical information can be utilized to support term extraction, e.g., frequency (Damerau, 1990), mutual information (Damerau, 1993), C-value (Frantzi and Ananiadou, 1996), NC-value (Frantzi et al., 1998), imp function (Nakagawa, 2001), KFIDF measure (Xu et al., 2002), standard deviation (Lin, 2004) and entropy (Chang, 2005), to name but a few. A multi-word term is assumed to carry a key concept and is thus expected to behave like an atomic text unit. Many of these statistical measures are applied to explore such unity or structural stability of a multi-word candidate, namely, its unithood. Besides, a bootstrapping approach is reported in Chen et al. (2003) to learn domain specific terms from unannotated texts on a subject. Wermter and Hahn (2005) identify multi-word terms among n-grams of words in a large biomedical corpus, measuring their termhood in terms of their paradigmatic modifiability. Although statistical approaches share an advantage, i.e., their language independency, they are far from reliable while working on small corpora. Hybrid Linguistic knowledge is used in conjunction with statistical information in most hybrid approaches to ATR. For example, some syntactic patterns are first applied to identify term candidates, by filtering out those unqualified ones, and then a statistical measure is applied to validate the true terms among them. Daille (1994) presents an integrated approach to ATR that works this way. In addition, a hybrid approach can also be applied to combine several independent term recognizers for a better performance than any of them alone, as reported in Vivaldi et al. (2001). 254  Corpus Comparison This approach is a popular direction in recent ATR research. Its basic idea is to utilize the distinct distributions of terms and non-terms in different corpora to facilitate term extraction. That is, true terms are more prominent in their own subject field than in others. The original idea of this approach can be traced back to Yang (1986) that attempts to identify scientific terms by their statistical distributional difference between science and general texts. The statistics in use for this purpose include document frequency, average frequency, relative standard-deviation, etc. Ahmad et al. (1994) quantify similar contrasting distributions of terms in different corpora by means of the ratio of relative frequencies of a word in a domain corpus and a background corpus. The words with a score larger than 1.0 are then identified as the most potential terms. Chung (2003) applies a similar scheme called normalized frequency ratio to extract single-word terms in anatomy, reporting a performance of about 86% overlap with the results from a manual rating approach. In Uchimoto et al. (2001), more statistical characteristics (e.g., term frequency, document frequency and field frequency) of a term candidate are explored, achieving an F-score of 58.49%. Kit and Liu (2007) propose to measure mono-word termhood in terms of a candidate’s rank difference in a domain and a background corpus. The approach of corpus comparison can also be accomplished by statistical tests based on the null hypothesis that there is no difference between the observed frequencies for the same word in different corpora. Words with large testing values indicate a statistically significant difference between corpora and hence are more likely to be terms. The statistical tests include loglikelihood ratio (Rayson and Garside 2000), χ2-test, Mann-Whitney ranks test and t-test (Kilgarriff 2001). Drouin (2003) bases a term extraction process on a statistical test, which uses a normal distribution as an approximation to words' binomial distribution, obtaining an overall precision of 81% on term recognition. Based on Drouin’s work, Lemay et al. (2005) examine various corpus comparison approaches in different terminological settings. Among them, one is to use a general corpus as background and another is to break down the specialized corpus into six topical sub-corpora for comparison to the entire specialized corpus. Methodologically, a corpus comparison approach takes advantage of the intrinsic statistical characteristics of true terms in different corpora and thus has a preferable theoretical grounding over others that utilize only a special domain corpus. Our research reported in this paper falls into the category of corpus comparison approach with necessary elaboration for further enhancement. 3. Term Extraction Terms are linguistic representations of domain specific concepts to encode our special knowledge about a subject field. Emerging pattern (EP) (Dong and Li, 1999) presents a similar idea to corpus comparison in the field of database for knowledge discovery. EPs are defined as itemsets whose growth rates, i.e., the ratios of their supports1 in one dataset over those in another, are larger than a predefined threshold. When applied to datasets with classes (e.g., cancerous vs. normal tissues, poisonous vs. edible mushrooms), EPs can capture significant differences or useful contrasts between the classes in terms of their growth rates. In principle, the larger the growth rates, the more significant the patterns. This approach has been successfully deployed in several applications of data mining, e.g., Li and Wong (2002) on identification of good diagnostic gene groups from gene expression profiles. While the EP approach works on well-structured databases, corpus comparison deals with unstructured texts. Following the essential principle shared by the two, we consider domain specific term recognition from a thematic corpus of documents an issue of identifying words and expressions as EPs in the form of string highly peculiar to their own subject fields than to any others. In this sense, the higher peculiarity of a term candidate to a particular domain but lower to the others, the more likely it is to be a true term in that domain. Accordingly, we opt to quantify the peculiarity of a term candidate to a subject field in terms of its emerging difference, which is to be scored according to statistical information such as fre- 
a  Rahmad Mahendra , Septina Dian Larasati , and Ruli Manurung  a Faculty of Computer Science, University of Indonesia, Depok 16424, Indonesia rama42@ui.edu, septina.larasati@gmail.com, maruli@cs.ui.ac.id  Abstract. We adopt a previously developed model of deep syntactic and semantic processing to support question answering for Bahasa Indonesia, and extend it by adding a number of axioms designed to encode useful knowledge for answering questions, thus increasing the inferential power of the QA system. We believe this approach can increase the robustness of semantic analysis-based QA systems, whilst simultaneously lightening the burden of complexity in designing semantic attachment rules that transduce logical forms from syntactic structures. We show how these added axioms enable the system to answer questions which previously could not have been answered. Keywords: bahasa Indonesia, knowledge representation, QA systems, semantic analysis.  1. Introduction A question anwering (QA) system seeks to provide answers to questions expressed in natural language, where the answers are to be found in a given collection of documents. QA systems typically require more sophisticated linguistic analysis than conventional information retrieval, as they need to reason about various other factors, among others the types of questions, predicate argument structure, and result aggregation. In the work presented in this paper, we start from a unification-based grammar augmented with lambda-calculus rules that constructs semantic representations of Indonesian declarative sentences. To these representations we subsequently combine a suite of axioms designed to encode linguistic and world knowledge, and assert them into a knowledge base. A separate QA module answers queries by unifying the question semantic representation with the augmented set knowledge base. In Sections 2 and 3 we first discuss some relevant past work. Section 4 presents the overall framework of our system, and Section 5 discusses the semantic representation underlying our approach, arguing for some form of axiomatic post-processing. Finally, Sections 6 and 7 present the axioms themselves, along with some examples of how they contribute to the QA process. 2. Lightweight semantic approaches to Question Answering In general, there are two approaches to QA: the bottom-up approach employs “shallow” statistical methods such as keyword-based retrieval, which benefit from the sheer size of large electronic collections of documents nowadays available (e.g. the web), and are very robust. Unfortunately, these probabilistic methods are sometimes unable to perform the required inference for answering complex questions. On the other hand, the top-down approach uses “deeper” linguistic methods to obtain semantic representations of both the question and (a subset of) documents. The resulting logical forms enable precise identification of answers, sometimes in cases where they are not explicitly stated in the source documents. However, these * Copyright 2008 by Rahmad Mahendra, Septina Dian Larasati, and Ruli Manurung 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 262–271 262  deep methods typically require carefully-engineered, language-specific resources that are very costly to produce and not very robust. More recently, work has been done in developing QA systems that try to combine the two approaches, e.g. (Moldovan et al., 2003), (Narayanan and Harabagiu, 2004), and (Shen and Lapata, 2007). Crucially, these systems capture predicate argument structure that is shown to be essential for complex question answering. Additionally, semantic representations enable logical inference, allowing the QA systems to answer more complex queries by exploiting knowledge encoded in ontologies such as WordNet, SUMO, and various other Semantic Web-based resources. COGEX, the system reported in (Moldovan et al., 2003), is a QA system that employs a robust syntactic parser that essentially outputs a quasi-logical form containing part of speech information and general predicate argument structure. This output is passed to a theorem prover, and question answering is modelled as a theorem proving task. To aid this process, several axioms are added: NLP axioms establish the semantic content ignored by the robust parser from syntactic constructions such as complex nominals, coordinated conjunctions, appositions, and possesives. World knowledge axioms augment the knowledge extracted from the document collection with knowledge from existing ontologies, e.g. WordNet (Fellbaum, 1998). 3. Question answering in bahasa Indonesia Bahasa Indonesia (hereinafter simply ‘Indonesian’) is the official language of Indonesia, spoken by over 100 million people. Given this fact, we believe it is underrepresented in terms of research into Indonesian QA, and Indonesian NLP in general. There has been some work on developing QA systems for Indonesian. (Wijono et al., 2006) sought to achieve multilingual QA by answering queries in Indonesian based on English documents. Questions are classified based on a manually constructed taxonomy of Indonesian questions. The query is then automatically translated into English using a commercial translator available online1, and from then on is handled as a purely English QA task. (Purwarianti et al., 2007) uses a machine learning method to develop the question and answer classifier modules based on a corpus of raw text. (Larasati and Manurung, 2007) presented a purely symbolic approach that adopts a deeper linguistic approach, leveraging a previously built syntactic parser for the Indonesian language (Joice, 2002). We adopt this approach and extend it with some post-processing of the semantic representations with a suite of axioms. 4. Our QA system framework The overall framework of our Indonesian QA system consists of the following modules: a syntactic parser, a semantic analyser, and a question answering module augmented with axioms. Following (Larasati and Manurung, 2007), we use a unification-based grammar implemented as a set of DCG rules in Prolog. Since wide coverage is currently not the main aim of our research, we developed a relatively small yet usable handcrafted grammar and lexicon based on the official Indonesian grammar (Alwi et al., 1998). The semantic analyser module transduces semantic representations from parse trees. These semantic representations are designed to abstract away syntactic variations, allowing sophisticated automated processing of Indonesian texts. We adopt a ‘flat’ semantic representation (Hobbs, 1985). Details of the specific representation we use is presented in Section 5.1. Adopting the well-known rule-to-rule hypothesis, we augmented the lexicon with semantic information (Section 5.2), and developed semantic attachment rules for each grammar rule (Section 5.3). Although the above Indonesian semantic analyser is intended to be general-purpose, we have a specific concrete aim of developing a question answering system for Indonesian. Currently, we have implemented a prototype query processor in Prolog. The semantic representations of 
c  Ruli Manurung , Graeme Ritchie , and Henry Thompson  a Faculty of Computer Science, University of Indonesia, Depok 16424, Indonesia maruli@cs.ui.ac.id b Dept. of Computing Science, University of Aberdeen, King’s College, Aberdeen AB24 3UE, UK g.ritchie@abdn.ac.uk c HCRC, University of Edinburgh, Informatics Forum, 10 Crichton St., Edinburgh EH8 9AB, UK ht@inf.ed.ac.uk  Abstract. This paper proposes performing natural language generation using genetic algorithms to address two issues: the difficulty in controlling global textual features which arise from a large number of interdependent local decisions, and the difficulty in applying conventional NLG wisdom in domains where the communicative goal lacks sufficient detail. It presents details of an implemented system that embodies the aforementioned proposal, and discusses the results of an empirical study conducted using the system. Keywords: natural language generation, genetic algorithms, poetry, creative language.  1. Background This paper proposes an approach to natural language generation (NLG) using the genetic algorithm (GA), a widely used stochastic search method. In this section we discuss two well known issues in NLG, and in Section 2 we discuss why and how genetic algorithms can address these issues. In Sections 3 to 5 we present an implementation of an NLG system that embodies the proposed approach. Our own interest is in developing a generator that conveys a given semantics as a text that simultaneously exhibits a certain metre, i.e. regular patterns in the rhythm of the text. Consequently, some of the design decisions, particularly concerning the evaluation functions, are domain-specific. However, we believe the architecture as a whole is of general-purpose interest. The paper concludes with some examples and discussion in Section 6. 1.1.Achieving fidelity and fluency Oberlander and Brew (2000) argue that NLG systems must achieve fidelity and fluency goals, where fidelity is the faithful representation of the relevant knowledge contained within the communicative goal, and fluency is the ability to do it in a natural-sounding way such that it engenders a positive evaluation of the system by the user. In practice, applied NLG systems can often sidestep the fluency goal given a very restricted domain of output, with a limited style that may be just enough to serve the purpose of the application. Unfortunately, fluency may be controlled by global textual features which arise from a large number of local decisions, few of which are based on stylistic considerations. This does not suit * The work reported in this paper was carried out while all authors were at the University of Edinburgh. Copyright 2008 by Ruli Manurung, Graeme Ritchie, and Henry Thompson 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 272–281 272  the prevalent paradigm of NLG as a top-down, goal-driven process, decomposed into the stages of content determination, text planning, and surface realisation, typically implemented within a pipeline architecture (Reiter, 1994) (cf. the “generation gap” problem in Meteer (1991)). Oberlander and Brew propose an architecture which consists of two collaborating modules: an author and a reviewer. The author faces the task of generating a text that conveys the correct propositional content, i.e. achieving fidelity, whereas the reviewer must ensure that the author’s output satisfies whatever macroscopic properties have been imposed on it, i.e. achieving fluency. Recent corpus-based NLG systems (Langkilde and Knight, 1998) essentially embody this architecture: a symbolic generator acts as the author, and a language model acts as the reviewer. 1.2.Vague communicative goals Most NLG systems make two basic assumptions: that text generation is communicative goaldriven, and that these goals are sufficient to dictate a top-down approach for the planning of the text’s structure and decomposition of goals. However, Mellish et al. (1998b) claim there is a class of NLG problems for which these basic assumptions do not apply. In the case of the ILEX system, this is due to two factors. Firstly, as ILEX produces explanation labels of jewelry items on display, there is often no clear plan or goal to be conveyed beyond “say something coherent and interesting about this artifact within the space available”. Secondly, ILEX can not plan far in advance, as it has to generate text in real-time based on the user’s choices. The alternative approach they adopt is opportunistic planning, whose key elements are interleaving of planning and execution, flexible choice of tasks from an agenda, expanding “sketchy plans” as needed, taking into account the current state of the world, and recognition of opportunities through detection of reference features. 2. Using genetic algorithms to do NLG In addressing the above issues, we advocate treating the NLG process as a constraint satisfaction problem, where a solution is a text that satisfies multiple interdependent constraints relating to various levels of linguistic representation, e.g. semantic, syntactic, pragmatic, stylistic. Finding such a solution requires searching a space that is undoubtedly immense. Our proposed solution is to employ the genetic algorithm (GA), a widely-used heuristic search strategy that relies on random traversal of a search space with a bias towards more promising solutions. Specifically, it evolves a population of individuals over time, through an iterative process of evaluation, selection, and evolution. Upon termination, the fittest individual is hoped to be an optimal, or near-optimal, solution (Bäck et al., 1997). Using GAs to do NLG has been done before, e.g. Mellish et al. (1998a). However, these previous attempts employed GAs as optimisation functions for specific subtasks of NLG. We believe that handling the entire NLG process through GAs opens up the potential for various flexible approaches, which we discuss in Section 2.1. In particular, employing GAs allows a measure of opportunistic planning (Section 1.2), where the evolutionary cycle enables fitness functions to recognize opportunities and provide feedback to the executor, i.e. genetic operators. Finally, we note that using genetic algorithms for NLG also reflects a discriminative model of generation, where domain knowledge is stated declaratively, i.e. what a good text should look like instead of how to write one (cf. the corpus-based systems in Section 1.1). 2.1.Representing linguistic constraints and the encoding of domain knowledge When using GAs for NLG, a solution is a text that must achieve fluency and fidelity goals. These goals can be expanded as a set of constraints to be satisfied by a text, e.g. it must be grammatical, it must convey some given meaning, it must be readable, etc. There are two ways constraints can be implemented: ensuring that all possibly evolvable solutions never violate the constraint, or imposing penalties on individuals that violate a constraint. There is a trade-off: the former approach is obviously ideal, but its intractability is often the very reason GAs are employed in the first place. On the other hand, imposing 273  excessively heavy penalties often leads to premature convergence on the first found well-formed solution, whereas if the penalties are too light, the GA may continue to evolve ill-formed solutions that score better than well-formed ones. Within this framework, there is large scope for flexibility in terms of where domain-specific knowledge is encoded to help satisfy these constraints. For NLG, it seems reasonable to assume that candidate solutions must at least be grammatically well-formed. Oberlander & Brew’s author-reviewer model specifies that the author focuses on achieving fidelity, whereas the reviewer focuses on maximizing fluency goals. In GAs, this suggests devising genetic operators that explicitly work towards realising some input semantics, and fitness functions that measure fluency factors such as readability, length, coherence, etc. However, other setups are possible. For instance, one could envisage an author (genetic operator) concentrating on fluency whilst a separate reviewer (fitness function) assessed the output for fidelity, or a pair of reviewers assessing a document of grammatical nonsense1, each concentrating on fluency and fidelity respectively. Such approaches may seem unnecessarily awkward for conventional NLG tasks, but may provide a more suitable platform for NLG systems without well-defined communicative goals (see Section1.2). Note that the various components, i.e. the ensemble of authors and reviewers, can more or less be defined independently of each other, modulo the need for a common representation of a candidate text. This addresses the “engineering argument”, one of the main arguments supporting the pipeline architecture as opposed to an integrated architecture (Reiter, 1994), i.e. it enables a modular decomposition of an NLG system, thus resulting in a more manageable implementation. 3. Linguistic representation Our system represents candidate texts as lexicalized tree adjoining grammar (LTAG) derivation trees, augmented with the use of feature structures (Vijay-Shanker and Joshi, 1988). A derivation tree can be seen as the basic formal object that is constructed during the course of sentence generation from a semantic representation (Joshi, 1987). However, derivation trees are also the ideal data structure within our system for another reason, i.e. the non-monotonic structure building nature of GAs. Since the genetic operators may involve randomly altering subtrees through subtree deletion and swapping, we must somehow undo the unification of certain feature structures. Using the derivation tree as our primary data structure, we are able to store all local feature structures in their respective elementary trees (cf. Kilger (1992)). When required, e.g. to evaluate certain properties of the resulting text, the derived tree is rebuilt. Redundant computation is minimized by reusing a cached derived tree if it has not been modified between iterations. Within evolutionary theory, the LTAG derivation tree can be viewed as the genotypic representation of candidate solutions, from which we can compute the phenotypic information of semantic (Section 5.2) and prosodic (Section 5.1) features via the derived tree. We adopt a simple ‘flat’ semantic representation that is often used in NLG (Koller and Striegnitz, 2002). A semantic expression is a set of first order logic literals, which is logically interpreted as a conjunction of all its members. The arguments of these literals represent domain concepts such as objects and events, while the functors state relations between these concepts. See Section 5.2 for some examples. The semantic form of a tree is the union of the semantic expressions of its constituent elementary trees, with binding of variables during substitution and adjunction to control predicate-argument structure; cf. Stone et al. (2001). Finally, since our system requires information on prosody, each word is associated with its phonetic spelling, taken from the CMU pronouncing dictionary2. 
 {wannex_007@yahoo.com}  This study attempted to determine the types of semantic change of the selected Cebuano words from the written texts, specifically the Bisaya magazine and spoken language of Cebuano speakers aged 15 – 40 years old living in Cebu province. Twenty (20) Cebuano words were analyzed using Campbell’s (1998) and Crowley’s (1997) classifications of the semantic change. The results revealed that metaphor was the dominant type of semantic change in the written text and broadening was frequently-used in the spoken language. Keywords: broadening, metaphor, semantic change, widening 1. Introduction  “Earlier grammars and dictionaries reveal that Cebuano-Bisaya has undergone rapid changes in the course of the last four centuries.” (Wollf, 2001, p. 121). There are words listed in the earlier dictionaries that seem to be unknown in the current time. In fact, some of the young Cebuano speakers who are more exposed to other languages such as English do not understand the speech of their elders who used old Cebuano terms.  Trask (1994) also pointed out that the young generation had a hard time in reading what the ancestors wrote 400 years ago. In the same manner, the descendants would surely find reading what the current generation of speakers is writing presently or the future generation could even hardly understand the tape recordings and films shown these days.  Because of the cultural influences, the language was also affected. There are many Cebuano terms that are currently used by the native speakers that were originated in English and Spanish. In fact, “when the Spaniards left the Philippines in 1898 after almost 350 years of colonization…” as reported by Gonzalez (2004, p. 73), ten percent of the Filipinos spoke in Spanish.  * Acknowledgment This paper was made possible because of the following persons: Dr. Shirley Dita, who invited me to submit an article to PACLIC, Dr. Hsiu-chuan who encouraged me to write a paper in Historical Linguistics, Ms. Ma. Lourdes Apodaca, Mrs. Letty Espina, Mr. Joselito Flores, Miss Haidee Palapar, Dr. Hope S. Yu, and Mrs. Norly Plasencia, who were my informants and consultant, respectively. Copyright 2008 by Rowanne Marie R. Maxilom  22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 282–289 282  However, thirty percent of the Filipinos could speak in English after the Philippines had been colonized by the Americans for forty eight years. In this manner, the colonization by the Spaniards and Americans could possibly account for the semantic change that was undergone by some Cebuano words. Bloomfield (1933) and Campbell (1998) defined semantic change as a change in the concepts that were associated with a term and the innovations that change the meaning of words. With regard to the types of semantic change, Campbell (1998) and Crowley (1997) have the following classifications: First, widening or broadening refers to the increase of the meaning of words (Campbell, 1998) and a change in meaning which could result in a word processing additional meanings while retaining the original meaning (Crowley, 1997). For example, the English word ‘dog’ had a specific meaning of ‘a powerful breed of dog’ which has a broader meaning that includes ‘all breeds of dog’. Second, narrowing involves the change of meaning that decreases its range of reference into a fewer context (Campbell, 1998) and occurs when a word refers to only part of the original meaning (Crowley, 1997). For instance, the word ‘starve’ (i.e. to suffer or perish from hunger) came from the Old English word ‘steorfan’ (i.e. to die). Third, metaphor is a type of semantic change that involves one kind of thing in relation to another kind of thing that is somehow similar to the previous thing (Campbell, 1997). For example, the Cebuano word ‘higante’ which is similar to the concept ‘big’ is used in describing a great writer. Fourth, litotes is another type of semantic change that refers to exaggeration through understatement (Campbell, 1998). For instance, the phrase ‘of no small importance’ is mentioned that would mean ‘something that is too important’. Other types of semantic change that were identified by Campbell (1998) are metonymy, synechdoche, degeneratin or pejoration, and elevation or amelioration. Metonymy occurs when there is a change of meaning of a word which includes other senses that are not originally present but are closely associated with the word’s previous meaning. An example of metonymy is the Spanish word ‘plata’ (i.e. silver) that has been elaborated to mean ‘money’. Another type of semantic change is synechdoche (i.e. a part-to-whole relationship). For instance, the word ‘tongue’ means ‘language’. In addition, degeneration or pejoration takes place when the sense of the word has a negative assessment in the minds of the users. For instance, the word ‘spinster’ has a negative connotation of an unmarried older woman. Elevation or amelioration (i.e. a shift of meaning which has an increasingly positive value judgment) is also another type of semantic change. An example of this type of semantic change is the Spanish word ‘casa’ (i.e. house) which had a previous meaning in Latin that is ‘hut’ or ‘cottage’. The other types of semantic change are taboo replacement and avoidance of obscenity which can also be called euphemism, hyperbole or exaggeration (i.e. shifts in meaning because of exaggeration through overstatement), and bifurcation or semantic split classified by Crowley (1997) (i.e. when a term acquires another meaning that relates to the original meaning). To provide a concrete example of taboo replacement is the use of the term ‘rooster’ instead of ‘cock’ to avoid obscene association of ‘cock’ with ‘penis’. In addition, an example of hyperbole is the use of word ‘terribly’ that means little more compared to ‘very’. Lastly, an illustration of 283  bifurcation is the English word ‘silly’ which had a cognate in German word ‘selig’ (i.e. blessed) and is derived from Seele ‘soul’. Semantic shift takes place when the meaning ‘blessed’ changes its meaning into ‘stupid’ or ‘reckless’ in Modern English. Moreover, Campbell (1998) and Crowley (1997) argued that there were various causes of language change. First, climate and geography could account for language change. The languages of mountainous areas as pointed out by Crowley (1997) tended to change swiftly than the languages spoken closer to the sea level due to higher altitude. Second, foreign influence or substratum is also one of the causes of language change. According to Campbell (1998), the substratum theory of linguistic change involves the idea that if people migrate into an area and their language is acquired by the original inhabitants of the area, then any changes in the language can be put down to the influence of the original language. For example, the colonization Spaniards to the Philippines has influenced the Cebuano language, especially the counting system. Third, simplification is another cause of language change. In this cause, the production of sounds in one language (e.g. Spanish /v/) in the word verde would be simplified into Cebuano berde which means ‘green’. 2. Related Literature This section focuses on the studies concerning the semantic correlates in Cebuano (Adeva, 2005), semantic meanings of Cebuano and Hiligaynon enclitic adverbs (Lising, 2004), and the semantic reconstruction in Austronesian Linguistics (Zorc, 2004). In Adeva’s (2005) study, the semantic correlate in Cebuano and the way the semantic correlate were reflected in the morpho-syntax and semantics of Cebuano were investigated. This study also determined the transitive and intransitive constructions of the Cebuano stories. Using Hoppes and Thompson’s (1980) transitivity hypothesis and Dixon’s (1994) syntactic-semantic primitive cited in Adeva’s (2005) study, the results revealed that in actor focus, two affixes (mi-/ni-~m-replesive) were evident and three affixes (i-, -un, -an) were evident in goal the focus. However, this study had only few number of Bisaya magazines where the stories were taken from. The parameter could have been clearly described so that the scales are clear and the semantic correlates of the grammar could be systematically presented. Lising (2004) analyzed the semantic meanings and syntactic distribution of the Cebuano and Hiligaynon counterparts of Tagalog enclitic adverbs in Cebuano short stories and Hiligaynon plays. The data of this study were taken from the published short stories (i.e. Mga Piling Kuwentong Sebuwano, 1986) cited in Lising (2004) and plays (i.e. Dulaang Hiligaynon, 1996) cited in Lising (2004). The results of this study revealed that all 15 Tagalog enclitic adverbs have counterparts in Cebuano and Hiligaynon except for the politeness markers such as ‘ho’ and ‘po’. However, this study used two different genres (i.e. short stories and plays). The results of this study could be more reliable and valid when only a single genre was used. Zorc (2004) conducted a study on the semantic reconstruction in Austronesian linguistics. In this study, Zorc (2004) focused on the important points such as 1) the part was relatively silent, yet important steps have recently been made which should guide the researchers in the art and science of assigning meanings to etymon, 2) a full citation of semantic information for each entry should be done, 3) a careful investigation of the breadth and meaning 284  of cognates within any given set should be conducted, 4) all information with synchronic values should be compared and contrasted within the semantic system of the languages shown, 5) having done steps 2, 3, and 4, the extrapolation of a common core could only be successfully undertaken, and 6) the results could be assessed through consulting current and past semantic theory. In this study, Zorc (2004) also argued that the semantic relationships (e.g. metaphor) could be considered as checks and balances for the semantic reconstruction method. On the contrary, the process of showing a full citation from every dictionary entry was time-consuming for the researchers. Although these studies dealt with the semantic aspects of language, none of these studies focused on the semantic change. Hence, a study on semantic change of Cebuano words should be conducted. The semantic aspects of language have not been given much attention by the researchers. Zorc (2004) argued that semantics was considered outside the domain of most formal linguistic analysis. Ward (1971) who conducted a bibliographic survey of the Philippine linguistic studies also found that no studies have been conducted concerning the semantic change in Cebuano. Campbell (1998) and Crowley (1997) added that the semantic aspects of language were overlooked by many of the researchers in historical linguistics. Thus, a study on the semantic change of Cebuano words should be conducted to fill this gap. This paper attempted to determine and analyze the semantic change of the selected Cebuano words. Also, this paper aimed at determining the types of semantic change undertaken by the Cebuano words from the written texts and spoken data. 3. Methodology The data were taken from Bisaya, a Cebuano-Bisaya magazine issued in 2004 and 2005. Twenty (20) words were chosen (i.e. 10 words from the short stories and feature articles of the authors residing in Cebu province to avoid lexical variation from the other Cebuano-speaking communities and 10 words from the spoken discourse). The list of words from the spoken language was assumed to be used by the Cebuano speakers who are assumed to be aged 15-40 years old living within Cebu province. Textual analysis was used in this study because the data were based on the magazine named Bisaya. Word is the unit of analysis since only the Cebuano words that underwent semantic change were chosen. In analyzing the types of semantic change, the classifications of Campbell (1998), and Crowley (1997) were used. To validate the reason in using a metaphor in writing Cebuano literature such as short stories and feature articles, five (5) informants were asked about their personal reasons in using a metaphor in writing short stories and feature articles. Three informants were Cebuano writers of short stories and feature articles and two informants were writers of Filipino short stories. A consultant who was a student taking up Doctor of Arts in Languages and Literature at De La Salle University – Manila was also chosen to make the analysis of the data more reliable and valid. 4. Results and Discussion 
 a School of Informatics, University of Edinburgh, Informatics Forum, 10 Crichton Street, Edinburgh EH8 9AB, Scotland, UK b Faculty of Informatics, Osaka Gakuin University, 2-36-1 Kishibe-minami, Suita, Osaka 564-8511, Japan {aotani, steedman}@inf.ed.ac.uk  Abstract. This paper describes desiderative constructions in Japanese with the main focus on ta(i) ‘want’ desideratives. In spite of the morphological one-word status, desiderative constructions have been claimed to have a complex structure at some abstract level of representation. We claim that there are two types of desideratives, and that their predicates have different lexical representations within the framework of Combinatory Categorial Grammar. Building on the proposed analysis, we also discuss the difference between the two types of desideratives in terms of adverbial modification and passivizability. Keywords: Japanese Desideratives, Ta(i) ‘want’ Morpheme and Particle, Adverbial Modification, Passivizability, Combinatory Categorial Grammar (CCG) 1. Introduction Ranging across a number of differing expressions in differing languages, there are various constructions described as complex predicates. Japanese also abounds in such predicates, which consist of a stem verb or gerundive expression followed by another morpheme. Passives and causatives, for example, have been a focus of attention in many linguistic studies. However, only few attempts have so far been made at desideratives by comparison. In this paper we conduct a detailed examination of desiderative constructions in Japanese with the main focus on the suffix, ta(i) ‘want’. Ta(i) ‘want’ is suffixed to a stem verb and forms an adjective as exemplified in (1): The active counter part for (1) is the following (2): As shown in (1), the object argument of the stem verb can be marked with either nominative ga or accusative wo, though the stem verb originally marks its object by only accusative as in (2). * We are indebted to two anonymous PACLIC reviewers and Paul Crook for their invaluable comments on an earlier version of this paper. Our thanks also go to Mr. Yoshiyasu Shirai, the president of Osaka Gakuin University. All remaining inadequacies are our own. Copyright 2008 by Akira Ohtani and Mark Steedman 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 290–301 290  We call the type with nominative object ga-desiderative, and the type with accusative object wo-desiderative. One question here is why the object of desiderative construction can be marked with either nominative or accusative. Under the previous approaches based on series of transformations or movements, the desideratives have been claimed to have different complex structures at some abstract level of derivation or representation. However, there are some data that cannot be accounted for in terms of the only structural distinction. In this paper we discuss the syntactic and semantic properties of desideratives with the main focus on ta(i) ‘want’, show the lexical representation of the suffix, and answer to the question. 2. Previous Analyses and Adverbial Modification Kuno treats ta(i) as a sentential predicate (Kuno 1973) and then as a ‘transitive’ Deep Structure predicate (Kuno 1983). The latter predicate, for example, creates a biclausal control-type Deep Structure, which is reduced to a monoclausal Surface Structure by Predicate Raising and Tree Pruning. Inoue (1989a, 1989b) and Nishigauchi (1993) propose that complex predicates such as desideratives are formed by the process of Verb Incorporation, following Baker (1988). Inoue, for example, proposes that the two desideratives share the same D-structure but two patterns of incorporation available for the structure derive two types of S-structure, whose object NP is marked with nominative and accusative, respectively. There are, however, some problems in those analyses both empirically and theoretically. Putting aside the theoretical problems, we point out that all the above analyses fail to capture adverbial modification. See (3):1 One prediction resulting from their view is that both ga-desideratives and wo-desideratives should allow the same range of time and place adverbials, asita-kara ‘tomorrow-from’ and tonari-no heya-de ‘in the next room’, to be modifiers of the stem verb. Such adverbials, however, are restricted in the case of ga-desideratives as shown in (3). Thus, the contrast on adverbial modification in (3) cannot be explained by their analyses. Sugioka (1984) claims that ta(i) is suffixed to a V' as an instance of syntactic suffixation, thereby producing a wo-desiderative predicate. She also notes certain monoclausal properties of ga-desiderative predicates regarding adverbial modification. Adverbials that modify the stem verb alone cannot be placed between a nominative NP and a desiderative predicate as shown in (4) below: This observation, together with other considerations, has led her to propose that the nominative case marking results from the restructuring of a complex complement structure to a simplex structure at Surface structures as in (5):2 
In many situations, we use subjective expressions that convey emotions, feelings and evaluations about someone or something. It is essential to be able to understand such expressions, and in particular, to be able to use them in language generation tasks. One kind of text that involves subjective expressions is a review of consumer goods and services. In this work, we focus on the task of generating review sentences. This study considers emotional expressions as a subset of subjective expressions. Reviews of stories may include many emotional expressions that emerge in response to the story being reviewed. In this sense, we call the expressions of these emerging emotions ‘emotion-emerged expressions’ (hereafter, E3) in this paper. We first built lexicons for E3, and then made a prototype system to generate story reviews. 2. Related Studies Sentence generation technology has been approached from the viewpoint of the semantic network (Ozaki et al., 1997) or lexical relations (Seki et al., 1999). However, subjective expressions have not been considered. Sentences should be coherent, as emotional coherence affects the naturalness of a sentence. Consider these three sentences: ‧ She receives a present that feels joyous. ‧ She receives a present that is sad. ‧ She receives a present that is afraid. We can select the first sentence ‘She receives a present that feels joyous.’ as the most natural. In this study, we confirm the relationship between the naturalness of a sentence and the coherence of the emotion it expresses. 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 302–310 302  Related studies on emotion analysis include those of Kozareva et al. (2007), Yang et al. (2007) and Mihalcea and Liu (2006). Kozareva et al. propose an emotional classification approach based on frequency and co-occurrence. They calculate an emotional score for each word using mutual information. A word can express several types of emotions, and we aim to compile these expressions for text generation. Some methods are available for extracting sentences and paragraphs that have emotional content from the blog corpus. Yang et al. used an emoticon as a marker for emotional expression and extracted emotion expressing sentences. Mihalcea and Liu used blog tags such as ‘happy’ or ‘sad’ and extracted emotion expressing blogs. However, the tagged blog content that can be collected is limited. One characteristic of our method is considering words used not only in blog text but also in their titles. Yang el al. built a lexicon of emotions, and Mihalcea and Liu compiled phrases identified as happiness factor. They formed lexicons from 1gram, 2gram or 3gram. We form lexicons from syntactic pieces (Aoki and Yamamoto, 2007). Our purpose is to generate a natural sentence that includes subjective phrases. Therefore, we use syntactic pieces that have structural information and generate sentences effectively. 3. Emotion-Emerged Expression Lexicons (E3 Lexicons) This section presents now, we construct E3 lexicons that are collections of phrases associated with a certain emotion. We design the lexicons using blogs as a large corpus. Figure 1 illustrates the construction method. First, we identify blogs that express a certain emotion. Second, we form a model of classifier using these blogs. Third, the model estimates the emotional content of new blogs. Finally, we construct the E3 lexicons from these new blogs.  Figure 1: Construction process for an E3 lexicon  3.1. Emotional Word Lexicons Before constructing the E3 lexicons, we prepared emotional word lexicons. Emotional words clearly represent an emotion; for example, a word ‘glad’ clearly represents a happy emotion. Therefore, the lexicons provide useful information for extraction of particular emotion expressing blogs( in section 3.2). Emotional words are collected by a human as follows. First, we give adjectives and adverbial words extracted from the IPA thesaurus to 5 examinees. The examinees select the emotion associated with the words from eight possible emotions. Those are defined by Plutchik (1960). We used the words for which there is more than 75% agreement. Table 1 shows the eight possible emotions and the number of words associated with each emotion.  Table 1: The number of words in each emotional word lexicon Joy sadness acceptance disgust surprise anticipation fear anger  67 100  6  302 
1. Introduction In the realm of artificial intelligence, there is no denying that the ultimate goal for man-machine communication is to use natural human languages. Humans aspire to build a machine capable of understanding and responding to commands issued in the form of spoken language. Research towards this goal encompasses various research disciplines, and the fruits of such labor include machine translation, speech synthesis and recognition, and spoken language understanding. Spoken language understanding has been a challenge for scientists and engineers for many decades. Early efforts were primarily aimed at simply recognizing speech, and extensive research has been conducted to improve performance of speech recognition systems. Much of the improvements can be attributed to combining various research disciplines including linguistics, psychology, computer science, and signal processing, as well as improvements in modeling techniques, such as statistical and symbolic pattern recognition. Speech models now incorporate linguistic modeling to account for coarticulation and grammatical constraints to reduce the search * The author would like to thank the Citadel Foundation for its financial support in the form of a presentation grant. Copyright 2008 by Siripong Potisuk 311 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 311–320  space for the correct utterance. Also, steady advances in the computational efficiency of computers and hardware have enabled researchers to implement computationally intensive models that were deemed nearly impossible in the past. With the remarkable advances in speech recognition, the focus has now shifted toward the development of spoken language understanding systems. The goal is to build a machine capable of understanding naturally spoken language by combining together speech recognition and natural language processing technologies. Such systems must be capable of recognizing a large number of words (on the order of tens of thousand), and they also need to make better use of additional information present in the speech signal, such as prosody. It is well known that prosodic information, such as pause, stress, intonation, etc., facilitates human speech communication by helping disambiguate utterances. Prosody is an important aspect of speech that needs to be fully explored and utilized as a disambiguating knowledge source in spoken language understanding systems. Statistical modeling of prosodic information has the potential to be used as an additional knowledge source for both lowlevel processing (recognizing) and high-level processing (parsing) of spoken sentences. A knowledge source may be thought of as a module in spoken language understanding system, which contributes to the understanding of a spoken sentence. Low-level use of prosodic information potentially allows for more accurate recognition at the phonetic and phonological levels. It may also prove to be useful at the stage of lexical access during word hypothesization. The most relevant prosodic information at this level is intra-word and syllable-level stress. A lexicon containing word stress patterns may help the recognizer discard word hypotheses with poorly matched stress patterns. A syllable is considered stressed if it is pronounced more prominently than adjacent syllables. Stressed syllables are usually louder, longer, and higher in pitch. High-level use of prosodic information can help resolve ambiguities inherent in natural language independent of contextual information since computers do not currently utilize all the knowledge sources that humans do. Relevant prosodic information at this level includes pauses, sentential stress, and intonation contours. Sentential or inter-word stress information can be used in speech recognizers to resolve lexical and syntactic ambiguities. For example, the degree of stress among words in a spoken sentence provides an acoustic cue for distinguishing between content (noun, verb) and function (auxiliary, preposition, etc.) words. Furthermore, the marking of prosodic phrases may improve parsing performance by reducing syntactic ambiguities since prosodic groupings may rule out some of the syntactic groupings for a syntactically ambiguous sentence. Also, the identification of sentence mood (i.e., declarative, interrogative, command, etc.) from intonation contours may reduce syntactic and pragmatic ambiguity. The overall objective of this research is to study the role of prosody in the implementation of a spoken language understanding system for Thai since prosodic information has the potential to be used as a pruning mechanism at both the low and high levels of spoken language processing. In particular, we specifically examine how salient prosodic features of Thai (e.g., stress) can be integrated with the overall language modeling scheme. Thai is the official language of Thailand, a country in the Southeast Asia region. The language is spoken by approximately 65 million people throughout different parts of the country. The written form is used in school and all official forms of communication. Language modeling is one of the many important aspects in natural (both written and spoken) language processing by computer. For example, in a spoken language understanding system, a good language model not only improves the accuracy of low-level acoustic models of speech, but also reduces task perplexity (the average number of choices at any decision point) by making better use of high-level knowledge sources including prosodic, syntactic, semantic, and pragmatic knowledge 312  sources. A language model often consists of a grammar written using some formalism which is applied to a sentence by utilizing some sort of parsing algorithm. To overcome the difficulties in parsing Thai, we believe that a constraint dependency grammar (CDG) parser proposed by Potisuk (1996) appears to be an attractive choice for analyzing Thai sentences, considering vantage points from both written and spoken language processing aspects of an automatic system. CDG parsers rule out ungrammatical sentences by propagating constraints. Constraints are developed based on a dependency-based representation of syntax. The motivation for our choice of dependency grammar, instead of phrase-structure grammar, stems from the fact that it appears that Thai syntax might be better described by the former representation. Our work in this paper extends the adopted CDG parsing approach by incorporating prosodic constraints into a grammar for Thai. Prosodic information is specifically used as part of a disambiguation process. Disambiguation is accomplished through the use of prosodic constraints which identify the strength of association between prosodic and syntactic structures of the sentence hypotheses. We next describe the proposed basic framework for a Thai spoken language understanding system and detail the necessary steps for achieving the goal of integrating prosodic information with other aspects of the language model. 2. A Conceptual Model of a Thai Spoken Language Understanding System The elusive goal of building a machine capable of understanding naturally spoken language that covers a very large vocabulary has not yet been realized. Presently, state-of-the-art spoken language interfaces merely recognize, rather than understand speech, and such systems only achieve high recognition on tasks that have low perplexity. To achieve understanding, we need to improve performance of speech recognition systems as well as combine speech recognition with natural language processing technology. A spoken language understanding system may generally be thought of as comprising low-level (recognizing) and high-level (parsing) processing components although the detailed configuration may vary from system to system. The low-level processing usually involves acoustic modeling of the speech signal for recognition purposes. At the high level of processing, high-level knowledge sources (such as prosody, syntax, semantics, and pragmatics) are utilized not only for improving recognition performance, but also for analyzing the structure of the sentence hypotheses to obtain the best parse to map to a semantic representation in order to achieve understanding. This is commonly known as the language modeling aspect of the system. Spoken language understanding should be viewed as a computer-human interaction problem. Understanding how various cues contribute to system performance in the context of spoken language interfaces to task-oriented mixed-initiative systems is crucial in the design of a language model. Such systems are best evaluated and judged in terms of their success in supporting users in accomplishing tasks. A language model usually consists of a grammar which is applied to a sentence by utilizing a parsing algorithm to account for syntactic representation of the recognized string of words. In addition to syntactic parsing, the language model must be designed to be capable of using other high-level knowledge sources. The goal should be to not only improve the accuracy of low-level acoustic models of speech, but also reduce task perplexity. The choice of the approach to language modeling depends on how easily the model can be integrated with the acoustic model. Of primary concern are the issues of separability, scalability, computational tractability, and inter-module communication. These issues specify the level of interaction between the speech component and the language model, which can be classified into three categories: tightly-coupled, semi-coupled, or loosely-coupled systems. In this research, we choose a constraint-based system of integration, which can be classified as a loosely-coupled system of integration. It utilizes the language model, which is based on a CDG 313  
A disadvantage of this medium however is that the focus of discussion is prone to topic drifting; this drifting is primarily due to the time-lags inherent in the asynchronous mode of discussion. Another problem is that the manual monitoring and assessment of message contributions are time-consuming and often tedious, especially for teachers. For these reasons, automated analysis for discussion understanding and better methods of topic-focus monitoring to enable more efficient information assessment are much sought for (Ravi & Kim, 2007). In our study, we aim to investigate the feasibility of utilizing the functionality of word-space models for the task of analyzing online discussion transcripts. More particularly, we want to determine how word-space models can be used to detect the relevance of individual message posts Copyright 2008 by Rodolfo Raga, Jennifer Raga, Eric Bonus, and Raymund Sison 321 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 321–330  relative to the overall topic of discussion. Several methods for generating word-space models are available and have been successfully tested on detecting similarity between terms in groups of texts, but to our knowledge very few have been applied and tested on analyzing online discussion transcripts, a rare example is that of (McArthur and Bruza, 2003) which applied Latent Semantic Analysis on a small dataset of email correspondence to detect conversational implicatures. In our experiments, we chose to use the Random Indexing (RI) technique. RI is a word-space modeling technique primarily designed to measure similarity between terms and not between documents. In our work, we extended the capability of RI to work both at the document and message size levels. The rest of the paper has the following organization: The second section endeavored to put our study into context by providing a brief background on the problem of topic drifting in online discussions and cites a recent approach used to address it. The third section discussed the concept of word-space models and provides a brief introduction on how it can be implemented using RI. The fourth section presented a description of the experiments we conducted. In the fifth section we presented the results of the experiments and provided some discussion. We ended the paper with our conclusion and expected future directions. 2. Review of Literature The topic focus of online discussions constantly changes from one topic to another (Beaudin, 1999). This phenomenon is what (Potter, 2007) referred to as topic drifts or the tendency of online discussions to stray from their announced topic. In academic settings, researchers try to limit the occurrence of this problem by implementing anchored discussions (Guzdial and Turns, 2000). These educational online discussions are structured to align the contributions of participants to a single topic represented by the contents of a reference document called the anchor document. Literature shows that this method yields a more coherent discussion than traditional forums by promoting conscious topic monitoring (van der Pol et al, 2006). However, participants are unconsciously still prone to introduce irrelevant posts which often induce off-topic discussion. As such, tools that can help mediators automatically detect, at the earliest, such irrelevant contributions are still needed. An initial requirement for the development of such tools is a method for determining the alignment of each message to the overall topic focus of the discussion. This necessitates a content analysis approach similar to the strategy proposed by (Teufel and Moens, 1998) whereby the alignment of one text with another was measured by a mechanism that analyzes several characteristics of the aligned sentences such as the presence of particular phrases and occurrence of thematic words and proper names. For our purpose, we are looking towards utilizing the functionalities of word-space models for the same task. The context of the work we’re pursuing is the anchored discussion mentioned above. As such, the basis of the relevance value we are trying to measure is the closeness of the semantic information between the contents of each message to the contents of an anchor document. Word-space models have previously been observed to provide two modes of semantic similarity between terms. Sahlgren (2006) noted that these modes depend on whether the paradigmatic or syntagmatic relationship of the terms within a context are captured. Syntagmatic context refers to the linear relationship of words and applies to linguistic entities that occur in sequential combinations while paradigmatic context refers to the substitutional relationship of words and applies to words that can be used in the same context but not at the same time. We aim to initially investigate, among other things, whether or not these two modes will provide different levels of performance for this task, and if so, which provides the best result. 3. Word-Space Models and Random Indexing (RI) In this section, we present a simple introduction to the concept of word-space modeling, how it is used to assign meaning to a term, and how it can be implemented using Random Indexing. 322  3.1. Word Space Model A word-space model is a spatial representation that derives the meaning of words by plotting these words in an n-dimensional geometric space (Sahlgren, 2005). This process is similar to the way points are plotted in a two dimensional graphing paper. The main difference is that, in the case of a word-space, the dimension n can be arbitrarily large. The size of this dimension is determined by the number of unique word type in the set of words to be plotted. Usually, the coordinates used to plot each word depends upon the frequency of the contextual feature that each word co-occur with within a text. For example, words that do not co-occur with the word to be plotted within a given context are assigned a coordinate value of zero. The set of zero and non-zero values corresponding to the coordinates of a word in a word-space are recorded in a context vector. Because most of the words in any text dataset will never co-occur with a particular word, the context vectors are often sparse or full of zero values. By itself, the position of a word in a word space does not indicate anything about its meaning. To deduce a certain level of meaning, this position needs to be measured relative to the position of other words. In this sense, a linguistic concept known as the distributional hypothesis which states that “words that occur in the same contexts tend to have similar meanings” is applied. Having similar contexts means that words are surrounded or that they co-occur with same set of words. Thus, if we plot these words in a word-space they would be positioned close to each other. The level of closeness of words in the word-space is often referred to as the spatial proximity of words. This spatial proximity is what is used to represent the semantic similarity of words. A common approach used to determine spatial proximity is to measure the cosine of the angle generated between the plotted context vectors. The formula for computing cosine is as follows:  Q * D  CoSim(Q,D) =  (1)  |Q| * |D|  Where: Q is a vector representing one term or document, D is a vector representing another term or document related to Q, and |Q| and |D| are the magnitudes of Q and D. Currently, there are three major approaches to implement a word space. These include: Latent Semantic Analysis (LSA), Hyperspace Analogue to Language (HAL), and Random Indexing (Sahlgren, 2005). For our purpose, we opted to use the Random Indexing approach. 3.2. Random Indexing Random Indexing (RI) is a word space modeling technique that can be used with any type of linguistic context. It is inherently incremental and incorporates a built-in dimension reduction phase (Sahlgren, 2005). There are two basic steps involved in using Random Indexing to implement a word-space: 1. The first step involves the assignment of a unique and randomly generated label called an index vector to each word in the data. These vectors are sparse, high-dimensional, and ternary. High-dimensional means that it uses a large number of dimensions while ternary means that the label consists of a small number of randomly distributed +1s and -1s, with the rest of the elements of the vectors set to 0. 2. Then, context vectors with the same number of dimensionality as the index vectors are automatically produced by scanning through the text, and each time a word occurs in a context (e.g. in a document, or within a sliding context window), that word's index vector is added to the context vector for the word in question.1 RI has attracted much attention with successful applications in term similarity measurement. For our purpose, we have selected RI because of its incremental approach; this means that the  
 b  Yoshihide Sato , Harumi Kawashima , Hidenori Okuda , and Masahiro Oku  a NTT West Corporation b NTT Cyber Solutions Laboratories, NTT Corporation 1-1, Hikarino-oka, Yokosuka, Kanagawa, 239-0847 Japan y.sato@west.east.ntt.co.jp, {kawashima.harumi, okuda.hidenori, oku.masahiro}@labs.ntt.co.jp  Abstract. The ability to detect new topics and track them is important given the huge amounts of documents. This paper introduces a trend-based document clustering algorithm for analyzing them. Its key characteristic is that it gives scores to words on the basis of the fluctuation in word frequency. The algorithm generates clusters in a practical time, with O(n) processing cost due to preliminary calculation of document distances. The attribute allows the user to settle on the best level of granularity for identifying topics. Experiments prove that our algorithm can gather relevant documents with F measure of 63.0% on average from the beginning to the end of topic lifetime and it largely surpasses other algorithms. Keywords: trend, clustering, gradient model, word frequency 1. Introduction Due to the information explosion on the WWW, the cost of catching up with the latest trends has risen. Consumer Generated Media (CGM), such as weblogs and social networking service (SNS), are only accelerating this explosion. The best approach to recognizing trends from among the huge number of documents being created is to analyze the topics in them. The goal of Topic Detection and Tracking (TDT) is to find state-of-the-art events in a stream of broadcast news stories (Allan et al., 1998). The study defines segmentation, new event detection, and event tracking as the major tasks. Segmentation proceeds by automatically dividing a text stream into topically homogeneous blocks. New event detection identifies stories in several continuous news streams that pertain to new or previously unidentified events. Event tracking identifies any and all subsequent stories describing the same event as sample instances of stories describing the event. Document clustering is an efficient approach to find topics in many documents. In the tasks, new event detection is intimately related to clustering, and involves the functions of retrospective detection and on-line detection. The input to retrospective detection task is the entire corpus, and it is desired to divide them into event-specific groups. The input to on-line detection is a chronologically ordered document stream, and the change point of topics should be found. On the WWW, where documents are numerous and increasing hourly, our goal is to provide an environment that supports users on finding and tracking the topics. In particular, sensitive detection of new topics is needed there. Then, our research is categorized as both on-line event detection and event tracking in TDT. As a matter of fact, both aspects are essential for adequately grasping the topics. This paper introduces a trend-based document clustering algorithm that enables the detection of topic occurrence at the earliest possible stage and the observation of topic transition. The remainder of this paper is organized as follows: Section 2 describes related work; Section 3 describes our clustering algorithm; Section 4 describes our experiments and their results; and we conclude in Section 5. 2. Related Work New event detection is the target of incremental clustering algorithms for on-line documents. In new event detection, conventionally, the similarity between new document and existing clsuters are * Copyright 2008 by Yoshihide Sato, Harumi Kawashima, Hidenori Okuda, and Masahiro Oku 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 331–340 331  calculated, and it is judged that which cluster is appropriate to include the document or any one is inappropriate. Developments of similarity measure (Dharanipragada et al., 1999) and term weighting (Brants et al., 2003) have proposed for better detection performance. Many clustering algorithms have been applied for the task, such as single-pass based algorithm (Papka and Allan, 1998) and incremental k-means algorithm (Walls et al., 1999). They do not consider trends in on-line documents. However, it is required to focus attention on “time” for the sensitive topic detection. The time-focused approach attempts to enhance detection performance by attenuating document similarities on the basis of time interval between documents (Yang et al., 1998). The strategy yielded measurable improvements in their on-line detection experiments. Word distribution in a corpus is used to choose core lexicon in the corpus (Zhang et al., 2004). The algorithm is applied to choose topical words in document if the documents are divided into some parts by their timestamps, though time temporal continuity is not considered. Another incremental clustering algorithm F2ICM (Ishikawa et al., 2001; Khy et al., 2006) is characterized by its ease in updating the statistics value used for calculating document similarities when new documents arrive. It defines the forgetting model as being exponential. It attenuates worth of documents exponentially as time passes, as if they are forgotten. In their model, recent documents are likely to be situated closer to each other, and older ones are likely to be more widely separated. The algorithm tends to generate clusters containing especially newer documents. On the other hand, persistent clusters are seldom generated. Thus, the algorithm is not the best way to observe topics continuously in terms of event tracking. 3. Trend-based Clustering Algorithm What the prior studies lack in is the responsiveness to the current trends. Our approach to accomplish the goal is based on the trends in documents. More and more documents describing the same event are created when people’s interest in the event arises. In on-line documents, a rapid increase in the frequency of a word indicates a trend toward one or more topics relevant to the word. Taking such trends into account, when clustering documents, yields the sensitive detection of new topics. The most remarkable feature in our algorithm is that it senses current trends by word frequency fluctuation and gathers relevant documents based on the latest trends. Since its clustering process finishes in a short time after the classification granularity is indicated, it helps users to find adequate clustering results interactively that meet their intentions. Word weights in our algorithm involve word appearance growth and its accumulative appearance. We declare the gradient model in the following part before describing word weights. The concept of gradient, essential idea in our algorithm, represents the growth of the two aspects. Word weighting algorithm is described in the second subsection, and the clustering algorithm is detailed in the third subsection. 3.1. Gradient Model The impression of word appearance in a document declines over time. Suppose that the initial intensity of the impression is one, the intensity after time Δt can be defined as e−Δt /Tl following the forgetting model in F2ICM (Ishikawa et al., 2001). Tl denotes the parameter deciding the rate of intensity attenuation. 
 Kazutaka Shimada , Daigo Hashimoto , and Tsutomu Endo  a Department of Artificial Intelligence, Kyushu Institute of Technology 680-4 Iizuka Fukuoka 820-8502, Japan {shimada, d_hashimoto, endo}@pluto.ai.kyutech.ac.jp  Abstract. As the World Wide Web rapidly grows, a huge number of online documents are easily accessible on the Web. We obtain a huge number of review documents that include user’s opinions for products. To classify the opinions is one of the hottest topics in natural language processing. In general, we need a large amount of training data for the classification process. However, construction of training data by hand is costly. The goal of our study is to construct a sentiment tagging tool for particular domains. In this paper, we propose a method of sentiment sentence extraction for the 1st step of the system. For the task, we use a Hierarchical Directed Acyclic Graph (HDAG) structure. We obtained high accuracy with the graph based approach. Furthermore, we apply a bootstrap approach to the sentiment sentence extraction process. The experimental result shows the effectiveness of the method. Keywords: Sentiment analysis, sentence extraction, similarity, HDAG. 1. Introduction As the World Wide Web rapidly grows, a huge number of online documents are easily accessible on the Web. Finding information relevant to user needs has become increasingly important. The most important information on the Web is usually contained in the text. We obtain a huge number of review documents that include user’s opinions for products. Buying products, users usually survey the product reviews. More precise and effective methods for evaluating the products are useful for users. To classify the opinions is one of the hottest topics in natural language processing. Many researchers have recently studied extraction and classification of opinions (Hatzivassiloglou and McKeown 1997, Kobayashi et al. 2005, Pang et al. 2002, Wiebe and Riloff 2005). There are many research areas for sentiment analysis; extraction of sentiment expressions, identification of sentiment polarity of sentences, classification of review documents and so on. The goal of our study is to easily construct a corpus for sentiment information for particular domains that users want. For the purpose we need to extract sentiment sentences from documents as the 1st step of the corpus construction. Extraction of sentiment expressions or sentiment sentences is one of the most important tasks in the sentiment analysis because classification tasks usually need a large amount of training data to generate a high accuracy classifier. There are several reports for classification of sentences (Kudo and Matsumoto 2004, Osajima et al .2005). However, the purpose of these studies is to classify sentences into positive and negative opinions. Our purpose in this paper is to classify sentences into opinions and non- * Copyright 2008 by Kazutaka Shimada, Daigo Hashimoto, and Tsutomu Endo. This research was partially supported by the Ministry of Education, Science, Sports and Culture, Grant-in-Aid for Scientific Research (C), 18500115, 2008. 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 341–349 341  opinions. Touge et al. (2004) and Kawaguchi et al. (2006) have proposed methods for opinion extraction. However, these approaches essentially need a large amount of training data for the process. Construction of training data by hand is costly. Kaji and Kitsuregara (2006) have reported a method of acquisition of sentiment sentences in HTML documents. The method required only several rules by hand and obtained high accuracy. Also they have proposed a method for building lexicon for sentiment analysis (Kaji and Kitsuregawa 2007). The knowledge extracted from the Web by using the proposed methods contains the huge quantities of words and sentences. Takamura et al. (2005) also have reported a method for extracting polarity of words. These dictionaries are versatile and valuable for users because they do not depend on a specific domain. Here, assume that we need to construct a system for a domain. In that case, we often desire domain-specific knowledge for the system. Therefore, we need to efficiently extract sentiment sentences, which depend on a particular domain or topic. In this paper, we propose a method of sentiment sentence extraction. It uses several sample sentences for the extraction process. In the process, we compute a similarity between the sample sentences and target sentences. Yu and Hatzivassiloglou (2003) have reported a similarity based method using words, phrases and WordNet synsets for sentiment sentence extraction. However, word-level features are not always suitable for the extraction process because of lack of relations between words. For the similarity calculation we, therefore, employ the graph-based approach, called Hierarchical Directed Acyclic Graph (HDAG), which has been proposed by Suzuki et al. (2006). Furthermore, we apply a bootstrap approach into the sentiment sentence extraction process. The number of extracted sentiment sentences increases with the bootstrap approach. 2. A Graph-based Data Structure In this section, we explain a graph-based data structure to compute a similarity. 2.1. Hierarchical Directed Acyclic Graph In natural language processing, bag-of-words representation is the most general way to express features of a sentence for the similarity calculation. However, it is insufficient to represent the features of a sentence because of lack of relations between words. To solve the problems, Suzuki et al. (2006) have reported a new graph-based approach, called Hierarchical Directed Acyclic Graph kernels (HDAG). The method can handle many linguistic features in a sentence and includes characteristics of tree and sequence kernels. The HDAG is a hierarchized graph-in-graph structure. It represents semantic or grammatical information in a sentence. In this paper, we use the HDAG structure for the sentiment sentence extraction. We compute a similarity between HDAGs generated from sentences. See (Suzuki et al. 2006) for more information about the HDAG. 2.2. Layer Layers in the HDAG denote grammatical information in a sentence. To compute similarity between sentences correctly, we add some layers to a naive HDAG structure. The HDAG in this paper consists of three layers as follows: •Combined POS tag layer This layer consists of part of speech tags of words. We unify the POS tags of words in a bunsetsu1 into one node. This layer expresses a pack of POS tags in each bunsetsu. •POS tag layer This layer consists of the POS tags of each word or each compound noun. •Word/Compound noun layer This layer contains the surface expression of each word. We can use the surface information for calculation of similarity by adding this layer. The layer is used for handling compound 
 Kazutaka Shimada , Satomi Horiguchi , and Tsutomu Endo  a Department of Artificial Intelligence, Kyushu Institute of Technology 680-4 Iizuka Fukuoka 820-8502, Japan {shimada, s_horiguchi, endo}@pluto.ai.kyutech.ac.jp  Abstract. In this paper, we propose a simple and effective method for speech understanding. The method incorporates some speech recognizers. We use two recognizers, a large vocabulary continuous speech recognizer and a domain-specific speech recognizer. The integrated recognizer is a robust and flexible method for speech understanding. For the integration process, we use a simple edit distance measure of each output sentence from each recognizer. Our method has high scalability and accuracy. The experimental results show the effectiveness of the proposed method. Keywords: Multiple speech recognizer, Integration, Output selection, Edit distance. 1. Introduction Speech understanding and dialogue systems have been developed for practical use recently. These systems often recognize user utterances incorrectly. It is important to deal with speech recognition errors for speech understanding systems. Extracting keywords and understanding an utterance using them reduce speech recognition errors (Bouwman et al. 1999, Komatani and Kawahara 2000). Another approach is to use domain-specific grammars and linguistic models. However these methods can not handle out of domain and spontaneous utterances. One approach for the improvement is to repair recognition errors by users. There are many studies on detection of recognition errors in a speech output. Goto et al. (2005) have proposed some systems with nonverbal speech information, such as “SPEECH STARTER” and “SPEECH SPOTTER”. Ogata and Goto (2005) have proposed a speech input interface with a speech-repair function. Although repairing recognition errors by humans is effective in terms of development of a speech understanding system with high recognition accuracy, it is costly for users. Combining some recognizers is one of the best approaches to improve the accuracy of speech understanding systems (Isobe et al. 2007, Utsuro et al. 2004). Utsuro et al. (2004) have obtained high accuracy by using some speech recognizers' outputs. However they dealt with word error reduction only. Although Isobe et al. (2007) have proposed a multi-domain speech recognition system by using some domain-specific recognizers, their system cannot treat out-of-domain utterances such as a chat between users. However the chat utterances often include a significant role as the context of the dialogue. In this paper we propose a simple and effective speech understanding method based on a large vocabulary continuous speech recognizer (LVCSR) and some domain-specific speech * Copyright 2008 by Kazutaka Shimada, Satomi Horiguchi, and Tsutomu Endo. This research was supported by the New Energy and Industrial Technology Development Organization (NEDO), Intelligent RT Software Project, 2008. 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 350–357 350  recognizers (DSSR). The task of this system is speech understanding for a livelihood support robot. The DSSRs recognize particular utterances about orders; e.g., order utterances from elders who need care and order utterances from nurses. Figure 1 shows the outline of the proposed method. We construct the grammar-based DDSR for order utterances with small vocabulary and high accuracy for each order type. We use the LVCSR for recognition of utterances that the DDSR can not recognize, such as a chat between users. The information recognized by the LVCSR is of assistance for context construction of a dialogue. If we handle these different speech recognizers selectively and integratively, we realize a flexible and robust speech understanding method. Figure 2 shows the effectiveness of the proposed multiple recognizer. The DDSR achieves the order recognition with high accuracy and the LVCSR supplies lack of information in the order utterances. In this paper we use two recognizers, a large vocabulary continuous speech recognizer and a domain-specific speech recognizer for user’s order utterance understanding. In the experiment we focus on the selective usage of the multiple speech recognizer. In other words, it is to select outputs from each recognizer. For example, with respect to the utterance “Please pick up the remote” in Figure 2, it is important which result to select. For the selection we propose the One Commoner and Some Specialists (OCSS) model. In our system, the LVCSR is the commoner, namely domain-independent, and the DSSRs are specialists, namely domain-dependent. We focus on the difference between outputs generated from the commoner and specialists. For the method, we compare several features to judge whether an input utterance is an order to the robot or not.  Input utterance Order to the robot or Chat (non-order)  * Construction of a dialogue context * Solution to unknown words in grammar-based recognizers  A large vocabulary continuous speech recognizer Grammar-based recognizer 1  Integration or Selection  Grammar-based recognizer 2 ... Grammar-based recognizer n  * Order recognition with high accuracy  Recognition result  Figure 1: The outline of the proposed system.  LVCSR  DSSR for Order  Non-Order Utterance I dropped a remote controller under this bed....  *Select  Recognition  remote under this bed  ? (misrecognize)  * LVCSR often makes a mistake  Order Utterance Please pick up the remote.  Recognition  *Select  big up the remote  pick up the remote  Context information  Supplement  Results with high accuracy  Figure 2: The effectiveness of a multiple recognizer.  351  2. Grammar-based recognizer We use Julius as the LVCSR and Julian as the DSSR (Lee et al. 2001). The Julian consists of a vocabulary and a grammar file. For the grammar file we describe sentence structures in a BNF style, using word category names as terminate symbols. The vocabulary file defines words with its pronunciations (i.e., phoneme sequences) for each category. Here we design grammar and vocabulary files of the Julian which accepts only order utterances from users. The acceptable utterances by our DSSR recognizer for order sentences are as follows: • [Noun] wo [Verb] shi te kudasai or kureru ? [e.g., Please pick up the cellular telephone.] • [Noun] wo [Verb] shitai [e.g., I want to eat the snack.] • [Noun] wo [verb] kudasai or kureru? [e.g., Give me it.] • [Location] ni aru [Noun] wo [Verb] shite kudasai or kureru? [e.g., Please bring the remote controller on the table.] • [Noun] [e.g., The cellular telephone.] We evaluated this grammar-based recognizer with 50 test utterances and 2 test subjects (male and female). The accuracy rate was 97% on the utterance-level. Here the utterance-level denotes that we judged that the output was correct if all words in an utterance were correct. This result shows that the DSSR used in our multiple recognizer is a robust and high accurate speech recognizer for the order utterances. 3. Output Selection In our system, we need to judge whether an input utterance is an order to the robot or not. In this section we explain features and rules for the output selection. 3.1. Features For the output selection, we compared each output in a preliminary exrperiment. As a result, we obtained 4 effective features for the selection; (1) confidence, (2) the number of candidates, (3) existance of a short pause mark and (4) a similarity between outputs. • Confidence: This is a confidence measure that is computed from the speech recgnizer Julius/Julian. This score is based on a posterior probability of each word (Lee et al. 2004). The range is from 0 to 1. •The number of candidates: The 2nd feature is the number of candidates of each DSSR. The number of candidates in a DSSR’s output usually becomes small in the case that an input utterance is an order sentence. The reason is that the DSSR has high accuracy for target utterances because of small vocabulary. On the other hand, the number of candidates of a DSSR becomes large if the input is not an order utterance. In this situation, the DSSR generates many misunderstood results because the DSSR can not accept the input essentially. •Short paused mark: Julius/Julian can deal with a short pause in an utterance. If a short pause exists in an utterance, it output a short pause mark in the recognition result. The DSSR often contained the short pause mark in the result in the case that an input was not an order utterance1. Therefore the existance of the short pause mark in the output of the DSSR is effective to judge whether a input utterance is an order to the robot or not. • Similarity: The 4th feature is based on a similarity measure between outputs of the LVCSR and DSSR. Even human beings tend to misunderstand words which consist of similar pronunciations (Komatani et al. 2005). Here we focus on the output of the LVCSR. If an input is an order utterenace, the DSSR and the LVCSR generate similar outputs on phoneme-level because the 
1. Introduction The following examples taken from Lasnik and Uriagereka (2005) bring out an issue concerning the relationship between semantic similarity of verbs and the corresponding subcategorization frames. (1) a. She asked what time it was. b. She asked the time. c. She wondered what time it was. b. *She wondered the time. Sentences in (1) show that verbs like ask and wonder, which are similar in meaning, show different ways of argument realization. Although it is true that arbitrariness is at the core of the form-meaning mapping in any language, there are other aspects of language where formal or grammatical realization seems to be largely determined by meaning. Levin (1993) provides many * Acknowledgments: We owe special thanks to Jieun Jeon♡ for her assistance and partial participation in the preparation of this paper, especially for extracting relevant data from ICE-GB. We also would like to express our thanks to Prof. Seok-Hoon You for his support for our project. Finally, comments from three anonymous reviewers for PACLIC 22 are much appreciated. Copyright 2008 by Sanghoun Song, Jae-Woong Choe 358 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 358–367  examples of semantic classes of English verbs that lead to the same or similar pattern of argument realizations. For example, the cut verbs (e.g. chip, cut, scrape, snip, etc.), a semantically identifiable group of words in the sense that they all involve notions of motion, contact, and effect, do not allow causative alternations. That is, their subcategorization frames are affected by their meaning. (2) a. Carol cut the bread. b. *The bread cut. The question we raise in this paper is if the meaning restriction applies in a significant way to the realization of argument structure, if it does, to what extent it holds. In other words, how much is a syntactic pattern tied up with the semantic properties of the English verbs? In order to test this research question in a rather comprehensive way, we make use of large sets of language resources, in particular, ICE-GB and WordNet. In the process, we propose a more articulated methodology to take advantage of the language resources statistically and computationally. Also introduced in this paper are a software package to measure similarity of lexicons, an algorithm to cluster words in natural languages, and a kind of stochastic model to calculate selectional preference strength. 2. Methodology There are two main methods used in this study; one is clustering, and the other is the use of frequency value. In order to measure semantic relatedness between verbs, clustering method was used to divide verbs into various subsets in accordance with semantic similarity. As a way to ensure consistency in defining semantic similarity, WordNet (ver. 2.1) has been adopted as the basis for similarity measurement. The measurement itself was carried out by a module called WordNet::Similarity1 that calculates similarity between words in WordNet (Pedersen et al. 2004). The results from the module, then, went through a clustering process on the basis of the hierarchical bottom-up clustering algorithm, adopted from Manning and Schütze (1999). This study also required a method that can calculate the selectional preference strength of subcategorization frames. Selectional preference, in this paper, refers to the degree of relationship between a lexicon and its relevant items. In other words, this study seeks to figure out not merely whether a subcategorization frame can be realized or not, but how much relevance a verb has with the subcategorization frame. 3. Data The initial set of English verbs for this study was extracted from the Collins Cobuild English Dictionary (henceforth CCED), which divides English words into five level groups according to each word’s frequency. CCED includes approximately 1,600 words that belong to the highest or the second highest level groups. We then compared the list with the verb list in WordNet, and only those that appear in both lists were selected, resulting in 799 verbs. Starting with these 799 verbs, we tried to build up the hierarchical cluster and as a result obtained 61 meaningful clusters that cover 157 verbs out of 799. The next section provides an explanation of the whole procedure in detail. 4. The Analysis 4.1.Semantic Similarity 
 a College of Computer Studies, De La Salle University-Manila 2401 Taft Avenue, Malate, Manila, Philippines roxasr@dlsu.edu.ph  Abstract. An automated approach of extracting bilingual lexicon from comparable, nonparallel corpora was developed for a target language with limited linguistic resources. We combined approaches from previous researches which only concentrated on context extraction, clustering techniques, or usage of part of speech tags for defining the different senses of a word. The domain-specific corpora for the source language contain 381,553 English words, while the target language with minimal language resources contain 92,610 Tagalog word, with 4,817 and 3,421 distinct root words, respectively. Despite the use of limited amount of corpora (400k vs Sadat’s (2003) 39M word corpora) and seed lexicon (9,026 entries vs Rapp’s (1999) 16,380 entries), the evaluation yielded promising results. The 50 high and 50 low frequency words yielded 50.29% and 31.37% recall values, and 56.12% and 21.98% precision values, respectively, which are within the range of values from previous studies, 39 - 84.45% (Koehn et al., 2002 and Zhou et al., 2001). Ranking showed an improvement to overall F-measure from 7.32% to 10.65%. Keywords: Automatic lexicon extraction. 1. Introduction Automatic bilingual lexicon extraction is the automated process of acquiring bilingual word pairs from corpora in order to construct a lexicon. The product of this process, a bilingual lexicon (or a dictionary), is commonly used for Machine Translation, Natural Language Processing, and other linguistic usages. Since acquiring lexicon from parallel corpora already yields 99% accuracy (Rapp, 1999), this study focused into processing non-parallel corpora, specifically on comparable corpora. In addition, a lexicon is more helpful if it has the feature of grouping similar senses together. This feature is called word sense discrimination. In this study, the syntax and senses of the word are the contributing factors to discriminate words that have several meanings. Throughout the years, linguists and translators compile different types of lexicons manually from experts and automatically from corpora, which are collections of texts and other forms of writings. Parallel corpora refer to a source text and its translation into one or more target languages (Ahrenberg, et al., 1999). Parallel corpora are used for lexicon extraction due to the following characteristics (Fung, 1998): (1) Words have one sense per corpus; (2) Words have single translation per corpus; (3) No missing translations in the target document; (4) Frequencies of bilingual word occurrences are comparable; and (5) Positions of bilingual word occurrences are comparable. However, acquiring parallel corpora is labor intensive and time consuming. It is also unlikely that one can find parallel corpora in any given domain in electronic form especially for minority languages. This problem can be solved by using non-parallel corpora. Non-parallel corpora * Copyright 2007 by Eileen Pamela Tiu, and Rachel Edita O. Roxas 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 368–376 368  come in two forms: comparable and non-comparable corpora. Comparable corpora are collections of documents that have common domains or topics but different authors and published dates, while non-comparable corpora have totally different domains, authors, and published dates. This factor is crucial for languages with minimal language resources, and the utilization of existing language resource (such as non-comparable corpora) should be maximized to automatically generate other language resources (such as a bilingual lexicon). Several methods have been developed to utilize these types of corpora to automatically construct a lexicon such as the co-occurrence model (Rapp, 1999), the Convec algorithm (Fung, 1998), the exploration of other clues (Koehn and Knight, 2002), the dependency grammar approach (Zhou, 2001), and the word filtering approach (Sadat et al., 2003). Others utilize these types of resources for word sense disambiguation (WSD) (Kikui, 1999 and Kaji, H et al., 2005). Rapp (1999) yielded 65% accuracy when the first word in the ranked list was considered and 72% when other senses were considered. Convec (Fung, 1998) yielded a 30-76% precision when the top 20 ranks of the translations were considered. Zhou et al. (2001) achieved a 70.8% accuracy for high frequency verbs and 81.48% for low frequency words. Koehn et al. (2002) achieved 39% noun translation accuracy. The average precision of Sadat et al. (2003) was 41.7%. 
In this work we extend our previous investigation on learning approaches to Portuguese personal pronoun resolution in (Cuevas et. al., 2008.) In doing so, we focus on so-called ‘lowcost’ learning features, that is, we will limit the proposed solution to the knowledge readily obtainable from basic NLP tools such as part-of-speech taggers, and we will largely bypass deep syntactic or semantic analysis. In this sense, our work resembles the knowledge-poor approach in Kennedy & Boguraev (1996), which consists of a re-interpretation of the ‘classic’ algorithm proposed in Lappin & Leass (1994) using shallow rather than in-depth analysis. In addition to that, as we do not intend to explicitly write any anaphora resolution algorithms or rules (but rather induce them automatically) our work is mainly related to machine learning approaches such as Soon et. al. (2001), McCarthy and Lehnert (1995) and Ng & Cardie (2002). However, in discussing a possible ‘low-cost’ learning approach to Portuguese third person plural pronouns (“Eles/Elas”), we will focus more on the choice of learning features, and less on the results of a particular machine learning approach, which are to be discussed elsewhere. The rest of this paper is structured as follows. Section 2 reviews previous work taken as the basis for our present investigation. Section 3 proposed an extended set of features for the problem at hand. Results of a standard decision-tree induction algorithm using the new features are presented in Section 4. Finally, Section 5 draws a number of comparisons with related work in Portuguese pronoun resolution and Section 6 describes our future work. 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 377–383 377  2. Previous Work As in Cuevas et. al. (2008), we will follow Soon et. al. (2001) and regard anaphora resolution as a machine learning classification task. Accordingly, a pronoun j and a potential antecedent term i may be classified as co-referent or not, that is, for each pair (i, j) in the text, we intend to label a binary class coref as being co-referential or non co-referential. Positive instances of co-reference will consist of pairs (i, j) explicitly defined as co-referential in the training data by human annotators, and negative instances will consist of all pairs (i, j) in which i is an intermediate NP between j and its actual antecedent in the text. For instance, the pronoun j1 in the following text gives rise to one positive (i1, j1) and two negative ( (i2, j1) and (i3, j1) instances of anaphora. Analogously, pronoun j3 also co-refers with i11, and pronouns j2 and j4 both co-refer with i2:  Scientistsi1 know that the phenomenoni2 occurs once every three to seven yearsi3: theyj1 can detect when itj2 is coming, theyj3 perceive when itj4 is going away. The starting point of the work in Cuevas et. al. (2008) was the Portuguese portion of an English-Portuguese-Spanish parallel corpus tagged using the PALAVRAS tool (Bick, 2000), comprising 646 articles (440,690 words in total) from the Environment, Science, Humanities, Politics and Technology supplements of the on-line edition of the “Revista Pesquisa FAPESP”, a Brazilian journal on scientific news. Focusing on instances of third person plural pronouns (male) “Eles” and (female) “Elas”, two independent annotators created a data set of 2595 instances of co-reference, being 483 positive and 2112 negative, with an average of 4.4 intermediate antecedents between each pronoun and the actual antecedent. About 10% of the positive instances were set aside with their negative counterparts for testing purposes. Thus, the test data comprised 234 instances and the reminder 2361 instances (being 435 positive or coreferential, and 1926 negative or non co-referential) became our training data. As we are still in the process of defining which precise features are applicable to the task at hand, our investigation is currently based on the training data set only, leaving the test data reserved for future use. It was also shown in Cuevas et. al. (2008) that a simple set of syntactically-motivated features (based on distance, gender and number agreement) may achieve overall positive results in pronoun resolution (85.81% success rate) using C.4.5. ten-fold cross-validation decision-tree induction (Quinlan, 1993). However, this simple approach still suffers from low precision for the co-referential cases, making the resulting algorithm only partially useful for practical purposes. A more conservative (and possibly more reliable) analysis of these results focusing on the positive (i.e., co-referential) instances only shows a 70.5% score in F-measure. The following Table 1 summarizes those findings.  Table 1: Results from Cuevas et. al. (2008.).  Class  Precision Recall  Co-referential 0.572 0.910  Non Co-ref. 0.977 0.846  F-measure 0.703 0.907  
CBMS is a poverty monitoring system that is now used in Pasay City. It tracks poverty by surveying the people living in a certain area. Once this is done, they input them using the programs Census Professional (CSPro) and CMS-Natural Resource Database (NRDB), which are customized free software used to encode the data and to digitize spot maps. The output of CSPro is a text file, which is then used as an input to the program STATA, which they use to actually statistically monitor poverty. Since STATA needs technical skills to be used effectively, the data is very hard to access. Thus, people who actually must have direct access to these pieces of information must first ask the people/person who are/is capable of using the software to get the data for them, which might take some time. To address this problem, a Natural Language Database Interface (NLDBI) was developed for CBMS that will allow users to access the CBMS data without having to learn STATA. A NLDBI allows users to access a database using natural language query. It accepts a user query, extracts pertinent data from the query, converts the extracted data to SQL, then retrieves the data from the database. AlLaDIn is domain-dependent, database-dependent natural language interface for CBMS-Pasay. 2. System Architecture AlLaDIn is concerned with translating the English query by the user into its corresponding SQL query to retrieve the data from the database. The result will then be displayed to the user. This system has four major modules, as shown in the architectural design of the system in Figure 1. This design is partially based on the Masque/SQL system (Androutsopoulos et al., 1995) and the FST mapping technique (Gala, 2005). 2.1 Parser and Semantic Analyzer This phase involves the (1) the tokenization of input, (2) parsing and analyzing the semantics of * Copyright 2008 by Krissanne Kaye Garcia, Ma. Angelica Lumain, Jose Antonio Wong, Jhovee Gerard Yap, Charibeth Cheng 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 384–390 384  words, (3) retrieval of syntactic and semantic properties of words in the lexicon, and (4) checking the grammatical and semantic rules of the input. A context-free grammar for the English language is used to check if the sentence is grammatically correct.  Figure 1: Architectural Design of NLDBI-CBMS Once the sentence has been inputted, the Parser and Semantic Analyzer would tokenize the sentence. Each token would then be passed to WordNet for it to be properly tagged with its partof-speech (POS), other information tag (eg. singular, person), and the token’s synonyms. After being tagged by WordNet, it would then be passed to the lexicon. The lexicon contains domainspecific words or terminologies that are not found in WordNet. It would then tag each token that was not tagged by WordNet. After tagging all tokens, the sentence would then be parsed and checked if it is grammatically correct.  2.2 Semantic Post-Processor  Once the natural language query is accepted and confirmed by the system as grammatically and  semantically correct, the query is then passed to the semantic postprocessor. The semantic post-  processor is necessary for queries present the referencing problems of anaphora and ellipses  (Androutsopoulos et al., 1995). Consider the compound user query “How many people  live in Barangay 1? What about Barangay 2?” The second query is 
1. Introduction Controlled Language is a sublanguage of a natural language designed to improve the readability and the translatability of a text. Originally, the concept of a controlled language was introduced in the field of technical documentations to prevent the misunderstanding of texts. 1 In the last couple of years the necessity of a controlled language has increased significantly, especially in the technical documentation domain, as the number of pages to be translated has increased enormously. The increase of the volume of the documents to be translated made it necessary to employ a full automatic translation system like an MT or a machine-aided translation system like translation memory. In Korea, however, MT systems have not been widely welcomed by the experts in the localization business, because the quality of the translations failed to match their expectations. The recent researches on the controlled languages show that the use of a controlled language can generally lead to the improvement of machine translation quality, thus reducing the cost of 
Comparable corpora are corpora which select similar texts in more than one language or language variety1. These texts are typically gathered during the same time period. Comparable corpora are different from parallel corpora are widely used as resources for statistical machine translation, bilingual lexicons. Comparable corpora overcome the scarcity and limitations of parallel corpora, since sources for original, monolingual texts are much more abundant (Barzilay and Elhadad, 2003; Munteanu et al., 2004; Shao and Ng, 2004; Talvensaari et al., 2007). The degree and nature of lexical similarity and contrast among Mandarin Chinese used in different Chinese speaking societies were widely observed but not thoroughly studied due to the lack of comparable corpora. Recently, LDC’s Chinese Gigaword (2003)contains three sets of monolingual corpora selected according to the same set of criteria but in different language varieties from China, Singapore and Taiwan. We will explore it as a comparable corpus for variations of Chinese in this paper. In particular, we propose a measure of top-bag-of-word similarity for comparing the language variants contained in Chinese GigaWord corpus. Texts from the same period of time from Central News Agency (Taiwan), Xinhua News Agency (PRC) and Lianhe Zaobao (Singapore) are extracted and compared in our study. By comparing these three varieties of Mandarin Chinese, we hope to find the language significant lexical contrasts and meaning variations. We also propose a constrative approach towards automatic text source classification based on co-occurence similarity measures with documents from the same time period of Chinese Gigaword. Experimental resultes indicated that our proposed constrastive approach is reliable and robust. The rest of this paper is organized as follows. Section 2 investigates related literature in word similarity measures in comparable corpus and a brief introduction to Chinese Gigaword. Section 3 describe the text source classification based on co-occurence similarity. Section 4 presnts experimental results and further discussion. Finally, Section 5 concludes this study. 
On the other hand, it is difficult to design a tag set (labels) that can be used to annotate a corpus because the design of a tag set depends on the domain and the task. Therefore, we have to redesign the tag set and construct a corpus annotated with a new tag set if we apply our system to different domains or tasks. In addition, designing a tag set that can be used in any domain or task is very difficult. However, we have to annotate DA tags on a corpus, because many applications require predefined DA tags. This paper discusses an unsupervised approach to infer the user’s intention in a situation by using a dialog system. Unsupervised approach may not achieve highly accurate results when compared to the supervised approach. However, in any domain or task, the unsupervised approach can yield human DA annotators with machine judgments of the DA classification that may be useful to keep the consistency of DA annotation results for a corpus. In addition, annotating a corpus with given labels is very time-consuming. An unsupervised method is independent of annotation and designing the tag set. In order to achieve an unsupervised method, we need an unsupervised clustering method. So far, many clustering methods have been proposed and discussed for applications in natural language processing (NLP), such as works by Zhao and Karypis (Zhao and Karypis, 2005). However, an utterance is very short against a document that is used in a common NLP application. In addition, the 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 445–451 445  feature space that is used to express any natural language expression is extremely large and an utterance is expressed by a very sparse vector in the feature space. Therefore, it is very important to handle a sparse feature vector of an utterance in the huge feature space. 2. DA Annotation Here, we construct a dialogue system to make an itinerary of one-day sightseeing tour and also develop a dialog corpus for this system. The corpus consists of 100 dialogues between a professional tour guide and a tourist. Each dialog is almost 30-min long. An annotated corpus with DA is needed to construct our dialogue system. Therefore, we have started to design a DA tag set and annotate the DA tags on the corpus. However, there are several problems that make it difficult for us to maintain consistency in the annotation as follows: (a) segmentation, (b) pragmatics, and (c) multifunctionality. Sometimes, utterances are fragmental, and it is difficult to recognize an appropriate boundary of an utterance for a DA tag. Hinarejos et al. reported that the correct segmentation for DA is very important for obtaining an accurate result in DA tagging (Hinarejos et al., 2006). There is a pragmatic problem in the annotation of DA tags. For example, the utterance “Do you know what time it is?” can be recognized as a yes/no question from the surface information, but the speaker’s intention is a request such as “Please tell me the time.” In addition, utterances are generally multifunctional. This problem is closely related to the design of the DA tag set. So far, many DA tag sets have been proposed and used to annotate corpora. Some of them have several layers (e.g., DAMSL (Allen and Core, 1997)) and dimensions. In this paper, we focus on the pragmatic problem in the DA annotation. We try to resolve the pragmatics problem by paraphrasing. If a euphemism is paraphrased into a straightforward expression, the dialogue system can easily understand the expression. 3. Unsupervised method for DA annotation In this section, we describe an unsupervised approach to classify an utterance. The overview of the unsupervised approach is as follows: 1. Construct a feature vector from an utterance. 2. Reduce the dimensions of the feature space using a latent variable model. 3. Classify the vector whose dimension was reduced using an unsupervised classification method. After constructing the feature vector, we use a latent variable model to reduce the dimension of the feature space. Then, we use an unsupervised classification method to classify the vector that produced by using a latent variable model. Finally, we find the class to which the utterance belongs. We also introduce a rule-based paraphraser to reduce the variety of expressions because a different expression is treated to be completely different in a latent variable model. 3.1. Latent variable models Several unsupervised text modeling methods, such as PLSI (probabilistic latent semantic indexing (Hofmann, 1999)) and LDA (latent Dirichlet allocation (Blei et al., 2003)), are available to model a text based on the features of words and their frequencies. In general, the latent variables indicate the topics of each segment (some sentences for text or some utterances for speech), and we can use the topic information indicated by the latent variables of the model as a compact surrogate expression for a given feature vector of an utterance. In other words, we can use these models to reduce the dimension of the feature space. Once the model parameters are learned from a corpus, we can infer the topic of a given utterance. If we constructed a latent 446  variable model with k latent variables, we get a k -dimensional vector. This vector is called a  topic vector. We used PLSI—a latent variable model— for general co-occurrence data that associates an unobserved topic variable z ∈ Z = {z1,L, zk } with each observation, i.e. with each occurrence of word w ∈W = {w1,L, wM } in document d ∈ D = {d1,L, dN } . The probability of a topic under the document ( P(z | d ) ) is approximated by the following  formula:  P(z)2∏ P(w | z)∑ n(d, w)P(w | z) , (1)  w∈d  w  where n(d, w) indicates the frequency of word w in the document d . The details about how  to introduce Equation (1) have been previously shown (Ohtake, 2005). In that paper, Ohtake used PLSI and LDA to evaluate whether a paraphrasing pair is contextually independent or not, as well as if there was not a big difference in the performances between them. Therefore, we use PLSI because it is simpler and faster than LDA.  3.2. Unsupervised clustering method There are several unsupervised clustering methods. We used the K-means clustering algorithm (e.g., (Duda et al., 2000)) that is very simple because, at the moment, a highly sophisticated method in which analyzing the tendency of the results by an unsupervised approach and manually annotated labels is not necessary. In addition, we have to investigate whether a topic vector reasonably expresses a DA before using a sophisticated clustering method.  3.3. Paraphrasing to reduce variety of expressions The use of a wide variety of expressions that conveys the same information is natural. However, a different expression is treated to be completely different in a feature space. Therefore, paraphrasing techniques seem to be promising approaches to understand the variety of expressions. In particular, in Japanese, the ending of a sentence or utterance has many expressions even though they convey the same meaning. These expressions are related to the Japanese honorific system, and in most cases the difference in the expression does not affect the DA classification. We construct a rule-based paraphraser that is very similar to the paraphraser proposed by Ohtake and Yamamoto (Ohtake and Yamamoto, 2001), and most of the rules in the honorific system were derived from their paraphraser. The paraphraser was carefully designed to be free from errors and developed to paraphrase a variety of expressions that convey the same meaning into a standard expression. The rules of the paraphraser are based on a morphological analysis. We can use regular expressions for pattern matching in a rule and we can conjugate any morphemes that have conjugation to fit in its context. Therefore, a small number of rules cover a large number of targets that need to be paraphrased.  4. Experiments In this section, we describe our experiments and introduce the data set. We also mention the features that were used in the construction of the PLSI models. 4.1. Data set We used the ATR Dialogue Database (Morimoto et al., 1994). This database consists of 1,983 dialogues (83,052 utterances) in traveling situations. We used manually transcribed Japanese texts in the database. In the transcribed texts, fillers and disfluencies are tagged with a marker. In order to use precisely analyzed results, we eliminated the fillers and disfluencies in the transcribed texts by a morphological analyzer that was used to obtain morphemes as units like words.  447  We annotated 13 dialogues (489 utterances) with DA tags used in the paper by Tanaka and Yokoo (Tanaka and Yokoo, 1999) to evaluate the unsupervised classification. The remainder of the data, namely 1,970 dialogues (82,563 utterances), were used to estimate the parameters of the PLSI model. The original DA tag set that consisted of 26 tags was designed to annotate the dialogue segments that were shorter than an utterance. Therefore, there were multi-labeled utterances in our annotation results because in some cases, a person utters several things in a single utterance. For example, when a person is asked a YES or NO question (YN-QUESTION), the person who answers might say “Yes, I will…(YES, INFORM).” In this case, we treated the last DA tag as the labeled tag of the utterance. In the annotated dialogues, 16 tags were actually used. 4.2. Features for PLSI We used uni-gram and bi-gram word frequencies. In this paper, a word is considered as a morpheme1 in Japanese. An element of the feature consists of a pair of morpheme’s basic form and POS (part of speech). However, numbers and proper names are generalized by eliminating this basic form. In other words, the features of the numbers and proper names are recognized by only by their POS. In general, PLSI requires words and their frequencies in order to construct a model from a corpus. However, Serafin et al. showed that adding extra features works well with latent semantic analysis in the DA classification (Serafin et al., 2004). The PLSI model can be regarded as a probabilistic version of a latent semantic analysis. Therefore, we can expect the same effect on PLSI, and we introduced the uni-gram and bi-gram features. The segment for a unit of a document consists of the utterance and its previous utterance. The dialogues in the database are conversations between two people such as a customer and a clerk. 4.3. Number of variables and performance on differentiation We constructed PLSI models2 on the number of latent variables, namely 10, 50, 100, 200, and 300, in order to determine the number of latent variables. The parameter for tempered EM (TEM)—a technique used to ease the over-fitting problem—was set to 0.9 (we use this value in all of the experiments in this study) because this value exhibited the best performance in the preliminary experiments. We formulated topic vectors from the evaluation dialogue set, and we prepared the average vectors for each DA label from these topic vectors. Finally, we compared each average vector with the others according to their cosine values, and we averaged the cosine values. Therefore, these numbers indicate the distinguishing ability of topic vectors, where a smaller number is better. The average values for each number of latent variables (10, 50, 100, 200, and 300) with all the DA labels are as follows: 0.607, 0.334, 0.288, 0.290 and 0.275, respectively. 4.4. Impact of paraphrasing We applied the rule-based paraphraser to the data set (83,052 utterances), and all of the 56,027 utterances were paraphrased. First, we show the result of an unsupervised clustering result with manually annotated labels using a non-paraphrased corpus. We constructed the PLSI model with 100 latent variables from the learning corpus that was not paraphrased. The test set was fed to the PLSI model, yielding the topic vectors. Then, we used the K-means clustering method with 16 clusters because the size of the tag set that was used to annotate the test set is 16. The result is shown in the “without paraphraser” column of Table 1. Second, we show the result of an unsupervised clustering result with manually annotated labels using a paraphrased corpus. The result is shown in the “with paraphraser” column of 
Because manually creating templates can be tedious and time consuming, several researches have worked on automatically extracting templates from training examples that have been preprocessed. In our previous example-based MT, SalinWika (Bautista et al, 2005), templates are extracted from a bilingual corpus that has been pre-tagged and manually annotated with word features, resulting in a long training process. Its successor, TExt (Go et al, 2006), did away with a tagger and instead requires a parallel bilingual corpus and an English-Filipino lexicon to align pairs of untagged sentences to extract translation templates. Our pun generator, T-Peg (Hong, 2008), on the other hand, subjects the training examples through a pre-processing stage to identify nouns, verbs and adjectives. Instead of manually annotating the example set, the training algorithm relies on existing linguistic resources and tools to perform its task. * Acknowledgments: TExt Translation is part of the “Hybrid English-Filipino Machine Translation System” project that is funded by the Department of Science and Technology – Philippine Center for Advanced Science and Technology Research and Development (DOST-PCASTRD). It was developed by Kathleen Go, Manimin Morga, Vince Nunez, and Francis Veto as their undergraduate thesis. Copyright 2007 by Ethel Ong, Bryan Anthony Hong, and Vince Andrew Nuñez 452 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 452–459  2. Extracting and Using Templates for Machine Translation TExt Translation (Go et al, 2006) is an EBMT system that automatically extracts translation templates from a bilingual corpus and uses these to translate English text to Filipino and vice versa. It relies on a bilingual corpus for its training examples, which contains a set of sentences in the source language with a corresponding translation in the target language. Correspondences between the sentences are learned and stored in a database of translation templates. A translation template is a bilingual pair of patterns where corresponding words and phrases are aligned and replaced with variables. Each template is a sentence preserving the syntactic structure and ordering of words in the source text, regardless of the variance in the sentence structures of the source and target languages. During translation, the input sentence is used to find a matching source template, while the target template is used to generate the translation.  2.1.Translation Templates and Chunks TExt learns two types of translation templates using the Translation Template Learner heuristic presented in (Cicekli and Güvenir, 2003). A similarity translation template contains a sequence of similar items learned from a pair of input sentences and variables representing the differences. A difference translation template contains a sequence of differing items from the pair of input sentences, and variables representing the similarities. Consider the sentence pairs S1 and S2, and the learned similarity (T1) and difference templates (T2 and T3). S1: The boy is walking. Naglalakad ang batang lalaki. S2: The teacher is walking. Naglalakad ang guro. T1: The [1] is walking. Naglalakad ang [1]. T2: [2] boy [3]. [3] [2] batang lalaki. T3: [2] teacher [3]. [3] [2] guro.  Using the lexicon to align the corresponding English and Filipino words in the input sentences, the tokens “The”, “is walking”, and “Naglalakad ang” are retained as constants of T1, while “boy/batang lalaki”, and “teacher/guro” are retained as constants of T2 and T3, respectively. [1], [2], and [3] are variables in the template. A template variable, called chunk, is represented by a numeric value, e.g., [1], to refer to its domain. The domain allows chunks to have a reference from their source template. Specific chunks are labelled as [X.n], where X is its domain and n is its sequence number in the domain. Only the domain is needed to identify if a chunk can be used in translation. For example, if the domain in a template is [X], then any chunk with a domain “X” can be used to fill the variables in the template. From S1 and S2, chunks [1.1] and [1.2] are learned. If another chunk [1.3] is learned from a different set of input sentence pairs in a later training session, then all these chunks can be used during translation to fill variable [1] in T1.  [1.1]: boy batang lalaki [1.3]: carpenter karpentero  [1.2]: teacher guro  2.2.Learning Translation Templates Aligned sentence pairs are analyzed and translation templates are extracted following three steps, namely template refinement (TR), deriving templates from sentences (DTS), and deriving templates from chunks (DTC). TR compares an aligned sentence pair against existing templates in the database. An aligned sentence pair is said to match a given template if it contains a token that matches exactly with a corresponding token in the template itself. There must be a corresponding match in both the source and target languages for the template to be considered. Through these similarities, a candidate refinement is identified. Consider the input sentence pair S3, and the existing template T4 and chunks [4], [5] and [6].  S3: The boy is hopping in the park.  Nagkakandirit ang lalaki sa parke.  T4: The [4] is [5] in the [6].  [5] ang [4] sa [6].  [4.1]: girl babae  [5.1]: walking naglalakad  [6.1]: street kalsada  453  TR considers S3 as a candidate refinement for T4 because of their matching tokens (in italics). The identified differences are used to create new chunks, namely [4.2], [5.2] and [6.2].  [4.2]: boy lalaki [6.2]: park parke  [5.2]: hopping nagkakandirit  If refinement cannot be performed, DTS is performed to compare the new sentences pair with other aligned sentence pairs. Both Similarity Template Learning (STL) and Difference Template Learning (DTL), as presented in Cicekli and Guvenir (2003), are performed. The differing elements in the input are created as chunks for the similarity templates, while the similar elements are created as chunks for the difference templates. DTL always generates two difference templates for each matching input sentence pairs. Consider sentence pairs S4 and S5.  S4: My favorite pet is a dog. Aso ang aking paboritong alaga. S5: My favorite color is red. Pula ang aking paboritong kulay.  All similar tokens between S4 and S5 (in italics) are preserved as constants in the new similarity template T5 while the differing elements are created as chunks [7] and [8]. On the other hand, all differing tokens are preserved as constants in the new difference templates T6 and T7 while the similar element is created as a new chunk [9].  T5: [7.1]: [8.1]: T6: T7: [9.1]:  My favorite [7] is [8] [8] ang aking paboritong [7]  pet alaga  [7.2]: color kulay  a dog aso  [8.2]: red pula  [9] pet is a dog Aso ang [9] alaga.  
There have been many computational researches about coordinate structures (Kaplan and Maxwell, 1988; Kosy, 1986). But, it is unrealistic to apply most of them to large-scale MT systems as mentioned in (Okumura and Muraki, 1994; Kurohashi and Nagao,. 1994). The more practical approaches about analyzing coordinate structures such as (Okumura and Muraki, 1994; * This work was funded by the Ministry of Information and Communication of Korean government. Copyright 2008 by Yoon-Hyung Roh, Ki-Young Lee, Sung-Kwon Choi, Oh-Woog Kwon, and YoungGil Kim 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 460–466 460  Kurohashi and Nagao, 1994; Agarwal and Boggess, 1992) all analyze coordinate structures using parallelism between conjuncts, but they are mainly targeted to recognizing two conjuncts (i.e., pre-conjuct and post-conjuct). Out of them, (Kurohashi and Nagao,. 1994) is one of the most practical approaches. They recognize coordinate structures in a Japanese sentence by constructing a similarity matrix between bunsetsus and searching a path with the highest parallelism in the similarity matrix using a dynamic programming method. However, that method is inadequate to apply to patent documents which have usually a large number of coordinate nodes and sometimes complex modification such as an inserted clause. We devised an appropriate method to recognize coordinate structures for patent documents using a similarity table. Although our method seems similar to that method in appearance, it is considerably different from that in the manner of constructing a similarity table and finding coordinate structures. Our method is simpler but more effective in patent documents. In the next section, we outline the characteristics of coordinate structures in patent documents. And in the section 3, we present a method to recognize coordinate structures. In the section 4, we show experimental results and some analysis of the erroneous results, and then conclude our paper with some future works. 2. Coordination in Patent Documents Figure1 shows a typical sentence in an abstract of a patent document. This sentence belongs to enumeration in the patent description style, and describes elements of a product. By analyzing many example sentences in patent abstracts, we can outline the characteristics of the enumeration sentences of the patent abstracts as follows: 1) They often have some keywords such as “include, comprising, having, step_of, unit, means” 2) In case of enumeration of noun phrase (NP), the definite article such as “the, each, said” is not used in the head node of NP. The definite article is usually used as elaboration. 3) They generally follows the normal form “ X (, X)* (,) and X”, where “()” means optional, and “*” means any number of repetition. In this paper, on the basis of above features, we describe a method to recognize coordinate structures especially corresponding to enumeration in patent abstracts. A machine translation and telecommunications system includes a machine translation engine for translation of input text from a source language to a target language, a dictionary database including a core dictionary and a plurality of sublanguage (domain) dictionaries usable for translation from a source to a target language, a receiving interface for receiving text input from any of a plurality of users, each text input being accompanied by control information including user ID data indicative of one or more sublanguages preferred by a particular user, an output interface, and a dictionary control module coupled to the receiving interface responsive to the user ID data indicative of a sublanguage preference of a particular user for selecting a corresponding sublanguage dictionary of the dictionary database to be used by the machine translation engine along with the core dictionary for performing translation of the particular user's text input. Figure 1: An example sentence of patent abstract 3. Recognizing Coordinate Structures by Similarity Table As mentioned above, an enumeration sentence usually enumerates many elements or procedures and each element or procedure can have modifiers and nested coordination. So, overall sentence structure can be excessively complicated and the process of recognizing coordinate structures can be difficult. For this, we simplify the analysis target by recognizing all possible coordinate nodes and construct a similarity table for using parallelism between the coordinate nodes. 3.1.Recognizing Possible Starting Points of Parallel Nodes Recognizing coordinate structures is carried out after mophological analysis, tagging and base NP(BNP) chunking. The figure 2 represents the result of BNP chunking of the example 461  sentence. Assuming that all of coordinate structures follow the normal form “Xs (, Xm)* (,) and Xe”, there are three types of nodes, which are a starting node(Xs), a middle node(Xm), and an ending node(Xe). The starting points of coordinate nodes are recognized by some syntactic cues. By corpus analysis, we find that the syntactic tags of coordinate structures are mainly a noun phrase(NP), a verbal phrase(VP), and a that-clause(SBAR). The syntactic cues for recognizing starting points of coordinate nodes are shown in the table 1.  Table 1: The syntactic cues for recognizing the starting points of coordinate nodes.  Node Tag  Starting Node  Middle Node  Ending Node  NP  PREP|VERB /BNP  , /BNP  (,) and /BNP  VP  PREP|VERB /(ADV) VBG  , /VBG  (,) and /VBG  SBAR  PREP|VERB /that  , /that  (,) and /that  In the table 1, “VBG”, “VERB”, “PREP”, and “ADV” repesents a verb with ing-form, a verb, a preposition, and an adverb respectivey. And ’|’ means “or” and ’/’ means the starting point of a coordinate node. In the figure 2, the mark ’/’ represents the candidate starting points of the coordinate nodes recongnized by the given syntactic cue.1 In case that there comes a main verb before a comma or the end of the sentence after the ending node of a NP coordinate structure, we exclude that point from the starting points of the candidate coordinate nodes, regarding it as a starting point of a new clause. [A machine translation] and [telecommunications system] includes /[a machine translation engine] for [translation] of [input text] from /[a source language] to /[a target language], /[a dictionary database] /including /[a core dictionary] and /[a plurality of sublanguage (domain) dictionaries] usable for [translation] from /[a source] to /[a target language], /[a receiving interface] for /receiving [text input] from /[any of a plurality of users], [each text input] /being accompanied by [control information] /including [user ID data] indicative of /[one or more sublanguages] preferred by /[a particular user], /[an output interface], and /[a dictionary control module] coupled to [the receiving interface] responsive to [the user ID data] indicative of /[a sublanguage preference] of /[a particular user] for /selecting /[a corresponding sublanguage dictionary] of [the dictionary database] to be used by [the machine translation engine] along with [the core dictionary] for /performing [translation] of [the particular user]'s [text input] Figure 2: The result of BNP chunking and recognition of starting points of coordinate nodes 3.2.Constructing Similarity Table We construct a similarity table between the recognized nodes in order to use parallelism between the coordinate nodes. In the similarity table, the value of i-th row and j-th column(Si,j) means the similarity between the i-th candidate node and the j-th candidate node. The similarity between nodes is composed of the head node simiraity(s0), the head word similarity(s1, s3), and the structural similarity(s2). Then, Si,j is calculated as follows: Si,j = s0*(s1 + s2 + s3) (1) s0: tag similarity of the node (e.g., 1 if their tags are the same and are not NP, otherwise 1 if both of their tags are NP and their determiner types are compatible, otherwise 0, the determiner type will be explained later)  
Filipino is a language whose verb conjugations are very complex because it uses different affix (prefix, infix, and suffix) combinations and even duplication of syllables or words. When looking up a word in Filipino dictionary, one needs to know the infinitive form of the verb to be able to find it. This is a difficult task for someone who is not familiar with the way Filipino verbs are conjugated into any of the three tenses: Pangnagdaan (Past), Pangkasalukuyan (Present), and Panghinaharap (Future). These three are commonly known today as Aspektong Perpektibo, Aspektong Imperpektibo, and Aspektong Kontemplatibo, respectively (Dizon, 2006). Because of the complexity of the language, automated morphological analyzers will be difficult to construct. So researches should be focused on how to capture all possible forms so that a machine translation system can produce accurate translation. One approach is to use a lexicon that stores the context-words that help determine the appropriate equivalent word(s) in the target language. The lexicon must use the infinitive forms of the verbs to facilitate the look up of verbs and use headwords for non-verbs. So with this kind of lexicon in mind, a morphological analyzer that returns the infinitive form of a verb, its tense, and its affix(es) used is necessary. 2.Review of Related Works When someone attempts to create any machine translation system for natural languages, it is not possible to do away with morphological analyzers. One area of focus for morphological analyzers is the analysis of verbs. Verbs are usually conjugated according to tense, number, voice, and mode while nouns and adjectives are usually declined according to person, number, case, and degree. Some languages have simple forms of verb inflection or conjugation while other languages have complex forms of inflection. Japanese was originally thought to have simple verb inflection, thus it was not the central subject on Natural Language Processing * Copyright 2008 by Robert R. Roxas and Gersam T. Mula 22nd Paciﬁc Asia Conference on Language, Information and Computation, pages 467–473 467  (NLP). But in recent past, it had become an important subject (Hisamitsu and Nitta, 1994). For other languages, verbs are also given much attention in research. These languages include Chinese (Kim, et. al, 2002), Japanese (Nakamura, 2007), and Korean (Hong, et. al., 2004; Jun, 2007) to name a few. For languages that have complex ways of adding affixes and duplication of syllables or words, an excellent morphological analyzer is highly necessary. Filipino or Tagalog is one the most complex languages in the world. So several researches have been conducted in the area of morphological analysis for the Filipino language. One is the TagSA (Tagalog Stemmer Algorithm), which tackles on the extraction of the stem of any Tagalog word (Bonus, 2003). Another morphological analyzer, called TagMA (Tagalog Morphological Analyzer), is devoted to extracting the root word both for concatenative and non-concatenative formation (Fortes, 2002). A system called T2CMT (Tagalog-to-Cebuano Machine Translation) was developed (Fat, 2004) that used TagMA and TagSA for its morphological analyzer. Even before the TagMA and TagSA, there was already a morphological analyzer that was created and used in a prototype system that supported English-Filipino and Filipino-English machine translation system (Roxas, 1998). TagMA (Fortes, 2002) produces three morphological structures (morpheme, CV, and syllabication) to represent an input verb. To do this, it needs to scan the entire word in several stages. It starts by assigning the symbols “C” for consonants or “V” for vowels to the morpheme structure character by character to get the CV structure. Then it scans the CV structure character by character to assign some codes (0-2) to a “C” or “V” to get the syllabification structure. Then the input representation is fed to the GEN function in order to produce a candidate set. It then scans the input string syllable by syllable until the last syllable is encountered. Then the result is subjected to the EVAL function, where the output of the GEN function is checked against the two lexicons (root and affixes) and subjected to some constraints to be able to get the right root of the verb. Although TagMA was able to morphologically analyze 96% of the sample verbs accurately (Fortes, 2002), the process of analyzing an input verb is quite long and tedious. It only outputs the root, tense, and its affix for a particular input verb. It does not produce the infinitive or dictionary form of the original input verb. The infinitive form is also useful because that is what one usually uses to lookup a word in a dictionary. In fact, T2CMT (Fat, 2004) that used TagMA wrongly translated the Tagalog word “namatay” (to die) to “pinaagi” (by means of or through) in Cebuano, when in fact, the Cebuano translation should be “namatay” also. If a morphological analyzer produces the infinitive form, the translation would be correct. Our proposed system uses the morphological analyzer developed in (Roxas, 1989), which is being extended to cover more possible verb conjugations. If you pass “namatay” to our morphological analyzer, it will give the infinitive form “mamatay” and tense is past or perpektibo. If you consult a dictionary, you will find that its English translation is “to die,” which is the intended meaning. Therefore, we believe that there is still a need to develop a morphological analyzer that uses the infinitive forms when checking a word in a dictionary. 3.The Proposed Morphological Analyzer The morphological analyzer presented in this paper is just one of the components of our Context-driven Filipino-English Machine Translation System. The analyzer will be used during the translation process. This morphological analyzer examines any Filipino verb in various possible inflections and produces the affix(es), the infinitive form, and the tense of the input verb. It should be pointed out that we have no data as to how many infinitive forms are there in Tagalog. We don’t know of any study trying to count the infinitive forms of a certain language. Natural languages are evolving. So it is difficult to categorically say that a particular language has that number of infinitive forms. The lexicon will have to be updated from time to time as the language evolves. 468  In this research, we don't bother so much on generating the root word because the entries in our lexicon will contain the infinitive or dictionary forms of the verbs and will be accessed using that form. We do, however, recognize the value of getting the root of a word, as being done in TagSA and TagMA, but extracting the root of the word is more useful in Information Retrieval System (IRS) than in a machine translation system. We also understand that the root of a word is used in some systems to get the right semantics of compound words in order to come up with a better translation. This is not necessary in our proposed Context-driven Filipino-English Machine Translation System, of which the morphological analyzer being presented here is just a subsystem. We choose the right English equivalent of a Filipino word by checking the neighboring words in a sentence and even in a paragraph. These neighboring words serve as context of the word being translated. So our lexicon must be a different one than the usual dictionary. The affixes are important because they help in determining the structure of the sentence, the meaning of the entire sentence, etc. The subject of the sentence as well as the object will be easily recognized once the affix has been determined already. For example, if the affix used in the main verb is “UM,” the subject of the sentence starts with the word “ang,” “si” for personal subject, or uses the nominative case of pronoun. The object starts with the word “ng,” “ni” for personal object, or uses the objective case of pronoun. This will be very helpful in the translation itself. As for the tense of the verb that will be output by the analyzer, it is also very useful for the translation process.  Input Verb  Form the  no  infinitive form  of the verb  regular  Check Lexicon  existent  non-existent  Option  yes  exhausted?  no  First time?  yes  Check if verb is irregular irregular Manipulate the irregular verb  Output the infinitive form, its affix(es), and its tense Error  Figure 1: The flow of the morphological analyzer’s activities.  469  4.Extracting Affixes, Infinitive Forms, and Tenses In getting the infinitive form, the tense, and affix(es) of a Filipino verb, it first consults the lexicon if the word is there. If not, it checks whether or not all possible options were exhausted, in which case, it reports an error. If there are still options, it checks whether or not it is the first time to process the verb. If it is, it has to check whether or not the verb is irregular. If it is, it performs some manipulation and prepares for the formation of the possible infinitive form of the verb. If the verb is regular, the analyzer immediately tries to form the possible infinitive form. Figure 1 shows the flow of actions to take as the input verb is being analyzed.  Tense = past  /* default tense, will be changed depending on 
Tree-Local Multi-Component TAGs (called hereafter just MC-TAG for short) are known to be weakly equivalent to standard TAGs, however, they can describe structures not derivable in the standard TAG. There are other variants of MC-TAG, such as MC-TAG with (a) flexible composition and (b) multiple adjoining of modifier (non-predicative) auxiliary trees that are also weakly equivalent to TAGs, but can describe structures not derivable with MC-TAG. Our main goal in this paper is to determine the word order patterns that can be generated in these MC-TAG variants while respecting semantic dependencies in the grammar and derivation. We use some word order phenomena such as scrambling and clitic climbing to illustrate our approach. This is not a study of scrambling or clitic climbing per se. We do not claim that the patterns of dependencies that are derivable are all equally acceptable. Other considerations such as processing will also come into play. However, patterns that are not derivable are predicted to be clearly unacceptable. 
 Flexible composition is an extension of TAG that has been used in a variety of TAG-analyses. In this paper, we present a dedicated study of the formal and linguistic properties of TAGs with ﬂexible composition (TAG-FC). We start by presenting a survey of existing applications of ﬂexible composition. In the main part of the paper, we discuss a formal deﬁnition of TAGFCs and give a proof of equivalence of TAG-FC to tree-local MCTAG, via a formalism called delayed tree-local MCTAG. We then proceed to argue that delayed treelocality is more intuitive for the analysis of many cases where ﬂexible composition has been employed.  
We propose a psycholinguistically motivated version of TAG which is designed to model key properties of human sentence processing, viz., incrementality, connectedness, and prediction. We use ﬁndings from human experiments to motivate an incremental grammar formalism that makes it possible to build fully connected structures on a word-by-word basis. A key idea of the approach is to explicitly model the prediction of upcoming material and the subsequent veriﬁcation and integration processes. We also propose a linking theory that links the predictions of our formalism to experimental data such as reading times, and illustrate how it can capture psycholinguistic results on the processing of either . . . or structures and relative clauses. 
In this paper, we propose a compositional semantics for DP/VP coordination, using Synchronous Tree Adjoining Grammar (STAG). We ﬁrst present a new STAG approach to quantiﬁcation and scope ambiguity, using Generalized Quantiﬁers (GQ). The proposed GQ analysis is then used in our account of DP/VP coordination. 
It is well known that standard TAG cannot deal with certain instances of longdistance scrambling in German (Rambow, 1994). That CCG can deal with many instances of non-local scrambling in languages such as Turkish has previously been observed (e.g. by Hoffman (1995a) and Baldridge (2002)). We show here that CCG can derive German scrambling cases which are problematic for TAG, and give CCG analyses for other German constructions that require more expressive power than TAG provides. Such analyses raise the question of the linguistic signiﬁcance of the TAG-CCG equivalence. We revisit the original equivalence proof, and show that a careful examination of the translation of CCG and TAG into Indexed Grammar reveals that the IG which is strongly equivalent to CCG can generate dependencies which the corresponding IG obtained from an LTAG cannot generate. 
We present a method for deriving an Earley recognizer for multiple context-free grammars with the correct preﬁx property. This is done by representing an MCFG by a Datalog program and applying generalized supplementary magic-sets rewriting. To secure the correct preﬁx property, a simple extra rewriting must be performed before the magic-sets rewriting. The correctness of the method is easy to see, and a straightforward application of the method to tree-adjoining grammars yields a recognizer whose running time is O(n6). 
TT-MCTAG lets one abstract away from the relative order of co-complements in the ﬁnal derived tree, which is more appropriate than classic TAG when dealing with ﬂexible word order in German. In this paper, we present the analyses for sentential complements, i.e., wh-extraction, thatcomplementation and bridging, and we work out the crucial differences between these and respective accounts in XTAG (for English) and V-TAG (for German). 
Two recent extension of the nonassociative Lambek calculus, the LambekGrishin calculus and the multimodal Lambek calculus, are shown to generate class of languages as tree adjoining grammars, using (tree generating) hyperedge replacement grammars as an intermediate step. As a consequence both extensions are mildly context-sensitive formalisms and beneﬁt from polynomial parsing algorithms. 
Recent work has used the synchronous tree-adjoining grammar (STAG) formalism to demonstrate that many of the cases in which syntactic and semantic derivations appeared to be divergent could be handled elegantly through synchronization. This research has provided syntax and semantics for diverse and complex linguistic phenomena. However, certain hard cases push the STAG formalism to its limits, requiring awkward analyses or leaving no clear solution at all. In this paper a new variant of STAG, synchronous vector TAG (SV-TAG), and demonstrate that it has the potential to handle hard cases such as control verbs, relative clauses, and inverse linking, while maintaining the simplicity of previous STAG syntax-semantics analyses. 
In this paper, we introduce a formalization of various elliptical coordination structures within the Multi-Component TAG framework. Numerous authors describe elliptic coordination as parallel constructions where symmetric derivations can be observed from a desired predicate-argument structure analysis. We show that most famous coordinate structures, including zeugma constructions, can be analyzed simply with the addition of a simple synchronous mechanism to the MCTAG framework . 
We examined the multilingual comprehension and learning of cross-serial and embedded constructions in Germanspeaking learners of Dutch using magnetoencephalography (MEG). In several experimental sessions, learners performed a sentence-scene matching task with Dutch sentences including two different verb orders (Dutch or German verb order). The results indicated a larger evoked response for the German order relative to the Dutch order over frontal sensors after three months, but not initially. The response implies that sensitivity to violations of verb order remains plastic into adulthood. 
Nesson and Shieber (2006) argue that the synchronous TAG (STAG) formalism provides an empirically adequate, yet formally restrictive solution to the problem of associating semantic interpretations with TAG derivations. In this paper, I further explore this approach, focusing on the semantics of reﬂexives. I ﬁnd that STAG indeed permits a simple analysis of core cases of reﬂexives. This analysis does not, however, easily extend to contexts in which the reﬂexive and its antecedent are arguments of distinct elementary trees. I consider three possible extensions to the analysis which remedy these difﬁculties. 
 The problem of inferring an agent’s intentions from her spatio-temporal behavior is called mobile intention recognition problem. Using formal grammars we can state these problems as parsing problems. We argue that context-free formalisms are not sufﬁcient for important use cases. We introduce Spatially Constrained Tree-Adjoining Grammars which enrich TAGs with knowledge about the structure of space, and about how space and intentions are connected.  
Minimalist grammars cannot provide adequate descriptions of constructions in which a single ﬁller saturates two mutually independent gaps, as is commonly analyzed to be the case in parasitic gap constructions and other across-the-board extraction phenomena. In this paper, I show how a simple addition to the minimalist grammar formalism allows for a uniﬁed treatment of control and parasitic gap phenomena, and can be restricted in such a way as to account for across-the-board exceptions to the coordinate structure constraint. In the context of standard constraints on movement, the weak generative capacity of the formalism remains unaffected.  ing in the desired ∀x.φ[x]).1 Now, although minimalist grammars can cap- ture naturally various kinds of non-local dependencies in this way, something needs to be added to the system to allow it to account for apparent non-resource-sensitive behaviour. Pursuing the logical formula metaphor introduced above, MGs can deﬁne only (closed) linear formulae, where each variable is bound by a distinct quantiﬁer, and each quantiﬁer binds exactly one variable. However, the phenomena of control and parasitic gaps both involve a single ﬁller being associated with multiple gaps—in other words, the ‘chains’ here are tree-structured (see ﬁg.1).2 Op x x  
In this paper we present a parsing architecture that allows processing of different mildly context-sensitive formalisms, in particular Tree-Adjoining Grammar (TAG), Multi-Component Tree-Adjoining Grammar with Tree Tuples (TT-MCTAG) and simple Range Concatenation Grammar (RCG). Furthermore, for tree-based grammars, the parser computes not only syntactic analyses but also the corresponding semantic representations. 
We present in this paper an initial investigation into the use of a metagrammar for explicitly sharing abstract grammatical specifications for the Vietnamese language. We first introduce the essential syntactic mechanisms of the Vietnamese language. We then show that the basic subcategorization frames of Vietnamese can be compactly represented by classes using the XMG formalism (eXtensible MetaGrammar). Finally, we report on the implementation the first metagrammar producing verbal elementary trees recognizing basic Vietnamese sentences. 
 2 TAG and Semantics  We explore the semantics of conjunction using a neo-Davidsonian semantics expressed in a synchronous grammar. We propose to model conjunction as quantiﬁcation over the set of conjoined entities, and discuss problems that arise in this approach when we have conjoined quantiﬁed noun phrases. 
The derivation trees of a tree adjoining grammar provide a ﬁrst insight into the sentence semantics, and are thus prime targets for generation systems. We deﬁne a formalism, feature-based regular tree grammars, and a translation from feature based tree adjoining grammars into this new formalism. The translation preserves the derivation structures of the original grammar, and accounts for feature uniﬁcation. 
In this paper, we present an STAG analysis of English reﬂexives. In the spirit of Ryant and Schefﬂer (2006) and Kallmeyer and Romero (2007), reﬂexives are represented as a multi-component set in the syntax, with a degenerate auxiliary tree controlling the φ feature agreement between a reﬂexive and its antecedent. On the semantics side, the reﬂexive is a valence-reducing λexpression, identifying two arguments of a single predicate. We then demonstrate that with minimal modiﬁcations, our analysis can be extended to capture raising and ECM cases. Finally, we argue that Condition A of Chomsky’s binding theory can be derived as a consequence of our treatment of reﬂexives. 
This article describes the design of a common syntactic description for the core grammar of a group of related dialects. The common description does not rely on an abstract sub-linguistic structure like a metagrammar: it consists in a single FSLTAG where the actual speciﬁc language is included as one of the attributes in the set of attribute types deﬁned for the features. When the lan attribute is instantiated, the selected subset of the grammar is equivalent to the grammar of one dialect. When it is not, we have a model of a hybrid multidialectal linguistic system. This principle is used for a group of creole languages of the West-Atlantic area, namely the French-based Creoles of Haiti, Guadeloupe, Martinique and French Guiana. 
This paper proposes a method of constructing an accurate probabilistic subcategorization (SCF) lexicon for a lexicalized grammar extracted from a treebank. We employ a latent variable model to smooth co-occurrence probabilities between verbs and SCF types in the extracted lexicalized grammar. We applied our method to a verb SCF lexicon of an HPSG grammar acquired from the Penn Treebank. Experimental results show that probabilistic SCF lexicons obtained by our model achieved a lower test-set perplexity against ones obtained by a naive smoothing model using twice as large training data. 
In this paper, we report experiments that explore learning of syntactic and semantic representations. First, we extend a state-of-the-art statistical parser to produce a richly annotated tree that identiﬁes and labels nodes with semantic role labels as well as syntactic labels. Secondly, we explore rule-based and learning techniques to extract predicate-argument structures from this enriched output. The learning method is competitive with previous single-system proposals for semantic role labelling, yields the best reported precision, and produces a rich output. In combination with other high recall systems it yields an F-measure of 81%. 
 We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees. The formalism allows a rich set of parse-tree features, including PCFGbased features, bigram and trigram dependency features, and surface features. A severe challenge in applying such an approach to full syntactic parsing is the efﬁciency of the parsing algorithms involved. We show that efﬁcient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efﬁcient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.  
Combination of features contributes to a signiﬁcant improvement in accuracy on tasks such as part-of-speech (POS) tagging and text chunking, compared with using atomic features. However, selecting combination of features on learning with large-scale and feature-rich training data requires long training time. We propose a fast boosting-based algorithm for learning rules represented by combination of features. Our algorithm constructs a set of rules by repeating the process to select several rules from a small proportion of candidate rules. The candidate rules are generated from a subset of all the features with a technique similar to beam search. Then we propose POS tagging and text chunking based on our learning algorithm. Our tagger and chunker use candidate POS tags or chunk tags of each word collected from automatically tagged data. We evaluate our methods with English POS tagging and text chunking. The experimental results show that the training time of our algorithm are about 50 times faster than Support Vector Machines with polynomial kernel on the average while maintaining stateof-the-art accuracy and faster classiﬁcation speed. 
This article investigates the effect of a set of linguistically motivated features on argument disambiguation in data-driven dependency parsing of Swedish. We present results from experiments with gold standard features, such as animacy, deﬁniteness and ﬁniteness, as well as corresponding experiments where these features have been acquired automatically and show signiﬁcant improvements both in overall parse results and in the analysis of speciﬁc argument relations, such as subjects, objects and predicatives. 
Detecting the semantic coherence of a document is a challenging task and has several applications such as in text segmentation and categorization. This paper is an attempt to distinguish between a ‘semantically coherent’ true document and a ‘randomly generated’ false document through topic detection in the framework of latent Dirichlet analysis. Based on the premise that a true document contains only a few topics and a false document is made up of many topics, it is asserted that the entropy of the topic distribution will be lower for a true document than that for a false document. This hypothesis is tested on several false document sets generated by various methods and is found to be useful for fake content detection applications. 
This paper investigates, in a ﬁrst stage, some methods for the automatic acquisition of verb-particle constructions (VPCs) taking into account their statistical properties and some regular patterns found in productive combinations of verbs and particles. Given the limited coverage provided by lexical resources, such as dictionaries, and the constantly growing number of VPCs, possible ways of automatically identifying them are crucial for any NLP task that requires some degree of semantic interpretation. In a second stage we also study whether the combination of statistical and linguistic properties can provide some indication of the degree of idiomaticity of a given VPC. The results obtained show that such combination can successfully be used to detect VPCs and distinguish idiomatic from compositional cases. 
Children can determine the meaning of a new word from hearing it used in a familiar context—an ability often referred to as fast mapping. In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data. 
The most accurate unsupervised word segmentation systems that are currently available (Brent, 1999; Venkataraman, 2001; Goldwater, 2007) use a simple unigram model of phonotactics. While this simpliﬁes some of the calculations, it overlooks cues that infant language acquisition researchers have shown to be useful for segmentation (Mattys et al., 1999; Mattys and Jusczyk, 2001). Here we explore the utility of using bigram and trigram phonotactic models by enhancing Brent’s (1999) MBDP-1 algorithm. The results show the improved MBDP-Phon model outperforms other unsupervised word segmentation systems (e.g., Brent, 1999; Venkataraman, 2001; Goldwater, 2007). 
This paper presents an iterative model of knowledge acquisition of gender information associated with word endings in French. Gender knowledge is represented as a set of rules containing exceptions. Our model takes noun-gender pairs as input and constantly maintains a list of rules and exceptions which is both coherent with the input data and minimal with respect to a minimum description length criterion. This model was compared to human data at various ages and showed a good fit. We also compared the kind of rules discovered by the model with rules usually extracted by linguists and found interesting discrepancies. 
A fundamental task in sentence comprehension is to assign semantic roles to sentence constituents. The structure-mapping account proposes that children start with a shallow structural analysis of sentences: children treat the number of nouns in the sentence as a cue to its semantic predicateargument structure, and represent language experience in an abstract format that permits rapid generalization to new verbs. In this paper, we tested the consequences of these representational assumptions via experiments with a system for automatic semantic role labeling (SRL), trained on a sample of child-directed speech. When the SRL was presented with representations of sentence structure consisting simply of an ordered set of nouns, it mimicked experimental ﬁndings with toddlers, including a striking error found in children. Adding features representing the position of the verb increased accuracy and eliminated the error. We show the SRL system can use incremental knowledge gain to switch from error-prone noun order features to a more accurate representation, demonstrating a possible mechanism for this process in child development. 
 We present an incremental Bayesian model for the unsupervised learning of syntactic categories from raw text. The model draws information from the distributional cues of words within an utterance, while explicitly bootstrapping its development on its own partiallylearned knowledge of syntactic categories. Testing our model on actual child-directed data, we demonstrate that it is robust to noise, learns reasonable categories, manages lexical ambiguity, and in general shows learning behaviours similar to those observed in children.  
Raycho Mukelov HULTIG University of Beira Interior raicho@hultig.di.ubi.pt  Guillaume Cleuziou LIFO University of Orléans cleuziou@univ-orleans.pt  Abstract. In this paper, we propose a new methodology based on directed graphs and the TextRank algorithm to automatically induce general-specific noun relations from web corpora frequency counts. Different asymmetric association measures are implemented to build the graphs upon which the TextRank algorithm is applied and produces an ordered list of nouns from the most general to the most specific. Experiments are conducted based on the WordNet noun hierarchy and assess 65.69% of correct word ordering. 
Catchwords refer to those popular words or phrases in a time period. In this paper, we propose a novel approach for automatic extraction of Chinese catchwords. By analyzing features of catchwords, we define three aspects to describe Popular Degree of catchwords. Then we use curve fitting in Time Series Analysis to build Popular Degree Curves of the extracted terms. Finally we give a formula that can calculate Popular Degree values of catchwords and get a ranking list of catchword candidates. Experiments show that the method is effective. 
Pictorial communication systems convert natural language text into pictures to assist people with limited literacy. We deﬁne a novel and challenging problem: picture layout optimization. Given an input sentence, we seek the optimal way to lay out word icons such that the resulting picture best conveys the meaning of the input sentence. To this end, we propose a family of intuitive “ABC” layouts, which organize icons in three groups. We formalize layout optimization as a sequence labeling problem, employing conditional random ﬁelds as our machine learning method. Enabled by novel applications of semantic role labeling and syntactic parsing, our trained model makes layout predictions that agree well with human annotators. In addition, we conduct a user study to compare our ABC layout versus the standard linear layout. The study shows that our semantically enhanced layout is preferred by non-native speakers, suggesting it has the potential to be useful for people with other forms of limited literacy, too. 
We propose a data-driven method for automatically analyzing the morphology of ancient Greek. This method improves on existing ancient Greek analyzers in two ways. First, through the use of a nearestneighbor machine learning framework, the analyzer requires no hand-crafted rules. Second, it is able to predict novel roots, and to rerank its predictions by exploiting a large, unlabelled corpus of ancient Greek. 
In this paper, we present a novel morphology preprocessing technique for ArabicEnglish translation. We exploit the Arabic morphology-English alignment to learn a model removing nonaligned Arabic morphemes. The model is an instance of the Conditional Random Field (Lafferty et al., 2001) model; it deletes a morpheme based on the morpheme’s context. We achieved around two BLEU points improvement over the original Arabic translation for both a travel-domain system trained on 20K sentence pairs and a news domain system trained on 177K sentence pairs, and showed a potential improvement for a large-scale SMT system trained on 5 million sentence pairs. 
Though phrase-based SMT has achieved high translation quality, it still lacks of generalization ability to capture word order differences between languages. In this paper we describe a general method for tree-to-string phrasebased SMT. We study how syntactic transformation is incorporated into phrase-based SMT and its effectiveness. We design syntactic transformation models using unlexicalized form of synchronous context-free grammars. These models can be learned from sourceparsed bitext. Our system can naturally make use of both constituent and non-constituent phrasal translations in the decoding phase. We considered various levels of syntactic analysis ranging from chunking to full parsing. Our experimental results of English-Japanese and English-Vietnamese translation showed a signiﬁcant improvement over two baseline phrase-based SMT systems. 
Previous work in referring expression generation has explored general purpose techniques for attribute selection and surface realization. However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information. In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques. 
This paper describes a system to solve the joint learning of syntactic and semantic dependencies. An directed graphical model is put forward to integrate dependency relation classiﬁcation and semantic role labeling. We present a bilayer directed graph to express probabilistic relationships between syntactic and semantic relations. Maximum Entropy Markov Models are implemented to estimate conditional probability distribution and to do inference. The submitted model yields 76.28% macro-average F1 performance, for the joint task, 85.75% syntactic dependencies LAS and 66.61% semantic dependencies F1. 
This paper proposes a novel method to analyze syntactic dependencies and label semantic dependencies around both the verbal predicates and the nouns. In this method, a probabilistic model is designed to obtain a global optimal result. Moreover, a predicate identification model and a disambiguation model are proposed to label predicates and their senses. The experimental results obtained on the wsj and brown test sets show that our system obtains 77% of labeled macro F1 score for the whole task, 84.47% of labeled attachment score for syntactic dependency task, and 69.45% of labeled F1 score for semantic dependency task. 
The paper presents a computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains. The model is purely lexemebased. The proposed morphological structure consists of (1) binary relations that connect each headword with words that are morphologically related, and especially with the members of its morphological family and its derivational series, and of (2) the analogies that hold between the words. The model has been tested on the lexicon of French using the TLFi machine readable dictionary. 
We argue in favor of using a graph-based representation for language meaning and propose a novel learning method to map natural language text to its graph-based meaning representation. We present a grammar formalism, which combines syntax and semantics, and has ontology constraints at the rule level. These constraints establish links between language expressions and the entities they refer to in the real world. We present a relational learning algorithm that learns these grammars from a small representative set of annotated examples, and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations. 
Meaning cannot be based on dictionary deﬁnitions all the way down: at some point the circularity of deﬁnitions must be broken in some way, by grounding the meanings of certain words in sensorimotor categories learned from experience or shaped by evolution. This is the “symbol grounding problem”. We introduce the concept of a reachable set — a larger vocabulary whose meanings can be learned from a smaller vocabulary through deﬁnition alone, as long as the meanings of the smaller vocabulary are themselves already grounded. We provide simple algorithms to compute reachable sets for any given dictionary. 
In this paper, we provide a statistical machine learning representation of textual entailment via syntactic graphs constituted by tree pairs. We show that the natural way of representing the syntactic relations between text and hypothesis consists in the huge feature space of all possible syntactic tree fragment pairs, which can only be managed using kernel methods. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation. 
This paper presents a method for semantic classication of onomatopoetic words like “ひゅーひゅー (hum)” and “からん ころん (clip clop)” which exist in every language, especially Japanese being rich in onomatopoetic words. We used a graph-based clustering algorithm called N ewman clustering. The algorithm calculates a simple quality function to test whether a particular division is meaningful. The quality function is calculated based on the weights of edges between nodes. We combined two different similarity measures, distributional similarity, and orthographic similarity to calculate weights. The results obtained by using the Web data showed a 9.0% improvement over the baseline single distributional similarity measure. 
Several language processing tasks can be inherently represented by a weighted graph where the weights are interpreted as a measure of relatedness between two vertices. Measuring similarity between arbitary pairs of vertices is essential in solving several language processing problems on these datasets. Random walk based measures perform better than other path based measures like shortest-path. We evaluate several random walk measures and propose a new measure based on commute time. We use the psuedo inverse of the Laplacian to derive estimates for commute times in graphs. Further, we show that this pseudo inverse based measure could be improved by discarding the least signiﬁcant eigenvectors, corresponding to the noise in the graph construction process, using singular value decomposition. 
A common problem for clustering techniques is that clusters overlap, which makes graphing the statistical structure in the data difﬁcult. A related problem is that we often want to see the distribution of factors (variables) as well as classes (objects). Correspondence Analysis (CA) offers a solution to both these problems. The structure that CA discovers may be an important step in representing similarity. We have performed an analysis for Italian verbs and nouns, and conﬁrmed that similar structures are found for English. 
One of the main problems in research on automatic summarization is the inaccurate semantic interpretation of the source. Using speciﬁc domain knowledge can considerably alleviate the problem. In this paper, we introduce an ontology-based extractive method for summarization. It is based on mapping the text to concepts and representing the document and its sentences as graphs. We have applied our approach to summarize biomedical literature, taking advantages of free resources as UMLS. Preliminary empirical results are presented and pending problems are identiﬁed. 
Word association data in dictionary form can be simulated through the combination of three components: a bipartite graph with an imbalance in set sizes; a scale-free graph of the Barabási-Albert model; and a normal distribution connecting the two graphs. Such a model makes it possible to simulate the complex features in degree distributions and the interesting graph clustering results that are typically observed for real data. 
Words play a major role in language production, hence ﬁnding them is of vital importance, be it for speaking or writing. Words are stored in a dictionary, and the general belief holds, the bigger the better. Yet, to be truly useful the resource should contain not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information. Information access depends crucially on the organization of the data (words) and on the navigational tools. It also depends on the grouping, ranking and indexing of the data, a factor too often overlooked. We will present here some preliminary results, showing how an existing electronic dictionary could be enhanced to support language producers to ﬁnd the word they are looking for. To this end we have started to build a corpus-based association matrix, composed of target words and access keys (meaning elements, related concepts/words), the two being connected at their intersection in terms of weight and type of link, information used subsequently for grouping, ranking and navigation. 
This paper describes the functional design of an interface for an online scholarly dictionary of contemporary standard Dutch, the ANW. One of the main innovations of the ANW is a twofold meaning description: definitions are accompanied by ‘semagrams’. In this paper we focus on the strategies that are available for accessing information in the dictionary and the role semagrams play in the dictionary practice. 
ProPOSEL is a prosody and PoS English lexicon, purpose-built to integrate and leverage domain knowledge from several well-established lexical resources for machine learning and NLP applications. The lexicon of 104049 separate entries is in accessible text file format, is human and machine-readable, and is intended for open source distribution with the Natural Language ToolKit. It is therefore supported by Python software tools which transform ProPOSEL into a Python dictionary or associative array of linguistic concepts mapped to compound lookup keys. Users can also conduct searches on a subset of the lexicon and access entries by word class, phonetic transcription, syllable count and lexical stress pattern. ProPOSEL caters for a range of different cognitive aspects of the lexicon©. 
When consulting a dictionary, people can find the meaning of a word via the definition, which usually contains the relevant information to fulfil their requirement. Lexicographers produce dictionaries and their work consists in presenting information essential for grasping the meaning of words. However, when people need to find a word it is likely that they do not obtain the information they are looking for. There is a gap between dictionary definitions and the information being available in peoples’ mind. This paper attempts to present the conceptualisation people engage in, in order to arrive at a word from its meaning. The insights of an experiment conducted show us the differences between the knowledge available in peoples’ minds and in dictionary definitions. 
In this paper we propose a model for conceptual access to multilingual lexicon based on shared orthography. Our proposal relies crucially on two facts: That both Chinese and Japanese conventionally use Chinese orthography in their respective writing systems, and that the Chinese orthography is anchored on a system of radical parts which encodes basic concepts. Each orthographic unit, called hanzi and kanji respectively, contains a radical which indicates the broad semantic class of the meaning of that unit. Our study utilizes the homomorphism between the Chinese hanzi and Japanese kanji systems to ide1ntify bilingual word correspondences. We use bilingual dictionaries, including WordNet, to verify semantic relation between the crosslingual pairs. These bilingual pairs are then mapped to an ontology constructed based on relations to the relation between the meaning of each character and the © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved.  Ya-Min Chou Ming Chuan University 250 Zhong Shan N. Rd., Sec. 5, Taipei 111, Taiwan milesymchou@yahoo.com.tw Sheng-Yi Chen Institute of Linguistics, Academia Sinica Nanking, Taipei, Taiwan 115 eagles@gate.sinica.edu.tw basic concept of their radical parts. The conceptual structure of the radical ontology is proposed as a model for simultaneous conceptual access to both languages. A study based on words containing characters composed of the “口(mouth)” radical is given to illustrate the proposal and the actual model. The fact that this model works for two typologically very different languages and that the model contains generative lexicon like coersive links suggests that this model has the conceptual robustness to be applied to other languages. 
This paper aims to introduce a new parsing strategy for large dictionary (thesauri) parsing, called Dictionary Sense Segmentation & Dependency (DSSD), devoted to obtain the sense tree, i.e. the hierarchy of the defined meanings, for a dictionary entry. The real novelty of the proposed approach is that, contrary to dictionary ‘standard’ parsing, DSSD looks for and succeeds to separate the two essential processes within a dictionary entry parsing: sense tree construction and sense definition parsing. The key tools to accomplish the task of (autonomous) sense tree building consist in defining the dictionary sense marker classes, establishing a tree-like hierarchy of these classes, and using a proper searching procedure of sense markers within the DSSD parsing algorithm. A similar but more general approach, using the same techniques and data structures for (Romanian) free text parsing is SCD (Segmentation-CohesionDependency) (Curteanu; 1988, 2006), which DSSD is inspired from. A DSSDbased parser is implemented in Java, building currently 91% correct sense trees from DTLR (Dicţionarul Tezaur al © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved.  Limbii Române – Romanian Language Thesaurus) entries, with significant resources to improve and enlarge the DTLR lexical semantics analysis. 
ETAP-3 is a system of machine translation consisting of various types of rules and dictionaries. Those dictionaries, being created especially for NLP system, provide for every lexeme not only data about its characteristics as a separate item, but also different types of information about its syntactic and semantic links to other lexemes. The paper shows how the information about certain types of semantic links between lexemes represented in the dictionaries can be used in a machine translation system. The paper deals with correspondences between lexicalfunctional constructions of different types in the Russian and the English languages. Lexical-functional construction is a word-combination consisting of an argument of a lexical function and a value of this lexical function for this argument. The paper describes the cases when a lexical functional construction in one of these languages corresponds to a lexicalfunctional construction in the other language, but lexical functions represented by these two constructions are different. The paper lists different types of correspondences and gives the reasons for their existence. It also shows how the information about these correspondences can be used to improve the work of the linguistic component of the machine translation system ETAP-3.  
In this paper we aim to detect some aspects of adjectival meanings. Concepts of adjectives are distributed by SOM (SelfOrganizing map) whose feature vectors are calculated by MI (Mutual Information). For the SOM obtained, we make tight clusters from map nodes, calculated by cosine. In addition, the number of tight clusters obtained by cosine was increased using map nodes and Japanese thesaurus. As a result, the number of extended clusters of concepts was 149 clusters. From the map, we found 8 adjectival clusters in super-ordinate level and some tendencies of similar and dissimilar clusters. 
Rephrasing text spans is a common task when revising a text. However, traditional dictionaries often cannot provide direct assistance to writers in performing this task. In this article, we describe an approach to obtain a monolingual phrase lexicon using techniques used in Statistical Machine Translation. A part to be rephrased is ﬁrst translated into a pivot language, and then translated back into the original language. Models for assessing ﬂuency, meaning preservation and lexical divergence are used to rank possible rephrasings, and their relative weight can be tuned by the user so as to better address her needs. An evaluation shows that these models can be used successfully to select rephrasings that are likely to be useful to a writer. 
We compare a psycholinguistic approach of mental lexicon organization with a computational approach of implicit lexical organization as found in dictionaries. In this work, we associate dictionaries with ’small world’ graphs. This multidisciplinary approach aims at showing that implicit structure of dictionaries, mathematically identiﬁed, ﬁts the way young children categorize. These dictionary graphs might therefore be considered as ’cognitive artifacts’. This shows the importance of semantic proximity both in cognitive and computational organization of verbs lexicon. 
Providing sets of semantically related words in the lexical entries of an electronic dictionary should help language learners quickly understand the meaning of the target words. Relational information might also improve memorisation, by allowing the generation of structured vocabulary study lists. However, an open issue is which semantic relations are cognitively most salient, and should therefore be used for dictionary construction. In this paper, we present a concept description elicitation experiment conducted with German and Italian speakers. The analysis of the experimental data suggests that there is a small set of concept-class–dependent relation types that are stable across languages and robust enough to allow discrimination across broad concept domains. Our further research will focus on harvesting instantiations of these classes from corpora. 
 OBSERVED RESPONSE hot ice warm water freeze wet feet freezing nose room sneeze sore winter  NUMBER OF SUBJECTS 34 10 7 5 3 3 2 2 2 2 2 2 2  PREDICTED NUMBER OF  RESPONSE SUBJECTS  hot  34  winter  2  weather  0  warm  7  water  5  heat  
Question Answering (QA) systems are often built modularly, with a text retrieval component feeding forward into an answer extraction component. Conventional wisdom suggests that, the higher the quality of the retrieval results used as input to the answer extraction module, the better the extracted answers, and hence system accuracy, will be. This turns out to be a poor assumption, because text retrieval and answer extraction are tightly coupled. Improvements in retrieval quality can be lost at the answer extraction module, which can not necessarily recognize the additional answer candidates provided by improved retrieval. Going forward, to improve accuracy on the QA task, systems will need greater coordination between text retrieval and answer extraction modules. 
Question answering (QA) is the task of ﬁnding a concise answer to a natural language question. The ﬁrst stage of QA involves information retrieval. Therefore, performance of an information retrieval subsystem serves as an upper bound for the performance of a QA system. In this work we use phrases automatically identiﬁed from questions as exact match constituents to search queries. Our results show an improvement over baseline on several document and sentence retrieval measures on the WEB dataset. We get a 20% relative improvement in MRR for sentence extraction on the WEB dataset when using automatically generated phrases and a further 9.5% relative improvement when using manually annotated phrases. Surprisingly, a separate experiment on the indexed AQUAINT dataset showed no effect on IR performance of using exact phrases. 
Passage retrieval is used in QA to ﬁlter large document collections in order to ﬁnd text units relevant for answering given questions. In our QA system we apply standard IR techniques and index-time passaging in the retrieval component. In this paper we investigate several ways of dividing documents into passages. In particular we look at semantically motivated approaches (using coreference chains and discourse clues) compared with simple window-based techniques. We evaluate retrieval performance and the overall QA performance in order to study the impact of the different segmentation approaches. From our experiments we can conclude that the simple techniques using ﬁxedsized windows clearly outperform the semantically motivated approaches, which indicates that uniformity in size seems to be more important than semantic coherence in our setup. 
The information retrieval (IR) community has investigated many different techniques to retrieve passages from large collections of documents for question answering (QA). In this paper, we speciﬁcally examine and quantitatively compare the impact of passage retrieval for QA using sliding windows and disjoint windows. We consider two different data sets, the TREC 2002–2003 QA data set, and 93 whyquestions against INEX Wikipedia. We discovered that, compared to disjoint windows, using sliding windows results in improved performance of TREC-QA in terms of TDRR, and in improved performance of why-QA in terms of success@n and MRR. 
Automated answering of natural language questions is an interesting and useful problem to solve. Question answering (QA) systems often perform information retrieval at an initial stage. Information retrieval (IR) performance, provided by engines such as Lucene, places a bound on overall system performance. For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions. In this paper, answer texts from previous QA evaluations held as part of the Text REtrieval Conferences (TREC) are paired with queries and analysed in an attempt to identify performance-enhancing words. These words are then used to evaluate the performance of a query expansion method. Data driven extension words were found to help in over 70% of difﬁcult questions. These words can be used to improve and evaluate query expansion methods. Simple blind relevance feedback (RF) was correctly predicted as unlikely to help overall performance, and an possible explanation is provided for its low value in IR for QA. 
In this paper,an information distance based approach is proposed to perform answer validation for question answering system. To validate an answer candidate, the approach calculates the conditional information distance between the question focus and the candidate under certain condition pattern set. Heuristic methods are designed to extract question focus and generate proper condition patterns from question. General search engines are employed to estimate the Kolmogorov complexity, hence the information distance. Experimental results show that our approach is stable and ﬂexible, and outperforms traditional tﬁdf methods. 
In this paper we investigate the use of several types of lexico-semantic information for query expansion in the passage retrieval component of our QA system. We have used four corpus-based methods to acquire semantically related words, and we have used one hand-built resource. We evaluate our techniques on the Dutch CLEF QA track.1 In our experiments expansions that try to bridge the terminological gap between question and document collection do not result in any improvements. However, expansions bridging the knowledge gap show modest improvements. 
Having gold standards allows us to evaluate new methods and approaches against a common benchmark. In this paper we describe a set of gold standard question reformulations and associated reformulation guidelines that we have created to support research into automatic interpretation of questions in TREC question series, where questions may refer anaphorically to the target of the series or to answers to previous questions. We also assess various string comparison metrics for their utility as evaluation measures of the proximity of an automated system’s reformulations to the gold standard. Finally we show how we have used this approach to assess the question processing capability of our own QA system and to pinpoint areas for improvement. 
The method of Topic Indexing and Retrieval for QA persented in this paper enables fast and efﬁcent QA for questions with named entity answers. This is achieved by identifying all possible named entity answers in a corpus off-line and gathering all possible evidence for their direct retrieval as answer candidates using standard IR techniques. An evaluation of this method on 377 TREC questions produced a score of 0.342 in Accuracy and 0.413 in Mean Reciprocal Rank (MRR). 
Semantic Role Labeling (SRL) has been used successfully in several stages of automated Question Answering (QA) systems but its inherent slow procedures make it difﬁcult to use at the indexing stage of the document retrieval component. In this paper we conﬁrm the intuition that SRL at indexing stage improves the performance of QA and propose a simpliﬁed technique named the Question Prediction Language Model (QPLM), which provides similar information with a much lower cost. The methods were tested on four different QA systems and the results suggest that QPLM can be used as a good compromise between speed and accuracy. 
In this paper, we present an open-source parsing environment (Tu¨bingen Linguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar (RCG) as a pivot formalism, thus opening the way to the parsing of several mildly context-sensitive formalisms. This environment currently supports tree-based grammars (namely Tree-Adjoining Grammars (TAG) and Multi-Component TreeAdjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not only of syntactic structures, but also of the corresponding semantic representations. It is used for the development of a tree-based grammar for German. 
We present an overview of the development environment for Regulus, an Open Source platform for construction of grammar-based speech-enabled systems, focussing on recent work whose goal has been to introduce uniformity between text and speech views of Regulus-based applications. We argue the advantages of being able to switch quickly between text and speech modalities in interactive and ofﬂine testing, and describe how the new functionalities enable rapid prototyping of spoken dialogue systems and speech translators. 
This paper describes a more precise analysis of punctuation for a bi-directional, broad coverage English grammar extracted from the CCGbank (Hockenmaier and Steedman, 2007). We discuss various approaches which have been proposed in the literature to constrain overgeneration with punctuation, and illustrate how aspects of Briscoe’s (1994) inﬂuential approach, which relies on syntactic features to constrain the appearance of balanced and unbalanced commas and dashes to appropriate sentential contexts, is unattractive for CCG. As an interim solution to constrain overgeneration, we propose a rule-based ﬁlter which bars illicit sequences of punctuation and cases of improperly unbalanced apposition. Using the OpenCCG toolkit, we demonstrate that our punctuation-augmented grammar yields substantial increases in surface realization coverage and quality, helping to achieve state-of-the-art BLEU scores. 
Grammar development makes up a large part of the multilingual rule-based application development cycle. One way to decrease the required grammar development efforts is to base the systems on multilingual grammar resources. This paper presents a detailed description of a parametrization mechanism used for building multilingual grammar rules. We show how these rules, which had originally been designed and developed for typologically different languages (English, Japanese and Finnish) are applied to a new language (Greek). The developed shared grammar system has been implemented for a domain speciﬁc speech-to-speech translation application. A majority of these rules (54%) are shared amongst the four languages, 75% of the rules are shared for at least two languages. The main beneﬁt of the described approach is shorter development cycles for new system languages. 
In this paper we present a method for greatly reducing parse times in LFG parsing, while at the same time maintaining parse accuracy. We evaluate the methodology on data from English, German and Norwegian and show that the same patterns hold across languages. We achieve a speedup of 67% on the English data and 49% on the German data. On a small amount of data for Norwegian, we achieve a speedup of 40%, although with more training data we expect this ﬁgure to increase. 
The paper presents a code for enumerating verb-construction templates, from which lexical type inventories of computational grammars can be derived, and test suites can be systematically developed. The templates also serve for descriptive and typological research. The code is string-based, with divisions into slots providing modularity and flexibility of specification. 
In complex grammar-based systems, even small changes may have an unforeseeable impact on overall system performance. Regression testing of the system and its components becomes crucial for the grammar engineers developing the system. As part of this regression testing, the testsuites themselves must be designed to accurately assess coverage and progress and to help rapidly identify problems. We describe a system of passage-query pairs divided into three types of phenomenon-based testsuites (sanity, query, basic correct). These allow for rapid development and for speciﬁc coverage assessment. In addition, real-world testsuites allow for overall performance and coverage assessment. These testsuites are used in conjunction with the more traditional representation-based regression testsuites used by grammar engineers. 
In this paper we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and at the same time ensuring maintainability and re-usability of deep lexicalised grammars. Using the error mining techniques proposed in (van Noord, 2004) we show very convincingly that the main hindrance to portability of deep lexicalised grammars to domains other than the ones originally developed in, as well as to robustness of systems using such grammars is low lexical coverage. To this effect, we develop linguistically-driven methods that use detailed morphosyntactic information to automatically enhance the performance of deep lexicalised grammars maintaining at the same time their usually already achieved high linguistic quality. 
This paper presents a CRF (Conditional Random Field) model for Semantic Chunk Annotation in a Chinese Question and Answering System (SCACQA). The model was derived from a corpus of real world questions, which are collected from some discussion groups on the Internet. The questions are supposed to be answered by other people, so some of the questions are very complex. Mutual information was adopted for feature selection. The training data collection consists of 14000 sentences and the testing data collection consists of 4000 sentences. The result shows an F-score of 93.07%. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved.  
 2 What is a complement-taking noun?  It is important to identify complementtaking nouns in order to properly analyze the grammatical and implicative structure of the sentence. This paper examines the ways in which these nouns were identiﬁed and classiﬁed for addition to the BRIDGE natural language understanding system. 
In a realistic Interactive Question Answering (IQA) setting, users frequently ask follow-up questions. By modeling how the questions’ focus evolves in IQA dialogues, we want to describe what makes a particular follow-up question salient. We introduce a new focus model, and describe an implementation of an IQA system that we use for exploring our theory. To learn properties of salient focus transitions from data, we use logistic regression models that we validate on the basis of predicted answer correctness. 
Two of the current issues of Question Answering (QA) systems are the lack of personalization to the individual users’ needs, and the lack of interactivity by which at the end of each Q/A session the context of interaction is lost. We address these issues by designing and implementing a model of personalized, interactive QA based on a User Modelling component and on a conversational interface. Our evaluation with respect to a baseline QA system yields encouraging results in both personalization and interactivity. 
antonin follet@hotmail.fr  Patrick Saint-Dizier IRIT - CNRS, 118 route de Narbonne, 31062 Toulouse Cedex, France. stdizier@irit.fr  Abstract In this paper, we present the explanation structure of procedural texts, that supports and motivates the goal-instruction structure. We focus in particular on arguments, and show how arguments of type warnings and advices can be extracted. Finally, we show how a domain dependent knowhow textual knowledge base can be constructed and queried. 
The concept classiﬁer has been used as a translation unit in speech-to-speech translation systems. However, the sparsity of the training data is the bottle neck of its effectiveness. Here, a new method based on using a statistical machine translation system has been introduced to mitigate the effects of data sparsity for training classiﬁers. Also, the effects of the background model which is necessary to compensate the above problem, is investigated. Experimental evaluation in the context of crosslingual doctor-patient interaction application show the superiority of the proposed method. 
Grammatical Framework (GF) is a grammar formalism which supports interlinguabased translation, library-based grammar engineering, and compilation to speech recognition grammars. We show how these features can be used in the construction of portable high-precision domain-speciﬁc speech translators. 
This paper proposes a novel integrated dialog simulation technique for evaluating spoken dialog systems. Many techniques for simulating users and errors have been proposed for use in improving and evaluating spoken dialog systems, but most of them are not easily applied to various dialog systems or domains because some are limited to speciﬁc domains or others require heuristic rules. In this paper, we propose a highly-portable technique for simulating user intention, utterance and Automatic Speech Recognition (ASR) channels. This technique can be used to rapidly build a dialog simulation system for evaluating spoken dialog systems. We propose a novel user intention modeling and generating method that uses a linear-chain conditional random ﬁeld, a data-driven domain speciﬁc user utterance simulation method, and a novel ASR channel simulation method with adjustable error recognition rates. Experiments using these techniques were carried out to evaluate the performance and behavior of previously developed dialog systems designed for navigation dialogs, and it turned out that our approach is easy to set up and shows the similar tendencies of real users. 
Voice over IP and the open source technologies are becoming popular choices for organizations. However, while accessing the VoiceXML gateways these systems fail to attract the global users economically. The objective of this paper is to demonstrate how an existing web application can be modified using VoiceXML to enable non-visual access from any phone. Moreover, we unleash a way for linking an existing PSTN-based phone line to a VoiceXML gateway even though the voice service provider (VSP) does not provide a local geographical number to global customers to access the application. In addition, we introduce an economical way for small sized businesses to overcome the high cost of setting up and using a commercial VoiceXML gateway. The method is based on Asterisk server. In order to elucidate the entire process, we present a sample Package Tracking System application, which is based on an existing website and provides the same functionality as the website does. We also present an online demonstration, which provides global access to commercial voice platforms (i.e. Voxeo, Tellme Studio, Bevocal and DemandVoice). This paper also discusses various scenarios in which spoken interaction can play a significant role. 
As spoken dialogue systems move beyond task oriented dialogues and become distributed in the pervasive computing environments, their growing complexity calls for more modular structures. When different aspects of a single system can be accessed with different interfaces, knowledge representation and separation of low level interaction modeling from high level reasoning on domain level becomes important. In this paper, a model utilizing a dialogue plan to communicate information from domain level planner to dialogue management and from there to a separate mobile interface is presented. The model enables each part of the system handle the same information from their own perspectives without containing overlapping logic. 
This paper discusses language understanding in the Maryland Virtual Patient environment. Language understanding is just one of many cognitive functions of the virtual patients in MVP, others including decision making about healthcare and lifestyle, and the experiencing and remembering of interoceptive events. 
 Spoken Language Translation systems have usually been produced for such specific domains as health care or military use. Ideally, such systems would be easily portable to other domains in which translation is mission critical, such as emergency response or law enforcement. However, porting has in practice proven difficult. This paper will comment on the sources of this difficulty and briefly present an approach to rapid inter-domain portability. Three aspects will be discussed: (1) large general-purpose lexicons for automatic speech recognition and machine translation, made reliable and usable through interactive facilities for monitoring and correcting errors; (2) easily modifiable facilities for instant translation of frequent phrases; and (3) quickly modifiable custom glossaries. As support for our approach, we apply our current SLT system, now optimized for the health care domain, to sample utterances from the military, emergency service, and law enforcement domains, with discussion of numerous specific sentences.  
 We describe Ayudame, a system designed to recognize and translate Spanish emergency calls for better dispatching. We analyze the research challenges in adapting speech translation technology to 9-1-1 domain. We report our initial research in 9-1-1 translation system design, ASR experiments, and utterance classifi- cation for translation.  
S-MINDS is a speech translation system, which allows an English speaker to communicate with a limited English proficiency speaker easily within a question-and-answer, interview-style format. It can handle dialogs in specific settings such as nurse-patient interaction, or medical triage. We have built and tested an English-Spanish system for enabling nurse-patient interaction in a number of domains in Kaiser Permanente achieving a total translation accuracy of 92.8% (for both English and Spanish). We will give an overview of the system as well as the quantitative and qualitatively system performance. 
We report on research on matching names in different scripts across languages. We explore two trainable approaches based on comparing pronunciations. The first, a cross-lingual approach, uses an automatic name-matching program that exploits rules based on phonological comparisons of the two languages carried out by humans. The second, monolingual approach, relies only on automatic comparison of the phonological representations of each pair. Alignments produced by each approach are fed to a machine learning algorithm. Results show that the monolingual approach results in machine-learning based comparison of person-names in English and Chinese at an accuracy of over 97.0 F-measure. 
Hallå Norden is a web site with information regarding mobility between the Nordic countries in five different languages; Swedish, Danish, Norwegian, Icelandic and Finnish. We wanted to create a Nordic cross-language dictionary for the use in a cross-language search engine for Hallå Norden. The entire set of texts on the web site was treated as one multilingual parallel corpus. From this we extracted parallel corpora for each language pair. The corpora were very sparse, containing on average less than 80 000 words per language pair. We have used the Uplug word alignment system (Tiedemann 2003a), for the creation of the dictionaries. The results gave on average 213 new dictionary words (frequency > 3) per language pair. The average error rate was 16 percent. Different combinations with Finnish had a higher error rate, 33 percent, whereas the error rate for the remaining language pairs only yielded on average 9 percent errors. The high error rate for Finnish is possibly due to the fact that the Finnish language belongs to a different language family. Although the corpora were very sparse the word alignment results for the combinations of Swedish, Danish, Norwegian and Icelandic were surprisingly good compared to other experiments with larger corpora. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved.  
In this paper, we introduce and compare between two novel approaches, supervised and unsupervised, for identifying the keywords to be used in extractive summarization of text documents. Both our approaches are based on the graph-based syntactic representation of text and web documents, which enhances the traditional vector-space model by taking into account some structural document features. In the supervised approach, we train classiﬁcation algorithms on a summarized collection of documents with the purpose of inducing a keyword identiﬁcation model. In the unsupervised approach, we run the HITS algorithm on document graphs under the assumption that the top-ranked nodes should represent the document keywords. Our experiments on a collection of benchmark summaries show that given a set of summarized training documents, the supervised classiﬁcation provides the highest keyword identiﬁcation accuracy, while the highest F-measure is reached with a simple degree-based ranking. In addition, it is sufﬁcient to perform only the ﬁrst iteration of HITS rather than running it to its convergence. 
This paper describes a generic, opendomain multi-document summarisation system which combines new and existing techniques in a novel way. The system is capable of automatically identifying query-related online documents and compiling a report from the most useful sources, whilst presenting the result in such a way as to make it easy for the researcher to look up the information in its original context. 
Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disﬂuencies, or difﬁculties in the accurate identiﬁcation of sentence boundaries. We propose the inclusion of related, solid background information to cope with the difﬁculties of summarizing spoken language and the use of multi-document summarization techniques in single document speechto-text summarization. In this work, we explore the possibilities offered by phonetic information to select the background information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information. Results show that summaries generated using this approach are considerably better than those produced by an up-to-date latent semantic analysis (LSA) summarization method and suggest that humans prefer summaries restricted to the information conveyed in the input source. 
This paper reports an initial study that aims to assess the viability of a state-of-the-art multi-document summarizer for automatic captioning of geo-referenced images. The automatic captioning procedure requires summarizing multiple web documents that contain information related to images’ location. We use SUMMA (Saggion and Gaizauskas, 2005) to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics (Lin, 2004) relative to human generated summaries. Results show that, even though query-based summaries perform better than generic ones, they are still not selecting the information that human participants do. In particular, the areas of interest that human summaries display (history, travel information, etc.) are not contained in the query-based summaries. For our future work in automatic image captioning this result suggests that developing the query-based summarizer further and biasing it to account for user-speciﬁc requirements will prove worthwhile. 
The Europe Media Monitor system (EMM) gathers and aggregates an average of 50,000 newspaper articles per day in over 40 languages. To manage the information overflow, it was decided to group similar articles per day and per language into clusters and to link daily clusters over time into stories. A story automatically comes into existence when related groups of articles occur within a 7-day window. While cross-lingual links across 19 languages for individual news clusters have been displayed since 2004 as part of a freely accessible online application (http://press.jrc.it/NewsExplorer), the newest development is work on linking entire stories across languages. The evaluation of the monolingual aggregation of historical clusters into stories and of the linking of stories across languages yielded mostly satisfying results. 
In a large-scale project to list bibliographical references to all of the ca 7 000 languages of the world, the need arises to automatically annotated the bibliographical entries with ISO-639-3 language identiﬁers. The task can be seen as a special case of a more general Information Extraction problem: to classify short text snippets in various languages into a large number of classes. We will explore supervised and unsupervised approaches motivated by distributional characterists of the speciﬁc domain and availability of data sets. In all cases, we make use of a database with language names and identiﬁers. The suggested methods are rigorously evaluated on a fresh representative data set. 
This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could beneﬁt from automatic text understanding. For such purposes, we argue that dependency schemes must follow a simple design and provide semantically contentful information, as well as offer an automatic procedure to extract the relations. We consider the underlying design principles of the Stanford scheme from this perspective, and compare it to the GR and PARC representations. Finally, we address the question of the suitability of the Stanford scheme for parser evaluation. 
We investigate auxiliary distributions (Johnson and Riezler, 2000) for domain adaptation of a supervised parsing system of Dutch. To overcome the limited target domain training data, we exploit an original and larger out-of-domain model as auxiliary distribution. However, our empirical results exhibit that the auxiliary distribution does not help: even when very little target training data is available the incorporation of the out-of-domain model does not contribute to parsing accuracy on the target domain; instead, better results are achieved either without adaptation or by simple model combination. 
In the area of parser evaluation, formats like GR and SD which are based on dependencies, the simplest representation of syntactic information, are proposed as framework-independent metrics for parser evaluation. The assumption behind these proposals is that the simplicity of dependencies would make conversion from syntactic structures and semantic representations used in other formalisms to GR/SD a easy job. But (Miyao et al., 2007) reports that even conversion between these two formats is not easy at all. Not to mention that the 80% success rate of conversion is not meaningful for parsers that boast 90% accuracy. In this paper, we make an attempt at evaluation across frameworks without format conversion. This is achieved by generating a list of names of phenomena with each parse. These names of phenomena are matched against the phenomena given in the gold standard. The number of matches found is used for evaluating the parser that produces the parses. The evaluation method is more effective than evaluation methods which involve format conversion because the generation of names of phenomena from the output of a parser loaded is done by a recognizer that has a 100% success rate of recognizing a phenomenon illustrated by a sentence. The success rate is made possible by the reuse of native codes: codes c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.  used for writing the parser and rules of the grammar loaded into the parser. 
This article presents the methodology of the PASSAGE project, aiming at syntactically annotating large corpora by composing annotations. It introduces the annotation format and the syntactic annotation speciﬁcations. It describes an important component of the methodolgy, namely an WEB-based evaluation service, deployed in the context of the ﬁrst PASSAGE parser evaluation campaign. 
In this paper, we evaluate ﬁve distinct systems of labelled grammatical dependency against the kind of input we require for semantic interpretation, in particular for the deep semantic interpreter underlying a tutorial dialogue system. We focus on the following linguistic phenomena: passive, control and raising, noun modiﬁers, and meaningful vs. non-meaningful prepositions. We conclude that no one system provides all the features that we require, although each such feature is contained within at least one of the competing systems. 
We address the problem of distinguishing between two sources of disagreement in annotations: genuine subjectivity and slip of attention. The latter is especially likely when the classiﬁcation task has a default class, as in tasks where annotators need to ﬁnd instances of the phenomenon of interest, such as in a metaphor detection task discussed here. We apply and extend a data analysis technique proposed by Beigman Klebanov and Shamir (2006) to ﬁrst distill reliably deliberate (non-chance) annotations and then to estimate the amount of attention slips vs genuine disagreement in the reliably deliberate annotations. 
Many interesting phenomena in conversation can only be annotated as a subjective task, requiring interpretative judgements from annotators. This leads to data which is annotated with lower levels of agreement not only due to errors in the annotation, but also due to the differences in how annotators interpret conversations. This paper constitutes an attempt to ﬁnd out how subjective annotations with a low level of agreement can proﬁtably be used for machine learning purposes. We analyse the (dis)agreements between annotators for two different cases in a multimodal annotated corpus and explicitly relate the results to the way machinelearning algorithms perform on the annotated data. Finally we present two new concepts, namely ‘subjective entity’ classiﬁers resp. ‘consensus objective’ classiﬁers, and give recommendations for using subjective data in machine-learning applications. 
The relevance of human judgment in an evaluation campaign is illustrated here through the DEFT text mining campaigns. In a ﬁrst step, testing a topic for a campaign among a limited number of human evaluators informs us about the feasibility of a task. This information comes from the results obtained by the judges, as well as from their personal impressions after passing the test. In a second step, results from individual judges, as well as their pairwise matching, are used in order to adjust the task (choice of a marking scale for DEFT’07 and selection of topical categories for DEFT’08). Finally, the mutual comparison of competitors’ results, at the end of the evaluation campaign, conﬁrms the choices we made at its starting point, and provides means to redeﬁne the task when we shall launch a future campaign based on the same topic. 
Evaluation and annotation are two of the greatest challenges in developing NLP instructional or diagnostic tools to mark grammar and usage errors in the writing of non-native speakers. Past approaches have commonly used only one rater to annotate a corpus of learner errors to compare to system output. In this paper, we show how using only one rater can skew system evaluation and then we present a sampling approach that makes it possible to evaluate a system more efﬁciently. 
Sense inventories for polysemous predicates are often comprised by a number of related senses. In this paper, we examine different types of relations within sense inventories and give a qualitative analysis of the effects they have on decisions made by the annotators and annotator error. We also discuss some common traps and pitfalls in design of sense inventories. We use the data set developed speciﬁcally for the task of annotating sense distinctions dependent predominantly on semantics of the arguments and only to a lesser extent on syntactic frame. 
There has been extensive work on eliciting human judgements on the sentiment of words and the resulting annotated word lists have frequently been used for opinion mining applications in Natural Language Processing (NLP). However, this word-based approach does not take different senses of a word into account, which might differ in whether and what kind of sentiment they evoke. In this paper, we therefore introduce a human annotation scheme for judging both the subjectivity and polarity of word senses. We show that the scheme is overall reliable, making this a well-deﬁned task for automatic processing. We also discuss three issues that surfaced during annotation: the role of annotation bias, hierarchical annotation (or underspeciﬁcation) and bias in the sense inventory used. 
We have built a parallel treebank that includes word and phrase alignment. The alignment information was manually checked using a graphical tool that allows the annotator to view a pair of trees from parallel sentences. We found the compilation of clear alignment guidelines to be a difﬁcult task. However, experiments with a group of students have shown that we are on the right track with up to 89% overlap between the student annotation and our own. At the same time these experiments have helped us to pin-point the weaknesses in the guidelines, many of which concerned unclear rules related to differences in grammatical forms between the languages. 
An affective text may be judged to belong to multiple affect categories as it may evoke different affects with varying degree of intensity. For affect classiﬁcation of text, it is often required to annotate text corpus with affect categories. This task is often performed by a number of human judges. This paper presents a new agreement measure inspired by Kappa coefﬁcient to compute inter-annotator reliability when the annotators have freedom to categorize a text into more than one class. The extended reliability coefﬁcient has been applied to measure the quality of an affective text corpus. An analysis of the factors that inﬂuence corpus quality has been provided. 
 1.1 Structure of the Review  We review psycholinguistic research on the use of intonation in dialogue, focusing on our own recent work. In experiments using complex real-world tasks and naïve speakers and listeners, we show that speakers reliably specific prosodic cues to signal their intensions, and that listeners use these cues to recognize syntactic and pragmatic aspects of discourse meaning. 
It is an honor to have this chance to tie together themes from my recent research, and to sketch some challenges and opportunities for NLG in face-to-face conversational interaction. Communication reﬂects our general involvement in one anothers’ lives. Through the choices we manifest with one another, we share our thoughts and feelings, strengthen our relationships and further our joint projects. We rely not only on words to articulate our perspectives, but also on a heterogeneous array of accompanying efforts: embodied deixis, expressive movement, presentation of iconic imagery and instrumental action in the world. Words showcase the distinctive linguistic knowledge which human communication exploits. But people’s diverse choices in conversation in fact come together to reveal multifaceted, interrelated meanings, in which all our actions, verbal and nonverbal, ﬁt the situation and further social purposes. In the best case, they let interlocutors understand not just each other’s words, but each other. As NLG researchers, I argue, we have good reason to work towards models of social cognition that embrace the breadth of conversation. Scientiﬁcally, it connects us to an emerging consensus in favor of a general human pragmatic competence, rooted in capacities for engagement, coordination, shared intentionality and extended relationships. Technically, it lets us position ourselves as part of an emerging revolution in integrative Artiﬁcial Intelligence, characterized by research challenges like human–robot interaction and the design of virtual humans, and  applications in assistive and educational technology and interactive entertainment. Researchers are already hard at work to place our accounts of embodied action in conversation in contact with pragmatic theories derived from text discourse and spoken dialogue. In my own experience, such work proves both illuminating and exciting. For example, it challenges us to support and reﬁne theories of discourse coherence by accounting for the discourse relations and default inference that determine the joint interpretation of coverbal gesture and its accompanying speech (Lascarides and Stone, 2008). And it challenges us to show how speakers work across modalities to engage with, disambiguate, and (on acceptance) recapitulate each others’ communicative actions, to ground their meanings (Lascarides and Stone, In Preparation). The closer we look at conversation, the more we can ﬁt all its behaviors into a unitary framework—inviting us to implement behavioral control for embodied social agents through a pervasive analogy to NLG. We can already pursue such implementations easily. Computationally, motion is just sequence data, and we can manipulate it in parallel ways to the speech data we already use in spoken language generation (Stone et al., 2004). At a higher level, we can represent an embodied performance through a matrix of discrete actions selected and synchronized to an abstract time-line, as in our RUTH system (DeCarlo et al., 2004; Stone and Oh, 2008). This lets us use any NLG method that manipulates structured selections of discrete actions as an architecture for the production of embodied behavior. Templates, as in (Stone and DeCarlo, 2003; Stone et al., 2004), offer  5 
Information graphics, such as bar charts and line graphs, play an important role in multimodal documents. This paper presents a novel approach to producing a brief textual summary of a simple bar chart. It outlines our approach to augmenting the core message of the graphic to produce a brief summary. Our method simultaneously constructs both the discourse and sentence structures of the textual summary using a bottom-up approach. The result is then realized in natural language. An evaluation study validates our generation methodology. 
Summarising georeferenced (can be identiﬁed according to it’s location) data in natural language is challenging because it requires linking events describing its nongeographic attributes to their underlying geography. This mapping is not straightforward as often the only explicit geographic information such data contains is latitude and longitude. In this paper we present an approach to generating textual summaries of georeferenced data based on spatial reference frames. This approach has been implemented in a data-to-text system we have deployed in the weather forecasting domain. 
We present a novel unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees. An automatic evaluation shows that our method obtains result comparable or superior to the state of the art. We demonstrate that the choice of the parser affects the performance of the system. We also apply the method to German and report the results of an evaluation with humans. 
Extractive summarization is the strategy of concatenating extracts taken from a corpus into a summary, while abstractive summarization involves paraphrasing the corpus using novel sentences. We define a novel measure of corpus controversiality of opinions contained in evaluative text, and report the results of a user study comparing extractive and NLG-based abstractive summarization at different levels of controversiality. While the abstractive summarizer performs better overall, the results suggest that the margin by which abstraction outperforms extraction is greater when controversiality is high, providing a context in which the need for generationbased methods is especially great. 
In this paper, we propose to reinterpret the problem of generating referring expressions (GRE) as the problem of computing a formula in a description logic that is only satisﬁed by the referent. This view offers a new unifying perspective under which existing GRE algorithms can be compared. We also show that by applying existing algorithms for computing simulation classes in description logic, we can obtain extremely efﬁcient algorithms for relational referring expressions without any danger of running into inﬁnite regress. 
There is a prevailing assumption in the literature on referring expression generation that relations are used in descriptions only ‘as a last resort’, typically on the basis that including the second entity in the relation introduces an additional cognitive load for either speaker or hearer. In this paper, we describe an experiemt that attempts to test this assumption; we determine that, even in simple scenes where the use of relations is not strictly required in order to identify an entity, relations are in fact often used. We draw some conclusions as to what this means for the development of algorithms for the generation of referring expressions. 
This paper reports on attempts at Aberdeen1 to measure the effects on readers’ emotions of positively and negatively “slanted” texts with the same basic message. The “slanting” methods could be implemented in an (NLG) system. We discuss a number of possible reasons why the studies were unable to show clear, statistically signiﬁcant differences between the effects of the different texts. 
We present a technique that opens up grammar-based generation to a wider range of practical applications by dramatically reducing the development costs and linguistic expertise that are required. Our method infers the grammatical resources needed for generation from a set of declarative examples that link surface expressions directly to the application’s available semantic representations. The same examples further serve to optimize a run-time search strategy that generates the best output that can be found within an application-speciﬁc time frame. Our method offers substantially lower development costs than hand-crafted grammars for applicationspeciﬁc NLG, while maintaining high output quality and diversity. 
We describe three PCFG-based models for Chinese sentence realisation from LexicalFunctional Grammar (LFG) f-structures. Both the lexicalised model and the history-based model improve on the accuracy of a simple wide-coverage PCFG model by adding lexical and contextual information to weaken inappropriate independence assumptions implicit in the PCFG models. In addition, we provide techniques for lexical smoothing and rule smoothing to increase the generation coverage. Trained on 15,663 automatically LFG fstructure annotated sentences of the Penn Chinese treebank and tested on 500 sentences randomly selected from the treebank test set, the lexicalised model achieves a BLEU score of 0.7265 at 100% coverage, while the historybased model achieves a BLEU score of 0.7245 also at 100% coverage. 
When evaluating a generation system, if a corpus of target outputs is available, a common and simple strategy is to compare the system output against the corpus contents. However, cross-validation metrics that test whether the system makes exactly the same choices as the corpus on each item have recently been shown not to correlate well with human judgements of quality. An alternative evaluation strategy is to compute intrinsic, task-speciﬁc properties of the generated output; this requires more domain-speciﬁc metrics, but can often produce a better assessment of the output. In this paper, a range of metrics using both of these techniques are used to evaluate three methods for selecting the facial displays of an embodied conversational agent, and the predictions of the metrics are compared with human judgements of the same generated output. The corpus-reproduction metrics show no relationship with the human judgements, while the intrinsic metrics that capture the number and variety of facial displays show a signiﬁcant correlation with the preferences of the human users. 
To generate natural language feedback for an intelligent tutoring system, we developed a simple planning model with a distinguishing feature: its plan operators are derived automatically, on the basis of the association rules mined from our tutorial dialog corpus. Automatically mined rules are also used for realization. We evaluated 5 different versions of a system that tutors on an abstract sequence learning task. The version that uses our planning framework is signiﬁcantly more effective than the other four versions. We compared this version to the human tutors we employed in our tutorial dialogs, with intriguing results. 
In this paper we describe content determination issues involved in the Atlas.txt project, which aims to automatically describe georeferenced information such as census data as text for the visually-impaired (VI). Texts communicating geo-referenced census information contain census data abstractions and their corresponding geographic references. Because visually impaired users ﬁnd interpreting geographic references hard, we hypothesized that an introduction message about the underlying geography should help the users to interpret the geographic references easily. We performed user studies to design and evaluate the introduction message. An initial evaluation study with several sighted users and one partially sighted user showed that an introduction message is certainly preferred by most participants. Many of them used an introduction message themselves when they described maps textually. But the study also showed that the introduction message made no difference when the participants were asked to draw maps using the information in the textual descriptions. 
This paper explores how the main question addressed in Text Planning has evolved over the last twenty years. Earlier approaches to text planning asked the question: How do we write a good text?, and whatever answers were found were programmed directly into code. With the introduction of search-based text planning in recent years, the focus shifted to the evaluation function, and thus the question became: How do we tell if a text is good? This paper will explore these evolving questions, and subsequent refinements of them as the field matures. Introduction Given the growing interest in the application of search-based planning methods in NLG, we can describe how text planning has evolved from its beginning in terms of the kinds of questions that have been addressed. 
A dialogue system can present itself and/or address the user as an active agent by means of linguistic constructions in personal style, or suppress agentivity by using impersonal style. We describe how we generate and control personal and impersonal style variation in the output of SAMMIE, a multimodal in-car dialogue system for an MP3 player. We carried out an experiment to compare subjective evaluation judgments and input style alignment behavior of users interacting with versions of the system generating output in personal vs. impersonal style. Although our results are consistent with earlier ﬁndings obtained with simulated systems, the effects are weaker. 
This paper describes an evaluation study of an ontology-driven WYSIWYM interface for metadata creation. Although the results are encouraging, they are not as positive as those of a similar tool developed for the medical domain. We believe this may be due, not to the WYSIWYM interface, but to the complexity of the underlying ontologies and the fact that subjects were unfamiliar with them. We discuss the ways in which ontology development might be inﬂuenced by issues stemming from using an NLG approach for user access to data, and the effect these factors have on general usability. 
The BABYTALK BT-45 system generates textual summaries of clinical data about babies in a neonatal intensive care unit. A recent taskbased evaluation of the system suggested that these summaries are useful, but not as effective as they could be. In this paper we present a qualitative analysis of problems that the evaluation highlighted in BT-45 texts. Many of these problems are due to the fact that BT45 does not generate good narrative texts; this is a topic which has not previously received much attention from the NLG research community, but seems to be quite important for creating good data-to-text systems. 
Natural language generation technology is mature enough for implementing an NLG system in a commercial environment, but the circumstances differ significantly from building a research system. This paper describes the challenges and rewards of building a commercial NLG component for an electronic medical records system. While the resulting NLG system has been successfully completed, the path to that success could have been somewhat smoother knowing the issues in advance.  writing recognition and extensive graphical representation of human anatomy. Its foundation is an elaborate database of medical content, outlining specific requirements for information collection. Much of this medical information is arranged in templates, one for each complaint. When a patient comes into the doctor's office complaining of chest pains, the template for Chest Pain provides the appropriate selections for the doctor to record pertinent information related to that condition. Other parts of the system deal with physical examinations, procedures, prescription of medication, orders for lab tests and procedures, and so on.  
Referring Expression Generation (REG) is the task that deals with references to entities appearing in a spoken or written discourse. If these referents are organized in terms of a taxonomy, there are two problems when establishing a reference that would distinguish an intended referent from its possible distractors. The ﬁrst one is the choice of the set of possible distractrors or contrast set in the given situation. The second is to identify at what level of the taxonomy to phrase the reference so that it unambiguously picks out only the intended referent, leaving all possible distractors in different branches of the taxonomy. We discuss the use of ontologies to deal with the REG task, paying special attention to the choice of the the contrast set and to the use of the information of the ontology to select the most appropriate type to be used for the referent. 
While the effect of domain variation on Penntreebank-trained probabilistic parsers has been investigated in previous work, we study its effect on a Penn-Treebank-trained probabilistic generator. We show that applying the generator to data from the British National Corpus results in a performance drop (from a BLEU score of 0.66 on the standard WSJ test set to a BLEU score of 0.54 on our BNC test set). We develop a generator retraining method where the domain-speciﬁc training data is automatically produced using state-of-the-art parser output. The retraining method recovers a substantial portion of the performance drop, resulting in a generator which achieves a BLEU score of 0.61 on our BNC test data. 
We describe the creation of a new domain for the Methodius Natural Language Generation System, and an evaluation of Methodius’ parameterized comparison generation algorithm. The new domain was based around music and performers, and texts about the domain were generated using Methodius. Our evaluation showed that test subjects learned more from texts that contained comparisons than from those that did not. We also established that the comparison generation algorithm could generalize to the music domain. 
This paper presents a reordering algorithm for generating multiple stories from different perspectives based on a single baseball game. We take a description of a game and a neutral summary, reorder the content of the neutral summary based on event features, and produce two summaries that the users rated as showing perspectives of each of the two teams. We describe the results from an initial user survey that revealed the power of reordering on the users’ perception of perspective. Then we describe our reordering algorithm which was derived from analyzing the corpus of local newspaper articles of teams involved in the games as well as a neutral corpus for the respective games. The resulting reordering algorithm is successful at turning a neutral article into two different summary articles that express the two teams’ perspectives. 
Natural language generation (NLG) applications must occasionally deliver rhetorically coherent output under length constraints. For example, certain types of documents must fit on a single webpage, on a cell phone screen, or into a fixed number of printed pages. To date, applications have achieved this goal by structuring their content as a rhetorical tree and using a greedy algorithm to pick the discourse elements to include in the final document. Greedy algorithms are known to pick sub-optimal solutions. This paper presents an alternate approach based on dynamic programming. 
REG’08 got underway with a ﬁrst announcement in September 2007. We released samples for both datasets and invited preliminary registrations in January 2008, and released the full Participants’ Pack including instructions and training/development data on 22nd February to registered participants. Twelve teams registered for one or more of the TUNA tasks, and ﬁve for the GREC Task. Among the participants were teams from Australia, Spain, Ireland, UK, USA, Brazil, Belgium, Netherlands, India and Germany. 
The GREC Task at REG’08 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia. Three teams submitted a total of 6 systems, and we additionally created four baseline systems. Systems were tested automatically using a range of existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in a reading/comprehension experiment involving human subjects. This report describes the GREC Task and the evaluation methods, gives brief descriptions of the participating systems, and presents the evaluation results. 
The GREC task of the Referring Expression Generation Challenge 2008 is to select appropriate references to the main subject in given texts. This means to select the correct type of the referring expressions such as name, pronoun, common, or elision (empty). We employ for the selection different learning techniques with the aim to ﬁnd the most appropriate one for the task and the used attributes. As training data, we use the syntactic category of the searched referring expressions and additionally gathered data from the text itself. 
In this paper we describe our machine learning approach to the generation of referring expressions. As our algorithm we use memory-based learning. Our results show that in case of predicting the TYPE of the expression, having one general classiﬁer gives the best results. On the contrary, when predicting the full set of properties of an expression, a combined set of specialized classiﬁers for each subdomain gives the best performance. 
 3 Predicates  Selection of natural-sounding referring expressions is useful in text generation and information summarization (Kan et al., 2001). We use discourse-level feature predicates in a maximum entropy classiﬁer (Berger et al., 1996) with binary and n-class classiﬁcation to select referring expressions from a list. We ﬁnd that while mention-type n-class classiﬁcation produces higher accuracy of type, binary classiﬁcation of individual referring expressions helps to avoid use of awkward referring expressions.  We created 13 predicates, in addition to the six predicates available with the corpus. All predicates can be used with the binary classiﬁcation method; only non-RE-level predicates can be used with the n-class classiﬁcation method. Predicates describe: string similarity of the RE and the title of the article, the mention’s order in the article, distance between previous mention and current mention, and detection of a contrastive discourse entity in the text.1 4 Maximum Entropy Classiﬁer  
In the ﬁrst REG competition, researchers proposed several general-purpose algorithms for attribute selection for referring expression generation. However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information. In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques. 
Both greedy and domain-oriented REG algorithms have significant strengths but tend to perform poorly according to humanlikeness criteria as measured by, e.g., Dice scores. In this work we describe an attempt to combine both perspectives into a single attribute selection strategy to be used as part of the Dale & Reiter Incremental algorithm in the REG Challenge 2008, and the results in both Furniture and People domains.  (from the computational point of view), but also less natural than simply using typical (e.g., frequent) attributes. On the other hand, as the number of distractors decreases, it may become gradually clearer for the speaker which attributes are most helpful to achieve uniqueness, up to the point in which she may naturally switch to a ‘greedy’ strategy and finalize the description. These assumptions are implemented as an attribute selection strategy to be used with the Incremental algorithm (Dale & Reiter, 1995) described below. 2 System Description  
This system’s approach to the attribute selection task was to use a genetic programming algorithm to search for a solution to the task. The evolved programs for the furniture and people domain exhibit quite naive behavior, and the DICE and MASI scores on the training sets reﬂect the poor humanlikeness of the programs.  valueIn target distractors eqA set empty set add set remove add remove  {variables} sequence canUseLoc ifte for and, or, not {attributeERC} {attributeListERC}  Table 1: Functions supplied to the GP algorithm  
We describe a graph-based generation system that participated in the TUNA attribute selection and realisation task of the REG 2008 Challenge. Using a stochastic cost function (with certain properties for free), and trying attributes from cheapest to more expensive, the system achieves overall .76 DICE and .54 MASI scores for attribute selection on the development set. For realisation, it turns out that in some cases higher attribute selection accuracy leads to larger differences between system-generated and human descriptions. 
This paper presents a Prefix Tree based model of Generation of Referring Expression (RE). Our algorithm PTBSGRE works in two phases. First, an encoded prefix tree is constructed describing the domain structure. Subsequently, RE is generated using that structure. We evaluated our system using Dice, MASI, Accuracy, Minimality and Uniqueness scoring method using standard TEVAl tool and the result is encouraging.  algorithm MakeRefExpr(r,p,T,L) is shown in figure 2, where T is a node pointer and p is pointer to parent of that node. Our algorithm MakeRefExpr returns set of attribute-values L to identify r in the domain. [[Ni]]= {d |d∈D and d is stored at node Ni where Ni is an i-th level node}. Card(N) is cardinality of set of objects in node N.  
This document describes the development of a surface realisation component for the Portuguese language that takes advantage of the data and evaluation tools provided by the REG-2008 team. At this initial stage, our work uses simple n-gram statistics to produce descriptions in the Furniture domain, with little or no linguistic variation. Preliminary results suggest that, unlike the generation of English descriptions, contextual information may be required to account for Portuguese word order. 
This paper presents a method and implementation of parsing German V2 word order by means of constraints that reside in lexical heads. It ﬁrst describes the design of the underlying parsing engine: the head-corner chart parsing that incorporates a procedure that dynamically enforces word order constraints. While the parser could potentially generate all the permutations of terminal symbols, constraint checking is conducted locally in an efﬁcient manner. The paper then shows how this parser can adequately cover a variety of V2 word order patterns with sets of lexically encoded constraints, including non-local preposing of an embedded argument or an adverbial. 
We report on some recent parse selection experiments carried out with GG, a large-scale HPSG grammar for German. Using a manually disambiguated treebank derived from the Verbmobil corpus, we achieve over 81% exact match accuracy compared to a 21.4% random baseline, corresponding to an error reduction rate of 3.8. 
In this paper, we introduce the German version of the multilingual Fips parsing system. We focus on the evaluation of its part-ofspeech tagging component with the help of the TIGER treebank. We explain how Fips can be adapted to the tagset used by TIGER and report ﬁrst results of this study: currently, 87% of words are tagged correctly. We also discuss some common errors and explore a possible extension of this study to parsing. 
Recent parsing research has started addressing the questions a) how parsers trained on different syntactic resources differ in their performance and b) how to conduct a meaningful evaluation of the parsing results across such a range of syntactic representations. Two German treebanks, Negra and Tu¨Ba-D/Z, constitute an interesting testing ground for such research given that the two treebanks make very different representational choices for this language, which also is of general interest given that German is situated between the extremes of ﬁxed and free word order. We show that previous work comparing PCFG parsing with these two treebanks employed PARSEVAL and grammatical function comparisons which were skewed by differences between the two corpus annotation schemes. Focusing on the grammatical dependency triples as an essential dimension of comparison, we show that the two very distinct corpora result in comparable parsing performance. 
We describe experiments on learning latent variable grammars for various German treebanks, using a language-agnostic statistical approach. In our method, a minimal initial grammar is hierarchically reﬁned using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language speciﬁc or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 
Previous work on German parsing has provided confusing and conﬂicting results concerning the difﬁculty of the task and whether techniques that are useful for English, such as lexicalization, are effective for German. This paper aims to provide some understanding and solid baseline numbers for the task. We examine the performance of three techniques on three treebanks (Negra, Tiger, and Tu¨Ba-D/Z): (i) Markovization, (ii) lexicalization, and (iii) state splitting. We additionally explore parsing with the inclusion of grammatical function information. Explicit grammatical functions are important to German language understanding, but they are numerous, and na¨ıvely incorporating them into a parser which assumes a small phrasal category inventory causes large performance reductions due to increasing sparsity. 
We present a dependency-driven parser that parses both dependency structures and constituent structures. Constituency representations are automatically transformed into dependency representations with complex arc labels, which makes it possible to recover the constituent structure with both constituent labels and grammatical functions. We report a labeled attachment score close to 90% for dependency versions of the TIGER and Tu¨BaD/Z treebanks. Moreover, the parser is able to recover both constituent labels and grammatical functions with an F-Score over 75% for Tu¨Ba-D/Z and over 65% for TIGER. 
We outline a new ICALL system for learners of Russian, focusing on the processing needed for basic morphological errors. By setting out an appropriate design for a lexicon and distinguishing the types of morphological errors to be detected, we establish a foundation for error detection across exercises. 
We present a domain-independent technique for assessing learners’ constructed responses. The system exceeds the accuracy of the majority class baseline by 15.4% and a lexical baseline by 5.9%. The emphasis of this paper is to provide an error analysis of performance, describing the types of errors committed, their frequency, and some issues in their resolution. 
King Alfred is the name of both an innovative textbook and a computational environment deployed in parallel in an undergraduate course on Anglo-Saxon literature. This paper details the ways in which it brings dynamicallygenerated resources to the aid of the language student. We store the feature-rich grammar of Anglo-Saxon in a bi-level glossary, provide an annotation context for use during the translation task, and are currently working toward the implementation of automatic evaluation of student-generated translations. 
This paper describes a method for recognizing romanized Japanese words in learner English. They become noise and problematic in a variety of tasks including Part-Of-Speech tagging, spell checking, and error detection because they are mostly unknown words. A problem one encounters when recognizing romanized Japanese words in learner English is that the spelling rules of romanized Japanese words are often violated by spelling errors. To address the problem, the described method uses a clustering algorithm reinforced by a small set of rules. Experiments show that it achieves an -measure of 0.879 and outperforms other methods. They also show that it only requires the target text and a fair size of English word list. 
We present the STYX system, which is designed as an electronic corpus-based exercise book of Czech morphology and syntax with sentences directly selected from the Prague Dependency Treebank, the largest annotated corpus of the Czech language. The exercise book offers complex sentence processing with respect to both morphological and syntactic phenomena, i. e. the exercises allow students of basic and secondary schools to practice classifying parts of speech and particular morphological categories of words and in the parsing of sentences and classifying the syntactic functions of words. The corpus-based exercise book presents a novel usage of annotated corpora outside their original context. 
Information overload is a well-known problem which can be particularly detrimental to learners. In this paper, we propose a method to support learners in the information seeking process which consists in answering their questions by retrieving question paraphrases and their corresponding answers from social Q&A sites. Given the novelty of this kind of data, it is crucial to get a better understanding of how questions in social Q&A sites can be automatically analysed and retrieved. We discuss and evaluate several pre-processing strategies and question similarity metrics, using a new question paraphrase corpus collected from the WikiAnswers Q&A site. The results show that viable performance levels of more than 80% accuracy can be obtained for the task of question paraphrase retrieval. 
Tutorial dialogue has been the subject of increasing attention in recent years, and it has become evident that empirical studies of human-human tutorial dialogue can contribute important insights to the design of computational models of dialogue. This paper reports on a corpus study of human-human tutorial dialogue transpiring in the course of problemsolving in a learning environment for introductory computer science. Analyses suggest that the choice of corrective tutorial strategy makes a significant difference in the outcomes of both student learning gains and selfefficacy gains. The findings reveal that tutorial strategies intended to maximize student motivational outcomes (e.g., self-efficacy gain) may not be the same strategies that maximize cognitive outcomes (i.e., learning gain). In light of recent findings that learner characteristics influence the structure of tutorial dialogue, we explore the importance of understanding the interaction between learner characteristics and tutorial dialogue strategy choice when designing tutorial dialogue systems. 
This paper reports on the ﬁrst stage of building an educational tool for international graduate students to improve their academic writing skills. Taking a text-categorization approach, we experimented with several models to automatically classify sentences in research article introductions into one of three rhetorical moves. The paper begins by situating the project within the larger framework of intelligent computer-assisted language learning. It then presents the details of the study with very encouraging results. The paper then concludes by commenting on how the system may be improved and how the project is intended to be pursued and evaluated. 
A reading difﬁculty measure can be described as a function or model that maps a text to a numerical value corresponding to a difﬁculty or grade level. We describe a measure of readability that uses a combination of lexical features and grammatical features that are derived from subtrees of syntactic parses. We also tested statistical models for nominal, ordinal, and interval scales of measurement. The results indicate that a model for ordinal regression, such as the proportional odds model, using a combination of grammatical and lexical features is most effective at predicting reading difﬁculty. 
Finding appropriate, authentic reading materials is a challenge for language instructors. The Web is a vast resource of texts, but most pages are not suitable for reading practice, and commercial search engines are not well suited to ﬁnding texts that satisfy pedagogical constraints such as reading level, length, text quality, and presence of target vocabulary. We present a system that uses various language technologies to facilitate the retrieval and presentation of authentic reading materials gathered from the Web. It is currently deployed in two English as a Second Language courses at the University of Pittsburgh. 
The automatic analysis and categorization of web text has witnessed a booming interest due to the increased text availability of different formats, content, genre and authorship. We present a new tool that searches the web and performs in real-time a) html-free text extraction, b) classiﬁcation for thematic content and c) evaluation of expected reading difﬁculty. This tool will be useful to adolescent and adult low-level reading students who face, among other challenges, a troubling lack of reading material for their age, interests and reading level. 
This paper describes a system aimed at automatically scoring two task types of high and medium-high linguistic entropy from a spoken English test with a total of six widely differing task types. We describe the speech recognizer used for this system and its acoustic model and language model adaptation; the speech features computed based on the recognition output; and finally the scoring models based on multiple regression and classification trees. For both tasks, agreement measures between machine and human scores (correlation, kappa) are close to or reach inter-human agreements. 
A common focus of systems in Intelligent Computer-Assisted Language Learning (ICALL) is to provide immediate feedback to language learners working on exercises. Most of this research has focused on providing feedback on the form of the learner input. Foreign language practice and second language acquisition research, on the other hand, emphasizes the importance of exercises that require the learner to manipulate meaning. The ability of an ICALL system to diagnose and provide feedback on the meaning conveyed by a learner response depends on how well it can deal with the response variation allowed by an activity. We focus on short-answer reading comprehension questions which have a clearly deﬁned target response but the learner may convey the meaning of the target in multiple ways. As empirical basis of our work, we collected an English as a Second Language (ESL) learner corpus of short-answer reading comprehension questions, for which two graders provided target answers and correctness judgments. On this basis, we developed a Content-Assessment Module (CAM), which performs shallow semantic analysis to diagnose meaning errors. It reaches an accuracy of 88% for semantic error detection and 87% on semantic error diagnosis on a held-out test data set. 
We describe a multimodal dialogue system for interacting with a home entertainment center via a mobile device. In our working prototype, users may utilize both a graphical and speech user interface to search TV listings, record and play television programs, and listen to music. The developed framework is quite generic, potentially supporting a wide variety of applications, as we demonstrate by integrating a weather forecast application. In the prototype, the mobile device serves as the locus of interaction, providing both a small touchscreen display, and speech input and output; while the TV screen features a larger, richer GUI. The system architecture is agnostic to the location of the natural language processing components: a consistent user experience is maintained regardless of whether they run on a remote server or on the device itself. 
In this paper we present a wearable, headset integrated eyes- and hands-free speech-tospeech (S2S) translation system. The S2S system described here is configured for translingual communication between English and colloquial Iraqi Arabic. It employs an n-gram speech recognition engine, a rudimentary phrase-based translator for translating recognized Iraqi text, and a rudimentary text-tospeech (TTS) synthesis engine for playing back the English translation. This paper describes the system architecture, the functionality of its components, and the configurations of the speech recognition and machine translation engines. 
We propose an information extraction system that is designed for mobile devices with low hardware resources. The proposed system extracts temporal instances (dates and times) and named instances (locations and topics) from Korean short messages in an appointment management domain. To efficiently extract temporal instances with limited numbers of surface forms, the proposed system uses wellrefined finite state automata. To effectively extract various surface forms of named instances with low hardware resources, the proposed system uses a modified HMM based on syllable n-grams. In the experiment on instance boundary labeling, the proposed system showed better performances than traditional classifiers. 
The application of statistical NLP systems to resource constrained devices is limited by the need to maintain parameters for a large number of features and an alphabet mapping features to parameters. We introduce random feature mixing to eliminate alphabet storage and reduce the number of parameters without severely impacting model performance. 
In an automatic speech recognition system using a tied-mixture acoustic model, the main cost in CPU time and memory lies not in the evaluation and storage of Gaussians themselves but rather in evaluating the mixture likelihoods for each state output distribution. Using a simple entropy-based technique for pruning the mixture weight distributions, we can achieve a signiﬁcant speedup in recognition for a 5000-word vocabulary with a negligible increase in word error rate. This allows us to achieve real-time connected-word dictation on an ARM-based mobile device. 
This paper reflects on our work in providing communication support for people with speech and language disabilities. We discuss the role of mobile technologies in assistive systems and share ongoing research efforts. 
The paper presents an experimental machine translation system for mobile devices and its main component — a distributed database which is used in the module of lexical transfer. The database contains data shared among multiple devices and provides their automatic synchronization. 
The performance of automatic speech recognition systems varies widely across different contexts. Very good performance can be achieved on single-speaker, large-vocabulary dictation in a clean acoustic environment, as well as on very small vocabulary tasks with much fewer constraints on the speakers and acoustic conditions. In other domains, speech recognition is still far from usable for real-world applications. One domain that is still elusive is that of spontaneous conversational speech. This type of speech poses a number of challenges, such as the presence of disﬂuencies, a mix of speech and non-speech sounds such as laughter, and extreme variation in pronunciation. In this talk, I will focus on the challenge of pronunciation variation. A number of analyses suggest that this variability is responsible for a large part of the drop in recognition performance between read (dictated) speech and conversational speech. I will describe efforts in the speech recognition community to characterize and model pronunciation variation, both for conversational speech and in general. The work can be roughly divided into several types of approaches, including: augmentation of a phonetic pronunciation lexicon with phonological rules; the use of large (syllable- or word-sized) units instead of the more traditional phonetic ones; and the use of smaller units, such as distinctive or articulatory features. Of these, the ﬁrst is the most thoroughly studied and also the most disappointing: Despite successes in a few domains, it has been difﬁcult to obtain signiﬁcant recognition improvements by including in the lexicon those phonetic pronunciations that appear to exist in the data. In part as a reaction to this, many have advocated the use of a “null pronunciation model,” i.e. a very limited lexicon including only canonical pronunciations. The assumption in this approach is that the observation model—the distribution of the acoustics given phonetic units—will better model the “noise” introduced by pronunciation variability. I will advocate an alternative view: that the phone unit may not be the most appropriate for modeling the lexicon. When considering a variety of pronunciation phenomena, it becomes apparent that phonetic transcription often obscures some of the fundamental processes that are at play. I will describe approaches using both larger and “smaller” units. Larger units are typically syllables or words, and allow greater freedom to model the component states of each unit. In the class of “smaller” unit models, ideas from articulatory and autosegmental phonology motivate multi-tier models in which different features (or groups of features) have semi-independent behavior. I will present a particular model in which articulatory features are represented as variables in a dynamic Bayesian network. Non-phonetic pronunciation models can involve signiﬁcantly different model structures than those typically used in speech recognition, and as a result they may also entail modiﬁcations to other components such as the observation model and training algorithms. At this point it is not clear what the “winning” approach will be. The success of a given approach may depend on the domain or on the amount and type of training data available. I will describe some of the current challenges and ongoing work, with a particular focus on the role of phonological theories in statistical models of pronunciation (and vice versa?). 
This paper is an analysis of the claim that a universal ban on certain (‘anti-markedness’) grammars is necessary in order to explain their non-occurrence in the languages of the world. To assess the validity of this hypothesis I examine the implications of one sound change (a > ) for learning in a specific phonological domain (stress assignment), making explicit assumptions about the type of data that results, and the learning function that computes over that data. The preliminary conclusion is that restrictions on possible end-point languages are unneeded, and that the most likely outcome of change is a lexicon that is inconsistent with respect to a single generating rule. 
A stochastic approach to learning phonology. The model presented captures 7-15% more phonologically plausible underlying forms than a simple majority solution, because it prefers “pure” alternations. It could be useful in cases where an approximate solution is needed, or as a seed for more complex models. A similar process could be involved in some stages of child language acquisition; in particular, early learning of phonotactics. 
This paper describes a variety of nonparametric Bayesian models of word segmentation based on Adaptor Grammars that model different aspects of the input and incorporate different kinds of prior knowledge, and applies them to the Bantu language Sesotho. While we ﬁnd overall word segmentation accuracies lower than these models achieve on English, we also ﬁnd some interesting differences in which factors contribute to better word segmentation. Speciﬁcally, we found little improvement to word segmentation accuracy when we modeled contextual dependencies, while modeling morphological structure did improve segmentation accuracy. 
We examine the typology of quantityinsensitive (QI) stress systems and ask to what extent an existing optimality theoretic model of QI stress can predict the observed typological frequencies of stress patterns. We ﬁnd three signiﬁcant correlates of pattern attestation and frequency: the trigram entropy of a pattern, the degree to which it is “confusable” with other patterns predicted by the model, and the number of constraint rankings that specify the pattern. 
Two analyses of Ma¯ori passives and gerunds have been debated in the literature. Both assume that the thematic consonants in these forms are unpredictable. This paper reports on three computational experiments designed to test whether this assumption is sound. The results suggest that thematic consonants are predictable from the phonotactic probabilities of their active counterparts. This study has potential implications for allomorphy in other Polynesian languages. It also exempliﬁes the beneﬁts of using computational methods in linguistic analyses.  Active /FeRa/ /oma/ /inu/ /eke/ /tupu/ /aFi/ /huna/ /kata/ /ako/ /heke/  Passive /FeRahia/ /omakia/ /inumia/ /ekeNia/ /tupuRia/ /aFitia/ /hunaia/ /kataina/ /akona/ /hekea/  Gloss ‘to spread’ ‘to run’ ‘to drink’ ‘to climb’ ‘to grow’ ‘to embrace’ ‘to conceal’ ‘to laugh’ ‘to teach’ ‘to descend’  Table 1: Examples of active and passive verbs in Ma¯ori.  
This paper describes and evaluates a modification to the segmentation model used in the unsupervised morphology induction system, ParaMor. Our improved segmentation model permits multiple morpheme boundaries in a single word. To prepare ParaMor to effectively apply the new agglutinative segmentation model, two heuristics improve ParaMor’s precision. These precision-enhancing heuristics are adaptations of those used in other unsupervised morphology induction systems, including work by Hafer and Weiss (1974) and Goldsmith (2006). By reformulating the segmentation model used in ParaMor, we significantly improve ParaMor’s performance in all language tracks and in both the linguistic evaluation as well as in the task based information retrieval (IR) evaluation of the peer operated competition Morpho Challenge 2007. ParaMor’s improved morpheme recall in the linguistic evaluations of German, Finnish, and Turkish is higher than that of any system which competed in the Challenge. In the three languages of the IR evaluation, our enhanced ParaMor significantly outperforms, at average precision over newswire queries, a morphologically naïve baseline; scoring just behind the leading system from Morpho Challenge 2007 in English and ahead of the first place system in German. 
In this paper, we propose a graph kernel based approach for the automated extraction of protein-protein interactions (PPI) from scientiﬁc literature. In contrast to earlier approaches to PPI extraction, the introduced alldependency-paths kernel has the capability to consider full, general dependency graphs. We evaluate the proposed method across ﬁve publicly available PPI corpora providing the most comprehensive evaluation done for a machine learning based PPI-extraction system. Our method is shown to achieve state-of-theart performance with respect to comparable evaluations, achieving 56.4 F-score and 84.8 AUC on the AImed corpus. Further, we identify several pitfalls that can make evaluations of PPI-extraction systems incomparable, or even invalid. These include incorrect crossvalidation strategies and problems related to comparing F-score results achieved on different evaluation resources. 
The Clinical E-Science Framework (CLEF) project has built a system to extract clinically signiﬁcant information from the textual component of medical records, for clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. One part of this system is the identiﬁcation of relationships between clinically important entities in the text. Typical approaches to relationship extraction in this domain have used full parses, domain-speciﬁc grammars, and large knowledge bases encoding domain knowledge. In other areas of biomedical NLP, statistical machine learning approaches are now routinely applied to relationship extraction. We report on the novel application of these statistical techniques to clinical relationships. We describe a supervised machine learning system, trained with a corpus of oncology narratives hand-annotated with clinically important relationships. Various shallow features are extracted from these texts, and used to train statistical classiﬁers. We compare the suitability of these features for clinical relationship extraction, how extraction varies between inter- and intra-sentential relationships, and examine the amount of training data needed to learn various relationships. 
An adaptable relation extraction system for the biomedical domain is presented. The system makes use of a large set of contextual and shallow syntactic features, which can be automatically optimised for each relation type. The system is tested on three diﬀerent relation types; protein-protein interactions, tissue expression relations and fragment to parent protein relations. 
eGIFT (Extracting Gene Information From Text) is an intelligent system which is intended to aid scientists in surveying literature relevant to genes of interest. From a gene speciﬁc set of abstracts retrieved from PubMed, eGIFT determines the most important terms associated with the given gene. Annotators using eGIFT can quickly ﬁnd articles describing gene functions and individuals scientists surveying the results of highthroughput experiments can quickly extract information important to their hits.  corepressor, segmentation, neurogenesis and wd40. This might immediately inform a user that Groucho is probably a transcriptional corepressor, that it might be involved in the processes of segmentation and neurogenesis and that it might contain the wd40 domain, which allows them to draw further inferences about the gene. To enable the scientists to get a deeper understanding, eGIFT further allows the retrieval of all sentences from this gene’s literature containing the key phrase in question. The sentences can be displayed in isolation or in the context of the abstract in which they appear. 2 Ranking Key Terms  
This paper presents an active learning-like framework for reducing the human effort for making named entity annotations in a corpus. In this framework, the annotation work is performed as an iterative and interactive process between the human annotator and a probabilistic named entity tagger. At each iteration, sentences that are most likely to contain named entities of the target category are selected by the probabilistic tagger and presented to the annotator. This iterative annotation process is repeated until the estimated coverage reaches the desired level. Unlike active learning approaches, our framework produces a named entity corpus that is free from the sampling bias introduced by the active strategy. We evaluated our framework by simulating the annotation process using two named entity corpora and show that our approach could drastically reduce the number of sentences to be annotated when applied to sparse named entities. 
This article reports on a corpus annotation project that has produced a freely available resource for research on handling negation and uncertainty in biomedical texts (we call this corpus the BioScope corpus). The corpus consists of three parts, namely medical free texts, biological full papers and biological scientific abstracts. The dataset contains annotations at the token level for negative and speculative keywords and at the sentence level for their linguistic scope. The annotation process was carried out by two independent linguist annotators and a chief annotator – also responsible for setting up the annotation guidelines – who resolved cases where the annotators disagreed. We will report our statistics on corpus size, ambiguity levels and the consistency of annotations. 
We explore a linguistically motivated approach to the problem of recognizing speculative language (“hedging”) in biomedical research articles. We describe a method, which draws on prior linguistic work as well as existing lexical resources and extends them by introducing syntactic patterns and a simple weighting scheme to estimate the speculation level of the sentences. We show that speculative language can be recognized successfully with such an approach, discuss some shortcomings of the method and point out future research possibilities. 
Chemical named entities represent an important facet of biomedical text. We have developed a system to use character-based ngrams, Maximum Entropy Markov Models and rescoring to recognise chemical names and other such entities, and to make conﬁdence estimates for the extracted entities. An adjustable threshold allows the system to be tuned to high precision or high recall. At a threshold set for balanced precision and recall, we were able to extract named entities at an F score of 80.7% from chemistry papers and 83.2% from PubMed abstracts. Furthermore, we were able to achieve 57.6% and 60.3% recall at 95% precision, and 58.9% and 49.1% precision at 90% recall. These results show that chemical named entities can be extracted with good performance, and that the properties of the extraction can be tuned to suit the demands of the task. 
When term ambiguity and variability are very high, dictionary-based Named Entity Recognition (NER) is not an ideal solution even though large-scale terminological resources are available. Many researches on statistical NER have tried to cope with these problems. However, it is not straightforward how to exploit existing and additional Named Entity (NE) dictionaries in statistical NER. Presumably, addition of NEs to an NE dictionary leads to better performance. However, in reality, the retraining of NER models is required to achieve this. We have established a novel way to improve the NER performance by addition of NEs to an NE dictionary without retraining. We chose protein name recognition as a case study because it most suffers the problems related to heavy term variation and ambiguity. In our approach, ﬁrst, known NEs are identiﬁed in parallel with Part-of-Speech (POS) tagging based on a general word dictionary and an NE dictionary. Then, statistical NER is trained on the tagger outputs with correct NE labels attached. We evaluated performance of our NER on the standard JNLPBA-2004 data set. The F-score on the test set has been improved from 73.14 to 73.78 after adding the protein names appearing in the training data to the POS tagger dictionary without any model retraining. The performance further increased to 78.72 after enriching the tagging dictionary with test set protein names. Our approach has demonstrated high performance in protein name recognition, which indicates how to make the most of known NEs in statistical NER.  
An important task in information extraction (IE) from biomedical articles is term identiﬁcation (TI), which concerns linking entity mentions (e.g., terms denoting proteins) in text to unambiguous identiﬁers in standard databases (e.g., RefSeq). Previous work on TI has focused on species-speciﬁc documents. However, biomedical documents, especially full-length articles, often talk about entities across a number of species, in which case resolving species ambiguity becomes an indispensable part of TI. This paper describes our rule-based and machine-learning based approaches to species disambiguation and demonstrates that performance of TI can be improved by over 20% if the correct species are known. We also show that using the species predicted by the automatic species taggers can improve TI by a large margin. 
Like text in other domains, biomedical documents contain a range of terms with more than one possible meaning. These ambiguities form a signiﬁcant obstacle to the automatic processing of biomedical texts. Previous approaches to resolving this problem have made use of a variety of knowledge sources including linguistic information (from the context in which the ambiguous term is used) and domain-speciﬁc resources (such as UMLS). In this paper we compare a range of knowledge sources which have been previously used and introduce a novel one: MeSH terms. The best performance is obtained using linguistic features in combination with MeSH terms. Results from our system outperform published results for previously reported systems on a standard test set (the NLM-WSD corpus). 
This paper describes the use and customization of Inductive Logic Programming (ILP) to infer indexing rules from MEDLINE citations. Preliminary results suggest this method may enhance the subheading attachment module of the Medical Text Indexer, a system for assisting MEDLINE indexers. 
The goal of the Penn Discourse Treebank (PDTB) project is to develop a large-scale corpus, annotated with coherence relations marked by discourse connectives. Currently, the primary application of the PDTB annotation has been to news articles. In this study, we tested whether the PDTB guidelines can be adapted to a different genre. We annotated discourse connectives and their arguments in one 4,937-token full-text biomedical article. Two linguist annotators showed an agreement of 85% after simple conventions were added. For the remaining 15% cases, we found that biomedical domain-specific knowledge is needed to capture the linguistic cues that can be used to resolve inter-annotator disagreement. We found that the two annotators were able to reach an agreement after discussion. Thus our experiments suggest that the PDTB annotation can be adapted to new domains by minimally adjusting the guidelines and by adding some further domain-specific linguistic cues. 
We present a comparative study between two machine learning methods, Conditional Random Fields and Support Vector Machines for clinical named entity recognition. We explore their applicability to clinical domain. Evaluation against a set of gold standard named entities shows that CRFs outperform SVMs. The best F-score with CRFs is 0.86 and for the SVMs is 0.64 as compared to a baseline of 0.60. 
We hypothesize that machine-learning algorithms (MLA) can classify completer and simulated suicide notes as well as mental health professionals (MHP). Five MHPs classiﬁed 66 simulated or completer notes; MLAs were used for the same task. Results: MHPs were accurate 71% of the time; using the sequential minimization optimization algorithm (SMO) MLAs were accurate 78% of the time. There was no signiﬁcant difference between the MLA and MPH classiﬁers. This is an important ﬁrst step in developing an evidence based suicide predictor for emergency department use. 
This paper presents a system1 for drug name identification and classification in biomedical texts.  the USAN Council and published in 20072. The affixes allow a specific classification of drugs on pharmacological families, which ULMS Semantic NetWork is unable to provide. 2 The System  
Clinicians write the reports in natural language which contains a large amount of informal medical term. Automating conversion of text into clinical terminologies allows reliable retrieval and analysis of the clinical notes. We have created an algorithm that maps medical expressions in clinical notes into a medical terminology. This algorithm indexes medical terms into an augmented lexicon. It performs lexical searches in text and finds the longest possible matches in the target terminology, SNOMED CT. The mapping system was run on a collection of 470,000 clinical notes from an Intensive Care Service (ICS). The evaluation on a small part of the corpus shows the precision is 70.4%. 
We developed a temporal annotation schema that provides a structured method to capture contextual and temporal features of clinical conditions found in clinical reports. In this poster we describe the elements of the annotation schema and provide results of an initial annotation study on a document set comprising six different types of clinical reports. 
This work proposes a case-based classifier to tackle the gene/protein mention problem in biomedical literature. The so called gene mention problem consists of the recognition of gene and protein entities in scientific texts. A classification process aiming at deciding if a term is a gene mention or not is carried out for each word in the text. It is based on the selection of the best or most similar case in a base of known and unknown cases. The approach was evaluated on several datasets for different organisms and results show the suitability of this approach for the gene mention problem. 
While there are several corpora which claim to have annotations for protein references, the heterogeneity between the annotations is recognized as an obstacle to develop expensive resources in a synergistic way. Here we present a series of experimental results which show the differences of protein mention annotations made to two corpora, GENIA and AImed. 
 plored. We describe an adaptive IE framework to characterize the activities involved in complex IE  Biomedical information extraction tasks are of-  tasks. Figure 1 depicts the adaptive information ex-  ten more complex and contain uncertainty at each step during problem solving processes. We present an adaptive information extraction framework and demonstrate how to explore uncertainty using feedback integration.  traction framework. This procedure emphasizes one important adap- tive step between the learning and application phases. If the IE result is not adequate, some adaptations are required:  
Computing precision and recall metrics for named entity tagging and resolution involves classifying text spans as true positives, false positives, or false negatives. There are many factors that make this classiﬁcation complicated for real world systems. We describe an evaluation system that attempts to control this complexity through a set of rules and a forward chaining inference engine. 
Type checking deﬁnes and constrains system output and intermediate representations. We report on the advantages of introducing multiple levels of type checking in deep parsing systems, even with untyped formalisms.  the FD for NUM(ber) and SPEC(iﬁer). NUM takes an atomic value, while SPEC takes an f-structure containing the features ADJUNCT, AQUANT, etc.  (1) a. NUM: - $ pl sg .  b. SPEC: -  [ADJUNCT AQUANT DET  NUMBER POSS QUANT SPEC-TYPE].  
Experiments in natural language processing and machine learning typically involve running a complicated network of programs to create, process, and evaluate data. Researchers often write one or more UNIX shell scripts to “glue” together these various pieces, but such scripts are suboptimal for several reasons. Without signiﬁcant additional work, a script does not handle recovering from failures, it requires keeping track of complicated ﬁlenames, and it does not support running processes in parallel. In this paper, we present zymake as a solution to all these problems. zymake scripts look like shell scripts, but have semantics similar to makeﬁles. Using zymake improves repeatability and scalability of running experiments, and provides a clean, simple interface for assembling components. A zymake script also serves as documentation for the complete workﬂow. We present a zymake script for a published set of NLP experiments, and demonstrate that it is superior to alternative solutions, including shell scripts and makeﬁles, while being far simpler to use than scientiﬁc grid computing systems. 
Natural language processing modules such as part-of-speech taggers, named-entity recognizers and syntactic parsers are commonly evaluated in isolation, under the assumption that artificial evaluation metrics for individual parts are predictive of practical performance of more complex language technology systems that perform practical tasks. Although this is an important issue in the design and engineering of systems that use natural language input, it is often unclear how the accuracy of an end-user application is affected by parameters that affect individual NLP modules. We explore this issue in the context of a specific task by examining the relationship between the accuracy of a syntactic parser and the overall performance of an information extraction system for biomedical text that includes the parser as one of its components. We present an empirical investigation of the relationship between factors that affect the accuracy of syntactic analysis, and how the difference in parse accuracy affects the overall system. 
This paper describes the structure of a test suite for evaluation of clinical question answering systems; presents several manually compiled resources found useful for test suite generation; and describes the adaptation of these resources for evaluation of a clinical question answering system. 
It is a widely accepted belief in natural language processing research that naturally occurring data is the best (and perhaps the only appropriate) data for testing text mining systems. This paper compares code coverage using a suite of functional tests and using a large corpus and ﬁnds that higher class, line, and branch coverage is achieved with structured tests than with even a very large corpus. 
In this paper, we describe our efforts to build on WORDNET resources, using WORDNET lexical data, the data format that it comes with and WORDNET’s software infrastructure in order to generate a biomedical extension of WORDNET, the BIOWORDNET. We began our efforts on the assumption that the software resources were stable and reliable. In the course of our work, it turned out that this belief was far too optimistic. We discuss the stumbling blocks that we encountered, point out an error in the WORDNET software with implications for research based on it, and conclude that building on the legacy of WORDNET data structures and its associated software might preclude sustainable extensions that go beyond the domain of general English. 
In this paper we describe a natural language generation system which takes as its input a set of assertions encoded as a semantic graph and outputs a data structure connecting the semantic graph to a text which expresses those assertions, encoded as a TAG syntactic tree. The scope of the system is restricted to controlled natural language, and this allows the generator to work within a tightly restricted domain of locality. We can exploit this feature of the system to ensure fast and efﬁcient generation, and also to make the generator reliable by providing a rapid algorithm which can exhaustively test at compile time the completeness of the linguistic resources with respect to the range of potential meanings. The system can be exported for deployment with a minimal build of the semantic and linguistic resources that is veriﬁed to ensure that no runtime errors will result from missing resources. The framework is targeted at using natural language generation technology to build semantic web applications where machine-readable information can be automatically expressed in natural language on demand. 
Training word alignment models on large corpora is a very time-consuming processes. This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process. One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology. Results show a near-linear speedup according to the number of CPUs used, and alignment quality is preserved. 
We present a description of the implementation of the open source decoder for statistical machine translation which has become popular with many researchers in SMT research. The goal of the project is to create an open, high quality phrase-based decoder which can reduce the time and barrier to entry for researchers wishing to do SMT research. We discuss the major design objective for the Moses decoder, its performance relative to other SMT decoders, and the steps we are taking to ensure that its success will continue. 
The morphology of the Arabic language is rich and complex; words are inflected to express variations in tense-aspect, person, number, and gender, while they may also appear with clitics attached to express possession on nouns, objects on verbs and prepositions, and conjunctions. Furthermore, Arabic script allows the omission of short vowel diacritics. For the Arabic language learner trying to understand non-diacritized text, the challenge when reading new vocabulary is first to isolate individual words within text tokens and then to determine the underlying lemma and root forms to look up the word in an Arabic dictionary. 
Our work in this area started as a research project but when L2F joined TecnoVoz, a Portuguese national consortium including Academia and Industry partners, our focus shifted to real-time professional solutions. The integration of our domain-independent Spoken Dialogue System (SDS) framework into commercial products led to a major reengineering process. This paper describes the changes that the framework went through and that deeply affected its entire architecture. The communication core was enhanced, the modules interfaces were redeﬁned for an easier integration, the SDS deployment process was optimized and the framework robustness was improved. The work was done according to software engineering guidelines and making use of design patterns. 
In current statistical machine translation (SMT), erroneous word reordering is one of the most serious problems. To resolve this problem, many word-reordering constraint techniques have been proposed. The inversion transduction grammar (ITG) is one of these constraints. In ITG constraints, targetside word order is obtained by rotating nodes of the source-side binary tree. In these node rotations, the source binary tree instance is not considered. Therefore, stronger constraints for word reordering can be obtained by imposing further constraints derived from the source tree on the ITG constraints. For example, for the source word sequence { a b c d }, ITG constraints allow a total of twenty-two target word orderings. However, when the source binary tree instance ((a b) (c d)) is given, our proposed ”imposing source tree on ITG” (IST-ITG) constraints allow only eight word orderings. The reduction in the number of word-order permutations by our proposed stronger constraints efﬁciently suppresses erroneous word orderings. In our experiments with IST-ITG using the NIST MT08 English-to-Chinese translation track’s data, the proposed method resulted in a 1.8-points improvement in character BLEU-4 (35.2 to 37.0) and a 6.2% lower CER (74.1 to 67.9%) compared with our baseline condition. 
We describe a scalable decoder for parsingbased machine translation. The decoder is written in JAVA and implements all the essential algorithms described in Chiang (2007): chart-parsing, m-gram language model integration, beam- and cube-pruning, and unique k-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. We also propose an algorithm to maintain equivalent language model states that exploits the back-off property of m-gram language models: instead of maintaining a separate state for each distinguished sequence of “state” words, we merge multiple states that can be made equivalent for language model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 
This paper presents an improved formally syntax-based SMT model, which is enriched by linguistically syntactic knowledge obtained from statistical constituent parsers. We propose a linguistically-motivated prior derivation model to score hypothesis derivations on top of the baseline model during the translation decoding. Moreover, we devise a fast training algorithm to achieve such improved models based on tree kernel methods. Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models, while both of them achieved signiﬁcant improvements over a state-of-theart phrase-based SMT system. 
We investigate translation modeling based on exponential estimates which generalize essential components of standard translation models. In application to a hierarchical phrasebased system the simplest generalization allows its models of lexical selection and reordering to be conditioned on arbitrary attributes of the source sentence and its annotation. Viewing these estimates as approximations of sentence-level probabilities motivates further elaborations that seek to exploit general syntactic and morphological patterns. Dimensionality control with 1 regularizers makes it possible to negotiate the tradeoff between translation quality and decoding speed. Putting together and extending several recent advances in phrase-based translation we arrive at a ﬂexible modeling framework that allows efﬁcient leveraging of monolingual resources and tools. Experiments with features derived from the output of Chinese and Arabic parsers and an Arabic lemmatizer show significant improvements over a strong baseline. 
The paper presents an extension of a dynamic programming (DP) decoder for phrase-based SMT (Koehn, 2004; Och and Ney, 2004) that tightly integrates POS-based re-order rules (Crego and Marino, 2006) into a left-to-right beam-search algorithm, rather than handling them in a pre-processing or re-order graph generation step. The novel decoding algorithm can handle tens of thousands of rules efﬁciently. An improvement over a standard phrase-based decoder is shown on an ArabicEnglish translation task with respect to translation accuracy and speed for large re-order window sizes. 
We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT. This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules. In decoding, the alternatives are scored based on the output word order, not the order of the input. Unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based SMT. On an EnglishDanish task, we achieve an absolute improvement in translation quality of 1.1 % BLEU. Manual evaluation supports the claim that the present approach is signiﬁcantly superior to previous approaches. 
We describe experiments on discriminating English to French phrase-based translations through the use of syntactic “coupling” features. Using a robust rule-based dependency parser, we parse both the English source and the French translation candidates from the nbest list returned by our phrase-based system; we compute for each candidate a number of coupling features, that is, values that depend on the amount of alignment between edges in the source and target structures, and discriminatively train the weights of these coupling features. We compare different feature combinations. Although the improvements in terms of automatic measures such as Bleu and Nist are inconclusive, an initial human assessment of the results appears to show certain qualitative improvements. 
This paper presents a method to integrate multiple reordering strategies in phrase-based statistical machine translation. Recently there has been much research effort in reordering problems in machine translation. State-of-the-art decoders incorporate sophisticated local reordering strategies, but there is little research on a unified approach to incorporate various kinds of reordering methods. We present a phrase-based decoder which easily allows multiple reordering schemes. We show how to use this framework to perform distance-based reordering and HIERO-style (Chiang 2005) hierarchical reordering. We also present two novel syntax-based reordering methods, one built on part-of-speech tags and the other based on parse trees. We will give experimental results using these relatively easy to implement methods on standard tests. 
We introduce a word alignment framework that facilitates the incorporation of syntax encoded in bilingual dependency tree pairs. Our model consists of two sub-models: an anchor word alignment model which aims to ﬁnd a set of high-precision anchor links and a syntaxenhanced word alignment model which focuses on aligning the remaining words relying on dependency information invoked by the acquired anchor links. We show that our syntaxenhanced word alignment approach leads to a 10.32% and 5.57% relative decrease in alignment error rate compared to a generative word alignment model and a syntax-proof discriminative word alignment model respectively. Furthermore, our approach is evaluated extrinsically using a phrase-based statistical machine translation system. The results show that SMT systems based on our word alignment approach tend to generate shorter outputs. Without length penalty, using our word alignments yields statistically signiﬁcant improvement in Chinese–English machine translation in comparison with the baseline word alignment. 
Syntax-based Machine Translation systems have recently become a focus of research with much hope that they will outperform traditional Phrase-Based Statistical Machine Translation (PBSMT). Toward this goal, we present a method for analyzing the morphosyntactic content of language from an Elicitation Corpus such as the one included in the LDC’s upcoming LCTL language packs. The presented method discovers a mapping between morphemes and linguistically relevant features. By providing this tool that can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved. We conclude by outlining how the resulting output can then be used in inducing a morphosyntactically feature-rich grammar for AVENUE, a modern syntax-based MT system. 
We describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages. The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees. We ﬁrst apply a newly developed algorithm for aligning parse-tree nodes between the two parallel trees. Next, we extract all aligned sub-sentential syntactic constituents from the parallel sentences, and create a syntax-based phrase-table. Finally, we treat the node alignments as tree decomposition points and extract from the corpus all possible synchronous parallel tree fragments. These are then converted into synchronous context-free rules. We describe the approach and analyze its application to Chinese-English parallel data. 
The treatment of ‘spurious’ words of source language is an important problem but often ignored in the discussion on phrase-based SMT. This paper explains why it is important and why it is not a trivial problem, and proposes three models to handle spurious source words. Experiments show that any source word deletion model can improve a phrase-based system by at least 1.6 BLEU points and the most sophisticated model improves by nearly 2 BLEU points. This paper also explores the impact of training data size and training data domain/genre on source word deletion. 
In this paper a new discriminative word alignment method is presented. This approach models directly the alignment matrix by a conditional random ﬁeld (CRF) and so no restrictions to the alignments have to be made. Furthermore, it is easy to add features and so all available information can be used. Since the structure of the CRFs can get complex, the inference can only be done approximately and the standard algorithms had to be adapted. In addition, different methods to train the model have been developed. Using this approach the alignment quality could be improved by up to 23 percent for 3 different language pairs compared to a combination of both IBM4alignments. Furthermore the word alignment was used to generate new phrase tables. These could improve the translation quality signiﬁcantly. 
Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powell’s method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically signiﬁcant gains when combined with both Powell’s method and coordinate descent. 
We present an extensive experimental study of a Statistical Machine Translation system, Moses (Koehn et al., 2007), from the point of view of its learning capabilities. Very accurate learning curves are obtained, by using high-performance computing, and extrapolations are provided of the projected performance of the system under different conditions. We provide a discussion of learning curves, and we suggest that: 1) the representation power of the system is not currently a limitation to its performance, 2) the inference of its models from ﬁnite sets of i.i.d. data is responsible for current performance limitations, 3) it is unlikely that increasing dataset sizes will result in signiﬁcant improvements (at least in traditional i.i.d. setting), 4) it is unlikely that novel statistical estimation methods will result in signiﬁcant improvements. The current performance wall is mostly a consequence of Zipf’s law, and this should be taken into account when designing a statistical machine translation system. A few possible research directions are discussed as a result of this investigation, most notably the integration of linguistic rules into the model inference phase, and the development of active learning procedures. 
Word alignments that violate syntactic correspondences interfere with the extraction of string-to-tree transducer rules for syntaxbased machine translation. We present an algorithm for identifying and deleting incorrect word alignment links, using features of the extracted rules. We obtain gains in both alignment quality and translation quality in Chinese-English and Arabic-English translation experiments relative to a GIZA++ union baseline. 
We describe two methods to improve SMT accuracy using shallow syntax information. First, we use chunks to reﬁne the set of word alignments typically used as a starting point in SMT systems. Second, we extend an N -grambased SMT system with chunk tags to better account for long-distance reorderings. Experiments are reported on an Arabic-English task showing signiﬁcant improvements. A human error analysis indicates that long-distance reorderings are captured effectively. 
We propose three enhancements to the treeto-string (TTS) transducer for machine translation: ﬁrst-level expansion-based normalization for TTS templates, a syntactic alignment framework integrating the insertion of unaligned target words, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 
This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information. 
 2 Translation via immediate transfer  In this article, we present MetaMorpho, a rule based machine translation system that was used to create MorphoLogic’s submission to the WMT08 shared Hungarian to English translation task. The architecture of MetaMorpho does not fit easily into traditional categories of rule based systems: the building blocks of its grammar are pairs of rules that describe source and target language structures in a parallel fashion and translated structures are created while parsing the input. 
This paper describes an initial version of a general purpose French/English statistical machine translation system. The main features of this system are the open-source Moses decoder, the integration of a bilingual dictionary and a continuous space target language model. We analyze the performance of this system on the test data of the WMT’08 evaluation. 
 2 Data and Basic Preprocessing  This paper present the University of Washington’s submission to the 2008 ACL SMT shared machine translation task. Two systems, for English-toSpanish and German-to-Spanish translation are described. Our main focus was on testing a novel boosting framework for N-best list reranking and on handling German morphology in the German-toSpanish system. While boosted N-best list reranking did not yield any improvements for this task, simplifying German morphology as part of the preprocessing step did result in signiﬁcant gains. 
This paper reports on the participation of the TALP Research Center of the UPC (Universitat Politècnica de Catalunya) to the ACL WMT 2008 evaluation campaign. This year’s system is the evolution of the one we employed for the 2007 campaign. Main updates and extensions involve linguistically motivated word reordering based on the reordering patterns technique. In addition, this system introduces a target language model, based on linguistic classes (Part-of-Speech), morphology reduction for an inﬂectional language (Spanish) and an improved optimization procedure. Results obtained over the development and test sets on Spanish to English (and the other way round) translations for both the traditional Europarl and a challenging News stories tasks are analyzed and commented. 
We describe the experiments of the UC Berkeley team on improving English-Spanish machine translation of news text, as part of the WMT’08 Shared Translation Task. We experiment with domain adaptation, combining a small in-domain news bi-text and a large out-of-domain one from the Europarl corpus, building two separate phrase translation models and two separate language models. We further add a third phrase translation model trained on a version of the news bi-text augmented with monolingual sentencelevel syntactic paraphrases on the sourcelanguage side, and we combine all models in a log-linear model using minimum error rate training. Finally, we experiment with different tokenization and recasing rules, achieving 35.09% Bleu score on the WMT’07 news test data when translating from English to Spanish, which is a sizable improvement over the highest Bleu score achieved on that dataset at WMT’07: 33.10% (in fact, by our system). On the WMT’08 English to Spanish news translation, we achieve 21.92%, which makes our team the second best on Bleu score. 
This paper describes the statistical machine translation systems submitted to the ACL-WMT 2008 shared translation task. Systems were submitted for two translation directions: English→Spanish and Spanish→English. Using sentence pair conﬁdence scores estimated with source and target language models, improvements are observed on the NewsCommentary test sets. Genre-dependent sentence pair conﬁdence score and integration of sentence pair conﬁdence score into phrase table are also investigated. 
The novel kernel regression model for SMT only demonstrated encouraging results on small-scale toy data sets in previous works due to the complexities of kernel methods. It is the ﬁrst time results based on the real-world data from the shared translation task will be reported at ACL 2008 Workshop on Statistical Machine Translation. This paper presents the key modules of our system, including the kernel ridge regression model, retrieval-based sparse approximation, the decoding algorithm, as well as language modeling issues under this framework. 
Our participation in the shared translation task at WMT-08 focusses on news translation from English to French. Our main goal is to contrast a baseline version of the phrase-based MATRAX system, with a version that incorporates syntactic “coupling” features in order to discriminate translations produced by the baseline system. We report results comparing different feature combinations. 
We present a new English→Czech machine translation system combining linguistically motivated layers of language description (as deﬁned in the Prague Dependency Treebank annotation scenario) with statistical NLP approaches. 
In this paper, we give a description of the machine translation system developed at DCU that was used for our participation in the evaluation campaign of the Third Workshop on Statistical Machine Translation at ACL 2008. We describe the modular design of our datadriven MT system with particular focus on the components used in this participation. We also describe some of the signiﬁcant modules which were unused in this task. We participated in the EuroParl task for the following translation directions: Spanish– English and French–English, in which we employed our hybrid EBMT-SMT architecture to translate. We also participated in the Czech– English News and News Commentary tasks which represented a previously untested language pair for our system. We report results on the provided development and test sets. 
Based on an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup, we present new results that show that this type of system combination can actually increase the lexical coverage of the resulting hybrid system, at least as far as this can be measured via BLEU score.  terms of BLEU score. A closer investigation revealed that the experiments had suffered from a couple of technical difﬁculties, such as mismatches in character encodings generated by different MT engines and similar problems. This motivated us to re-do these experiments in a somewhat more systematic way for this year’s shared translation task, paying the required attention to all the technical details and also to try it out on more language pairs.  
Confusion network decoding has been the most successful approach in combining outputs from multiple machine translation (MT) systems in the recent DARPA GALE and NIST Open MT evaluations. Due to the varying word order between outputs from different MT systems, the hypothesis alignment presents the biggest challenge in confusion network decoding. This paper describes an incremental alignment method to build confusion networks based on the translation edit rate (TER) algorithm. This new algorithm yields signiﬁcant BLEU score improvements over other recent alignment methods on the GALE test sets and was used in BBN’s submission to the WMT08 shared translation task. 
Previous studies have shown automatic evaluation metrics to be more reliable when compared against many human translations. However, multiple human references may not always be available. It is more common to have only a single human reference (extracted from parallel texts) or no reference at all. Our earlier work suggested that one way to address this problem is to train a metric to evaluate a sentence by comparing it against pseudo references, or imperfect “references” produced by off-the-shelf MT systems. In this paper, we further examine the approach both in terms of the training methodology and in terms of the role of the human and pseudo references. Our expanded experiments show that the approach generalizes well across multiple years and different source languages. 
Automatic evaluation of machine translation (MT) systems is an important research topic for the advancement of MT technology. Most automatic evaluation methods proposed to date are score-based: they compute scores that represent translation quality, and MT systems are compared on the basis of these scores. We advocate an alternative perspective of automatic MT evaluation based on ranking. Instead of producing scores, we directly produce a ranking over the set of MT systems to be compared. This perspective is often simpler when the evaluation goal is system comparison. We argue that it is easier to elicit human judgments of ranking and develop a machine learning approach to train on rank data. We compare this ranking method to a score-based regression method on WMT07 data. Results indicate that ranking achieves higher correlation to human judgments, especially in cases where ranking-speciﬁc features are used. 
In recent years, the quantity of parallel training data available for statistical machine translation has increased far more rapidly than the performance of individual computers, resulting in a potentially serious impediment to progress. Parallelization of the modelbuilding algorithms that process this data on computer clusters is fraught with challenges such as synchronization, data exchange, and fault tolerance. However, the MapReduce programming paradigm has recently emerged as one solution to these issues: a powerful functional abstraction hides system-level details from the researcher, allowing programs to be transparently distributed across potentially very large clusters of commodity hardware. We describe MapReduce implementations of two algorithms used to estimate the parameters for two word alignment models and one phrase-based translation model, all of which rely on maximum likelihood probability estimates. On a 20-machine cluster, experimental results show that our solutions exhibit good scaling characteristics compared to a hypothetical, optimally-parallelized version of current state-of-the-art single-core tools. 
This paper presents a technique for classdependent decoding for statistical machine translation (SMT). The approach differs from previous methods of class-dependent translation in that the class-dependent forms of all models are integrated directly into the decoding process. We employ probabilistic mixture weights between models that can change dynamically on a segment-by-segment basis depending on the characteristics of the source segment. The effectiveness of this approach is demonstrated by evaluating its performance on travel conversation data. We used the approach to tackle the translation of questions and declarative sentences using classdependent models. To achieve this, our system integrated two sets of models specifically built to deal with sentences that fall into one of two classes of dialog sentence: questions and declarations, with a third set of models built to handle the general class. The technique was thoroughly evaluated on data from 17 language pairs using 6 machine translation evaluation metrics. We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial. 
Chinese word segmentation (CWS) is a necessary step in Chinese-English statistical machine translation (SMT) and its performance has an impact on the results of SMT. However, there are many settings involved in creating a CWS system such as various speciﬁcations and CWS methods. This paper investigates the effect of these settings to SMT. We tested dictionarybased and CRF-based approaches and found there was no signiﬁcant difference between the two in the qualty of the resulting translations. We also found the correlation between the CWS F-score and SMT BLEU score was very weak. This paper also proposes two methods of combining advantages of different speciﬁcations: a simple concatenation of training data and a feature interpolation approach in which the same types of features of translation models from various CWS schemes are linearly interpolated. We found these approaches were very effective in improving quality of translations. 
Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood. In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance. We ﬁnd that other factors such as segmentation consistency and granularity of Chinese “words” can be more important for machine translation. Based on these ﬁndings, we implement methods inside a conditional random ﬁeld segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU. We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase. 
We describe course adaptation and development for teaching computational linguistics for the diverse body of undergraduate and graduate students the Department of Linguistics at the University of Texas at Austin. We also discuss classroom tools and teaching aids we have used and created, and we mention our efforts to develop a campus-wide computational linguistics program. 
We present the design of a professional master’s program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in ﬂexibility to accommodate students from more diverse backgrounds and with more diverse goals. 
In the course of the European Bologna accord on higher education, German universities have been reorganizing their traditional ”Magister” and ”Diplom” studies into modularized bachelor’s and master’s programs. This revision provides a chance to update the programs. In this paper we introduce the curriculum of a ﬁrst semester B.A. program in Computational Linguistics which was taught for the ﬁrst time last semester. In addition, we analyze the syllabi of four mandatory courses of the ﬁrst semester to identify overlapping content which led to redundancies. We suggest for future semesters to reorganize the schedules in a way that students encounter recurring topics iteratively in a constructive way. 
Discourse in and about computational linguistics depends on a shared body of knowledge. However, little content is shared across the introductory courses in this ﬁeld. Instead, they typically cover a diverse assortment of topics tailored to the capabilities of the students and the interests of the instructor. If the core body of knowledge could be agreed and incorporated into introductory courses several beneﬁts would ensue, such as the proliferation of instructional materials, software support, and extension modules building on a common foundation. This paper argues that it is worthwhile to articulate a core body of knowledge, and proposes a starting point based on the ACM Computer Science Curriculum. A variety of issues speciﬁc to the multidisciplinary nature of computational linguistics are explored. 
Many of the computational linguistics classes at Ohio State draw a diverse crowd of students, who bring different levels of preparation to the classroom. In the same classroom, we often get graduate and undergraduate students from Linguistics, Computer Science, Electrical Engineering and other departments; teaching the same material to all of these students presents an interesting challenge to the instructor. In this paper, I discuss some of the teaching strategies that I have employed to help integrate students in two classes on automatic speech recognition topics; strategies for a graduate seminar class and a standard “lecture” class are presented. Both courses make use of communal, online activities to facilitate interaction between students. 
 2 Background  This paper describes the evolution of a statistical NLP course, which I have been teaching every year for the past three years. The paper will focus on major changes made to the course (including the course design, assignments, and the use of discussion board) and highlight the lessons learned from this experience. 
This paper describes the design of a pilot research and educational effort at the University of Maryland centered around technologies for tackling Web-scale problems. In the context of a “cloud computing” initiative lead by Google and IBM, students and researchers are provided access to a computer cluster running Hadoop, an open-source Java implementation of Google’s MapReduce framework. This technology provides an opportunity for students to explore large-data issues in the context of a course organized around teams of graduate and undergraduate students, in which they tackle open research problems in the human language technologies. This design represents one attempt to bridge traditional instruction with real-world, large-data research challenges. 
The Natural Language Toolkit (NLTK) is widely used for teaching natural language processing to students majoring in linguistics or computer science. This paper describes the design of NLTK, and reports on how it has been used effectively in classes that involve different mixes of linguistics and computer science students. We focus on three key issues: getting started with a course, delivering interactive demonstrations in the classroom, and organizing assignments and projects. In each case, we report on practical experience and make recommendations on how to use NLTK to maximum effect. 
We describe our ﬁrst attempts to re-engineer the curriculum of our introductory NLP course by using two important building blocks: (1) Access to an easy-to-learn programming language and framework to build hands-on programming assignments with real-world data and corpora and, (2) Incorporation of interesting ideas from recent NLP research publications into assignment and examination problems. We believe that these are extremely important components of a curriculum aimed at a diverse audience consisting primarily of ﬁrstyear graduate students from both linguistics and computer science. Based on overwhelmingly positive student feedback, we ﬁnd that our attempts were hugely successful. 
This paper describes a Computational Linguistics course designed for Linguistics students. The course is structured around the architecture of a Spoken Dialogue System and makes extensive use of the dialogue system tools and examples available in the Regulus Open Source Project. Although only a quarter long course, students learn Computational Linguistics and programming sufﬁcient to build their own Spoken Dialogue System as a course project. 
Just as programming is the traditional introduction to computer science, writing grammars by hand is an excellent introduction to many topics in computational linguistics. We present and justify a well-tested introductory activity in which teams of mixed background compete to write probabilistic context-free grammars of English. The exercise brings together symbolic, probabilistic, algorithmic, and experimental issues in a way that is accessible to novices and enjoyable. 
Teaching Computational Linguistics is inherently multi-disciplinary and frequently poses challenges and provides opportunities in teaching to a student body with diverse educational backgrounds and goals. This paper describes the use of a computational environment (SIDGrid) that facilitates interdisciplinary instruction by providing support for students with little computational background as well as extending the scale of projects accessible to students with more advanced computational skills. The environment facilitates the use of hands-on exercises and is being applied to interdisciplinary instruction in Discourse and Dialogue.  tools and techniques to expand the range and scope of problems they can investigate. While there are many facets of these instructional challenges that must be addressed to support a successful course with a multi-disciplinary class and perspective, this paper focuses on the use and development of a computational environment to support laboratory exercises for students from diverse backgrounds. The framework aims to facilitate collaborative projects, reduce barriers of entry for students with little prior computational experience, and to provide access to large-scale distributed processing resources for students with greater computational expertise to expand the scope and scale of their projects and exercises.  
Most computer science majors at Northern Illinois University, whether at the B.S. or M.S. level, are professionally oriented. However, some of the best students are willing to try something completely different. NLP is a challenge for them because most have no background in linguistics or artificial intelligence, have little experience in reading traditional academic prose, and are unused to open-ended assignments with gray areas. In this paper I describe a syllabus for Introduction to NLP that concentrates on applications and motivates concepts through student experiments. Core materials include an introductory linguistics textbook, the Jurafsky and Martin textbook, the NLTK book, and a Python textbook.  from the web and in-class experiments. I explicitly teach the linguistics background that they need. 2 Student background I started from the following assumptions derived from several years of teaching Introduction to Artificial Intelligence and Introduction to NLP at NIU. Linguistic background: 1. Students have never studied linguistics. 2. Students are not familiar with the common syntactic constructions of English taught in traditional English grammar, and are often unsure about parts of speech. 3. Students have little experience with languages other than English.  
Computational modeling of human language processes is a small but growing subfield of computational linguistics. This paper describes a course that makes use of recent research in psychocomputational modeling as a framework to introduce a number of mainstream computational linguistics concepts to an audience of linguistics, cognitive science and computer science doctoral students. The emphasis on what I take to be the largely interdisciplinary nature of computational linguistics is particularly germane for the computer science students. Since 2002 the course has been taught three times under the auspices of the MA/PhD program in Linguistics at The City University of New York’s Graduate Center. A brief description of some of the students’ experiences after having taken the course is also provided. 
This paper argues for teaching computer science to linguists through a general course at the introductory graduate level whose goal is to prepare students of all backgrounds for collaborative computational research, especially in the sciences. We describe our work over the past three years in creating a model course in the area, called Computational Thinking. What makes this course distinctive is its combined emphasis on the formulation and solution of computational problems, strategies for interdisciplinary communication, and critical thinking about computational explanations. 
This paper describes a novel algorithm to dynamically set endpointing thresholds based on a rich set of dialogue features to detect the end of user utterances in a dialogue system. By analyzing the relationship between silences in user’s speech to a spoken dialogue system and a wide range of automatically extracted features from discourse, semantics, prosody, timing and speaker characteristics, we found that all features correlate with pause duration and with whether a silence indicates the end of the turn, with semantics and timing being the most informative. Based on these features, the proposed method reduces latency by up to 24% over a ﬁxed threshold baseline. Ofﬂine evaluation results were conﬁrmed by implementing the proposed algorithm in the Let’s Go system. 
Spoken and multimodal dialogue systems typically make use of conﬁdence scores to choose among (or reject) a speech recognizer’s Nbest hypotheses for a particular utterance. We argue that it is beneﬁcial to instead choose among a list of candidate system responses. We propose a novel method in which a conﬁdence score for each response is derived from a classiﬁer trained on acoustic and lexical features emitted by the recognizer, as well as features culled from the generation of the candidate response itself. Our responsebased method yields statistically signiﬁcant improvements in F-measure over a baseline in which hypotheses are chosen based on recognition conﬁdence scores only. 
We describe a novel n-best correction model that can leverage implicit user feedback (in the form of clicks) to improve performance in a multi-modal speech-search application. The proposed model works in two stages. First, the n-best list generated by the speech recognizer is expanded with additional candidates, based on confusability information captured via user click statistics. In the second stage, this expanded list is rescored and pruned to produce a more accurate and compact n-best list. Results indicate that the proposed n-best correction model leads to significant improvements over the existing baseline, as well as other traditional n-best rescoring approaches. 
In this paper we deﬁne agreement in terms of shared public commitments, and implicit agreement is conditioned on the semantics of the relational speech acts (e.g., Narration, Explanation) that each agent performs. We provide a consistent interpretation of disputes, and updating a logical form with the current utterance always involves extending it and not revising it, even if the current utterance denies earlier content. 
We explore the role of redundancy, both in anticipation of and in response to listener confusion, in task-oriented dialogue. We find that direction-givers provide redundant utterances in response to both verbal and non-verbal signals of listener confusion. We also examine the effects of prior acquaintance and visibility upon redundancy. As expected, givers use more redundant utterances overall, and more redundant utterances in response to listener questions, when communicating with strangers. We discuss our findings in relation to theories of redundancy, the balance of speaker and listener effort, and potential applications. 
A key problem for models of dialogue is to explain how semantic co-ordination in dialogue is achieved and sustained. This paper presents findings from a series of Maze Task experiments which are not readily explained by the primary co-ordination mechanisms of existing models. It demonstrates that alignment in dialogue is not simply an outcome of successful interaction, but a communicative resource exploited by interlocutors in converging on a semantic model. We argue this suggests mechanisms of co-ordination in dialogue which are of relevance for a general account of how semantic co-ordination is achieved. 
We introduce the Degrees of Grounding model, which deﬁnes the extent to which material being discussed in a dialogue has been grounded. This model has been developed and evaluated by a corpus analysis, and includes a set of types of evidence of understanding, a set of degrees of groundedness, a set of grounding criteria, and methods for identifying each of these. We describe how this model can be used for dialogue management. 
Grammar-based approaches to spoken language understanding are utilized to a great extent in industry, particularly when developers are confronted with data sparsity. In order to ensure wide grammar coverage, developers typically modify their grammars in an iterative process of deploying the application, collecting and transcribing user utterances, and adjusting the grammar. In this paper, we explore enhancing this iterative process by leveraging active learning with back-off grammars. Because the back-off grammars expand coverage of user utterances, developers have a safety net for deploying applications earlier. Furthermore, the statistics related to the back-off can be used for active learning, thus reducing the effort and cost of data transcription. In experiments conducted on a commercially deployed application, the approach achieved levels of semantic accuracy comparable to transcribing all failed utterances with 87% less transcriptions. 
We present the ADAMACH data centric dialog system, that allows to perform on- and offline mining of dialog context, speech recognition results and other system-generated representations, both within and across dialogs. The architecture implements a “fat pipeline” for speech and language processing. We detail how the approach integrates domain knowledge and evolving empirical data, based on a user study in the University Helpdesk domain. 
Humans produce speech incrementally and on-line as the dialogue progresses using information from several different sources in parallel. A dialogue system that generates output in a stepwise manner and not in preplanned syntactically correct sentences needs to signal how new dialogue contributions relate to previous discourse. This paper describes a data collection which is the foundation for an effort towards more humanlike language generation in DEAL, a spoken dialogue system developed at KTH. Two annotators labelled cue phrases in the corpus with high inter-annotator agreement (kappa coefficient 0.82). 
 2 Corpus Study  We look at the average frequency of contrastive connectives in the SPaRKy Restaurant Corpus with respect to realization ratings by human judges. We implement a discriminative n-gram ranker to model these ratings and analyze the resulting n-gram weights to determine if our ranker learns this distribution. Surprisingly, our ranker learns to avoid contrastive connectives. We look at possible explanations for this distribution, and recommend improvements to both the generator and ranker of the sentence plans/realizations. 
Signiﬁcant research efforts have been devoted to speech summarization, including automatic approaches and evaluation metrics. However, a fundamental problem about what summaries are for the speech data and whether humans agree with each other remains unclear. This paper performs an analysis of human annotated extractive summaries using the ICSI meeting corpus with an aim to examine their consistency and the factors impacting human agreement. In addition to using Kappa statistics and ROUGE scores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better deﬁne the speech summarization problem. 
 We present a method for resolving deﬁnite exophoric reference to visually shared objects that is based on a) an automatically learned, simple mapping of words to visual features (“visual word semantics”), b) an automatically learned, semantically-motivated utterance segmentation (“visual grammar”), and c) a procedure that, given an utterance, uses b) to combine a) to yield a resolution. We evaluated the method both on a pre-recorded corpus and in an online setting, where it performed with 81% (chance: 14%) and 66% accuracy, respectively. This is comparable to results reported in related work on simpler settings. 
This paper presents a novel framework for building symbol-level control modules of animated agents and robots having a spoken dialogue interface. It features distributed modules called experts each of which is specialized to perform certain kinds of tasks. A common interface that all experts must support is speciﬁed, and any kind of expert can be incorporated if it has the interface. Several modules running in parallel coordinate the experts by accessing them through the interface, so that the whole system can achieve ﬂexible control, such as interruption handling and parallel task execution. 
In this paper DiaGen is presented, a tool that provides support in generating code for embedded dialogue applications. By aid of it, the dialogue development process is speeded up considerably. At the same time it is guaranteed that only well-formed and well-deﬁned constructs are used. Having had its roots in the EU-funded project GEMINI, fundamental changes were necessary to adopt it to the requirements of the application environment. Additionally within this paper the basics of embedded speech dialogue systems are covered. 
This paper presents a coding protocol that allows naïve users to annotate dialogue transcripts for anaphora and ellipsis. Cohen's kappa statistic demonstrates that the protocol is sufficiently robust in terms of reliability. It is proposed that quantitative ellipsis data may be used as an index of mutual-engagement. Current and potential uses of ellipsis coding are described. 1. Introduction Spontaneously generated dialogue, whether naturally occurring or task-oriented, rarely sticks to accepted rules of grammar or even politeness. Interruptions, ungrammatical utterances and grunts or other noises are found in the majority of contributions in dialogue corpora. One reason for this is the ubiquitous use of ellipsis; the omission of words or phrases from a contribution which can be inferred or extracted from previous contributions. Ellipsis is optional; the full constituent could serve communication as well as the elliptical version. Where ellipsis occurs across speakers i.e., one participant makes (elliptical) use of another’s contribution, it provides a direct index of the mutualaccessibility of the current conversational context (cf. Healey et. al. 2007; Eshghi and Healey, 2007). In some cases elliptical contributions are obvious, as in the polar response 'yeah', signifying that a question has been heard, understood and consid-  ered; however, there are degrees of complexity that would seem to require a close understanding of what another participant is referring to. It is this issue of mutual-accessibility or 'grounding' that we propose can be investigated through the quantification of elliptical phenomena. These phenomena are, we propose, also related to the way referring expressions can contract over repeated use. (e.g. Schober and Clark, 1989; Wilkes-Gibbs and Clark, 1992). The approach taken in Clark et al.'s 'collaborative theory' is that as mutual understanding increases, dialogue contributions become shorter as referring terms become part of the common ground. Clark and Krych (2004) note that various elliptical phrases can be used to establish common ground, from continuers ('uh-huh', 'yeah') or assessments ('gosh') to establishing shared attention through deictic expressions such as 'this', 'that', 'here' and 'there'. Healey et al. (2007) demonstrated the basic concept and viability of quantifying ellipsis phenomena as a quantitative index of mutual-accessibility of context. They showed that the frequency of use of cross-speaker elliptical expressions in online chat varies systematically depending on whether communication is ‘local’ i.e. within a single chat room or ‘remote’. However, the coding of ellipsis in this study did not follow an explicit protocol. It relied mainly on the distinctions made by Fernandez et al. (2004) but specific measures of reliability and validity were not calculated.  96 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 96–99, Columbus, June 2008. c 2008 Association for Computational Linguistics  Figure 1. ‘Anaphora’ decision chart  Figure 2. ‘Answers’ decision chart  In this paper we present an ellipsis coding protocol that provides a set of coding categories and we report the inter-rater reliability scores that have been obtained with it. In order to simplify coding and increase reliability, categories suggested by Fernandez et al. have been collapsed into broader ones. It should be pointed out that we are not, in general, trying to produce an accurate or definitive analysis of ellipsis. The protocol is rather the product of contending with the compromise between robust coding categories and linguistic elegance. The categories presented here are generally ordered in terms of occurrence in order to assist the coder. A contribution to dialogue may contain more than one type of elliptical utterance; contributions are not assigned to one mutually exclusive category. Rather, coders are able to use the protocol to label any part of a dialogue that is elliptical. 2. The Ellipsis Protocol The protocol is designed as a tool for coding one aspect of dialogue, developed with the intention  that users with no specific knowledge of linguistics can use it. As can be seen from Figures 1-4, it consists of four binary branching decision trees that are applied to each contribution in an interaction. Full instructions for use of the protocol have also been written and are available from the authors. 3. Inter-rater reliability In order to demonstrate reliability between coders, two coders (one computer scientist, one psychologist) applied the ellipsis protocol to a sample of task oriented dialogue. This was taken from the HCRC Map Task corpus (Anderson et al, 1991); a series of dialogues in which one participant attempts to describe a route on a fictional map to another. The longest of these dialogues was chosen to be coded (transcript Q1NC1) which consisted of 446 turns and 5533 words. Cohen's kappa was calculated using the procedure outlined in Howell (1994); see Carletta (1996) for a discussion of the use of kappa in dialogue coding. Kappa in this instance was .81, which shows very high reliability, even by conservative standards (Krippendorff,  97  Figure 3. ‘Questions’ decision chart  Figure 4. ‘Statements’ decision chart  1980). Table 1 below presents a breakdown of the instances of categories that were agreed upon. Table 1 shows the total number and approximate percentage of agreements. Also given, '1.dis' and '2.dis' are the number of observed instances by coders one and two respectively identified but disputed for that particular category. The total number of elliptical or non-elliptical instances coded, from single words or phrases to entire turns was 624; of these, 100 (16%) were disagreed upon and 78 instances (12.5%) were agreed to contain no elliptical phenomena (no ellipsis disagreements = 50). Some categories have very low frequencies; however, previous work suggests that these categories are necessary. To some extent this table shows the limitations of the kappa statistic; coder agreement varies considerably across these categories.  Endophor Cataphor  Exaphor  Total % 1.dis 2.dis  119 19 12 10 Polar Answer  2 .03 1 3 Acknowledge  8 1.3 1 17 Prompted NSU Ans.  Total 113  78  
An attempt was made to statistically estimate proposals which survived the discussion to be incorporated in the ﬁnal agreement in an instance of a Japanese design conversation. Low level speech and vision features of hearer behaviors corresponding to aiduti, noddings and gaze were found to be a positive predictor of survival. The result suggests that non-linguistic hearer responses work as implicit proposal ﬁlters in consensus building, and could provide promising candidate features for the purpose of recognition and summarization of meeting events. 
Voice-Rate is an experimental dialog system through which a user can call to get product information. In this paper, we describe an optimal dialog management algorithm for Voice-Rate. Our algorithm uses a POMDP framework, which is probabilistic and captures uncertainty in speech recognition and user knowledge. We propose a novel method to learn a user knowledge model from a review database. Simulation results show that the POMDP system performs signiﬁcantly better than a deterministic baseline system in terms of both dialog failure rate and dialog interaction time. To the best of our knowledge, our work is the ﬁrst to show that a POMDP can be successfully used for disambiguation in a complex voice search domain like Voice-Rate. 
This paper investigates the claim that a dialogue manager modelled as a Partially Observable Markov Decision Process (POMDP) can achieve improved robustness to noise compared to conventional state-based dialogue managers. Using the Hidden Information State (HIS) POMDP dialogue manager as an exemplar, and an MDP-based dialogue manager as a baseline, evaluation results are presented for both simulated and real dialogues in a Tourist Information Domain. The results on the simulated data show that the inherent ability to model uncertainty, allows the POMDP model to exploit alternative hypotheses from the speech understanding system. The results obtained from a user trial show that the HIS system with a trained policy performed signiﬁcantly better than the MDP baseline. 
This paper proposes a probabilistic framework for spoken dialog management using dialog examples. To overcome the complexity problems of the classic partially observable Markov decision processes (POMDPs) based dialog manager, we use a frame-based belief state representation that reduces the complexity of belief update. We also used dialog examples to maintain a reasonable number of system actions to reduce the complexity of the optimizing policy. We developed weather information and car navigation dialog system that employed a frame-based probabilistic framework. This framework enables people to develop a spoken dialog system using a probabilistic approach without complexity problem of POMDP. 
This work proposes opinion frames as a representation of discourse-level associations that arise from related opinion targets and which are common in task-oriented meeting dialogs. We deﬁne the opinion frames and explain their interpretation. Additionally we present an annotation scheme that realizes the opinion frames and via human annotation studies, we show that these can be reliably identiﬁed. 
Argumentation is an emerging topic in the ﬁeld of human computer dialogue. In this paper we describe a novel approach to dialogue management that has been developed to achieve persuasion using a textual argumentation dialogue system. The paper introduces a layered management architecture that mixes task-oriented dialogue techniques with chatbot techniques to achieve better persuasiveness in the dialogue. 
An important task in automatic conversation understanding is the inference of social structure governing participant behavior. We explore the dependence between several social dimensions, including assigned role, gender, and seniority, and a set of low-level features descriptive of talkspurt deployment in a multiparticipant context. Experiments conducted on two large, publicly available meeting corpora suggest that our features are quite useful in predicting these dimensions, excepting gender. The classiﬁcation experiments we present exhibit a relative error rate reduction of 37% to 67% compared to choosing the majority class. 
We describe a process for automatically detecting decision-making sub-dialogues in transcripts of multi-party, human-human meetings. Extending our previous work on action item identiﬁcation, we propose a structured approach that takes into account the different roles utterances play in the decisionmaking process. We show that this structured approach outperforms the accuracy achieved by existing decision detection systems based on ﬂat annotations, while enabling the extraction of more ﬁne-grained information that can be used for summarization and reporting. 
We propose to use user simulation for testing during the development of a sophisticated dialog system. While the limited behaviors of the state-of-the-art user simulation may not cover important aspects in the dialog system testing, our proposed approach extends the functionality of the simulation so that it can be used at least for the early stage testing before the system reaches stable performance for evaluation involving human users. The proposed approach includes a set of evaluation measures that can be computed automatically from the interaction logs between the user simulator and the dialog system. We first validate these measures on human user dialogs using user satisfaction scores. We also build a regression model to estimate the user satisfaction scores using these evaluation measures. Then, we apply the evaluation measures on a simulated dialog corpus trained from the real user corpus. We show that the user satisfaction scores estimated from the simulated corpus are not statistically different from the real users’ satisfaction scores. 
Evaluating a dialogue system is seen as a major challenge within the dialogue research community. Due to the very nature of the task, most of the evaluation methods need a substantial amount of human involvement. Following the tradition in machine translation, summarization and discourse coherence modeling, we introduce the the idea of evaluation understudy for dialogue coherence models. Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models. This paper reports ﬁndings about the reliability of the information ordering task as applied to dialogues. We ﬁnd that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence. 
Improvements in the quality, usability and acceptability of spoken dialog systems can be facilitated by better evaluation methods. To support early and efﬁcient evaluation of dialog systems and their components, this paper presents a tripartite framework describing the evaluation problem. One part models the behavior of user and system during the interaction, the second one the perception and judgment processes taking place inside the user, and the third part models what matters to system designers and service providers. The paper reviews available approaches for some of the model parts, and indicates how anticipated improvements may serve not only developers and users but also researchers working on advanced dialog functions and features. 
A dialogue system can present itself and/or address the user as an active agent by means of linguistic constructions in personal style, or suppress agentivity by using impersonal style. We compare system evaluation judgments and input style alignment of users interacting with an in-car dialogue system generating output in personal vs. impersonal style. Although our results are consistent with earlier ﬁndings obtained with simulated systems, the effects are weaker. 
We present a development pipeline and associated algorithms designed to make grammarbased generation easier to deploy in implemented dialogue systems. Our approach realizes a practical trade-off between the capabilities of a system’s generation component and the authoring and maintenance burdens imposed on the generation content author for a deployed system. To evaluate our approach, we performed a human rating study with system builders who work on a common largescale spoken dialogue system. Our results demonstrate the viability of our approach and illustrate authoring/performance trade-offs between hand-authored text, our grammar-based approach, and a competing shallow statistical NLG technique. 
Research in Question Answering (QA) has been dominated by the TREC methodology of black-box system evaluation. This makes it difﬁcult to evaluate the effectiveness of individual components and requires human involvement. We have collected a set of answer locations within the AQUAINT corpus for a sample of TREC questions, in doing so we also analyse the ability of humans to retrieve answers. Our answer corpus allows us to track answer attenuation through a QA system. We use this method to evaluate the Pronto QA system (Bos et al., 2007). 
Sentiment classiﬁers attempt to determine whether a document expresses a generally positive or negative sentiment about its topic. Previous work has shown that overall performance can be improved by combining per-document classiﬁcations with information about agreement between documents. This paper explores approaches to sentiment classiﬁcation of U.S Congressional ﬂoor debate transcripts that use a model for incorporating multiple sources of information about agreements between speakers. An empirical evaluation demonstrates accuracy improvements over previously published results. 
Good performance on Text Classiﬁcation (TC) tasks depends on effective and statistically signiﬁcant features. Typically, the simple bag-of-words representation is widely used because unigram counts are more likely to be significant compared to more compound features. This research explores the idea that the major cause of poor performance of some complex features is sparsity. Syntactic features are usually complex being made up of both lexical and syntactic information. This paper introduces the use of a class of automatically extractable, syntactic features to the TC task. These features are based on subtrees of parse trees. As such, a large number of these features are generated. Our results suggest that generating a diverse set of these features may help in increasing performance. Partial abstraction of the features also seems to boost performance by counteracting sparsity. We will show that various subsets of our syntactic features do outperform the bag-of-words representation alone. 
The Lexical Access (LA) problem in Computer Science aims to match a phoneme sequence produced by the user to a correctly spelled word in a lexicon, with minimal human intervention and in a short amount of time. Lexical Access is useful in the case where the user knows the spoken form of a word but cannot guess its written form or where the users best guess is inappropriate for look-up in a standard dictionary or by traditional computer spellcheckers. Previous approaches to this problem have attempted to match user-generated phoneme sequences to phoneme sequences in dictionary entries and then output the stored spelling for the sequences. Our approach includes a phoneme-to-grapheme conversion step followed by spelling-to-spelling alignment. This ensures that more distortion to the data can be handled as well as reducing error rates and also allows us to combine the Lexical Access problem with a related task in Natural Language Processing, that of spelling Out of Vocabulary (OOV) items. We call this combination the Hybrid Lexical Access (HLA) Problem. The performance of our system on this task is assessed by the percentage of correct word matches in the output after edit distance is performed to ﬁnd the closest match to a stored item. 
The limitations of existing data sets for training parsers has led to a need for additional data. However, the cost of manually annotating the amount and range of data required is prohibitive. For a number of simple facts like those sought in Question Answering, we compile a corpus of sentences extracted from the Web that contain the fact keywords. We use a state-of-the-art parser to parse these sentences, constraining the analysis of the more complex sentences using information from the simpler sentences. This allows us to automatically create additional annotated sentences which we then use to augment our existing training data. 
In this paper we present interface techniques that support the writing process of machine-oriented controlled natural languages which are well-deﬁned and tractable fragments of English that can be translated unambiguously into a formal target language. Since these languages have grammatical and lexical restrictions, it is important to provide a text editor that assists the writing process by using lookahead information derived from the grammar. We will discuss the requirements to such a lookahead text editor and introduce the semantic wiki AceWiki as an application where this technology plays an important role. We investigate two different approaches how lookahead information can be generated dynamically while a text is written and compare the runtimes and practicality of these approaches in detail. 
Manually maintaining comprehensive databases of multi-word expressions, for example Verb-Particle Constructions (VPCs), is infeasible. We describe a new type level classiﬁer for potential VPCs, which uses information in the Google Web1T corpus to perform a simple linguistic constituency test. Speciﬁcally, we consider the fronting test, comparing the frequencies of the two possible orderings of the given verb and particle. Using only a small set of queries for each verb-particle pair, the system was able to achieve an F-score of 75.7% in our evaluation while processing thousands of queries a second. 
Many workplace tasks are managed through email communication, involving the exchange of requests and commitments. Our aim is to build a tool that can automatically identify and manage such requests and commitments. A detailed analysis of real data, however, reveals a range of interesting edge cases that make even human annotation of training data difﬁcult. In this paper, as an important step in the development of annotation guidelines for wider use in the growing email processing community, we identify eight categories of problematic data and propose how they should be handled in annotation and extraction tasks. 
Query expansion has long been suggested as a technique for dealing with word mismatch problem in information retrieval. In this paper, we describe a novel query expansion method which incorporates text mining techniques into query expansion for improving Chinese information retrieval performance. Unlike most of the existing query expansion strategies which generally select indexing terms from the top N retrieved documents and use them to expand the query, in our proposed method, we apply text mining techniques to find patterns from the retrieved documents which contain relevant terms to the query terms, then use these relevant terms which can be indexing terms or indexing term patterns to expand the query. The experiment with NTCIR-5 collection shows apparent improvement in both precision and recall. 
Event reference identiﬁcation is often treated as a sentence level classiﬁcation task. However, several different event references can occur within a single sentence. We present a set of experiments involving real world event reference identiﬁcation at the word level in newspaper and newswire documents, addressing the issue of effective text representation for classiﬁcation of events using support vector machines. Our ﬁnal system achieved an F-score of 0.764, signiﬁcantly exceeding that of our baseline system. Additionally we achieved a marginally higher performance than a more complex comparable system. 
This paper describes an experiment that attempts to automatically map English words and concepts, derived from the Princeton WordNet, to their Indonesian analogues appearing in a widely-used Indonesian dictionary, using Latent Semantic Analysis (LSA). A bilingual semantic model is derived from an EnglishIndonesian parallel corpus. Given a particular word or concept, the semantic model is then used to identify its neighbours in a highdimensional semantic space. Results from various experiments indicate that for bilingual word mapping, LSA is consistently outperformed by the basic vector space model, i.e. where the full-rank word-document matrix is applied. We speculate that this is due to the fact that the ‘smoothing’ effect LSA has on the worddocument matrix, whilst very useful for revealing implicit semantic patterns, blurs the cooccurrence information that is necessary for establishing word translations. 
We present the Weighted Mutual Exclusion Bootstrapping (WMEB) algorithm for simultaneously extracting precise semantic lexicons and templates for multiple categories. WMEB is capable of extracting larger lexicons with higher precision than previous techniques, successfully reducing semantic drift by incorporating new weighting functions and a cumulative template pool while still enforcing mutual exclusion between the categories. We compare WMEB and two state-of-theart approaches on the Web 1T corpus and two large biomedical literature collections. WMEB is more efﬁcient and scalable, and we demonstrate that it signiﬁcantly outperforms the other approaches on the noisy web corpus and biomedical text. 
We develop a data set of Malay lexemes labelled with count classiﬁers, that are attested in raw or lemmatised corpora. A maximum entropy classiﬁer based on simple, languageinspeciﬁc features generated from context tokens achieves about 50% F-score, or about 65% precision when a suite of binary classiﬁers is built to aid multi-class prediction of headword nouns. Surprisingly, numeric features are not observed to aid classiﬁcation. This system represents a useful step for semisupervised lexicography across a range of languages. 
Statistical named entity recognisers require costly hand-labelled training data and, as a result, most existing corpora are small. We exploit Wikipedia to create a massive corpus of named entity annotated text. We transform Wikipedia’s links into named entity annotations by classifying the target articles into common entity types (e.g. person, organisation and location). Comparing to MUC, CONLL and BBN corpora, Wikipedia generally performs better than other cross-corpus train/test pairs. 
Reasoning about how much content to generate when space is limited presents an increasingly important challenge for generation systems, as the diversity of potential delivery channels continues to grow. The problem is multi-facetted: the generated text must fit into the allocated space, the space available must be well utilised, and the resulting text must convey its intended message, and be coherent, well structured and balanced. To address this problem, we use a discourse planning approach. Our system reasons about the discourse structure to decide how much content to realise. In this paper, we present two algorithms that perform this reasoning and analyse their effectiveness. 
This paper presents our efforts at developing an Indonesian morphological analyser that provides a detailed analysis of the rich affixation process1. We model Indonesian morphology using a two-level morphology approach, decomposing the process into a set of morphotactic and morphophonemic rules. These rules are modelled as a network of finite state transducers and implemented using xfst and lexc. Our approach is able to handle reduplication, a non-concatenative morphological process. 
Although punctuation is pervasive in written text, their treatment in parsers and corpora is often second-class. We examine the treatment of commas in CCGbank, a wide-coverage corpus for Combinatory Categorial Grammar (CCG), reanalysing its comma structures in order to eliminate a class of redundant rules, obtaining a more consistent treebank. We then eliminate these rules from C&C, a wide-coverage statistical CCG parser, obtaining a 37% increase in parsing speed on the standard CCGbank test set and a considerable reduction in memory consumed, without affecting parser accuracy. 
When we describe an object in order to enable a listener to identify it, we often do so by indicating the location of that object with respect to other objects in a scene. This requires the use of a relational referring expression; while these are very common, they are relatively unexplored in work on referring expression generation. In this paper, we describe an experiment in which we gathered data on how humans use relational referring expressions in simple scenes, with the aim of identifying the factors that make a difference to the ways in which humans construct referring expressions.  generation of relational references is a relatively unexplored task. The few algorithms that address this task (Dale and Haddock (1991), Gardent (2002), Krahmer and Theune (2002), Varges (2005), Kelleher and Kruiff (2005, 2006)) typically adopt fairly simple approaches: they only consider spatial relations if it is not possible to fully distinguish the target referent from the surrounding objects in any other way, or they treat them in exactly the same as nonrelational properties. As acknowledged by some of this work, this creates additional problems such as inﬁnite regress and the inclusion of relations without regard for the properties of the landmarks that are associated with them.  
While the intuition that morphological preprocessing of languages in various applications can be beneﬁcial appears to be often true, especially in the case of morphologically richer languages, it is not always the case. Previous work on translation between Nordic languages, including the morphologically rich Finnish, found that morphological analysis and preprocessing actually led to a decrease in translation quality below that of the unprocessed baseline. In this paper we investigate the proposition that the effect on translation quality depends on the kind of morphological preprocessing; and in particular that a speciﬁc kind of morphological preprocessing before translation could improve translation quality, a preprocessing that ﬁrst transforms the source language to look more like the target, adapted from work on preprocessing via syntactically motivated reordering. We show that this is indeed the case in translating from Finnish, and that the results hold for different target languages and different morphological analysers. 
This is a demonstration of a voice dialer, implemented as a partially observable Markov decision process (POMDP). A realtime graphical display shows the POMDP’s probability distribution over different possible dialog states, and shows how system output is generated and selected. The system demonstrated here includes several recent advances, including an action selection mechanism which uniﬁes a hand-crafted controller and reinforcement learning. The voice dialer itself is in use today in AT&T Labs and receives daily calls. 
We describe an application that generates web pages for research institutions by summarising terms extracted from individual researchers’ publication titles. Our online demo covers all researchers and research groups in the Computer Laboratory, University of Cambridge. We also present a novel visualisation interface for browsing collaborations. 
Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort, yet there is very limited availability of off-the shelf tools for researchers whose interests are not in coreference, or for researchers who want to concentrate on a speciﬁc aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of the Soon et al. (2001) proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classiﬁers. 
This paper introduced the main features of the UAM CorpusTool, software for human and semi-automatic annotation of text and images. The demonstration will show how to set up an annotation project, how to annotate text files at multiple annotation levels, how to automatically assign tags to segments matching lexical patterns, and how to perform crosslayer searches of the corpus. 
We will demonstrate a novel graphical interface for correcting search errors in the output of a speech recognizer. This interface allows the user to visualize the word lattice by “pulling apart” regions of the hypothesis to reveal a cloud of words simlar to the “tag clouds” popular in many Web applications. This interface is potentially useful for dictation on portable touchscreen devices such as the Nokia N800 and other mobile Internet devices. 
Yawat1 is a tool for the visualization and manipulation of word- and phrase-level alignments of parallel text. Unlike most other tools for manual word alignment, it relies on dynamic markup to visualize alignment relations, that is, markup is shown and hidden depending on the current mouse position. This reduces the visual complexity of the visualization and allows the annotator to focus on one item at a time. For a bird’s-eye view of alignment patterns within a sentence, the tool is also able to display alignments as alignment matrices. In addition, it allows for manual labeling of alignment relations with customizable tag sets. Different text colors are used to indicate which words in a given sentence pair have already been aligned, and which ones still need to be aligned. Tag sets and color schemes can easily be adapted to the needs of speciﬁc annotation projects through conﬁguration ﬁles. The tool is implemented in JavaScript and designed to run as a web application. 
In this type-II demo, we introduce SIDE1 (the Summarization Integrated Development Environment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect summary for all purposes. Rather, the quality of a summary is subjective, task dependent, and possibly specific to a user. The SIDE framework allows users flexibility in determining what they find more useful in a summary, both in terms of structure and content. As an educational tool, it has been successfully user tested by a class of 21 students in a graduate course on Summarization and Personal Information Management. 
We will demonstrate the ModelTalker Voice Recorder (MT Voice Recorder) – an interface system that lets individuals record and bank a speech database for the creation of a synthetic voice. The system guides users through an automatic calibration process that sets pitch, amplitude, and silence. The system then prompts users with both visual (text-based) and auditory prompts. Each recording is screened for pitch, amplitude and pronunciation and users are given immediate feedback on the acceptability of each recording. Users can then rerecord an unacceptable utterance. Recordings are automatically labeled and saved and a speech database is created from these recordings. The system’s intention is to make the process of recording a corpus of utterances relatively easy for those inexperienced in linguistic analysis. Ultimately, the recorded corpus and the resulting speech database is used for concatenative synthetic speech, thus allowing individuals at home or in clinics to create a synthetic voice in their own voice. The interface may prove useful for other purposes as well. The system facilitates the recording and labeling of large corpora of speech, making it useful for speech and linguistic research, and it provides immediate feedback on pronunciation, thus making it useful as a clinical learning tool.  
This paper describes the online demo of the QuALiM Question Answering system. While the system actually gets answers from the web by querying major search engines, during presentation answers are supplemented with relevant passages from Wikipedia. We believe that this additional information improves a user’s search experience.  
Distributional similarity has been widely used to capture the semantic relatedness of words in many NLP tasks. However, various parameters such as similarity measures must be handtuned to make it work effectively. Instead, we propose a novel approach to synonym identiﬁcation based on supervised learning and distributional features, which correspond to the commonality of individual context types shared by word pairs. Considering the integration with pattern-based features, we have built and compared ﬁve synonym classiﬁers. The evaluation experiment has shown a dramatic performance increase of over 120% on the F-1 measure basis, compared to the conventional similarity-based classiﬁcation. On the other hand, the pattern-based features have appeared almost redundant. 
The aim of this research is to provide a principled account of the generation of embedded constructions (called parentheticals) and to implement the results in a natural language generation system. Parenthetical constructions are frequently used in texts written in a good writing style and have an important role in text understanding. We propose a framework to model the rhetorical properties of parentheticals based on a corpus study and develop a uniﬁed natural language generation architecture which integrates syntax, semantics, rhetorical and document structure into a complex representation, which can be easily extended to handle parentheticals. 
Many applications in NLP, such as questionanswering and summarization, either require or would greatly beneﬁt from the knowledge of when an event occurred. Creating an effective algorithm for identifying the activity time of an event in news is difﬁcult in part because of the sparsity of explicit temporal expressions. This paper describes a domain-independent machine-learning based approach to assign activity times to events in news. We demonstrate that by applying topic models to text, we are able to cluster sentences that describe the same event, and utilize the temporal information within these event clusters to infer activity times for all sentences. Experimental evidence suggests that this is a promising approach, given evaluations performed on three distinct news article sets against the baseline of assigning the publication date. Our approach achieves 90%, 88.7%, and 68.7% accuracy, respectively, outperforming the baseline twice. 
A Named Entity Recognizer (NER) generally has worse performance on machine translated text, because of the poor syntax of the MT output and other errors in the translation. As some tagging distinctions are clearer in the source, and some in the target, we tried to integrate the tag information from both source and target to improve target language tagging performance, especially recall. In our experiments with Chinese-to-English MT output, we first used a simple merge of the outputs from an ET (Entity Translation) system and an English NER system, getting an absolute gain of 7.15% in F-measure, from 73.53% to 80.68%. We then trained an MEMM module to integrate them more discriminatively, and got a further average gain of 2.74% in F-measure, from 80.68% to 83.42%. 
Machine learning methods have been extensively employed in developing MT evaluation metrics and several studies show that it can help to achieve a better correlation with human assessments. Adopting the regression SVM framework, this paper discusses the linguistic motivated feature formulation strategy. We argue that “blind” combination of available features does not yield a general metrics with high correlation rate with human assessments. Instead, certain simple intuitive features serve better in establishing the regression SVM evaluation model. With six features selected, we show evidences to support our view through a few experiments in this paper. 
The focus of this study is positive feedback in one-on-one tutoring, its computational modeling, and its application to the design of more effective Intelligent Tutoring Systems. A data collection of tutoring sessions in the domain of basic Computer Science data structures has been carried out. A methodology based on multiple regression is proposed, and some preliminary results are presented. A prototype Intelligent Tutoring System on linked lists has been developed and deployed in a collegelevel Computer Science class. 
In morphologically rich languages such as Arabic, the abundance of word forms resulting from increased morpheme combinations is signiﬁcantly greater than for languages with fewer inﬂected forms (Kirchhoff et al., 2006). This exacerbates the out-of-vocabulary (OOV) problem. Test set words are more likely to be unknown, limiting the effectiveness of the model. The goal of this study is to use the regularities of Arabic inﬂectional morphology to reduce the OOV problem in that language. We hope that success in this task will result in a decrease in word error rate in Arabic automatic speech recognition. 
Even though collaboration in peer learning has been shown to have a positive impact for students, there has been little research into collaborative peer learning dialogues. We analyze such dialogues in order to derive a model of knowledge co-construction that incorporates initiative and the balance of initiative. This model will be embedded in an artiﬁcial agent that will collaborate with students. 
This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all-word disambiguation. We explore using contextual information from the Uniﬁed Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM-WSD dataset and with SenseClusters using conﬂated data from the 2005 Medline Baseline. 
This paper presents a system capable of automatically acquiring subcategorization frames (SCFs) for French verbs from the analysis of large corpora. We applied the system to a large newspaper corpus (consisting of 10 years of the French newspaper ’Le Monde’) and acquired subcategorization information for 3267 verbs. The system learned 286 SCF types for these verbs. From the analysis of 25 representative verbs, we obtained 0.82 precision, 0.59 recall and 0.69 F-measure. These results are comparable with those reported in recent related work. 
We present the development and tuning of a topic-adapted language model for word prediction, which improves keystroke savings over a comparable baseline. We outline our plans to develop and integrate style adaptations, building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts. 
This paper proposes a hierarchical text categorization (TC) approach to encoding free-text clinical notes with ICD-9-CM codes. Preliminary experimental result on the 2007 Computational Medicine Challenge data shows a hierarchical TC system has achieved a microaveraged F1 value of 86.6, which is comparable to the performance of state-of-the-art ﬂat classiﬁcation systems. 
This paper studies the impact of written language variations and the way it affects the capitalization task over time. A discriminative approach, based on maximum entropy models, is proposed to perform capitalization, taking the language changes into consideration. The proposed method makes it possible to use large corpora for training. The evaluation is performed over newspaper corpora using different testing periods. The achieved results reveal a strong relation between the capitalization performance and the elapsed time between the training and testing data periods. 
An incremental dependency parser’s probability model is entered as a predictor in a linear mixed-effects model of German readers’ eye-ﬁxation durations. This dependencybased predictor improves a baseline that takes into account word length, n-gram probability, and Cloze predictability that are typically applied in models of human reading. This improvement obtains even when the dependency parser explores a tiny fraction of its search space, as suggested by narrow-beam accounts of human sentence processing such as Garden Path theory. 
We consider the problem of answering complex questions that require inferencing and synthesizing information from multiple documents and can be seen as a kind of topicoriented, informative multi-document summarization. The stochastic, graph-based method for computing the relative importance of textual units (i.e. sentences) is very successful in generic summarization. In this method, a sentence is encoded as a vector in which each component represents the occurrence frequency (TF*IDF) of a word. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. In this paper, we study the impact of syntactic and shallow semantic information in the graph-based method for answering complex questions. 
Current research in automatic subjectivity analysis deals with various kinds of subjective statements involving human attitudes and emotions. While all of them are related to subjectivity, these statements usually touch on multiple dimensions such as non-objectivity1, uncertainty, vagueness, non-objective measurability, imprecision, and ambiguity, which are inherently different. This paper discusses the differences and relations of six dimensions of subjectivity. Conceptual and linguistic characteristics of each dimension will be demonstrated under different contexts. 
This paper describes an extractive summarizer for educational science content called COGENT. COGENT extends MEAD based on strategies elicited from an empirical study with domain and instructional experts. COGENT implements a hybrid approach integrating both domain independent sentence scoring features and domain-aware features. Initial evaluation results indicate that COGENT outperforms existing summarizers and generates summaries that closely resemble those generated by human experts. 
The variation in speech due to dialect is a factor which significantly impacts speech system performance. In this study, we investigate effective methods of combining acoustic and language information to take advantage of (i) speaker based acoustic traits as well as (ii) content based word selection across the text sequence. For acoustics, a GMM based system is employed and for text based dialect classification, we proposed n-gram language models combined with Latent Semantic Analysis (LSA) based dialect classifiers. The performance of the individual classifiers is established for the three dialect family case (DC rates vary from 69.1%-72.4%). The final combined system achieved a DC accuracy of 79.5% and significantly outperforms the baseline acoustic classifier with a relative improvement of 30%, confirming that an integrated dialect classification system is effective for American, British and Australian dialects. 
Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that ﬁnding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of ﬁnding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efﬁcient. 
We propose a novel method for extracting semantic information about a verb's arguments and apply it to Verb Sense Disambiguation (VSD). We contrast this method with two popular approaches to retrieving this informa tion and show that it improves the performance of our VSD system and outperforms the other two approaches 
Data driven POS tagging has achieved good performance for English, but can still lag behind linguistic rule based taggers for morphologically complex languages, such as Icelandic. We extend a statistical tagger to handle ﬁne grained tagsets and improve over the best Icelandic POS tagger. Additionally, we develop a case tagger for non-local case and gender decisions. An error analysis of our system suggests future directions. 
Current re-ranking algorithms for machine translation rely on log-linear models, which have the potential problem of underﬁtting the training data. We present BoostedMERT, a novel boosting algorithm that uses Minimum Error Rate Training (MERT) as a weak learner and builds a re-ranker far more expressive than log-linear models. BoostedMERT is easy to implement, inherits the efﬁcient optimization properties of MERT, and can quickly boost the BLEU score on N-best re-ranking tasks. In this paper, we describe the general algorithm and present preliminary results on the IWSLT 2007 Arabic-English task. 
Research on coreference resolution and summarization has modeled the way entities are realized as concrete phrases in discourse. In particular there exist models of the noun phrase syntax used for discourse-new versus discourse-old referents, and models describing the likely distance between a pronoun and its antecedent. However, models of discourse coherence, as applied to information ordering tasks, have ignored these kinds of information. We apply a discourse-new classiﬁer and pronoun coreference algorithm to the information ordering task, and show signiﬁcant improvements in performance over the entity grid, a popular model of local coherence. 
A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a ﬁnal assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classiﬁer over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classiﬁer to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the b3 scorer, and up to 16.5% using cluster f-measure. 
In this paper we build user simulations of older and younger adults using a corpus of interactions with a Wizard-of-Oz appointment scheduling system. We measure the quality of these models with standard metrics proposed in the literature. Our results agree with predictions based on statistical analysis of the corpus and previous ﬁndings about the diversity of older people’s behaviour. Furthermore, our results show that these metrics can be a good predictor of the behaviour of different types of users, which provides evidence for the validity of current user simulation evaluation metrics. 
 This paper introduces a new method for identifying named-entity (NE) transliterations within bilingual corpora. Current state-of-theart approaches usually require annotated data and relevant linguistic knowledge which may not be available for all languages. We show how to effectively train an accurate transliteration classiﬁer using very little data, obtained automatically. To perform this task, we introduce a new active sampling paradigm for guiding and adapting the sample selection process. We also investigate how to improve the classiﬁer by identifying repeated patterns in the training data. We evaluated our approach using English, Russian and Hebrew corpora. 
 2 Related Work  We present four techniques for online handling of Out-of-Vocabulary words in Phrasebased Statistical Machine Translation. The techniques use spelling expansion, morphological expansion, dictionary term expansion and proper name transliteration to reuse or extend a phrase table. We compare the performance of these techniques and combine them. Our results show a consistent improvement over a state-of-the-art baseline in terms of BLEU and a manual error analysis. 
A process that attempts to solve abbreviation ambiguity is presented. Various contextrelated features and statistical features have been explored. Almost all features are domain independent and language independent. The application domain is Jewish Law documents written in Hebrew. Such documents are known to be rich in ambiguous abbreviations. Various implementations of the one sense per discourse hypothesis are used, improving the features with new variants. An accuracy of 96.09% has been achieved by SVM. 
Traditional Active Learning (AL) techniques assume that the annotation of each datum costs the same. This is not the case when annotating sequences; some sequences will take longer than others. We show that the AL technique which performs best depends on how cost is measured. Applying an hourly cost model based on the results of an annotation user study, we approximate the amount of time necessary to annotate a given sentence. This model allows us to evaluate the effectiveness of AL sampling methods in terms of time spent in annotation. We acheive a 77% reduction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Treebank. More signiﬁcantly, we make the case for measuring cost in assessing AL methods. 
This paper presents an approach to text categorization that i) uses no machine learning and ii) reacts on-the-ﬂy to unknown words. These features are important for categorizing Blog articles, which are updated on a daily basis and ﬁlled with newly coined words. We categorize 600 Blog articles into 12 domains. As a result, our categorization method achieved an accuracy of 94.0% (564/600).  
In spoken dialogue systems, Partially Observable Markov Decision Processes (POMDPs) provide a formal framework for making dialogue management decisions under uncertainty, but efﬁciency and interpretability considerations mean that most current statistical dialogue managers are only MDPs. These MDP systems encode uncertainty explicitly in a single state representation. We formalise such MDP states in terms of distributions over POMDP states, and propose a new dialogue system architecture (Mixture Model POMDPs) which uses mixtures of these distributions to efﬁciently represent uncertainty. We also provide initial evaluation results (with real users) for this architecture. 
In this paper we describe recent improvements to components and methods used in our statistical machine translation system for ChineseEnglish used in the January 2008 GALE evaluation. Main improvements are results of consistent data processing, larger statistical models and a POS-based word reordering approach. 
Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are signiﬁcantly better (in terms of BLEU and TER) than those in tercom-based confusion networks. 
A solution to the problem of homograph (words with multiple distinct meanings) identification is proposed and evaluated in this paper. It is demonstrated that a mixture model based framework is better suited for this task than the standard classification algorithms – relative improvement of 7% in F1 measure and 14% in Cohen’s kappa score is observed. 
Relation extraction is the task of finding semantic relations between two entities from text. In this paper, we propose a novel feature-based Chinese relation extraction approach that explicitly defines and explores nine positional structures between two entities. We also suggest some correction and inference mechanisms based on relation hierarchy and co-reference information etc. The approach is effective when evaluated on the ACE 2005 Chinese data set. 
Chinese characters that are similar in their pronunciations or in their internal structures are useful for computer-assisted language learning and for psycholinguistic studies. Although it is possible for us to employ imagebased methods to identify visually similar characters, the resulting computational costs can be very high. We propose methods for identifying visually similar Chinese characters by adopting and extending the basic concepts of a proven Chinese input method--Cangjie. We present the methods, illustrate how they work, and discuss their weakness in this paper. 
Question answering communities such as Yahoo! Answers have emerged as a popular alternative to general-purpose web search. By directly interacting with other participants, information seekers can obtain speciﬁc answers to their questions. However, user success in obtaining satisfactory answers varies greatly. We hypothesize that satisfaction with the contributed answers is largely determined by the asker’s prior experience, expectations, and personal preferences. Hence, we begin to develop personalized models of asker satisfaction to predict whether a particular question author will be satisﬁed with the answers contributed by the community participants. We formalize this problem, and explore a variety of content, structure, and interaction features for this task using standard machine learning techniques. Our experimental evaluation over thousands of real questions indicates that indeed it is beneﬁcial to personalize satisfaction predictions when sufﬁcient prior user history exists, signiﬁcantly improving accuracy over a “one-size-ﬁts-all” prediction model. 
Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. Here we apply this technique to parser adaptation. In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts. This achieves an f -score of 84.3% on a standard test set of biomedical abstracts from the Genia corpus. This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set). 
This paper describes a syntactic representation for modeling speech repairs. This representation makes use of a right corner transform of syntax trees to produce a tree representation in which speech repairs require very few special syntax rules, making better use of training data. PCFGs trained on syntax trees using this model achieve high accuracy on the standard Switchboard parsing task. 
The omnipresence of unknown words is a problem that any NLP component needs to address in some form. While there exist many established techniques for dealing with unknown words in the realm of POS-tagging, for example, guessing unknown words’ semantic properties is a less-explored area with greater challenges. In this paper, we study the semantic ﬁeld of sentiment and propose ﬁve methods for assigning prior sentiment polarities to unknown words based on known sentiment carriers. Tested on 2000 cases, the methods mirror human judgements closely in three- and twoway polarity classiﬁcation tasks, and reach accuracies above 63% and 81%, respectively. 
Natural Language Processing (NLP) for Information Retrieval has always been an interesting and challenging research area. Despite the high expectations, most of the results indicate that successfully using NLP is very complex. In this paper, we show how Support Vector Machines along with kernel functions can effectively represent syntax and semantics. Our experiments on question/answer classiﬁcation show that the above models highly improve on bag-of-words on a TREC dataset. 
We investigate the tasks of general morphological tagging, diacritization, and lemmatization for Arabic. We show that for all tasks we consider, both modeling the lexeme explicitly, and retuning the weights of individual classiﬁers for the speciﬁc task, improve the performance. 
We use an EM algorithm to learn user models in a spoken dialog system. Our method requires automatically transcribed (with ASR) dialog corpora, plus a model of transcription errors, but does not otherwise need any manual transcription effort. We tested our method on a voice-controlled telephone directory application, and show that our learned models better replicate the true distribution of user actions than those trained by simpler methods and are very similar to user models estimated from manually transcribed dialogs. 
This paper proposes a novel method to extract named entities including unfamiliar words which do not occur or occur few times in a training corpus using a large unannotated corpus. The proposed method consists of two steps. The ﬁrst step is to assign the most similar and familiar word to each unfamiliar word based on their context vectors calculated from a large unannotated corpus. After that, traditional machine learning approaches are employed as the second step. The experiments of extracting Japanese named entities from IREX corpus and NHK corpus show the effectiveness of the proposed method. 
We investigate elaborative summarisation, where the aim is to identify supplementary information that expands upon a key fact. We envisage such summaries being useful when browsing certain kinds of (hyper-)linked document sets, such as Wikipedia articles or repositories of publications linked by citations. For these collections, an elaborative summary is intended to provide additional information on the linking anchor text. Our contribution in this paper focuses on identifying and exploring a real task in which summarisation is situated, realised as an In-Browser tool. We also introduce a neighbourhood scoring heuristic as a means of scoring matches to relevant passages of the document. In a preliminary evaluation using this method, our summarisation system scores above our baselines and achieves a recall of 57% annotated gold standard sentences. 
Lyric-based song sentiment classification seeks to assign songs appropriate sentiment labels such as light-hearted and heavy-hearted. Four problems render vector space model (VSM)-based text classification approach ineffective: 1) Many words within song lyrics actually contribute little to sentiment; 2) Nouns and verbs used to express sentiment are ambiguous; 3) Negations and modifiers around the sentiment keywords make particular contributions to sentiment; 4) Song lyric is usually very short. To address these problems, the sentiment vector space model (s-VSM) is proposed to represent song lyric document. The preliminary experiments prove that the sVSM model outperforms the VSM model in the lyric-based song sentiment classification task. 
A well-recognized limitation of research on supervised sentence compression is the dearth of available training data. We propose a new and bountiful resource for such training data, which we obtain by mining the revision history of Wikipedia for sentence compressions and expansions. Using only a fraction of the available Wikipedia data, we have collected a training corpus of over 380,000 sentence pairs, two orders of magnitude larger than the standardly used Ziff-Davis corpus. Using this newfound data, we propose a novel lexicalized noisy channel model for sentence compression, achieving improved results in grammaticality and compression rate criteria with a slight decrease in importance.  ishingly rare. Indeed, most work on sentence compression has used the Ziff-Davis corpus (Knight and Marcu, 2000), which consists of a mere 1067 sentence pairs. While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. Our contribution in this paper is twofold. First, we solve the data sparsity issue by showing that abundant sentence compressions can be extracted from Wikipedia’s revision history. Second, we use this data to validate the channel model approach for text compression, and improve upon it by creating a novel fully lexicalized compression model. Our model improves grammaticality and compression rate with only a slight decrease in importance.  
Frequency counts from very large corpora, such as the Web 1T dataset, have recently become available for language modeling. Omission of low frequency n-gram counts is a practical necessity for datasets of this size. Naive implementations of standard smoothing methods do not realize the full potential of such large datasets with missing counts. In this paper I present a new smoothing algorithm that combines the Dirichlet prior form of (Mackay and Peto, 1995) with the modiﬁed back-off estimates of (Kneser and Ney, 1995) that leads to a 31% perplexity reduction on the Brown corpus compared to a baseline implementation of Kneser-Ney discounting. 
This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations. The method yields a model capable of matching events with an F-measure of 66.5%.  training and test instance in a feature space. Conceptually, our features are of three diﬀerent varieties. This section describes the ﬁrst two kinds, which we call “low-level” features, in that they attempt to capture how much of the basic information of an event e is present in a sentence s.  
In this paper, we propose a linguistically annotated reordering model for BTG-based statistical machine translation. The model incorporates linguistic knowledge to predict orders for both syntactic and non-syntactic phrases. The linguistic knowledge is automatically learned from source-side parse trees through an annotation algorithm. We empirically demonstrate that the proposed model leads to a signiﬁcant improvement of 1.55% in the BLEU score over the baseline reordering model on the NIST MT-05 Chinese-to-English translation task. 
In this paper, we report on a set of initial results for English-to-Arabic Statistical Machine Translation (SMT). We show that morphological decomposition of the Arabic source is beneﬁcial, especially for smaller-size corpora, and investigate different recombination techniques. We also report on the use of Factored Translation Models for Englishto-Arabic translation. 
Word and n-gram posterior probabilities estimated on N-best hypotheses have been used to improve the performance of statistical machine translation (SMT) in a rescoring framework. In this paper, we extend the idea to estimate the posterior probabilities on N-best hypotheses for translation phrase-pairs, target language n-grams, and source word reorderings. The SMT system is self-enhanced with the posterior knowledge learned from Nbest hypotheses in a re-decoding framework. Experiments on NIST Chinese-to-English task show performance improvements for all the strategies. Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 BLEU score on NIST-2003 set, and 0.64 on NIST2005 set, respectively. 
This paper presents a partial matching strategy for phrase-based statistical machine translation (PBSMT). Source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases. The advantage of this method is that it can alleviate the data sparseness problem if the amount of bilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically signiﬁcant improvements on both small and large corpora. 
Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from ﬂuent, large-vocabulary speech. 
Cognitive theories of dialogue hold that entrainment, the automatic alignment between dialogue partners at many levels of linguistic representation, is key to facilitating both production and comprehension in dialogue. In this paper we examine novel types of entrainment in two corpora—Switchboard and the Columbia Games corpus. We examine entrainment in use of high-frequency words (the most common words in the corpus), and its association with dialogue naturalness and ﬂow, as well as with task success. Our results show that such entrainment is predictive of the perceived naturalness of dialogues and is signiﬁcantly correlated with task success; in overall interaction ﬂow, higher degrees of entrainment are associated with more overlaps and fewer interruptions. 
While speech recognition systems have come a long way in the last thirty years, there is still room for improvement. Although readily available, these systems are sometimes inaccurate and insufficient. The research presented here outlines a technique called Distributed Listening which demonstrates noticeable improvements to existing speech recognition methods. The Distributed Listening architecture introduces the idea of multiple, parallel, yet physically separate automatic speech recognizers called listeners. Distributed Listening also uses a piece of middleware called an interpreter. The interpreter resolves multiple interpretations using the Phrase Resolution Algorithm (PRA). These efforts work together to increase the accuracy of the transcription of spoken utterances.  ers, with each recognizer having its own input source (Gilbert, 2005). Each recognizer is a listener. Once input is collected from the listeners, one machine, the interpreter, processes all of the input (see figure 1). To process the input, a phrase resolution algorithm is used. This approach is analogous to a crime scene with multiple witnesses (the listeners) and a detective (the interpreter) who pieces together the stories of the witnesses using his/her knowledge of crime scenes to form a hypothesis of the actual event. Each witness will have a portion of the story that is the same as the other witnesses. It is up to the detective to fill in the blanks. With Distributed Listening, the process is very similar. Each listener will have common recognition results and the interpreter will use the phrase resolution algorithm to resolve conflicts.  
Finding temporal and causal relations is crucial to understanding the semantic structure of a text. Since existing corpora provide no parallel temporal and causal annotations, we annotated 1000 conjoined event pairs, achieving inter-annotator agreement of 81.2% on temporal relations and 77.8% on causal relations. We trained machine learning models using features derived from WordNet and the Google N-gram corpus, and they outperformed a variety of baselines, achieving an F-measure of 49.0 for temporals and 52.4 for causals. Analysis of these models suggests that additional data will improve performance, and that temporal information is crucial to causal relation identiﬁcation. 
Automatic extraction of collocations from large corpora has been the focus of many research efforts. Most approaches concentrate on improving and combining known lexical association measures. In this paper, we describe a genetic programming approach for evolving new association measures, which is not limited to any speciﬁc language, corpus, or type of collocation. Our preliminary experimental results show that the evolved measures outperform three known association measures. 
This paper introduces a machine learning method based on bayesian networks which is applied to the mapping between deep semantic representations and lexical semantic resources. A probabilistic model comprising Minimal Recursion Semantics (MRS) structures and lexicalist oriented semantic features is acquired. Lexical semantic roles enriching the MRS structures are inferred, which are useful to improve the accuracy of deep semantic parsing. Verb classes inference was also investigated, which, together with lexical semantic information provided by VerbNet and PropBank resources, can be substantially beneﬁcial to the parse disambiguation task. 
We show that question-based sentence fusion is a better deﬁned task than generic sentence fusion (Q-based fusions are shorter, display less variety in length, yield more identical results and have higher normalized Rouge scores). Moreover, we show that in a QA setting, participants strongly prefer Q-based fusions over generic ones, and have a preference for union over intersection fusions. 
In this paper we present research in which we apply (i) the kind of intrinsic evaluation metrics that are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and ﬁnd that there are no signiﬁcant correlations between intrinsic and extrinsic evaluation measures for this task. 
Automatic summarization evaluation is critical to the development of summarization systems. While ROUGE has been shown to correlate well with human evaluation for content match in text summarization, there are many characteristics in multiparty meeting domain, which may pose potential problems to ROUGE. In this paper, we carefully examine how well the ROUGE scores correlate with human evaluation for extractive meeting summarization. Our experiments show that generally the correlation is rather low, but a signiﬁcantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disﬂuencies and speaker information, especially when evaluating system-generated summaries. 
We present a fast query-based multi-document summarizer called FastSum based solely on word-frequency features of clusters, documents and topics. Summary sentences are ranked by a regression SVM. The summarizer does not use any expensive NLP techniques such as parsing, tagging of names or even part of speech information. Still, the achieved accuracy is comparable to the best systems presented in recent academic competitions (i.e., Document Understanding Conference (DUC)). Because of a detailed feature analysis using Least Angle Regression (LARS), FastSum can rely on a minimal set of features leading to fast processing times: 1250 news documents in 60 seconds. 
Earlier work in parsing Arabic has speculated that attachment to construct state constructions decreases parsing performance. We make this speculation precise and deﬁne the problem of attachment to construct state constructions in the Arabic Treebank. We present the ﬁrst statistics that quantify the problem. We provide a baseline and the results from a ﬁrst attempt at a discriminative learning procedure for this task, achieving 80% accuracy. 
This paper investigates transforms of split dependency grammars into unlexicalised context-free grammars annotated with hidden symbols. Our best unlexicalised grammar achieves an accuracy of 88% on the Penn Treebank data set, that represents a 50% reduction in error over previously published results on unlexicalised dependency parsing. 
Computing confidence scores for applications, such as dialogue system, information retrieving and extraction, is an active research area. However, its focus has been primarily on computing word-, concept-, or utterance-level confidences. Motivated by the need from sophisticated dialogue systems for more effective dialogs, we generalize the confidence annotation to all the subtrees, the first effort in this line of research. The other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores. Using Conditional Maximum Entropy (CME) classifier with all the selected features, we reached an annotation error rate of 26.0% in the SWBD corpus, compared with a subtree error rate of 41.91%, a closely related benchmark with the Charniak parser from (Kahn et al., 2005). 
 2 Background  We present a robust parser which is trained on a treebank of ungrammatical sentences. The treebank is created automatically by modifying Penn treebank sentences so that they contain one or more syntactic errors. We evaluate an existing Penn-treebank-trained parser on the ungrammatical treebank to see how it reacts to noise in the form of grammatical errors. We re-train this parser on the training section of the ungrammatical treebank, leading to an signiﬁcantly improved performance on the ungrammatical test sets. We show how a classiﬁer can be used to prevent performance degradation on the original grammatical data. 
Current statistical speech translation approaches predominantly rely on just text transcripts and do not adequately utilize the rich contextual information such as conveyed through prosody and discourse function. In this paper, we explore the role of context characterized through dialog acts (DAs) in statistical translation. We demonstrate the integration of the dialog acts in a phrase-based statistical translation framework, employing 3 limited domain parallel corpora (Farsi-English, Japanese-English and Chinese-English). For all three language pairs, in addition to producing interpretable DA enriched target language translations, we also obtain improvements in terms of objective evaluation metrics such as lexical selection accuracy and BLEU score. 
 there are many practical needs, as shown in Figure 1.  Speaker’s intention prediction modules can be widely used as a pre-processor for reducing the search space of an automatic speech recognizer. They also can be used as a preprocessor for generating a proper sentence in a dialogue system. We propose a statistical model to predict speakers’ intentions by using multi-level features. Using the multi-level features (morpheme-level features, discourselevel features, and domain knowledge-level features), the proposed model predicts speakers’ intentions that may be implicated in next utterances. In the experiments, the proposed model showed better performances (about 29% higher accuracies) than the previous model. Based on the experiments, we found that the proposed multi-level features are very effective in speaker’s intention prediction. 
Active learning is a machine learning approach to achieving high-accuracy with a small amount of labels by letting the learning algorithm choose instances to be labeled. Most of previous approaches based on discriminative learning use the margin for choosing instances. We present a method for incorporating conﬁdence into the margin by using a newly introduced online learning algorithm and show empirically that conﬁdence improves active learning. 
We present a fast, space efﬁcient and nonheuristic method for calculating the decision function of polynomial kernel classiﬁers for NLP applications. We apply the method to the MaltParser system, resulting in a Java parser that parses over 50 sentences per second on modest hardware without loss of accuracy (a 30 time speedup over existing methods). The method implementation is available as the open-source splitSVM Java library.  memory, or resort to heuristics – computing only an approximation to the real decision function. This work aims at speeding up the decision function computation for low-degree polynomial kernel classiﬁers while using only a modest amount of memory and still computing the exact function. This is achieved by taking into account the Zipﬁan nature of natural language data, and structuring the computation accordingly. On a sample application (replacing the libsvm classiﬁer used by MaltParser (Nivre et al., 2006) with our own), we observe a speedup factor of 30 in parsing time.  
Underspeciﬁcation-based algorithms for processing partially disambiguated discourse structure must cope with extremely high numbers of readings. Based on previous work on dominance graphs and weighted tree grammars, we provide the ﬁrst possibility for computing an underspeciﬁed discourse description and a best discourse representation efﬁciently enough to process even the longest discourses in the RST Discourse Treebank.  used as an underspeciﬁcation formalism in semantics (Koller et al., 2008). We show how to compute RTGs for discourse from dominance-based underspeciﬁed representations more efﬁciently (by a typical factor of 100) than before. Furthermore, we show how weighted RTGs can be used to represent constraints and preferences on the discourse structure. Taking all these results together, we show for the ﬁrst time how the globally optimal discourse representation based on some preference model can be computed efﬁciently from an UDR.  
Supervised word sense disambiguation requires training corpora that have been tagged with word senses, which begs the question of which word senses to tag with. The default choice has been WordNet, with its broad coverage and easy accessibility. However, concerns have been raised about the appropriateness of its fine-grained word senses for WSD. WSD systems have been far more successful in distinguishing coarsegrained senses than fine-grained ones (Navigli, 2006), but does that approach neglect necessary meaning differences? Recent psycholinguistic evidence seems to indicate that closely related word senses may be represented in the mental lexicon much like a single sense, whereas distantly related senses may be represented more like discrete entities. These results suggest that, for the purposes of WSD, closely related word senses can be clustered together into a more general sense with little meaning loss. The current paper will describe this psycholinguistic research and its implications for automatic word sense disambiguation. 
Splitting compound words has proved to be useful in areas such as Machine Translation, Speech Recognition or Information Retrieval (IR). Furthermore, real-time IR systems (such as search engines) need to cope with noisy data, as user queries are sometimes written quickly and submitted without review. In this paper we apply a state-of-the-art procedure for German decompounding to other compounding languages, and we show that it is possible to have a single decompounding model that is applicable across languages. 
This paper addresses a new task in sentiment classification, called multi-domain sentiment classification, that aims to improve performance through fusing training data from multiple domains. To achieve this, we propose two approaches of fusion, feature-level and classifier-level, to use training data from multiple domains simultaneously. Experimental studies show that multi-domain sentiment classification using the classifier-level approach performs much better than single domain classification (using the training data individually). 
Researchers typically evaluate word prediction using keystroke savings, however, this measure is not straightforward. We present several complications in computing keystroke savings which may affect interpretation and comparison of results. We address this problem by developing two gold standards as a frame for interpretation. These gold standards measure the maximum keystroke savings under two different approximations of an ideal language model. The gold standards additionally narrow the scope of deﬁciencies in a word prediction system. 
This paper presents a MapReduce algorithm for computing pairwise document similarity in large document collections. MapReduce is an attractive framework because it allows us to decompose the inner products involved in computing document similarity into separate multiplication and summation stages in a way that is well matched to efﬁcient disk access patterns across several machines. On a collection consisting of approximately 900,000 newswire articles, our algorithm exhibits linear growth in running time and space in terms of the number of documents. 
In this paper we propose a domainindependent text segmentation method, which consists of three components. Latent Dirichlet allocation (LDA) is employed to compute words semantic distribution, and we measure semantic similarity by the Fisher kernel. Finally global best segmentation is achieved by dynamic programming. Experiments on Chinese data sets with the technique show it can be effective. Introducing latent semantic information, our algorithm is robust on irregular-sized segments. 
In this paper, we describe a system by which the multilingual characteristics of Wikipedia can be utilized to annotate a large corpus of text with Named Entity Recognition (NER) tags requiring minimal human intervention and no linguistic expertise. This process, though of value in languages for which resources exist, is particularly useful for less commonly taught languages. We show how the Wikipedia format can be used to identify possible named entities and discuss in detail the process by which we use the Category structure inherent to Wikipedia to determine the named entity type of a proposed entity. We further describe the methods by which English language data can be used to bootstrap the NER process in other languages. We demonstrate the system by using the generated corpus as training sets for a variant of BBN's Identifinder in French, Ukrainian, Spanish, Polish, Russian, and Portuguese, achieving overall F-scores as high as 84.7% on independent, human-annotated corpora, comparable to a system trained on up to 40,000 words of human-annotated newswire. 
We present an automatic approach to determining whether a pronoun in text refers to a preceding noun phrase or is instead nonreferential. We extract the surrounding textual context of the pronoun and gather, from a large corpus, the distribution of words that occur within that context. We learn to reliably classify these distributions as representing either referential or non-referential pronoun instances. Despite its simplicity, experimental results on classifying the English pronoun it show the system achieves the highest performance yet attained on this important task. 
A new approach to large-scale information extraction exploits both Web documents and query logs to acquire thousands of opendomain classes of instances, along with relevant sets of open-domain class attributes at precision levels previously obtained only on small-scale, manually-assembled classes. 
Linguistically annotated treebanks play an essential part in the modern computational linguistics. The more complex the treebanks become, the more sophisticated tools are required for using them, namely for searching in the data. We study linguistic phenomena annotated in the Prague Dependency Treebank 2.0 and create a list of requirements these phenomena set on a search tool, especially on its query language. 
This paper presents a comparative evaluation of several state-of-the-art English parsers based on different frameworks. Our approach is to measure the impact of each parser when it is used as a component of an information extraction system that performs protein-protein interaction (PPI) identiﬁcation in biomedical papers. We evaluate eight parsers (based on dependency parsing, phrase structure parsing, or deep parsing) using ﬁve different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identiﬁcation accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-speciﬁc data. 
We propose an automatic machine translation (MT) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences. Unlike most metrics, we compute a similarity score between items across the two sentences. We then ﬁnd a maximum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence. This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison, such as n-grams, dependency relations, etc. When evaluated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic MT evaluation metrics that were evaluated during the workshop. 
The third PASCAL Recognizing Textual Entailment Challenge (RTE-3) contained an optional task that extended the main entailment task by requiring a system to make three-way entailment decisions (entails, contradicts, neither) and to justify its response. Contradiction was rare in the RTE-3 test set, occurring in only about 10% of the cases, and systems found accurately detecting it difﬁcult. Subsequent analysis of the results shows a test set must contain many more entailment pairs for the three-way decision task than the traditional two-way task to have equal conﬁdence in system comparisons. Each of six human judges representing eventual end users rated the quality of a justiﬁcation by assigning “understandability” and “correctness” scores. Ratings of the same justiﬁcation across judges differed signiﬁcantly, signaling the need for a better characterization of the justiﬁcation task. 
Phrase-based decoding produces state-of-theart translations with no regard for syntax. We add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence. The constraint allows the decoder to employ arbitrary, non-syntactic phrases, but ensures that those phrases are translated in an order that respects the source tree’s structure. In this way, we target the phrasal decoder’s weakness in order modeling, without affecting its strengths. To further increase ﬂexibility, we incorporate cohesion as a decoder feature, creating a soft constraint. The resulting cohesive, phrase-based decoder is shown to produce translations that are preferred over non-cohesive output in both automatic and human evaluations. 
In this work, the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming for a balanced precision and recall. We present a generic phrase training algorithm which is parameterized with feature functions and can be optimized jointly with the translation engine to directly maximize the end-to-end system performance. Multiple data-driven feature functions are proposed to capture the quality and conﬁdence of phrases and phrase pairs. Experimental results demonstrate consistent and signiﬁcant improvement over the widely used method that is based on word alignment matrix only. 
Measure words in Chinese are used to indicate the count of nouns. Conventional statistical machine translation (SMT) systems do not perform well on measure word generation due to data sparseness and the potential long distance dependency between measure words and their corresponding head words. In this paper, we propose a statistical model to generate appropriate measure words of nouns for an English-to-Chinese SMT system. We model the probability of measure word generation by utilizing lexical and syntactic knowledge from both source and target sentences. Our model works as a post-processing procedure over output of statistical machine translation systems, and can work with any SMT system. Experimental results show our method can achieve high precision and recall in measure word generation. 
We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs. The structured space of a synchronous grammar is a natural ﬁt for phrase pair probability estimation, though the search space can be prohibitively large. Therefore we explore efﬁcient algorithms for pruning this space that lead to empirically effective results. Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to signiﬁcant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 
We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree. The language model is applied by means of an N-best rescoring step, which allows to directly measure the performance gains relative to the baseline system without rescoring. To demonstrate that our approach is feasible and beneﬁcial for non-trivial broad-domain speech recognition tasks, we applied it to a simpliﬁed German broadcast-news transcription task. We report a signiﬁcant reduction in word error rate compared to a state-of-the-art baseline system. 
Written documents created through dictation differ signiﬁcantly from a true verbatim transcript of the recorded speech. This poses an obstacle in automatic dictation systems as speech recognition output needs to undergo a fair amount of editing in order to turn it into a document that complies with the customary standards. We present an approach that attempts to perform this edit from recognized words to ﬁnal document automatically by learning the appropriate transformations from example documents. This addresses a number of problems in an integrated way, which have so far been studied independently, in particular automatic punctuation, text segmentation, error correction and disﬂuency repair. We study two different learning methods, one based on rule induction and one based on a probabilistic sequence model. Quantitative evaluation shows that the probabilistic method performs more accurately. 
Grounded language models represent the relationship between words and the non-linguistic context in which they are said. This paper describes how they are learned from large corpora of unlabeled video, and are applied to the task of automatic speech recognition of sports video. Results show that grounded language models improve perplexity and word error rate over text based language models, and further, support video information retrieval better than human generated speech transcriptions. 
This paper presents a new unsupervised algorithm (WordEnds) for inferring word boundaries from transcribed adult conversations. Phone ngrams before and after observed pauses are used to bootstrap a simple discriminative model of boundary marking. This fast algorithm delivers high performance even on morphologically complex words in English and Arabic, and promising results on accurate phonetic transcriptions with extensive pronunciation variation. Expanding training data beyond the traditional miniature datasets pushes performance numbers well above those previously reported. This suggests that WordEnds is a viable model of child language acquisition and might be useful in speech understanding. 
Query expansion is an effective technique to improve the performance of information retrieval systems. Although hand-crafted lexical resources, such as WordNet, could provide more reliable related terms, previous studies showed that query expansion using only WordNet leads to very limited performance improvement. One of the main challenges is how to assign appropriate weights to expanded terms. In this paper, we re-examine this problem using recently proposed axiomatic approaches and ﬁnd that, with appropriate term weighting strategy, we are able to exploit the information from lexical resources to signiﬁcantly improve the retrieval performance. Our empirical results on six TREC collections show that query expansion using only hand-crafted lexical resources leads to significant performance improvement. The performance can be further improved if the proposed method is combined with query expansion using co-occurrence-based resources. 
Query expansion by word alterations (alternative forms of a word) is often used in Web search to replace word stemming. This allows users to specify particular word forms in a query. However, if many alterations are added, query traffic will be greatly increased. In this paper, we propose methods to select only a few useful word alterations for query expansion. The selection is made according to the appropriateness of the alteration to the query context (using a bigram language model), or according to its expected impact on the retrieval effectiveness (using a regression model). Our experiments on two TREC collections will show that both methods only select a few expansion terms, but the retrieval effectiveness can be improved significantly. 
This paper is concerned with the problem of question search. In question search, given a question as query, we are to return questions semantically equivalent or close to the queried question. In this paper, we propose to conduct question search by identifying question topic and question focus. More specifically, we first summarize questions in a data structure consisting of question topic and question focus. Then we model question topic and question focus in a language modeling framework for search. We also propose to use the MDLbased tree cut model for identifying question topic and question focus automatically. Experimental results indicate that our approach of identifying question topic and question focus for search significantly outperforms the baseline methods such as Vector Space Model (VSM) and Language Model for Information Retrieval (LMIR). 
Previous work on statistical language generation has primarily focused on grammaticality and naturalness, scoring generation possibilities according to a language model or user feedback. More recent work has investigated data-driven techniques for controlling linguistic style without overgeneration, by reproducing variation dimensions extracted from corpora. Another line of work has produced handcrafted rule-based systems to control speciﬁc stylistic dimensions, such as politeness and personality. This paper describes a novel approach that automatically learns to produce recognisable variation along a meaningful stylistic dimension— personality—without the computational cost incurred by overgeneration techniques. We present the ﬁrst evaluation of a data-driven generation method that projects multiple personality traits simultaneously and on a continuous scale. We compare our performance to a rule-based generator in the same domain. 
This paper proposes a method to correct English verb form errors made by non-native speakers. A basic approach is template matching on parse trees. The proposed method improves on this approach in two ways. To improve recall, irregularities in parse trees caused by verb form errors are taken into account; to improve precision, n-gram counts are utilized to ﬁlter proposed corrections. Evaluation on non-native corpora, representing two genres and mother tongues, shows promising results. 
In lexicalized grammatical formalisms, it is possible to separate lexical category assignment from the combinatory processes that make use of such categories, such as parsing and realization. We adapt techniques from supertagging — a relatively recent technique that performs complex lexical tagging before full parsing (Bangalore and Joshi, 1999; Clark, 2002) — for chart realization in OpenCCG, an open-source NLP toolkit for CCG. We call this approach hypertagging, as it operates at a level “above” the syntax, tagging semantic representations with syntactic lexical categories. Our results demonstrate that a hypertagger-informed chart realizer can achieve substantial improvements in realization speed (being approximately twice as fast) with superior realization quality. 
Among syntax-based translation models, the tree-based approach, which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart. However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors. We propose a forest-based approach that translates a packed forest of exponentially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 
Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems. We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. 
We take a multi-pass approach to machine translation decoding when using synchronous context-free grammars as the translation model and n-gram language models: the ﬁrst pass uses a bigram language model, and the resulting parse forest is used in the second pass to guide search with a trigram language model. The trigram pass closes most of the performance gap between a bigram decoder and a much slower trigram decoder, but takes time that is insigniﬁcant in comparison to the bigram pass. An additional fast decoding pass maximizing the expected count of correct translation hypotheses increases the BLEU score signiﬁcantly. 
We propose the use of regular tree grammars (RTGs) as a formalism for the underspeciﬁed processing of scope ambiguities. By applying standard results on RTGs, we obtain a novel algorithm for eliminating equivalent readings and the ﬁrst efﬁcient algorithm for computing the best reading of a scope ambiguity. We also show how to derive RTGs from more traditional underspeciﬁed descriptions. 
This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. 
We present a novel hierarchical prior structure for supervised transfer learning in named entity recognition, motivated by the common structure of feature spaces for this task across natural language data sets. The problem of transfer learning, where information gained in one learning task is used to improve performance in another related task, is an important new area of research. In the subproblem of domain adaptation, a model trained over a source domain is generalized to perform well on a related target domain, where the two domains’ data are distributed similarly, but not identically. We introduce the concept of groups of closely-related domains, called genres, and show how inter-genre adaptation is related to domain adaptation. We also examine multitask learning, where two domains may be related, but where the concept to be learned in each case is distinct. We show that our prior conveys useful information across domains, genres and tasks, while remaining robust to spurious signals not related to the target domain and concept. We further show that our model generalizes a class of similar hierarchical priors, smoothed to varying degrees, and lay the groundwork for future exploration in this area. 
We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “discourse” from one single document to a cluster of topically-related documents. We employ a similar approach to propagate consistent event arguments across sentences and documents. Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event extraction task1. Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence. 
This paper demonstrates a new method for leveraging free-text annotations to infer semantic properties of documents. Free-text annotations are becoming increasingly abundant, due to the recent dramatic growth in semistructured, user-generated online content. An example of such content is product reviews, which are often annotated by their authors with pros/cons keyphrases such as “a real bargain” or “good value.” To exploit such noisy annotations, we simultaneously ﬁnd a hidden paraphrase structure of the keyphrases, a model of the document texts, and the underlying semantic properties that link the two. This allows us to predict properties of unannotated documents. Our approach is implemented as a hierarchical Bayesian model with joint inference, which increases the robustness of the keyphrase clustering and encourages the document model to correlate with semantically meaningful properties. We perform several evaluations of our model, and ﬁnd that it substantially outperforms alternative approaches. 
The availability of databases of images labeled with keywords is necessary for developing and evaluating image annotation models. Dataset collection is however a costly and time consuming task. In this paper we exploit the vast resource of images available on the web. We create a database of pictures that are naturally embedded into news articles and propose to use their captions as a proxy for annotation keywords. Experimental results show that an image annotation model can be developed on this dataset alone without the overhead of manual annotation. We also demonstrate that the news article associated with the picture can be used to boost image annotation performance. 
Since facts or statements in a hedge or negated context typically appear as false positives, the proper handling of these language phenomena is of great importance in biomedical text mining. In this paper we demonstrate the importance of hedge classiﬁcation experimentally in two real life scenarios, namely the ICD9-CM coding of radiology reports and gene name Entity Extraction from scientiﬁc texts. We analysed the major differences of speculative language in these tasks and developed a maxent-based solution for both the free text and scientiﬁc text processing tasks. Based on our results, we draw conclusions on the possible ways of tackling speculative language in biomedical texts. 
This study presents a novel approach to the problem of system portability across different domains: a sentiment annotation system that integrates a corpus-based classiﬁer trained on a small set of annotated in-domain data and a lexicon-based system trained on WordNet. The paper explores the challenges of system portability across domains and text genres (movie reviews, news, blogs, and product reviews), highlights the factors affecting system performance on out-of-domain and smallset in-domain data, and presents a new system consisting of the ensemble of two classiﬁers with precision-based vote weighting, that provides signiﬁcant gains in accuracy and recall over the corpus-based classiﬁer and the lexicon-based system taken individually. 
The paper presents a novel sentence trimmer in Japanese, which combines a non-statistical yet generic tree generation model and Conditional Random Fields (CRFs), to address improving the grammaticality of compression while retaining its relevance. Experiments found that the present approach outperforms in grammaticality and in relevance a dependency-centric approach (Oguro et al., 2000; Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007) − the only line of work in prior literature (on Japanese compression) we are aware of that allows replication and permits a direct comparison. 
Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings – a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals. 
To date, parsers have made limited use of semantic information, but there is evidence to suggest that semantic features can enhance parse disambiguation. This paper shows that semantic classes help to obtain signiﬁcant improvement in both parsing and PP attachment tasks. We devise a gold-standard sense- and parse tree-annotated dataset based on the intersection of the Penn Treebank and SemCor, and experiment with different approaches to both semantic representation and disambiguation. For the Bikel parser, we achieved a maximal error reduction rate over the baseline parser of 6.9% and 20.5%, for parsing and PP-attachment respectively, using an unsupervised WSD strategy. This demonstrates that word sense information can indeed enhance the performance of syntactic disambiguation. 
The standard set of rules deﬁned in Combinatory Categorial Grammar (CCG) fails to provide satisfactory analyses for a number of syntactic structures found in natural languages. These structures can be analyzed elegantly by augmenting CCG with a class of rules based on the combinator D (Curry and Feys, 1958). We show two ways to derive the D rules: one based on unary composition and the other based on a logical characterization of CCG’s rule base (Baldridge, 2002). We also show how Eisner’s (1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities. 
 (N  Statistical parsing of noun phrase (NP) structure has been hampered by a lack of goldstandard data. This is a signiﬁcant problem for CCGbank, where binary branching NP derivations are often incorrect, a result of the auto-  (N/N lung) (N (N/N cancer) (N deaths) ) ) This structure is correct for most English NPs and is the best solution that doesn’t require manual re-  matic conversion from the Penn Treebank.  annotation. However, the resulting derivations often  We correct these errors in CCGbank using a  contain errors. This can be seen in the previous ex-  gold-standard corpus of NP structure, resulting in a much more accurate corpus. We also implement novel NER features that generalise the lexical information needed to parse NPs and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modiﬁed corpus and novel features, with an increase in  ample, where lung cancer should form a constituent, but does not. The ﬁrst contribution of this paper is to correct these CCGbank errors. We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). Over a quarter of the sentences in CCGbank need to be altered,  parser performance of 1.51%.  demonstrating the magnitude of the NP problem and  how important it is that these errors are ﬁxed.  
Parse-tree paths are commonly used to incorporate information from syntactic parses into NLP systems. These systems typically treat the paths as atomic (or nearly atomic) features; these features are quite sparse due to the immense variety of syntactic expression. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patterns — for example, one rule “depassivizes” a sentence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible transformations to a sentence, we are left with a set of candidate simpliﬁed sentences. We apply our simpliﬁcation system to semantic role labeling (SRL). As we do not have labeled examples of correct simpliﬁcations, we use labeled training data for the SRL task to jointly learn both the weights of the simpliﬁcation model and of an SRL model, treating the simpliﬁcation as a hidden variable. By extracting and labeling simpliﬁed sentences, this combined simpliﬁcation/SRL system better generalizes across syntactic variation. It achieves a statistically signiﬁcant 1.2% F1 measure increase over a strong baseline on the Conll2005 SRL task, attaining near-state-of-the-art performance. 
In this paper, we study the problem of summarizing email conversations. We ﬁrst build a sentence quotation graph that captures the conversation structure among emails. We adopt three cohesion measures: clue words, semantic similarity and cosine similarity as the weight of the edges. Second, we use two graph-based summarization approaches, Generalized ClueWordSummarizer and PageRank, to extract sentences as summaries. Third, we propose a summarization approach based on subjective opinions and integrate it with the graph-based ones. The empirical evaluation shows that the basic clue words have the highest accuracy among the three cohesion measures. Moreover, subjective words can signiﬁcantly improve accuracy. 
We outline the problem of ad hoc rules in treebanks, rules used for speciﬁc constructions in one data set and unlikely to be used again. These include ungeneralizable rules, erroneous rules, rules for ungrammatical text, and rules which are not consistent with the rest of the annotation scheme. Based on a simple notion of rule equivalence and on the idea of ﬁnding rules unlike any others, we develop two methods for detecting ad hoc rules in ﬂat treebanks and show they are successful in detecting such rules. This is done by examining evidence across the grammar and without making any reference to context. 
Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far. 
Many factors are thought to increase the chances of misrecognizing a word in ASR, including low frequency, nearby disﬂuencies, short duration, and being at the start of a turn. However, few of these factors have been formally examined. This paper analyzes a variety of lexical, prosodic, and disﬂuency factors to determine which are likely to increase ASR error rates. Findings include the following. (1) For disﬂuencies, effects depend on the type of disﬂuency: errors increase by up to 15% (absolute) for words near fragments, but decrease by up to 7.2% (absolute) for words near repetitions. This decrease seems to be due to longer word duration. (2) For prosodic features, there are more errors for words with extreme values than words with typical values. (3) Although our results are based on output from a system with speaker adaptation, speaker differences are a major factor inﬂuencing error rates, and the effects of features such as frequency, pitch, and intensity may vary between speakers. 
We present a method to transliterate names in the framework of end-to-end statistical machine translation. The system is trained to learn when to transliterate. For Arabic to English MT, we developed and trained a transliterator on a bitext of 7 million sentences and Google’s English terabyte ngrams and achieved better name translation accuracy than 3 out of 4 professional translators. The paper also includes a discussion of challenges in name translation evaluation.  
Adaptor grammars (Johnson et al., 2007b) are a non-parametric Bayesian extension of Probabilistic Context-Free Grammars (PCFGs) which in effect learn the probabilities of entire subtrees. In practice, this means that an adaptor grammar learns the structures useful for generating the training data as well as their probabilities. We present several different adaptor grammars that learn to segment phonemic input into words by modeling different linguistic properties of the input. One of the advantages of a grammar-based framework is that it is easy to combine grammars, and we use this ability to compare models that capture different kinds of linguistic structure. We show that incorporating both unsupervised syllabiﬁcation and collocation-ﬁnding into the adaptor grammar signiﬁcantly improves unsupervised word-segmentation accuracy over that achieved by adaptor grammars that model only one of these linguistic phenomena. 
We propose using large-scale clustering of dependency relations between verbs and multiword nouns (MNs) to construct a gazetteer for named entity recognition (NER). Since dependency relations capture the semantics of MNs well, the MN clusters constructed by using dependency relations should serve as a good gazetteer. However, the high level of computational cost has prevented the use of clustering for constructing gazetteers. We parallelized a clustering algorithm based on expectationmaximization (EM) and thus enabled the construction of large-scale MN clusters. We demonstrated with the IREX dataset for the Japanese NER that using the constructed clusters as a gazetteer (cluster gazetteer) is a effective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 
Roget’s Thesaurus has gone through many revisions since it was ﬁrst published 150 years ago. But how do these revisions affect Roget’s usefulness for NLP? We examine the differences in content between the 1911 and 1987 versions of Roget’s, and we test both versions with each other and WordNet on problems such as synonym identiﬁcation and word relatedness. We also present a novel method for measuring sentence relatedness that can be implemented in either version of Roget’s or in WordNet. Although the 1987 version of the Thesaurus is better, we show that the 1911 version performs surprisingly well and that often the differences between the versions of Roget’s and WordNet are not statistically significant. We hope that this work will encourage others to use the 1911 Roget’s Thesaurus in NLP tasks. 
Chinese abbreviations are widely used in modern Chinese texts. Compared with English abbreviations (which are mostly acronyms and truncations), the formation of Chinese abbreviations is much more complex. Due to the richness of Chinese abbreviations, many of them may not appear in available parallel corpora, in which case current machine translation systems simply treat them as unknown words and leave them untranslated. In this paper, we present a novel unsupervised method that automatically extracts the relation between a full-form phrase and its abbreviation from monolingual corpora, and induces translation entries for the abbreviation by using its full-form as a bridge. Our method does not require any additional annotated data other than the data that a regular translation system uses. We integrate our method into a state-ofthe-art baseline translation system and show that it consistently improves the performance of the baseline system on various NIST MT test sets. 
In this work, we develop and evaluate a wide range of feature spaces for deriving Levinstyle verb classiﬁcations (Levin, 1993). We perform the classiﬁcation experiments using Bayesian Multinomial Regression (an efﬁcient log-linear modeling framework which we found to outperform SVMs for this task) with the proposed feature spaces. Our experiments suggest that subcategorization frames are not the most effective features for automatic verb classiﬁcation. A mixture of syntactic information and lexical information works best for this task. 
Question answering research has only recently started to spread from short factoid questions to more complex ones. One signiﬁcant challenge is the evaluation: manual evaluation is a difﬁcult, time-consuming process and not applicable within efﬁcient development of systems. Automatic evaluation requires a corpus of questions and answers, a deﬁnition of what is a correct answer, and a way to compare the correct answers to automatic answers produced by a system. For this purpose we present a Wikipedia-based corpus of Whyquestions and corresponding answers and articles. The corpus was built by a novel method: paid participants were contacted through a Web-interface, a procedure which allowed dynamic, fast and inexpensive development of data collection methods. Each question in the corpus has several corresponding, partly overlapping answers, which is an asset when estimating the correctness of answers. In addition, the corpus contains information related to the corpus collection process. We believe this additional information can be used to post-process the data, and to develop an automatic approval system for further data collection projects conducted in a similar manner. 
We present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns. The approach leverages the vast size of the Web in order to build lexically-speciﬁc features. The main idea is to look for verbs, prepositions, and coordinating conjunctions that can help make explicit the hidden relations between the target nouns. Using these features in instance-based classiﬁers, we demonstrate state-of-the-art results on various relational similarity problems, including mapping noun-modiﬁer pairs to abstract relations like TIME, LOCATION and CONTAINER, characterizing noun-noun compounds in terms of abstract linguistic predicates like CAUSE, USE, and FROM, classifying the relations between nominals in context, and solving SAT verbal analogy problems. In essence, the approach puts together some existing ideas, showing that they apply generally to various semantic tasks, ﬁnding that verbs are especially useful features. 
Rapid and inexpensive techniques for automatic transcription of speech have the potential to dramatically expand the types of content to which information retrieval techniques can be productively applied, but limitations in accuracy and robustness must be overcome before that promise can be fully realized. Combining retrieval results from systems built on various errorful representations of the same collection offers some potential to address these challenges. This paper explores that potential by applying Generalized Additive Models to optimize the combination of ranked retrieval results obtained using transcripts produced automatically for the same spoken content by substantially different recognition systems. Topic-averaged retrieval effectiveness better than any previously reported for the same collection was obtained, and even larger gains are apparent when using an alternative measure emphasizing results on the most difﬁcult topics. 
We assess the current state of the art in speech summarization, by comparing a typical summarizer on two different domains: lecture data and the SWITCHBOARD corpus. Our results cast signiﬁcant doubt on the merits of this area’s accepted evaluation standards in terms of: baselines chosen, the correspondence of results to our intuition of what “summaries” should be, and the value of adding speechrelated features to summarizers that already use transcripts from automatic speech recognition (ASR) systems. 
Despite its long history, and a great deal of research producing many useful algorithms and observations, research in cooperative response generation has had little impact on the recent commercialization of dialogue technologies, particularly within the spoken dialogue community. We hypothesize that a particular type of cooperative response, intensional summaries, are eﬀective for when users are unfamiliar with the domain. We evaluate this hypothesis with two experiments with cruiser, a DS for in-car or mobile users to access restaurant information. First, we compare cruiser with a baseline system-initiative DS, and show that users prefer cruiser. Then, we experiment with four algorithms for constructing intensional summaries in cruiser, and show that two summary types are equally eﬀective: summaries that maximize domain coverage and summaries that maximize utility with respect to a user model.  U1: Tell me about restaurants in London. SI1: What kind of cuisine are you interested in? C1: I know of 596 restaurants in London. I know of 3 inexpensive vegetarian restaurants and 14 inexpensive Chinese restaurants. I also know of 4 inexpensive Greek restaurants. U2: Chinese. SI2: Do you have a price range in mind? C2: I know of 27 restaurants in London that serve Chinese food. There are 8 inexpensive Chinese restaurants in Chinatown and 2 inexpensive Chinese restaurants in Hampstead/Kilburn. I also know of 1 inexpensive Chinese restaurant in Soho. U3: How about a cheap one? SI3: What neighborhood would you like? C3: I know of 1 inexpensive Chinese restaurant in Hampstead/Kilburn with very good food quality and 1 in Bayswater with good food quality. I also know of 2 in Chinatown with medium food quality. Figure 1: Intensional summaries (C = cruiser) as compared with a system initiative (SI) strategy in the London restaurant domain. U = User  
Statistical machine learning methods are employed to train a Named Entity Recognizer from annotated data. Methods like Maximum Entropy and Conditional Random Fields make use of features for the training purpose. These methods tend to overﬁt when the available training corpus is limited especially if the number of features is large or the number of values for a feature is large. To overcome this we proposed two techniques for feature reduction based on word clustering and selection. A number of word similarity measures are proposed for clustering words for the Named Entity Recognition task. A few corpus based statistical measures are used for important word selection. The feature reduction techniques lead to a substantial performance improvement over baseline Maximum Entropy technique. 
This paper presents an innovative, complex approach to semantic verb classiﬁcation that relies on selectional preferences as verb properties. The probabilistic verb class model underlying the semantic classes is trained by a combination of the EM algorithm and the MDL principle, providing soft clusters with two dimensions (verb senses and subcategorisation frames with selectional preferences) as a result. A language-model-based evaluation shows that after 10 training iterations the verb class model results are above the baseline results. 
We propose a succinct randomized language model which employs a perfect hash function to encode ﬁngerprints of n-grams and their associated probabilities, backoff weights, or other parameters. The scheme can represent any standard n-gram model and is easily combined with existing model reduction techniques such as entropy-pruning. We demonstrate the space-savings of the scheme via machine translation experiments within a distributed language modeling framework. 
We improve the quality of statistical machine translation (SMT) by applying models that predict word forms from their stems using extensive morphological and syntactic information from both the source and target languages. Our inﬂection generation models are trained independently of the SMT system. We investigate different ways of combining the inﬂection prediction component with the SMT system by training the base MT system on fully inﬂected forms or on word stems. We applied our inﬂection generation models in translating English into two morphologically complex languages, Russian and Arabic, and show that our model improves the quality of SMT over both phrasal and syntax-based SMT systems according to BLEU and human judgements. 
People rarely articulate explicitly what a native speaker of a language is already assumed to know. So to acquire the stereotypical knowledge that underpins much of what is said in a given culture, one must look to what is implied by language rather than what is overtly stated. Similes are a convenient vehicle for this kind of knowledge, insofar as they mark out the most salient aspects of the most frequently evoked concepts. In this paper we perform a multilingual exploration of the space of common-place similes, by mining a large body of Chinese similes from the web and comparing these to the English similes harvested by Veale and Hao (2007). We demonstrate that while the simile-frame is inherently leaky in both languages, a multilingual analysis allows us to ﬁlter much of the noise that otherwise hinders the knowledge extraction process. In doing so, we can also identify a core set of stereotypical descriptions that exist in both languages and accurately map these descriptions onto a multilingual lexical ontology like HowNet. Finally, we demonstrate that conceptual descriptions that are derived from common-place similes are extremely compact and predictive of ontological structure. 
We present a novel semi-supervised training algorithm for learning dependency parsers. By combining a supervised large margin loss with an unsupervised least squares loss, a discriminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the beneﬁts of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efﬁciently learned from semi-supervised data that signiﬁcantly outperforms corresponding supervised methods. 
In this paper, we present a novel backward transliteration approach which can further assist the existing statistical model by mining monolingual web resources. Firstly, we employ the syllable-based search to revise the transliteration candidates from the statistical model. By mapping all of them into existing words, we can filter or correct some pseudo candidates and improve the overall recall. Secondly, an AdaBoost model is used to rerank the revised candidates based on the information extracted from monolingual web pages. To get a better precision during the reranking process, a variety of web-based information is exploited to adjust the ranking score, so that some candidates which are less possible to be transliteration names will be assigned with lower ranks. The experimental results show that the proposed framework can significantly outperform the baseline transliteration system in both precision and recall. 
This paper presents an empirical study on the robustness and generalization of two alternative role sets for semantic role labeling: PropBank numbered roles and VerbNet thematic roles. By testing a state–of–the–art SRL system with the two alternative role annotations, we show that the PropBank role set is more robust to the lack of verb–speciﬁc semantic information and generalizes better to infrequent and unseen predicates. Keeping in mind that thematic roles are better for application needs, we also tested the best way to generate VerbNet annotation. We conclude that tagging ﬁrst PropBank roles and mapping into VerbNet roles is as effective as training and tagging directly on VerbNet, and more robust for domain shifts. 
This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase. The model leverages on the strengths of both phrase-based and linguistically syntax-based method. It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 
We present the ﬁrst English syllabiﬁcation system to improve the accuracy of letter-tophoneme conversion. We propose a novel discriminative approach to automatic syllabiﬁcation based on structured SVMs. In comparison with a state-of-the-art syllabiﬁcation system, we reduce the syllabiﬁcation word error rate for English by 33%. Our approach also performs well on other languages, comparing favorably with published results on German and Dutch. 
In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 
Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our ﬁnal result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank. 
We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance. 
Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-language syntax and semantics. Current research in both of these areas is actively pursuing its incorporation. However, STAG parsing is known to be NP-hard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures. Given a particular grammar, the polynomial degree of efﬁcient STAG parsing algorithms depends directly on the rank of the grammar: the maximum number of correspondences that appear within a single elementary structure. In this paper we present a compile-time algorithm for transforming a STAG into a strongly-equivalent STAG that optimally minimizes the rank, k, across the grammar. The algorithm performs in O(|G| + |Y | · L3G) time where LG is the maximum number of links in any single synchronous tree pair in the grammar and Y is the set of synchronous tree pairs of G. 
This paper describes how external resources can be used to improve parser performance for heavily lexicalised grammars, looking at both robustness and efﬁciency. In terms of robustness, we try using different types of external data to increase lexical coverage, and ﬁnd that simple POS tags have the most effect, increasing coverage on unseen data by up to 45%. We also show that ﬁltering lexical items in a supertagging manner is very effective in increasing efﬁciency. Even using vanilla POS tags we achieve some efﬁciency gains, but when using detailed lexical types as supertags we manage to halve parsing time with minimal loss of coverage or precision. 
Previous studies evaluate simulated dialog corpora using evaluation measures which can be automatically extracted from the dialog systems’ logs. However, the validity of these automatic measures has not been fully proven. In this study, we ﬁrst recruit human judges to assess the quality of three simulated dialog corpora and then use human judgments as the gold standard to validate the conclusions drawn from the automatic measures. We observe that it is hard for the human judges to reach good agreement when asked to rate the quality of the dialogs from given perspectives. However, the human ratings give consistent ranking of the quality of simulated corpora generated by different simulation models. When building prediction models of human judgments using previously proposed automatic measures, we ﬁnd that we cannot reliably predict human ratings using a regression model, but we can predict human rankings by a ranking model. 
This work presents an agenda-based approach to improve the robustness of the dialog manager by using dialog examples and n-best recognition hypotheses. This approach supports n-best hypotheses in the dialog manager and keeps track of the dialog state using a discourse interpretation algorithm with the agenda graph and focus stack. Given the agenda graph and n-best hypotheses, the system can predict the next system actions to maximize multi-level score functions. To evaluate the proposed method, a spoken dialog system for a building guidance robot was developed. Preliminary evaluation shows this approach would be effective to improve the robustness of example-based dialog modeling. 
We address two problems in the ﬁeld of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and evaluating the result with real users. We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is “bootstrapped” from small amounts of Wizard-of-Oz (WOZ) data. This use of WOZ data allows development of optimal strategies for domains where no working prototype is available. We compare the RL-based strategy against a supervised strategy which mimics the wizards’ policies. This comparison allows us to measure relative improvement over the training data. Our results show that RL signiﬁcantly outperforms Supervised Learning when interacting in simulation as well as for interactions with real users. The RL-based policy gains on average 50-times more reward when tested in simulation, and almost 18-times more reward when interacting with real users. Users also subjectively rate the RL-based policy on average 10% higher. 
Entropy Guided Transformation Learning (ETL) is a new machine learning strategy that combines the advantages of decision trees (DT) and Transformation Based Learning (TBL). In this work, we apply the ETL framework to four phrase chunking tasks: Portuguese noun phrase chunking, English base noun phrase chunking, English text chunking and Hindi text chunking. In all four tasks, ETL shows better results than Decision Trees and also than TBL with hand-crafted templates. ETL provides a new training strategy that accelerates transformation learning. For the English text chunking task this corresponds to a factor of ﬁve speedup. For Portuguese noun phrase chunking, ETL shows the best reported results for the task. For the other three linguistic tasks, ETL shows state-of-theart competitive results and maintains the advantages of using a rule based system. 
Traditional wisdom holds that once documents are turned into bag-of-words (unigram count) vectors, word orders are completely lost. We introduce an approach that, perhaps surprisingly, is able to learn a bigram language model from a set of bag-of-words documents. At its heart, our approach is an EM algorithm that seeks a model which maximizes the regularized marginal likelihood of the bagof-words documents. In experiments on seven corpora, we observed that our learned bigram language models: i) achieve better test set perplexity than unigram models trained on the same bag-of-words documents, and are not far behind “oracle bigram models” trained on the corresponding ordered documents; ii) assign higher probabilities to sensible bigram word pairs; iii) improve the accuracy of ordereddocument recovery from a bag-of-words. Our approach opens the door to novel phenomena, for example, privacy leakage from index ﬁles. 
Paraphrases have proved to be useful in many applications, including Machine Translation, Question Answering, Summarization, and Information Retrieval. Paraphrase acquisition methods that use a single monolingual corpus often produce only syntactic paraphrases. We present a method for obtaining surface paraphrases, using a 150GB (25 billion words) monolingual corpus. Our method achieves an accuracy of around 70% on the paraphrase acquisition task. We further show that we can use these paraphrases to generate surface patterns for relation extraction. Our patterns are much more precise than those obtained by using a state of the art baseline and can extract relations with more than 80% precision for each of the test relations.  
The validity of semantic inferences depends on the contexts in which they are applied. We propose a generic framework for handling contextual considerations within applied inference, termed Contextual Preferences. This framework deﬁnes the various context-aware components needed for inference and their relationships. Contextual preferences extend and generalize previous notions, such as selectional preferences, while experiments show that the extended framework allows improving inference quality on real application data. 
We present a novel framework for the discovery and representation of general semantic relationships that hold between lexical items. We propose that each such relationship can be identiﬁed with a cluster of patterns that captures this relationship. We give a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters and merges highfrequency words-based patterns around randomly selected hook words. Pattern clusters can be used to extract instances of the corresponding relationships. To assess the quality of discovered relationships, we use the pattern clusters to automatically generate SAT analogy questions. We also compare to a set of known relationships, achieving very good results in both methods. The evaluation (done in both English and Russian) substantiates the premise that our pattern clusters indeed reﬂect relationships perceived by humans. 
Web search engines today typically show results as a list of titles and short snippets that summarize how the retrieved documents are related to the query. However, recent research suggests that longer summaries can be preferable for certain types of queries. This paper presents empirical evidence that judges can predict appropriate search result summary lengths, and that perceptions of search result quality can be affected by varying these result lengths. These ﬁndings have important implications for search results presentation, especially for natural language queries. 
Online forum discussions often contain vast amounts of questions that are the focuses of discussions. Extracting contexts and answers together with the questions will yield not only a coherent forum summary but also a valuable QA knowledge base. In this paper, we propose a general framework based on Conditional Random Fields (CRFs) to detect the contexts and answers of questions from forum threads. We improve the basic framework by Skip-chain CRFs and 2D CRFs to better accommodate the features of forums for better performance. Experimental results show that our techniques are very promising. 
This work describes an answer ranking engine for non-factoid questions built using a large online community-generated question-answer collection (Yahoo! Answers). We show how such collections may be used to effectively set up large supervised learning experiments. Furthermore we investigate a wide range of feature types, some exploiting NLP processors, and demonstrate that using them in combination leads to considerable improvements in accuracy.  High Quality Low Quality  Q: How do you quiet a squeaky door? A: Spray WD-40 directly onto the hinges of the door. Open and close the door several times. Remove hinges if the door still squeaks. Remove any rust, dirt or loose paint. Apply WD-40 to removed hinges. Put the hinges back, open and close door several times again. Q: How to extract html tags from an html documents with c++? A: very carefully  Table 1: Sample content from Yahoo! Answers.  
For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identiﬁes cross-lingual morpheme patterns, or abstract morphemes. We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family. 
We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective initial estimations p(t|w). We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efﬁcient learning methods. 
In statistical language modeling, one technique to reduce the problematic eﬀects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the eﬀects of applying such a technique to higherorder n-gram models trained on large corpora. We introduce a modiﬁcation of the exchange clustering algorithm with improved eﬃciency for certain partially class-based models and a distributed version of this algorithm to eﬃciently obtain automatic word classiﬁcations for large vocabularies (>1 million words) using such large training corpora (>30 billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with wordbased n-gram models in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.  
We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For English–Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%. 
We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types. 
Paraphrase patterns are useful in paraphrase recognition and generation. In this paper, we present a pivot approach for extracting paraphrase patterns from bilingual parallel corpora, whereby the English paraphrase patterns are extracted using the sentences in a foreign language as pivots. We propose a loglinear model to compute the paraphrase likelihood of two patterns and exploit feature functions based on maximum likelihood estimation (MLE) and lexical weighting (LW). Using the presented method, we extract over 1,000,000 pairs of paraphrase patterns from 2M bilingual sentence pairs, the precision of which exceeds 67%. The evaluation results show that: (1) The pivot approach is effective in extracting paraphrase patterns, which signiﬁcantly outperforms the conventional method DIRT. Especially, the log-linear model with the proposed feature functions achieves high performance. (2) The coverage of the extracted paraphrase patterns is high, which is above 84%. (3) The extracted paraphrase patterns can be classiﬁed into 5 types, which are useful in various applications. 
Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The ﬁrst uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classiﬁer to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. We introduce two evaluations: the narrative cloze to evaluate event relatedness, and an order coherence task to evaluate narrative order. We show a 36% improvement over baseline for narrative prediction and 25% for temporal coherence. 
There is a widely held belief in the natural language and computational linguistics communities that Semantic Role Labeling (SRL) is a signiﬁcant step toward improving important applications, e.g. question answering and information extraction. In this paper, we present an SRL system for Modern Standard Arabic that exploits many aspects of the rich morphological features of the language. The experiments on the pilot Arabic Propbank data show that our system based on Support Vector Machines and Kernel Methods yields a global SRL F1 score of 82.17%, which improves the current state-of-the-art in Arabic SRL. 
We describe an unsupervised approach to multi-document sentence-extraction based summarization for the task of producing biographies. We utilize Wikipedia to automatically construct a corpus of biographical sentences and TDT4 to construct a corpus of non-biographical sentences. We build a biographical-sentence classiﬁer from these corpora and an SVM regression model for sentence ordering from the Wikipedia corpus. We evaluate our work on the DUC2004 evaluation data and with human judges. Overall, our system signiﬁcantly outperforms all systems that participated in DUC2004, according to the ROUGE-L metric, and is preferred by human subjects. 
In this paper, we present a study of a novel summarization problem, i.e., summarizing the impact of a scientiﬁc publication. Given a paper and its citation context, we study how to extract sentences that can represent the most inﬂuential content of the paper. We propose language modeling methods for solving this problem, and study how to incorporate features such as authority and proximity to accurately estimate the impact language model. Experiment results on a SIGIR publication collection show that the proposed methods are effective for generating impact-based summaries. 
Different summarization requirements could make the writing of a good summary more difﬁcult, or easier. Summary length and the characteristics of the input are such constraints inﬂuencing the quality of a potential summary. In this paper we report the results of a quantitative analysis on data from large-scale evaluations of multi-document summarization, empirically conﬁrming this hypothesis. We further show that features measuring the cohesiveness of the input are highly correlated with eventual summary quality and that it is possible to use these as features to predict the difﬁculty of new, unseen, summarization inputs. 
When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. This is, to our knowledge, the ﬁrst such corpus for internet chat. We propose a graph-theoretic model for disentanglement, using discourse-based features which have not been previously applied to this task. The model’s predicted disentanglements are highly correlated with manual annotations. 
The traditional mention-pair model for coreference resolution cannot capture information beyond mention pairs for both learning and testing. To deal with this problem, we present an expressive entity-mention model that performs coreference resolution at an entity level. The model adopts the Inductive Logic Programming (ILP) algorithm, which provides a relational way to organize different knowledge of entities and mentions. The solution can explicitly express relations between an entity and the contained mentions, and automatically learn ﬁrst-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 
This paper explores the relationship between discourse segmentation and coverbal gesture. Introducing the idea of gestural cohesion, we show that coherent topic segments are characterized by homogeneous gestural forms and that changes in the distribution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manual and automaticallyrecognized speech transcripts. 
We extend the classical single-task active learning (AL) approach. In the multi-task active learning (MTAL) paradigm, we select examples for several annotation tasks rather than for a single one as usually done in the context of AL. We introduce two MTAL metaprotocols, alternating selection and rank combination, and propose a method to implement them in practice. We experiment with a twotask annotation scenario that includes named entity and syntactic parse tree annotations on three different corpora. MTAL outperforms random selection and a stronger baseline, onesided example selection, in which one task is pursued using AL and the selected examples are provided also to the other task. 
This paper presents a semi-supervised training method for linear-chain conditional random ﬁelds that makes use of labeled features rather than labeled instances. This is accomplished by using generalized expectation criteria to express a preference for parameter settings in which the model’s distribution on unlabeled data matches a target distribution. We induce target conditional probability distributions of labels given features from both annotated feature occurrences in context and adhoc feature majority label assignment. The use of generalized expectation criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available. 
We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efﬁcient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model. 
For Chinese POS tagging, word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing POS information, segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efﬁcient decoding very hard. Recent research has explored the integration of segmentation and POS tagging, by decoding under restricted versions of the full combined search space. In this paper, we propose a joint segmentation and POS tagging model that does not impose any hard constraints on the interaction between word and POS information. Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach. 
We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efﬁciently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of 18.5% on segmentation and 12% on joint segmentation and part-of-speech tagging over the perceptron-only baseline. 
We present a discriminative structureprediction model for the letter-to-phoneme task, a crucial step in text-to-speech processing. Our method encompasses three tasks that have been previously handled separately: input segmentation, phoneme prediction, and sequence modeling. The key idea is online discriminative training, which updates parameters according to a comparison of the current system output to the desired output, allowing us to train all of our components together. By folding the three steps of a pipeline approach into a uniﬁed dynamic programming framework, we are able to achieve substantial performance gains. Our results surpass the current state-of-the-art on six publicly available data sets representing four different languages. 
Expert search, in which given a query a ranked list of experts instead of documents is returned, has been intensively studied recently due to its importance in facilitating the needs of both information access and knowledge discovery. Many approaches have been proposed, including metadata extraction, expert profile building, and formal model generation. However, all of them conduct expert search with a coarse-grained approach. With these, further improvements on expert search are hard to achieve. In this paper, we propose conducting expert search with a fine-grained approach. Specifically, we utilize more specific evidences existing in the documents. An evidence-oriented probabilistic model for expert search and a method for the implementation are proposed. Experimental results show that the proposed model and the implementation are highly effective. 
Topical blog post retrieval is the task of ranking blog posts with respect to their relevance for a given topic. To improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process. We consider two groups of indicators: post level (determined using information about individual blog posts only) and blog level (determined using information from the underlying blogs). We describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models. Experiments on the TREC Blog track test set show that both groups of credibility indicators signiﬁcantly improve retrieval effectiveness; the best performance is achieved when combining them. 
In this paper we present a supervised method for back-of-the-book index construction. We introduce a novel set of features that goes beyond the typical frequency-based analysis, including features based on discourse comprehension, syntactic patterns, and information drawn from an online encyclopedia. In experiments carried out on a book collection, the method was found to lead to an improvement of roughly 140% as compared to an existing state-of-the-art supervised method. 
This paper describes a computational approach to resolving the true referent of a named mention of a person in the body of an email. A generative model of mention generation is used to guide mention resolution. Results on three relatively small collections indicate that the accuracy of this approach compares favorably to the best known techniques, and results on the full CMU Enron collection indicate that it scales well to larger collections. 
Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artiﬁcially short sentences, we present the ﬁrst general, featurerich discriminative parser, based on a conditional random ﬁeld model, which has been successfully scaled to the full WSJ parsing data. Our efﬁciency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart preﬁltering. On WSJ15, we attain a state-of-the-art F-score of 90.9%, a 14% relative reduction in error over previous models, while being two orders of magnitude faster. On sentences of length 40, our system achieves an F-score of 89.0%, a 36% relative reduction in error over a generative baseline. 
We deﬁne a new formalism, based on Sikkel’s parsing schemata for constituency parsers, that can be used to describe, analyze and compare dependency parsing algorithms. This abstraction allows us to establish clear relations between several existing projective dependency parsers and prove their correctness. 
This paper evaluates the LinGO Grammar Matrix, a cross-linguistic resource for the development of precision broad coverage grammars, by applying it to the Australian language Wambaya. Despite large typological differences between Wambaya and the languages on which the development of the resource was based, the Grammar Matrix is found to provide a signiﬁcant jump-start in the creation of the grammar for Wambaya: With less than 5.5 person-weeks of development, the Wambaya grammar was able to assign correct semantic representations to 76% of the sentences in a naturally occurring text. While the work on Wambaya identiﬁed some areas of reﬁnement for the Grammar Matrix, 59% of the Matrix-provided types were invoked in the ﬁnal Wambaya grammar, and only 4% of the Matrix-provided types required modiﬁcation. 
Automatic word alignment is a key step in training statistical machine translation systems. Despite much recent work on word alignment methods, alignment accuracy increases often produce little or no improvements in machine translation quality. In this work we analyze a recently proposed agreement-constrained EM algorithm for unsupervised alignment models. We attempt to tease apart the effects that this simple but effective modiﬁcation has on alignment precision and recall trade-offs, and how rare and common words are affected across several language pairs. We propose and extensively evaluate a simple method for using alignment models to produce alignments better-suited for phrase-based MT systems, and show signiﬁcant gains (as measured by BLEU score) in end-to-end translation systems for six languages pairs used in recent MT competitions. 
Documents in languages such as Chinese, Japanese and Korean sometimes annotate terms with their translations in English inside a pair of parentheses. We present a method to extract such translations from a large collection of web documents by building a partially parallel corpus and use a word alignment algorithm to identify the terms being translated. The method is able to generalize across the translations for different terms and can reliably extract translations that occurred only once in the entire web. Our experiment on Chinese web pages produced more than 26 million pairs of translations, which is over two orders of magnitude more than previous results. We show that the addition of the extracted translation pairs as training data provides significant increase in the BLEU score for a statistical machine translation system. 
In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then ﬁnding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English. 
Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using ﬁnite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a signiﬁcant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for ChineseEnglish and Arabic-English translation.  
This paper proposes a novel method that exploits multiple resources to improve statistical machine translation (SMT) based paraphrasing. In detail, a phrasal paraphrase table and a feature function are derived from each resource, which are then combined in a log-linear SMT model for sentence-level paraphrase generation. Experimental results show that the SMT-based paraphrasing model can be enhanced using multiple resources. The phrase-level and sentence-level precision of the generated paraphrases are above 60% and 55%, respectively. In addition, the contribution of each resource is evaluated, which indicates that all the exploited resources are useful for generating paraphrases of high quality. 
This paper studies textual inference by investigating comma structures, which are highly frequent elements whose major role in the extraction of semantic relations has not been hitherto recognized. We introduce the problem of comma resolution, deﬁned as understanding the role of commas and extracting the relations they imply. We show the importance of the problem using examples from Textual Entailment tasks, and present A Sentence Transformation Rule Learner (ASTRL), a machine learning algorithm that uses a syntactic analysis of the sentence to learn sentence transformation rules that can then be used to extract relations. We have manually annotated a corpus identifying comma structures and relations they entail and experimented with both gold standard parses and parses created by a leading statistical parser, obtaining F-scores of 80.2% and 70.4% respectively. 
Detecting conﬂicting statements is a foundational text understanding task with applications in information analysis. We propose an appropriate deﬁnition of contradiction for NLP tasks and develop available corpora, from which we construct a typology of contradictions. We demonstrate that a system for contradiction needs to make more ﬁne-grained distinctions than the common systems for entailment. In particular, we argue for the centrality of event coreference and therefore incorporate such a component based on topicality. We present the ﬁrst detailed breakdown of performance on this task. Detecting some types of contradiction requires deeper inferential paths than our system is capable of, but we achieve good performance on types arising from negation and antonymy. 
We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based extractions: popularity and productivity. Intuitively, a candidate is popular if it was discovered many times by other instances in the hyponym pattern. A candidate is productive if it frequently leads to the discovery of other instances. Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members. We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies. 
Search interface detection is an essential task for extracting information from the hidden Web. The challenge for this task is that search interface data is represented in high-dimensional and sparse features with many missing values. This paper presents a new multi-classifier ensemble approach to solving this problem. In this approach, we have extended the random forest algorithm with a weighted feature selection method to build the individual classifiers. With this improved random forest algorithm (IRFA), each classifier can be learned from a weighted subset of the feature space so that the ensemble of decision trees can fully exploit the useful features of search interface patterns. We have compared our ensemble approach with other well-known classification algorithms, such as SVM, C4.5, Naïve Bayes, and original random forest algorithm (RFA). The experimental results have shown that our method is more effective in detecting search interfaces of the hidden Web. Keywords: Search Interface Detection, Random Forest, Hidden Web, Form Classification 1. Introduction Hidden Web refers to the Web pages that are dynamically generated from searchable structured or unstructured databases. Different from the Publicly Indexable Web that is accessible through static hyperlinks, the pages in a hidden Web can only be obtained through queries submitted via the search interface to the databases containing data about the hidden Web. Search interfaces are usually encoded as HTML forms that need to be filled out and  ＊ Shenzhen Graduate School, Harbin Institute of Technology, China E-mail: yeyunming@hit.edu.cn; {wave118, dawndeng}@gmail.com ＋ E-Business Technolgy Institute, The University of Hong Kong, Pokfulam Road, Hong Kong E-mail: jhuang@eti.hku.hk [Received August 31, 2008; Revised February 23, 2009; Accepted February 16, 2009 ]  388  Yunming Ye et al.  submitted by users to obtain information. On the Web, there are many different HTML forms, and many of them are not search interfaces (He, Patel, Zhang, & Chang, 2007). To extract useful information from hidden Web pages, effectively detecting the search interfaces is an essential step since the interface is the only entrance to the hidden Web. Therefore, we first need to find the entrance to the hidden database. The entrance (i.e., search forms) is mixed with lots of non-search forms in HTML pages. Thus, it is very important to distinguish the two types of forms in order to enable the Hidden Web crawler to locate the entrance and extract information further. Information extraction from the hidden Web has been a hot research topic (BrightPlanet.com, 2000) since the term “Hidden Web” was first coined (Florescu, Levy, & Mendelzon, 1998). Most previous work, however, has been focused on the problems of automatic query generation (Ntoulas, Zerfos, & Cho, 2005), form filling (Cavelee, Liu, & Probe, 2004), and wrapper generation for extracting structured information (Wang & Lochovsky, 2003), where detecting search interface was performed manually or by some heuristic methods. Using heuristic rules to find search forms is the simplest and most effective method (Raghavan & Garcia-Molina, 2001; Lage, Silva, Golgher, & Laender, 2004). As hidden Web sites adopt different search forms, though, it is time-consuming to compose different rules for different search forms. Employing machine learning and information retrieval techniques to learn classification models from the content of search forms and using the models to classify different forms automatically is a more desirable approach with respect to scalability and robustness. This approach regards search interface detection as a two-class classification problem. One example of this is using decision trees to build form classification models to detect search interface (Cope, Craswell, & Hawking, 2003). Automatic search interface detection was first explored by Raghavan and Garcia-Molina in their hidden Web crawler HiWE (Hidden Web Exposer) (Raghavan & Garcia-Molina, 2001). Their crawling system used heuristic rules to detect the search entrance to the hidden database. Juliano P. Lage (Lage, Silva, Golgher, & Laender, 2004) used two heuristic rules to perform detection tasks. The first heuristic was the same as HiWE. The second one was to check whether the form contains the “password” HTML element or not. The disadvantage of this method, however, lies in that it does not have auto-leaning capability. Moreover, it is not robust and scalable to diverse hidden Web databases because the rules are too simple to match different form structures. Cope et al. (2003)used a decision tree classification algorithm to detect search interfaces. This method usually generates long rules due to the large size of the feature space in the training set (the number of training samples is too small compared to the number of features). Therefore, it is prone to overfitting, and the classification precision is not satisfying. Zhang et al. (2004) presented a best-effort parsing framework to address the problem of  Feature Weighting Random Forest for Detection of  389  Hidden Web Search Interfaces  understanding Web search interfaces. The authors transformed search interfaces into a visual language under the hypothesis that automatic construction of search interfaces is guided by a hidden syntax. This hypothesis enables parsing as a principled framework to understand the semantic model of the visual language. The experimental results testified the effectiveness of their approach. To summarize, little previous work has addressed the special characteristics of the search interface detection domain, for instance, large and diverse features, small size of training samples with many missing values, etc. The high dimension and sparse data of search interfaces present a tricky problem for the traditional single classifier approach. As collecting training samples (i.e., HTML forms) is costly, the training data set is usually small, while the number of features in the learning space is relatively large, due to multi-type features existing in forms. It’s difficult for a single classifier to fully exploit the rich feature space (very sparse in the data matrix). For many classification methods, the single classifier tends to be overfitting. To attack this problem, we propose a multi-classifier ensemble approach in this paper. Our method is based on the random forest algorithm. A random forest model consists of a set of decision trees that are constructed by bootstrapping the training data. In our approach, we develop a weighted feature selection method to select a subset of features for each decision tree in the tree induction process. Classification is made by aggregating predictions of individual decision trees in the forest. Since each classifier is learned from a subset of the feature space, the ensemble approach can fully exploit the useful features in search forms. We have conducted experiments on several real data sets. The experimental results have shown that our random forest approach improves the classification accuracy in search interface detection. The contributions of this paper can be summarized as follows: 1. We explored the random forest approach to attacking the problem of detecting search interfaces from the sparse feature space of hidden Webs where specific feature extraction and representation techniques were used. 2. We extended the random forest algorithm with a weighted feature selection method to select a subset of features for each decision tree. The new algorithm can automatically remove the noisy features in search forms so that decision tree classifiers can be learned from more representative subsets of the feature space. 3. We conducted experiments on real data sets to compare the improved random forest algorithm with other well-known classification algorithms, such as SVM and C4.5. The experimental results have shown that the new method is more effective in detecting search interfaces of the hidden Web.  390  Yunming Ye et al.  The rest of this paper is organized as follows. In Section 2, we formalize the detecting search interface problem as a form classification problem and present the feature extraction techniques. Section 3 describes the improved random forest algorithm for form classification. Experimental results and analysis are presented in Section 4. Section 5 concludes this paper and presents our future work. 2. Feature Extraction for Form Classification Search interface detection is a process of distinguishing the search forms of the hidden Web from non-search forms. It is a two-class classification problem in machine learning. This section describes how to extract form features from HTML pages and discusses the characteristics of the data matrix for form classification. 2.1 Feature Extraction Rules An HTML form usually begins with the tag < FORM > and ends with the tag < /FORM > . According to this rule, HTML forms can be extracted by searching the < FORM > tag in HTML pages. Each extracted form is a sample in the training set. The features of each form are generated by parsing the corresponding < FORM > HTML block. HTML forms contain two kinds of features: one is the attributes of forms and elements, and the other is the statistics of those attributes. A form mainly contains four kinds of elements, that is, “INPUT”, “SELECT”, “LABEL” and “TEXTAREA”, which are the children elements of “FORM” element. Element “INPUT” contains several types, such as “text”, “hidden”, etc. The hierarchy of a form is shown in Figure 1. All of these elements contain a set of attributes, such as “name”, “value”, “size”, etc. The attributes of “form” elements are “method”, “action”, and “name”. Attribute “method” indicates the method for the form to submit query data, such as “POST” or “GET”. Attribute “action” indicates the address of the corresponding server of the form, and attribute “name” indicates the name of the form. Some elements and attributes can be removed because they are not useful for form classification, for instance “option”, “size”, “width”, etc. Besides, the statistics about the number of elements or attributes in each element can also be computed as important features.  Figure 1. The hierarchy of form elements  Feature Weighting Random Forest for Detection of  391  Hidden Web Search Interfaces  Figure 2 shows an example of a search form. The form contains three elements: one “SELECT” element and two “INPUT” elements. There are three attributes in the “SELECT” element : “name” with value “and”, “size” with value “1”, and “width” with value “50”. It also contains several “OPTION” elements. The “size” and “width” attributes, along with the “OPTION” elements are not useful for form classification, so they can be removed. The corresponding HTML codes are shown in Figure 3.  Figure 2. A search form  Figure 3. The HTML codes of the form in Figure 2 Figure 4 shows an example of a non-search form. The corresponding HTML codes are shown in Figure 5. According the feature extraction rules, the useful form elements in this form include “INPUT”, “LABEL”, and “FORM”, which can be used to compose the feature space. Elements “TABEL”, “FONT”, and “TR” can be removed because they are not useful. Figure 4. A non-search form  392  Yunming Ye et al.  Figure 5. The HTML codes of the form in Figure 4 There are some important differences between the features of search form and non-search form. First, the number of “INPUT”, “SELECT”, and “TEXTAREA” elements in search forms is larger than that in non-search forms. Second, the value of the “method” attribute in “FORM” elements is always set as “POST” in search forms, while it is always set as “GET” in non-search forms. Moreover, the elements’ values of search forms often contain some keywords such as “search”, “find”, or other words that have the same meaning as “search”. These differences, however, are not the only decisive factors. There are other features that can be explored by classification algorithms. According to the major differences, six kinds of rules are used in the feature extraction process as follows: 1. Extract the “name” attribute values from “input”, “select”, “textarea”, and “label” elements; 2. Extract the “value” attribute value from “input”, “textarea”, and “label” elements; 3. Extract the “name” and “method” attribute values from “form” elements; 4. Extract the words that appear between slashes(/) in the “action” attributes of the “form” elements; 5. Extract the words that appear between slashes(/) in the “src” and “alt” attributes of the “input-image” element; 6. Calculate the number of “input”, “select”, “label”, and “textarea” elements in each form. The next step is to standardize the value of the features that are extracted from the forms.  Feature Weighting Random Forest for Detection of  393  Hidden Web Search Interfaces  First, all strings are transformed into lowercases; then the string type values are aggregated and mapped to specific enumerating values. For instance, the values of “search”, “find”, or “srch” are mapped to “search”.  2.2 The Sparse Data Matrix  The extracted features and the labels of forms are used to compose the data matrix for the classification algorithm. The formalized data matrix is shown in Table 1.  Table 1. The data matrix for form classification  class t1  t 2  t i  t m  c 1  a11 a12  a1i  a1m  c j  a j1 a j2  a ji  a jm  c n  an1 an2  ani  anm  The set T = {t1,t2 ,tm} in Table 1 represents the names of the form features. For form classification, the label of a form is represented as an element in the set C = {yes, no} , while “yes” indicates that the form is a search interface of hidden Web and “no” indicates a non-search interface. Each row is a sample form. a ji represents the value of feature ti in the j th form, and c j indicates the class of the j th form. Table 2 illustrates two examples of a search form and a non-search form as shown in Figure 2 and Figure 4.  The expression of ti is a four-tuple of “element name”-“type”-“attribute name”-“sequence number”. The “element name” contains six values: “FORM”, “SELECT”, “INPUT”, “TEXT AREA”, “LABEL”, and their statistics. For element, “INPUT”, the value of “type” can be “text”, “hidden”, and so on. “attribute name” has six options: “name”, “value”, “src”, “alt”, “method”, and “action”. Sequence number represents the sequence of the features in the form.  As illustrated in Table 2, the combination of “element name”,“type”,“attribute name”, and “sequence number” has many unique alternatives. This will result in a high-dimensional feature space for form classification. Furthermore, since each form has just a few features, the data matrix for classification is very sparse and there are many missing values and noisy features. This problem presents a big challenge for search form detection.  Table 2. Two examples of form vectors  class form-action-1 form-action-2 form-action-3  yes www.thearda.com cgi-bin  search  no  servlet  login  ?  input-text-number input-submit-number  
In recent years, the hierarchical taxonomy integration problem has obtained considerable attention in many research studies. Many types of implicit information embedded in the source taxonomy are explored to improve the integration performance. The semantic information embedded in the source taxonomy, however, has not been discussed in previous research. In this paper, an enhanced integration approach called SFE (Semantic Feature Expansion) is proposed to exploit the semantic information of the category-specific terms. From our experiments on two hierarchical Web taxonomies, the results show that the integration performance can be further improved with the SFE scheme. Keywords: Hierarchical Taxonomy Integration, Semantic Feature Expansion, Category-Specific Terms, Hierarchical Thesauri Information 1. Introduction In many daily information processing tasks, merging two classified information sources to create a larger taxonomy with abundant information is in great demand. For example, an e-commerce service provider may merge various catalogs from other vendors into its local catalog to provide customers with versatile contents. A Web user may also want to integrate different blog catalogs from Web 2.0 portals to organize a personal information management library. In these examples, people may need an efficient automatic integration approach to process the huge amount of information. In recent years, the taxonomy integration problem has obtained much attention in many research studies (e.g. Agrawal & Srikan, 2001; Sarawagi, Chakrabarti, & Godbole, 2003; * Dept. of Computer Sci. and Eng., Yuan Ze University, 135 Yuan-Tung Rd., Chungli, 320, Taiwan. Tel.: +886-3-4638800 ext: 2361 Fax: +886-3-4638850. E-mail: {czyang,sean,chris,pjwu}@syslab.cse.yzu.edu.tw [Received September 12, 2008; Revised December 11, 2008; Accepted February 19, 2009]  422  Cheng-Zen Yang et al.  Zhang & Lee, 2004a; Zhang & Lee, 2004b; Zhu, Yang, & Lam, 2004; Chen, Ho, & Yang, 2005; Wu, Tsai, & Hsu, 2005; Ho, Chen, & Yang, 2006; Chen, Ho, & Yang, 2007; Cheng & Wei, 2008; Wu, Tsai, Lee, & Hsu, 2008). As pointed out in these studies, the integration work is more subtle than traditional classification work because the integration accuracy can be further improved with different kinds of implicit information embedded in the source or destination taxonomy. A taxonomy, or catalog, usually contains a set of objects divided into several categories according to some classified characteristics. In the taxonomy integration problem, the objects in a taxonomy, the source taxonomy S, are integrated into another taxonomy, the destination taxonomy D. As shown in earlier research, this problem is more than a traditional document classification problem because different kinds of implicit information in the source taxonomy are explored to greatly help integrate source documents into the destination taxonomy. For example, a Naive Bayes classification approach (Agrawal & Srikan, 2001) with the classification relationship information implicitly existing in the source catalog can achieve integration accuracy improvement. Several SVM (Support Vector Machines) approaches (Chen, Ho, & Yang, 2005) can also have similar improvement with other implicit source information. The implicit source information studied in previous enhanced approaches generally includes the following features: (1) co-occurrence relationships of source objects (Agrawal & Srikan, 2001; Zhu, Yang, & Lam, 2004; Chen, Ho, & Yang, 2005), (2) latent source-destination mappings (Sarawagi, Chakrabarti, & Godbole, 2003; Zhang & Lee, 2004b; Cheng & Wei, 2008), (3) inter-category centroid information (Zhang & Lee, 2004a), and (4) parent-children relationships in the source hierarchy (Wu, Tsai, & Hsu, 2005; Ho, Chen, & Yang, 2006; Wu, Tsai, Lee, & Hsu, 2008). In our survey, however, the semantic information embedded in the source taxonomy has not been discussed. Since different applications have shown that the semantic information can benefit the task performance (Krikos, Stamou, Kokosis, Ntoulas, & Christodoulakis, 2005; Hsu, Tsai, & Chen, 2006), such information should be able to achieve similar improvements for taxonomy integration. In addition, we further study the hierarchical taxonomy integration problem because many taxonomies, such as Web catalogs, existing in the real world are hierarchical. In this paper, we propose an enhanced integration approach by exploiting the implicit semantic information in the source taxonomy with a semantic feature expansion (SFE) mechanism. The basic idea behind SFE is that some semantically related terms can be found to represent a source category, and these representative terms can be further viewed as the additional common category labels for all documents in the category. Augmented with these additional semantic category labels, the source documents should be more precisely integrated into the correct destination category. The semantic expanding scheme, however, needs to consider the polysemy situation to avoid introducing many topic-irrelevant features. Therefore,  Hierarchical Taxonomy Integration Using  423  Semantic Feature Expansion on Category-Specific Terms  SFE employs an efficient correlation coefficient method to select representative semantically-related terms. To study the effectiveness of SFE, we implemented it based on a hierarchical taxonomy integration approach (EHCI) proposed in Ho et al. (2006) and Chen et al. (2007) with the Maximum Entropy (ME) model classifiers. We have conducted experiments with real-world Web catalogs from Yahoo! and Google, and measured the integration performance with precision, recall, and F1 measures. The results show that the SFE mechanism consistently can improve the integration performance of the EHCI approach. The rest of the paper is organized as follows. Section 2 describes the problem definition and Section 3 reviews previous related research. Section 4 elaborates the proposed semantic feature expansion approach and the hierarchical integration process. Section 5 presents the experimental results, and discusses the factors that influence the experiments. Section 6 concludes the paper and discusses some future directions of our work. 2. Problem Statement Following the definitions in Ho et al. (2006), we assume that two homogeneous hierarchical taxonomies, the source taxonomy S and the destination taxonomy D, participate in the integration process. The taxonomies are said to be homogeneous if the topics of the two taxonomies are similar. In addition, the taxonomies under consideration are required to overlap with a significant number of common documents. For example, in our experimental data sets, 20.6% of the total documents (436/2117) in the Autos directory of Yahoo! also appear in the corresponding Google directory.  S  D  …  S1  Sm  d  
Wikipedia is the world’s largest collaboratively edited source of encyclopedic knowledge. Wikibook is a sub-project of Wikipedia that is intended to create a book that can be edited by various contributors, similar to how Wikipedia is composed and edited. Editing a book, however, requires more effort than editing separate articles. Therefore, methods of quickly prototyping a book is a new research issue. In this paper, we investigate how to automatically extract content from Wikipedia and generate a prototype of a Wikibook as a start point for further editing. Applying search technology, our system can retrieve relevant articles from Wikipedia. A table of contents is built automatically and is based on a two-stage searching method. Our experiments show that, given a keyword as the title of a book, our system can generate a table of contents, which can be treated as a prototype of a Wikibook. Such a system can help free textbook editing. We propose an evaluation method based on the comparison of system results to a traditional textbook and show the coverage of our system. Keywords: Wikipedia, Wikibook, Table of Contents Generation 1. Introduction The ability to quickly construct a free encyclopedia, such as Wikipedia, has shown that the Web 2.0 has been successful. Community and interactivity among users on the Internet has become a popular topic. A project named “Science Online,” which brought the wiki scheme to schools, lets students participate in collaborative writing (Forte & Bruckman, 2007). Wikipedia is useful in college education, both for general topics (Lally & Dunford, 2007, May/June) and for specific topics, such as physics (Muchnik, Itzhack, Solomon, & Louzoun, 2007). In this paper, we focus on another project of the Wikimedia Foundation, Wikibook, which is also useful in the classroom (Sajjapanroj, Bonk, Lee, & Lin, 2006). Wikibook provides free textbooks on the Internet via the Wiki system, letting global users edit the  * Department of Computer Science and Information Engineering, Chaoyang University of Technology, Taiwan R.O.C. E-mail :shwu@cyut.edu.tw [Received September 15, 2008; Revised Mar. 19, 2009; Accepted Mar. 25, 2009]  444  Jen-Liang Chou, and Shih-Hung Wu  contents of textbooks. Creating a book without supporting data, however, is difficult. An expert or a system that can provide a general framework and useful references for a book is of much help. Thus, we propose an automatic Wikibook prototyping system that can pick relevant articles from Wikipedia and construct a hierarchy as the table of contents for a given topic. Our system consists of information retrieval and web mining technology. In previous work, the TOC and anchor text of a Wikipedia entry has been used to form the TOC of a Wikibook (Yang, Han, Oh, & Kwak, 2007). A relevant research area is Topic Maps. Topic Maps are analogous to the Table of Contents (TOC) of a textbook. Users can realize and memorize the relevant concepts of a topic. “Topic Maps for learning” (TM4L) (Dicheva & Dichev, 2005) is an application of Topic Maps. These methods, though, mostly rely on humans without using information retrieval technology, which can provide a considerable amount of relevant information. Studies in knowledge acquisition provide some hints. Semi-automatic methods can aggregate a domain ontology via Internet search (Roberson & Dicheva, 2007). We propose a method that can help prototype a Wikibook automatically using the contents from Wikipedia. In the following sections, we will describe our methodology in Section 2, system implementation in Section 3, experimental results in Section 4, discussions in Section 5, and give conclusion in the final section. 2. Methodology We propose a framework for Wikibook prototyping which involves several modules. These modules can be replaced to fulfill the needs of different requirements. For example, we might customize the system for different languages, users of different ages, or topics with different contexts. 2.1 Corpus Preparation The first module is the preparation of a corpus. We can use the whole of Wikipedia, certain language versions, or subsets as the searching target. The system then extracts and analyzes contents from the corpus. As a knowledge source, Wikipedia provides not only the content but also a lot of links to contexts, which are also valuable. We will extract keywords from the content of Wikipedia pages, and will find related terms from the anchor text in these pages. 2.2 Search Engine and Anchor Text Miner The second module is a search engine and an anchor text miner. As mentioned above, relevant topics can be found not only from a full text keyword search, but also from links in Wikipedia. This module is important from the technical point of view. Our system searches relevant  Automatic Wikibook Prototyping via Mining Wikipedia  445  topics and their hyponyms using information retrieval technology. On the other hand, the anchor text miner can extract related terms from the anchor texts of the retrieved pages. 2.3 Hierarchical Construction Given relevant topics, our system then generates a hierarchy. This hierarchy can be viewed as the table of contents of a Wikibook for further editing. 3. Implementation issues Our methodology gives a general idea on how to generate a Wikibook automatically within a flexible framework. In the following sections, we discuss our system and experiments on computer science topics in both the English and Chinese versions of Wikipedia. Figure 1 shows the architecture of our system.  Wikipedia Articles indexing Wikipedia full-text index  keyword User Search  Article title (Heading) Sub-Search Article title (sub-heading)  Mining anchor text Anchor text  1. Heading 1 2. Heading 2 2.1 sub-heading 3. Heading 3 3.1 sub-heading 3.2 sub-heading 4. Heading 4 ……… TOC Figure 1. System Architecture  446  Jen-Liang Chou, and Shih-Hung Wu  3.1 Corpus Choice We chose Wikipedia as our corpus in the experiments for two reasons. First, the idea of the availability of the corpus; it is an ample resource of information which is available for every potential Wikibook editor and there are no copyright issues in regards to using the content for a Wikibook. Second, the quality of content: there is more professional knowledge in Wikipedia than in general Websites, and potential editors can compile the content from Wikipedia with less effort.  3.2 Search Strategy  Our search strategy is automatic two-stage iterative searching. The system takes a term as the query and performs context searching via a standard information retrieval process. We use pseudo-relevance feedback as our searching algorithm. The resulting set of the first search will be used as the corpus in the second stage.  The system ranks the search results according to a traditional TF-IDF scoring function  that is restated in Formula (1) where Scorei is the ranking score of an article, i denotes an article, j denotes a term occurring within the query, and T denotes the number of terms in the  query. TFj is the frequency of j occurring within the article title, TFij is the frequency that term j occurs in article i. We assume tokenij = { Ti − j } as the number of the rest words after deleting term j from the article i, where Ti is the number of terms in the article i:  ⎛  ⎞  ⎜  Scorei  =  T ∑  ⎜ ⎜  j⎜  ⎜⎝  ( ) TFj × IDFj ∑ TFj × IDFj j  2  ⎟ ⎟ ⎟ ⎟  ×  ⎛ ⎜ ⎜⎝  TFij × IDF token j  j  ⎞ ⎟ ⎟⎠  ⎟⎠  (1)  where  IDFj  ⎛ = log ⎜ ⎜⎝  D Dj  ⎞ + 1⎟ , measures term j involvement in other documents, D is all the ⎟⎠  articles in the index, Dj is articles that contain j.  Table 1 shows a collection of four documents, and the corresponding value of each  variable in Formula (1) is shown in Table 2. Suppose the query terms are “A B C”, then T is 3,  and i can be 1 to 4 as the Doc ID in Table 1. Then, the Score of a document according to the  formula is shown in the last column of Table 2. Where Score1 = 0.551; Score2 = 0.260; Score3 = 0.675; Score4 = 0, we rank these scores from high to low. From this, we can attain results such that, if we input the query “A B C”, the system will output the order of the document as 3,  1, 2, 4.  Automatic Wikibook Prototyping via Mining Wikipedia  447  Table 1. Example Documents  Doc ID Title  Contents  
In order to train machines to ‘understand’ natural language, we propose a meaning representation mechanism called E-HowNet to encode lexical senses. In this paper, we take interrogatives as examples to demonstrate the mechanisms of semantic representation and composition of interrogative constructions under the framework of E-HowNet. We classify the interrogative words into five classes according to their query types, and represent each type of interrogatives with fine-grained features and operators. The process of semantic composition and the difficulties of representation, such as word sense disambiguation, are addressed. Finally, machine understanding is tested by showing how machines derive the same deep semantic structure for synonymous sentences with different surface structures. Keywords: Semantic Representation, Sense Disambiguation, Interrogatives, E-HowNet 1. Introduction Electronic dictionaries are designed for the purpose of providing users (or computers) convenient access to relevant knowledge of words to understand language. When we say that a sentence is ‘understood’, we mean that the concepts and the conceptual relations expressed by the sentence are unambiguously identified and we can make the correct inferences/responses. To have a computer understand a sentence, we must have a framework for representing lexical knowledge and performing semantic composition and disambiguation processes. Extended-HowNet (E-HowNet for short) is a frame-based entity-relation knowledge representation model, which was extended from HowNet [Dong et al. 1988] to encode concepts. Concepts are represented and understood by their definitions and association links to ＊CKIP, Institute of Information Science, Academia Sinica E-mail: kchen@iis.sinica.edu.tw ＋Department of Language and Literature Studies, National Hsinchu University of Education E-mail: slhuang@mail.nhcue.edu.tw [Received November 27, 2007; Revised August 12, 2008; Accepted August 27, 2008]  256  Shu-Ling Huang, and Keh-Jiann Chen  other concepts. Compared to WordNet, HowNet’s architecture provides richer information apart from hyponymy relations. It also enriches relational links between words via encoded feature relations. The advantages of HowNet are (a) its inherent properties are derived from encoded feature relations in addition to hypernym concepts, and (b) information regarding conceptual differences between different concepts and information regarding morph-semantic structure are encoded. Therefore, we adopt a similar mechanism to define word sense in E-HowNet, but represent concepts in a more accurate and flexible way by (a) defining new concepts by well-defined concepts, (b) providing a uniform representational framework for both function words and content words, and (c) embedding semantic composition and decomposition capabilities. More detailed discussions can be seen at [Chen et al. 2005]. In E-HowNet, we define each lexical sense by the composition of well-defined concepts and/or basic concepts, called sememes in HowNet. The sememes are linked to their sense equivalence WordNet synsets [Fellbaum 1998]. Take 土地公 ‘God of earth’ as an example: (1) 土地公 Tu di gong ‘God of earth’ Def:{God|神:telic={manage|管理:patient={land|陸地},agent={~}}} Here, ‘God’ is the hypernym of the target word 土地公 ‘God of earth’, ‘manage’ and ‘land’ are its related concepts. ‘telic’, ‘patient’, and ‘agent’ are relations which link these concepts. Obviously, to achieve mechanical understanding of natural language, the same or similar concepts must have the same or similar underlying semantic representation. However, natural language can be ambiguous. Different sentences might express the same meaning, and the same sentence can also express different meanings. The following sentences (2) and (3) show the former phenomenon, and (4) and (5) show the latter: (2) 我能否拍照？ Wo neng fou pai zhao? Is it OK for me to take pictures? (3) 我可不可以照相？ Wo ke bu ke yi zhao xiang? Can I take photos? (4) 土地公有政策。Tu di gong you zheng ce. The policy of public sharing of the land. (5) 土地公有政策。Tu di gong you zheng ce. God of earth has his policy. Thus, transforming the surface structure of a sentence into a canonical semantic representation  Knowledge Representation and Sense Disambiguation for  257  Interrogatives in E-HowNet  and simultaneously solving the problem of word sense ambiguity are major research issues. In summary, lexical semantic representation and composition (including disambiguation) are the most demanding techniques for understanding natural language by machines and the design of E-HowNet is aimed for these objectives. In this paper, we will take interrogatives as examples to demonstrate the mechanism of lexical semantic representation and composition in E-HowNet [Chen et al. 2004]. The goal is to achieve near canonical semantic representation for synonyms and sense equivalent sentences. Take sentences (2) and (3) as examples. Although their syntactic structure and surface strings are very different, by composing lexical sense representations, we hope the machine can ‘understand’ synonymy of sentences in different surface forms. Analysis of interrogative constructions is of great interest to linguists, as well as to computer scientists, for example, those who are engaged in QA techniques. Interrogative constructions have played a central role in the development of modern syntactic theory. Ginzburg and A. Sag [2000] have pointed out that the interrogative has been at the heart of work in generative grammar, along with government and binding (GB) theory and head-driven phrase structure grammar (HPSG). Nonetheless, to date, most syntacticians take quite different approaches from semantic and pragmatic points of view on interrogatives. Taking questions in Mandarin Chinese as example, Shao [1996] has summed up the current study of interrogatives and listed the main research themes as follows: the type of question, interrogative particles, querying focus and its answer, degree of doubt, special interrogative sentence patterns, etc. Most of the above themes are purely grammatical analysis. To build a frame-based entity-relation knowledge representation model, we find interrogative construction a good and challenging example because it combines problems of syntax, semantics, and pragmatics. In the following section, we briefly describe the previous works for interrogatives. Then, we introduce our analysis of type classifications for interrogatives and their representation in E-HowNet. Next, we present the semantic composition process of interrogative sentences and the difficulties encountered. We conclude the paper by discussing our results and future work. 2. Background Interrogatives in Chinese studies are traditionally attributed to the mood category of syntax. Ma [1935] wrote the first grammar book for Mandarin Chinese. He classified interrogatives into the mood category. Later, Li [1930] and Lv [1942] carried forward his viewpoint and influenced modern linguistic theory on interrogatives deeply. Most linguists consider there to be four grammatical types that explicitly mark an utterance as an interrogative [Lv 1942; Li and Thompson 1997; Tang 1983; Lu 1984; Shao 1996]. First, is a question which can be answered by ‘yes’ or ‘no’, called a factual question, true and false interrogative, or yes/no  258  Shu-Ling Huang, and Keh-Jiann Chen  interrogative. Second, is a question which includes Wh-words such as ‘who’, ‘what’, and ‘when’, called a Wh-word interrogative, or an information seeking interrogative. Third, is a question which mentions two or more possible alternative answers, called a disjunctive interrogative or an either/or interrogative. Fourth, is a question which is composed of a statement followed by an A not A form, such as dui bu dui ‘right or wrong’, xing bu xing ‘accept or reject’, etc., called an A not A interrogative or a tag interrogative.1 From different analytical perspectives, these four question types may have different hierarchy. For example, Lv [1942] viewed them as (6) while Lu [1984] structured them as (7): (6) interrogative – Wh-word interrogative – true/false interrogative – A not A interrogative – disjunctive interrogative (7) interrogative – true/false interrogative – non true/false interrogative – Wh-word interrogative – disjunctive interrogative – A not A interrogative Generally speaking, true and false interrogative and Wh-word interrogative are regarded as basic types. 3. Semantic Representation for Interrogatives 3.1 Our Classification of Interrogatives For QA applications, we are more concerned about semantic discrimination of different interrogatives. Therefore, we take a sense-based approach to create a hierarchical classification which is guided by a layered semantic hierarchy of answer types, and eventually classify interrogative sentences into fine-grained classes, shown as (8):  
We observe that current language resource tools only provide limited help for ESL/EFL writers with insufficient language knowledge. In particular, there is no convenient way for ESL/EFL writers to look for answers to the frequent questions of correct and appropriate language use. We have developed a language information retrieval method to exploit corporal resources and provide effective referential utility for ESL/EFL writing. This method involves the sequential operation of three modules, an expression element module, a retrieval module, and a ranking module. The primary design purpose is to allow flexible and easy transformation from questions to queries and to find relevant examples so that uncertainty of language use can be quickly resolved. We implemented the method and developed a prototype system called SAW (Sentence Assistance for Writing). Simulated language use problems were tested on SAW to evaluate the system’s referential utility. Experimental results indicate that the proposed language information retrieval method is effective in providing help to ESL/EFL writers. Keywords: Language Information Retrieval, Language Resources, ESL/EFL Writing 1. Introduction Writing is a significant form of expression for conveying information and experiences. Effective writing demands appropriate articulation of meaning and prudent selection of words. For many ESL/EFL (English as Second Language/English as Foreign Language) learners, writing in English is an especially difficult task in which authors constantly face uncertainty as far as how to convert their thoughts into the second language. Writing production of ESL/EFL learners is also error-prone due to insufficient knowledge of the second language ＊ Department of Computer Science, National Chengchi University, Taipei, Taiwan E-mail: jsliu@cs.nccu.edu.tw ＋ Department of English, National Taiwan Normal University, Taipei, Taiwan ＃ Department of Applied Foreign Languages, Kang Ning Junior College, Taipei, Taiwan [Received January 13, 2008; Revised June 20, 2008; Accepted August 8, 2008]  280  Jyi-Shane Liu et al.  and the interference of the native language [Kobayashi and Rinnert 1992]. At the same time, writing in English (as a second language) is a very time-consuming process. ESL/EFL writers usually take considerable time in searching for the right ways of language expression with help from various language resources and tools. However, the substantial amount of time spent by the authors often results in less than satisfactory improvement in writing quality. This has frustrated many ESL/EFL writers and has become a major obstacle in effective writing production. In view of English writing as an information processing task by ESL/EFL writers, the difficulty of the task seems to be related to a number of factors, such as language information insufficiency, costly language information acquisition, and limited gain in language information use. Recent developments in corpus linguistics have offered a new aspect for the study of language teaching and learning. A corpus is a large collection of sampled texts from existing written pieces or spoken records presented in electronic form [Marcus et al. 1993]. Subsequently, a corpus can be compiled to reflect the natural and actual occurrence of language use; thus, it provides abundant language resources for linguistics study and language acquisition. For instance, corpus linguistics has exploited corporal data for statistical analysis and comparative interpretation of language use [McEnery and Wilson 1996]. Efforts have also been directed to utilize corporal data for language learning references [Conrad 1999; Tsui 2005]. Indeed, corporal data allow many forms of data processing tasks to derive language information for various purposes. In particular, the idea of using a corpus as a book of reference is especially appealing to second language writers who beg for in-context examples of actual usage, so that uncertainty can be resolved and text fragments can be verified and re-used. To this end, an effective tool must focus on second language writers' special needs and provide referential corporal cases that fill in their unknown gaps of language use. The benefit of such a solution to ESL/EFL writing includes increased writing efficiency, improved writing quality, and better writing experience. One of the primary tools for corpus exploration is concordancing. A concordance is a list of occurrences of a particular word with their immediate context drawn from a collection of texts. The targeted word is referred to as keyword. A concordance is usually displayed as a series of text lines in which the keyword is centered in its context. Concordancing has been frequently used as an analytic tool in linguistics for various issues of language study, such as analyzing word usage, computing word frequencies, finding and analyzing collocational units [Sinclair 1991]. The use of concordances in English Language Teaching (ELT) has also been advocated so that students learn certain language phenomena in an inductive way [Weber 2001; Sun 2003; de O'Sullivan and Chambers 2006]. However, Yoon [2005] pointed out that a concordance is not particularly helpful for ESL/EFL writers. Unlike language study and learning in which adequate exploration is encouraged, second language writing is a  A Language Information Retrieval Approach to Writing Assistance  281  time-constrained problem-solving task. Language information critically needed and directly useful in the ESL/EFL writing process is not readily available through concordancing. From the point of view of information processing techniques, concordancing is a basic level application of information retrieval. As discussed above, concordancing employs a simple query form, straightforward hit or miss decision, and no ranking on retrieved results. In essence, a concordance is intended to be used as an overall observation tool on language phenomena of specific target words. The approach is less than satisfactory, and sometimes is even incapable of supplying useful information for the guidance and referential help needed by ESL/EFL writers. After all, writing is a production process that is based on language knowledge. In order to successfully convey the intended message, authors must contemplate the correct use of vocabulary and arrange appropriate word combinations and sequences, so as to construct concrete and coherent text content. Being language deficient in the text content construction process, ESL/EFL writers constantly face uncertainty and need to look for answers. Sometimes, they may even get off on the wrong foot due to misconceptions and spend much time in vain. Many ESL/EFL writers have been struggling with these problems and have not received sufficient help from the currently available language resources and tools. We stress that a new corpus exploration tool must be developed to better assist ESL/EFL writers. Such a writing assistance tool would need to offer more flexible types of queries and retrievals and would need to present the results that best suit users' needs in the writing process. We propose a language information retrieval method to address the problem of corpus utilization. The approach is intended to help language deficient ESL/EFL writers find useful language use examples from a corpus and assist their decision making on correct language use. Our language information retrieval method includes three modules. The first module is a flexible expression model to allow a variety of combinations of semantic elements that reflect users' partial language knowledge. The semantic elements may include words, collocations, phrases, and formulaic expressions in complete or partial forms. Users can form a query by combining these semantic elements that are partly known and partly unknown. The second module is a selective retrieval mechanism with search options of exact match and partial match. Both options are supported, so as to retrieve adequate and useful examples according to the user’s confidence level on the query. The third module is an evaluation and ranking mechanism. After a submitted query and a selected search option, the retrieved results are evaluated for their consultation values and ranked accordingly. The final output is a list of exemplar sentences with decreasing consultation values so that users can receive help quickly. We implemented the approach and developed a corpus utilization tool for the purpose of ESL/EFL writing assistance. The tool is named SAW (Sentence Assistance for Writing). We used both objective measures and human subjects to gauge SAW's performance and observed  282  Jyi-Shane Liu et al.  that SAW is capable of providing adequate references and satisfactory guidance for users’ language information need in the ESL/EFL writing process. The results attest to the efficacy of the proposed language information retrieval method in exploiting corpus to assist ESL/EFL writing. 2. ESL/EFL Writing and Language Resource Tools Compared to other language skills such as listening, speaking, and reading, writing is usually considered to be more difficult to develop and requires more in-depth language cognition to perform. High-quality written texts are founded on comprehensive language knowledge and its skillful utilization in production. Besides being language deficient, ESL/EFL writers are also subject to interference from their native languages. Both factors lead to text production that may contain incorrect words, grammar, and structures. For instance, to express the notion of music composition in a verb-noun pair, Chinese students tend to use "make", "create", or "produce" as the verb with "music" as the noun. Previous research indicated that ESL/EFL writers need three types of pre-requisite knowledge - words, collocations, and grammatical structures [Shei and Pain 2000; Altenberg and Granger 2001]. Among them, collocations are particularly unfamiliar to ESL/EFL writers. Collocations are a small group of words that co-occur with high frequency and become fixed word combinations. For example, the word "problem" as a noun usually goes with "cause", "create", and "solve" as verbs in an English verb-noun pair. In contrast, the combination of "make" as a verb and "problem" as a noun is rare, yet it is a straightforward translation from Chinese. Collocations are commonly accepted agreements and habits in language use and are not transitive from language to language. Ilson et al. [1997] classified collocations into two types - grammatical collocations and lexical collocations. Grammatical collocations refer to compositions of a dominant word and a preposition, an article, or a conjunction, such as "decide on" and "determined by". Lexical collocations are frequently used combinations of noun, verb, adjective, and adverb, such as "strong tea", "absolutely not", and "notoriously difficult". Due to the interference of native languages, ESL/EFL writers are particularly prone to collocation errors, which may find resolution from better utilization of language resources. A recent trend in language resources development has been to exploit a corpus for language use information in practical contexts [Biber et al. 1998]. Given the need to investigate the language phenomena of a specific word, corpus tools are used to provide both a quantitative assessment and actual examples of the different usage situations. Take Collins-Cobuild English Dictionary, published by Haper-Collins, as an example. The tool leverages an in-house corpus to provide collocational information on the target word. The collocational information includes a list of co-occurring words that are immediately before or after the lookup word. The co-occurrence list is also statistically assessed and ranked by  A Language Information Retrieval Approach to Writing Assistance  283  T-scores. The collocational information of words has also been of interest to the lexicography community. In general, the purpose is to exploit corporal resources to investigate lexical behavior with the use of statistical measures of word co-occurrence. One of the representative studies is the work of Word Sketches, developed by Kilgarriff and associates at the University of Brighton [Kilgarriff and Rundell 2002]. Word Sketches is lexical profiling software designed for lexicographers to uncover the key features of a word's behavior. An inventory of grammatical relations is adopted to provide target types for developing collocation lists. Given lexicographic interest in a particular word, the output of Word Sketches is a list of words categorized by grammatical relations to the input word and associated by a statistical measure of its collocational significance. Another type of corpus tool emphasizes offering reference examples of the lookup item and allows more variety of lookup items. For instance, VIEW, developed by Mark Davies at Brigham Young University, accepts lookup items in the type of a word, a partial word, a part-of-speech, or a phrase [Davies 2005]. The tool works on the British National Corpus as its language resource and produces a keyword in context as the primary output format. The output is a list of contexts, a specified window of words around the target, in which the lookup item appear. No filtering or ranking is attempted. Users are expected to look for useful information among the overloaded list of appearance. Our position on language resources are in line with the current trend of corpus tools development. However, we argue that, for the purpose of ESL/EFL writing assistance, a specialized corpus tool should be developed to attend to the immediate need of language use decision in the writing task. Due to the difference in language cognition and in the levels of language knowledge, ESL/EFL writers often use a variety of query items in hope of obtaining potential references for the same intended expression element. However, current corpus tools are restricted in the types of allowed query; therefore, they are unable to provide help when users cannot come up with the appropriate queries. For example, some ESL/EFL writers are able to use the exact phrase "by and large" as a query to obtain its usage references. Other ESL/EFL writers who are not familiar with the phrase may try to use "large" or "by large" as query items according to their vague cognition. These incorrect or ambiguous queries would not lead to direct and useful references with the current corpus tools. This is exactly the dilemma of conflict between ESL/EFL writing need and current corpus tools. The lower the language knowledge of the ESL/EFL writer, the more language use help he or she needs. Yet, the usefulness of current corpus tools seems to hinge on the language level of the users. This usage obstacle has excluded many low to middle level ESL/EFL writers from getting help from corpus resources. Much of the problem can be attributed to a lack of proper design in the particular method of assisting less language skillful users.  284  Jyi-Shane Liu et al.  3. Language Information Retrieval and Recommendation We propose a method for language information retrieval to tackle the problem of ineffective corpus utilization for ESL/EFL writing. The language information retrieval method allows incorrect or ambiguous queries and tries to find the best reference examples so that users can explore and confirm the correct expression elements to convey their intended messages. Retrieved reference examples are evaluated in relevance to the user's query that indicates the language information needed. The final set of references are ranked and recommended in a sequential relevance order to provide users an efficient way to decide appropriate use of language. The method is designed to anticipate users with various levels of language knowledge and provide flexible query forms to cope with partial language cognition of users. The data source for retrieval is a corpus (or a set of corpora) and the retrieved results are ranked and displayed with sentences as a unit. This will facilitate users’ ability to observe, learn, and confirm usage of certain expression elements in a complete exemplar sentence. The language information retrieval method is a process that involves receiving a user query as an information need, retrieving sentences from data sources, and recommending a set of relevant sentences in ranked order. The method is implemented with three modules: expression element, retrieval, and ranking. The expression element module enables users to convey their language information need with a set of flexibly combined expression elements. The design is to allow a variety of query forms that come from different levels and aspects of language cognition. The retrieval module converts the expression element combination into its corresponding query condition and selects a set of sentences from the data source that match the query condition. The ranking module evaluates the set of selected sentences in relevance to the user’s information need and recommends referential sentences to users in a ranked order. 
Question answering systems provide an elegant way for people to access an underlying knowledge base. However, people are interested in not only factual questions, but also opinions. This paper deals with question analysis and answer passage retrieval in opinion QA systems. For question analysis, six opinion question types are defined. A two-layered framework utilizing two question type classifiers is proposed. Algorithms for these two classifiers are described. The performance achieves 87.8% in general question classification and 92.5% in opinion question classification. The question focus is detected to form a query for the information retrieval system and the question polarity is detected to retain relevant sentences which have the same polarity as the question. For answer passage retrieval, three components are introduced. Relevant sentences retrieved are further identified as to whether the focus (Focus Detection) is in a scope of opinion (Opinion Scope Identification) or not, and, if yes, whether the polarity of the scope and the polarity of the question (Polarity Detection) match with each other. The best model achieves an F-measure of 40.59% by adopting partial match for relevance detection at the level of meaningful unit. With relevance issues removed, the F-measure of the best model boosts up to 84.96%. Keywords: Opinion Extraction, Question Answering, Question Type, Answer Passage Retrieval 1. Introduction Most of the state-of-the-art Question Answering (QA) systems serve the needs of answering factual questions such as “When was James Dean born?” and “Who won the Nobel Peace Prize in 1991?” However, in addition to facts, people would also like to know about others’ opinions, thoughts, and feelings toward some specific topics, groups, and events. Opinion ＊Department of Computer Science and Information Engineering, National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan TEL: +886-2-33664888*311 FAX: +886-2-23628167 E-mail: {lwku, eagan}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw [Received November 16, 2007; Revised July 1, 2008; Accepted August 8, 2008]  308  Lun-Wei Ku et al.  questions reveal answers about people’s opinions (e.g., “What do Americans think of the US-Iraq war?” and “What is the public opinion on human cloning?”) which tend to scatter across different documents. Traditional QA approaches for factual questions are not effective enough to retrieve answers for opinion questions [Stoyanov et al. 2005], so an opinion QA system is essential. Most of the research on QA systems has been developed for factual questions, and the association of subjective information with question answering has not yet been studied much. As for subjective information, Wiebe [2000] proposed a method to identify strong clues of subjectivity of adjectives. Riloff et al. [2003] presented a subjectivity classifier using lists of subjective nouns learned by bootstrapping algorithms. Riloff and Wiebe [2003] proposed a bootstrapping process to learn linguistically rich extraction patterns for subjective expressions. Kim and Hovy [2004] presented a system to determine word sentiments and combined sentiments within a sentence. Pang, Lee, and Vaithyanathan [2002] classified documents not by topic, but by the overall sentiment, and then determined the polarity of a review. Wiebe et al. [2002] proposed a method for opinion summarization. Wilson et al. [2005] presented a phrase-level sentiment analysis to automatically identify the contextual polarity. Ku et al. [2006] proposed a method to automatically mine and organize opinions from heterogeneous information sources. Some research has gone from opinion analysis in texts toward that in QA systems. Cardie et al. [2003] took advantage of opinion summarization to support Multi-Perspective Question Answering (MPQA) system which aims to extract opinion-oriented information of a question. Yu and Hatzivassiloglou [2003] separated opinions from facts at both document and sentence levels. They intended to cluster opinion sentences from the same perspective together and summarize them as answers to opinion questions. Kim and Hovy [2005] identified opinion holders, which are frequently asked in opinion questions. This paper deals with two major problems in opinion QA systems, question analysis and answer passage retrieval. Several issues, including how to separate opinion questions from factual ones, how to define question types for opinion questions, how to correctly classify opinion questions into corresponding types, how to present answers for different types of opinion questions, and how to retrieve answer passages for opinion questions are discussed. In this paper, the unit of a passage is a sentence, though a passage can sometimes refer to a set of sentences, such as a paragraph.  Question Analysis and Answer Passage Retrieval for  309  Opinion Question Answering Systems  2. An Opinion QA Framework Figure 1 is a framework of the opinion QA system. The question (Question Q) is initially submitted into a part of speech tagger (POS Tagger), then the question is analyzed in three aspects by two-layered classification (Two-Layered Classification), including the question focus (Q Focus), the question polarity (Q Polarity), and the opinion question type (Opinion Q Type). The question focus defines the main concept of the question, while the question polarity refers to the positive, neutral, or negative tendency of the opinionated question. The former two attributes are further applied in answer passage retrieval (Answer Passage Retrieval). The question focus is the query for an information retrieval (IR) system to retrieve relevant sentences. The question polarity, which is the opinion polarity of the question, is utilized to screen out relevant sentences with different polarities to the question. For example, the polarity of the question “Who would like to use a Civil ID card?” should be positive, and non-supportive evidence should not be extracted for further processing. With answer passages retrieved, answer extraction extracts text spans as answers according to the opinion question types, and outputs answers to users. This paper focuses on the retrieved answer passages and the opinion type of the question for answer extraction. Answer extraction is not included in our discussion and is left as a future work.  Figure 1. An Opinion QA System Framework.  310  Lun-Wei Ku et al.  3. Experimental Corpus Preparation  The experimental corpus comes from four sources, TREC1, NTCIR2, the Internet Polls, and OPQ. TREC and NTCIR are two of the three major information retrieval evaluation forums in the world. Their evaluation tracks are in natural language processing and information retrieval domains such as large-scale information retrieval, question answering, genomics, cross language processing, and so on. We collected 500 factual questions from the main task of QA Track in TREC-11. Since the documents for answer extraction are in Chinese, the English questions were translated into Chinese manually for the experiments. A total of 1,577 factual questions are obtained from the developing question set of the CLQA task in NTCIR-5. Questions from public opinion polls in three public media websites – say, China Times, Era, and TVBS, are crawled. OPQ is developed for this research, and it contains both factual and opinion questions. To construct the question corpus OPQ, annotators are given titles and descriptions of six opinion topics selected from NTCIR-2 and NTCIR-3. Annotators freely ask any three factual questions and seven opinion questions for each topic. Duplicated questions are dropped and a total of 1,011 questions are collected. Within these 1,011 questions in OPQ, 304 are factual questions and the other 707 are opinion questions.  In total, we collected 2,443 factual questions and 1,289 opinion questions from four different sources. These 3,732 questions, shown in Table 1, are used for our experiments.  Table 1. Statistics of Experimental Questions.  Q type Corpus TREC  Factual 500  Opinion 0  Total 500  NTCIR  1,577  0 1,577  Polls OPQ  62  582  644  304  707 1,011  Total  2,443 1,289 3,732  There are some challenging issues in extracting answers automatically by opinion QA systems. Opinionated questions are generally related to holders, targets, and opinions. Holders are the named entities who express opinions, while targets are the objects these opinions are related to. Opinions are comments that holders express toward targets. We categorize the challenges in question analysis into three parts: holders, opinions and concepts.  
In this paper, an HNM based scheme is developed to synthesize Mandarin syllable signals. With this scheme, a Mandarin syllable can be recorded just once, and diverse prosodic characteristics can be synthesized for it without suffering significant signal-quality degradation. In our scheme, a synthetic syllable’s duration is subdivided to its comprising phonemes and a piece-wise linear mapping function is constructed. With this mapping function, a control point on a synthetic syllable can be mapped to locate its corresponding analysis frames. Then, the analysis frames’ HNM parameters are interpolated to obtain the HNM parameters for the control point. Furthermore, for pitch-height adjusting, another timbre-preserving interpolation is performed on the HNM parameters of a control point. Thereafter, signal samples are synthesized according to the HNM synthesis equations rewritten here. This HNM based scheme has been programmed to synthesize Mandarin speech. According to the perception tests, our HNM based scheme is found to be apparently better than a PSOLA based scheme in signal clarity, i.e. much clearer and no reverberation. Keywords: Speech Synthesis, Harmonic-plus-noise Model, Voice Timbre, Pitch Contour. 1. Introduction Since the introduction of PSOLA (pitch synchronous overlap and add) [Moulines et al. 1900], it has been widely used to synthesize speech signal. However, the signal quality of the synthetic speech by PSOLA is not stable. The quality will be degraded a lot if the pitch-contours or durations of the recorded syllables are considerably changed [Dutoit 1997]. Here, signal quality actually means signal clarity, i.e. a signal that is less reverberant and less noisy is better in quality. It may be argued that the prosodic characteristics of a syllable need ＊ Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, 43 Keelung Rd., Sec. 4, Taipei, Taiwan E-mail: {guhy, M9315058}@mail.ntust.edu.tw [Received December 28, 2007; Revised August 25, 2008; Accepted August 27, 2008]  328  Hung-Yan Gu, and Yan-Zuo Zhou  only be slightly changed in a corpus-based approach [Chou 1999; Chang 2005]. This argument will hold only if a sufficiently large quantity of speech data is recorded and used. Otherwise, pitch contours between some adjacent syllables may not be smoothly connected and the speaking rate may not be kept constant within a synthetic sentence. Then, pitch-contours and durations will still need to be changed considerably. In addition, the potential for economically transferring a speech synthesis scheme from Mandarin to another language (e.g., Min-nan or Hakka) is an important consideration factor for us. Therefore, we tend not to adopt an expensive approach, such as corpus-based re-sequencing. Mandarin is a tonal language, and the distinction of the five tones of Mandarin mainly relies on the height and shape of a syllable’s pitch-contour. When a signal-model based approach is adopted, the pitch-contour and duration of a syllable inevitably needs considerable change. Thus, the synthesis method, PSOLA, will not be adequate for use, and another suitable technique should be found or developed. Recently, we have found that HNM (harmonic-plus-noise model) is a good base because it can be improved to synthesize Mandarin syllable signals with much higher signal quality. HNM was proposed by Y. Stylianou to model speech signals to retain high signal quality after such processing as coding and synthesis [Stylianou 1996; Stylianou 2005]. It may be viewed as improving the sinusoidal model [Quatieri 2002] to better model the noise signal components in the higher frequency band of speech signal. In HNM, an MVF (maximum voiced frequency) detection method is provided to divide a speech frame’s spectrum into lower and higher frequency parts. The lower-frequency part is modeled as a sum of harmonic partials as in sinusoidal model. In contrast, the higher-frequency part is modeled with a smoothed spectrum envelope that is represented with some cepstrum coefficients. When applying HNM to synthesize Mandarin syllables, we find some issues that are not clearly explained or solved in the literature on HNM. The first issue (not clearly explained) is how to keep the timbre of synthetic syllables consistent, i.e. the timbre consistent issue. Note that we intend to record each of the 408 different Mandarin syllables just once then modify the height and shape of a recorded syllable’s pitch-contour to that of a different tone’s. When the pitch-contour of a syllable to be synthesized is given, the parameter values of the harmonic partials should be adjusted in a way that the timbre can be kept consistent. The second issue is how to determine the HNM parameter values for a control point [Dodge 1997; Moore 1990] placed at the synthetic time axis (of a synthetic syllable), i.e. the parameter determination issue. In speech synthesis, one must adjust a recorded syllable’s duration to meet the duration requirement given by the prosodic parameter generation unit. When a control point at the synthetic time axis is mapped to a time point between two analysis frames of a recorded syllable, some method of interpolation is needed to determine the HNM parameter values for the control point. In addition, the third issue is how to warp the time axis of a synthetic  An HNM Based Scheme for Synthesizing Mandarin Syllable Signal  329  syllable in order that more fluent syllables and sentences can be synthesized, i.e. the time warping issue. This issue is more relevant to speech synthesis than HNM. When a syllable’s duration needs to be lengthened or shortened, a simple time warping method, i.e. linear warping, will usually result in lower perceived fluency. In this paper, the three issues mentioned above are investigated, and equations for signal synthesis with HNM are rewritten in a clearer notation. In addition, a system based on the extensions and rewritten equations for HNM signal synthesis is developed to synthesize Mandarin syllable signal. The main processing flow of the system is drawn in Figure 1. When a syllable’s signal is to be synthesized, its prosodic parameters’ values are readily determined by the prosody unit. Hence, in the first block of Figure 1, a synthetic syllable’s time length can be planned and subdivided to its comprising phonemes. For example, the syllable /man/ has three phonemes, /m/, /a/, and /n/. Then, a piece-wise linear time mapping function is constructed to map the synthetic phonemes to their corresponding phonemes in the recorded syllables. In the second block of Figure 1, control points are uniformly placed on the synthetic time axis. Then, HNM parameters’ values for each control point are determined. In the following blocks, three types of signals are classified and synthesized separately. Here, the signal of a short unvoiced syllable-initial is directly copied from the recorded to the synthesized. The signal of a long unvoiced syllable-initial is synthesized as noise signal components in HNM while the signals of voiced initial and syllable-final are synthesized as the sum of both the harmonic and noise signal components.  S yllab le S ig n al S yn th es is D eterm in e ph o n em es ’ len g ths an d co n stru ct tim e- wa rp ing f u n ctio n  D eter m in e H N M p ar am eters f or each control point  Sy n th esize th e lo n g  un v o iced in itial as HN M noise signal  Y  T he in itial is short unvoiced ? N T h e in itial is lo n g u nv o iced ? N  Dir ectly co p y sig n al  Y  samples of the un v o iced in itial  Vo iced p art: Sy nth esize HN M h ar m o n ic sig n al Sy nth esize HN M n o ise s ig n al  stop  Figure 1. Main processing flow of the HNM based syllable-signal synthesis scheme.  330  Hung-Yan Gu, and Yan-Zuo Zhou  2. Phoneme Duration Planning and Time Axis Mapping The issues of duration planning and time-axis mapping are not mentioned in the literature on HNM [Stylianou 1996; Stylianou 2005]. Mandarin syllables have the structure, CxVCn. The component, Cx, may be null, a voiced consonant, or an unvoiced consonant while the component, Cn, may be null or a nasal /n/ or /ng/. Also, the component, V, may be a vowel, diphthong, or triphthong. When Cx is an unvoiced consonant, we classify it as a short-unvoiced (e.g. /b/, not aspirated) or long-unvoiced (e.g. /p/, aspirated). For a short-unvoiced, its signal will be directly copied from the initial part of the recorded syllable to the initial part of the synthetic syllable. This processing is indicated in the block at the right side of Figure 1. However, for a long-unvoiced, its signal will be synthesized as the sum of noise signal components with HNM. This processing is indicated in the block at the left side of Figure 1. In addition, Cx is a voiced consonant or null, and it will be synthesized together with the syllable final, VCn, as the sum of both the harmonic and noise signal components. When a syllable is started with a short-unvoiced consonant, e.g. /bau/, the time length of the consonant is planned as the corresponding consonant’s length in the recorded syllable. In contrast, when started with a long-unvoiced consonant, the length of the consonant is planned by multiplying its original length with a factor, Fu. The value of Fu is first computed as the synthetic syllable’s length divided by its corresponding recorded syllable’s length. However, the value, Fu, is restricted to the range from 0.6 to 1.4, i.e. set to 1.4 when larger than 1.4 and set to 0.6 when smaller than 0.6. After the length of the unvoiced part, Du, is determined, the length of the voiced part, Dv, is apparently the synthetic syllable’s length minus Du. To plan the lengths of the phonemes within the voiced part, consider the example syllable, /man/. Suppose that in the recorded signal of /man/, the three phonemes, /m/, /a/, and /n/, occupy Rm, Ra, and Rn seconds, respectively, and Rv = Rm + Ra + Rn. Also, suppose that Dm, Da, and Dn represent the time lengths of the three phonemes within the synthetic syllable, and Dv = Dm + Da + Dn. Note that Dm (or Rm) is used here to denote the time length of the initial voiced consonant of a syllable, Da (or Ra) denotes the time length of the vowel nucleus, and Dn (or Rn) denotes the time length of the final nasal consonant. In this study, the values of Dm, Da, and Dn are planned according to an observation. That is, the consonant-to-vowel duration ratio, (Rm + Rn) / Rv, will become smaller when the syllable is uttered within a sentence instead of uttered in isolation. The planning procedure is as below.  An HNM Based Scheme for Synthesizing Mandarin Syllable Signal  331  r = 0.85; while ( r >= 0.1 ) { Dm = (Rm/Rv) * r * Dv; Dn = (Rn/Rv) * r * Dv; Da = Dv – Dm – Dn; if (Da > Dv*0.5) break; r = r – 0.05; } Db = Dm + Dn; if (Dm > 0 && Dm/Db < 0.35) { Dm = 0.35*Db; if (Dn > 0 && Dn/Db < 0.35) { Dn = 0.35*Db;  Dn=Db-Dm; } Dm=Db-Dn; }  In this procedure, the value of Dm is planned by multiplying a duration reduction rate, r, with the time ratio (Rm / Rv) of its counterpart, Rm, in the recorded syllable. In the same way, the value of Dn is planned. By trying to decrease the value of r iteratively, the values of Dm and Dn are decreased gradually, and the value of Da finally becomes sufficiently large. As to the initial value of r, i.e. 0.85, and the vowel duration threshold, i.e. 0.5, they are set according to analyzing some real spoken sentences. If the structure of a syllable is same as /san/ or /an/, i.e. without voiced initial consonant, then the values of Rm and Dm can be set to zero directly. Similarly, if the structure of a syllable is the same /ma/, i.e. without an ending nasal, then the values of Rn and Dn can be set to zero directly. After the values of Dm, Da, and Dn are determined, a mapping function from the phonemes in the synthetic syllable to their corresponding phonemes in the recorded syllable can be established and used in the second block of Figure 1. The mapping function adopted here is as depicted in Figure 2. That is, it is a piece-wise linear function. Although a simple mapping function is adopted here, we think the fluency level of the synthetic speech can still be improved a lot. In the future, we will study the mapping problem between the source and synthetic syllables with a more systematic method. 
This paper considers minimum phone error (MPE) based discriminative training of acoustic models for Mandarin broadcast news recognition. We present a new phone accuracy function based on the frame-level accuracy of hypothesized phone arcs instead of using the raw phone accuracy function of MPE training. Moreover, a novel data selection approach based on the frame-level normalized entropy of Gaussian posterior probabilities obtained from the word lattice of the training utterance is explored. It has the merit of making the training algorithm focus much more on the training statistics of those frame samples that center nearly around the decision boundary for better discrimination. The underlying characteristics of the presented approaches are extensively investigated, and their performance is verified by comparison with the standard MPE training approach as well as the other related work. Experiments conducted on broadcast news collected in Taiwan demonstrate that the integration of the frame-level phone accuracy calculation and data selection yields slight but consistent improvements over the baseline system. Keywords: Discriminative Training, Minimum Phone Error, Phone Accuracy Function, Training Data Selection, Large Vocabulary Continuous Speech Recognition 1. Introduction Speech is the primary and the most convenient means of communication between individuals. Due to the successful development of much smaller electronic devices and the popularity of wireless communication and networking, it is widely believed that speech will possibly serve as a major human-machine interface for the interaction between people and different kinds of smart devices in the near future. On the other hand, huge quantities of multimedia information, ＊Department of Computer Science & Information Engineering, National Taiwan Normal University E-mail: {g93470185, g94470144, g96470198, berlin}@csie.ntnu.edu.tw [Received January 6, 2008; Revised August 21, 2008; Accepted August 27, 2008]  344  Shih-Hung Liu et al.  such as that in broadcast radio and television programs, voice mails, digital archives, and so on are continuously growing and filling our computers, networks, and daily lives. Speech is obviously one of the most important information-bearing sources for the great volumes of multimedia. Based on these observations, it is expected that automatic speech recognition (ASR) technology will play a very important role in human-machine interaction, as well as in organization and retrieval of multimedia content. When considering the development of an ASR system, acoustic modeling is always an indispensable and crucial ingredient we have to carefully manipulate. The purpose of acoustic modeling is to provide a method for calculating the likelihood of a speech utterance occurring given a word sequence. In principle, the word sequence can be decomposed into a sequence of phone-like (subword, e.g. INITIAL or FINAL in Mandarin Chinese) units or acoustic models, each of which is normally represented by a continuous density hidden Markov model (HMM), and the corresponding model parameters can be estimated from a corpus of orthographically transcribed training utterances using maximum likelihood (ML) training [Rabiner 1989]. The acoustic models can be alternatively trained with discriminative training algorithms, such as maximum mutual information (MMI) training [Bahl et al. 1986] and minimum phone error (MPE) training [Povey 2004; Kuo et al. 2006]. These algorithms were developed in an attempt to correctly discriminate the recognition hypotheses for the best recognition results rather than just to fit the model distributions as done by ML training; therefore, they have continuously been a focus of considerable active research in a wide variety of large vocabulary continuous speech recognition (LVCSR) tasks over the past few years. Moreover, in contrast to ML training, discriminative training considers not only the reference (or correct) transcript of a training utterance, but also the competing (or incorrect) hypotheses that are often obtained by performing LVCSR on the utterance. In this paper, we consider minimum phone error (MPE) based discriminative training of acoustic models for Mandarin broadcast news recognition. In order to remedy the defect in the phone accuracy function of the MPE training algorithm, we present a new phone accuracy function based on the frame-level accuracy of hypothesized phone arcs. Moreover, a novel data selection approach based on the frame-level normalized entropy of Gaussian posterior probabilities obtained from the word lattice of the training utterance is explored, which has the merit of making the MPE training algorithm focus much more on the training statistics of those frame samples that center nearly around the decision boundary for better discrimination. The underlying characteristics of the presented approaches are extensively investigated and their performance is verified by comparison with the original MPE training approach as well as other related work. The remainder of this paper is organized as follows. In Section 2, the general background of MPE based acoustic model training is briefly reviewed. Section 3 elucidates our proposed  Improved Minimum Phone Error based Discriminative Training of  345  Acoustic Models for Mandarin Large Vocabulary Continuous Speech Recognition  new accuracy function for MPE training, and Section 4 presents two novel training data selection approaches based on frame-level normalized entropy information. The experimental setup is detailed in Section 5, and a series of speech recognition experiments is described in Section 6. Finally, we present the conclusions drawn from the research in Section 7.  2. Review of Minimum Phone Error (MPE) Training  Given a training set of K acoustic vector sequences O = {O1,..,Ok ,..,OK } , the MPE criterion for acoustic model training aims to minimize the expected phone errors of these acoustic  vector sequences using the following objective function [Povey and Woodland 2002]:  FMPE  (λ )  =  ∑  K k =1  ∑Wk  ∈Wklat  RawAcc(Wk  )Pλ (Wk  | Ok  ),  (1)  where λ denotes a set of phone-like acoustic models; Wklat is the corresponding word lattice  [Ortmanns et al. 1997] of Ok obtained using LVCSR, as graphically illustrated in Figure 1; Wk is one of the hypothesized word sequences in Wklat ; P(Wk | Ok ) is the posterior  probability of hypothesis Wk given Ok ; RawAcc(Wk ) is the “raw phone accuracy” of  Wk in comparison to the corresponding reference transcript, which is typically computed as  the sum of the phone accuracy measures of all phone hypotheses in Wk . Then, the objective  function in Equation (1) can be maximized by applying the Extended Baum-Welch algorithm  [Gopalakrishnan et al. 1989] to update the mean  μhmd and  variance  σ  2 hmd  for each  dimension d of a Gaussian mixture component m of a multi-state (or single-state)HMM  h using the following equations:  μhmd  =  θ  num hmd  (O )  −  θ  den hmd  (O )  +  D μhmd  γ  num qm  −  γ  den qm  +  D  ,  (2)  σ  2 hmd  =  θ  num hmd  (O 2  )  −  θ  den hmd  (O 2  )  +  D (σ  2 hmd  γ  num hm  −  γ  den hm  +  D  +  μ  2 hmd  )  −  μ h2m d  ,  (3)  γ  num hm  =  K ∑  ∑  eq ∑  γ  k qm  (t )  m ax(0, γ  k q  MPE  ),  (4)  k =1  q∈  W  la k  t  ,  q  =  h  t = sq  γ  den hm  =  K ∑  ∑  eq ∑  γ  k qm  (t )  max(0,  −γ  k q  MPE  ),  (5)  k =1  q  ∈  W  la k  t  ,  q  =  h  t = sq  θhnmumd (O ) =  K ∑  ∑  eq ∑  γ  k qm  (t  )  max(0,  γ  k q  MPE  )ot  (d ),  (6)  k =1 q∈Wklat ,q=h t =sq  ( ) θ  num hmd  O2  K =∑  ∑  eq ∑  γ  k qm  (t )  max(0,γ  k MPE q  )ot  (d  )2  ,  (7)  k =1  q∈  W  lat k  ,  q  =  h  t =sq  γ k MPE q  =  γ  k q  ( c qk  −  c  k a  vg  ),  (8)  346  Shih-Hung Liu et al.  疫情 (epidemic situation)  影響 (influence)  肺炎 (pneumonic)  t3 疫情  t6 隱藏  隱藏 t9 (hide)  生命  (life)  SIL (silence)  0  t1  飛蛾 (flying moth)  一群 t5 (a flock of)  t3 疫情  疫情  影響  生活  t7  (living) t12  SIL  t8 生活  SIL t11  T  肺炎  t4  生命  t2  一群  影響  t6  t10  Figure 1. An illustration of a word lattice, in which each arc, together with its corresponding start and end speech frames, represents a candidate word hypothesis. A word arc can be further aligned into a sequence of phone arcs for MPE training.  where q ∈ Wklat , q = h denotes that a phone q arc belongs to the word lattice Wklat and  physically refers to the HMM  h;  c k avg  is the average phone accuracy over all hypothesized  word sequences in the word lattice; cqk is the expected phone accuracy over all hypothesized  word sequences containing a phone arc q ; ot (d ) is the observation vector component at  frame  t;  sq and  eq are the start and end times of phone arc  q;  γ  k q  the posterior  probability for phone arc  q  of utterance  k;  γ  k qm  (t  )  is the posterior probability for mixture  component  m  of phone arc  q  of utterance  k  at frame  t;  ( ) γ  num qm  ,  θ  num qmd  O  and  ( ) θ  num qmd  O  2  are the accumulated training statistics for mixture component m of phone arc q  ( ) whose  c qk  is larger than  cakvg , and vice-versa for  γ  den qm  ,  θ  den qmd  (O  )  and  θ  den qmd  O2  ;  μqmd  and  σ  2 qmd  are, respectively, the mean and variance estimated in the previous iteration; and  D is a constant used to ensure positive variance values. On the other hand, the calculation of  cakvg and cqk is actually based on the phone accuracies of phone arcs in the word lattice. For  example, the raw phone accuracy for each word sequence Wk in the lattice can be calculated  in terms of the sum of the accuracy of each phone contained in Wk [Povey and Woodland  2002]:  R a w A cc (W k ) = ∑ q∈W k P h o n eA cc ( q ),  (9)  where PhoneAcc(q) is the raw phone accuracy for a phone arc q in Wk , which can be defined as follows:  Improved Minimum Phone Error based Discriminative Training of  347  Acoustic Models for Mandarin Large Vocabulary Continuous Speech Recognition  PhoneAcc(q)  =  max z j∈Zk  ⎧⎪−1+ 2e(z j , q) / l(z j ),  ⎨ ⎪⎩  −1 +  e(z  j  ,  q)  /  l(z  j  ),  zj zj  = ≠  qq ⎫⎪⎬⎪⎭ ,  (10)  where Zk is the set of phone labels in the corresponding reference transcript, and e(z j , q) is the overlap length in frames (or in time) for a phone label z j in Zk and a hypothesized phone arc q in Wk , l(zj ) is the length in frames for z j . We can observe from Equations (4)-(8), for MPE training, those hypotheses having raw phone accuracies higher than the average can provide positive contributions, and vice-versa for those hypotheses with accuracies lower than the average. Interested readers can refer to [Povey 2004; Kuo et al. 2006] for more derivation details of MPE training.  3. New Accuracy Functions  It is known that the standard MPE training approach has some drawbacks [Zheng and Stolcke  2005]. One of them is that MPE training does not sufficiently penalize deletion errors. In  general, the original MPE objective function discourages insertion errors more than deletion  and substitution errors. Inspired by the work of word lattice rescoring (or decoding) using  frame-level accuracy information [Wessel et al. 2001], in this paper we present an alternative  phone accuracy function that can look into the frame-level phone accuracies of all  hypothesized word sequences to replace the original raw phone accuracy function for MPE  training [Liu et al. 2007a]. The frame-level phone accuracy function (FA) is defined as:  FrameAcc(q)  =  ∑  eq t=  sq  δ  (q,  Z  k  (t )) ,  (11)  eq − sq + 1  and  δ  (q,  Zk  (t ))  =  ⎧⎪1  ⎨ ⎩⎪  −  ρ  , if , if  q q  = ≠  Zk Zk  (t ) (t ),  0  <  ρ  <  ⎫⎪ ⎬ 1⎭⎪  ,  (12)  where Zk (t) is the phone label of the reference transcript Zk at frame t ; ρ is a tunable positive parameter used to control the penalty if the phone arc q is incorrect in its label; and the value of FrameAcc(q) will range from −ρ to 1. For each frame t , we thus can easily evaluate whether the phone arc of each hypothesized word sequence in the word lattice is identical to that of the reference transcript or not. Actually, the presented frame-level phone accuracy function emphasizes the deletion penalty on the incompletely correct phone arc; whereas the insertion and substitution errors of the hypothesized word sequences, as well as  the errors caused by inaccurate time boundaries of the phone arcs, are also taken into consideration evenly. As illustrated in Figure 2, given the reference phone transcript “a-b-c”, the first hypothesized phone sequence “a-b-c” will be regarded as partially correct (with a score of two) using the original MPE raw phone accuracy function, as shown in Eq. (10);  348  Shih-Hung Liu et al.  Reference Hypothesis 1 Hypothesis 2  a  b  c  a  b  c  a  c  0  5 10 15 20 25 30  Raw phone accuracy of Hypothesis 1 = 2  Raw phone accuracy of Hypothesis 2 = 2  Frame-level phone accuracy of Hypothesis 1 = 2.56  Frame-level phone accuracy of Hypothesis 2 = 1.27  Figure 2. An illustration of the frame-level accuracy. The shaded box indicates where the frame-level errors occur.  while the presented frame-level phone accuracy function, as shown in Eq. (11), will give it a score of 2.56 (with ρ set to 0.1) by similarly taking into account the incorrect time boundaries of the associated phone arcs. On the other hand, for the second hypothesized phone sequence “a-c”, it is obvious that there exists a deletion error of the phone arc “b.” Nevertheless, the original MPE raw phone accuracy function gives the second hypothesized phone sequence a score of two, which is equivalent to that of the first hypothesized phone sequence, and the phone arcs (“a” and “c”) of it will be treated as completely correct. While using our proposed frame-level phone accuracy function, both of the two phone arcs in the second hypothesized phone sequence will instead be treated as partially correct by considering the frame-level substitution errors. Thus, the frame-level phone accuracy function will only assign a total score of 1.27 (with ρ set to 0.1) to the second hypothesized phone sequence.  Another frame-level phone accuracy function that uses the Sigmoid function to normalize the phone accuracy value in a range between -1 and 1 is also investigated in this paper (SFA):  SigFrameAcc(q) =  2  − 1,  (13)  
Due to abundant resources not always being available for resource-limited languages, training an acoustic model with unbalanced training data for multilingual speech recognition is an interesting research issue. In this paper, we propose a three-step data-driven phone clustering method to train a multilingual acoustic model. The first step is to obtain a clustering rule of context independent phone models driven from a well-trained acoustic model using a similarity measurement. For the second step, we further clustered the sub-phone units using hierarchical agglomerative clustering with delta Bayesian information criteria according to the clustering rules. Then, we chose a parametric modeling technique -- model complexity selection -- to adjust the number of Gaussian components in a Gaussian mixture for optimizing the acoustic model between the new phoneme set and the available training data. We used an unbalanced trilingual corpus where the percentages of the amounts of the training sets for Mandarin, Taiwanese, and Hakka are about 60%, 30%, and 10%, respectively. The experimental results show that the proposed sub-phone clustering approach reduced relative syllable error rate by 4.5% over the best result of the decision tree based approach and 13.5% over the best result of the knowledge-based approach. Keywords: Cross-lingual Phone Set Optimization, Speech Recognition, Delta-BIC ＊ Department of Electrical Engineering, Chang Gung University, Taiwan E-mail: d9221003@stmail.cgu.edu.tw ＋ Institute of Information Science, Academia Sinica, Taiwan E-mail: chunnan@iis.sinica.edu.tw ＃ Institute of Statistics, National Tsing Hua University, Taiwan E-mail: jjchiang1@hotmail.com § Computer Science and Information Engineering, Chang Gung University, Taiwan E-mail: renyuan.lyu@gmail.com [Received January 3, 2008; Revised July 7, 2008; Accepted August 8, 2008]  364  Dau-Cheng Lyu et al.  1. Introduction Multilingual speech recognition, integrating several language-specific recognizers into one recognizer, is one of the popular research topics in the speech recognition field [Schultz et al. 2006; Uebler 2001; Kohler 2001; Kumar et al. 2005; Lyu et al. 2002]. Many multilingual speech recognizers depend on a large-scale speech database for each language in order to obtain high performance. However, such a large-scale speech corpus is not always available for all of the languages to be used. For example, in Taiwanese, which is one of the most two important languages in Taiwan, the size of the available corpus of Taiwanese is not comparable to that of other majority languages, such as English, Mandarin, and Spanish. In fact, researchers who want to build a multilingual speech recognizer by collecting a well-designed, large-scale speech corpus for every language find that it is not feasible for all languages. Automatic speech recognition (ASR) systems can be prohibitively expensive to develop because of time consumption and expense as the process involves a huge amount of data collection and calls for the complete building of an acoustic model for the minor language. In this paper, we try to find an approach which uses the majority languages, defined as those languages with large-scale training data available, to help the minor languages and build a reliable multilingual acoustic model. Several approaches are proposed for the multilingual ASR system [Schultz et al. 2006; Uebler 2001; Kohler et al. 2001; Wu et al. 2006; Liu et al. 2005; Mak et al. 1996]. One approach is to map a language-dependent phone set to a global inventory of the multilingual phonetic phone set based on phonetic knowledge to construct the universal phone inventory. Under this approach, the same phonetic representation within different languages shares the same training data. However, the approach based on universal phone inventory only uses the phonetic knowledge, and it does not consider the spectral properties of the variations of the sounds with different speakers. Another approach is to merge the language-dependent phones using a data-driven approach to map the phones in different languages by specific distance measure between well-trained acoustic models. The advantage of this approach is that the distance is estimated from the statistical measure of similarity of speech sounds among the languages. Nevertheless, optimizing the final number of the universal phone set is an important issue in such a data-driven approach. In this paper, we use a constrained data-driven approach to select the optimal phoneme set which directly reflects the characteristics of the unbalanced training trilingual speech data. The training corpora include the three most popular languages in Taiwan -- Mandarin, Taiwanese, and Hakka -- where the amount of Mandarin data is two times as large as that of Taiwanese and five times as large as that of Hakka. The training procedures for the trilingual acoustic model can be divided into the following steps. First, we use the Bhattacharyya distance [Mak et al. 1996] to measure the phonemes distance among the languages according  Acoustic Model Optimization for Multilingual Speech Recognition  365  to well-trained acoustic models. Then, we apply a 2-step clustering using the hierarchical agglomerative clustering (HAC) [Fowlkes et al. 1983] with delta Bayesian information criteria (△BIC) [Tristschler et al. 1999] to automatically select the optimal context dependent phoneme (CDP) set. The first clustering step is to generate context-independent phoneme (CIP) clustering rules. In the second clustering step, we use the rules as phonetic knowledge to constrain the CDP clustering. After the clustering procedures, we generate the new phoneme set where the sounds with the same label will share the training data. Finally, to optimize the acoustic model of the new phoneme set, we use a model complexity selection (MCS) [Anguera et al. 2007] to adjust the number of Gaussian components in the Gaussian mixture to balance between the new phoneme set and the unbalanced training data. This paper is organized as follows. In Section 2, we introduce the three main languages in Taiwan -- Mandarin, Taiwanese, and Hakka -- and also discuss their linguistic and phonetic characteristics. In Section 3, we use a knowledge-based approach to build two baseline systems, a language-dependent and a language-independent recognizer, on an unbalanced speech corpus. In Section 4, we describe our proposed approach to automatically optimize a multilingual acoustic model, including three components: CIP clustering, CDP clustering, and MCS. Then, several experiments are conducted for comparison with the baseline system and the decision tree based approach. Finally, we summarize our key contributions of this paper. 2. Linguistic and Phonetic Properties of Mandarin, Taiwanese and Hakka The goal of this paper is to use a majority language, like Mandarin, to assist minor languages, such as Taiwanese and Hakka, to train a unified acoustic model for a trilingual ASR system. In order to efficiently combine the acoustic units among the languages, some basic knowledge, such as the phonetics, phonology, and other linguistic aspects, is essential. In this section, we will introduce the linguistic and phonetic characteristics of the three languages. 2.1 Linguistic Properties Mandarin, Taiwanese, and Hakka are the three main languages in Taiwan. Mandarin is the most important language in ethnically Chinese societies, because of its huge population and the potentially huge market. Another language, Taiwanese, is the mother tongue of more than 75% of the population in Taiwan. In addition, 10% of the local population uses Hakka as their first language. In fact, many people can use any two of these three languages. Due to this distribution, Taiwan is a bilingual, or even a trilingual society. On the other hand, the three languages, also used by many overseas Chinese living in Singapore, Malaysia, Philippine, and other areas of Southeast Asia, are the members of the Chinese language family. Some people prefer to call them "dialects" but, unlike dialects in other languages, which are usually mutually intelligible to each other, the "dialects" in spoken Chinese are almost completely  366  Dau-Cheng Lyu et al.  mutually unintelligible. Due to their mutual unintelligibility, we prefer to call Mandarin, Taiwanese, and Hakka three languages.  2.2 Phonetic Properties  To express the three languages in phonetic properties, we applied a heuristic mapping approach based on the Formosa Phonetic Alphabet (ForPA), an easy-to-use transcribing system for the trilingual speech data of these three languages [Lyu et al. 2004]. Basically, the Mandarin Phonetic Alphabet (MPA, also called zhu-in-fu-hao) and Pinyin (han-yu-pin-yin) are the most widely known phonetic symbol sets to transcribe Mandarin Chinese, but both of these systems are designed for Mandarin. ForPA is designed for transcribing Mandarin, Taiwanese, and Hakka for linguistic computing. Generally speaking, ForPA might be considered as a subset of IPA [Mathews et al. 1975], but it is more suitable for applications to the languages used in Taiwan.  These three languages are monosyllabic languages; therefore, a syllable, the smallest meaningful unit, is the basic pronunciation unit of the Chinese languages. Although the three languages have similar CV (C: Consonant, V: Vowel) structure at a syllabic level, Taiwanese and Hakka have much more abundant variation in allowing syllable-final consonants, including -p, -t, -k, -h. Totally, there are 408 base syllables in Mandarin, 709 base syllables in Taiwanese, and 777 base syllables in Hakka. The total number of the union set of syllables among the languages is 1333, and only 141 syllables are in common.  In the phonemic level, there are 21 consonants and 9 vowels in Mandarin, while there are 18 consonants and 16 vowels in Taiwanese. In Hakka, there are 20 consonants and 25 vowels. Therefore, the total number of phonemes is 30, 34, and 45 for Mandarin, Taiwanese, and Hakka, respectively. Additionally, some of the phonemes in the three languages are labeled with the same symbols by phoneticians, meaning that they are phonetically very close. In our observation, only 26 phonemes are truly in common. The common phonemes can share acoustic data among the languages. We list the statistical information of several different phonetic units in these three languages in Table 1.  Table 1. The statistic information of all Mandarin (M) Taiwanese (T) and Hakka (H) linguistic units in two levels: the numbers of phoneme (Np), the numbers of syllables (NS), where ∩and∪ represent intersection and union of sets, respectively.  M  T  H  M ∪T ∪H M ∩T ∩ H  Np  30  34  45  73  26  NS  408  709  777  1333  141  Ntri  530  1000  1049  1740  214  Acoustic Model Optimization for Multilingual Speech Recognition  367  3. Language-Dependent and Language-Independent System In this section, we describe two ASR systems A) language-dependent ASR and B) language-independent ASR. The first one is solely trained on a single language for each language, and we combine each recognizer into one recognizer as the language-dependent ASR system. The second one uses a universal phoneme inventory to map phonemes among the languages and build the common recognizer in one level. In the following, we first introduce the speech corpora for training the acoustic models, and then we report the performance of the two systems. Of course, some discussions are also included at the end of this section.  3.1 Trilingual Speech Corpus  In order to simulate the available speech data in Taiwan, we use an unbalanced trilingual speech corpus for all of the experiments of this paper. The details of the corpus are listed in Table 2. Three languages are included -- Mandarin Taiwanese, and Hakka -- which are recorded with a 16 kHz, 16-bit, microphone channel. There are 100 speakers, 50 speakers each for Mandarin and Taiwanese, for the training set. Each of the speakers recorded two sets of phonetically balanced utterances, and each of the utterances contained one to four syllables. In the Hakka training set, there were only two speakers; one speaker recorded two monosyllabic sets, and the other speaker recorded 4320 phonetically balanced utterances. In the two monosyllabic sets, we included the two main Hakka accents, Miaoli and Hsinzu, of Taiwan. The former contains 2269 tonal syllables, and the latter contains 2970 tonal syllables. As we mentioned before, the training sets of the corpus are unbalanced, where the total number of hours for Taiwanese is about half of that of Mandarin, and the amount of the speech data in Mandarin is five times as large as that of Hakka. For the test set, another 21 speakers recorded the test speech, with a total length of almost 1 hour.  Table 2. Statistics of training and testing the trilingual speech corpus.  language  number of speakers  number of utterances  speech length (in hours)  Mandarin  100  43,087  10.9  Training Set  Taiwanese  50  22,851  5.2  Hakka  2  9559  2.1  Mandarin  10  1,000  0.29  Test Set  Taiwanese  10  1,000  0.27  Hakka  
Text Summarization is very effective in relevant assessment tasks. The Multiple Document Summarizer presents a novel approach to select sentences from documents according to several heuristic features. Summaries are generated modeling the set of documents as Semantic Vector Space Model (SVSM) and applying Principal Component Analysis (PCA) to extract topic features. Pure Statistical VSM assumes terms to be independent of each other and may result in inconsistent results. Vector space is enhanced semantically by modifying the weight of the word vector governed by Appearance and Disappearance (Action class) words. The knowledge base for Action words is maintained by classifying the words as Appearance or Disappearance with the help of Wordnet. The weights of the action words are modified in accordance with the Object list prepared by the collection of nouns corresponding to the action words. Summary thus generated provides more informative content as semantics of natural language has been taken into consideration. Keywords: Principal Component Analysis (PCA), Semantic Vector Space Model (SVSM), Summarization, Topic Feature, Wordnet 1. Introduction With the advent of the information revolution, electronic documents are becoming a principal media of business and academic information. The Internet is being populated with hundreds of thousands of electronic documents each day. In order to fully utilize these on-line documents effectively, it is crucial to be able to extract the main idea of these documents. Having a Text Summarization system would thus be immensely useful in serving this need. Multiple Document Summarization System aids to provide the summary of a document set that ＊Indian Institute of Information Technology and Management, Gwalior, India- 474010 E-mail: {omvikas, akhil, girrajmeena, amitgupta}@iiitm.ac.in [Received December 30, 2006; Revised July 14, 2007; Accepted July 19, 2008]  142  Om Vikas et al.  contains documents which belong to same topic. It can also be used to generate the summary of a single document. In the present work, we propose a method of text summarization that uses semantics of data in order to form efficient and relevant summary. Summary is generated by constructing Statistical Vector Space Model (3.1) and then modifying it using the concept of Action words to form Semantic Vector Space Model (3.2). Action Words are identified using the Action Word Classifier which makes use of Wordnet [Kedar et al.] in order to analyze the semantics of word. Principal Component Analysis (3.3) is then applied on SVSM to reduce the dimension of multidimensional data sets. Singular Value Decomposition (SVD) is carried out on SVSM as a part of PCA to yield singular values and eigen vectors. Backprojection is then performed to project the documents onto the eigen space yielding projected values of documents which are henceforth compared with the singular values to yield the most relevant document/topic. Sentence Extraction (3.4) from multiple document sets has been assigned weight on the basis of keywords obtained from the most important document/topic. Sentences with higher weight are taken to form a summary. 2. Related Work Various multiple document summarization systems already exist. This document summarizer is based on Kupeic 95 [Kupeic et al. 1995] which is a method of training a Bayesian classifier to recognize sentences that should belong in a summary. The classifier estimates the probability that a sentence belongs in a summary given a vector of features that are computed over the sentence. It identifies a set of features that correspond to the absence/presence of certain words or phrases and avoids the problem of having to analyze sentence structure. Their work focused on analyzing a single document at a time. Since then, there has been lot of work on the related problem of Multiple-document Summarization [Regina et al. 1999; Radev et al. 1998], where a system summarizes multiple documents on the same topic. For example, a system might summarize multiple news accounts of the recent massacre in Nepal; into a single document. Our hypothesis is that the similarities and differences between documents of the same type (e.g. bios of CS professors, earnings releases, etc.) provide information about the features that make a summary informative. The intuition is that the ‘information content’ of a document can be measured by the relationship between the document and a corpus of related documents. To be an informative summary, an abstract has to capture as much of the ‘information content’ as possible. To gain a handle on the problem of capturing the relationship between a document and a corpus, we examined several papers on Multiple-Document Summarization [Regina et al. 1999; Radev et al. 1998, 2000, 2004; Otterbacher et al. 2002]. However, we found most of their approaches were not applicable to  Multiple Document Summarization Using  143  Principal Component Analysis Incorporating Semantic Vector Space Model  our problem since they are mostly trying to match sentences of the same meaning to align multiple documents. The MEAD summarizer [Radev et al. 2000, 2001], which was developed at the University of Michigan and at the Johns Hopkins University 2001 Summer Workshop on Automatic Summarization, produces summaries of one or more source articles (or a ‘cluster’ of topically related articles). Our Summarizer works on the documents belonging to same topic. It is strongly motivated by the analogy between this problem and the problem of face identification, where a system learns features for facial identification by applying PCA to find the characteristic eigenfaces [Turk et al. 1991; Pentland et al. 1994; Moon et al. 2001]. 3. New Methodology Any set of documents dealing with the same subject is decomposed using Vector Space model. The important keywords can be extracted from the Vector Space Model using a threshold. Such keywords are called thematic keywords which are based on statistics. Important sentences can be extracted and a summary can be made using thematic keywords. We propose a new methodology for multiple document summarization by enhancing the VSM using semantics and identifying topic features based keywords to make the multiple document summary. The approach is: 1. Statistical VSM construction from Multiple Document Set. 2. Semantic VSM generation using the concept of Contextual Action Words using Wordnet. 3. Application of PCA on Semantic VSM to reduce the dimension of the multidimensional data set yielding the most important Keywords. 4. Score Sentences based on several features such as sentence length cut-off feature, position feature, keyword weight, etc. 5. Generation of Summary extracting the sentences with high Score. 3.1 Statistical VSM Construction The Multiple Document Summarizer models the set of documents related to the same topic as the Statistical Vector Space Model based on several heuristics. The simplest way to transform a document into a vector is to define each unique word as a feature. The weight of a feature being decided based on the contribution of various parameters such as Cue-phrase Keywords, topic keywords and term frequency in document. The weight of the feature is being termed as Feature Combination. The vector representations of the documents; collectively define an n-dimensional vector space (where each document is an nx1 vector). The m document vectors taken as the columns  144  Om Vikas et al.  of an nxm matrix D, define a linear transformation into the vector space. 3.2 Semantic VSM Construction The existing vector space model is statistical in nature. This vector space is input to a number of tools and processes like a summarizer and information retrieval system. PCA/SVD Technique has been applied earlier for Summarization based on statistical vector space [Gong et al. 2001]. Some times this statistically generated model is unable to define the context. Keywords identified by a statistical model can be non-contextual in nature. Therefore, an effort is to be made in the direction of identification of contextual keywords and modification of existing model so that it can be more helpful and contextual for various applications like text summarization and text retrieval. To identify the contextual keywords, we try to exploit human psychology. In any article, we identify that those words are important which either give a sense of either appearance or disappearance of any object/event. Thus, after we have the pure statistical vector space we need to enhance the vector space semantically by modifying the weights of the word vector by identifying the Appearance and Disappearance (ACTION class) words. To do so, we need to have a knowledgebase (KB) with some seed wordlist which belongs to appearance or disappearance. Following, are the steps involved in the semantic vector space model. 1. Get the tf matrix, T from existing document D. 2. Identify the set of action words, A from the given tf matrix, T, (number of action words =n). 3. Find the associated object list Oi for action word Ai; Ai ε A, 0<i<n. 4. Find contextual objects Co from Object list O1,O2,….,On. 5. Modify weight of contextual objects in T to form semantic vector space S{T}. 3.2.1 Identification of Action words Action words are the backbone of the semantic vector space model. Definition: Action words are verbs that are used to strengthen the way experiences are presented whether it is expressing positive or negative experience. With the help of Wordnet, the terms from the tf (term frequency) matrix which belong to the ACTION class can be easily classified. The algorithm uses a seed word list to identify the action words.  Multiple Document Summarization Using  145  Principal Component Analysis Incorporating Semantic Vector Space Model  Definition: Seed Word List is the collection of action words. (Appendix A)  Whenever a term from the tf matrix is fetched, it is matched against seed word list. If it is matched, then the fetched term is action word; otherwise, synonyms of the fetched terms are matched. Given Input: T = {t1,t2,…..,tn}. List type: A = { }, integer type: depth Do: for every t ε T depth = 0; match(t,seedwrdlst) if found then A = A U t. else, not found if(depth == 0) match(extractsynonym (t),seedwordlist) depth = 1 else continue endif endif endfor Output : A = {t1,t2,….tm}. Document D  Parser  Tf Matrix  Term Fetching  Seed WordList  Matching  Not found && depth = 0  found Action word list of Document D  Synonyms Extraction and depth = 1 WordNet  Figure 1. ACTION Word Classifier  To decide whether the word belongs to action list or not, we have to build a seed wordlist and compare them with standard meaning. For example, let ‘devastation’ be the word to be decided as action or not. After searching in WordNet, the following meanings were obtained: • desolation (an event that results in total destruction)  146  Om Vikas et al.  • ravaging, (plundering with excessive damage and destruction) • destruction, (the termination of something by causing so much damage to it that it cannot be repaired or no longer exists) From the first and last meaning, it clearly lies in the phenomenon of appear/disappear so it will be appended into the seed list along with its Synset. 3.2.2 Finding the Objects of the Action Merely acquiring the ACTION words doesn’t provide the semantic to the vector space. We have to find whether these words are really important. The importance of the word can be estimated by the application of the word in the article. Objects corresponding to the Action Words and their weight in Statistical VSM have to be identified in order to determine the extent of relevancy of Action Words. The Objects are the Nouns or Adjectives for the Action. The nearest Noun for Verb is identified using POS Tagger and termed as Object of Action. Only those sentences are to be chosen which contain action words.  Today broke fire in Delhi. (Action is verb) Today/NN broke/VBD fire/NN in/IN Delhi/NNP  Destruction of material happens due to this fire. Destruction//NN of/IN material/NN happens/VBZ due/JJ to/TO this/DT fire/NN ./.  Many suffered from the broken glass in the road. (Action is Adjective) Many/JJ suffered/VBD from/IN the/DT broken/JJ glass/NN in/IN the/DT road/NN. /.  The authority arrives here soon. The/DT authority/NN arrives/VBZ here/RB soon/RB. /.  Table 1. Action-Object List  Action word  Objects  broke  fire, glass  arrives  Authority  destruction  material  The bold ones are selected as objects for the Action. The action-object list is prepared by the help of POS tagger and Contextual Action Words are determined.  Multiple Document Summarization Using  147  Principal Component Analysis Incorporating Semantic Vector Space Model  3.2.3 Classification of Contextual Words Contextual words are being defined as those action words which are applied to the important object. The Weight of Action Word is being taken as the maximum weight amongst all the objects corresponding to the given Action Word. The weight obtained is added to the weight of the corresponding Action Words in Statistical VSM yielding Semantic Vector Space Model for the given set of Documents. If we take an example of a single document:  Today broke fire in Delhi. Mass Destruction of material happens due to this fire. Many suffered from the broken glass in the road. The authority arrives here soon. Till now there is report of any casualties in these fire except from few injures. Thanks for the local communities for help.  the Vector Space generated on the basis of term frequency feature is  Table 2. Vector Space of Single Document (1 x 20)  Broke  0.1889 Injuries  0.1889  Fire  0.5669 Thanks  0.1889  Delhi  0.1889 Local  0.1889  Mass  0.1889 communities 0.1889  Destruction 0.1889 Help  0.1889  Material  0.1889 Authority  0.1889  Happens  0.1889 Arrives  0.1889  Suffered  0.1889 Report  0.1889  Broken  0.1889 Casualties  0.1889  glass  0.1889 Road  0.1889  The Action Word List obtained corresponding to the above example is: broke, destruction, and arrives. Now, the Action-Object list is prepared by identifying the Object words in which the ACTION words are acted.  Table 3. Object-Action List for example above  Action word broke destruction arrives  Objects today, fire, glass Material Authority  148  Om Vikas et al.  Each ACTION word has been given weight as per the contextual word obtained corresponding to it.  Table 4. Contextual Action List  Action word weight factor (wt)  broke  Max (0.1889,0.5669) = 0.5669  destruction  0.1889  arrives  0.1889  The Statistical VSM is now modified and the Semantic VSM is being generated as follows  Table 5. Semantic Vector Space Model  Broke  0.7558 Injuries  0.1889  Fire  0.5669 Thanks  0.1889  Delhi  0.1889 Local  0.1889  Mass  0.1889 communities 0.1889  Destruction 0.3778 Help  0.1889  Material  0.1889 Authority  0.1889  Happens  0.1889 Arrives  0.3778  Suffered  0.1889 Report  0.1889  Broken  0.1889 Casualties  0.1889  glass  0.1889 Road  0.1889  Similarly, the model is extended for multiple documents. This Semantic Vector Space Model is used further to determine important Keywords and henceforth, the summary.  3.3 Principal Component Analysis Principal Component Analysis (PCA) [Michael et al. 2003] is used to reduce the multidimensional datasets to lower dimensions for analysis. Singular Value Decomposition (SVD) [Michael et al. 2003] is carried out on Semantic VSM to find the principal components of Vector Space. The singular value decomposition (SVD) of matrix Amxn is the factorization A=U∑VT, where U and V are orthogonal, and ∑= diag (σ1,…,σr), r= min (m,n), with σ1≥ σ2≥ ……≥ σr ≥0 . The columns of V are the ‘hidden’ dimensions that we are looking for. The diagonal of ∑ are the singular values which are the weights for the new set of basis vectors. ∑ is symmetric, its singular values are its eigen values and its basis vectors are the eigen vectors. Given an eigen vector e, we can find the corresponding dimension in document space. d= D.e  Multiple Document Summarization Using  149  Principal Component Analysis Incorporating Semantic Vector Space Model  After determining out the dimension of eigen vector in document space, backprojection of d* is carried out. Commonly, composing a vector in terms of the principal components is called backprojection. Since our principal components or eigen documents are all orthogonal vectors, this is easy to accomplish. Let E be the matrix formed from the eigen documents then vector p is the document projected onto the eigenspace. p = ET d Relevance of the topic/document is calculated by dividing projected component by the corresponding Singular Value. Metrics thus obtained is arranged in decreasing order excluding out the negative metrics. Main topic/Document is the one with highest metric value. After selecting the main topic, we now need the topic keywords. We simply take the eigen document vector corresponding to main document and select the words with high weight. These are the set of Keywords which are of high relevance in summary. 3.4 Sentence Extraction To identify sentences that should belong to summary, several features have been taken into consideration. • Sentence-Length Cut off Feature – If the sentence length is greater than 4 words, only then it is taken into consideration. • Position Feature – Sentences have been given some weight based on their position in the paragraph whether it is in initial, middle or final. • Keywords - Sentence weight also depends not only on the number of keywords present in it but also the weight of each keyword. • Upper Case Feature - Sentences containing upper case words have been given additional weight as it is probable that they may contain proper nouns. Sentences with higher weight are taken as the relevant sentences for the summary and arranged in the order they appear in the document yielding the required summary. The rearrangement becomes a challenge in the case of multiple documents. In that case, sentences are kept at the position at which they appear in original document (initial/middle/final). This rearrangement technique provides fair results. 4. Implementation The Multiple Document Summarization System is implemented in Java using JAMA (Java Matrix Package) and WVTool (Word Vector Tool) packages. JAMA is used to perform all the matrix operations as computing SVD, eigen vector, Backprojection, etc. WVTool is used to  150  Om Vikas et al.  generate the Statistical Vector Space Model taking input as the Multiple Document Set or a Single document based on user requirement. 5. Evaluation of Summarizer The present section will focus on the accuracy of the proposed summarization method. The accuracy of the method was examined on both single as well as multiple document summaries:  5.1 Single Document summary  Text belonging to different areas was taken. Summaries to the same texts were made by sentence extractions by different people. Based on the set of the summaries, we ranked sentences of the texts.  We then carried out the summarization process using our algorithm, the Auto Summarizer in MS Word, and the Gnome Summarizer and compared their agreement on the extracted sentences with the human sentence extractions.  The results are given in the following table.  Table 6. Summarization Algorithm Results  Article #  Our  MS Word  Summarizer Summarizer  Science (789 words)  60.0%  50.0%  Geography (725 words)  55.56%  33.33%  History (557 words)  70.0%  50.0%  Average accuracy  61.85%  44.44%  Gnome Summarizer 70.0% 22.5% 48.5% 47%  On an average, we get an average accuracy of 61.85% and improvement of 39.17% with respect to MS Word Summarizer.  5.2 Multiple Documents Summary The set of Documents belonging to “Introduction to Web crawler” were taken and then summary was generated using the proposed algorithm, and it was observed that the summary thus generated was in coherence with most of the documents. The input documents set consisting of documents related to the topic for summarization has been shown in Table 7.  Multiple Document Summarization Using  151  Principal Component Analysis Incorporating Semantic Vector Space Model  %age in summ ary  Multiple Document SummaryResult  50.00% 40.00% 30.00% 20.00% 10.00% 0.00%  
Ensuring consistency of Part-Of-Speech (POS) tagging plays an important role in the construction of high-quality Chinese corpora. After having analyzed the POS tagging of multi-category words in large-scale corpora, we propose a novel classification-based consistency checking method of POS tagging in this paper. Our method builds a vector model of the context of multi-category words along with using the k-NN algorithm to classify context vectors constructed from POS tagging sequences and to judge their consistency. These methods are evaluated on our 1.5M-word corpus. The experimental results indicate that the proposed method is feasible and effective. Keywords: Multi-Category Words, Consistency Checking, Part of Speech Tagging, Chinese Corpus, Classification 1. Introduction The construction of high-quality and large-scale corpora has always been a fundamental research area in the field of Chinese natural language processing. In recent years, rapid developments in the fields of machine translation (MT), information retrieval (IR), etc. are demanding more Chinese corpora of higher quality and larger scale. Ensuring the consistency of Part-of-Speech (POS) tagging plays an important role in the construction of high-quality Chinese corpora. In particular, we focus on consistency checking of the POS tagging of multi-tagged words, which consist of same Chinese characters and are nearly synonymous, yet have different grammatical functions. No matter how many different POS tags a multi-category word may be tagged with, it must be assigned the same POS tag when it appears in a similar context. 
This paper describes an annotation guideline for a temporal relation-tagged corpus of Chinese. Our goal is construction of corpora to be used for a corpus-based analysis of temporal relations among events. Since annotating all combinations of events is inefficient, we examine the use of dependency structure to efficiently recognize temporal relations. We annotate a part of Treebank based on our guidelines. Then, we survey a small tagged data set to investigate the coverage of our method. While we find that use of dependency structure drastically reduces manual effort in constructing a tagged corpus with temporal relations, the coverage of the methods achieves about 63%. Keywords: Temporal Entities, Event Entities, Temporal Reasoning, Event Semantics, Dependency Structure 1. Introduction Extracting temporal information in documents is a useful technique for many NLP applications such as question answering, text summarization, machine translation, and so on. Temporal information includes three elements: 1. temporal expressions, which describe time or period in the real or virtual world; 2. event or situation expressions that occur instantaneously or that last for a period of time; 3. temporal relations, which describe the ordering relation between an event expression and a temporal expression or between two event expressions. There is a great deal of research dealing with temporal expressions and event expressions. Extracting temporal expressions is a subtask of NER [IREX committee 1999] and is widely studied in many languages [Mani et al. 2006b]. Normalizing temporal expressions has been investigated in evaluation workshops [Chinchor 1997]. Event semantics has been investigated in linguistics and in AI fields [Bach 1986]. However, research on temporal relation extraction is still limited. Temporal relation extraction includes the following issues: identifying events, anchoring an event to the timeline, ordering events, and reasoning of ∗ Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan E-mail: {yuchan-c, masayu-a, matsu}@is.naist.jp [Received May 21, 2007; Revised September 3, 2007; Accepted September 11, 2007]  172  Yuchang Cheng et al.  contextually underspecified temporal expressions. To extract temporal relations, several knowledge sources are necessary, such as tense and aspect of verbs, temporal adverbs, and world knowledge [Mani et al. 2006b]. The goal of our research is to efficiently construct a temporal relation tagged corpus of Chinese. In English, TimeBank [Pustejovsky et al. 2006], a temporal information tagged corpus, is available for introducing machine learning approaches to automatically extract temporal relations. In Chinese, there is some related research on temporal expression extraction [Li et al. 2005]. However, there is no publicly available resource for temporal information processing in Chinese. Currently, such resources, event and temporal relation tagged corpora, are being made. Annotating all temporal relations of event pairs is time-consuming. Therefore, we propose a dependency structure based method to annotate temporal relations manually on a limited set of event pairs and extend the relations using inference rules. This method reduces manual effort. The dependency structure helps to detect subordinate and coordinate structures in sentences. We also describe a guideline for corpus annotation. Our annotation guideline is based on TimeML [Saurí et al. 2005], which was originally designed for English texts. We have developed a machine learning based dependency analyzer for Chinese [Cheng 2005]1 and a Chinese morphological analyzer [GOH 2006]. We can use our dependency analyzer to analyze raw text then use the output to annotate the temporal relations. However, in this paper, we use a syntactic tagged Chinese treebank (Penn Chinese Treebank) [Palmer et al. 2005] to create a temporal information annotated corpus. Finally, we survey the distribution of the temporal relations in our tagged corpus. We also evaluate the coverage of limited event pairs in our criteria. 2. Background We investigated the distribution of events and temporal expressions in TimeBank [Pustejovsky et al. 2006] and Penn Chinese Treebank. TimeBank is a temporal information tagged corpus of English with TimeML annotation guideline. We found that the distribution of events and temporal expressions is uneven. Therefore, our corpus does not focus on the relations between an event and a temporal expression, but between two events. TimeML is a corpus annotation guideline of temporal information for English news articles. In this section, first, we introduce the resource -- TimeBank and describe the temporal relation links of original TimeML. Second, we investigate the distribution of events and temporal expressions in TimeBank and Penn Chinese Treebank. Finally, we analyze the temporal relation links to observe the correlation between dependency structure and TimeML links. 
This study attempts to explore the effects of formal schemata or rhetorical patterns on reading comprehension through detailed analysis of a case study of 45 non-English majors from X University. The subjects were selected from three classes of comparable English level and were divided into three groups. Each group was asked to recall the text and finish a cloze test after reading one of three versions of a passage with identical content but different formal schemata: description schema, comparison and contrast schema, and problem-solution schema. Both quantitative and qualitative analyses of the recall protocol indicate that subjects displayed better recall of the text with highly structured schema than the one with loosely controlled schema, which suggests that formal schemata has a significant effect on written communication and the teaching of formal schemata to students is necessary to enhance their writing ability. Keywords: Formal Schema, Schema Theory, Reading Comprehension 1. Introduction For many people, reading is the most important of the four skills in a second language, especially in English as a second or foreign language. According to Carrell [2006], effective reading in a second language is critical for students in EFL contexts, at an advanced level of proficiency, or with a need for English for Academic Purposes. For a long time, EFL reading was viewed as a rather passive, bottom-up process. In other words, EFL reading was primarily a decoding process of reconstructing the author’s intended meaning through identifying the printed letters and words and building up a meaning for a text from the smallest textual units at the “bottom” (letters and words) to larger and larger units at the “top” (phrases, clauses,  ＊Xi’an University of Finance and Economics, Xi’an, China E-mail: zhgydyx@yahoo.com.cn [Received July 8, 2007; Revised February 13, 2008; Accepted April 15, 2008]  198  Xiaoyan Zhang  intersentential linkages). Correspondingly, problems of second language reading and reading comprehension were considered essentially decoding problems, deriving meaning from print [Rivers 1964, 1968; Plaister 1968; Yorio 1971]. Only since 1979, has a truly top-down approach been proposed in second language reading [Steffensen et al. 1979; Carrell 1981, 1982; Carrell and Eisterhold 1983; Johnson 1981, 1982; Hudson 1982]. The top-down perspective of reading process has had a profound impact on reading comprehension, and it views the top-down perspective of the reading process as a substitute for the bottom-up, decoding view of the reading process, rather than being its complement. Only after the appearance of the schema theory, has it been made clear that effective and efficient reading – either in a first or second language — requires both top-down and bottom-up strategies operating interactively [Rumelhart 1977a, 1980; Sanford and Garrod 1981; van Dijk and Kintsch 1983]. Rumelhart [1977b] views reading comprehension as the process of choosing and verifying conceptual schemata for the text. A schema is said to be “a cognitive template against which new inputs can be matched and in terms of which they can be comprehended” [Rumelhart and Ortony 1977: 131]. According to the schema theory, not only is the reader’s prior linguistic knowledge (linguistic schemata) and level of proficiency in the second language important, but the reader’s prior background knowledge of the content area of the text (“content” schemata) as well as of the rhetorical structure of the text (“formal” schemata) are also important [Carrell 2006]. The importance of linguistic schemata in reading comprehension has long been recognized because of the long history of the bottom up view of reading comprehension, and, with the rise of the schema theory, researchers have showed great interest in the importance of content schemata and formal schemata. However, compared with the studies on content schema, studies on formal schema are much less frequent. Therefore, the present study focused on the effects of formal schema on reading comprehension with a view to arriving at a better understanding of the unique nature of EFL reading. This paper consists of five parts. Part One provides an overview of the study. Part Two presents the background of the study. The elaboration of the experiment is found in Part Three, while the results and discussion are presented in Part Four. Part Five summarizes the findings of the experiment and discusses the implications of the results to the development of EFL reading instruction as well as the limitations of this study. 2. Literature Review Formal schemata [Sharp 2002] are part of the macrostructure of a text and contain the logical organization of the text which the writer has used to represent the intended meaning.  The Effects of Formal Schema on  199  Reading Comprehension-An Experiment withChinese EFL Readers  Meyer and Freedle [1979] explored the effects of different formal schemata on recall. The 4 types of formal schemata compared were: (1) contrastive schema; (2) cause-effect schema; (3) problem-solution schema; and (4) collection–of–descriptions schema. The first three types of formal schemata have “an extra link of relationship” over the descriptive schema. Results demonstrated that subjects who were exposed to formal schemata 1 and 2 recalled more than formal schemata 3 and 4. The results can be explained by schema theory. Based on this theory, recall of information relayed by the first three formal schemata, which offer extra linkage, should be better than that of the descriptive schema. Meyer et al. [1980] conducted another experiment to confirm that readers who adopted the strategy of identifying the author’s organization structure would be able to recall more information than students who did not. Results were consistent with the predicted outcome. The experiments described above were conducted on L1 readers. Research dealing with L2 readers has been more limited. However, Carrell [1984] used Meyer’s passage on “Loss of body water” presented in different schemata to test the effects of top-level organization on ESL readers. Results indicated that certain highly structured schemata facilitated the recall for L2 readers in general. L2 readers tested included Spanish, Arabic, Oriental languages (Korean and Chinese), and Malaysian. Carrell’s work was duplicated by Foo [1989], Goh [1990], and Talbot et al. [1991] using exactly the same texts. For all of these investigations, the general conclusion was that the formal schemata of the texts had an effect on reading comprehension as measured by recall and the more tightly controlled schema seemed to be easier to comprehend. Peregoy and Boyle [2000] also stated that text structure knowledge enhanced comprehension by helping readers to anticipate and predict the direction of a plot or argument, thereby facilitating attention to the larger meaning of the text. Besides, Sharp [2002] studied the effects of the four formal schemata of expositions on comprehension by cloze test and recall protocol. The four formal schemata include description, cause-effect, listing and problem-solution. The result was consistent with the previous work in that it clearly demonstrates that formal schemata do affect reading comprehension. However, the results indicated that it was the most loosely organized text (description) that scored the highest, which was quite different from other investigations of this type [e. g. Carrell 1984; Foo 1989; Goh 1990]. The above-mentioned studies have provided a general view upon the role that formal schema plays in EFL reading comprehension. However, some aspects still remain unexplored. First, some results were quite contradictory, for example, Sharp [2002] versus Carrell [1984] as well as Foo [1989]. Besides, no study has been conducted to study the effects of formal schemata in English reading on a specific cultural group — Chinese EFL non-English college students. A text is a complete linguistic unit to discuss a topic, while around the topic different  200  Xiaoyan Zhang  people have different ways of developing it [Xiao 2001]. Kaplan [1966] claims that the rhetorical pattern of a text is language-specific and culture-unique and reflects the thought pattern of a particular group. For instance, the English thought pattern is straightforward and the topic is usually developed using a deductive method, while the Chinese thought pattern is spiral and the topic is usually developed via an inductive method [Xiao 2001]. Therefore, the question this study addresses is: Will the formal schemata of EFL (represented by three different patterns) affect the reading comprehension of native Chinese non-English major college students? 3. Method 3.1 Subjects Fifty-five sophomores from the School of Liberal Arts and Law of X University were selected to participate in this study. They ranged from 17 to 19 years old, and 15 of them were male. Before entering university, these students already had six years of contact with English as a foreign language, with an average of four hours of English classes per week. By the time they entered university, they had learned about 2,000 words, and they could read simplified English texts and write short compositions. As non-English majors at X University, the sophomores took a College English course totaling 4 hours per week. The 45 participants were selected out of 150 students from three classes in School of Liberal Arts and Law and students from the same class fell into the same group (each group 15 participants). The researcher excluded the top 35 students and poor students and chose those 15 intermediary students in each class on the basis of the final exam of the English course. In addition, a one–way ANOVA indicated no significant differences existed among three groups of subjects. 3.2 Material Three versions of a passage with identical content but different formal schemata were used (see Appendix A). The three types of formal schemata selected in the study are description, comparison and contrast, and problem-solution. This combination is unique for a couple of reasons: the previously-mentioned studies [Sharp 2002; Foo 1989; Carrell 1984] all studied the first and the last type of schemata while bringing out contradictory results, along with the comparison and contrast type being missed in both studies [Sharp 2002; Foo 1989]. Passage 1 is description, and no clear relationship can be seen among its components. Therefore, it is highly loosely organized. The macrostructure of Passage 2 (comparison and contrast) follows the three-part pattern — introduction, body, and conclusion. In the introduction part, not only are the subjects to be compared and contrasted identified, but also the points made about them  The Effects of Formal Schema on  201  Reading Comprehension-An Experiment withChinese EFL Readers  are stated clearly. The body adopts the subject-by-subject pattern. One subject is to be discussed fully, then the next subject. In discussing each subject, some aspects are examined in detail. The conclusion part summarizes the main points. Passage 3 is problem-solution which is arranged into three parts. The introduction introduces the problem, the body expands on the solutions to the problem, and the conclusion is the summary of the solutions to the problem. The discourse signals in Passage 2 and Passage 3 are highlighted in Appendix A. The three passages are appraised as the same level of difficulty by three English professors in Y University in terms of word length, word choice, and sentence type. The numbers of words in the three passages were 192, 193, and 196, respectively. No uncommon words appeared in any of those passages. There were four complex sentences in each of the three passages. 3.3 Instruments A considerable number of methods have been used to measure reading behavior and reading comprehension. A text-based cloze procedure and a recall protocol were considered the most appropriate methods for this study. Both methods have been used in reading comprehension investigation extensively, and both methods allow a large number of subjects to be assessed. 3.3.1 Cloze Test A cloze test was employed in this study. Cloze tests have aroused much discussion and have drawn lots of criticism. However, there is enough evidence to support them as a measure of reading comprehension. Bormouth [1968] concluded that cloze tests seemed to be valid measures of passage difficulty. Cloze tests appear to be valid, reliable language proficiency tests that can be easily constructed and used by ESL teachers [Aitken 1975; Stubbs and Tucker 1974; Oller 1973; Brown 1980]. Besides, both Bachman [1985] and Jonz [1990] reviewed the investigations supporting cloze. The type of cloze construction adopted in the present study is similar to the one in Sharp’s study [2002], based on Farhady and Keramati’s study [1996]. Farhady and Keramati’s design calculates deletion rates on the basis of noun phrases in a text and they assert that such a design takes better account of the discoursal and linguistic structure of the language used and is a comparatively good test of reading comprehension because of improved reliability and validity. In this design, noun phrase calculation should conform to the following rules: conjoined NPs were regarded as single units; complex NPs (NPs with embedded NPs) were counted as single units and pronouns were ignored. Exact word scoring, which requires the word put in to be the exact word used in the original text, was used. Deletion rates for the text were: description every 6th word, 29 deletions; comparison and contrast every 7th word, 25 deletions; problem-solution every 6th word, 30 deletions. Deletion rate changes between 6 and  202  Xiaoyan Zhang  7 are not likely to have an effect [Alderson 1979; Porter 1978]. A sample cloze together with calculation is shown in Appendix B. 3.3.2 Recall Protocol This study also used a recall protocol to measure subjects’ reading comprehension of the given texts. Employing a recall protocol to test reading comprehension is a common practice in research of this sort. For example, Morrow [1988] and Salvia and Hughes [1990] strongly recommended recall as a method of classroom assessment for instructional and diagnostic purposes. This method requires that the text be divided into idea units. An idea unit, also called a linguistic unit [Bransford and Franks 1971; Carrell 1983] and an information unit [Roller 1990], is the minimal words necessary to express a thought or idea. Therefore, subjects’ reading comprehension is measured by the number of idea units recalled, i.e. the amount of information recalled. The segmentation of texts in this study was similar to Johnson’s [1970] to allow quantitative assessment of recall. Furthermore, to account for qualitative level differences in recall, the researcher followed Sharp’s practice [2002]: the idea units were also rated for importance within the text. The quantity and quality of idea units was determined by the agreement reached by eight English professors in Y University and was shown in Table 1 (see Appendix C). 3.4 Data Collection At the very beginning of the test, subjects were briefly informed that the text was about houses. Since recall protocols allow the possibility of rote learning without real understanding of the text, they were not told that they would be asked to recall and they were asked to fill a questionnaire about personal information (see Appendix D) as a distraction task between the initial reading of the text and the recalling of the text. When the test started, the three texts with different formal schemata were distributed to the three groups of subjects, respectively, and the first sentences of all three texts were the same. Then, they were asked to read the text in five minutes. After three minutes of answering the questionnaire, they were told that that they should take a test in English within ten minutes. Eventually, a ten-minute cloze test was administered. 3.5 Data Analysis The 45 cloze items (15 descriptions, 15 comparison and contrasts, and 15 problem-solutions) were rated by two raters. They were completely agreeable with each other because cloze requires exact words. In addition, the 45 recall protocols were also sent to these two raters to assess both in terms of quantity and quality on the basis of the templates reached by eight English professors in Y University. For the convenience of comparison, both the quantitative  The Effects of Formal Schema on  203  Reading Comprehension-An Experiment withChinese EFL Readers  score and qualitative score of recall protocols adopted the percentage system. The quantitative score of each recall protocol was calculated according to the following formula: quantitative score = (the idea units recalled / the total idea units in the recalled passage)×100. The qualitative score of each recall protocol were obtained based on another formula: quantitative score = (sum of the importance level of each recalled unit / sum of the importance level of all idea units in the recalled passage)×100. In this study, misspellings and grammatical mistakes did not affect the score of a subject’s recall because such mistakes did not mirror readers’ understanding of the passage. The reliability of the essay ratings for the two raters was excellent (see Table 2) and the final score of each one was the average of the two scores given by the two raters. Both raters selected for this study were intensive reading teachers of English with more than five years in the English department of Y University, so they were highly competent to fulfill this task.  Table 2. Interrater Reliability for the Two Raters  Reading text  Correlation coefficient 1 (quantity)  Correlation coefficient 2 (quality)  Text 1  0.97  0.94  Text 2  0.88  0.87  Text 3  0.93  0.89  The quantitative and qualitative scores of recall protocols were input in Statistical Package for Social Sciences (SPSS) for descriptive and inferential analysis. The effect of formal schema on reading comprehension was measured through one way ANOVA.  4. Results and Discussion  Research Question:  Will the rhetorical pattern of EFL (represented by three different patterns) affect the reading comprehension of native Chinese non-English major college students?  Table 3. Cloze, Recall 1 (Quantitative) and Recall 2 (Qualitative) scores for the 3 texts  Text  Cloze Mean  Recall 1 Mean (Quantitative)  Recall 2 Mean (Qualitative)  Text 1 51.0000  39.6667  43.5333  Text 2 53.3333  48.2667  49.9333  Text 3 59.5333  58.4667  62.6667  Overall 54.6222  48.8000  52.0444  F  1.120  8.863  8.373  Sig.  .336  .001  .001  204  Xiaoyan Zhang  The means of three kinds of score: cloze, recall 1 (quantitative), and recall 2 (qualitative) for the three texts with different formal schemata (description, comparison and contrast, problem-solution) are presented in Table 3. As shown in Table 3, the cloze test did not demonstrate significant differences among the three schematically different texts (p=0.336>0.05). However, when it comes to recall protocol, both the quantitative and qualitative scores indicated significant differences among the text types. In terms of quantitative measure the three schematically texts were significantly different at the level of 0.001, with Text 1 (description) scoring the lowest (mean=39.6667) and Text 3 scoring the highest (mean=58.4667). Text 2 scored in between (mean=48.2667). As for qualitative measure, the texts again witnessed significant differences at the level of 0.001. Text 3 (mean=62.6667) still obtained the highest score while Text 1 (mean=43.5333) achieved the lowest and Text 2 was in the middle (mean=49.9333). This means that, in terms of the cloze test, no significant difference could be found among the three texts with different schemata, but, according to the number and the importance of the idea units recalled, significant differences did exist among them. In general, the effect of formal schemata on reading comprehension observed in this study is quite similar to the profile documented in many research papers [Meyer and Freedle 1979; Meyer et al. 1980; Carrell 1984; Foo 1989; Goh 1990; Talbot et al. 1991; Sharp 2002 ] in that it revealed that formal schemata indeed affects reading comprehension. However, in this study, no significant difference was found in the cloze test among the schematically different texts as opposed to Sharp’ study [2002], in which the four text types — description, cause-effect, listing, and problem-solution differed significantly in the cloze score with description scoring the highest. This can be attributed to the teaching and learning style of Chinese teachers and students. In China, especially in mainland China, both English teachers and learners attach great importance to grammar and stress precision, particularly at the sentence level. In addition, a cloze test is an inevitable part of any kind of English test in mainland China, CET-4 and CET-6 in particular, so preparing for such exams requires one to be acquainted with the relevant skills in doing such an exercise. The effect of formal schemata on reading comprehension might be overridden by the effect of those techniques. The differences in teaching systems between Hong Kong and mainland China, together with the difference in the subjects between this study and Sharp’s [2002], correspondingly, led to different findings. Hong Kong was a colony of Britain; accordingly, its current teaching system (including the teaching aim, method and strategies) was strongly influenced by Britain. A wide gap exists between the teaching systems (including the policies on educational administration, the language of instruction, the allocation of funds, the examination system, the system of academic awards and the recognition of educational qualifications) of Hong Kong and mainland China [Bray 1997]. For example, English is the medium in class in  The Effects of Formal Schema on  205  Reading Comprehension-An Experiment withChinese EFL Readers  Sharp’s study [2002] while Chinese is the instruction language in this study except in English class, which inevitably exerted a certain effect on the results of the two studies. Furthermore, the subjects in Sharp’s study [2002] were Hong Kong Chinese school children (mean age 14.1) while the subjects in this study are sophomores in a mainland China university. They differ greatly in terms of age, language environment, etc. However, in the light of recall protocol, both quantitatively and qualitatively, significant differences can be observed among different text types. This result can be explained well by the schemata theory. The last two types of formal schema — comparison and contrast, along with problem-solution — have an additional link of relationship over the descriptive schema. Accordingly, recall of information conveyed by the last two formal schemata, which offer extra linkage, should be better than that of the descriptive schema only if the subjects have a great ability of identifying the formal schemata. This is because Foo’s study [1989] suggests that rhetorical structures (formal schemata) which help information recall are not necessarily easy to recognize. The subjects of this study were brought up in mainland China’s education system which is exam-oriented, and they are strongly influenced by it. Besides, the majority of important English writing tests in mainland China such as CET-4, which is of vital importance to college students like the subjects in this study, is often exposition/argumentation writing and is seldom description writing. Therefore, both teachers and students take great effort to analyze and to practice the rhetorical patterns of such writings. This can explain why they recall better in comparison and contrast and in problem-solution than in description. The difference between results from the cloze test and the recall test of the present study can be viewed from another perspective — being attributed to the inherent difference between cloze and recall tests. Some researchers [Alderson 1979; Lado 1986; Markham 1985] claim that the cloze test in general is more a test of linguistic skills (e. g., grammatical and lexical knowledge) than of reading comprehension because cloze items are often based on cues from the immediate environment around the blank rather than on information from the whole text. In contrast, a recall test involving production of information may additionally require the selection and coordination of ideas and impressions, formulation, and ordering of remembered information [Meyer 1984]. The production skills that are required by recall tasks may affect both the quantity and the quality of information students recall [Johnston 1983]. According to Johnston’s description of the cognitive requirements of recall tasks, the reader must understand and store the information, must be able to retrieve it on demand, and must decide on a starting point, a path through the information. The above discussion boils down to one point that, in comparison with cloze test, a recall test relies more on text’s rhetorical structure, which might be the cause for different results from the cloze and recall tests being obtained.  206  Xiaoyan Zhang  5. Conclusion In this thesis, the author has mainly studied the effect of formal schemata on reading comprehension by making a comparison of the reading scores of three schematically different texts by three groups of subjects, respectively. The three text types are description, comparison and contrast, and problem-solution. The results indicate that significant differences do exist among the text types both in terms of the quantity and quality of the recall protocol with the highly structured schema — problem-solution scoring the highest and the loosely controlled schema — description scoring the lowest. However, significant differences are not found among these three types in the cloze test. This study has called attention to the formal schemata in written communication. Traditionally, teaching of writing emphasizes the instruction of new words and syntactic structures so students are unlikely to create communication problems at the sentence level, while producing many essays described by western scholars as written in a roundabout way, being flashy, and lacking consistence and logic [Coe and Hu 1989]. As this research demonstrates, texts written with clear structure and logical connection yield efficient communication. Therefore, it is the responsibility on the part of English writing teachers to get students acquainted with the practice of embodying the particular formal schema in their specific writing task. There are several limitations to the present study due to time and other resources. The major limitation of the study was the sample size. In this study, the sample size is not large enough. With the small number of participants, the experiment was conducted in a small scale; the data thus collected may not be large enough for statistically significant generalization. Thus, conclusions are drawn within this context. Another limitation is the text type. The text type studied only includes description, comparison-contrast, and problem-solution. Future research should consider exploring other text types and using larger samples. References Aitken, K. G., “Problems in Cloze Testing Re-examined,” TESL Reporter, 8, 1975, pp. 2. Alderson, J. C., “The Cloze Procedure and Proficiency in English as a Foreign Language,” TESOL Quarterly, 13(2), 1979, pp. 219-223. Bachman, L. F., “Performance on Cloze Tests with Fixed-ratio and Rational Deletions,” TESOL Quarterly, 19(3), 1985, pp. 535-556. Bormuth, J. R., “Cloze Test Readability: Criterion Reference Scores,” Journal of Educational Measurement, 5(3), 1968, pp. 189-196. Bransford, J. D., and J. J. Franks, “The Abstraction of Linguistic Ideas,” Cognitive Psychology, 2(4), 1971, pp. 331-350.  The Effects of Formal Schema on  207  Reading Comprehension-An Experiment withChinese EFL Readers  Bray, M., “Education and Colonial Transition: The Hong Kong Experience in Comparative Perspective,” Comparative Education, 33(2), 1997, pp. 157-169. Brown, J. D., “Relative Merits of Four Methods for Scoring Cloze Tests,” Modern Language Journal, 64(3), 1980, pp. 311-317. Carrell, P. L., “Culture-specific Schemata in L2 Comprehension,” Selected Papers from the Ninth Illinois TESOL/BE Annual Convention, the First Midwest TESOL Conference, ed. by R. Orem & J. Haskell, Illinois TESOL/BE, Chicago, 1981, pp. 123-132. Carrell, P. L., “Cohesion Is Not Coherence,” TESOL Quarterly, 16(4), 1982, pp. 479-488. Carrell, P. L., “Some Issues in Studying the Role of Schemata or Background Knowledge in Second Language Comprehension,” Reading in a Foreign Language, 1(1), 1983, pp. 81-92. Carrell, P. L., “The Effects of Rhetorical Organization on ESL Readers,” TESOL Quarterly, 18(3), 1984, pp. 441-469. Carrell, P. L., “Introduction: Interactive Approaches to Second Language Reading,” Interactive Approaches to Second Language Reading, ed. by P. L. Carrell, J. Devine, & D. E. Eskey, Cambridge University Press, New York, 2006, pp. 1-7. Carrell, P. L., and J. C. Eisterhold, “Schema Theory and ESL Reading Pedagogy,” TESOL Quarterly, 17(4), 1983, pp. 553-573. Coe, R. M., and S. Hu, “A Preliminary Study of Contrastive Rhetorical Patterns in English and Chinese,” Journal of Foreign Languages, (2), 1989, pp. 40-46. van Dijk, T., and W. Kintsch, Strategies of Discourse Comprehension, Academic Press, New York, 1983. Farhady, H., and M. N. Keramati, “A Text Driven Method for the Deletion Procedure in Cloze Passages,” Language Testing, 13(2), 1996, pp. 191-207. Foo, R. W. K., “A Reading Experiment with L2 Readers of English in Hong Kong: Effects of the Rhetorical Structure of Expository Texts on Reading Comprehension,” Hong Kong Papers in Linguistics and Language Teaching, 12, 1989, pp. 49-62. Goh, S. T., “The Effects of Rhetorical Organization on Expository Prose on ESL Readers in Singapore,” RELC Journal, 21(2), 1990, pp. 1-11. Hudson, T., “The Effects of Induced Schemata on the ‘Short Circuit’ in L2 Reading: Non-decoding Factors in L2 Reading Performance,” Language Learning, 32(1), 1982, pp. 1-31. Johnson, R. E., “Recall of Prose as a Function of the Structural Importance of the Linguistic Units,” Journal of Verbal Learning and Verbal Behaviour, 9(1), 1970, pp. 12-20. Johnson, P., “Effects on Reading Comprehension of Language Complexity and Cultural Background of a Text,” TESOL Quarterly, 15(2), 1981, pp. 169-181. Johnson, P., “Effects on Reading Comprehension of Building Background Knowledge,” TESOL Quarterly, 16(4), 1982, pp. 503-516.  208  Xiaoyan Zhang  Johnston, P. H., Reading Comprehension Assessment: A Cognitive Basis, International Reading Association, Newark, 1983. Jonz, J., “Another Turn in the Conversation: What Does Cloze Measure,” TESOL Quarterly, 24(1), 1990, pp. 61-83. Kaplan, R. B., “Cultural Thought Patterns in Intercultural Education,” Language Learning, 16(1), 1966, pp. 1-20. Lado, R., “Analysis of Native Speaker Performance on a Cloze Test,” Language Learning, 3(2), 1986, pp.130-146. Markham, P. L., “The Rational Deletion Cloze and Global Comprehension in German,” Language Learning, 35(3), 1985, pp. 423-430. Meyer, B. J. F., “Organizational Aspects of Text: Effects on Reading Comprehension and Applications for the Classroom,” Promoting Reading Comprehension, ed. by J. Flood, International Reading Association, Newark, 1984, pp. 113-138. Meyer, B. J. F., and R. Freedle., The Effects of Different Discourse Types on Recall, Educational Testing Service, Princeton, 1979. Meyer, B. J. F., D. Brandt and G. Bluth., “Use of Top-Level Structure in Text: Key for Reading Comprehension of Ninth-grade Students,” Reading Research Quarterly, 15(1), 1980, pp. 72-102. Morrow, L. M., “Retelling Stories as a Diagnostic Tool,” Re-examining Reading Diagnosis, ed. by S. M. Glazer, L. W. Searfoss, & L. M. Gentile, International Reading Association, Newark, 1988, pp. 128-149. Oller, J. W., “Cloze Tests of Second Language Proficiency and What They Measure,” Language Learning, 23(1), 1973, pp. 105-118. Peregoy, S. F., and O. F. Boyle, “English Learners Reading English: What We Know, What We Need to Know,” Theory into Practice, 39(4), 2000, pp. 237-247. Plaister, T., “Reading Instruction for College Level Foreign Students,” TESOL Quarterly, 2(3), 1968, pp. 164-168. Porter, D., “Cloze Procedure and Equivalence,” Language Learning, 28(2), 1978, pp. 333-340. Rivers, W., The Psychologist and the Foreign-language Teacher, University of Chicago Press, Chicago, 1964. Rivers, W., Teaching Foreign Language Skills, University of Chicago Press, Chicago, 1968. Roller, C. M., “Commentary: The Interaction of Knowledge and Structure Variables in the Processing of Expository Prose,” Reading Research Quarterly, 25(2), 1990, pp. 79-89. Rumelhart, D. E., “Toward an Interactive Model of Reading,” Attention and Performance, Vol. 6, ed. by S. Dornic, Academic Press, New York, 1977a, pp. 573-603. Rumelhart, D. E., “Understanding and Summarizing Brief Stories,” Basic Processes in Reading: Perception and Comprehension, ed. by D. LaBerge & S. Samuel, Erlbaum, Hillsdale, 1977b, pp. 265-303.  The Effects of Formal Schema on  209  Reading Comprehension-An Experiment withChinese EFL Readers  Rumelhart, D. E., “Schemata: The Building Blocks of Cognition,” Theoretical Issues in Reading Comprehension, ed. by R. J. Spiro, B. C. Bruce, & W. F. Brewer, Erlbaum, Hillsdale, 1980, pp. 33-58. Rumelhart, D. E., and A. Ortony, “The Representation of Knowledge in Memory,” Schooling and the Acquisition of Knowledge, ed. by R. C. Anderson, R. J. Spiro, & W. E. Montaque, Erlbaum, Hillsdale, 1977, pp. 99-135. Salvia, J., and C. Hughes, Curriculum-based Assessment: Testing What Is Taught, Macmillan Publishing Company, New York, 1990. Sanford, A. J., and S. C. Garrod, Understanding Written Language, Wiley, New York, 1981. Sharp, A., “Chinese L1 Schoolchildren Reading in English: The Effects of Rhetorical Patterns,” Reading in a Foreign Language, 14 (2), 2002, pp. 1-20. Steffensen, M. S., C. Joag-dev, and R.C. Anderson, “A Cross-cultural Perspective on Reading Comprehension,” Reading Research Quarterly, 15 (1), 1979, pp. 10-29. Stubbs, J. B., and G. R. Tucker, “The Cloze Test as a Measure of English Proficiency,” Modern Language Journal, 58(5/6), 1974, pp. 239-241. Talbot, D., P. Ng, and A. Allan, “Hong Kong Students Reading Expository Prose: Replication of the Effects of Rhetorical Organization on ESL Readers by Patricia Carrell,” Working Papers of the Department of English, City Polytechnic of Hong Kong, 3(1), 1991, pp. 52-63. Xiao, Liming, English-Chinese Comparative Studies & Translation, Shanghai Foreign Language Education Press, Shanghai, 2001. Yorio, C. A., “Some Sources of Reading Problems for Foreign Language Learners,” Language Learning, 21(1), 1971, pp. 107-115.  210  Xiaoyan Zhang  Appendix A: The texts used in the experiment Passage 1 (Description) With the rapid expansion of cities, a huge number of people flocked there, and therefore, the demand of houses exceeds the supply of houses. Quite a lot people have no house to live in. Modern science and technology have made it possible for people to build high buildings and to supply water, electricity and elevators for people living in them. It is also proposed that government should open up underground housing area. High buildings can be set up on the vast pieces of land, on which old buildings of one, two or three stories have been pulled down. In this way a great number of people can get released from this trouble. Opening up an underground housing area is unrealistic because it needs much more money and takes a much longer time to carry out. Besides, since workers have to work under streets and other high buildings in order to build underground houses, it is rather dangerous. In the meantime, thousands of people are waiting anxiously, so enhancing the efficiency of their work not only concerns the happy life of many families but also bears upon the establishment of a harmonious society. (192 words) Passage 2 (Comparison and Contrast) With the rapid expansion of cities, a huge number of people flocked there, which could not offer enough housing. Building high buildings and opening up underground housing areas are the two suggestions proposed to alleviate housing shortage. HOWEVER, THE FORMER IS MORE PRACTICAL THAN THE LATTER. BUILDING HIGH BUILDINGS IS EASIER TO CARRY OUT AND LESS EXPENSIVE. Modern science and technology have made it possible for people to build high buildings and to supply water, electricity and elevators for people living in them. Furthermore, high buildings can be set up on the vast pieces of land, on which old buildings of one, two or three stories have been pulled down. In this way a great number of people’s housing problems can be solved in a relatively short time. IN CONTRAST, opening up an underground housing area is unrealistic. It needs much more money and takes a much longer time to carry out. Besides, since workers have to work under streets and other high buildings in order to build underground houses, it is very dangerous. Though both methods can fulfill the task, BUILDING HIGH BUILDINGS IS MORE PRACTICAL THAN OPENING UP UNDERGROUND HOUSING AREAS. (193 words) Passage 3 (Problem-solution) With the rapid expansion of cities, a huge number of people flocked there, and THE HOUSING PROBLEM in big cities became one of the most serious of all the great problems which face us at the present time. And it is very urgent for us to take effective steps TO  The Effects of Formal Schema on  211  Reading Comprehension-An Experiment withChinese EFL Readers  SOLVE THIS PROBLEM. THERE IS MORE THAN ONE WAY THAT PEOPLE SUGGEST TO SOLVE THIS PROBLEM. We could build high buildings. Modern science and technology have made it possible for people to build high buildings and to supply water, electricity and elevators for people living in them. Furthermore, high buildings can be set up on the vast pieces of land, on which old buildings of one, two or three stories have been pulled down. IN THIS WAY A GREAT NUMBER OF PEOPLE’S HOUSING PROBLEMS CAN BE SOLVED IN A RELATIVELY SHORT TIME. ANOTHER SOLUTION TO THIS PROBLEM is to open up an underground housing area. This method demands more money and a much longer time. Besides, workers have to work under streets and other high buildings in order to build underground houses. If the two methods are adopted, THE HOUSING PROBLEM IS SURE TO BE SOLVED. (196 words) Note: The capitalized words explicitly signal the discourse type of each passage. The interrelationship of the components is unstructured in the description text, with no clear relationships being evident and therefore, there is no clear textual signals in that text.  212  Xiaoyan Zhang  Appendix B: Sample Cloze (based on 6th word deletion)  Problem-solution  With the rapid expansion of cities, a huge number of people flocked there, and the  housing problem in 1.  (big) cities became one of the 2.  (most) serious of all  the great 3.  (problems) which face us at the 4.  (present) time. And it is very  5.  (urgent) for us to take effective 6.  (steps) to solve this problem.  There 7.  (is) more than one way that 8.  (people) suggest to solve this  problem. 9.  (We) could build high buildings. Modern10.  (science) and  technology have made it 11.  (possible) for people to build high 12.  (buildings) and to supply water, electricity 13.  (and) elevators for people living in 14.  (them). Furthermore, high buildings can be 15.  (set) up on the vast pieces 16.  (of) land, on which old buildings 17.  (of) one, two or three stories 18.  (have)  been pulled down. In this 19.  (way) a great number of people’s 20.  (housing) problems can be solved in 21.  (a) relatively short time. Another solution  22.  (to) this problem is to open 23.  (up) an underground housing area. This  24.  (method) demands more money and a 25.  (much) longer time. Besides,  workers have 26.  (to) work under streets and other 27.  (high) buildings in  order to build 28.  (underground) houses.  If the two methods 29. (sure) to be solved.  (are) adopted, the housing problem is 30.  The Effects of Formal Schema on  213  Reading Comprehension-An Experiment withChinese EFL Readers  Appendix C  Table 1. Idea Units and Their Hierarchical Arrangement of the Problem-solution Text  Level of importance  Idea unit  
This study examines voice onset time (VOT) for phonetically voiceless word-initial stops in Mandarin Chinese and in English, as spoken by 11 Mandarin speakers and 4 British English speakers. The purpose of this paper is to compare Mandarin and English VOT patterns and to categorize their stop realizations along the VOT continuum. As expected, the findings reveal that voiceless aspirated stops in Mandarin and in English occur at different places along the VOT continuum and the differences reach significance. The results also suggest that the three universal VOT categories (i.e. long lead, short lag, and long lag) are not fine enough to distinguish the voiceless stops of these two languages. Keywords: Voice Onset Time (VOT), Voiceless Stops 1. Introduction Over the past few decades, beginning with Lisker and Abramson’s [Lisker and Abramson 1964] study, a considerable number of studies have investigated voicing contrasts in stops by the use of voice onset time (VOT). VOT has come to be regarded as one of the most important methods for examining the timing of voicing in stops (especially in word-initial position) and has been applied in studies of many languages. However, only a few attempts have been made to examine VOT patterns in Mandarin so far. Three universal categories of phonetically voiceless stops are generally recognized [Lisker and Abramson 1964], and Mandarin and English occupy the same place along the VOT continuum according to this general categorization. Nevertheless, differences between voiceless stops in Mandarin and English do exist. Since no existing studies compare the VOT patterns of these two languages, the aim of the present study is to provide a comparison of phonetically voiceless stops in Mandarin and in English, and to pinpoint the differences between their VOT patterns. ＊Department of Foreign Languages and Literature, National Cheng Kung University, 1 University Rd., Tainan, Taiwan. Telephone: (06)2757575 ext. 52231 E-mail: leemay@mail.ncku.edu.tw The author for correspondence is Li-mei Chen. [Received July 18, 2007; Revised March 13, 2008; Accepted April 15, 2008]  216  Kuan-Yi Chao and Li-Mei Chen  2. Literature Review Voicing contrast in stops has been widely discussed in phonetics and phonology. Voice Onset Time (VOT), the acoustic cue used to measure the timing of voicing, was first described by Lisker and Abramson in their well-known cross-language study of voicing in initial stops in 11 languages. According to Lisker and Abramson, voice onset time serves as a device for ‘separating the stop categories of a number of languages in which both the number and phonetic characteristics of such categories are said to differ.’ The authors also indicate that the measure of VOT is found to be highly effective in separating phonemic categories, such as voiced and voiceless, although the languages under study differ both in the number of those categories and their phonetic features. For example, in English, the minimal pair ‘pan’ and ‘ban’ can only be distinguished by voicing contrast. Thus, it can be seen that VOT plays an important role in differentiating voiced from voiceless stops, especially for lexical purposes. 2.1 VOT Definition Lisker and Abramson’s study defined VOT as ‘the time interval between the burst that marks release of the stop closure and the onset of quasi-periodicity that reflects laryngeal vibration’ ([Lisker and Abramson 1964: 422]), and used the concept to examine word-initial stops in 11 languages. Since then, a considerable number of studies of many languages has been undertaken, including a report on VOT in 51 languages [Keating et al. 1983], and another more recent study on VOT in 18 languages [Cho and Ladefoged 1999]. Although VOT is now in widespread use for measuring the timing of voicing in stops, its role as a reliable measure to distinguish between voiced and voiceless stops has been brought into question. Bohn and Flege’s [Bohn and Flege 1993] findings suggest that VOT, the acoustic parameter of voicing contrasts in word-initial stops, may not be as important to the perception of stop voicing as is commonly supposed. Docherty [Docherty 1992] argues that voice onset time focuses narrowly on the timing of voicing in word-initial stops and does not take into account stops in word-final and word-medial positions. Caramazza, Yeni-Komshian, Zurif, and Carbone [Caramazza et al. 1973] also conclude that VOT is an ‘insufficient’ cue to the voicing contrast for French-English bilinguals. On the other hand, some researchers argue that other acoustic cues play a role. For example, Klatt [Klatt 1975: 695] suggests that there are five equally important acoustic cues in English other than voice onset time, namely, low frequency energy in following vowels, burst loudness, fundamental frequency, pre-voicing, and segmental duration. Nevertheless, despite some research showing the limitations of voice onset time, it is still regarded as one of the most important acoustic parameters and has been used extensively in measuring word-initial stops.  A Cross-Linguistic Study of Voice Onset Time in Stop Consonant Productions  217  2.2 VOT Category Lisker and Abramson [Lisker and Abramson 1964: 388] examine 11 languages and classify them into three groups according to the number of stop categories each language contains. They also suggest that each stop category falls into one of three ranges, –125 to –75 ms, 0 to +25 ms, and +60 to +100 ms, respectively (p. 403). Following Lisker and Abramson’s categorization, both Mandarin and English fall into the two-category group of languages and occupy the same range along the VOT continuum, that is, 0 to +25 ms for [p, t, k] and +60 to +100 ms for [p’, t’, k’]. However, this classification is too general to note the subtle variations between the two languages. Cho and Ladefoged [Cho and Ladefoge 1999: 223] go into further detail and classify the range for voiceless aspirated and unaspirated occlusives, concentrating particularly on velar stops across 18 languages. They distinguish four categories, which they name unaspirated (velar stops with a mean VOT of around 30 ms), slightly aspirated (with a mean VOT of around 50 ms), aspirated (with a mean VOT of around 90 ms), and highly aspirated (with a mean VOT of over 90 ms). According to Cho and Ladefoged [Cho and Ladefoge 1999: 223], Mandarin and English do not occupy the same place along the VOT continuum, especially for voiceless aspirated stops. Further discussion of VOT categories for Mandarin and English stops is in Section 2.4, below. 2.3 Effect on VOT Voice onset time (VOT) is known to vary with place of articulation. The general principle is that, the further back the place of articulation, the higher the VOT values [Fischer-Jorgensen 1954; Lisker and Abramson 1964; Docherty 1992; Cho and Ladefoged 1999]. Cho and Ladefoged suggest several ways of explaining this principle, including explanations based on the laws of aerodynamics, articulator movement and differences in mass of articulators. According to the explanation based on the laws of aerodynamics, the main reason for velar stops having a longer VOT than alveolar or bilabial stops is the relative size of the supraglottal cavity behind the constriction. With the velar stop, greater air pressure builds up in the vocal tract because the supraglottal cavity becomes smaller and it takes longer for the pressure to fall at the beginning of the release phase. In accordance with Lisker and Abramson’s findings, velar stops have consistently higher VOT values than the other stops. Rochet and Fei’s [Rochet and Fei 1991] study, which examines Mandarin stops, shows the same trend — velar stops consistently show the longest VOT. They also find that, in Mandarin voiceless aspirated stops (i.e. [p’, t’, k’]), the apical stop is correlated with slightly lower values than the labial one; this does not conform to the general agreement. Vowel quality is another intrinsic effect which plays a crucial part in affecting voice  218  Kuan-Yi Chao and Li-Mei Chen  onset time. Although Lisker and Abramson claim that vocalic environment does not have a major influence on VOT, a number of reports have questioned their claim [Klatt 1975; Weismer 1979; Port and Rotunno, 1979]. Generally, it has been found that tense high vowels have longer VOTs than lax low vowels [Klatt 1975; Weismer 1979; Port and Rotunno 1979]. However, owing to language-specific variation, the correlation between voice onset time and vowel quality does not allow any definite conclusions. Rochet and Fei state briefly that the mean VOT for both sets of Mandarin stops have greater values when they are followed by a high vowel /i/ or /u/ rather than the low vowel /a/. The study provides information on the phonetic features of VOICED and VOICELESS stops in Mandarin. (In this study, the upper-case forms — ‘VOICED’ and ‘VOICELESS’ — will be used to refer to stops’ phonological status, while the lower-case forms — ‘voiced’ and ‘voiceless’ — refer to their phonetic type.) More instrumental studies are needed in order to establish more complete and reliable Mandarin VOT patterns. 2.4 Mandarin and English Stops and VOT Patterns As mentioned above, a sizable body of studies has been carried out to investigate the phonetic characteristics of voiced and voiceless stops in various languages using voice onset time as an important acoustic cue. Most existing studies concentrate on English [Lisker and Abramson 1964; Klatt 1975; Port and Rotunno 1979; Weismer 1979; Keating et al. 1983; Docherty 1992]. Other languages examined include Spanish [Lisker and Abramson 1964; Flege and Hammond 1982; Flege and Eefting 1987; Fellbaum 1996], French [Caramazza et al. 1973; Rochet et al. 1987], Arabic [Flege 1980; Khattab 2000] and Japanese [Shimizu 1990; Riney and Takagi 1999]. Among these investigations, there is very little data available on VOT for Mandarin word-initial stops. Thus, it does not appear that Mandarin VOT patterns have been examined extensively; to our knowledge, only Rochet and Fei have examined Mandarin Chinese. This study will only discuss syllable initial stops, owing to the absence of stops in any other position in Mandarin Chinese. It is known that, in word-initial position, English VOICED stops are voiced or voiceless and unaspirated, and that VOICELESS stops are voiceless and aspirated [Keating et al. 1983; Keating 1984; Docherty 1992]. Although there are two possible phonetic implementations of English VOICED stops, Keating [Keating 1984: 43] indicates that ‘English divides up the VOT continuum with some lead values but mainly short lag vs. long lag.’ In Lisker and Abramson, VOT measurements occurring before the release burst are assigned negative values and called voicing lead, while VOT measurements occurring after the release burst are assigned positive values and called voicing lag. Lisker and Abramson [Lisker and Abramson 1964: 395] also provide two sets of values for English voiced stops (VOTs with lead and with short lag) and suggest that only a single type is  A Cross-Linguistic Study of Voice Onset Time in Stop Consonant Productions  219  produced by each native speaker. Based on the distinction of Keating and Lisker and Abramson, English is described as having, in general, short lag and long lag VOT patterns.  In comparison with English, Mandarin shows less variation in implementation. All Mandarin stops are phonetically voiceless and are only differentiated by aspiration. According to data provided by Rochet and Fei, VOT duration for Mandarin [p’, t’, k’] ranges between 90 and 110 ms, while that of Mandarin [p, t, k] ranges between 10 and 25 ms (depending on the place of articulation). In Keating’s study, Mandarin and English stops are classified as phonetically the same, and both fall into short lag vs. long lag patterns; however, there remain some subtle differences between the two languages.  Table 1 shows detailed measurements of English VOT means and ranges, including  American English [adopted from Lisker and Abramson 1964] and British English [adopted  from Docherty 1992]. The VOT pattern in Mandarin Chinese [adopted from Rochet and Fei  1991] is presented in Table 2. According to Cho and Ladefoged’s definition of voiceless  unaspirated stops, English [p, t, k] fall into the ‘unaspirated’ range with a VOT of under 30 ms.  However, for voiceless aspirated stops (i.e. [p’, t’, k’]) Mandarin occupies a ‘highly aspirated’  position along the VOT continuum, while English lies in the ‘aspirated’ region. The table also  indicates that although [p’, t’, k’] in Mandarin and English fall into two categories, they are  not completely different because the VOT ranges in these two languages overlap. Table 1. VOT means and ranges in English.  Lisker and Abramson 1964 (AE)  Docherty 1992 (BE)  Mean  Range  Mean  Range  p’  58  20 -120  42  10 - 80  t’  70  30 -105  64  30 - 110  k’  80  50 -135  62  30 - 150  p  
We propose a new approach for performing phonetic transcription of text that utilizes automatic speech recognition (ASR) to help traditional grapheme-to-phoneme (G2P) techniques. This approach was applied to transcribe Chinese text into Taiwanese phonetic symbols. By augmenting the text with speech and using automatic speech recognition with a sausage searching net constructed from multiple pronunciations of text, we are able to reduce the error rate of phonetic transcription. Using a pronunciation lexicon with multiple pronunciations for each item, a transcription error rate of 12.74% was achieved. Further improvement can be achieved by adapting the pronunciation lexicon with pronunciation variation (PV) rules derived manually from corrected transcription in a speech corpus. The PV rules can be categorized into two kinds: knowledge-based and data-driven rules. By incorporating the PV rules, an error rate of 10.56% could be achieved. Although this technique was developed for Taiwanese speech, it could easily be adapted to other Chinese spoken languages or dialects. Keywords: Automatic Phonetic Transcription, Phone Recognition, Grapheme-to-Phoneme (G2P), Pronunciation Variation, Chinese Text, Taiwanese (Min-Nan), Dialect, Buddhist Sutra.  ＊ Dept. of Electrical Engineering, Chang Gung University, 259 Wen-Hwa 1st Rd., Kwei-Shan Tao-Yuan, Taiwan E-mail: minsiong@gmail.com ＋ Dept. of Computer Science and Information Engineering, Chang Gung University, 259 Wen-Hwa 1st Rd., Kwei-Shan Tao-Yuan, Taiwan E-mail: renyuan.lyu@gmail.com ＃ Institute of Statistics, National Tsing Hua University, Hsinchu, 101, Section 2 Kuang Fu Rd., 30013, Taiwan E-mail: chiang@stat.nthu.edu.tw [Received March 11, 2008; Revised August 4, 2008; Accepted August 11, 2008]  234  Min-Siong Liang et al.  1. Introduction Automatic phonetic transcription is gaining popularity in the speech processing field, especially in speech recognition, text-to-speech, and speech database construction [Haeb-Umbach et al. 1995; Wu et al. 1999; Lamel et al. 2002; Evermann et al. 2004; Nanjo et al. 2004; Nouza et al. 2004; Sarada et al. 2004; Siohan et al. 2004; Soltau et al. 2005; Kim et al. 2005]. It is traditionally performed using two different approaches: an acoustic feature input method and a text input method. The former is the speech recognition task, or more specifically, the phoneme recognition task. The latter is the grapheme-to-phoneme (G2P) task. Both tasks, including phoneme recognition and G2P remain unsolved technology problems. The state-of-the-art speaker-independent (SI) phone recognition accuracy in a large vocabulary task is currently less than 80%, far from human expectations. Although the accuracy of G2P tasks seems much better, it relies on a “perfect” pronunciation lexicon and cannot effectively deal with pronunciation variation issues. This problem becomes non-trivial when the target text is the Chinese text (漢字). The Chinese writing system is widely used in China and in East/South Asian areas including Taiwan, Singapore, and Hong Kong. Although the same Chinese character is used in different areas, the pronunciation may be very different. Therefore, they are mutually unintelligible and considered different languages rather than dialects by most linguists. In this paper, we chose Buddhist Sutra (written collections of Buddhist teachings) as the target text processed in this research. Buddhism is a major religion in Taiwan (23% of the population) [IIP 2003]. The Buddhist Sutra, translated into Chinese text in a terse ancient style (古文) , is commonly read in Taiwanese (Min-nan). Due to a lack of proper education, most people are not capable of correctly pronouncing all of the text. Besides, no qualified pronunciation lexicon exists and very few appropriately computational linguistic research projects have been conducted to support developing a G2P system. Taiwanese uses Chinese characters as a part of the written form, with its own phonetic system differing greatly from Mandarin. This is in contrast to the case of Mandarin, where the problem of multiple pronunciations (MP) is less severe. A Chinese character in Taiwanese can commonly have a classic literate pronunciation (known as Wen-du-in, or “文讀音” in Chinese) and a colloquial pronunciation (known as Bai-du-in, or “白讀音” in Chinese) [Liang et al. 2004a]. In addition to MPs, Taiwanese also have pronunciation variation (PV) due to sub-dialectical accents, such as Tainan and Taipei accents. We use the term MPs to stress the fact that variation may cause more deterioration in phonetic transcription [Cremelie et al. 1999; Hain 2005; Raux 2004]. The traditional approach to transcribing Chinese Buddhist Sutra text is human dictation. A master monk or nun reads the text aloud, sentence by sentence. Then, some phonetic experts  Data Driven Approaches to Phonetic Transcription with Integration of  235  Automatic Speech Recognition and Grapheme-to-Phoneme for Spoken Buddhist Sutra  transcribe the text manually. The manual transcription process is tedious and prone to errors. An example is given in Table 1 as follows [Chen 2006; Tripitaka et al. 2005].  Table 1. An example of Transcription of Chinese Buddhist Sutra text into Taiwanese pronunciation, with English translation. The phonetic symbols used here are IPA followed by a digit representing one of several tone classes of the Taiwanese language.  Chinese text of Buddhist Sutra  地藏菩薩本願經：如是我聞。 一時佛在忉利天，為母說法。  Transcription of Taiwanese Pronunciation  tè tsòŋ p’ò sát pún gùan kíŋ: zù sī ŋó bùn ít sĭ hút tsài tɤ̄ lì tíen, uì biɤ̂ súat hūat  English translation in meaning  Sutra of Earth Treasure: Thus I heard, once the Buddha was in Dao Li Heaven to expound the Dharma to his mother  Since more transcribed Sutras are planned, we are interested in how G2P and ASR technology can help in this situation. Owing to the fact that human experts capable of phonetically transcribing the Sutra in Taiwanese are difficult to find, the first phonetically transcribed Sutra in Taiwanese did not appear until 2004 [Sik 2004a, 2004b]. As shown in Figure 1, our task is to discover which of them is actually pronounced when the Sutra text is segmented into a series of sentences and recorded by a senior master nun. Then, the output of transcription is formed in ForPA or Tongyong Pinin [Lyu et al. 2004]. These two phonetic symbol systems are well-designed in ASCII code and suitable for any learners with common understanding of the English phonetic system. This architecture is much easier for a person to use to record his/her reading of the text than acquiring a transcribing expert. For marginalized languages with serious MPs and PV problems, this technique is very useful.  Chinese Text  Text, e.g. “如是我聞 為母說法”  (Sutra)  G2P  Waveforms, e.g.  ASR  Phonetic Transcription e.g.“ zu si ŋo bun ui biɤ suat huat”  Figure 1. The process of transcribing Chinese text into Taiwanese pronunciation using the ASR technique.  236  Min-Siong Liang et al.  In this paper, we report two experiments using speech and text data, called the Taiwanese Buddhist Sutra (TBS) corpus [Sik 2004b]. The phonetic transcription framework is described in Section 2. Given a speech corpus with phonetic transcription for training, Section 3 reports the speech recognition results with and without the corresponding text for its phonetic transcription. Section 4 discusses the second experiment involving speech recognition with the corresponding text under various pronunciation variation conditions in the training corpus. Section 5 presents our conclusions.  2. The Phonetic Transcription Augmented by Speech Recognition Technique  Figure 2 is the framework of phonetic transcription using the speech recognition technique. While the input is a speech waveform and Chinese Sutra text, the output is a phonetic transcription corresponding to the input Chinese text. The entire framework can be divided into two major parts, i.e. an acoustic part and a linguistic part.  Based on flow chart in Figure 2, we define the following notations: s is the syllable sequence, while c and o are the input character and augmented acoustic sequences. The phonetic transcription target is to find the most probable syllable sequence s* given o and c . The formula is:  s* = arg max P( s | o, c)  (1)  ∀s∈S  Where c ∈C = {c | c = c1M = c1...cM , ci ∈C} , ci is an arbitrary Chinese character, C is the set of all Chinese characters, and the number of elements in C is n(C)≈13000. s ∈ S = {s | s = s1N = s1...sN , si ∈ S}, si is an arbitrary Taiwanese syllables, S is the set of all Taiwanese syllables, and the number of elements in S is n(S)≈1000. Using the Bayes theorem:  s* = arg max P( s | c)P(o | s, c)  (2)  ∀s∈S  P(o | c)  The acoustic sequence o is assumed dependent only on the syllable sequence s . Equation 2 could be simplified as:  s* = arg max P( s | c)P(o | s)  (3)  ∀s∈S  The first term, P(s | c) , of Equation 3 is independent of o and plays the major role in the linguistic part of the recognition scheme. The second term, P(o | s) , is the probability of observation given the syllable sequence, which plays the major role in the acoustic part.  For the acoustic part, which is the probability of observing an acoustic sequence o , given a phonetic syllable sequence s , it is well known that the Hidden Markov Model (HMM) can be used to model it. We can choose a speaker independent HMM model (SI-HMM) with  Data Driven Approaches to Phonetic Transcription with Integration of  237 
One major problem in multilingual Question Answering (QA) is the integration of information obtained from different languages into one single ranked list. This paper proposes two different architectures to overcome this problem. The first one performs the information merging at passage level, whereas the second does it at answer level. In both cases, we applied a set of traditional merging strategies from cross-lingual information retrieval. Experimental results evidence the appropriateness of these merging strategies for the task of multilingual QA, as well as the advantages of multilingual QA over the traditional monolingual approach. Keywords: Multilingual Question Answering, Cross-Lingual Information Retrieval, Information Merging. 1. Introduction Question Answering (QA) has become a promising research field whose aim is to provide more natural access to textual information than traditional document retrieval techniques [Laurent et al. 2006]. In essence, a QA system is a kind of search engine that responds to natural language questions with concise and precise answers. For instance, given the question “Where is the Popocatepetl Volcano located?”, a QA system has to respond “Mexico”, instead of returning a list of related documents to the volcano.  ∗ Laboratory of Language Technologies, National Institute of Astrophysics, Optics and Electronics (INAOE). Luis Enrique Erro #1, Sta. María Tonantzintla, Puebla, Mexico. Tel.: +52-222-2663100 ext: 8218 Fax: +52-222-2663152. The author for correspondence is Manuel Montes-y-Gómez. Email: mmontesg@inaoep.mx + Department of Computer Science, University of Jaén. Campus Las Lagunillas s/n, Edif D3, Jaén, Spain [Received June 30, 2007; Revised October 8, 2007; Accepted January 12, 2008]  28  Rita M. Aceves-Pérez et al.  At present, due to the internet explosion and the existence of several multicultural communities, one of the major challenges to face this kind of system is multilinguality. In a multilingual scenario, it is expected that QA systems will be able to: (i) answer questions formulated in several languages, and (ii) look for answers in a number of collections in different languages. There are two recognizable kinds of QA systems that allow management of information in various languages: cross-lingual QA systems and, strictly speaking, multilingual QA systems. The former addresses a situation where questions are formulated in a different language from that of the (single) document collection. The other, in contrast, performs the search over two or more document collections in different languages. It is important to mention that both kinds of systems have some advantages over standard monolingual QA. They mainly allow users to access more information in an easier and faster way than monolingual systems. However, they also introduce additional issues due to the language barrier. Generally speaking, a multilingual QA system can be described as an ensemble of several monolingual systems, where each one works on a different – monolingual – document collection. Under this schema, two additional tasks are required: first, the translation of incoming questions into all target languages, and second, the combination of relevant information extracted from different languages. The first problem, namely, the translation of questions from one language to another, has been widely studied in the context of cross-language QA [Aceves-Pérez et al. 2007; Neumann et al. 2005; Rosso et al. 2007; Sutcliffe et al. 2005]. In contrast, the second task, the merging of information obtained from different languages, has not been specifically addressed in QA. Nevertheless, it is important to mention that there is significant work on combining capacities from several monolingual QA systems [Chu-Carroll et al. 2003; Ahn et al. 2004; Sangoi-Pizzato et al. 2005], as well as on merging multilingual lists of documents for cross-lingual information retrieval applications [Lin et al. 2002; Savoy et al. 2004]. In line with these previous works, in this paper we propose two different architectures for multilingual question answering. These architectures differ from each other by the way they handle the combination of multilingual information. Mainly, they take advantage of the pipeline architecture of monolingual QA systems (which includes three main modules, one for question classification, one for passage retrieval, and one for answer extraction) to achieve this combination at two different stages: after the passage retrieval module by mixing together the sets of recovered passages, or after the answer extraction module by directly combining all extracted answers. In other words, our first architecture performs the combination at passage level, whereas the second approach does it at answer level. In both cases, we applied a set of  Two Approaches for Multilingual Question Answering:  29  Merging Passages vs. Merging Answers  well-known strategies for information merging from cross-lingual information retrieval, specifically, Round Robin, Raw Score Value (RSV), CombSUM, and CombMNZ [Lee et al. 1997; Lin et al. 2002; Savoy et al. 2004]. The contributions of this paper are two-fold. On the one hand, it represents – to our knowledge – the first attempt for doing “multilingual” QA. In particular, it proposes and compares two initial solutions to the problem of multilingual information merging in QA. In addition, this paper also provides some insights on the use of traditional ranking strategies from cross-language information retrieval into the context of multilingual QA. The rest of the paper is organized as follows. Section 2 describes some previous works on information merging. Section 3 presents the proposed architectures for multilingual QA. Section 4 describes the procedures for passage and answer merging. Section 5 shows some experimental results. Finally, section 6 presents our conclusions and outlines future work. 2. Related Work As we previously mentioned, a multilingual QA system has to consider, in addition to the traditional modules for monolingual QA, stages for question translation and information merging. The problem of question translation has already been widely studied. Most current approaches rest on the idea of combining capacities of several translation machines. They mainly consider the selection of the best instance from a given set of translations [Aceves-Pérez et al. 2007; Rosso et al. 2007] as well as the construction of a new question reformulation by gathering terms from all of them [Neumann et al. 2005; Sutcliffe et al. 2005; Aceves-Pérez et al. 2007]. On the other hand, the problem of information merging in multilingual QA has not yet been addressed. However, there is some relevant related work on constructing ensembles of monolingual QA systems. For instance, [Ahn et al. 2004] proposes a method that performs a number of sequential searches over different document collections. At each iteration, this method filters out or confirms the answers found in the previous step. Chu-Carroll et al. [2003] describes a method that applies a general ranking over the five-top answers obtained from different collections. They use a ranking function that is inspired in the well-known RSV technique from cross-language information retrieval. Finally, Sangoi-Pizzato et al. [2005] uses various search engines in order to extract from the Web a set of candidate answers for a given question. It also applies a general ranking over the extracted answers; nevertheless, in this case the ranking function is based on the confidence of search engines instead that on the redundancy of individual answers. Our proposal mainly differs from previous methods in that it not only considers the  30  Rita M. Aceves-Pérez et al.  Question Language x  Question Language x  Translation Module Question Language y  Translation Module Question Language z  Passage Retrieval  Collection Language x  Relevant Passages Language x  Passage Retrieval  Collection Language y  Relevant Passages Language y  Passage Retrieval  Collection Language z  Relevant Passages Language z  Passage Merging Combined Passages Answer Extraction Answer Figure 1. Multilingual QA based on passage merging integration of answers but also takes into account the combination of passages. That is, it also proposes a method that carries out the information merging at an internal stage of the QA process. The proposed merging approach is similar in spirit to Chu-Carroll et al. [2003] and Sangoi-Pizzato et al. [2005] in that it also applies a general ranking over the information extracted from different languages. Like Chu-Carroll et al. [2003], it uses the RSV ranking function, although it also applies other traditional ranking strategies such as Round Robin, CombSUM and CombMNZ. 3. Two Architectures for Multilingual QA The traditional architecture of a monolingual QA system considers three basic modules: (i) question classification, where the type of expected answer is determined; (ii) passage retrieval, where the passages with the greatest probability to contain the answer are obtained from the target document collection; and (iii) answer extraction, where candidate answers are ranked and the final answer recommendation of the system is produced. In addition, a multilingual QA system must include two other modules, one for question translation and another for  Two Approaches for Multilingual Question Answering:  31  Merging Passages vs. Merging Answers  Question Language x  Question Language x  Translation Module Question Language y  Translation Module Question Language z  Passage Retrieval  Collection Language x  Relevant Passages Language x  Passage Retrieval  Collection Language y  Relevant Passages Language y  Answer Extraction  Answer Extraction  Candidate Answers Language x  Candidate Answers Language y  Passage Retrieval  Collection Language z  Relevant Passages Language z  Answer Extraction Candidate Answers Language z  Answer Merging Answer Figure 2. Multilingual QA based on answer merging information merging. The purpose of the first module is to translate the input question to all target languages, whereas the second module is intended to integrate the information extracted from these languages into one single ranked list. Figures 1 and 2 show two different architectures for multilingual QA. For the sake of simplicity, in both cases, we do not consider the module for question classification. On the one hand, Figure 1 shows a multilingual QA architecture that does the information merging at passage level. The idea of this approach is to perform in parallel the recovery of relevant passages from all collections (i.e., from all different languages), then integrate these passages into one single ranked list, and then extract the answer from the combined set of passages. On the contrary, Figure 2 illustrates an architecture that achieves the information merging at answer level. In this case, the idea is to perform the complete QA process independently in all languages, and, after that, integrate the sets of answers into one single ranked list. It is important to mention that merging processes normally rely on the translation of information to a common language. This translation is required for some merging strategies in order to be able to compare and rank the passages and answers extracted from different  32  Rita M. Aceves-Pérez et al.  languages. The two proposed architectures have different advantages and disadvantages. For instance, doing the information merging at passage level commonly allows obtaining better translations for named entities (possible answers) since they are immersed in an extended context. On the other hand, doing the merging at answer level has the advantage of a clear (unambiguous) comparison of the multilingual information. In other words, comparing two answers (named entities) is a straightforward step, whereas comparing two passages requires the definition of a similarity measure and the determination of a criterion about how similar two different passages should be in order to be considered as equal. This previous problem is not present in monolingual QA ensembles, since in that case all individual QA systems search on the same document collection. The following section introduces some of the most popular information merging strategies used in the task of cross-lingual information retrieval. It also describes the way these strategies are used within the proposed architectures for integrating passages and answers. 4. Merging Passages and Answers 4.1 Merging Strategies Integrating information retrieved from different document collections or by different search engines is a longstanding problem in information retrieval. Researchers in this field have proposed several strategies for information merging; traditional ones are: Round Robin, RSV (Raw Score Value), CombSUM, and CombMNZ [Lee et al. 1997; Lin et al. 2002]. However, more sophisticated strategies have been proposed recently, such as the 2-step RSV [Martínez-Santiago et al. 2006], and the Z-score value [Savoy et al. 2004]. In this work, we mainly study the application of traditional merging strategies in the context of multilingual QA. The following paragraphs give a brief description of these strategies. Round Robin. The retrieved information (in this case, passages or answers) from different languages is interleaved according to its original monolingual rank. In other words, this strategy takes one result in turn from each individual list and alternates them in order to construct the final merged output. The hypothesis underlying this strategy is the homogeneous distribution of relevant information across all languages. In our particular case, as described in Table 1, this restriction was fulfilled for almost 60% of test questions. Raw Score Value (RSV). This strategy sorts all results (passages or answers) by their original score computed independently from each monolingual collection. Differing from Round Robin, this approach is based on the assumption that scores across different collections are comparable. Therefore, this method tends to work well when different collections are  Two Approaches for Multilingual Question Answering:  33  Merging Passages vs. Merging Answers  searched by the same or very similar methods. In our experiments (refer to Section 5), this condition was fully satisfied since it was applied the same QA system for all languages. CombSUM. In this strategy, the result scores from each language are initially (min-max) normalized. Afterward, the scores of duplicated results occurring in multiple collections are summed. In particular, we considered the implementation proposed by Lee et al. [1997]: we assigned a score of 21-i to the i-th ranked result from the top 20 of each language, this way, the top passage or answer was scored 20, the second one was scored 19, and so on. Any result not ranked in the top 20 was scored as 0. Finally, we added scores of duplicated results for all different monolingual runs and ranked these results in accordance to their new joint score. For instance, if an answer is ranked 3rd for one language, 10th for other one, and does not exist in a third language, then its score is (21-3) + (21-10) + 0 = 29. CombMNZ. It is based on the same normalization as CombSUM, but also attempts to account for the value of multiple evidence by multiplying the sum of the scores (CombSUM-value) of a result by the number of monolingual collections in which it occurs. Therefore, it can be said that CombSUM is equivalent to averaging, whereas CombMNZ is equivalent to weighted averaging. Using the same example as for the CombSUM strategy, the answer’s score is in this case 2 × ((21-3) + (21-10) + 0) = 58. It is important to point out that Round Robin and RSV strategies take advantage of the complementarity among collections (when answers are extracted from only one language), whereas ComSUM and CombMNZ also take into account the redundancies of answers (the repeated occurrence of an answer in several languages).  4.2 Merging Procedures Given several sets of relevant passages obtained from different languages, the procedure for passage merging considers the following two basic steps: 1. Translate all passages into one common language. This translation can be done by means of any translation method or online translation machine. However, we suggest translating all passages into the original question’s language in order to avoid translation errors in at least one passage set. It is important to clarify that translation is only required by the CombSUM and CombMNZ strategies. Nevertheless, all passages should be translated to one common language before entering the answer extraction module. 2. Combine the sets of passages according to a selected merging strategy. In the case of using the Round Robin or RSV approaches, the combination of passages is straightforward. In contrast, when applying CombSUM or CombMNZ, it is necessary to determine the occurrence of a given passage in two or more collections. Given that it is practically  34  Rita M. Aceves-Pérez et al.  impossible to obtain exactly the same passage from two different collections, it is necessary to define a criterion about how similar two different passages should be in order to be considered as equal. In particular, we measure the similarity of two passages by the Jaccard function (calculated as the cardinality of their vocabulary intersection divided by the cardinality of their vocabulary union) and consider them as equal only if their similarity is greater than a given specified threshold (empirically, we set the threshold value to 0.5). The procedure for answer merging is practically the same as that for passage merging. It also includes one step for answer translation and another step for answer combination. However, the combination of answers is much simpler than the combination of passages, since they are directly comparable. In this case, the application of all merging strategies is straightforward. 5. Evaluation  5.1 Experimental Setup  The following paragraphs describe the data and tools used in the experiments.  Languages. We considered three different languages: Spanish, Italian, and French.  Search Collections. We used the document sets from the QA@CLEF evaluation forum. In particular, the Spanish collection consists of 454,045 news documents, the Italian set has 157,558, and the French one contains 129,806.  Test questions. We selected a subset of 170 factoid questions from the MultiEight corpus of CLEF. From all these questions at least one monolingual QA system could extract the correct answer. Table 1 shows answer’s distributions across all languages.  Table 1. Distribution of questions by source language  Answers in:  SP FR IT SP, FR SP, IT FR, IT SP, FR, IT  Questions 37 21 15 20  25  23  29  Percentage 21% 12% 9% 12% 15% 14%  17%  It is important to note that this set of questions covers all types of currently-evaluated factoid questions; therefore, it is possible to formulate some accurate conclusions about the appropriateness of the proposed architectures. Monolingual QA System. We used the passage retrieval and answer extraction components of the TOVA question answering system [Montes-y-Gómez et al. 2005]. Its selection was mainly supported by its competence in dealing with all the considered languages. Indeed, it obtained the best precision rate for Italian and the second best for both Spanish and  Two Approaches for Multilingual Question Answering:  35  Merging Passages vs. Merging Answers  French in the CLEF-2005 evaluation exercise. Translation Machine. The translation of passages and answers was done using the Systran online translation machine (www.systranbox.com). On the other hand, questions were manually translated in order to avoid mistakes at early stages and therefore focus the evaluation on the merging phase. Merging strategies. As we mentioned in the previous section, we applied four traditional merging strategies, namely, Round Robin, RSV, CombSUM, and CombMNZ. Evaluation Measure. In all experiments, we used the precision as the evaluation measure. It indicates the general proportion of correctly answered questions. In order to enhance the analysis of results, we show the precision at one, three, and five positions. Baseline. We decided to use the results from the best monolingual system (the Spanish system in this case) as a baseline. In this way, it is possible to reach conclusions about the advantages of multilingual QA over the standard monolingual approach.  5.2 Experimental Results The objectives of the experiments were twofold: first, to compare the performance of both architectures; and second, to study the applicability and usefulness of traditional merging strategies in the problem of multilingual QA. Additionally, these experiments allowed us to analyze the advantages of multilingual QA over the traditional monolingual approach. The first experiment considered information merging at passage level. In this case, the passages obtained from different languages were combined, and the 20 top-ranked were delivered to the answer extraction module. Table 2 shows the precision results obtained using all merging strategies as well as the precision rates of the best monolingual run. From Table 2, it is clear that merging strategies relying on the complementarity of information (such as Round Robin and RSV) obtain better results than those also considering its redundancy (e.g. CombSUM and CombMNZ). We hypothesize that this behavior was mainly produced by three different factors: (i) the impact of translation errors on the CombSUM and CombMNZ strategies1; (ii) the complexity of assessing the redundancy of passages, i.e., the complexity of correctly deciding whether two different passages should be considered as equal; and (iii) the large number of questions (42%) that have an answer in just one language.  
Many Web news portals have provided clustered news categories for readers to browse many related news articles. However, to the best of our knowledge, they only provide monolingual services. For readers who want to find related news articles in different languages, the search process is very cumbersome. In this paper, we propose a cross-lingual news group recommendation framework using the cross-training technique to help readers find related cross-lingual news groups. The framework is studied with different implementations of SVM and Maximum Entropy models. We have conducted several experiments with news articles from Google News as the experimental data sets. From the experimental results, we find that the proposed cross-training framework can achieve accuracy improvement in most cases. Keywords: Cross-Lingual News Group Mapping, Cross-Training, Semantic Overlapping, Mapping Recommendation 1. Introduction As the Web becomes an abundant source of news information, it also becomes an important medium for people to learn recent tidings. To provide readers a convenient way of viewing a news event described by different news agencies, many Web news portals, such as AltaVista News and Google News, cluster news articles according to their relevance with consistent user interfaces. With such news clustering services, readers could easily acquire more details of an interesting news event from numerous reports. Ideally, they can simply click through an entry link to browse many related news reports without need of a cumbersome searching procedure. Nevertheless, if the news event is originally reported by foreign news agencies, the readers usually find that there are only few translated news articles and can only acquire an overview  * Dept. of Computer Sci. and Eng., Yuan Ze University, 135 Yuan-Tung Rd., Chungli, 320, Taiwan. Tel.: +886-3-4638800 ext: 2361 Fax: +886-3-4638850. E-mail: {czyang,sean,pjwu}@syslab.cse.yzu.edu.tw [Received June 30, 2007; Revised October 8, 2007; Accepted January 12, 2008]  42  Cheng-Zen Yang et al.  of the news event. If they want to find more related foreign news stories, they may generally get frustrated due to the following two reasons. First, the translated news articles seldom provide as much information as the original news articles. Second, the translation may add more interpretations that can mislead in the searching direction. The following example illustrates these situations. This news story, reported in BBC News [2006], is a good example to show these problems. The title of its English version is “First impressions count for web” and the article contains 15 paragraphs mainly focused on the impressions in a 20th of a second after first sight [BBC News 2006]. However, the title of its Chinese news story is “好網頁還需要讓讀 者一見鍾情” and may be translated into “Good web pages need to let readers fall in love at first sight”, which includes additional semantic information related to love. In addition, the Chinese news article has only 7 paragraphs. When readers read the Chinese news article (the source document) and want to find more information from (for example) English news articles (target documents), they will most likely search for the news article entitled with “fall in love at first sight” and find nothing related. Apparently, the readers cannot easily find the English news article. Additionally, the amount of information of the source news article may not be equal to that of the corresponding target news article. In this example, the amount of information of the translated Chinese article is much less than that of the original English article. The scant amount of translated information will perplex the readers in other searching operations. These observations suggest the need of a cross-lingual news recommendation framework for readers to get a broader view to a news event. To address the recommendation issue for cross-lingual news groups, the simplest approach is to directly translate the source news article and find the related news group in another language. Unfortunately, the quality of translation and the amount of news information highly influence the recommendation results. Readers may get translated results of poor quality. For instance, using Google Translation (http://www.google.com/translate_t?hl=zh-TW) to translate the Chinese news title of the above example gets “Readers need to make a good website was love at first sight”. As many Web news portals have provided monolingual cluster-based news browsing interfaces, the quality of cross-lingual news group recommendation can be improved if the cluster information of the source documents is exploited. Such exploration of cluster information has been studied recently in many applications, such as Web catalog integration [Agrawal and Srikan 2001; Tsay et al. 2003; Sarawagi et al. 2003; Zhang and Lee 2004a; Zhang and Lee 2004b; Chen 2005] and title generation [Tseng et al. 2006]. In this paper, we propose a cross-lingual news group recommendation framework using the cross-training approach from recent Web taxonomy integration techniques [Sarawagi et al. 2003] to find the possible semantic corresponding relationships between news groups of  Cross-Lingual News Group Recommendation Using  43  Cluster-Based Cross-Training  different languages. With the cross-training approach, the framework explores the implicit clustering information from the source news groups and the target news groups by learning the group features alternately. Then, the framework utilizes the implicit clustering information to improve the mapping accuracy between news groups of different languages. Such a framework has two major advantages. First, it will save considerable news searching effort resulting from the cumbersome searching procedure in which readers need to query different monolingual news portals in a trial-and-error manner. Second, it mitigates the translation inaccuracy to provide readers a broader panorama of news events from different aspects. The cross-training framework has been implemented in Support Vector Machines (SVM) and Maximum Entropy (ME) classifiers. We have also conducted experiments to investigate the accuracy improvement of the cross-training approach with a 21-day data set containing English and Chinese news articles collected from Google News. In the experiments, we measured the accuracy performance for different approaches. The experimental results show that the cross-training approach can benefit the mapping accuracy in most cases. The rest of the paper is organized as follows. In Section 2, we present the problem definitions and briefly review previous related research on Web catalog integration. Section 3 elaborates the proposed cross-training framework. Section 4 describes our experiments in which English news and Chinese news articles from Google News were used as the data sets. Section 5 concludes the paper and discusses future directions. 2. Problem Statement and Related Research For the recommendation problem of clustered news groups in different languages, we assume that the recommendation process deals with two Web news catalogs in two different languages to find the best semantically correlated relationships between the two news catalogs. We also assume that readers browse one news catalog and want to find related news articles in another news catalog of another language for the sake of simplicity. The catalog browsed by readers is the source S in which the news articles (source documents) are written in language Ls and have been classified into m event clusters S1, S2,…, Sm. The other is the target catalog T in which the news articles (target documents) are written in language Lt and have been also classified into n clusters T1, T2,…, Tn. The terms of the documents of each cluster comprise the feature space of the corresponding news event. In the recommendation process, therefore, the objective of the framework is to discover all possible cluster-to-cluster mapping relationships between S and T, and report these relationships to the readers for recommendation. For the sake of simplicity in discussion, we only consider the best mapping relationships in this paper, i.e., given a source catalog Si, the  44  Cheng-Zen Yang et al.  best corresponding target catalog Tj (Si→Tj) is identified in this study. Ideally, if both news clusters Si and Tj focus on the same news event, the news articles in both clusters should have semantic overlap, as shown in Figure 1. Generally, the mapping relationships are one-to-one and symmetric. However, in our observations, one-to-many situations indeed have occurred because more than one target cluster is overlapped by the same source cluster. Furthermore, source documents will be translated in Lt first, and the quality of the feature space of the translated source documents may be hindered due to the poor translation process. These factors may make the symmetric relationships asymmetric. Therefore, the reverse mappings (Tj→ Si) are separately considered. Generally, the cluster-to-cluster mapping discovery problem can be viewed as a generalization of the Web catalog integration problem on a coarse-grained basis. In the Web catalog integration problem, the objective of the integration process is to classify the documents in the source catalog into the target catalog with the enhancement of the implicit source information. In recent years, there have been many approaches proposed for the general catalog integration problem. For example, the Naïve Bayes approaches [Agrawal and Srikan 2001; Tsay et al. 2003], the SVM-based approaches [Sarawagi et al. 2003; Zhang and Lee 2004a; Zhang and Lee 2004b; Chen 2005], and the Maximum Entropy approach [Wu et al. 2005] have shown that the integration improvement can be effectively achieved.  Feature space of the source news cluster in Ls  Feature space of the target news cluster in Lt  Semantic overlapped feature space  News articles in Ls  News articles in Lt  The same news event  Figure 1. The relation of the news event and the correspondent news clusters in Ls and Lt.  Cross-Lingual News Group Recommendation Using  45  Cluster-Based Cross-Training  Some enhancement approaches, however, may not be suitable for the cross-lingual cluster-to-cluster mapping discovery problem. For example, the topic restriction approach proposed in Tsay et al. [2003] requires that the testing target clusters are the clusters containing common documents from the source cluster. Nonetheless, in the cross-lingual cluster-to-cluster mapping discovery problem, there cannot be such a common subset. The enhanced Naïve Bayes (ENB) approach proposed in [Agrawal and Srikan 2001] exploits the implicit source catalog information to enhance the integration accuracy performance. However, due to the diversity of news articles and the translation variety, the iterative algorithm may introduce many false-positive mappings to twist the overlapped space into a larger one. The shrinkage approach adopted in Wu et al. [2005] also needs to be adapted because the news clusters are usually not hierarchically organized. Our recommendation framework uses the cross-training approach adapted from the cross-training (CT) approach proposed in [Sarawagi et al. 2003]. The CT approach is a semi-supervised learning strategy. The idea behind CT is that a better classifier can be built with the assistance of another catalog that has semantic overlap. The overlapped document set is fully-labeled and partitioned into a development set and a test set where the development set is used to tune the system performance and the test set is used to evaluate the system. Through the cross-training process, the implicit information in the source taxonomy is learnt, and more source documents can be accurately integrated into the target taxonomy. The proposed framework utilizes the CT approach to first obtain the potential mapping relationships from the reverse mappings (Tj→ Si) through a learning process. The extracted information then is used to augment the feature space in the next learning phase. Finally, the mappings from Si to Tj are explored in a classification process. 3. Cross-Training for Mapping Discovery The main design principle of the cross-training framework is that the implicit mapping relationships are extracted through the first learning phase on reverse mappings. In this phase, the strength of each possible mapping is identified and ranked. For each Si, the framework can find the most possibly corresponding Tj. Before the second learning phase, the feature space of each Tj is expanded with the discovered mapping information. Then, the augmented classifiers are used to identify the mapping relationships from Si to Tj, and give the recommendations. 3.1 The Processing Flow Figure 2 depicts the processing flow in the cross-training framework. Without loss of generality, we use English and Chinese here as two language representatives for Ls and Lt to explain our bilingual recommendation process in this paper.  46  Cheng-Zen Yang et al.  English News Integration Portal English News Categories Document Parsing and Preprocessing Terms in the English News Categories Documents Translation Translated Terms in the English News Categories  Documents Transformation Cross-Training Approach Documents Transformation Cross-Training Approach  Document Parsing and Preprocessing Terms in the Chinese News Categories Documents Translation Translated Terms in the Chinese News Categories Chinese-toEnglish Results English-toChinese Results  Chinese News Integration Portal Chinese News Categories  Figure 2. The processing flow for bilingual news group recommendation in the cross-training framework. In the framework, the classification system first retrieves English and Chinese news articles from news portals, say Google News or Yahoo! News. These news articles have been usually clustered well in the news portals. The framework then performs parsing and preprocessing on each news cluster to get its feature space. The preprocessor parses the Web news, and eliminates stopwords [Fox 1992] and HTML tags. After the preprocessing, the source news groups are translated into the target language. For example, if a reader wants to find the possible English news groups for a designated Chinese news group, the English news articles are in the source news groups and will be translated into Chinese. After the translation process, all the source and target news groups are prepared as the data sets for further cross-training operations.  Cross-Lingual News Group Recommendation Using  47  Cluster-Based Cross-Training  A debate may arise about whether the framework should re-cluster the news articles after the translation process. Since the translation process may introduce semantic variety into the news clusters, re-clustering the news articles may produce clusters with better semantic integrity for the following recommendation process. Nonetheless, the observations in Chen et al. [2003] show that the re-clustering process can contrarily reduce the quality of the original semantic integrity. Therefore, the proposed framework will not re-cluster the news articles. 3.2 Parsing and Preprocessing As each Web news article is composed of plain text and HTML tags, it needs to be parsed first to extract useful information. For simplicity sake, the document parsing procedure is currently designed in a conservative manner by ignoring the HTML tags and extracting only the plain text. Both Chinese and English news articles are then preprocessed. There are four steps for English news articles: (1) tokenization, (2) stopword removal, (3) stemming, and (4) generation of term-frequency vectors. As there is no word boundary in Chinese sentences, the Chinese articles need to be segmented first [Nie and Ren 1999; Nie et al. 2000; Foo and Li 2004]. We use a hybrid approach proposed by Tseng [2002], which can achieve a high precision rate and a considerably good recall rate by considering unknown words. The hybrid approach combines the longest match dictionary-based segmentation method and a statistical-based approach which is a fast keyword/key-phrase extraction algorithm. With this hybrid approach, each sentence is scanned sequentially and the longest matched words based on the dictionary entries are extracted. This process is repeated until all characters are scanned. 3.3 Translation and Transformation After preprocessing, the Chinese and English news articles in each category are tokenized. Then, the Chinese news documents are translated. The translation can be based on a bilingual dictionary or a well-trained machine translation system. In the translation, we adopt a straightforward word expansion method. Each Chinese word is simply translated to a set of English terms listed in a bilingual dictionary or derived from a machine translation system. The same procedure is also applied to the English news articles. Currently, the translation process does not consider the word choice disambiguation problem when there are several candidates for each word. The translation quality is not further addressed using different translation technologies. Nonetheless, it can be found that the proposed cross-training approach achieves around 90% accuracy performance in top-1 ranking.  48  Cheng-Zen Yang et al.  Source News Features (English) training  1st Learning  Target News Features (Chinese) testing  testing  Semantically Overlapped Features training 2nd Learning  training  English-to-Chinese News Cluster Mappings Figure 3. The basic concept of the cross-training process. Finally, each news article is converted to a feature vector. For each index term in the feature vector, a weight is associated with the term to express the importance. In the current design, the weight of each term is calculated by TFx ∑TFi , where i denotes the number of the stemmed terms in each news article. 3.4 The Cross-Training Process Previous studies on the general Web catalog integration problem show that, if a source document can be integrated into a target category, there must be a sufficiently large semantic overlap between them [Agrawal and Srikan 2001; Sarawagi et al. 2003; Tsay et al. 2003; Zhang and Lee 2004a; Zhang and Lee 2004b; Wu et al. 2005; Yang 2006]. For the cluster-to-cluster mapping discovery problem, this observation is also an important basis. If an English news category can be associated with a Chinese news category, this mapping must be concluded from a situation in which the semantically overlapped feature space is sufficiently large. 3.4.1 Learning to Extract the Implicit Information The cross-training process is incorporated mainly for exploring the overlapped feature space. Figure 3 illustrates a cross-training process in which there are two learning phases. In the first phase, the source news clusters are used as the training data sets to train m classifiers, and the target news clusters are used as the testing data sets to extract the implicit mapping  Cross-Lingual News Group Recommendation Using  49  Cluster-Based Cross-Training  information. The m classifiers then calculate the mapping scores (Scij) for n target news clusters to predict the strengths of the semantic overlaps.  Since SVM and ME are studied in the framework implementations, the mapping score Scij of Tj→Si can be defined as either the ratio at which the target documents in Tj are classified into the source news cluster Si or the average weight derived from the classifier. For example, if the classification scheme used in the framework is SVM, the mapping score Scij can be calculated by either Eq. (1) where NTj is the news documents of the target cluster Tj or Eq. (2) which is the average of the distance from each document to the hyperplane. This  average can be viewed as the discriminative characteristic of all documents to the classifier.  Scij  =  # of  NTj classified # of NTj  in  Si  (1)  Scij  =  ∑ wi xi + b # of NTj  (2)  Basically, Equation (1) represents a voting scheme in which the predicted rank of a target cluster Tj depends on the number of the positively classified news articles in Tj. Equation (2) represents a weighting scheme in which the predicted rank of Tj depends on the average of the total distance to the hyperplane. For each source cluster, the target cluster with the highest mapping score is qualified as the potential candidate that may have the accurate Si→Tj mapping relationship in the second learning phase. The reason the mapping scores are considered in an asymmetric way is that the cross-training approach will adjust the feature vectors back and forth in each learning iteration. Other mapping discovery approaches may provide efficient schemes to consider both mapping scores of Si→Tj and Tj→ Si as an integrated scoring method. This has been left for our future study.  3.4.2 Learning to Find the Corresponding Mappings The implicit information explored in the first learning phase is then used as the prediction information in the second learning phase. The cross training process can be continued until the results converge. The category information of the corresponding source cluster, say Si, for the previously discovered candidate target cluster, say Tj, is inserted into the feature space of Tj. The category information can be category identifiers or the category title words. For example, we used identifiers starting from 1000001 to 1000040 for categories in the current experiments. Figure 4 depicts the detailed process of concatenating the predicted implicit information to the ordinary feature vectors of the target cluster in the cross-training approach. In the figure, FT is a feature vector for the term features of the target news articles, LT is a feature vector for the label features (category information) of the target cluster, and the test output contains the  50  Cheng-Zen Yang et al.  Target Language News Categories  Source Language News Catagories  Training  Test  1st-phase  Classifier  FT  Test Output LT  Training  2nd-phase  Test  Classifier  Cross-training Result Figure 4. Adding the predicted implicit information in the cross-training process. label features of the predicted source clusters. With the predicted mapping information, the discriminative power of the classifiers of the second phase can be enhanced. For controlling the discriminative power of the added semantically-overlapped implicit label information, as in Sarawagi et al. [2003], the ordinary feature weights in the augmented target vectors are scaled by a factor of f, and the weight of each label attribute by a factor of 1 − f. The parameter f is used to decide the relative weights of the label and term features and can be tuned for different application environments. In the current experiments, the results show that the best f value ranges from 0.02 to 0.05. The small f values show that the augmented information should not be overemphasized in the cross-training process. This observation for factoring is consistent with previous studies [Sarawagi et al. 2003; Chen et al. 2004]. Finally, the second-phase classifiers are trained with the augmented target vectors. The recommended source news groups of the target news groups are calculated using the same mapping scoring method.  Cross-Lingual News Group Recommendation Using  51  Cluster-Based Cross-Training  4. Experiments We have implemented the cross-training framework in SVM and ME classifiers. To rank the predictive corresponding target clusters, we implemented the voting scheme in the cross-training framework of SVM (SVM-VCT) and ME (ME-VCT), and the weighting scheme with SVM (SVM-WCT). As stated in Section 3.4.1, Equation (1) was used to rank the target clusters in SVM-VCT and ME-VCT. Equation (2) was used in the weighting scheme SVM-WCT. We also implemented the voting scheme and the weighting scheme in SVM (SVM-V and SVM-W) for comparison. In the experiments, an English news catalog and a Chinese news catalog from Google News were used as the representatives to demonstrate the classification performance of the proposed cross-training framework. We measured the accuracy performance at top-1, top-3, and top-5 ranks. The details of the experiments are presented as follows.  4.1 The Experimental Environment The framework is currently implemented in Java. The segmentation corpus is based on the Academia Sinica Bilingual Wordnet 1.0 published by the Association for Computational Linguistics and Chinese Language Processing (ACLCLP) [Sinica BOW 2005]. We used SVMlight (version 5.00) [Joachims 2002] as the SVM tool with a linear kernel, and the maximum-entropy toolkit (version 20041229) [Zhang 2004] as the Maximum Entropy model kernel. The bilingual word lists published by Linguistic Data Consortium (LDC) were used as the bilingual dictionaries. The Chinese-to-English dictionary ver. 2 (ldc2ce) has about 120,000 records, and the English-to-Chinese dictionary (ldc2ec) has about 110,000 records. In ldc2ce and ldc2ec, each entry is composed of a single word and several translated words separated by slashes without any indication of the importance. Therefore, the translated words are treated equally in our experiments. In the translation, each word in the source document was replaced with these translated words. The translation quality issue is not addressed in depth because we want to follow the normal reader behaviors. Furthermore, the implicit semantic information embedded in each category of news articles may mitigate the poorness of the translation.  4.2 Data Sets In our experiments, two news portals were chosen as the bilingual news sources: Google News U.S. version for English news and Google News Taiwan version for Chinese news. Both the Chinese and English news articles were retrieved from the world news category from May 10 to May 23, 2005 and from October 21 to October 27, 2007. The experiments were performed on the data set of each day. Twenty news categories were collected per day. All the English  52  Cheng-Zen Yang et al.  news articles were translated into Chinese with the bilingual dictionaries. The size of the English-to-Chinese data set is 454.5 Mbytes. All the Chinese news articles were also translated. The size of the Chinese-to-English data set is 341.5 Mbytes. The 21-day data sets contain 36,548 English news articles and 8,224 Chinese news articles. In the experiments, the mapping relations between the Chinese and English news reports were first identified by three graduate students manually and independently. The mapping between an English news category and a Chinese news category is recognized if at least two students have the same mapping identification. These manually-identified mapping relations were used to evaluate the accuracy performance of the bilingual classification systems. We found that there were 122 identified mappings in the Chinese-to-English recommendation task and 123 identified mappings in the English-to-Chinese recommendation task. The difference existed because an English category was identified that was to be mapped to two Chinese categories. The data sets collected currently cannot significantly reveal the influences of one-to-many situations. In our future work plan, more news categories need to be collected to verify our scheme for one-to-many cases. The experiments were conducted in two ways: finding the related Chinese news groups from the English news groups (Chinese-to-English) and finding the related English news groups from the Chinese news groups (English-to-Chinese). Here, we take the Chinese-to-English recommendation process as the example to present the experimental details. The English-to-Chinese recommendation process was conducted in a similar manner. In the Chinese-to-English experiments, each Chinese news catalog was first used as the training set in the first learning phase. To find a corresponding Chinese category (Si) of an English target category (Tj ), the news articles in Si were all used as the positive training examples, and the news articles in the other Chinese news categories (Sk, k≠i) were randomly selected as the negative training examples. Then, all mapping scores between English categories and Chinese categories were measured based on the first-phase classification results. The English category with the highest mapping score was considered as the possibly mapped category. In the second learning phase of the Chinese-to-English experiments, the category information of the previously identified English cluster was concatenated to the corresponding Chinese cluster. Then, the English categories were used as the training set to train the second-phase classifiers. The augmented source Chinese categories were classified to calculate the mapping scores for each English news category. Finally, we measured the accuracy performance for each day using the correct mappings at the top-1, top-3, and top-5 recommendation ranks by the following equation:  Cross-Lingual News Group Recommendation Using  53  Cluster-Based Cross-Training  Accuracy =  Number of the correctly discovered mapping in S → T ,  (3)  Total number of the correct mapping in S → T  which is similar to Agrawal and Srikan [2001]. Accuracy, rather than precision or recall, is used because the recommendation process is performed on a cluster-to-cluster basis. The error rate is the complement of the accuracy. In the English-to-Chinese experiments, the roles of two catalogs were switched.  4.3 Results and Discussion  Table 1. Experimental results of the correctly discovered Chinese-to-English mappings in the top-1 recommendation lists.  Day  Tagged Mappings  SVM-V  SVM-VCT  SVM-W  SVM-WCT  ME  ME-VCT  
Dictionary-based translation is a traditional approach in use by cross-language information retrieval systems. However, significant performance degradation is often observed when queries contain words that do not appear in the dictionary. This is called the Out of Vocabulary (OOV) problem. In recent years, Web mining has been shown to be one of the effective approaches for solving this problem. However, the questions of how to extract Multiword Lexical Units (MLUs) from the Web content and how to select the correct translations from the extracted candidate MLUs are still two difficult problems in Web mining based automated translation approaches. Most statistical approaches to MLU extraction rely on statistical information extracted from huge corpora. In the case of using Web mining techniques for automated translations, these approaches do not perform well because the size of the corpus is usually too small and statistical approaches that rely on a large sample can become unreliable. In this paper, we present a new Chinese term measurement and a new Chinese MLU extraction process that work well on small corpora. We also present our approach to the selection of MLUs in a more accurate manner. Our experiments show marked improvement in translation accuracy over other commonly used approaches. Keywords: Cross-Language Information Retrieval, CLIR, Query Translation, Web Mining, OOV Problem, Term Extraction 1. INTRODUCTION As more and more documents written in various languages become available on the Internet, users increasingly wish to explore documents that were written in either their native language ∗ Faculty of Information Technology, School of Software Engineering and Data Communications, Queensland University of Technology, Brisbane, QLD 4001, Australia E-mail: {c.lu,yue.xu,s.geva}@qut.edu.au [Received June 30, 2007; Revised October 8, 2007; Accepted January 12, 2008]  62  Chengye Lu et al.  or some language other than English. Cross-language information retrieval (CLIR) systems allow users to retrieve documents written in more than one language through queries written in a different language. This is a helpful end-user feature. Obviously, translation is needed in the CLIR process; either translating the query into the document language, or translating the documents into the query language. The common approach is to translate the query into the document language using a dictionary. Dictionary-based translation has been adopted in cross-language information retrieval because bilingual dictionaries are widely available, dictionary-based approaches are easy to implement, and the efficiency of word translation with a dictionary is high. However, due to the vocabulary limitation of dictionaries, very often the translations of some words in a query cannot be found in a dictionary. This problem is called the Out of Vocabulary (OOV) problem. Very often, the OOV terms are proper names or newly created words. Even using the best dictionary, the OOV problem is unavoidable. As input queries are usually short, query expansion does not provide enough information to help recover the missing words. Furthermore, in many cases, it is exactly the OOV terms that are the crucial words in the query. For example, a query “SARS, CHINA” may be entered by a user in order to find information about SARS in China. However, SARS is a newly created term and may not be included in a dictionary published only a few years ago. If the word SARS is left out of the translated query, it is most likely that the user will be unable to find any relevant documents. Moreover, a phrase cannot always be translated by translating each individual word in the phrase. For example, an idiom is a phrase and should not be translated by combining translations of the individual words because the correct translation may be a specific word which is not the combination of individual word translations of the original phrase. Another problem with the dictionary-based translation approach is the translation disambiguation problem. The problem is more serious for a language which does not have word boundaries, such as Chinese. Translation disambiguation refers to finding the most appropriate translation from several choices in the dictionary. For example, the English word STRING has over 20 different translations in Chinese, according to the Kingsoft online dictionary (www.kingsoft.com). One approach is to select the most likely translation [Eijk 1993] – usually the first one offered by a dictionary. However, even if the choices are ordered based on some criteria and the most likely a-priori translation is picked, in general, such an approach has a low probability of success. Another solution is to use all possible translations in the query with the OR operator. However, while this approach is likely to include the correct translation, it also introduces noise into the query. This can lead to the retrieval of many irrelevant documents which is, of course, undesirable. [Jang et al. 1999] and [Gao et al. 2001] report that this approach has precision that is 50% lower than the precision that is obtained by human translation.  Web-Based Query Translation for English-Chinese CLIR  63  In this paper, we present a Web-based approach to term extraction and translation selection. Specifically, we introduce a statistics-based approach to extracting terms and a translation disambiguation technique to improve the precision of the translations. The remainder of this paper is structured as follows: in Section 2, we present the existing approaches to query translation; in Section 3 we present our approach. Experimental evaluation and results discussion are presented in Section 4 and Section 5, respectively. Finally, we conclude the paper in Section 6. 2. PREVIOUS WORK 2.1 Translation Dictionary-based query translation is one of the conventional approaches in CLIR. The appearance of OOV terms is one of the main difficulties arising with this approach. In very early years, OOV terms were not translated at all, leaving out the original terms in the translated query. This approach may significantly limit retrieval performance. In this section, several existing approaches to OOV translation are reviewed. 2.1.1 Transliteration Proper names, such as personal names and place names, are a major source of OOV terms because many dictionaries do not include such terms. It is common for foreign names to be translated word-by-word based on phonetic pronunciations. In this manner, a name in one language will be pronounced similarly in another language – this is called transliteration. Such translation is usually done by a human when a new proper name is introduced from one language to another language. Some researchers [Paola et al. 2003; Yan et al. 2003] have applied the rule of transliteration to automatically translate proper names. Basically, the transliteration will first convert words in one language into phonetic symbols, then convert the phonetic symbols into another language. Some researchers have found that transliteration is quite useful in proper name translation [Paola et al. 2003; Yan et al. 2003]. However, transliteration is useful in only a few language pairs. When dealing with language pairs for which there are many phonemes in one language that are not present in the other, such as Chinese and English, the problem is exacerbated. There are even more problems when translating English to Chinese. First, as there is no standard for name translation in Chinese, different communities may translate a name in different ways. For example, the word “Disney” is translated as “迪斯尼” in mainland China but is translated as“迪士尼” in Taiwan. Both are pronounced similarly in Chinese, but use different Chinese characters. Even a human interpreter would have difficulty in unambiguously choosing which character should be used. Second, at times, the Chinese  64  Chengye Lu et al.  translation only uses some of the phonemes of the English names. For example, the translation of “American” is “美国” which only uses the second syllable of “American”. Finally, the translation of a name is not limited to only using translation but also to transliteration. Sometimes, the translation of a proper name may even use a mixed form of translation and transliteration. For example, the translation of “New Zealand” in mainland China is “新西兰”, where “新” is the translation of “New” and “西兰” is the transliteration of “Zealand”. 2.1.2 Parallel Text Mining Parallel text is a text in one language together with its translation in another language. The typical way to use parallel texts is to generate translation equivalence automatically, without using a dictionary. It has been used in several studies [Eijk 1993; Kupiec 1993; Smadja et al. 1996; Nie et al. 1999] on multilingual related tasks such as machine translation or CLIR. The idea of parallel text mining is straightforward. Since parallel texts are texts in two languages, it should be possible to identify corresponding sentences in two languages. When the corresponding sentences have been correctly identified, it is possible to learn the correspondence translation of each term in the sentence using statistical information since the term’s translation will always appear in the corresponding sentences. Therefore, an OOV term can be translated by mining parallel corpora. Many researchers have also reported that parallel text mining based translation can significantly improve the CLIR performance [Eijk 1993; Kupiec 1993; Smadja et al. 1996; Nie et al. 1999]. In the very early stages, parallel text based translation approaches were word-by-word based and only domain specific noun terms were translated. In general, these approaches [Eijk 1993; Kupiec 1993] first align the sentences in each corpus, then noun phrases are identified by a part-of-speech tagger. Finally, noun terms are mapped using simple frequency calculations. In such translation models, phrases, especially verb phrases, are very hard to translate. As phrases in one language may have different word order in another language, phrases cannot be translated on a word-by-word basis. This problem in parallel text based translation is called the collocation problem. Some later approaches [Smadja et al. 1996; Nie et al. 1999] started to use more complex strategies such as statistical association measurement or probabilistic translation models to solve the collocation problem. Smadja et al. [Smadja et al. 1996] proposed an approach that can translate word pairs and phrases. In particular, they used a statistical association measure of the Dice coefficient to deal with the problem of collocation translation. Nie et al. [Nie et al. 1999] proposed an approach based on a probabilistic model that demonstrates another approach to solving the collocation problem. Using parallel texts, their translation model can return p(t|S), which is the probability of having the term t of the target language in the translation of the source sentence S. As the probability model does not consider the order and  Web-Based Query Translation for English-Chinese CLIR  65  the position of words, collocation is no longer a problem. Some of the advantages of the parallel text based approaches include the very high accuracy of translation without using bilingual dictionaries and the extraction of multiple transitions with equivalent meaning that can be used for query expansion. However, the sources of parallel corpora tend to be limited to some particular domain and language pairs. Currently, large-scale parallel corpora are available only in the form of government proceedings, e.g. Canadian parliamentary proceedings in English and French, or Hong Kong government proceedings in Chinese and English. Obviously, such corpora are not suitable for translating newly created terms or domain-specific terms that are outside the domains of the corpora. As a result, current studies of parallel text based translation are focusing on constructing large-scale parallel corpora in various domains from the Web. 2.1.3 Web Mining Web mining for automated translation is based on the observation that there are a large number of Web pages on the Internet that contain parallel text in several languages. Investigation has found that when a new English term, such as a new technical term or a proper name, is introduced into Chinese, the Chinese translation to this term and the original English term very often appear together in literature publications in an attempt to avoid misunderstanding. Some earlier studies have already addressed the problem of extracting useful information from the Internet using Web search engines such as Google and Yahoo. These search engines search for English terms on pages in a certain language, e.g., Chinese or Japanese. The results of Web search engines are normally a long, ordered list of document titles and summaries to help users locate information. Mining the result lists can help find translations to the unknown query terms. Some studies [Cheng et al. 2004; Zhang et al. 2004] have shown that such approaches are rather effective for proper name translation. Generally, Web-based translation extraction approaches consist of three steps: 1. Web document retrieval: use a Web search engine to find the documents in the target language that contain the OOV term in the original language and collect the text (i.e. the summaries) in the result pages returned from the Web search engine. 2. Term extraction: extract the meaningful terms in the summaries where the OOV term appears and record the terms and their frequency in the summaries. As a term in one language could be translated to a phrase or even a sentence, the major difficulty in term extraction is the identification of correct MLUs in the summaries (refer to Section 2.2 for the definition of MLUs). 3. Translation selection: select the appropriate translation from the extracted words. As the previous steps may produce a long list of terms, translation selection has to find the correct translation from the extracted terms.  66  Chengye Lu et al.  The existing term extraction techniques in the second step fall into two main categories: approaches that are based on lexical analysis or dictionary-based word segmentation, and approaches that are based on co-occurrence statistics. When translating Chinese text into English, Chinese terms should be correctly detected first. As there are no word boundaries in Chinese text, the mining system has to perform segmentation of the Chinese sentences to find the candidate words. The quality of the segmentation greatly influences the quality of the term extraction because incorrect segmentation of the Chinese text may break the correct translation of an English term into two or more words so that the correct word is lost. The translation selection in the third step also suffers from the problem that selection of the most frequent word or the longest word, which is the more popular techniques, does not always produce a correct translation. The term extraction and translation selection problems will be further addressed in subsequent sections. 2.2 Term Extraction Term extraction is mainly the task of finding MLUs in the corpus. The concept of MLU is important for applications that exploit language properties, such as Natural Language Processing (NLP), information retrieval and machine translation. An MLU is a group of words that always occur together to convey a specific meaning. For example, compound nouns like Disneyland, phrasal verbs like take into account, adverbial locutions like as soon as possible, and idioms like cutting edge are MLUs. In most cases, it is necessary to extract MLUs rather than individual words from a corpus because the meaning of an MLU is not always the combination of individual words in the MLU. The meaning of the MLU ‘cutting edge’ is not the combination of the meaning of individual words, ‘cutting’ and ‘edge’. Finding MLUs from the summaries returned by a search engine is important in Web mining based automated translation. If only words are extracted from the summaries, the following process may not be able to find the correct translation since the translation might be a phrase rather than a word. For Chinese text, a word consisting of several characters is not explicitly delimited since Chinese text contains sequences of Chinese characters without spaces between them. Chinese word segmentation is the process of marking word boundaries. The Chinese word segmentation is actually similar to the extraction of MLUs in English documents as the MLU extraction in English documents also needs to mark the lexical boundaries between MLUs. Therefore, term extraction in Chinese documents can be considered as Chinese word segmentation. Many existing systems use lexicon-based or dictionary-based segmentation techniques to determine word boundaries in Chinese text. However, in the case of Web mining for automated translation, as an OOV term is an unknown term to the system, the dictionary-based segmenters usually cannot correctly identify the OOV terms in the sentence. Therefore, the translation of an OOV term cannot be found in  Web-Based Query Translation for English-Chinese CLIR  67  a later process. Some researchers have suggested approaches that are based on co-occurrence statistics model for Chinese word segmentation to avoid this problem [Chen et al. 2000; Maeda et al. 2000; Gao et al. 2001; Pirkola et al. 2001].  2.2.1 Mutual Information and its Variations  One of the most popular statistics-based extraction approaches is to use mutual information  [Chien 1997; Silva et al. 1999]. Mutual information is defined as:  MI (x, y) = log2  p( x, y) p(x) p(y)  = log2  Nf (x, y) f (x) f (y)  ,  (1)  The mutual information measurement quantifies the distance between the joint distribution of terms X and Y and the product of their marginal distributions. When using mutual information in Chinese segmentation, x, y are two Chinese characters; f(x), f(y), f(x,y) are the frequencies that x appears, y appears, and x and y appear together, respectively; N is the size of the corpus. A string XY will be judged as a term if the MI value is greater than a predefined threshold.  Chien [Chien 1997] suggests a variation of the mutual information measurement called  significance estimation to extract Chinese keywords from corpora. The significance estimation  of a Chinese string is defined as:  SE(c) =  f (c)  ,  (2)  f (a) + f (b) − f (c)  where c is a Chinese string with n characters; a and b are the two longest composed substrings of c with length n-1; f is the function to calculate the frequency of a string. Two thresholds are predefined: THF and THSE. This approach identifies a Chinese string as an MLU by the following steps. For the whole string c, if f(c)>THF, c is considered a Chinese term. For the two (n-1)-substrings a and b of c, if SE(c)>=THSE, both a and b are not a Chinese term. If SE(c)<THSE, and f(a)>>f(b) or f(b)>>f(a) , a or b is a Chinese term, respectively. Then, for each a and b, the method is recursively applied to determine whether their substrings are terms.  2.2.2 Local Maxima Based Approaches  All mutual information based approaches require tuning the thresholds for generic use. Silva  and Lopes suggest an approach called Local Maxima to extract MLU from corpora without  using any predefined threshold [Silva et al. 1999]. The equation used in Local Maxima is  known as SCP and is defined as follows:  SCP(s) = n−1  f (s)2  ,  (3)  
Unknown term translation is important to CLIR and MT systems, but it is still an unsolved problem. Recently, a few researchers have proposed several effective search-result-based term translation extraction methods which explore search results to discover translations of frequent unknown terms from Web search results. However, many infrequent unknown terms, such as abbreviations and proper names (or named entities), and their translations are still difficult to be obtained using these methods. Therefore, in this paper we present a new search-result-based abbreviation translation method and a new two-stage hybrid translation extraction method to solve the problem of extracting translations of infrequent unknown abbreviations and proper names from Web search results. In addition, to efficiently apply name transliteration techniques to mitigate the problems of proper name translation, we propose a mixed-syllable-mapping transliteration model and a Web-based unsupervised learning algorithm for dealing with online English-Chinese name transliteration. Our experimental results show that our proposed new methods can make great improvements compared with the previous search-result-based term translation extraction methods. Keywords: CLIR, Transliteration, Unknown Term Translation, Web Search Result, Machine Translation.  ∗ Dept. of Computer Sci. and Eng., National Cheng Kung University, No.1, University Road, Tainan City 701, Taiwan (R.O.C.) Tel.: +886-6-2757575 ext: 62545 Fax: +886-6-2747076 E-mail: whlu@mial.ncku.edu.tw, ys.chang1976@gmail.com + Penpower Technology Ltd. 7F, NO.47, Lane 2, Sec. 2, Guanfu Rd., Hsinchu City 300, Taiwan, R.O.C. Tel: +886-3-5722691 Fax: +886-3-5716243 E-mail: hunter@penpower.com.tw [Received June 30, 2007; Revised October 8, 2007; Accepted January 12, 2008]  92  Wen-Hsiang Lu et al.  1. Introduction Many existing cross-language information retrieval (CLIR) systems [Ballesteros and Croft 1997; Hull and Grefenstette 1996] encounter great difficulties in dealing with unknown term translation since these systems rely mostly on general-purpose bilingual dictionaries, which usually lack translations of abbreviations and proper names. Moreover, according to the report in a previous work [Cheng et al. 2004], even for frequent Web queries, about 64% of them are not covered in an English-Chinese lexicon with about 120K entries (provided by Linguistic Data Consortium). However, several automatic translation extraction methods based on parallel [Brown et al. 1993; Melamed 2000; Nie et al. 1999; Smadja et al. 1996] or comparable corpora [Rapp 1999; Fung and Yee 1998] eventually suffer from the problems of insufficient parallel texts and the shortage of translation accuracy of comparable corpora in various subject domains. The Web has been expanded with an enormous amount of multilingual hypertext resources in diverse subjects. Recently, a number of studies in natural language processing (NLP) have concentrated on the use of Web resources to complement insufficient text corpora [Cao and Li 2002; Kilgarriff and Grefenstette 2003]. To automatically collect huge amounts of parallel corpora from the Web in various domains, some researchers have developed feasible techniques of utilizing similar file names, text length, and link structures to extract parallel text pages from bilingual Web sites [Nie et al. 1999; Resnik 1999; Yang and Li 2003]. On the other hand, Lu et al. [2002] made the first attempt of mining unknown term translations from Web anchor texts. Both Cheng et al. [2004] and Zhang and Vines [2004] have explored language-mixed search-result pages for extracting translations of frequent unknown queries. Although these approaches have successfully enhanced the performance of frequent unknown query translation, they still suffer from the problems of data sparseness and indirect association errors in finding translations of infrequent unknown query terms, particularly for abbreviations and proper names [Melamed 2000]. In this paper, we focus on dealing with two kinds of translation of unknown query terms, including proper names and abbreviations. According to the report in Davis and Ogden [1998], about 50% of unknown terms in queries are proper names. Most methods handling translations of proper names are based on name transliteration techniques [Knight and Graehl 1998; Lin and Chen 2002; Lin et al. 2003; Li et al. 2004]. One major drawback of these methods is that they do not consider semantic information. Lam et al. [2004] proposed a named entity matching model, which considers both semantic and phonetic information, and applied it in mining unknown named entity translations from online daily Web news. Huang et al. [2005] also presented a method to extract key phrase translations from the language-mixed search-result pages with phonetic, semantic and frequency-distance features. As for abbreviation translation, less attention has been put on this research topic in the past few years.  Improving Translation of Queries with  93  Infrequent Unknown Abbreviations and Proper Names  Different from the above works, our major goal is to solve the problems of query translation to help users access English/Chinese information in cross-lingual Web searches. In this paper, therefore, we concentrate our attention on the challenge of dealing with the translations of infrequent unknown abbreviations and transliterated names in Web search queries, i.e., these unknown queries that appear infrequently in Web query logs. We present two new methods to effectively extract translations of these two kinds of infrequent unknown queries. First, we propose a search-result-based abbreviation translation method for handling bidirectional translation of abbreviations in Chinese/English. Second, a new two-stage hybrid translation extraction method, which combines Cheng et al.’s [2004] search-result-based term translation extraction method and a new Web-based transliteration method, is proposed to extract Chinese/English translations for infrequent unknown English/Chinese proper names. In addition, to train an effective transliteration model, we also present a Web-based unsupervised learning algorithm to automatically collect large amounts of diverse English-Chinese transliteration pairs from the Web. For application, we provide a real prototype website1 for users to translate unknown terms in practice. Our experimental results show that the proposed new methods can make great improvements in extracting infrequent unknown term translation. The remainder of this paper is organized as follows: Section 2 describes the problems of unknown term translation and our search-result-based term translation extraction approach. Section 3 evaluates the proposed approach. Section 4 provides a simple description and comparison with the related work. Section 5 gives our conclusions. 2. Search-Result-Based Unknown Term Translation  2.1 Problems Cheng et al.’s search-result-based term translation extraction method (refer to Section 2.3) is effective in extracting translations for frequent unknown query terms. However, for a lot of infrequent abbreviations and proper names, their translations are still difficult to extract. For example, while submitting an English abbreviation “AMIA” to LiveTrans2, an incorrect Chinese translation “系列” (series) is obtained. The reason might be that some abbreviations are semantically ambiguous and co-occur relatively infrequently with the correct Chinese translations of their full names (or original forms). However, we observe that for an English abbreviation, its full name may co-occur more frequently with its corresponding Chinese translation. Thus, to effectively extract correct translation for an infrequent abbreviation, our idea is to first identify its full name in search results, and then extract correct translation of its  1http://ws.csie.ncku.edu.tw/~jhlin/cgi-bin/index.htm 2 http://livetrans.iis.sinica.edu.tw/: This website is developed based on the search-result-based term translation extraction method by Web Knowledge Discovery lab of Academia Sinica, Taiwan.  94  Wen-Hsiang Lu et al.  full name, using the search-result-based term translation extraction method mentioned above. Generally, it should be more feasible to extract the correct translation of an abbreviation via its full name. For example, if we can extract the full name of the abbreviation “AMIA”, “American Medical Informatics Association”, then we can get its correct Chinese translation “美國醫學資訊協會” via LiveTrans. On the other hand, an English proper name might have multiple Chinese transliterated names which often vary with different translators due to phonetic variation and the lack of standard transliteration rules [Gao et al. 2004]. In other words, there may be several Chinese transliterated names corresponding to an English name. For example, the name “Disney” has various Chinese transliterated names, including “迪士尼”, “迪斯尼”, “迪斯奈”, “狄斯奈”, and “狄士尼”; the name “Hussein” also has several different Chinese transliterated names, including “海珊”, “哈珊”, and “侯塞因”. Obviously, it will be helpful for query translation in cross-lingual Web search if we can collect all possible transliterated names from the Web for each unknown proper name. However, it is a real challenge to find all the various transliterated names. Thus, we consider integrating name transliteration techniques into the process of translation extraction for infrequent unknown proper names. Our idea is that we first extract high-frequency terms from the search-result pages as transliteration candidates, and then filter out impossible candidates by using a name transliteration model. In fact, it is still challenging to build an effective transliteration model while lacking sufficient transliteration pairs for training. Therefore, we propose a Web-based unsupervised learning algorithm to automatically collect large amounts of English-Chinese transliteration pairs from Web search results. 2.2 Overview of the Proposed Approach Figure 1 demonstrates the process of our search-result-based query translation method. First, an unknown term is determined by a general-purpose dictionary. Then, an unknown term is recognized as an abbreviated term using our search-result-based abbreviation translation extraction methods. If the unknown term does not belong to an abbreviated term, we have to examine whether the unknown term is a transliteration based on our two-stage hybrid translation extraction method. To deal with unknown term translation, we employ the search-result-based term translation extraction method (described in Section 2.3) to handle translation of frequent (popular) unknown query terms, and propose two new infrequent unknown translation methods, namely the search-result-based abbreviation translation extraction method (Section 2.4) and two-stage hybrid translation extraction method (Section 2.5), to solve the problems of translation of abbreviated terms (i.e., abbreviations) and transliterated terms (i.e., proper names). To recognize the abbreviated terms in queries, we  Improving Translation of Queries with  95  Infrequent Unknown Abbreviations and Proper Names  collected an abbreviation list containing about 4K entries from the Wikipedia3 website and then generated some pre-defined abbreviation patterns like those used in Park and Byrd (2001). Besides these, we used a Web-based transliteration model to recognize a transliterated term (Section 2.5).  Figure 1. The process of our search-result-based query translation method 2.3 Search-Result-Based Term Translation Extraction Method In this section, we will describe Cheng et al.’s [2004] search-result-based term translation extraction method, which explores search-result pages utilizing co-occurrence relation and contextual information for extraction of translations of unknown query terms. (1) Chi-square Test Method On the basis of co-occurrence analysis, chi-square test (χ2) is adopted to estimate semantic similarity between the source term E and the target translation candidate C. The similarity measure is defined as: 3 http://en.wikipedia.org/wiki/List_of_acronyms_and_initialisms  96  Wen-Hsiang Lu et al.  Sχ2 (E,  C)  =  N × (a × d − b× c)2  ,  (a + b) × (a + c) × (b + d ) × (c + d )  (1)  where a, b, c and d are the numbers of pages retrieved from search engines by submitting Boolean queries: “E and C”, “E and not C”, “not E and C”, and “not E and not C”, respectively; N is the total number of pages, i.e., N = a + b + c + d.  (2) Context-Vector Analysis Method  Due to the property of Chinese-English mixed texts often appearing in Chinese pages, the  source term E and the target translation candidate C may share common contextual terms in  the search-result pages. The similarity between E and C is computed based on their context  feature vectors Ecv and Ccv in the vector-space model. The conventional tf-idf weighting scheme for each feature term ti in Ecv and Ccv, Ecv = <we1, we2, …, wem>, and Ccv = <wc1, wc2, …, wcm>, is used and defined as:  wti  = f (ti , p) × log( N ),  max f (t j , p)  n  (2)  j  where f(ti, p) is the frequency of term ti in the search-result page p, N is the total number of Web pages, and n is the number of the pages containing ti. Finally, we use the cosine measure to estimate the similarity between E and C as follows:  SCV (E, C) =  ∑im=1wei × wci  .  ∑im=1(wei )2 ×∑im=1(wci )2  (3)  2.4 Search-Result-Based Abbreviation Translation Extraction Method To effectively extract correct translations for infrequent abbreviated terms, we propose an integrated method in which an abbreviated term is transformed to its full name first, and then we extract the correct translation of the full name using the search-result-based term translation extraction method described above (Section 2.3). In the following, we describe two new proposed methods exploiting search results to extract full names for English and Chinese abbreviations, respectively.  2.4.1 Extracting Full Names for English Abbreviations To deal with the full names for a given English abbreviation, we designed an efficient process of identifying full names, which consists of three major steps based on the hybrid text mining approach proposed by Park and Byrd [2001]. First, we use the contextual terms around an abbreviated term in the search results to extract possible full name candidates. Second, we use occurrence frequency and Part-of-Speech (POS) information of full name candidates to filter out some impossible candidates. Finally, we propose a simple adaptive co-occurrence model  Improving Translation of Queries with  97  Infrequent Unknown Abbreviations and Proper Names  which utilizes several different augmenting and decaying factors in selecting the best full name candidate. More details are described in the following.  (1) Identifying Full Name Candidates  To solve the problem of identifying full names without sufficient texts [Park and Byrd 2001], we take advantage of Web search results as a corpus. Our idea is to take the given abbreviated term as a search term to fetch the top 200 search result snippets from Google. To extract possible full name candidates by exploring the search result snippets, we utilize contextual information of the abbreviated term in the snippets. These full name candidates must appear in the same snippets with the abbreviated term, and should have a minimum word length between |A|×2 and |A|+5, where |A| is the length of characters of the abbreviated term. In addition, to select more reliable full name candidates, we put a constraint on the identification process in which the first character of the first word of each full name candidate should match the first character of the abbreviated term.  (2) Filtering Impossible Full Name Candidates  To reduce computation time while extracting many full name candidates, we first select the top 20 frequent full name candidates and then filter out some impossible candidates whose first word or last word are prepositions, be-verbs, modal verbs, conjunctions, or pronouns [Park and Byrd 2001].  (3) Selecting Best Full Name Candidate  To select the best full name candidates, we propose an adaptive co-occurrence model by employing mutual information as well as four augmenting or decaying factors to compute the similarity between an abbreviated term A and its full name candidates FC. (A) Mutual Information: In this step, mutual information is used to compute the similarity between an abbreviated term A and its full name candidate FC. Mutual information is defined as follows:  MI ( A,  FC  )  =  P( A,  FC  ) × log(  P( A, FC ) P( A) × P(FC  ) )  .  (4)  Here P(A, FC) is the probability of co-occurrence of A and FC. P(A) and P(FC) are the probabilities of occurrence of A and FC in the Web, respectively. We can get the occurrence frequencies from search engines by submitting queries: “A”, “FC”, and “A and FC”, respectively.  (B) Syntactic Cues: To augment the identification of full names, we utilize the information of orthographic and syntactic structure. NSC indicates the number of abbreviation-full-name pairs appearing in the same snippets. Several frequent patterns of abbreviation-full-name pair are used as the syntactic cues [Park and Byrd 2001], including:  98  Wen-Hsiang Lu et al.  y abbreviation (full name) y full name (abbreviation) y abbreviation, or full name y full name, or abbreviation y full name, abbreviation for short y abbreviation … stands/short/acronym …full name  (C) Similarity of Character: To further determine correct full names, we add another  augmenting factor to estimate the similarity between an abbreviated term and its full name  candidates by adopting a fast and simple character matching method. We use two kinds of  character matching: (1) first-letter matching is used to compute the total number NF of  matching the first letter of each word in the full name candidate FC with each character in the  abbreviated terms, and (2) non-first-letter matching is used to computer the total number NNF  of matching the non-first letters of each word in the FC with each character in A. The score of  character matching of A and FC is defined as:  Overlap ( A, FC ) = α ∗ NF + (1− α ) ∗ NNF .  (5)  Here, the weighting parameter α is empirically set to 0.8. Basically, the first-letter matching  should be reasonably assigned higher weight for each matching pair. The character similarity  is defined as follows:  CharSim( A, FC  )  =  Overlap( A, | A|  FC  )  ,  (6)  where | A| is the number of characters of the abbreviated term A.  (D) Difference of Length: The number NLD to represent the difference between character  length |A| of the abbreviated term A and word length |FC| of the corresponding full name  candidate FC as a decaying factor. NLD is defined as follows:  NLD = | A | − | FC | .  (7)  (E) Number of Stop Words: The number NSW of stop words in the full name candidate FC is also used as a decaying factor.  (F) Adaptive Co-occurrence Model: We adaptively integrate the above two augmenting and  two decaying factors into the basic co-occurrence model to compute the similarity between A  and FC. Our adaptive co-occurrence model is defined as follows:  S AC ( A, FC ) =  MI ( A, FC ) × FAugument FDecay  ,  (8)  where the augmenting factor FAugument is integrated as  FAugument = CharSim ( A, FC ) × ( β1 + NSC ) ;  (9)  Improving Translation of Queries with  99  Infrequent Unknown Abbreviations and Proper Names  and the decaying factor FDecay is integrated as  FDecay = ( β2 + NLD + NSW ) .  (10)  To avoid the product being zero, here, β1 and β2 are the adaptable parameters and set to 1 heuristically.  2.4.2 Extracting Full Names for Chinese Abbreviations Due to language differences between Chinese and English, such as no space delimitation between Chinese words, it is more difficult to identify the full name for a given Chinese abbreviated term. Therefore, we designed a method slightly different from the method of extracting English full names described above. Our Chinese full name extraction method consists of three major steps. First, the possible full name candidates are extracted by using the PAT-tree-based keyword extraction method proposed by Chien [1997]. Second, we use the character similarity between an abbreviated term and its full name candidates to filter out some impossible candidates. Finally, to select the correct Chinese full name, we use the adaptive co-occurrence model (Equation (8)) but slightly modify the decaying factors. The following description will explain the different points in more details. (1) Identifying Full Name Candidates To identify the possible full name candidates for a given Chinese abbreviated term A, we adopt a PAT-tree-based keyword extraction method [Chien 1997] to extract Chinese phrases in the search results related to the abbreviated term A as full name candidates. In addition, to select more reliable full name candidates, we put a length constraint on the candidates. These candidates should have more than (|A| +2) characters, where |A| is the number of characters of A. (2) Filtering Impossible Full Name Candidates According to our observations, the Chinese full name candidates extracted by the PAT-tree-based keyword extraction method generally have higher reliability. Thus, we just use Equations (5) and (6) with a threshold of character similarity to filter out some impossible candidates. (3) Selecting Best Full Name Candidate Like the above method of selecting the best English full name candidates, we still use the proposed adaptive co-occurrence model (Equation (8)) to select the best Chinese full name candidates. Please note, though, that the processing of augmenting/decaying factors is a little different. For example, we remove the decaying factor of stopword number since most stopwords seldom appear in Chinese full names. Some different points will be described below.  100  Wen-Hsiang Lu et al.  (A) Syntactic Cues: We also manually choose several syntactic patterns of Chinese abbreviation- full name pairs as the augmenting factor: y abbreviation (full name) y full name (abbreviation) y abbreviation, 或 full name y full name, 或 abbreviation y abbreviation … 代表/簡稱/縮寫 …full name  Here the Chinese cues ”或”, “代表”, “簡稱”, “縮寫” correspond to the English words “or”, “present”, “short”, and “acronym”, respectively.  (B) Similarity of Character: First, we use the Chinese POS tagger to segment full name candidates. Then, we take character similarity (Equation (5) and (6)) as an augmenting factor.  (C) Difference of Length: Due to the fact that there is no space delimitation between Chinese words, we adopt a Chinese POS tagger4 to do word segmentation for full name candidates. Then, we use the number NLD to represent the difference between character length |A| of the abbreviated term A and word length |FC| of the corresponding full name candidate FC; this is considered a decaying factor (Equation (7)).  (D) Adaptive Co-occurrence Model: We adopt the same adaptive co-occurrence model (Equation (8)) with two augmenting factors and one decaying factor to compute the similarity between A and FC. The augmenting factors are the same as Equation (9), but the decaying factor in Equation (10) is modified adaptively by removing the stopword number as:  FDecay = ( β3 + NLD ) .  (11)  To avoid the product being zero, here β3 is an adaptable parameter and set to 1, heuristically.  2.5 Search-Result-Based Transliteration Name Extraction Method To improve the performance of unknown term translation extraction for infrequent proper names, we consider integrating name transliteration techniques into the process of translation extraction in order to filter out impossible transliterated name candidates. Our idea is to first extract terms from the search-result snippets as translation candidates (see Section 2.3), and then filter out impossible transliterated name candidates based on the name transliteration model (described in Section 2.5.2). Therefore, in this section we propose a two-stage hybrid translation extraction method, a Web-based transliteration model to deal with transliteration mapping between an English proper name and its corresponding Chinese, and a Web-based  4 http://ckipsvr.iis.sinica.edu.tw/demo.htm, which is a Chinese POS tagger developed by Chinese Knowledge and Information Processing group of Academia Sinica.  Improving Translation of Queries with  101  Infrequent Unknown Abbreviations and Proper Names  unsupervised learning algorithm to automatically collect diverse English-Chinese transliteration name pairs from Web search results for transliteration model training (Section 2.5.3).  2.5.1 Two-Stage Hybrid Translation Extraction Our proposed two-stage hybrid translation extraction method is composed of two major steps. First, we use the search-result-based translation extraction method (Section 2.3) to extract k (k = 20) terms with higher similarity scores as transliteration candidates. Second, some impossible candidates included in general-purpose bilingual dictionaries are filtered out, and then the rest of the candidates are ranked according to transliteration similarity with the source proper name, which is computed based on the proposed Web-based transliteration model below (Equation (15)).  2.5.2 Filtering Impossible Candidates Using Web-Based Transliteration Model (A) English Syllable Segmentation: Wan and Verspoor [1998] have developed a fully rule-based algorithm to transliterate English proper names into Chinese names. We simplify their syllabification techniques to generate a few simple heuristic rules of segmenting an English name into a sequence of syllables. Each English syllable is regarded as an English transliteration unit (ETU) in this work and has at most one corresponding character of the Chinese transliterated name. Initially, we used only five rules for English syllable segmentation listed below: • a, e, i, o, u are vowels, and y is also regarded as a vowel if it appears behind a consonant. All other letters are consonants. • Separate two consecutive vowels except the following cases: ai, au, ee, ea, ie, oa, oo, ou, etc. • Separate two consecutive consonants except the following cases: bh, ch, gh, ph, th, wh, ck, cz, zh, zk, ng, sc,ll, tt, etc. • l, m, n, r are combined with the prior vowel only if they are not followed by a vowel. • A consonant and a following vowel are regarded as an ETU. For example, “Nokia” (諾基亞) is segmented into three ETUs “no”, “ki”, and “a”, and “Epson” (愛普生) is segmented into three ETUs “e”, “p”, and “son”. Currently, although some English names may be segmented incorrectly, it is easy to manually update new rules to improve English syllable segmentation. (B) Web-based Transliteration Model: To avoid double errors of converting English phonetic representation to Chinese Pinyin and from Pinyin to Chinese characters, in this work, we adopted direct orthographic mapping for name transliteration. We use the probability P(ei, ci) to estimate the possibility of the mapping between an ETU ei and a Chinese character ci. Additionally, to build an efficient online name transliteration model, we propose a more simple transliteration  102  Wen-Hsiang Lu et al.  model. Our Web-based transliteration model is called forward-syllable-mapping transliteration  model:  SFSM  (E,C)  =  PFSM (E,C) D(E,C)  ,  (12)  where PFSM(E, C) is the co-occurrence probability of E and C and defined as  min(m,n) PFSM ( E,C) ≈ ∏ [(1−γ1)P(ei,ci ) +γ1], i=1  (13)  and γ1 is the smoothing weight. The decaying factor D(E, C) indicates the number of syllable difference between an English name E and a Chinese transliterated name C and is defined as:  D(E,C) = ε + | m − n | .  (14)  Here ε is a decaying parameter, m is the total number of ETUs, and n is the total number of Chinese characters.  To improve incorrect transliteration mapping between ETUs and Chinese characters  while an English-Chinese transliterated name pair with different numbers of transliteration  unit, we propose the reverse-syllable-mapping transliteration model to assist in learning more  correct mapping, which is defined below:  SRSM  (E,C)  =  PRSM (E,C) D(E,C)  ,  (15)  where  PRSM  (  E, C)  ≈  ⎧⎪⎪⎨⎪⎪⎩ii==mn∏∏−mn−mn++11[[((11−−γγ22))PP((ccii−,e(mi−−(nn)−,me)i ))  +γ2], +γ2],  m≥ n; m< n.  (16)  Here γ2 is the smoothing weight and D(E, C) is the same as Equation (14).  Our alternative transliteration model will combine the forward-syllable-mapping and  reverse-syllable-mapping transliteration model, which is called mixed-syllable-mapping  transliteration model, and defined as:  SMSM (E,C) = SFSM (E,C) × SRSM (E,C).  (17)  2.5.3 Web-Based Unsupervised Learning Algorithm To deal with the problems of the diversity of Chinese transliterated names to English proper names, we intend to take advantage of abundant language-mixed texts on the Web to collect various English-Chinese transliterated name pairs from the Web and build a an effective online transliteration model. Thus, we designed an unsupervised learning process for English-Chinese transliterated name mapping. The process is composed of three main stages:  Improving Translation of Queries with  103  Infrequent Unknown Abbreviations and Proper Names  extraction of Chinese transliterated names, extraction of English original names, and learning of transliterated name mapping. More details are described below and the unsupervised learning algorithm is illustrated as well in Figure 2.  Web-based Unsupervised Learning Algorithm for Collecting English-Chinese Transliteration Pairs and Training a Transliteration Model Input: initial transliterated name pair set Vec and a general-purpose bilingual dictionary D. Output: updating Vec and a transliteration model T. 
Experiments carried out within evaluation initiatives for information retrieval have been building a substantial resource for further detailed research. In this study, we present a comprehensive analysis of the data of the Cross Language Evaluation Forum (CLEF) from the years 2000 to 2004. Features of the topics are related to the detailed results of more than 100 runs. The analysis considers the performance of the systems for each individual topic. Named entities in topics revealed to be a major influencing factor on retrieval performance. They lead to a significant improvement of the retrieval quality in general and also for most systems and tasks. This knowledge, gained by data mining on the evaluation results, can be exploited for the improvement of retrieval systems as well as for the design of topics for future CLEF campaigns. Keywords: Cross-Lingual Information Retrieval, Evaluation Issues, Named Entities (NEs) 1. Introduction The Cross Language Evaluation Forum (CLEF) provides a forum for researchers in information retrieval and manages a testbed for mono- and cross-lingual information (CLIR) retrieval systems. CLEF allows the identification of successful approaches, algorithms, and tools in CLIR. Within CLEF, various strategies are employed in order to improve retrieval systems [Braschler and Peters 2004; di Nunzio et al. 2007]. We believe that the effort dedicated to large scale evaluation studies can be exploited beyond the optimization of individual systems. The amount of data created by organizers and participants remains a valuable source of knowledge awaiting exploration. Many lessons can still be learned from past data of evaluation initiatives such as CLEF, TREC [Voorhees and ∗ Information Sci., University of Hildesheim, Marienburger Platz 22, 31141 Hildesheim, Germany. Tel.: +49-5121-883 ext: 837 The author for correspondence is Thomas Mandl. E-mail: mandl@uni-hildesheim.de [Received June 30, 2007; Revised October 8, 2007; Accepted January 12, 2008]  122  Thomas Mandl and Christa Womser-Hacker  Buckland 2002], INEX [Fuhr 2003], NTCIR [Oyama et al. 2003], or IMIRSEL [Downie 2003]. Ultimately, further criteria and metrics for the evaluation of search and retrieval methods may be found. This could lead to improved algorithms, quality criteria, resources, and tools in cross language information retrieval [Harman 2004; Schneider et al. 2004]. This general research approach is illustrated in Figure 1. Topics are considered an essential component of experiments for information retrieval evaluation [Sparck Jones 1995]. In most evaluations, the variation between topics is larger than the variation between systems. The topic creation for a multilingual test environment requires special care in order to avoid cultural or linguistic bias influencing the semantics of topic formulations [Kluck and Womser-Hacker 2002]. It must be assured that each topic provides equal conditions as starting points for the systems. The question remains whether linguistic aspects randomly appearing within the topics have any influence on the retrieval performance. This is especially important, as we observed in some cases, as leaving out one topic from the CLEF campaign changes the ranking of the retrieval systems despite the fact that 50 topics are considered to be sufficiently reliable [Voorhees and Buckley 2002; Zobel 1998].  <top> <num> C001 <S-ti<tloep> Arqui<tneucmt>urCa00e1n Berlín <S-de<sSc->ti<ttloep>> EnconAtrrqauri<tndeuocmct>umrCea0n0teon1sBseorblríen la arqui<tSe-cdte<usSrc-a>tietnleB>erlín. <S-naErnrc>ontrar documentos sobre la Los daorcqumiAetrneqtcuotisutreracetleneuvraBanetreelnsínBt.reartlaínn, en ge<nSe-rna<lrS,r->dseosbcr>e los rasgos arquiLtoesctdEóonncciuocmnoetsnrtadores Brdeorlcleuívmnaennot,etsoesntrsaotbarne, la partiecnulgaernr,eqrusaioltb,ercestobluarreareleconosnBsrtaersruglcoícsni.ón arqui<tSe-cntaórnri>cos de Berlín o, en partiLcousladro,cusmobernetolsa rreecloenvsatnrtuecsción tratan, en general, sobre los rasgos arquitectónicos de Berlín  Topic Properties  Multilingual Topic Set Topic Properties  wSotWworSdpoetsworiSgdpotShsmorttdpeseSsmmrtBeeSmmRrteIFemnrdIenxdIenxdex Weights BRF Weights BRF Systems  Results per Run and Topic  Patterns  Figure 1. General overview of the research approach.  Most analysis of the data generated in CLEF is based on the average performance of the systems. This study concentrates on the retrieval quality of systems for individual topics. By identifying reasons for the failure of certain systems for some topics, these systems can be optimized. Our analysis identified a feature of the topics which can be exploited for future system improvement. In this study, we focused on the impact of named entities in topics and found a significant correlation with the average precision. Consequently, the goal of this study  Analyzing Information Retrieval Results With a Focus on Named Entities  123  is twofold: (a) to measure the effect of named entities on retrieval performance in CLEF (b) to optimize retrieval systems based on these results. Named entities pose a potential challenge to cross language retrieval systems, because these systems often rely on machine translation of the query. The following problems may occur when trying to translate a named entity: • The named entity may be out of vocabulary for translation • Copying a named entity into the target language often does not help, as the name may be spelled differently (e.g. German: “Gorbatschow” vs. English: “Gorbachev”) • A named entity can actually be translated (e.g. “Smith” could be interpreted as a name or a profession and as the latter, translated) Named entities are a feature which can be easily identified within queries. We consider the systems at CLEF as black boxes and have so far not undertaken any effort to analyze how these systems treat named entities and why that treatment may result in the effects we have observed. The data necessary for such an analysis is not provided by CLEF. The systems use very different approaches, tools and linguistic resources. Each system may treat the same named entity quite differently and successful retrieval may be due to a large number of factors like appropriate treatment as n-gram, proper translation by a translation service, or due to an entry in a linguistic resource. An analysis of the treatment of the named entities would lead merely to case studies. As a consequence, we find a statistical analysis of the overall effect as the appropriate research approach. The remainder of this paper is organized as follows. The next chapter provides a brief overview of the research on evaluation results and their validity. Chapter three describes the data for CLEF used in our study. In chapter four, the influence of named entities on the overall retrieval results are analyzed. Chapter five explores the relationship between named entities and the performance of individual systems. In chapter six, we show how the performance variation of systems due to named entities could be exploited for system optimization. 2. Analysis of Information Retrieval Evaluation Results The validity of large-scale information retrieval experiments has been the subject of a considerable amount of research. Zobel concluded that the TREC (Text REtrieval Conference) experiments are reliable as far as the ranking of the systems is concerned [Zobel 1998]. Voorhees and Buckley have analyzed the reliability of experiments as a function of the size of the topic set [Voorhees and Buckley 2002]. They concluded that the typical size of the topic set of some 50 topics in TREC is sufficient for a satisfactory level of reliability.  124  Thomas Mandl and Christa Womser-Hacker  Human judgments are necessary to evaluate the relevance of the documents. Relevance assessment is a very subjective task. Consequently, assessments by different jurors result in different sets of relevant documents. However, these different sets of relevant documents do not lead to different system rankings according to an empirical analysis [Voorhees 2000]. Thus, the subjectivity of the jurors does not call into question the validity of the evaluation results. Further research is dedicated toward the question of whether expensive human relevance judgments are necessary or whether the constructed document pool of the most highly ranked documents from all runs may serve as a valid approximation of the human judgments. According to a study by Cahan et al., the ranking of the systems in TREC correlates positively to a ranking based on the document pool without further human judgment [Cahan et al. 2001]. However, there are considerable differences in the ranking which are especially significant for the highest ranks. Another important aspect in evaluation studies is pooling. Not all submitted runs can be judged manually by jurors and relevant documents may remain undiscovered. Therefore, a pool of documents is built to which the systems contribute differently. In order to measure the potential effect of pooling, a study was conducted which calculated the final rankings of the systems by leaving out one run at a time [Braschler 2003]. It shows that the effect is negligible and that the rankings remain stable. However, our analysis shows that leaving out one topic during the result calculation changes the system ranking in most cases. It has also been noted that the differences between topics are larger than the differences between systems. This effect has been observed in TREC [Harman and Voorhees 1997] and also in CLEF [Gey 2001]. For example, when looking at run EIT01M3N in the CLEF 2001 campaign, we see that it has a fairly good average precision of 0.341. However, for one topic (nr. 44), which had an average difficulty, this run performs far below (0.07) the average for that topic (0.27). An intellectual analysis of the topics revealed that two of the most difficult topics contained no proper names and that both topics were from the sports domain (Topic 51 and 54). This effect has been noted in many evaluations and also in CLEF [Hollink et al. 2004]. As a consequence, topics are an important part of the design in an evaluation initiative and need to be created very carefully. Named entities seem to play an important role especially in multilingual information retrieval [Gey 2001]. This assumption is backed by experimental results. The influence of named entities on the retrieval performance is considerable. In an experiment, the removal of named entities from the topic decreased the quality considerably, whereas the use of named entities only in the query led to a much smaller decrease [Demner-Fushman and Oard 2003].  Analyzing Information Retrieval Results With a Focus on Named Entities  125  A study for the CLEF campaign 2001 revealed no strong correlation between any single linguistic phenomenon and the system difficulty of a topic. Not even the length of a topic showed any substantial effect, except for named entities. However, the sum of all phenomena was correlated to the performance. The more linguistic phenomena available, the better the systems solved a topic on average [Mandl and Womser-Hacker 2003]. The availability of more variations of a word seems to provide stemming algorithms with more evidence for extraction of the stem, for example. 3. Named Entities in the Multi-lingual Topic Set The data for this study stems from the Cross Language Evaluation Forum (CLEF) [Peters et al. 2003; Peters et al. 2004]. CLEF is a large evaluation initiative which is dedicated to cross-language retrieval for European languages. The setup is similar to the Text Retrieval Conference (TREC) [Harman and Voorhees 1997; Voorhees and Buckland 2002]. The main tasks for multilingual, ad-hoc retrieval are: • The core and most important track is the multilingual task. The participants choose one topic language and need to retrieve documents in all main languages. The final result set needs to integrate documents from all languages ordered according to relevance regardless of their language. • The bilingual task requires the retrieval of documents different from the chosen topic language. • The Monolingual task represents the traditional ad-hoc task in information retrieval and is allowed for some languages. All runs analyzed in this study are test runs based on topics for which no previous relevance judgments were known. For training runs, older topics can be used each year. Techniques and algorithms for cross-lingual and multilingual retrieval are described in the CLEF proceedings and are not the focus of this paper. The topic language of a run is the language which the system developers use to start the search and to construct their queries. The topic language needs to be stated by the participants and can be found in the appendix of the CLEF proceedings. The retrieval performance of the runs for the topics can also be extracted from the appendix of the CLEF proceedings [Peters et al. 2003; Peters et al. 2004]. Most important, the average precision of each run for each topic can be retrieved. 3.1 Topic Creation Process The topic creation for CLEF needs to assure that each topic is translated into all languages without modifying the content while providing equal chances for systems which start with  126  Thomas Mandl and Christa Womser-Hacker  different topic languages. Therefore, a thorough translation check of all translated topics in CLEF was performed to check if the translations to all languages resulted in the same meaning. Nevertheless, the topic generation process follows a natural method and avoids artificial constructions [Womser-Hacker 2002]. Figure 2 shows an exemplary topic from CLEF containing a named entity. The topic’s structure is built up by a short title, a description with a few words and a so-called narrative with one or more sentences. Participants of CLEF have to declare which parts are used for retrieval. <top lang="ES"> <num>C083</num> <ES-title> Subasta de objetos de Lennon. </ES-title> <ES-desc> Encontrar subastas públicas de objetos de John Lennon.</ES-desc> <ES-narr> Los documentos relevantes hablan de subastas que incluyen objetos que pertenecieron a John Lennon, o que se atribuyen a John Lennon.</ES-narr> </top> <top> <num>C083</num> <FR-title> Vente aux enchères de souvenirs de John Lennon </FR-title> <FR-desc> Trouvez les ventes aux enchères publiques des souvenirs de John Lennon. </FR-desc> <FR-narr> Des documents pertinents décriront les ventes aux enchères qui incluent les objets qui ont appartenu à John Lennon ou qui ont été attribués à John Lennon. </FR-narr> </top> Figure 2. Example of a CLEF topic with a named entity 3.2 Data An intellectual analysis of the results and the properties of the topics had identified named entities as a potential indicator of good retrieval performance. For that reason, named entities in the CLEF topic set were analyzed in more detail. Named entities were intellectually assessed according a published schema [Sekine et al. 2002]. The analysis included all topics from the campaigns in the years 2000 through 2004. The number of named entities in each topic was assessed intellectually. We focused on English, Spanish, and German as topic languages and considered monolingual, bilingual, and multilingual tasks. Table 1 shows the overall number of named entities found in the topic sets. The extraction was done intellectually by graduate students. We also assessed in which parts of the topic the name occurred, whether found in the title, the description, or the narrative. This detailed analysis was not exploited further because very few runs use a source other than title plus description. In very few cases, the topic narrative includes additional named entities not already present in the title and the description. For our analysis, the sum of named entities in all three parts was used. We analyzed the topic set in three languages, and in some cases, differences between the number of named entities between two versions of a topic occur.  Analyzing Information Retrieval Results With a Focus on Named Entities  127  These differences were considered. In 18 cases, a different number of named entities was assessed between German and English versions of topics 1 through 200, and in 49 cases, a difference was encountered between German and Spanish for topics 41 though 200. For example, topic 91 contains one named entity more for German because German has two potential abbreviations for United Nations (UN and UNO) and both are used.  The numbers given in Table 1 are based on the English versions of the topics and consider the number of types rather than tokens of named entities in title, description, and narrative together.  Table 1. Number of named entities in the CLEF topics  CLEF Number of Total number of Average number of year topics named entities named entities in topics  2000 40  52  1.14  2001 50  60  1.20  2002 50  86  1.72  2003 60  97  1.62  2004 50  72  1.44  Standard deviation of named entities in topics 1.12 1.06 1.54 1.18 1.30  Table 2. Overview of named entities in CLEF tasks  CLEF year  Task  Topic language  Nr. runs  Topics without named entities  Topics with one or two named entities  Topics with more than three named entities  2001 Bi German 9  16  24  7  2001 Multi German 5  16  24  7  2001 Bi English 3  16  24  7  2001 Multi English 17  17  26  7  2002 Mono German 21  12  21  17  2002 Mono Spanish 28  11  18  21  2002 Bi German 4  12  21  17  2002 Multi German 4  12  21  17  2002 Bi English 51  14  21  15  2002 Multi English 32  14  21  15  2003 Mono Spanish 38  6  33  21  2003 Multi Spanish 10  6  33  21  2003 Mono German 30  9  40  10  2003 Bi German 24  9  40  10  2003 Bi English 8  9  41  10  2003 Multi English 74  9  41  10  2004 Multi English 34  16  23  11  128  Thomas Mandl and Christa Womser-Hacker  The large number of named entities in the topic set shows their importance. Table 2 shows the number of runs within each task. For the analysis presented in chapter five, we divided the topics into three classes: (a) no named entities, (b) one or two named entities, and (c) three or more named entities. The distribution of topics over these three classes is also shown in Table 2. It can be seen that the three classes are best balanced in CLEF 2002, whereas topics in the second class dominate in CLEF 2003. Only topics for which no zero results were returned were considered for each sub-task. Since these topics differ between sub-tasks, there are slight differences between the numbers for each class even for one year. For further analysis, only tasks with more than eight runs were considered. 4. Named Entities and General Retrieval Performance Our first goal was to measure whether named entities had any influence on the overall quality of the retrieval results. In order to measure this effect, we first calculated the correlation between the overall retrieval quality achieved for a topic and the number of named entities encountered in this topic. In the second section, this analysis is refined to single tasks and specific topic languages.  4.1 Correlation Between Average Precision and Number of Named Entities  Table 3. Method a: Best run for each topic in relation to the number of named entities in the topic  Number of named entities Number of Topics Average of Best System per Topic Minimum of Best System per Topic Standard Deviation of Best System per Topic  012345 42 43 40 20 9 4 0.62 0.67 0.76 0.83 0.79 0.73 0.09 0.12 0.04 0.28 0.48 0.40 0.24 0.24 0.24 0.18 0.19 0.29  Table 4. Method b: Average precision of runs in relation to the number of named entities in the topic  Number of named entities Number of Topics Minimum of Average Performance per Topic Average of Average Performance per Topic Maximum of Average Performance per Topic Standard Deviation of Average Performance  012345 42 43 40 20 9 4 0.02 0.04 0.01 0.10 0.17 0.20 0.20 0.25 0.36 0.40 0.31 0.40 0.54 0.61 0.78 0.76 0.58 0.60 0.14 0.15 0.18 0.17 0.14 0.19  Analyzing Information Retrieval Results With a Focus on Named Entities  129  First, we determined the overall performance in relation to the number of named entities in a topic. The 200 analyzed topics contain between zero and six named entities. For each number n of named entities, we determine the overall performance by two methods: (a) take the best run for each topic and (b) take the average of all runs for a topic. For both methods, we obtain a set of values for n named entities. Within each set, we can determine the maximum, the average, and the minimum. For example, we determine for method (a) the following values: best topic for n named entities, average of all topics for n named entities, and worst topic among all topics with n named entities. The last value gives the performance for the most difficult topic within the set of topics containing n named entities. The maximum of the best runs is in most cases 1.0 and is, therefore, omitted. The following Tables 3 and 4 show these values for CLEF overall. Figures 3 and 4 show detailed analysis for specific tasks.  0.8  Average precision  0.7 Monolingual German  0.6 Monolingual Spanisch  0.5  Bilingual Topic Language  0.4  English  Multilingual English 0.3  Multilingual German 0.2  0.1  0  0  
The present paper notes that the lexical item PO, literally meaning ‘to break’, bears multiple semantic imports in Mandarin Chinese. Given the lack of well-documented research on the semantics of the lexical item, this paper aims to explore the various meanings of PO. By examining the collocations of PO, thirteen meanings are identified, with predicative and attributive senses. It is proposed that the manifold meanings are interrelated with each other and that several meanings are derived from the core verbal meaning of the lexical item. Three generalized metaphors are observed to assume a mediating role in the semantic extensions of PO. In light of the semantic relatedness of the various meanings, the polysemous nature of the lexical item PO is substantiated. Key words: PO, polysemy, semantic extension, lexical semantics 1. Introduction Since a growing number of psychological studies shed new light on human cognition in 1970s, the field of semantics has witnessed flourishing cognitive-oriented approaches to semantic representations of lexicon and grammar—especially lexical semantics and cognitive semantics (Rosch 1973, 1977, 1978, Lakoff and Johnson 1980, Lakoff 1987, 2002, Johnson 1987, Langacker 1987, 1990, 1999, Geerearts 1993, Talmy 1985, 2000a,b, Taylor 1989, 2002a,b, 2003, among others). These cognitive-theoretic proposals have spawned a voluminous literature pertaining to conceptualization, categorization, semantic extension, and grammaticalization of polysemous lexical items in Mandarin Chinese, such as guo ‘to cross’ (Wang 2002, Hsiao 1997, 2003, Wu 2003), gei ‘to give’ (Huang 2004), and kai ‘to open’ (Tsai 2006). When it comes to the issues of polysemy, one point meriting our note is the distinction between homonymy and polysemy. Homonymy refers to the relation between different lexical entries which have unrelated meanings but accidentally exhibit an identical linguistic form, orthographic or phonetic (Ravin and Leacock 2000). A polysemous word, in contrast, is one single lexical item which bears different, but etymologically related, meanings (Lyons 1995, Ravin and Leacock 2000). The English word break is a case of polysemy (Tang 2004), and breken ‘to break’ in Dutch also has multiple meanings (Kellerman 1978). The present  paper observes that the lexical item PO, literally meaning ‘to break’, seems to bear versatile semantic imports in Mandarin Chinese. A question arises as to whether PO is a polyseme or two or more homonyms in Mandarin Chinese. It is noted that studies on the semantics of the lexical item PO, if any, are underrepresented, or even undocumented. Hence, this study aims to probe into the manifold meanings of PO. A cognitive approach will be drawn on to explicate the relations between different semantics of PO and to substantiate the polysemous nature of the lexical item. This paper is organized as follows. Section 2 is concerned with the research background of the present analysis. Section 3 deals with the various senses of PO and proposes a possible account for the semantic relatedness of the manifold meanings. Section 4 concludes this paper. 2. Research Background In linguistics, the theory of prototypes has exerted a momentous impact on lexical semantics and cognitive linguistics (e.g. Rastier 1999, Chu and Chi 1999, Ravin and Leacock 2000). The prototypical category framework has laid theoretical foundations for research on polysemy, and mechanisms for meaning extension have also derived much inspiration from prototypes. The prototypical theory and apparatus for semantic extension are reviewed below. 2.1 The Prototypical Category Theory The human kind seems to have an innate ability for categorization; for example, our brain divides the world into two primary types of entities, things that exist and situations that take place (Huang, Li, and Li 2006). Frameworks for human’s categorization include the classical approach, the prototypical approach, and the relational approach.1 Among them, the notion of prototypes is adopted in this paper. Prototypes are amenable to two interpretations. The concept of prototypes is reminiscent of the renowned American psychologist Eleanor Rosch (1973, 1977, 1978). Rosch introduces the role of prototypes to elucidate human’s categorization. People categorize objects on the basis of the resemblance between the objects and the prototypical members of the category. According to Rosch (1978:36), prototypes can be defined as the ‘clearest cases of category membership defined operationally by people’s judgments of goodness of membership in the category’. A prototype of a category is thus viewed as a salient exemplar of the category. Some instances of a category are more typical than others and hence emerge in human’s mind more easily. For example, robin is a representative, prototypical instance of the category 
In the speech recognition, a mandarin syllable wave is compressed into a matrix of linear predict coding cepstra (LPCC), i.e., a matrix of LPCC represents a mandarin syllable. We use the Bayes decision rule on the matrix to identify a mandarin syllable. Suppose that there are K diﬀerent mandarin syllables, i.e., K classes. In the pattern classiﬁcation problem, it is known that the Bayes decision rule, which separates K classes, gives a minimum probability of misclassiﬁcation. In this study, a set of unknown syllables is used to learn all unknown parameters (means and variances) for each class. At the same time, in each class, we need one known sample (syllable) to identify its own means and variances among K classes. Finally, the Bayes decision rule classiﬁes the set of unknown syllables and input unknown syllables. It is an one-sample speech recognition. This classiﬁer can adapt itself to a better decision rule by making use of new unknown input syllables while the recognition system is put in use. In the speech experiment using unsupervised learning to ﬁnd the unknown parameters, the digit recognition rate is improved by 22%. Key words and phrases: classiﬁcation, dynamic processing algorithm, EM (estimate maximize) algorithm, empirical Bayes, maximum likelihood estimation, speech recognition. ————————————————— Corresponding author address: Tze Fen Li, Institute of Management, Ming Dao University, 369 Wen-Hua Road, Pee-Tow, Chang-Hua (52345), Taiwan, ROC. email address(Tze Fen Li): tﬂi@mdu.edu.tw 1. Introduction 1  A speech recognition system in general consists of feature extractor and classiﬁcation of an utterance [1-5]. The function of feature extractor is to extract the important features from the speech waveform of an input speech syllable. Let x denote the measurement of the signiﬁcant, characterizing features. This x will be called a feature value. The function performed by a classiﬁer is to assign each input syllable to one of several possible syllable classes. The decision is made on the basis of feature measurements supplied by the feature extractor in a recognition system. Since the measurement x of a pattern may have a variation or noise, a classiﬁer may classify an input syllable to a wrong class. The classiﬁcation criterion is usually the minimum probability of misclassiﬁcation [1]. In this study, a statistical classiﬁer, called an empirical Bayes (EB) decision rule, is applied to solving K-class pattern problems: all parameters of the conditional density function f (x | ω) are unknown, where ω denotes one of K classes, and the prior probability of each class is unknown. A set of n unidentiﬁed input mandarin monosyllables is used to establish the decision rule, which is used to separate K classes. After learning the unknown parameters, the EB decision rule will make the probability of misclassiﬁcation arbitrarily close to that of the Bayes rule when the number of unidentiﬁed patterns increases. The problem of learning from unidentiﬁed samples (called unsupervised learning or learning without a teacher) presents both theoretical and practical problems [6-8]. In fact, without any prior assumption, successful unsupervised learning is indeed unlikely. In our speech recognition using unsupervised learning, a syllable is denoted by a matrix of features. Since the matrix has 8x12 feature values, we use a dynamic processing algorithm to estimate the 96 feature parameters (means and variances). Our EB classiﬁer, after unsupervised learning of the unknown parameters, can adapt itself to a better and more accurate decision rule by making use of the unidentiﬁed input syllables after the speech system is put in use. The results of a digit speech experiment are given to show the recognition rates provided by the decision rule.  2. Empirical Bayes Decision Rules for Classiﬁcation  Let X be the present observation which belongs to one of K classes ci, i = 1, 2, · · · , K. Consider the  decision problem consisting of determining whether X belongs to ci. Let f (x | ω) be the conditional density  function of X given ω, where ω denotes one of K classes and let θi, i = 1, 2, · · · , K, be the prior probability  of ci with  K i=1  θi  =  1.  In  this  study,  both  the  parameters  of  f (x  |  ω)  and  the  θi  are  unknown.  Let  d  be  a  decision rule. A simple loss model is used such that the loss is 1 when d makes a wrong decision and the loss is  0 when d makes a correct decision. Let θ = {(θ1, θ2, · · · , θK ); θi > 0,  K i=1  θi  =  1}  be  the  prior  probabilities.  Let R(θ, d) denote the risk function (the probability of misclassiﬁcation) of d. Let Γi, i = 1, 2, · · · , K, be K  2  regions separated by d in the domain of X, i.e., d decides ci when X ∈ Γi. Let ξi denote all parameters of the conditional density function in class ci, i = 1, ..., K. Then  K  R(θ, d) =  θi f (x | ξi)dx  (1)  i=1 Γci  where Γci is the complement of Γi . Let D be the family of all decision rules which separate K pattern  classes. For θ ﬁxed, let the minimum probability of misclassiﬁcation be denoted by  R(θ) = inf R(θ, d).  (2)  d∈D  A decision rule dθ which satisﬁes (2) is called the Bayes decision rule with respect to the prior probability vector θ = (θ1, θ2, · · · , θK ) and given by Ref.[1]  dθ(x) = ci if θi f (x | ξi) > θj f (x | ξj) f or all j = i.  (3)  In the empirical Bayes (EB) decision problem [9], the past observations (ωm, Xm), m = 1, 2, · · · , n, and the present observation (ω, X) are i.i.d., and all Xm are drawn from the same conditional densities, i.e., f (xm | ωm) with p(ωm = ci) = θi. The EB decision problem is to establish a decision rule based on the set of past observations Xn = (X1, X2, · · · , Xn). In a pattern recognition system with unsupervised learning, Xn is a set of unidentiﬁed input patterns. The decision rule can be constructed using Xn to select a decision rule tn(Xn) which determines whether the present observation X belongs to ci. Let ξ = (ξ1, ..., ξK ). Then the risk of tn, conditioned on Xn = xn, is R(θ, tn(xn)) ≥ R(θ) and the overall risk of tn is  n  Rn(θ, tn) = R(θ, tn(xn)) p(xm | θ, ξ) dx1 · · · dxn  (4)  m=1  where p(xm | θ, ξ) is the marginal density of Xm with respect to the prior distribution of classes, i.e., p(xm |  θ, ξ) =  K i=1  θif  (xm  |  ξi).  The  EB  approach  has  been  recently  used  in  many  areas  including  classiﬁcation  [10,11], sequential estimation [12], reliability [13-15], multivariate analysis [16,17], linear models [18,19],  nonparametric estimation [20,21] and some other estimation problems [22,23]. Let  S = {(θ, ξ); θ = (θ1, ..., θK ), ξ = (ξ1, ..., ξK )}  (5)  deﬁne a parameter space of prior probabilities θi and parameters ξi representing the i-th class, i = 1, ..., K.  Let P be a probability distribution on the parameter space S. In this study, we want to ﬁnd an EB decision  rule which minimizes  Rˆn(P, tn) = Rn(θ, tn)dP (θ, ξ).  (6)  3  Similar approaches to constructing EB decision rules can be found in the recent literature [11,15,24]. From (1) and (4), (6) can be written as  K  n  Rˆn(P, tn) =  f (x | ξi)θi p(xm | θ, ξ)dP (θ, ξ) dx dx1 · · · dxn  (7)  i=1 Γci,n  m=1  where, in the domain of X, Γi,n, i = 1, 2, · · · , K, are K regions, separated by tn(Xn), i.e., tn(Xn) decides ci  when X ∈ Γi,n and hence they depend on the past observations Xn. The EB decision rule which minimizes  (7) can be found in Ref[24]. Since the unsupervised learning in this study is based on the following two  theorems given in Ref[24], both theorems and their simple proofs are provided in this paper.  Theorem 1 [24]. The EB decision rule tˆn with respect to P which minimizes the overall risk function (7) is given by  n  tˆn(xn)(x) = ci  if  f (x | ξi) θi p(xm | θ, ξ)dP (θ, ξ) >  m=1 n  (8)  f (x | ξj) θj p(xm | θ, ξ)dP (θ, ξ)  m=1  for all j = i, i.e., Γi,n is deﬁned by the deﬁnition of the inequality in (8).  Proof. To minimize the overall risk (7) is to minimize the integrand  K i=1 Γci,n  n f (x|ξi)θi p(xm|θ, ξ)dP (θ, ξ) dx m=1  of (7) for each past observations xn. Let the past obervations xn be ﬁxed and let i be ﬁxed for i = 1, ..., k.  Let  gi(x) =  n f (x|ξi)θi p(xm|θ, ξ)dP (θ, ξ). m=1  Then the integrand of (7) can be written as  K  gi(x)dx =  gi(x)dx + [  i=1 Γci,n  Γci,n  j=i  gj(x)dx −  gj (x)dx]  Γj,n  =  gj(x)dx +  [gi(x) − gj (x)]dx (Γci,n = Γj,n)  j=i  j=i Γj,n  j=i  which is minimum since Γj,n ⊂ {x|gj(x) > gi(x)} for all j = i by the deﬁnition of Γj,n.  In applications, we let the parameters ξi, i = 1, ..., K, be bounded by a ﬁnite numbers Mi. Let ρ > 0 and δ > 0. Consider the subset S1 of the parameter space S deﬁned by  4  S1 ={(n1ρ, n2ρ, ..., nK ρ, nK+1δ, nK+2δ, ..., n2K δ); integer ni > 0, i = 1, ..., K,  K  (9)  niρ = 1, |niδ| ≤ Mi, integer ni, i = K + 1, ..., 2K}  i=1  where (n1ρ, ..., nK ρ) are prior probabilities and (nK+1δ, ..., n2K δ) are the parameters of K classes. In order  to simplify the conditional density of (θ, ξ), let P be a uniform distribution on S1 so that the conditional  density can later be written as a recursive formula. The boundary for class i relative to another class j as  separated by (8) can be represented by the equation  E[f (x | ξi)θi | xn] = E[f (x | ξj)θj | xn]  (10)  where E[f (x | ξi)θi | xn] is the conditional expectation of f (x | ξi)θi given Xn = xn with the conditional probability function of (θ, ξ) given Xn = xn equal to  h(θ, ξ | xn) =  n m=1  p(xm  |  θ,  ξ)  (θ ξ )∈S1  n m=1  p(xm  |  θ  ,ξ  )  (11)  The actual region for class i as determined by (8) is the intersection of the regions whose borders are given  by (10), relative to all other classes.  The main result in Ref[24] is that the estimates E[θi | Xn] converge almost sure (a.s.) to a point arbitrarily close to the true prior probability and E[ξi|Xn] will converge to a point arbitrarily close to the true parameter in the conditional density for the i-th class. Let λ = (θ1, ..., θK , ξ1, ..., ξK ) in the parameter space S. Let λo be the true parameter of λ.  Lamma 1 (Kullback, 1973 [25]). Let  H(λo, λ) = ln p(x|λ)p(x|λo)dx.  Then the Kullback-Leibler information number H(λo, λo)−H(λo, λ) ≥ 0 with equality if and only if p(x|λ) = p(x|λo) for all x, i.e., H(λo, λ) has an absolutely maximum value at λ = λo.  Let λ = (θ , ξ ) ∈ S1 such that H(λo, λ ) = maxλ∈S1 H(λo, λ). Since S1 has a ﬁnite number of points, H(λo, λ ) − H(λo, λ) ≥ for some > 0 and for all λ ∈ S1. Since H(λo, λ) is a smooth (diﬀerentiable) function of λ ∈ S, the maximum point λ in S1 is arbitrarily close to the true parameter λo in S if the increments δ and ρ are small.  Theorem 2 [24]. Let λo be the true parameter of λ. Let λ = (θ, ξ) in S. The conditional probability  function h(λ|xn) given Xn = xn in (11) has the following property: for each λ ∈ S1,  lim h(λ | xn) = 0 if λ = λ  n→∞  (12)  = 1 if λ = λ  5  and hence E[λ | Xn] converges to λ with probability 1. Proof. H(λo, λ) has an absolutely maximum value at λ = λ on S1. Let λ ∈ S1 and λ = λ . Consider  
The goal of this study is to examine if there is a word superiority effect on perception of three-way contrast of stops in Taiwan Southern Min (TSM). Based on Ganong’s (1980) findings that English participants showed a significant lexical effect in phonetic perception, I hypothesize that there exists a difference in perception between real words and nonwords in TSM. The prediction is that the categorical boundary shifts as lexical status plays a role in perception of stops. Experiment 1 was conducted as a neutral set of perception of TSM bilabial stops b-p-ph. In experiment 2, cases of “nonword-word-nonword” set along the b-p-ph continuum were conducted. Results showed that real words corresponded to a wider range of VOT in the continuum compared to the neutral pattern. The categorical boundaries (both between /b/ and /p/ and between /p/ and /ph/) were found to shift away from the real word sides towards the nonword sides. The lexical effect may be explained by parallel processing in which a higher level of processing (lexical level) interacts with a lower level (phonemic level) in speech perception. Keywords: perception, categorical boundary, VOT, lexical effect, processing 1. Introduction Categorical perception, which represents that sounds within a phonemic category are perceived as indistinguishable regardless of the correct identification of each sound, supports the modular view that speech is perceptually special. In literature, consonants were found to be perceived categorically, but not gradual; that is, although listeners were able to distinguish the consonant stimuli between different phonetic categories with ease, mostly they failed to distinguish the stimuli within categories. Sometimes the perceptual difference between stops attributes only to the initial voicing feature. For instance, the difference between [b], [p] and [ph] is due to voice onset time (VOT): the interval between the stop release and the beginning of vocal cord vibrations. A  positive VOT value means such a lag exists (e.g. the aspirated [ph]); a VOT value of zero represents no delay in voicing; a negative VOT value refers to the phenomenon where the vocal fold vibrations begin before the articulatory release of the stop (the prevoiced [b]). As VOT varies gradually, the identification changes abruptly from one stop to another. This categorical perception drives how the modular view regards speech—as a modular system. However, this raises an interesting question—is it modular in such a way that consonants are always perceived without the affection by other processing information? There are some parallel models proposed for linguistic processing. One of those is the Trace model of speech perception (McClelland & Elman 1986). The model assumes that different levels of processing—features, phonemes, and words—are activated simultaneously during speech perception, which contradicts with the modularity view that phonemic processing in unaffected by higher levels of processing. Ganong (1980) investigated whether auditory word perception affected phonetic categorization. He constructed acoustic continua varying in voice onset time, each with an end being a real word and the other end a nonword (e.g., dash—tash vs. dask—task). The results showed a categorical boundary shift. Thus he argued that the lexical effect must arise at a processing stage sensitive to both lexical and auditory information. Based on previous studies on categorical perception and Ganong’s (1980) findings on the lexical effect that English participants showed a significant lexical effect in phonetic perception, it is hypothesized that there exists a lexical effect—a difference in perception between real words and nonwords. The present study examines a language of a three-way contrast in initial stops, Taiwan Southern Min, to see if there is a word superiority effect on speech perception. The prediction is that real words will correspond to a wider range of VOT in the continuum either comparing to the neutral pattern or to the nonword situation. The logic behind it is that listeners tend to perceive an ambiguous sound as a real word rather than a nonword. If the prediction is true, it should be found that the phoneme boundaries (both between /b/ and /p/ and between /p/ and /ph/) shift away from the real word sides towards the nonword sides. The current research questions are: (1) Where is the categorical boundary between b and p and that between p and ph in Taiwan Southern Min (TSM)? (2) Does lexical status (a real word or nonword) of a sound sequence affect the perception of TSM stops? If it does, how does the categorical boundary shift? (3) Categorical perception may be better explained by the modular theory or parallel processing models?  The pinyin system (including consonants, vowels and tones) adopted in this paper follows “台灣閩南語音標系統” released by Ministry of Education in 1998. Example lexemes were obtained through 臺灣閩南語辭典 by 董 et al. (2001).  Table 1. Tones used in the current study  Tone category Tone value  Example  陰上  53  飽 /pa53/ ‘full’  陰去  11  富 /pu11/ ‘rich’  陽去  33  密 /ba33/ ‘closely’  2. Experiment 1—identification of a neutral set Experiment 1 is an identification task designed to test the categorical boundaries of bilabial stops b, p, and ph in TSM. Serving as a neutral set, stimuli were not informed to participants beforehand about their lexical or non-lexical status in TSM. Rather, they were instructed to pay attention to any change of the consonant category only.  2.1 Methods 2.1.1 Participants Ten native speakers of TSM participated. All of them were graduate students from National Chung Cheng University, with self-reported fluency in TSM.  2.1.2 Materials 21 stimuli from the bilabial neutral continuum pair ba—pa—pha, with VOTs of -100, -90, -80, -70, -60, -50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90 and 100 msec manipulated in Praat (Boersma & Weenink 2007). Voice onset time (VOT) in msec of each syllable-initial stop was measured from the release of the stop closure to the beginning of the oscillating line which demonstrates voicing in the following vowel. These stimuli are adjusted from a TSM male speaker’s natural speech. Among the 21 stimuli, all attributes of sounds are identical except the VOT in syllable initials. The manipulation of prevoicing or aspiration is through deletion or addition from a range of repetitive circular noise lines. One thing to be careful is that the beginning and ending point in selection must match each other in terms of their amplitude position. Otherwise, low pitch noise or unnatural burst would be created during manipulation. Here is what some examples of stimuli look like:  Figure 1. A Stimulus with -100ms in VOT (prevoicing) Figure 2. A Stimulus with 0ms in VOT Figure 3. A Stimulus with 100ms in VOT (aspiration) 2.1.3 Procedure Stimuli were saved as a WAV file and played in a notebook computer. Participants were given a piece of paper where there are three columns (ba, pa, and pha) along the 21 stimuli. They are instructed to judge whether the CV syllable they are listening begins with b, p or ph and then fill in the corresponding column with a check. 2.2 Results and discussion Stimuli and the corresponding VOT values are shown below:  Table 2. Stimuli & Corresponding VOT values  Stimulus number 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  VOT (ms)  -100 -90 -80 -70 -60 -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 100  Percentage of indicated category  b  p  ph  100  100  90  90  80  80  70  70  60  60  50  50  40  40  30  30  20  20  10  10  0  0  
Synset and semantic relation based lexical knowledge base such as wordnet, have been well-studied and constructed in English and other European languages (EuroWordnet). The Chinese wordnet (CWN) has been launched by Academia Sinica basing on the similar paradigm. The synset that each word sense locates in CWN are manually labeled, however, the lexical semantic relations among synsets are not fully constructed yet. In this present paper, we try to propose a lexical pattern-based algorithm which can automatically discover the semantic relations among verbs, especially the troponymy relation. There are many ways that the structure of a language can indicate the meaning of lexical items. For Chinese verbs, we identify two sets of lexical syntactic patterns denoting the concept of hypernymy-troponymy relation. We describe a method for discovering these syntactic patterns and automatically extracting the target verbs and their corresponding hypernyms. Our system achieves satisfactory results and we beleive it will shed light on the task of automatic acquisition of Chinese lexical semantic relations and ontology learning as well. Key word: troponymy, automatic labeling, lexical syntactic pattern  
One of the main difﬁculties in Chinese-Korean cross-language information retrieval is to translate named entities (NE) in queries. Unlike common words, most NE’s are not found in bilingual dictionaries. This paper presents a pattern-based method of ﬁnding NE translations online. The most important feature of our system is that patterns are generated and weighed automatically, saving considerable human effort. Our experimental data consists of 160 Chinese-Korean NE pairs selected from Wikipedia in ﬁve domains. Our approach can achieve a very high MAP of 0.84, which demonstrates our system’s practicability. 摘要 中韓跨語檢索上的困難之處，即在於query term中的專有名詞，由於人類語言的變化無法如同 大多數的一般名詞一樣，能夠在雙語辭典中找到其對應的翻譯詞。本論文提出一種基於翻譯模 板在Web中尋找翻譯詞的方式。由於Web的資料幾乎涵蓋人類到目前為止的所有知識，且會隨 時更新，因此能確保找到翻譯詞的recall。本研究的一大特點在於，所有用於擷取翻譯詞的模 板，均為自動生成，因此不需耗費大量人力來建構。此外，我們會利用訓練資料集來評估各模 板的權重，藉以給與各候選詞適當的信心值。我們採用維基百科的中韓專有名詞pair做為本方 法所需之訓練集與測試集。經實驗過後，我們的方法可以達到MAP 0.84的高分，證明本論文 提出方法的實用性。 Keywords: Chinese-Korean named entity translation, the Web, pattern 關鍵詞：中韓專有名詞翻譯, 網路語料, 模板 
Current readability formulae have often been criticized for being unstable or not valid. They are mostly computed in regression analysis based on intuitively-chosen variables and graded readings. This study explores the relation between text readability and the conceptual categories proposed in Prototype Theory. These categories form a hierarchy: Basic level words like guitar represent the objects humans interact with most readily. They are acquired by children earlier than their superordinate words (or hypernyms) like stringed instrument and their subordinate words (or hyponyms) like acoustic guitar. Therefore, the readability of a text is presumably associated with the ratio of basic level words it contains. WordNet, a network of meaningfully related words, provides the best online open source database for studying such lexical relations. Our preliminary studies show that a basic level word can be identified by its frequency to form compounds (e.g. chair Æ armchair) and the length difference from its hyponyms in average. We compared selected high school English textbook readings in terms of their basic level word ratios and their values calculated in several readability formulae. Basic level word ratios turned out to be the only one positively correlated with the text levels. Keywords: Readability, Ontology, Prototype Theory, WordNet, Basic Level Word 1. Introduction Reading process is the core of language education. Teachers now have access to a vast amount of texts extractable from the Internet inter alia, but the materials thus found are rarely classified according to comprehension difficulty. It is not uncommon to see foreign language teachers using texts not compatible with the students’ reading abilities. Traditional methods of measuring text readability typically rely on the counting of sentences, words, syllables, or characters. However, these formulae have been criticized for being unstable and incapable of providing deeper information about the text. Recently, the focus of readability formula formation has shifted to the search for meaningful predictors and stronger association between the variables and the comprehension difficulty. We start our research by assuming in line with Rosch et al.’s Prototype Theory [1] that words form conceptual hierarchies in that words at different hierarchical levels pose different processing difficulties. This processing difficulty is presumably correlated with the reading difficulty of the text containing the words. Putting the logic into templates, the measurement  of text readability can be done by calculating the average hierarchical levels at which the words of a text fall. Our study comprises two stages. In the preliminary experiments, we utilized WordNet [2], an online lexical database of English, to identify basic level words. In the subsequent experiment, we compared selected readings in terms of their basic level word ratios and their values calculated in several readability formulae. Basic level word ratios turned out to be the only one positively correlated with the text levels. The remainder of this paper is organized as follows: Section 2 reviews the common indices the traditional readability formulae are based on and the criticism they have received. In Section 3, we first review an approach that centers on ontology structure, and then propose our own ontology-based approach. Section 4 is about methodology – how to identify basic level words, and how to assess the validity of our method against other readability formulae. Section 5 reports the results of the assessment and discusses the strength and weaknesses of our approach. In this section, we also suggest what can be done in further research. 2. Literature Review In this section we first summarize the indices of the traditional readability formulae and then give an account of the criticism these formulae face. 2.1 Indices of Readability – Vocabulary, Syntactic, and Semantic Complexity The earliest work on readability measurement goes back to Thorndike [3] where word frequency in corpus is considered an important index. This is based on the assumption that the more frequent a word is used, the easier it should be. Followers of this logic have compiled word lists that include either often-used or seldom-used words whose presence or absence is assumed to be able to determine vocabulary complexity, thus text complexity. Vocabulary complexity is otherwise measured in terms of word length, e.g., the Flesch formula [4] and FOG formula [5]. This is based on another assumption that the longer a word is, the more difficult it is to comprehend [6]. Many readability formulae presume the correlation between comprehension difficulty and syntactic complexity. For Dale and Chall [7], Flesch formula [4], and FOG index [5], syntactic complexity boils down to the average length of sentences in a text. Heilman, Collins-Thompson, Callan, and Eskenazi [8] also take morphological features as a readability index for morphosyntactically rich languages. Das & Roychoudhury’s readability index [9] for Bangla has two variables: average sentence length and number of syllables per word. Flesch [4] and Cohen [10] take semantic factors into account by counting the abstract words of a text. Kintsch [11] focuses on propositional density and inferences. Wiener, M., Rubano, M., and Shilkret, R. [12] propose a scale based on ten categories of semantic  relations including, e.g., temporal ordering and causality. They show that the utterances of fourth-, sixth-, and eighth-grade children can be differentiated on their semantic density scale. Since 1920, more than fifty readability formulae have been proposed in the hope of providing tools to measure readability more accurately and efficaciously [13]. Nonetheless, it is not surprising to see criticism over these formulae given that reading is a complex process. 2.2 Criticism of the Traditional Readability Formulae One type of criticism questions the link between readability and word lists. Bailin and Grafstein [14] argue that the validity of such a link is based on the prerequisite that words in a language remain relatively stable. However, different socio-cultural groups have different core vocabularies and rapid cultural change makes many words out of fashion. The authors also question the validity of measuring vocabulary complexity by word length, showing that many mono- or bi-syllabic words are actually more unfamiliar than longer polysyllabic terms. These authors also point out the flaw of a simple equation between syntactic complexity and sentence length by giving the sample sentences as follows: (1) I couldn’t answer your e-mail. There was a power outage. (2) I couldn’t answer your e-mail because there was a power outage. (2) is longer than (1), thus computed as more difficult, but the subordinator “because” which explicitly links the author’s inability to e-mail to the power outage actually aids the comprehension. The longer passage is accordingly easier than the shorter one. Hua and Wang [15] point out that researchers typically select, as the criterion passages, standard graded texts whose readability has been agreed upon. They then try to sort out the factors that may affect the readability of these texts. Regression analyses are used to determine the independent variables and the parameters of the variables. However, the researchers have no proof of the cause-effect relation between the selected independent variables and the dependent variable, i.e., readability. Challenge to the formula formation is also directed at the selection of criterion passages. Schriver [16] argue that readability formulae are inherently unreliable because they depend on criterion passages too short to reflect cohesiveness, too varied to support between-formula comparisons, and too text-oriented to account for the effects of lists, enumerated sequences and tables on text comprehension. The problems of the traditional readability formulae beg for re-examination of the correlation between the indices and the readability they are supposed to reflect. 3. Ontology-based Approach to Readability Measurement 3.1 An ontology-based method of retrieving information Yan, X., Li, X., and Song, D. [17] propose a domain-ontology method to rank documents on  the generality (or specificity) scale. A document is more specific if it has broader/deeper Document Scope (DS) and/or tighter Document Cohesion (DC). DS refers to a collection of terms that are matched with the query in a specific domain. If the concepts thus matched are associated with one another more closely, then DC is tighter. The authors in their subsequent study [18] apply DS and DC to compute text readability in domain specific documents and are able to perform better prediction than the traditional readability formulae. In what follows we describe the approach we take in this study, which is similar in spirit to Yan et al.’s [18] method. 3.2 An Ontology-based Approach to the Study of Lexical Relations In this small-scaled study, we focus on lexical complexity (or simplicity) of the words in a text and adopt Rosch et al.’s Prototype Theory [1]. 3.2.1 Prototype Theory According to Prototype Theory, our conceptual categorization exhibits a three-leveled hierarchy: basic levels, superordinate levels, and subordinate levels. Imagine an everyday conversation setting where a person says “Who owns this piano?”; the naming of an object with ‘piano’ will not strike us as noteworthy until the alternative “Who owns this string instrument?” is brought to our attention. Both terms are truth-conditionally adequate, but only the former is normally used. The word ‘piano’ conveys a basic level category, while ‘string instrument’ is a superordinate category. Suppose the piano in our example is of the large, expensive type, i.e., a grand piano, we expect a subordinate category word to be used in e.g. “Who owns this grand piano?” only when the differentiation between different types of pianos is necessary. Basic level is the privileged level in the hierarchy of categorical conceptualization. Developmentally, they are acquired earlier by children than their superordinate and subordinate words. Conceptually, basic level category represents the concepts humans interact with most readily. A picture of an apple is easy to draw, while drawing a fruit would be difficult, and drawing a crab apple requires expertise knowledge. Informatively, basic level category contains a bundle of co-occurring features – an apple has reddish or greenish skin, white pulp, and a round shape, while it is hard to pinpoint the features of ‘fruit’, and for a layman, hardly any significant features can be added to ‘crab apple’. Applying the hierarchical structure of conceptual categorization to lexical relations, we assume that a basic level word is easier for the reader than its superordinate and subordinate words, and one text should be easier than another if it contains more basic level words.  3.2.2 WordNet – An Ontology-Based Lexical Database of English WordNet [2] is a large online lexical database of English. The words are interlinked by means of conceptual-semantic and lexical relations. It can be used as a lexical ontology in computational linguistics. Its underlying design principle has much in common with the hierarchical structure proposed in Prototype Theory illustrated in 3.2.1. In the vertical dimension, the hypernym/hyponym relationships among the nouns can be interpreted as hierarchical relations between conceptual categories. The direct hypernym of ‘apple’ is ‘edible fruit’. One of the direct hyponyms of ‘apple’ is ‘crab apple’. Note, however, hypernyms and hyponyms are relativized notions in WordNet. The word ‘crab apple’, for instance, is also a hypernym in relation to ‘Siberian crab apple’. An ontological tree may well exceed three levels. No tags in WordNet tell us which nouns fall into the basic level category defined in Prototype Theory. In the next section we try to retrieve these nouns.  4. Methodology  4.1 Experiment 1  We examined twenty basic level words identified by Rosch et al. [1], checking the word length and lexical complexity of these basic level words and their direct hypernyms as well as direct hyponyms in WordNet [2]. A basic level word is assumed to have these features: (1) It is relatively short (containing less letters than their hypernyms/hyponyms in average); (2) Its direct hyponyms have more synsets1 than its direct hypernyms; (3) It is morphologically simple. Notice that some entries in WordNet [2] contain more than one word. We assume that an item composed of two or more words is NOT a basic level word. A lexical entry composed of two or more words is defined as a COMPOUND in this study. The first word of a compound may or may not be a noun, and there may or may not be spaces or hyphens between the component words of a compound.  Table 1: Twenty basic level words in comparison with their direct hypernyms and hyponyms on (average) word length, number of synsets, and morphological complexity*  Item guitar piano drum apple peach  Basic Level  Direct Hypernym  Direct Hyponym  W. Length M. Complexity W. Length  6  A  18  5  A  18  4  A  20  5  A  7.5  5  A  9  Synsets 
In this paper, we take Determinative-Measure Compounds as an example to demonstrate how the E-HowNet semantic composition mechanism works in deriving the sense representations for all determinative-measure (DM) compounds which is an open set. We define the sense of a closed set of each individual determinative and measure word in E-HowNet representation exhaustively. We then make semantic composition rules to produce candidate sense representations for any newly coined DM. Then we review development set to design sense disambiguation rules. We use these heuristic disambiguation rules to determine the correct context-dependent sense of a DM and its E-HowNet representation. The experiment shows that the current model reaches 88% accuracy in DM identification and sense derivation. 關鍵詞：語意合成，定量複合詞，語意表達，廣義知網，知網 Keywords: Semantic Composition, Determinative-Measure Compounds, Sense Representations, Extended How Net, How Net 1. Introduction Building knowledge base is a time consuming work. The CKIP Chinese Lexical Knowledge Base has about 80 thousand lexical entries and their senses are defined in terms of the E-HowNet format. E-HowNet is a lexical knowledge and common sense knowledge representation system. It was extended from HowNet [1] to encode concepts. Based on the  framework of E-HowNet, we intend to establish an automatic semantic composition mechanism to derive sense of compounds and phrases from lexical senses [2][3]. Determinative-Measure compounds (abbreviated as DM) are most common compounds in Chinese. Because a determinative and a measure normally coin a compound with unlimited versatility, the CKIP group does not define the E-HowNet representations for all DM compounds. Although the demonstrative, numerals, and measures may be listed exhaustively, their combination is inexhaustible. However their constructions are regular [4]. Therefore, an automatic identification schema in regular expression [4] and a semantic composition method under the framework of E-HowNet for DM compounds were developed. In this paper, we take DMs as an example to demonstrate how the E-HowNet semantic composition mechanism works in deriving the sense representations for all DM compounds. The remainder of this paper is organized as follows. The section 2 presents the background knowledge of DM compounds and sense representation in E-HowNet. We’ll describe our method in the section 3 and discuss the experiment result in the section 4 before we make conclusion in the section 5. 2. Background There are numerous studies on determinatives as well as measures, especially on the types of measures.1 Tai [5] asserts that in the literature on general grammar as well as Chinese grammar, classifiers and measures words are often treated together under one single framework of analysis. Chao [6] treats classifiers as one kind of measures. In his definition, a measure is a bound morpheme which forms a DM compound with the determinatives enumerated below. He also divides determinatives word into four subclasses: i.Demonstrative determinatives, e.g. 這” this”, that”那”… ii.Specifying determinatives, e.g. 每”every”, 各” each”… iii.Numeral determinatives, e.g. 二”two”, 百分之三”three percentage”, 四百五十” four hundred and fifty”… iv.Quantitative determinatives, e.g. 一” one”, 滿” full”, 許多” many”… Measures are divided into nine classes by Chao [6]. Classifiers are defined as ‘individual measures’, which is one of the nine kinds of measures. i.classifiers, e.g. 本”a (book)”, 
We propose a new method for organizing the numerous collocates into semantic thesaurus categories. The approach introduces a thesaurus-based semantic classification model automatically learning semantic relations for classifying adjective-noun (A-N) and verb-noun (V-N) collocations into different categories. Our model uses a random walk over weighted graph derived from WordNet semantic relation. We compute a semantic label stationary distribution via an iterative graphical algorithm. The performance for semantic cluster similarity and the conformity of semantic labels are both evaluated. The resulting semantic classification establishes as close consistency as human judgments. Moreover, our experimental results indicate that the thesaurus structure is successfully imposed to facilitate grasping concepts of collocations. It might improve the performance of the state-of-art collocation reference tools. Keywords: Collocations, Semantic classification, Semantic relations, Random walk algorithm, Meaning access index. 1. Introduction Submitting queries (e.g., a search keyword “beach” for a set of adjective collocates) to collocation reference tools typically return many collocates (e.g., collocate adjectives with a pivot word “beach”: “rocky”, “golden”, “beautiful”, “pebbly”, “splendid”, “crowded”, “superb”, etc.) extracted from a English corpus. Applications of automatic extraction of collocations such as TANGO (Jian, Chang & Chang, 2004) have been created to answer queries of collocation usage. Unfortunately, existing collocation reference tools sometimes present too much information in a batch for a single screen. With web corpus sizes rapidly growing, it is not uncommon to find thousands collocates for a query word. An effective reference tool might strike a balance between quantity and accessibility of information. To satisfy the need for presenting a digestible amount of information, a promising approach is to automatically partition words into various categories to support meaning access to search results and thus give a thesaurus index. Instead of generating a long list of collocates, a good, better presentation could be composed of clusters of collocates inserted into distinct semantic categories. We present a robust thesaurus-based classification model that automatically group collocates of a given pivot word focusing on: (1) the adjectives in adjective-noun pairs (A-N); (2) the verbs in verb-noun pairs (V-N); and (3) the nouns in verb-noun pairs (V-N) into semantically related classes. Our model has determined collocation pairs that learn the semantic labels automatically during random walk algorithm by applying an iterative graphical approach and partitions collocates for each collocation types (A-N, V-N and V-N mentioned above). At runtime, we start with collocates in question with a pivot word, which is to be assigned under a set of semantically  related labels for the semantic classification. An automatic classification model is developed for collocates from a set of A-N and V-N collocations. A random walk algorithm is proposed to disambiguate word senses, assign semantic labels and partition collocates into meaningful groups. As part of our evaluation, two metrics are designed. We assess the performance of collocation clusters classified by a robust evaluation metric and evaluate the conformity of semantic labels by a three-point rubric test over collocation pairs chosen randomly from the results. Our results indicate that the thesaurus structure is successfully imposed to facilitate grasping concepts of collocations and to improve the functionality of the state-of-art collocation reference tools. 2. Related Work 2.1 Collocations 
The performance of an automatic speech recognition system is often degraded due to the embedded noise in the processed speech signal. A variety of techniques have been proposed to deal with this problem, and one category of these techniques aims to normalize the temporal statistics of the speech features, which is the main direction of our proposed new approaches here. In this thesis, we propose a series of noise robustness approaches, all of which attempt to normalize the modulation spectrum of speech features. They include equi-ripple temporal filtering (ERTF), least-squares spectrum fitting (LSSF) and magnitude spectrum interpolation (MSI). With these approaches, the mismatch between the modulation spectra for clean and noise-corrupted speech features is reduced, and thus the resulting new features are expected to be more noise-robust. Recognition experiments implemented on Aurora-2 digit database show that the three new approaches effectively improve the recognition accuracy under a wide range of noise-corrupted environment. Moreover, it is also shown that they can be successfully  combined with some other noise robustness approaches, like CMVN and MVA, to achieve a more excellent recognition performance. 關鍵詞：語音辨識、調變頻譜正規化、強健性語音特徵參數 keyword: speech recognition, modulation spectrum, robust speech features 一、緒論 自動語音辨識系統(automatic speech recognition systems, ASR)，藉由多年來各方學者的 研究發展，逐漸達到實際應用的階段，而為人類生活帶來更多方便與幫助，雖然還不能 達到一個完美的地步，但是這方面的技術仍一直不斷地進步當中。 自動化語音辨認仍有許多相當具有挑戰性的研究課題，由於語音的變異性太多，例 如每位語者說話的方式與口氣都不一樣、不同語言有不同的特性、語者當時說話的情 緒、語者所處的環境是否有其他雜訊干擾等，這些變異對於語音辨識效果都有影響。在 真實應用環境下，語音辨識系統所遇到的主要問題其中兩個，分別為： （一）語者不匹配(speaker mismatch) 語者不匹配的問題是因為說話者先天條件（如口腔形狀）與後天習慣（如說話腔調） 的差異所產生的變異性，因此當以特定語者所訓練出來的聲學模型來辨識不屬於此特定 語者的語音時，辨識效果常會明顯下降，而要克服這一類問題的方法，通常是使用所謂 的語者調適(speaker adaptation)技術。也就是將原本訓練出來的聲學模型調適成接近當 下語者之語音特性的模型[1]，如此便可提高辨識率。 （二）環境不匹配(environment mismatch) 環境不匹配的問題是因為語音辨識系統訓練環境與我們實驗或應用時的環境不同 所致，其變異因子主要包含了加成性雜訊(additive noise)，如車站四周的雜訊、嘈雜街 道的人聲或車聲等，及摺積性雜訊(convolutional noise)，如不同的有線或無線電話線路 或麥克風所造成的通道效應等，語音辨識系統常會因這些雜訊的影響使辨識率降低。下 圖一為乾淨語音受雜訊干擾之示意圖。 圖一、乾淨語音受雜訊干擾之示意圖 在諸多降低雜訊影響、改進語音特徵的強健性技術中，有一大類的方法其目標是找出一 強健語音特徵表示式(robust speech feature representation)，降低語音特徵對雜訊的敏感 度，使雜訊產生的失真變小。此類著名的方法包括了倒頻譜平均消去法(cepstral mean subtraction, CMS)[2] 、 倒 頻 譜 平 均 與 變 異 數 正 規 化 法 (cepstral mean and variance normalization, CMVN)[3]、相對頻譜法(RelAtive SpecTrAl, RASTA)[4]、倒頻譜平均與變 異數正規化化結合自動回歸動態平均濾波器法(cepstral mean and variance normalization  plus auto-regressive-moving-average filtering, MVA)[5]、倒頻譜增益正規化法 (cepstral gain normalization, CGN) [6]、資料導向時間序列濾波器法(data-driven temporal filter design)[7]等。以上這些方法皆是在語音特徵的時間序列域(temporal domain)作處理，根 據語音訊號與雜訊在時間序列域上不同的特性，強調出語音的成分，而抑制雜訊的影響。 近來，新加坡大學之李海洲博士研究團隊，新推出了一套時間序列濾波器設計的新方 法，稱為『時間序列結構正規化法』(temporal structure normalization, TSN)[8]，此方法 的目的，在於將語音特徵序列之功率頻譜密度(power spectral density)正規化，使其輪廓 逼近於一參考功率頻譜密度，此方法所得的時間序列濾波器，可以因應不同雜訊環境的 語句特徵而加以調適，在其文獻[8]可知，當此新方法所得的時間序列濾波器作用於 CMVN 與 MVA 處理後的梅爾倒頻譜特徵參數時，在各種雜訊環境下所得到的語音辨識 精確率都能有大幅改進。 雖然 TSN 法對語音特徵具有優異的強健化效果，且執行複雜度極低，但根據我們的觀 察，此法仍然有幾點可以改進之處，首先，TSN 所得的初始濾波器係數是參考頻率響 應之反傅利葉轉換求得，然後將這些係數會乘上一個漢寧窗(Hanning window)以減緩不 當高頻成份的產生，此求取濾波器的方法未必是最佳化的，所得之濾波器係數其頻率響 應可能與參考頻率響應之間的誤差較大。其次，在 TSN 法中，濾波器係數和被正規化 為 1，代表其直流增益為一定值，此步驟使正規化後的特徵參數其功率頻譜密度並不會 趨近參考功率頻譜密度，只在輪廓上大致相同。最後一點，則是 TSN 法皆是根據 MVN 或 MVA 處理後的梅爾倒頻譜特徵所設計，進而得到良好的效能，我們希望能探討 TSN 法單純作用於未經任何處理的梅爾倒頻譜特徵時，其效果是否也一樣明顯。 根據以上對 TSN 法的分析與觀察，在本論文中，我們提出了三種語音特徵時間序列之 調變頻譜正規化(modulation spectrum normalization)的新方法，分別為等漣波時間序列濾 波器法(equi-ripple temporal filtering, ERTF)、最小平方頻譜擬合法(least-squares spectrum fitting, LSSF)與強度頻譜內插法(magnitude spectrum interpolation, MSI)，這三種方法之目 的與 TSN 類似，皆為了正規化語音特徵時間序列的功率頻譜密度，但我們會在後面章 節的實驗結果發現，這三種方法之效能皆比 TSN 法來得好，且並不需要與 MVN 或 MVA 法結合，即可以十分有效地處理梅爾倒頻譜特徵因雜訊干擾所造成的失真。然而，當它 們與 MVN 或 MVA 相結合時，也可以得到更佳的辨識精確率，此代表它們與 MVN 或 MVA 有良好的加成性。 本論文其餘的章節概要如下：在第二章，我們將討論時間序列結構正規化法，包括其執 行程序及初步效果，第三章為本論文的重點，我們將在此章中針對時間序列結構正規化 法作改進，而提出三種新的調變頻譜正規化法，並對其初步效果加以介紹。在第四章， 我們將執行一系列的語音辨識實驗，來驗證所提之新方法足以有效提昇語音特徵在雜訊 環境下的強健性，最後，第五章則為結論及未來展望。 二、時間序列結構正規化法(temporal structure normalization, TSN) （一）TSN 處理簡介 本章節主要介紹時間序列結構正規化法(temporal structure normalization, TSN)[8]， 在下一章中，我們將以 TSN 法之觀念為基礎，提出一系列的調變頻譜正規化的演算法。 TSN 是屬於一種時間序列濾波器(temporal filter)設計之強健性語音技術，原始的 MFCC 語音特徵參數序列經過 CMVN 法[3]或 MVA 法[5]處理後，先求取其功率頻譜密度(power spectral density)，接著藉由此功率密度與事先定好的參考功率密度來決定一濾波器的強 度響應(magnitude response)，此強度響應經反離散傅立葉轉換(inverse discrete Fourier transform, IDFT)、漢寧窗化(Hanning window)處理與直流增益正規化處理後，產生一組  濾波器係數，此即為 TSN 法所求得的時間序列濾波器，將語音特徵序列通過此濾波器 後，則預期可達到調變頻譜正規化的效果，而增加語音特徵之其強健性。圖二為 TSN 法的處理程序示意圖：  處理前的特徵序列  時間序列濾波器  處理後的特徵序列  x [n ]  h (n)  y [n ]  功率 頻譜密度  PXX (ω)  H (ω) IDFT  Hanning Window  DC Gain Normalization  h (n)  乾淨語句特徵 參數序列  PSS (ω) 參考之功率頻譜密度 圖二、TSN 法處理程序示意圖  在 TSN 法中，每一句訓練語料之某一維特徵序列 {s [n ]} 與測試語料同一維特徵序  列  {x  [n  ]}  ，先求取其功率頻譜密度，分別以  {PSS  (ω k  )}  與  {PXX  (ω k  )}  表示。接著將訓練語  料所有句子同一維的功率頻譜密度作平均，所得即為參考功率頻譜密度，如下所示：  PSS  (ω k  )  =  E  {PSS  (ω k  )},  (式 2.1)  在 TSN 法中所使用的濾波器，其初始的強度頻譜設定如下式所示：  H  (ω ) k  =  PSS  (ω ) k  PXX  (ω k  ),  (式 2.2)  其上式明顯看出，當任一測試語料 x [n ] 通過上式之濾波器時，其原始功率頻譜密度  PXX  (ω k  )  會被正規化為  PSS  (ω k  )  。  為了進一步求取濾波器的脈衝響應(impulse  response)，上式(2.2)中的  H  (j  ω k  )  先經  過反離散傅立葉轉換(inverse discrete Fourier transform, IDFT)，之後再乘上一個漢寧窗  (Hanning window)，並將濾波器係數總和正規化為 1，以達到直流增益正規化的目的。  其數學表示式如以下數式所示：  1、反離散傅立葉轉換：  ∑ h [m ] =  
A realistic Chinese word segmentation tool must adapt to textual variations with minimal training input and yet robust enough to yield reliable segmentation result for all variants. Various lexicon-driven approaches to Chinese segmentation, e.g. [1,16], achieve high f-scores yet require massive training for any variation. Text-driven approach, e.g. [12], can be easily adapted for domain and genre changes yet has difficulty matching the high f-scores of the lexicon-driven approaches. In this paper, we refine and implement an innovative text-driven word boundary decision (WBD) segmentation model proposed in [15]. The WBD model treats word segmentation simply and efficiently as a binary decision on whether to realize the natural textual break between two adjacent characters as a word boundary. The WBD model allows simple and quick training data preparation converting characters as contextual vectors for learning the word boundary decision. Machine learning experiments with four different classifiers show that training with 1,000 vectors and 1 million vectors achieve comparable and reliable results. In addition, when applied to SigHAN Bakeoff 3 competition data, the WBD model produces OOV recall rates that are higher than all published results. Unlike all  previous work, our OOV recall rate is comparable to our own F-score. Both experiments support the claim that the WBD model is a realistic model for Chinese word segmentation as it can be easily adapted for new variants with robust result. In conclusion, we will discuss linguistic ramifications as well as future implications for the WBD approach. Keywords: segmentation. 1. Background and Motivation The paper deals with the fundamental issue why Chinese word segmentation remains a research topic and not a language technology application after more than twenty years of intensive study. Chinese text is typically presented as a continuous string of characters without conventionalized demarcation of word boundaries. Hence tokenization of words, commonly called word segmentation in literature, is a pre-requisite first step for Chinese language processing. Recent advances in Chinese word segmentation (CWS) include popular standardized competitions run by ACL SigHAN and typically high F-scores around 0.95 from leading teams [8]. However, these results are achieved at the cost of high computational demands, including massive resources and long machine learning time. In fact, all leading systems are expected to under-perform substantially without prior substantial training. It is also important to note that SigHAN competitions are conducted under the assumption that a segmentation program must be tuned separately for different source texts and will perform differently. This is a bow to the fact that different communities may conventionalize the concept of word differently; but also an implicit concession that it is hard for existing segmentation programs to deal with textual variations robustly. [15] proposed an innovative model for Chinese word segmentation which formulates it as simple two class classification task without having to refer to massive lexical knowledge base. We refine and implement this Word Boundary Decision (WBD) model and show that it is indeed realistic and robust. With drastically smaller demand on computational resources, we achieved comparable F-score with leading Bakeoff3 teams and outperform all on OOV recall, the most reliable criterion to show that our system deals with new events effectively. In what follows, we will discuss modeling issues and survey previous work in the first section. The WBD model will be introduced in the second section. This is followed by a description of the machine learning model is trained in Section 4. Results of applying this implementation to SigHAN Bakeoff3 data is presented in Section 5. We conclude with  discussion of theoretical ramifications and implications in Section 6. 2. How to model Chinese word segmentation The performance of CWS systems is directly influenced by their design criteria and how Chinese word segmentation task is modeled. These modeling issues did not receive in-depth discussion in previous literature: Modeling Segmentation. The input to Chinese word segmentation is a string of characters. However, the task of segmentation can be modeled differently. All previous work share the assumption that the task of segmentation is to find our all segments of the string that are words. This can be done intuitively by dictionary lookup, or be looking at strength of collocation within a string, e.g. [12]. Recent studies, e.g. [14, 16, 5, 17], reduce the complexity of this model and avoided the thorny issue of the elusive concept of word at the same time by modeling segmentation as learning the likelihood of characters being the edges of these word strings. These studies showed that, with sufficient features, machine learning algorithms can learn from training corpus and use their inherent model to tokenize Chinese text satisfactorily. The antagonistic null hypothesis of treating segmentation as simply identifying inherent textual breaks between two adjacent characters was never pursued. Out-of-Vocabulary Words. Identification of Out-of Vocabulary words (OOV, sometimes conveniently referred to as new words) has been a challenge to all systems due to data sparseness problem, as well as for dealing with true neologisms which cannot be learned from training data per se. This requirement means that CWS system design must incorporate explicit or implicit morphology knowledge to assure appropriate sensitivity to context in which potential words occur as previously unseen character sequences. Language Variations. Especially among different Chinese speaking communities. Note that different Chinese speaking communities in PRC, Taiwan, Hong Kong Singapore etc. developed different textual conventions as well as lexical items. This is compounded by the usual text type, domain, and genre contrasts. A robust CWS system must be able to adapt to these variations without requiring massive retraining. A production environment with it's time restrictions possesses great demands on the segmentation system to be able to quickly accommodate even to mixture of text types, since such a mixture would introduce confusing contexts and confuse system that would rely too heavily on text type, i.e. particular lexicon choice and specific morphology, and too large a context.  Space and time demands. Current CWS systems cannot avoid long training times and large memory demands. This is a consequence of the segmentation model employed. This is acceptable when CWS systems are used for offline tasks such as corpora preprocessing, where time and space can be easily provided and when needed. However, for any typically web-based practical language engineering applications, such high demand on computing time is not acceptable. 2.1 Previous works: a critical review Two contrasting approaches to Chinese word segmentation summarize the dilemma of segmentation system design. A priori, one can argue that segmentation is the essential tool for building a (mental) lexicon hence segmentation cannot presuppose lexical knowledge. On the other hand, as a practical language technology issue, one can also argue that segmentation is simply matching all possible words from a (hypothetical) universal lexicon and can be simplified as mapping to a large yet incomplete lexicon. Hence we can largely divide previous approaches to Chinese word segmentation as lexicon-driven or text-driven. Text-Driven. Text-driven approach to segmentation relies on contextual information to identify words and do not assume any prior lexical knowledge. Researches in this approach typically emphasize the need for an empirical approach to define the concept of a word in a language. [12] work based on mutual information (MI) is the best-known and most comprehensive in this approach. The advantage of this approach it can be applied to all different variations of language and yet be highly adaptive. However, the basic implementation of MI applies bi-syllabic words only. In addition, it cannot differentiate between highly collocative bigrams (such as 就不 jiubu ―…then not…‖) and words. Hence it typically has lower recall and precision rate than current methods. Even though text-driven approaches are no longer popular, they are still widely used to deal with OOV with a lexicon-driven approach. Tokenization. The classical lexicon-driven segmentation model, described in [1] and is still adopted in many recent works. Segmentation is typically divided into two stages: dictionary look up and OOV word identification. This approach requires comparing and matching tens of thousands of dictionary entries in addition to guessing a good number of OOV words. In other words, it has a 104 x 104 scale mapping problem with unavoidable data sparseness. This model also has the unavoidable problem of overlapping ambiguity where e.g. a string [Ci-1, Ci, Ci+1] contains multiple sub-strings, such as [Ci-1, Ci] and [Ci, Ci+1], which are entries in the  dictionary. The degree of such ambiguities is estimated to fall between 5% to 20% [2, 6]. Character classification. Character classification or tagging, first proposed in [14], became a very popular approach recently since it is proved to be very effective in addressing problems of scalability and data sparseness [14, 4, 16, 17]. Since it tries to model the possible position of a character in a word as character-strings, it is still lexicon-driven. This approach has been also successfully applied by to name entity resolution, e.g. [17]. This approach is closely related to the adoption of the machine learning algorithm of conditional random field (CRF), [7]. CRF has been shown [11] to be optimal algorithm for sequence classification. The major disadvantages are big memory and computational time requirement. 3. Model Our approach is based on a simplified idea of Chinese text, which we have introduced earlier in [15]. Chinese text can be formalized as a sequence of characters and intervals as illustrated in Figure 1. c1, I 1, c2, I 2, ... , cn− 1, I n− 1, cn Figure 1: Chinese text formalization There is no indication of word boundaries in Chinese text, only string of characters ci. Characters in this string can be conceived as being separated by interval Ii. To obtain a segmented text, i.e. a text where individual words are delimited by some graphical mark such as space, we need to identify which of these intervals are to be replaced by such word delimiter. We can introduce a utility notion of imaginary intervals between characters, which we formally classify into two types: Type 0: a character boundary (CB) is an imaginary boundary between two characters Type 1: a word boundary (WB), an interval separating two words. With such a formulation, segmentation task can be easily defined as a classification task and machine learning algorithms can be employed to solve it. For conventional machine learning algorithms, classifications are made based on a set of features, which identify certain properties of the target to be classified. In a segmented text, all the intervals between characters are labeled as a word boundary or as  a character boundary, however, characters are not considered as being part of any particular word. Their sole function is to act as a contextual aid for identification of the most probable interval label. Since the intervals between characters (be it a word boundary or a character boundary) don't carry any information at all, we need to rely on the information provided by group of characters surrounding them. Now we can collect n-grams that will provide data for construction of features that will provide learning basis for machine learning algorithm. A sequence, such the one illustrated in Figure 1, can be obtained from segmented corpus, and hence the probability of word boundary with specified relation to each n-gram may be derived. The resulting table which consists of each distinct n-gram entry observed in the corpus and the probability of a word  bi-grams  preceeding containing  following  CCB CB CBC BC BCC  unigrams  Figure 2: The feature vectors used in this study. While C denotes a character in the sequence, B indicates the imaginary boundary. Thus CBC denotes a bi-gram containing the interval. boundary defines our n-gram collection.  Figure 2 shows the format of the feature vectors, or interval vectors, used in this study. We build the n-gram model up to n = 2.  To allow for a more fine-grained statistical information we have decomposed an interval  Basic corpus  Other corpus  CCB, CB, CBC, BC, BCC  N-gram collection  Training data  Classifier  Testing data  Segmented text  surrounding context into two unigrams and three bi-grams. For convenience, we can define each interval by the two characters that surround it. Then, for each interval <b,c> in a 4-character window abcd we collect two unigrams b and c and three bi-grams ab, bc, cd and compute probability of that interval being a word boundary. These five n-grams are stored in a vector, which is labeled as Type 0 (character boundary) or Type 1 (word boundary): <ab, b, bc, c, cb, 0> or <ab, b, bc, c, cb, 1>. An example of an encoding of a sample from the beginning of Bakeoff 3 AS training corpus: "時間：三月十日" (shijian:sanyueshiri), which  Table 1: Example of encoding and labeling of interval vectors in a 4-character window ABCD would be correctly segmented as "時間 ： 三月十日" (shijian : sanyue shiri) can be seen in Table 1. Set of such interval vectors provides a training corpus on which we apply machine learning algorithm, in our case logarithmic regression. Unsegmented text is prepared in the same fashion and the interval vectors are subsequently labeled by a classifier. 4. Training the Machine Learning Model It is our goal to develop a segmentation system that would be able handle different types of  text. A large uniform training corpus is desirable for high precision of segmentation, but that would cause a specialization of the classifier to types of texts contained in the corpus and system's generality would be compromised. Furthermore, using a training data set converted from an independent corpus may give supplementary information and provide certain adaptation mechanism for the classifier during training, but leave the basic n-gram collection untouched. However, a smaller set of Table 2: Performance during training training data may give similar performance but with much lower cost. If the features in the n-gram collection are properly defined, the final results from different machine learning algorithms may not differ too much. On the contrary, if the available n-gram collection does not provide efficient information, classifiers with ability to adjust the feature space may be necessary. In our preliminary tests, during which we wanted to decide which machine learning algorithm would be most appropriate, the Academia Sinica Balance Corpus (ASBC) is used for the derivation of the n-gram collection and training data. The CityU corpus from the SigHAN Bakeoff2 collection is used for testing. In order to verify the effect of the size of the training data, the full ASBC (~17 million intervals) and a subset of it (1 million randomly selected intervals) are used for training separately. Table 3: Performance during testing  Furthermore, four different classifiers, i.e., logistic regression (LogReg) [9], linear discriminative analysis (LDA)[13], multi-layer perceptron (NNET)[13], and support vector machine (SVM)[3], were tested. The segmentation results are compared with the "gold standard" provided by the SigHAN Bakeoff2. Table 4: Performance during training: new corpus Tables 2 and 3 show the training and testing accuracies of various classifiers trained with the ASBC. All classifiers tested perform as expected, with their training errors increase with the size of the training data, and the testing errors decrease with it. Table 2 clearly shows that the training data size has little effect on the testing error while it is above 1000. This proves that once a sufficient n-gram collection is provided for preparation of the interval vectors, classifier can be trained with little input. It is also shown in Table 2 that four classifiers give similar performance when the training data size is above 1000. However, while the training sample size drops to 100, the SVM and LDA algorithms show their strength by giving similar performance to the experiments trained with larger training data sets. Table 5: Performance during testing: new corpus  To further explore the effectiveness of our approach, we have modified the experiment to show the performance in model adaptation. In the modified experiments the training and testing data sets are both taken from a foreign corpus (CityU), while our n-gram collection is still from ASBC. The relation between the derived features and the true segmentation may be different from the ASBC, and hence is learned by the classifiers. The results of the modified experiments are shown in Tables 4 and 5. 5. Results In our test to compare our performance objectively with other approaches, we adopt logarithmic regression as our learning algorithm as it yielded best results during our test. We apply the segmentation system to two traditional Chinese corpora, CKIP and CityU, provided for SigHAN Bakeoff 3. In the first set of tests, we used training corpora provided by SigHAN Bakeoff3 for n-gram collection, training and testing. Results of these tests are presented in Table 6.  Table 7: Results (Bakeoff 3 dataset): traditional Chinese  Table 6: Combined results (Bakeoff 3 dataset): traditional Chinese  In addition, to underline the adaptability of this approach, we also tried combining both corpora and then ran training on random sample of vectors. This set of tests is designed to exclude the possibility of over-fitting and to underline the robustness of the WBD model. Note that such tests are not performed in SigHAN Bakeoffs as many of the best performances are likely over-fitted. Results of this test are shown in Table 7.  Table 6 and 7 show that our OOV recall is comparable with our overall F-score, especially when our system is trained on selected vectors from combined corpus. This is in direct contrast with all existing systems, which typically has a much lower OOV recall than IV recall. In other words, our approach applies robustly to all textual variations with reliably good results. Table 8 shows that indeed our OOV recall rate shows over 16% improvement  over the best Bakeoff3 result for CityU, and over 27% improvement over best result for CKIP data. Table 8: Our OOV recall results compared to best performing systems in (Levow, 2006) 6. Discussion We refined and implemented the WBD model for Chinese word segmentation and show that it is a robust and realistic model for Chinese language technology. Most crucially, we show that the WBD model is able to reconcile the two competitive goals of the lexicon-driven and text-driven approaches. The WBD model maintains comparable F-score level with the most recent CRF character-classification based results, yet improves substantially on the OOV recall. We showed that our system is robust and not over-fitted to a particular corpus, as it yields comparable and reliable results for both OOV and IV words. In addition, we show that same level of consistently high results can be achieved across different text sources. Our results show that Chinese word segmentation system can be quite efficient even when using very simple model and simple set of features. Our current system, which has not been optimized for speed, is able to segment text in less then 50 seconds. Time measurement includes preparation of testing data, but also training phase. We believe that with optimized and linked computing power, it will be easy to implement a real time application system based on our model. In the training stage, we have shown that sampling of around 1,000 vectors is enough to yield one of the best results. Again, this is a promise fact for the WBD model of segmentation to be robust. It is notable, that in case of training on combined corpora (CKIP and CityU) the results are even better than test in respective data sets, i.e. CKIP training corpus for segmenting CKIP testing text, or CityU respectively. This is undoubtedly the result of our strategy of granulation of the context around each interval. Since four characters that we use for representation of the interval context are broken up into two unigrams and three bi-grams, we let the system to get more  refined insight into the segmented area. Consequently, the system is learning morphology of Chinese with greater generality and this results in higher OOV scores. It can be argued that in our combined corpora test, the OOV recall is even higher, because the input contains two different variants of Chinese language, Taiwanese variant contained in CKIP corpus and Hong Kong variant contained in CityU corpus. Text preparation and post-processing also add to overall processing time. In our current results, apart from context vector preparation there was no other preprocessing employed and neither any post-processing. This fact also shows that our system is able to handle any type of input without the need to define special rules to pre- or post-process the text. Early results applying our model to simplified Chinese corpora are also promising. In sum, our WBD model for Chinese word segmentation yields one of the truly robust and realistic segmentation program for language technology applications. If these experiments are treated as simulation, our results also support the linguistic hypothesis that word can be reliably discovered without a built-in/innate lexicon. We will look into developing a more complete model to allow for more explanatory account for domain specific shifts as well as for effective bootstrapping with some lexical seeds. References [1] K.J Chen and S.H. Liu, ―Word Identification for Mandarin Chinese sentences―, in Proceedings of the 14th conference on Computational Linguistics, pp.101-107, 1992. [2] T.-H. Chiang, J.-S. Chang, M.-Y. Lin and K.-Y. Su, ―Statistical Word Segmentation‖, in C.-R. Huang, K.-J. Chen and B.K. T’sou (eds.): Journal of Chinese Linguistics, Monograph Series, Number 9, Readings in Chinese Natural Language Processing, pp. 147-173, 1996. [3] E. Dimitriadou, K. Hornik, F. Leisch, D. Meyer and A.Weingessel ―e1071: Misc. Functions of the Department of Statistics (e1071)‖, TU Wien R package version 1.5-17., 2007. [4] J. Gao, A. Wu, M. Li, C.-N. Huang, H. Li, X. Xia and H.Qin, ―Adaptive Chinese Word Segmentation‖, in Proceedings of ACL-2004, 2004.  [5] C.-N. Huang and H. Zhao, ―Which Is Essential for Chinese Word Segmentation: Character versus Word‖, The 20th Pacific Asia Conference on Language, Information and Computation (PACLIC-20), pp.1-12, 2006. 
Propositional terms in a research abstract (RA) generally convey the most important information for readers to quickly glean the contribution of a research article. This paper considers propositional term extraction from RAs as a sequence labeling task using the IOB (Inside, Outside, Beginning) encoding scheme. In this study, conditional random fields (CRFs) are used to initially detect the propositional terms, and the combined association measure (CAM) is applied to further adjust the term boundaries. This method can extract beyond simply NP-based propositional terms by combining multi-level features and inner lexical cohesion. Experimental results show that CRFs can significantly increase the recall rate of imperfect boundary term extraction and the CAM can further effectively improve the term boundaries. 摘要 命題術語(Propositional Term)表達文章中重要概念且引導讀者文章脈絡之發展。這篇論 文以學術論文摘要為實驗對象進行命題術語擷取，研究中整合條件隨機域(Conditional Random Fields, CRFs) 以及結合聯繫測量(Combined Association Measure, CAM) 兩種方 法，考量詞彙內部凝聚力和文脈兩大類訊息，截取出的命題術語不再侷限於名詞片語型 態，且可由單詞或多詞所構成。在命題術語擷取的過程中，將其視為一種序列資料標籤 的任務，並利用 IOB 編碼方式識別命題述語的邊界，CRF 考量多層次構成命題述語的 特徵，負責初步命題術語偵測，再利用 CAM 計算詞彙凝聚力，藉以加強確認命題術語 詞彙的邊界。實驗結果顯示 ，本研究所提出的方法比以往述語偵測方法在效能上有明 顯增進，其中，CRF 明顯增進非完美術語詞彙邊界辨識(Imperfect hits)的召回率，而 CAM 則有效修正術語詞彙邊界。 Keywords: Propositional Term Extraction, Conditional Random Fields, Combined Association Measure, Multi-Level Feature 關鍵詞：命題述語擷取，條件隨機域，結合聯繫測量，多層次特徵  1. Introduction Researchers generally review Research Abstracts (RAs) to quickly track recent research trends. However, many non-native speakers experience difficulties in writing and reading RAs [1]. The author-defined keywords and categories of the research articles currently utilized to provide researchers with access to content guiding information are cursory and general. Therefore, developing a propositional term extraction system is an attempt to exploit the linguistic evidence and other characteristics of RAs to achieve efficient paper comprehension. Other applications of the proposed method contain sentence extension, text generation, and content summarization. A term is a linguistic representation of a concept with a specific meaning in a particular field. It may be composed of a single word (called a simple term), or several words (a multiword term) [2]. A propositional term is a term that refers to the basic meaning of a sentence (the proposition) and helps to extend or control the development of ideas in a text. The main difference between a term and a propositional term is that a propositional term, which can guide the reader through the flow of the content, is determined by not only syntax or morphology but semantic information. Take RAs to illustrate the difference between a term and a propositional term. Cheng [3] indicted that a science RA is composed of background, manner, attribute, comparison and evaluation concepts. In Figure 1, the terms underlined are the propositional terms which convey the important information of the RA. In the clause “we present one of the first robust LVCSR systems that use a syllable-level acoustic unit for LVCSR, ＂ the terms “ LVCSR systems ＂ , “ syllable-level acoustic unit ＂ and “LVCSR＂ respectively represent the background, manner and background concepts of the research topic, and can thus be regarded as propositional terms in this RA. The background concepts can be identified by clues from the linguistic context, such as the phrases “most…LVCSR systems＂ and “in the past decade＂, which indicate the aspects of previous research on LVCSR. For the manner concept, contextual indicators such as the phrases “present one of…＂, “that use＂ and “for LVCSR＂ express the aspects of the methodology used in the research. Propositional terms may be composed of a variety of word forms and syntactic structures and thus may not only be NP-based, and therefore cannot be extracted by previous NP-based term extraction approaches. Most large vocabulary continuous speech recognition (LVCSR) systems in the past decade have used a context-dependent (CD) phone as the fundamental acoustic unit. In this paper, we present one of the first robust LVCSR systems that use a syllable-level acoustic unit for LVCSR on telephone-bandwidth speech. This effort is motivated by the inherent limitations in phone-based approaches-namely the lack of an easy and efficient way for modeling long-term temporal dependencies. A syllable unit spans a longer time frame, typically three phones, thereby offering a more parsimonious framework for modeling pronunciation variation in spontaneous speech. We present encouraging results which show that a syllable-based system exceeds the performance of a comparable triphone system both in terms of word error rate (WER) and complexity. The WER of the best syllable system reported here is 49.1% on a standard SWITCHBOARD evaluation, a small improvement over the triphone system. We also report results on a much smaller recognition task, OGI Alphadigits, which was used to validate some of the benefits syllables offer over triphones. The syllable-based system exceeds the performance of the triphone system by nearly 20%, an impressive accomplishment since the alphadigits application consists mostly of phone-level minimal pair distinctions. Figure1. A Manually-Tagged Example of Propositional Terms in an RA In the past, there were three main approaches to term extraction: linguistic [4], statistical [5, 6], and C/NC-value based [7,8] hybrid approaches. Most previous approaches can only achieve a good performance on a test article composed of a relatively large amount of words. Without the use of large amount of words, this study proposes a method for extracting and  weighting single- and multi-word propositional terms of varying syntactic structures. 2. System Design and Development This research extracts the propositional terms beyond simply the NP-based propositional terms from the abstract of technical papers and then regards propositional term extraction as a sequence labeling task. To this end, this approach employs an IOB (Inside, Outside, Beginning) encoding scheme [9] to specify the propositional term boundaries, and conditional random fields (CRFs) [10] to combine arbitrary observation features to find the globally optimal term boundaries. The combined association measure (CAM) [11] is further adopted to modify the propositional term boundaries. In other words, this research not only considers the multi-level contextual information of an RA (such as word statistics, tense, morphology, syntax, semantics, sentence structure, and cue words) but also computes the lexical cohesion of word sequences to determine whether or not a propositional term is formed, since contextual information and lexical cohesion are two major factors for propositional term generation. Figure 2. The System Framework of Propositional Term Extraction The system framework essentially consists of a training phase and a test phase. In the training phase, the multi-level features were extracted from specific domain papers which were gathered from the SCI (Science Citation Index)-indexed and SCIE (Science Citation Index Expanded)-indexed databases. The specific domain papers are annotated by experts and then parsed. The feature extraction module collects statistical, syntactic, semantic and morphological level global and local features, and the parameter estimation module calculates conditional probabilities and optimal weights. The propositional term detection CRF model was built with feature extraction module and the parameter estimation module. During the test phase users can input an RA and obtain system feedback, i.e. the propositional terms of the RA. When the CRF model produces the preliminary candidate propositional terms, the propositional term generation module utilizes the combined association measure (CAM) to adjust the propositional term boundaries. The system framework proposed in this paper for RA propositional term extraction is shown in Figure 2. A more detailed discussion is presented in the following subsections.  2.1. Assisted Resource  In order to produce different levels of information and further assist feature extraction in the training and test phases, several resources were employed. This study chooses the ACM Computing Classification System (ACM CSS) [12] to serve as the domain terminology list for propositional term extraction from computer science RAs. The ACM CSS provides important subject descriptors for computer science, and was developed by the Association for Computing Machinery. The ACM CSS also provides a list of Implicit Subject Descriptors, which includes names of languages, people, and products in the field of computing. A mapping database, derived from WordNet (http://wordnet.princeton.edu/) and SUMO (Suggested Upper Merged Ontology) (http://ontology.teknowledge.com/) [13], supplies the semantic concept information of each word and the hierarchical concept information from the ontology. The AWL (Academic Words List) (http://www.vuw.ac.nz/lals/research/awl/) [14] is an academic word list containing 570 word families whose words are selected from different subjects. The syntactic level information of the RAs was obtained using Charniak’s parser [15], which is a “maximum-entropy inspired” probabilistic generative model parser for English. 2.2. Conditional Random Fields (CRFs)  For this research goal, given a word sequence W ={w1, w2,..., wn}, the most likely propositional term label sequence S ={s1, s2,..., sn} in the CRF framework with the set of weights Ψ can be  obtained from the following equation.  Sˆ = arg max S PΨ ( S | W )  （1）  A CRF is a conditional probability sequence as well as an undirected graphical model which defines a conditional distribution over the entire label sequence given the observation sequence. Unlike Maximum Entropy Markov Models (MEMMs), CRFs use an exponential model for the joint probability of the whole label sequence given the observation to solve the label bias problem. CRFs also have a conditional nature and model the real-world data depending on non-independent and interacting features of the observation sequence. A CRF allows the combination of overlapping, arbitrary and agglomerative observation features from both the past and future. The propositional terms extracted by CRFs are not restricted by syntactic variations or multiword forms and the global optimum is generated from different global and local contributor types.  The CRF consists of the observed input word sequence W ={w1,w2,..., wn} and label state sequence S = {s1, s2,..., sn} such that the expansion joint probability of a state label sequence given an observation word sequence can be written as  ∑∑ ∑∑ P  (S  |W  )  =  
This paper utilizes Yamcha, a SVM tool designed by Taku Kudo, to train an NP-chunking model for Chinese. In addition to IOB and two words surrounding the focused word, we experimented on new features and exploited unlabeled data from web pages to enhance the previous model. Our experiments with supervised learning indicate that our chosen feature sets outperform those reported in previous studies. In addition, the proposed method of semisupervised learning is proved to be effective in distinguishing a noun phrase from a verb phrase both consisting of V N combination, thus enhancing the overall accuracy. 關鍵詞：名詞組辨識、YamCha、監督式學習、半監督式學習 Keywords：NP-chunking、YamCha、supervised learning、semi-supervised learning 一、緒論 名詞組的辨識一直以來都是自然語言研究及其相關領域，如網路探勘（web mining）、文件分類（text categorization）等非常關鍵的一個步驟。在自動問答系統 （question answering）中，關鍵詞多半以名詞搜尋為主。在自然語言問答系統裡，名 詞組的辨識也是不可或缺。我們每天都使用的搜尋引擎，大家輸入的關鍵詞以及搜尋 引擎統計出來的熱門關鍵詞亦以名詞組居多；大至搜尋引擎背後大量的資料庫、小至 普通文本建檔而成的資料庫，製作索引分類時，名詞組的使用也多於動詞、副詞；也 有越來越多網頁都在針對網頁中的名詞組做自動偵測以及外部連結 ... 除了這些例子之 外，語意角色標記（semantic role labeling）、專有名詞辨識（name entity identification）、文章處理中的回指（coreference），名詞組的辨識都是一個重要的步 驟，因此有好的 NP chunker，可以改善許多 NLP 的研究成果以及相關應用。  在這篇論文中，我們先用 Taku Kudo 所提出利用 SVM 的演算法當作一開始的模型，除 了許多參考文獻中常用的 IOB 標示法以及位置，我們還嘗試了以不同標示法以及加入 不同位置的句子部份資訊當作特徵，證明對於中文的處理，不論是封閉或開放實驗中， IOE 表示法和加入前後兩個位置的詞及中研院簡化標記，是我們利用中研院句法樹庫 Sinica Treebank 所能得到最好的結果的參數。接著，我們利用一個沒有句法結構訊息 的大型語料庫 word sketch engine 中的句子，加上半監督式學習法中，自我學習的概念， 利用網路上大量未標記的網頁，來彌補 Sinica TreeBank 裡不足的訊息，改善利用監督 式學習法實做出的 chunker。 實驗部份，除了封閉測試外，由於中研院樹庫圖中資料有限，我們額外收集了不同類 型的句子當作開放測試的語料，以分別比較兩種作法在名詞組辨識的效果及限制。實 驗結果顯示，我們選用的參數較前人選用的參數做出的模型在第一階段開放測試中高 出了 16 個百分比，在第二個開放測試中也有 70％的 f-rate；加入 unlabeled data 這個步 驟的半監督式學習法，也的確提昇監督式學習法的效果，使開放測試的 f-rate 提高至 78.79%，不但保存分類器的優點，也明顯提昇中文在難解的名物化歧義的名詞辨識結 果。 接下來的章節中，第二章為文獻回顧包含 Chunking, SVM, 半監督式學習法的基本介紹； 第三章及第四章分別為實驗方法說明和數據結果討論。最後為結論與未來展望。 二、 文獻回顧 （一）、規則法 Abney(1995)利用了有限狀態機做出的規則式剖析器。他的實驗利用語意的訊息例如字 形變化來當作特徵。他對英文及德文做了測試都成功並且快速的取出主要類型，包括 動詞、名詞、介係詞的詞組。不過他還是強調選取的文法的重要性。Kinyon(2000) 提 出一個適用於不同語言的 rule-based chunker。即使缺乏大量的訓練語料，只要有一些 能夠辨別結構邊界的規則，就能使用 Kinyon 所提出的方法。Igor (2005)比較利用 NLTK 工具實做完成的 rule-based chunker 以及利用 TnT(Trigram and tag) 統計方式這兩 種方法做出的 chunker 在名詞組和動詞組上的辨識效果，實驗證明近來鮮少被大家採 用的規則方式實做出來的 chunker 不但沒有比利用統計的 chunker 遜色，甚至在召回率 （recall）及 f-rate 的表現上要來的更好。在中文方面，雖然 Zhao 等(1999)對某些類型 的詞組整理出結構的規則，但 Zhao 等(1999)還是捨棄規則式的作法，使用記憶基礎學 習（memory-based learning）的方式。他們的實驗顯示若不加上詞彙本身的訊息，而只 有利用詞性的訊息下，效果會比較差。 （二）、監督式學習及統計方法 在大規模語料庫建立之前，名詞組辨識常利用組成名詞組結構規律透過有限狀態機找 出符合的模式 pattern，或從標記好詞性的語料庫以統計方式得到，或結合語言規律及 語料庫統計；隨著賓州大學樹庫圖（University of Penn TreeBank）開放給大家使用之 後，詞組辨識也朝向以機器學習的方法來解決：Skut and Brants(1998)、Koeling (2000) and Osborne (2000)使用最大熵演算法；Park and Zhang 採用規則以及記憶學習 （memory-based learning, MBL）綜合的方式；Kudo and Matsumoto(2000,2001)利用 8 個 Support Vector Machine（SVM）系統投票（voting）的方式得出 chunking 模型，其 他利用監督式學習（supervised learning）的方法還有 Hidden Markov Model(HMM)（Li (2004)）、transform-based learning(Ramshaw and Marcus (1995))這幾種，大都是利用語 料的結構及前後語境的特徵得到的。這些演算法也早已被用在其他跟自然語言處理有 關的議題上。  
The rapid development of speech processing techniques has made themselves successfully applied in more and more applications, such as automatic dialing, voice-based information retrieval, and identity authentication. However, some unexpected variations in speech signals deteriorate the performance of a speech processing system, and thus relatively limit its application range. Among these variations, the environmental mismatch caused by the embedded noise in the speech signal is the major concern of this paper. In this paper, we provide a more rigorous mathematical analysis for the effects of the additive noise on two energy-related speech features, i.e. the logarithmic energy (logE) and the zeroth cepstral coefficient (c0). Then based on these effects, we propose a new feature compensation scheme, named silence feature normalization (SFN), in order to improve the noise robustness of the above two features for speech recognition. It is shown that, regardless of its simplicity in implementation, SFN brings about very significant improvement in noisy speech recognition, and it behaves better than many well-known feature normalization approaches. Furthermore,  SFN can be easily integrated with other noise robustness techniques to achieve an even better recognition accuracy. 關鍵詞：自動語音辨識、對數能量特徵、第零維倒頻譜特徵係數、強健性語音特徵 Keywords: speech recognition, logarithmic energy feature, the zeroth cepstral coefficient, robust speech features 一、緒論 近年來科技發展迅速，但是自動語音辨識仍然是一門相當具有挑戰性的課題。通常 一自動語音辨識系統在不受外在雜訊干擾的研究室環境下，都可以獲得極高的辨識效 能，但若是應用到實際的環境中，系統辨識效能則通常會大幅降低，這主要是被現實環 境中許多的變異性(variation)所影響。而語音辨識的變異性種類繁多，例如訓練環境與 測試環境間存在的環境不匹配(environmental mismatch)、語者變異(speaker variation)以及 發音的變異(pronunciation variation)等。對於環境不匹配而言，其相關的變數可概略分為 下列幾項類型：加成性雜訊(additive noise)、摺積性雜訊(convolutional noise)以及頻寬的 限制(bandwidth limitation)等。圖一為乾淨語音訊號受到雜訊干擾之示意圖。  乾淨語音訊號  雜訊語音訊號  機場  街道  加成性雜訊  摺積性雜訊  圖一、乾淨語音受雜訊干擾之示意圖  本論文是以上述所提及的環境不匹配中的加成性雜訊因素，作為主要探討的主題， 以期將加成性雜訊對語音辨識的影響降低。在特徵參數抽取步驟時，我們經常計算語音 的能量值作為特徵之一；根據過去的文獻指出[1][2]，語音訊號的能量特徵(energy feature) 所包含的辨識資訊大過於其它特徵，且能量特徵的計算複雜度很低。所以根據上述能量 特徵的優勢，在本論文中，我們特別對其強健性技術加以分析、討論與發展。 近年來，有許多成功的強健性對數能量特徵(logarithmic energy, logE)的技術相繼被提 出 ， 例 如 ， 對 數 能 量 動 態 範 圍 正 規 化 法 (log-energy dynamic range normalization, LEDRN)[3]其目標是使訓練與測試的語音資料其對數能量值之動態範圍一致化；對數能 量尺度重刻法(log-energy rescaling normalization, LERN)[4]則是將對數能量特徵乘上一 個介於 0 與 1 間的權重值，試圖重建出乾淨語音的對數能量特徵；而本實驗室先前所提 出的靜音音框對數能量正規化法(silence energy normalization, SLEN)[5]，是將判別為非 語音音框(non-speech frame)的對數能量特徵設定為一極小值的常數。上述的三種方法， 皆傾向於將非語音部分的對數能量數值調低，並將語音部分的對數能量值保持不變；其 主要的原因是一段語音特徵中，能量較低的部分通常會比能量較高的部分更容易受到雜 訊的影響。本論文依據前人所發表的文獻加以改進，且針對語音訊號能量相關的特徵如 何受到雜訊影響，以較嚴謹的數學理論加以分析，並提出一套新的強健技術，稱為「靜 音特徵正規化法」 (silence feature normalization, SFN)，此方法可以有效地降低加成性雜 訊對語音能量相關特徵的干擾，進而提高系統的辨識效能。  本論文其它章節概要如下：在第二章中，我們先主要將對能量相關特徵受雜訊影響的效 應，做進一步的分析與探討，接著介紹本論文所新提出的之靜音特徵正規化法(SFN)； 第三章包含了各種針對能量相關特徵之處理技術的語音辨識實驗數據及相關討論，其中 除了介紹語音辨識實驗環境外，主要是評估靜音特徵正規化法的效能，並與其他方法作 比較，藉此驗證我們所提出新方法能有效提升能量相關特徵在雜訊環境下的強健性。在 第四章中，我們嘗試將所提的新方法結合其它的強健性特徵技術，對此類的結合作辨識 實驗所得到的辨識率加以探討與分析，以驗證我們所提出的靜音特徵正規化法是否與其 它技術有良好的加成性。第五章則為本論文結論與未來展望。  二、靜音特徵正規化法  首先，我們在第一節中，針對語音能量相關特徵：對數能量(logarithmic energy, logE) 與第零維倒頻譜係數(c0)受到環境雜訊干擾的變異現象做較深入的觀察分析與探討，接 著在第二節中，我們根據這些結果，提出靜音特徵正規化法的新強健性技術。  （一）對數能量特徵及第零維倒頻譜特徵係數受加成性雜訊干擾之現象的探討  加成性雜訊對於能量相關特徵(logE 與 c0)造成的效應可由圖二看出端倪。圖二(a)、(b) 與(c)分別表示一乾淨語音訊號(Aurora-2.0 資料庫中的"MAH_1390A"檔)的波形圖、對數 能量(logE)曲線圖與第零維倒頻譜特徵係數(c0)曲線圖；而(b)與(c)中紅色實線、綠色虛 線與藍色點線則分別為乾淨語音、訊雜比 15dB 的語音及訊雜比 5dB 的語音所對應的曲 線。由這三張圖中，可以很明顯地看出，在有語音存在的區域，logE 與 c0 特徵值較大， 較不容易受到雜訊的影響而失真，而且隨時間上下振盪的情況較為明顯；反之，在無語 音存在的區段，其特徵值前後變化較平緩，且受到雜訊的干擾後，其值會很明顯地被改 變許多。接下來，我們就以較嚴謹的數學理論，對以上兩種失真現象加以分析與探討。 首先，我們探討加成性雜訊對於 logE 特徵的影響。假設一段受加成性雜訊干擾的語音  (noisy speech)中，第n 個音框的訊號 x [m]可表示為： n  x [m] = s [m] + d [m] ，  n  n  n  式(2-1)  其中sn[m] 與dn [m ] 分別表示第n 個音框之乾淨語音訊號(clean speech)以及雜訊(noise)，  則此音框之 logE 特徵值可用下式表示：  (∑ ) (∑ ∑ ) E(x) [n ] = log  m xn2[m] ≈ log  msn2[m] + mdn2[m]  ( ( )) = log exp (E (s) [n ]) + exp E(d) [n ] ，  式(2-2)  其中 E (x) [n ] 、 E (s) [n ]與 E (d) [n ] 分別為 xn [m ]、sn [m ] 以及dn [m ] 所對應之 logE 特徵值。 因此，受到雜訊干擾所導致雜訊語音與乾淨語音訊號兩者間 logE 特徵的差異 ΔE [n ]可  用下式表示： ( ( )) ΔE [n ] = E (x) [n ] − E (s) [n ] ≈ log 1 + exp E (d) [n ] − E (s) [n ] 。  式(2-3)  由式(2-3)可觀察出，若在相同的雜訊能量( E(d) [n ] )下，此差異值 ΔE [n ]與乾淨語音訊號 之 E (s) [n ]兩者呈現負相關的關係，當 E (s) [n ]愈大時，ΔE [n ]愈小，反之則愈大。根據上 述的推導，可以看出一雜訊語音訊號中，含有語音成份的音框( E (s) [n ]較大)相較於純雜 訊音框( E(s)[n]較小)而言，其 logE 特徵被雜訊影響的情況較小(即失真量 ΔE [n ]較小)。 接下來，我們探討加成性雜訊對於語音訊號的 logE 特徵序列於調變頻譜(modulation spectrum)上的影響。首先，我們將式(2-2)以泰勒級數(Taylor series)展開，其展開的中心 ( ) 點設定為 E (s) [n ], E(d) [n ] = (0, 0) ，展開階層為 2 階，如式(2-4)所示：  (a) (a)  (b)  (c)  圖二、在不同 SNR 下，一語音訊號之波形圖及能量相關特徵時間序列圖，其中(a)為乾 淨語音波形、(b)為 logE 特徵曲線、(c)為 c0 特徵曲線  ( ( )) E (x) [n ] ≈ log exp (E (s) [n ]) + exp E (d) [n ]  ( ) ( ( ) ) ( ) ≈ log 2 + 1 E (s) [n ] + E (d) [n ] + 1 E n (s) [ ] 2 + E (d) [n ] 2 − E (s) [n ]E (d) [n ] .  式(2-4)  2  8  因此，若將上式(2-4)取傅立葉轉換，則此雜訊語音的對數能量序列{E (x) [n ]} 的調變  頻譜可用下式表示：  X (jω) ≈ (2π log 2)δ (ω) + 1 (S (jω) + D (jω)) 2  +  
Speech and music discrimination is one of the most important issues for multimedia information retrieval and efficient coding. While many features have been proposed, seldom of which show robustness under noisy condition, especially in telecommunication applications. In this paper two novel features based on real cepstrum are presented to represent essential differences between music and speech: Average Pitch Density (APD), Relative Tonal Power Density (RTPD). Separate histograms are used to prove the robustness of the novel features. Results of discrimination experiments show that these features are more robust than the commonly used features. The evaluation database consists of a reference collection and a set of telephone speech and music recorded in real world. Keywords: Speech/Music Discrimination, Multimedia Information Retrieval, Real Cepstrum. 1. Introduction In applications of multimedia information retrieval and effective coding for telecommunication, audio stream always needs to be diarized or labeled as speech, music or noise or silence, so that different segments can be implemented in different ways. However, speech signals often consist of many kinds of noise, and the styles of music such as personalized ring-back tone may differ in thousands ways. Those make the discrimination problem more difficult. A variety of systems for audio segmentation or classification have been proposed in the past and many features such as Root Mean Square (RMS) [1], Zero Crossing Rate (ZCR) [1,4,5], low frequency modulation [2,4,5], entropy and dynamism features [2,3,6], Mel Frequency Cepstral coefficients (MFCCs) have been used. Some features need high quality audio signal or refined spectrum detail, and some cause long delay so as not fit for telecommunication applications. While the classification frameworks including nearest neighbor, neural network, Hidden Markov Model (HMM), Gaussian Mixture Modal (GMM) and Support Vector Machine (SVM) have been adopted as the back end, features are still the crucial factor to the final performance. As shown in the following part of this paper, the discrimination abilities of some common features are poor with noisy speech. The main reason may explain as that they do not represent the essential difference between speech and music. In this paper, two novel features, called as Average Pitch Density (APD) and Relative Tonal  Power Density (RTPD) are proposed, which are based on real cepstrum analysis and show better robustness than the others. The evaluation database consists of two different data sets: one comes from Scheirer and Slaney [5], the other is collected from real telecommunication situation. The total lengths for music and speech are about 37 minutes and 28.7 minutes respectively. The rest of this paper is organized as follows: Section 2 introduces the novel features based on real cepstrum analysis. Section 3 describes the evaluation database and the comparative histograms of different features. The discrimination experiments and their results are given in section 4. Section 5 concludes this paper.  2. Features Based on Real Cepstrum  There are tremendous types of music, and the signal components of which can be divided into two classes: tonal-like and noise-like. The tonal-like class consists of tones played by all kinds of musical instruments, and these tones are catenated to construct the melody of music. The noise-like class is mainly played by percussion instruments such as drum, cymbal, gong, maracas, etc. The former class corresponds to the musical system, which construct by a set of predefined pitches according to phonology. The latter class can not play notes with certain pitch and is often used to construct rhythm.  The biggest difference between speech and music lies on the pitch. Because of the restriction of musical system, the pitch of music usually can only jump between discrete frequencies, except for vibratos or glissandi. But pitch of speech can change continuously and will not keep on a fixed frequency for a long time. Besides the difference of pitch character, the noise part of music, which is often played by percussion instrument, also has different features from speech. That part of music does not have pitch, but it usually has stronger power. This phenomenon seldom exists in speech signal, because generally the stronger part of speech is voiced signal, which does have pitch.  In order to describe the differences of pitch between speech and music, we use real cepstrum instead of spectrogram. Cepstrum analysis is a more powerful tool to analysis the detail of spectrum, which can separate pitch information from spectral envelop. The real cepstrum is defined as (Eq. (2) gives the Matlab expression)  ∫ ( ) RCx  =ˆ  real⎜⎜⎝⎛  
This paper mainly addresses the problem of determining voice activity in presence of noise, especially in a dynamically varying background noise. The proposed voice activity detection algorithm is based on structure of three-layer wavelet decomposition. Appling auto-correlation function into each subband exploits the fact that intensity of periodicity is more significant in sub-band domain than that in full-band domain. In addition, Teager energy operator (TEO) is used to eliminate the noise components from the wavelet coefficients on each subband. Experimental results show that the proposed wavelet-based algorithm is prior to others and can work in a dynamically varying background noise. Keywords: voice activity detection, auto-correlation function, wavelet transform, Teager energy operator 1. Introduction Voice activity detection (VAD) refers to the ability of distinguishing speech from noise and is an integral part of a variety of speech communication systems, such as speech coding, speech recognition, hand-free telephony, and echo cancellation. Although the existed VAD algorithms performed reliably, their feature parameters are almost depended on the energy level and sensitive to noisy environments [1-4]. So far, a wavelet-based VAD is rather less discussed although wavelet analysis is much suitable for speech property. S.H. Chen et al. [5] shown that the proposed VAD is based on wavelet transform and has an excellent performance. In fact, their approach is not suitable for practical application such as variable-level of noise conditions. Besides, a great computing time is needed for accomplishing wavelet reconstruction to decide whether is speech-active or not.  Compared with Chen's VAD approach, the proposed decision of VAD only depends on three-layer wavelet decomposition. This approach does not need any computing time to waste the wavelet reconstruction. In addition, the four non-uniform subbands are generated from the wavelet-based approach and the well-known "auto-correlaction function (ACF)" is adopted to detect the periodicity of subband. We refer the ACF defined in subband domain as subband auto-correlation function (SACF). Due to that periodic property is mainly focused on low frequency bands, so we let the low frequency bands have high resolution to enhance the periodic property by decomposing only low band on each layer. In addition to the SACF, enclosed herein the Teager energy operator (TEO) is regarded as a pre-processor for SACF. The TEO is a powerful nonlinear operator and has been successfully used in various speech processing applications [6-7]. F. Jabloun et al. [8] displayed that TEO can suppress the car engine noise and be easily implemented through time domain in Mel-scale subband. The later experimental result will prove that the TEO can further enhance the detection of subband periodicity. To accurately count the intensity of periodicity from the envelope of the SACF, the Mean-Delta (MD) method [9] is utilized on each subband. The MD-based feature parameter has been presented for the robust development of VAD, but is not performed well in the non-stationary noise shown in the followings. Eventually, summing up the four values of MDSACF (Mean-Delta of Subband Auto-Correlation Function, a new feature parameter called "speech activity envelope (SAE)" is further proposed. Experimental results show that the envelope of the new SAE parameter can point out the boundary of speech activity under the poor SNR conditions and it is also insensitive to variable-level of noise. This paper is organized as follows. Section 2 describes the concept of discrete wavelet transform (DWT) and shows the used structure of three-layer wavelet decomposition. Section 3 introductions the derivation of Teager energy operator (TEO) and displays the efficiency of subband noise suppression. Section 4 describes the proposed feature parameter, and the block diagram of proposed wavelet-based VAD algorithm is outlined in Section 5. Section 6 evaluates the performance of the algorithm and compare to other two wavelet-based VAD algorithm and ITU-T G.729B VAD. Finally, Section 7 discusses the conclusions of experimental results.  2. Wavelet transform The wavelet transform (WT) is based on a time-frequency signal analysis. The wavelet analysis represents a windowing technique with variable-sized regions. It allows the use of long time intervals where we want more precise low-frequency information, and shorter regions where we want high-frequency information. It is well known that speech signals contain many transient components and non-stationary property. Making use of the multi-resolution analysis (MRA) property of the WT, better time-resolution is needed a high frequency range to detect the rapid changing transient component of the signal, while better frequency resolution is needed at low frequency range to track the slowly time-varying formants more precisely [10]. Figure 1 displays the structure of three-layer wavelet decomposition utilized in this paper. We decompose an entire signal into four non-uniform subbands including three detailed scales such as D1, D2 and D3 and one appropriated scale such A3.  Figure 1. Structure of three-layer wavelet decomposition  3. Mean-delta method for subband auto-correlation function  The well-known definition of the term "Auto-Correlation Function (ACF)" is usually used for measuring the self-periodic intensity of signal sequences shown as below:  p−k  R(k) = ∑ s(n)s(n + k), k = 0,1,......p ,  (1)  n=0  where p is the length of ACF. k denotes as the shift of sample.  In order to increase the efficiency of ACF about making use of periodicity detection to detect speech, the ACF is defined in subband domain, which called "subband auto-correlation function (SACF)". Figure 2 clearly illustrates the normalized SACFs for each subband when input speech is contaminated by white noise. In addition, a normalization factor is applied to the computation of SACF. This major reason is to provide an offset for insensitivity on variable energy level. From this figure, it is observed that the SACF of voiced speech has more obviously peaks than that of unvoiced speech and white noise. Similarly, for unvoiced speech the ACF has greater periodic intensity than white noise especially in the approximation A3 .  Furthermore, a Mean-Delta (MD) method [9] over the envelope of each SACF is utilized herein to evaluate the corresponding intensity of periodicity on each subband. First, a measure which similar to delta cepstrum evaluation is mimicked to estimate the periodic intensity of SACF, namely "Delta Subband Auto-Correlation Function (DSACF)", shown below:  ∑ R&M  (k)  =  M m=−M  m  ⎛ ⎜⎝  R(k + m) R(0)  M  ⎞ ⎟⎠  ,  (2)  ∑ m2  m=−M  where R&M is DSACF over an M -sample neighborhood ( M = 3 in this study).  It is observed that the DSACF measure is almost like the local variation over the SACF. Second, averaging the delta of SACF over a M -sample neighborhood R&M , a mean of the absolute values of the DSACF (MDSACF) is given by  ∑ RM  =  
The noise robustness property for an automatic speech recognition system is one of the most important factors to determine its recognition accuracy under a noise-corrupted environment. Among the various approaches, normalizing the statistical quantities of speech features is a  very promising direction to create more noise-robust features. The related feature normalization approaches include cepsral mean subtraction (CMS), cepstral mean and variance normalization (CMVN), histogram equalization (HEQ), etc. In addition, the statistical quantities used in these techniques can be obtained in an utterance-wise manner or a codebook-wise manner. It has been shown that in most cases, the latter behaves better than the former. In this paper, we mainly focus on two issues. First, we develop a new procedure for developing the pseudo-stereo codebook, which is used in the codebook-based feature normalization approaches. The resulting new codebook is shown to provide a better estimate for the features statistics in order to enhance the performance of the codebook-based approaches. Second, we propose a series of new feature normalization approaches, including associative CMS (A-CMS), associative CMVN (A-CMVN) and associative HEQ (A-HEQ). In these approaches, two sources of statistic information for the features, the one from the utterance and the other from the codebook, are properly integrated. Experimental results show that these new feature normalization approaches perform significantly better than the conventional utterance-based and codebook-based ones. As the result, the proposed methods in this paper effectively improve the noise robustness of speech features. 關鍵詞：自動語音辨識、碼簿、強健性語音特徵 Keywords: automatic speech recognition, codebook, robust speech feature 一、緒論 本論文所討論及提出的強健式技術，主要是在加成性雜訊環境下，對訓練與測試二者的 語音特徵參數的統計特性加以正規化，以降低兩環境的不匹配。其中我們利用梅爾倒頻 譜係數(mel-frequency cepstral coefficients, MFCC)做為語音特徵，結合語音偵測技術 (voice activity detection, VAD)[1]與特徵統計值正規化的諸多技術，來提升語音特徵在加 成性雜訊環境下的強健性。本論文中所討論的特徵參數正規化法分別為： （一）整段式(utterance-based)特徵參數正規化法 即 傳 統 的 整 段 式 倒 頻 譜 平 均 消 去 法 (utterance-based cepstral mean subtraction, U-CMS)[2]、整段式倒頻譜平均值與變異數正規化法(utterance-based cepstral mean and variance normalization, U-CMVN)[3]與整段式統計圖等化法(utterance-based histogram equalization, U-HEQ)[4]。此類方法是以一整段語句為基準去估算每一維特徵參數的統計 特性，並執行特徵參數正規化。 （二）碼簿式(codebook-based)特徵參數正規化法 此類方法是藉由碼簿來幫助我們估算出代表訓練語音特徵與測試語音特徵的統計 值，藉此執行語音特徵正規化。在過去的研究裡[5][6][7]，發現此類的方法，包括碼簿 式倒頻譜平均消去法(codebook-based cepstral mean subtraction, C-CMS)與碼簿式倒頻譜 平均值與變異數正規化法(codebook-based cepstral mean and variance normalization, C-CMVN)等，其效果都比前一類之整段式特徵正規化法來的好。 本論文根據以上所述的二類方法提出一系列改進的技術，分述如下： ① 在過去碼簿式特徵正規化法中[5-7]，碼簿取得方式是將全部的訓練語料轉換的特徵 參數作向量量化，這樣的方式可能會使其中許多碼字是對應到非語音的靜音(silence)  或雜訊成份，而使這些碼字較缺乏語音特徵的代表性，同時，每個碼字的權重被設 為相等，這樣可能會使之後所欲計算的特徵統計值較不精確。在本論文中，我們應 用端點偵測(voice activity detection, VAD)技術偵測出一段訊號的語音(speech)成分與 非語音(silence)成分，然後只使用語音成分的特徵去製作碼簿，同時，不同的碼字根 據其對應的原始特徵數目多寡設定其權重(weight)，這種新的碼簿建構程序應可以改 善上述之缺點，進而提升各種碼簿式特徵正規化法的效能。 ② 我們提出了一新方法，稱為組合式(associative)特徵正規化法，其主要程序是我們整 合前述之碼簿式與整段式兩方所使用的特徵統計資訊，來計算特徵的統計值，藉此 來執行特徵的正規化。實驗結果發現此類組合式的方法比碼簿式與整段式的兩類方 法，能達到更佳的效果。可能原因在於，組合式的方法降低了碼簿式方法中只取每 段訊號前幾個音框作為純雜訊估測的不準確效應，而使所得的特徵統計值更為精確。 在之後的第二章裡，我們將簡單介紹整段式(utterance-based)特徵正規化技術。第三 章將說明新的虛擬雙通道碼簿的建立程序，藉此改進碼簿式(codebook-based)特徵正規 化法的效能。在第四章中，我們敘述本論文所新提出的組合式(associative)特徵正規化 法。第五章包含了本論文之實驗所使用之語料庫介紹與本論文所提到的各種特徵正規化 技術之實驗結果與相關的討論分析。最後，第六章為結論與未來展望。  二、整段式(utterance-based)特徵參數正規化技術  本章我們簡要介紹三種在強健性語音辨識中，常被應用的特徵參數正規化技術，分  別為整段式倒頻譜平均消去法(utterance-based cepstral mean subtraction, U-CMS)[2]、整  段 式 倒 頻 譜 平 均 值 與 變 異 數 正 規 化 法 (utterance-based cepstral mean and variance  normalization, U-CMVN)[3] 與 整 段 式 倒 頻 譜 統 計 圖 等 化 法 (utterance-based cepstral  histogram equalization, U-HEQ)[4]。  （一）整段式倒頻譜平均消去法 (U-CMS)  倒頻譜平均消去法(CMS)的目的是希望一語音特徵序列中，每一維度的倒頻譜係數  長時間平均值為0。假設其值不為0時，我們就將此視為通道雜訊而加以扣除，此種方法  對於降低通道雜訊效應是一種簡單且有用的技術，但是有時對於降低加成性雜訊上也有  一定的效果。在多數的作法上，首先我們將整段語音每一維的倒頻譜係數取平均值，然  後將每一維的係數減掉其平均值，如此即得到補償後之新特徵，此稱為整段式倒頻譜平  均消去法(utterance-based cepstral mean subtraction, U-CMS)。根據這樣的原則，我們假設 {X [n ],n = 1, 2,..., N } 為一段語音所擷取到的某一維倒頻譜特徵參數序列，在經過整段  式 倒 頻 譜 平 均 消 去 法 (U-CMS) 處 理 後 ， 得 到 新 的 經 過 補 償 的 特 徵 參 數 序 列  {XU −CMS [n ],n = 1, 2,..., N } ，其數學式如下所示：  XU −CMS  [n  ]  =  X  [n  ]  −  μ X  ,  n = 1, 2,..., N .  式(2.1)  ∑ 其中  μ X  =1 N  N X [n ] , n =1  N 為整段語音的音框個數。  因此，在  U-CMS  法中，用以正規化的平均值  μ X  是由原始整段的特徵序列所得。  （二）整段式倒頻譜平均值與變異數正規化法 (U-CMVN)  語音訊號在經過加成性雜訊的干擾之後，其倒頻譜之平均值和原本乾淨語音倒頻譜 平均值之間通常會存在一偏移量(bias)，同時其變異數相對於乾淨語音倒頻譜參數的變 異數而言則通常會有縮小的現象，如此便造成了訓練與測試特徵的不匹配，而降低辨識  效果。使用倒頻譜平均值與變異數正規化法(CMVN)的目的是把每一維的倒頻譜特徵參  數之平均值正規化為 0，並將其變異數正規化為 1，如此便能降低上述的失真，以達到  提升倒頻譜特徵參數的強健性。  在倒頻譜平均值與變異數正規化法(CMVN)的作法上，我們是先利用倒頻譜平均消  去法(CMS)去作處理(使處理過後的每一維倒頻譜係數平均值為0)，然後再將處理後的每  一維倒頻譜係數除以其標準差，如此得到新的特徵序列。在U-CMVN(utterance-based cepstral mean and variance normalization)法中，假設{X [n ],n = 1, 2,..., N } 是一段語音的  某 一 維 倒 頻 譜 特 徵 參 數 序 列 ， 在 經 過 U-CMVN 處 理 後 ， 得 到 新 的 特 徵 參 數  {XU −CMVN [n ], n = 1, 2, ..., N } ，其數學式如下所示：  XU −CMVN [n ]  =  X[n] − σ  μ X  ,  n = 1, 2,..., N .  X  式(2.2)  其中  ∑ ∑ μ X  =  
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included.
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time.
In this paper we present how resources and tools developed within the Human Language Technology Group at the University of Belgrade can be used for tuning queries before submitting them to a web search engine. We argue that the selection of words chosen for a query, which are of paramount importance for the quality of results obtained by the query, can be substantially improved by using various lexical resources, such as morphological dictionaries and wordnets. These dictionaries enable semantic and morphological expansion of the query, the latter being very important in highly inflective languages, such as Serbian. Wordnets can also be used for adding another language to a query, if appropriate, thus making the query bilingual. Problems encountered in retrieving documents of interest are discussed and illustrated by examples. A brief description of resources is given, followed by an outline of the web tool which enables their integration. Finally, a set of examples is chosen in order to illustrate the use of the lexical resources and tool in question. Results obtained for these examples show that the number of documents obtained through a query by using our approach can double and even quadruple in some cases.
The ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research.
The Linguistic Data Consortium (LDC) seeks to provide its members with quality linguistic resources and services. In order to pursue these ideals and to remain current, LDC monitors the needs and sentiments of its communities. One mechanism LDC uses to generate feedback on consortium and resource issues is the LDC Member Survey. The survey allows LDC Members and nonmembers to provide LDC with valuable insight into their own unique circumstances, their current and future data needs and their views on LDCs role in meeting them. When the 2006 Survey was found to be a useful tool for communicating with the Consortium membership, a 2007 Survey was organized and administered. As a result of the surveys, LDC has confirmed that it has made a positive impact on the community and has identified ways to improve the quality of service and the diversity of monthly offerings. Many respondents recommended ways to improve LDCs functions, ordering mechanism and webpage. Some of these comments have inspired changes to LDCs operation and strategy.
The emerging area of Geographic Information Systems (GIS) has proven to add an interesting dimension to many research projects. Within the language-sites initiative we have brought together a broad range of links to digital language corpora and resources. Via Google Earths visually appealing 3D-interface users can spin the globe, zoom into an area they are interested in and access directly the relevant language resources. This paper focuses on several ways of relating the map and the online data (lexica, annotations, multimedia recordings, etc.). Furthermore, we discuss some of the implementation choices that have been made, including future challenges. In addition, we show how scholars (both linguists and anthropologists) are using GIS tools to fulfill their specific research needs by making use of practical examples. This illustrates how both scientists and the general public can benefit from geography-based access to digital language data.
The paper provides a general introduction to the CLARIN project, a large-scale European research infrastructure project designed to establish an integrated and interoperable infrastructure of language resources and technologies. The goal is to make language resources and technology much more accessible to all researchers working with language material, particularly non-expert users in the Humanities and Social Sciences. CLARIN intends to build a virtual, distributed infrastructure consisting of a federation of trusted digital archives and repositories where language resources and tools are accessible through web services. The CLARIN project consists of 32 partners from 22 countries and is currently engaged in the preparatory phase of developing the infrastructure. The paper describes the objectives of the project in terms of its technical, legal, linguistic and user dimensions.
In this paper the dialogue act annotation of naive and expert annotators, both annotating the same data, are compared in order to characterise the insights annotations made by different kind of annotators may provide for evaluating dialogue act tagsets. It is argued that the agreement among naive annotators provides insight in the clarity of the tagset, whereas agreement among expert annotators provides an indication of how reliably the tagset can be applied when errors are ruled out that are due to deficiencies in understanding the concepts of the tagset, to a lack of experience in using the annotation tool, or to little experience in annotation more generally. An indication of the differences between the two groups in terms of inter-annotator agreement and tagging accuracy on task-oriented dialogue in different domains, annotated with the DIT++ dialogue act tagset is presented, and the annotations of both groups are assessed against a gold standard. Additionally, the effect of the reduction of the tagsets granularity on the performances of both groups is looked into. In general, it is concluded that the annotations of both groups provide complementary insights in reliability, clarity, and more fundamental conceptual issues.
In our paper we present a methodology used for low-cost validation of quality of Part-of-Speech annotation of the Prague Dependency Treebank based on multiple re-annotation of data samples carefully selected with the help of several different Part-of-Speech taggers.
Word sketches are part of the Sketch Engine corpus query system. They represent automatic, corpus-derived summaries of the words grammatical and collocational behaviour. Besides the corpus itself, word sketches require a sketch grammar, a regular expression-based shallow grammar over the part-of-speech tags, to extract evidence for the properties of the targeted words from the corpus. The paper presents a sketch grammar for German, a language which is not strictly configurational and which shows a considerable amount of case syncretism, and evaluates its accuracy, which has not been done for other sketch grammars. The evaluation focuses on NP case as a crucial part of the German grammar. We present various versions of NP definitions, so demonstrating the influence of grammar detail on precision and recall.
We evaluate the extent to which the distinction between semantically core and non-core dependents as used in the FrameNet corpus corresponds to the traditional distinction between syntactic complements and modifiers of a verb, for the purposes of harvesting a wide-coverage verb lexicon from FrameNet for use in deep linguistic processing applications. We use the VerbNet verb database as our gold standard for making judgements about complement-hood, in conjunction with our own intuitions in cases where VerbNet is incomplete. We conclude that there is enough agreement between the two notions (0.85) to make practical the simple expedient of equating core PP dependents in FrameNet with PP complements in our lexicon. Doing so means that we lose around 13{\%} of PP complements, whilst around 9{\%} of the PP dependents left in the lexicon are not complements.
The PIT corpus is a German multi-media corpus of multi-party dialogues recorded in a Wizard-of-Oz environment at the University of Ulm. The scenario involves two human dialogue partners interacting with a multi-modal dialogue system in the domain of restaurant selection. In this paper we present the characteristics of the data which was recorded in three sessions resulting in a total of 75 dialogues and about 14 hours of audio and video data. The corpus is available at http://www.uni-ulm.de/in/pit.
Looking for a better understanding of spontaneous speech-related phenomena and to improve automatic speech recognition (ASR), we present here a study on the relationship between the occurrence of overlapping speech segments and disfluencies (filled pauses, repetitions, revisions) in political interviews. First we present our data, and our overlap annotation scheme. We detail our choice of overlapping tags and our definition of disfluencies; the observed ratios of the different overlapping tags are examined, as well as their correlation with of the speaker role and propose two measures to characterise speakers interacting attitude: the attack/resist ratio and the attack density. We then study the relationship between the overlapping speech segments and the disfluencies in our corpus, before concluding on the perspectives that our experiments offer.
This paper describes in detail the data that was collected and annotated during the third and final year of the CHIL project. This data was used for the CLEAR evaluation campaign in spring 2007. The paper also introduces the CHIL Evaluation Package 2007 that resulted from this campaign including a complete description of the performed evaluation tasks. This evaluation package will be made available to the community through the ELRA General Catalogue.
Laughter is an intrinsic component of human-human interaction, and current automatic speech understanding paradigms stand to gain significantly from its detection and modeling. In the current work, we produce a manual segmentation of laughter in a large corpus of interactive multi-party seminars, which promises to be a valuable resource for acoustic modeling purposes. More importantly, we quantify the occurrence of laughter in this new domain, and contrast our observations with findings for laughter in multi-party meetings. Our analyses show that, with respect to the majority of measures we explore, the occurrence of laughter in both domains is quite similar.
SpatialML is an annotation scheme for marking up references to places in natural language. It covers both named and nominal references to places, grounding them where possible with geo-coordinates, including both relative and absolute locations, and characterizes relationships among places in terms of a region calculus. A freely available annotation editor has been developed for SpatialML, along with a corpus of annotated documents released by the Linguistic Data Consortium. Inter-annotator agreement on SpatialML is 77.0 F-measure for extents on that corpus. An automatic tagger for SpatialML extents scores 78.5 F-measure. A disambiguator scores 93.0 F-measure and 93.4 Predictive Accuracy. In adapting the extent tagger to new domains, merging the training data from the above corpus with annotated data in the new domain provides the best performance.
While recent corpus annotation efforts cover a wide variety of semantic structures, work on temporal and causal relations is still in its early stages. Annotation efforts have typically considered either temporal relations or causal relations, but not both, and no corpora currently exist that allow the relation between temporals and causals to be examined empirically. We have annotated a corpus of 1000 event pairs for both temporal and causal relations, focusing on a relatively frequent construction in which the events are conjoined by the word and. Temporal relations were annotated using an extension of the BEFORE and AFTER scheme used in the TempEval competition, and causal relations were annotated using a scheme based on connective phrases like and as a result. The annotators achieved 81.2{\%} agreement on temporal relations and 77.8{\%} agreement on causal relations. Analysis of the resulting corpus revealed some interesting findings, for example, that over 30{\%} of CAUSAL relations do not have an underlying BEFORE relation. The corpus was also explored using machine learning methods, and while model performance exceeded all baselines, the results suggested that simple grammatical cues may be insufficient for identifying the more difficult temporal and causal relations.
Verb lexical semantic properties are only one of the factors that contribute to the determination of the event type expressed by a sentence, which is instead the result of a complex interplay between the verb meaning and its linguistic context. We report on two computational models for the automatic identification of event type in Italian. Both models use linguistically-motivated features extracted from Italian corpora. The main goal of our experiments is to evaluate the contribution of different types of linguistic indicators to identify the event type of a sentence, as well as to model various cases of context-driven event type shift. In the first model, event type identification has been modelled as a supervised classification task, performed with Maximum Entropy classifiers. In the second model, Self-Organizing Maps have been used to define and identify event types in an unsupervised way. The interaction of various contextual factors in determining the event type expressed by a sentence makes event type identification a highly challenging task. Computational models can help us to shed new light on the real structure of event type classes as well as to gain a better understanding of context-driven semantic shifts.
The paper describes the construction and usage of the Romanian version of the TimeBank corpus. The success rate of 96.53{\%} for the automatic import of the temporal annotation from English to Romanian shows that the automatic transfer is a worth doing enterprise if temporality is to be studied in another language than the one for which TimeML, the annotation standard used, was developed. A preliminary study identifies the main situations that occurred during the automatic transfer, as well as temporal elements not (yet) marked in the English corpus.
In the context of Natural Language Processing, annotation is about recovering implicit information that is useful for natural language applications. In this paper we describe a tense annotation task for Chinese - a language that does not have grammatical tense - that is designed to infer the temporal location of a situation in relation to the temporal deixis, the moment of speech. If successful, this would be a highly rewarding endeavor as it has application in many natural language systems. Our preliminary experiments show that while this is a very challenging annotation task for which high annotation consistency is very difficult but not impossible to achieve. We show that guidelines that provide a conceptually intuitive framework will be crucial to the success of this annotation effort.
Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports, politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser. We employ statistical techniques for creating an ensemble of domain sensitive parsers, and explore methods for amalgamating their predictions. Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.
Developing linguistic resources, in particular grammars, is known to be a complex task in itself, because of (amongst others) redundancy and consistency issues. Furthermore some languages can reveal themselves hard to describe because of specific characteristics, e.g. the free word order in German. In this context, we present (i) a framework allowing to describe tree-based grammars, and (ii) an actual fragment of a core multicomponent tree-adjoining grammar with tree tuples (TT-MCTAG) for German developed using this framework. This framework combines a metagrammar compiler and a parser based on range concatenation grammar (RCG) to respectively check the consistency and the correction of the grammar. The German grammar being developed within this framework already deals with a wide range of scrambling and extraction phenomena.
Large-scale grammar-based parsing systems nowadays increasingly rely on independently developed, more specialized components for pre-processing their input. However, different tools make conflicting assumptions about very basic properties such as tokenization. To make linguistic annotation gathered in pre-processing available to deep parsing, a hybrid NLP system needs to establish a coherent mapping between the two universes. Our basic assumption is that tokens are best described by attribute value matrices (AVMs) that may be arbitrarily complex. We propose a powerful resource-sensitive rewrite formalism, chart mapping, that allows us to mediate between the token descriptions delivered by shallow pre-processing components and the input expected by the grammar. We furthermore propose a novel way of unknown word treatment where all generic lexical entries are instantiated that are licensed by a particular token AVM. Again, chart mapping is used to give the grammar writer full control as to which items (e.g. native vs. generic lexical items) enter syntactic parsing. We discuss several further uses of the original idea and report on early experiences with the new machinery.
In this work, we examine and attempt to extend the coverage of a German HPSG grammar. We use the grammar to parse a corpus of newspaper text and evaluate the proportion of sentences which have a correct attested parse, and analyse the cause of errors in terms of lexical or constructional gaps which prevent parsing. Then, using a maximum entropy model, we evaluate prediction of lexical types in the HPSG type hierarchy for unseen lexemes. By automatically adding entries to the lexicon, we observe that we can increase coverage without substantially decreasing precision.
In this paper we propose a partial parsing model which achieves robust parsing with a large HPSG grammar. Constraint-based precision grammars, like the HPSG grammar we are using for the experiments reported in this paper, typically lack robustness, especially when applied to real world texts. To maximally recover the linguistic knowledge from an unsuccessful parse, a proper selection model must be used. Also, the efficiency challenges usually presented by the selection model must be answered. Building on the work reported in (Zhang et al., 2007), we further propose a new partial parsing model that splits the parsing process into two stages, both of which use the bottom-up chart-based parsing algorithm. The algorithm is implemented and a preliminary experiment shows promising results.
News articles about the same event published over time have properties that challenge NLP and IR applications. A cluster of such texts typically exhibits instances of paraphrase and contradiction, as sources update the facts surrounding the story, often due to an ongoing investigation. The current hypothesis is that the stories evolve over time, beginning with the first text published on a given topic. This is tested using a phylogenetic approach as well as one based on language modeling. The fit of the evolutionary models is evaluated with respect to how well they facilitate the recovery of chronological relationships between the documents. Over all data clusters, the language modeling approach consistently outperforms the phylogenetics model. However, on manually collected clusters in which the documents are published within short time spans of one another, both have a similar performance, and produce statistically significant results on the document chronology recovery evaluation.
This paper describes the open source SemanticVectors package that efficiently creates semantic vectors for words and documents from a corpus of free text articles. We believe that this package can play an important role in furthering research in distributional semantics, and (perhaps more importantly) can help to significantly reduce the current gap that exists between good research results and valuable applications in production software. Two clear principles that have guided the creation of the package so far include ease-of-use and scalability. The basic package installs and runs easily on any Java-enabled platform, and depends only on Apache Lucene. Dimension reduction is performed using Random Projection, which enables the system to scale much more effectively than other algorithms used for the same purpose. This paper also describes a trial application in the Technology Management domain, which highlights some user-centred design challenges which we believe are also key to successful deployment of this technology.
Open answers in questionnaires contain valuable information that is very time-consuming to analyze manually. We present a method for hypothesis generation from questionnaires based on text clustering. Text clustering is used interactively on the open answers, and the user can explore the cluster contents. The exploration is guided by automatic evaluation of the clusters against a closed answer regarded as a categorization. This simplifies the process of selecting interesting clusters. The user formulates a hypothesis from the relation between the cluster content and the closed answer categorization. We have applied our method on an open answer regarding occupation compared to a closed answer on smoking habits. With no prior knowledge of smoking habits in different occupation groups we have generated the hypothesis that farmers smoke less than the average. The hypothesis is supported by several separate surveys. Closed answers are easy to analyze automatically but are restricted and may miss valuable aspects. Open answers, on the other hand, fully capture the dynamics and diversity of possible outcomes. With our method the process of analyzing open answers becomes feasible.
We present a new corpus for computational stylometry, more specifically authorship attribution and the prediction of author personality from text. Because of the large number of authors (145), the corpus will allow previously impossible studies of variation in features considered predictive for writing style. The innovative meta-information (personality profiles of the authors) associated with these texts allows the study of personality prediction, a not yet very well researched aspect of style. In this paper, we describe the contents of the corpus and show its use in both authorship attribution and personality prediction. We focus on features that have been proven useful in the field of author recognition. Syntactic features like part-of-speech n-grams are generally accepted as not being under the authors conscious control and therefore providing good clues for predicting gender or authorship. We want to test whether these features are helpful for personality prediction and authorship attribution on a large set of authors. Both tasks are approached as text categorization tasks. First a document representation is constructed based on feature selection from the linguistically analyzed corpus (using the Memory-Based Shallow Parser (MBSP)). These are associated with each of the 145 authors or each of the four components of the Myers-Briggs Type Indicator (Introverted-Extraverted, Sensing-iNtuitive, Thinking-Feeling, Judging-Perceiving). Authorship attribution on 145 authors achieves results around 50{\%}-accuracy. Preliminary results indicate that the first two personality dimensions can be predicted fairly accurately.
Traditional Authorship Attribution models extract normalized counts of lexical elements such as nouns, common words and punctuation and use these normalized counts or ratios as features for author fingerprinting. The text is viewed as a bag-of-words and the order of words and their position relative to other words is largely ignored. We propose a new method of feature extraction which quantifies the distribution of lexical elements within the text using Kolmogorov complexity estimates. Testing carried out on blog corpora indicates that such measures outperform ratios when used as features in an SVM authorship attribution model. Moreover, by adding complexity estimates to a model using ratios, we were able to increase the F-measure by 5.2-11.8{\%}
This paper presents the results of a joint effort of a group of multimodality researchers and tool developers to improve the interoperability between several tools used for the annotation of multimodality. We propose a multimodal annotation exchange format, based on the annotation graph formalism, which is supported by import and export routines in the respective tools.
Even though a wealth of speech data is available for the dialog systems research community, the particular field of situated language has yet to find an appropriate free resource. The corpus required to answer research questions related to situated language should connect world information to the human language. In this paper we report on the release of a corpus of English spontaneous instruction giving situated dialogs. The corpus was collected using the Quake environment, a first-person virtual reality game, and consists of pairs of participants completing a direction giver- direction follower scenario. The corpus contains the collected audio and video, as well as word-aligned transcriptions and the positional/gaze information of the player. Referring expressions in the corpus are annotated with the IDs of their virtual world referents.
The Data Category Registry is one of the ISO initiatives towards the establishment of standards for Language Resource management, creation and coding. Successful application of the DCR depends on the availability of tools that can interact with it. This paper describes the first steps that have been taken to provide users of the multimedia annotation tool ELAN, with the means to create references from tiers and annotations to data categories defined in the ISO Data Category Registry. It first gives a brief description of the capabilities of ELAN and the structure of the documents it creates. After a concise overview of the goals and current state of the ISO DCR infrastructure, a description is given of how the preliminary connectivity with the DCR is implemented in ELAN.
In the context of the CATCH research program that is currently carried out at a number of large Dutch cultural heritage institutions our ambition is to combine and exchange heterogeneous multimedia annotations between projects and institutions. As first step we designed an Annotation Meta Model: a simple but powerful RDF/OWL model mainly addressing the anchoring of annotations to segments of the many different media types used in the collections of the archives, museums and libraries involved. The model includes support for the annotation of annotations themselves, and of segments of annotation values, to be able to layer annotations and in this way enable projects to process each others annotation data as the primary data for further annotation. On basis of AMM we designed an application programming interface for accessing annotation repositories and implemented it both as a software library and as a web service. Finally, we report on our experiences with the application of model, API and repository when developing web applications for collection managers in cultural heritage institutions.
The paper presents a project of the Laboratoire Parole {\&} Langage which aims at collecting, annotating and exploiting a corpus of spoken French in a multimodal perspective. The project directly meets the present needs in linguistics where a growing number of researchers become aware of the fact that a theory of communication which aims at describing real interactions should take into account the complexity of these interactions. However, in order to take into account such a complexity, linguists should have access to spoken corpora annotated in different fields. The paper presents the annotation schemes used in phonetics, morphology and syntax, prosody, gestuality at the LPL together with the type of linguistic description made from the annotations seen in two examples.
This paper describes an attempt to use the information contained in VerbNet to obtain change of location inferences. We show that the information is available but not encoded in a consistent enough form to be optimally useful.
Several studies indicate that the level of predicate-argument structure is relevant for modeling prevalent phenomena in current textual entailment corpora. Although large resources like FrameNet have recently become available, attempts to integrate this type of information into a system for textual entailment did not confirm the expected gain in performance. The reasons for this are not fully obvious; candidates include FrameNets restricted coverage, limitations of semantic parsers, or insufficient modeling of FrameNet information. To enable further insight on this issue, in this paper we present FATE (FrameNet-Annotated Textual Entailment), a manually crafted, fully reliable frame-annotated RTE corpus. The annotation has been carried out over the 800 pairs of the RTE-2 test set. This dataset offers a safe basis for RTE systems to experiment, and enables researchers to develop clearer ideas on how to effectively integrate frame knowledge in semantic inferenence tasks like recognizing textual entailment. We describe and present statistics over the adopted annotation, which introduces a new schema based on full-text annotation of so called relevant frame evoking elements.
This paper describes a method of accurately projecting Propbank roles onto constituents in the CCGbank and automatically annotating verbal categories with the semantic roles of their arguments. This method will be used to improve the structure of the derivations in the CCGbank and to facilitate research on semantic role tagging and broad coverage generation with CCG.
Cornetto is a two-year Stevin project (project number STE05039) in which a lexical semantic database is built that combines Wordnet with Framenet-like information for Dutch. The combination of the two lexical resources (the Dutch Wordnet and the Referentie Bestand Nederlands) will result in a much richer relational database that may improve natural language processing (NLP) technologies, such as word sense-disambiguation, and language-generation systems. In addition to merging the Dutch lexicons, the database is also mapped to a formal ontology to provide a more solid semantic backbone. Since the database represents different traditions and perspectives of semantic organization, a key issue in the project is the alignment of concepts across the resources. This paper discusses our methodology to first automatically align the word meanings and secondly to manually revise the most critical cases.
This paper presents the complete and consistent ontological annotation of the nominal part of WordNet. The annotation has been carried out using the semantic features defined in the EuroWordNet Top Concept Ontology and made available to the NLP community. Up to now only an initial core set of 1,024 synsets, the so-called Base Concepts, was ontologized in such a way. The work has been achieved by following a methodology based on an iterative and incremental expansion of the initial labeling through the hierarchy while setting inheritance blockage points. Since this labeling has been set on the EuroWordNets Interlingual Index (ILI), it can be also used to populate any other wordnet linked to it through a simple porting process. This feature-annotated WordNet is intended to be useful for a large number of semantic NLP tasks and for testing for the first time componential analysis on real environments. Moreover, the quantitative analysis of the work shows that more than 40{\%} of the nominal part of WordNet is involved in structure errors or inadequacies.
People use the Internet to find a wide variety of images. Existing image search engines do not understand the pictures they return. The introduction of semantic layers in information retrieval frameworks may enhance the quality of the results compared to existing systems. One important challenge in the field is to develop architectures that fit the requirements of real-life applications, like the Internet search engines. In this paper, we describe Olive, an image retrieval application that exploits a large scale conceptual hierarchy (extracted from WordNet) to automatically reformulate user queries, search for associated images and present results in an interactive and structured fashion. When searching a concept in the hierarchy, Olive reformulates the query using its deepest subtypes in WordNet. On the answers page, the system displays a selection of related classes and proposes a content based retrieval functionality among the pictures sharing the same linguistic label. In order to validate our approach, we run to series of tests to assess the performances of the application and report the results here. First, two precision evaluations over a panel of concepts from different domains are realized and second, a user test is designed so as to assess the interaction with the system.
Language models used in current automatic speech recognition systems are trained on general-purpose corpora and are therefore not relevant to transcribe spoken documents dealing with successive precise topics, such as long multimedia streams, frequently tacking reportages and debates. To overcome this problem, this paper shows that Web resources and natural language processing techniques can be effective to automatically adapt the baseline language model of an automatic speech recognition system to any encountered topic. More precisely, we detail how to characterize the topic of transcription segment and how to collect Web pages from which a topic-specific language model can be trained. Then, an adapted language model is obtained by combining the topic-specific language model with the general-purpose language model. Finally, new transcriptions are generated using the adapted language model and are compared with transcriptions previously obtained with the baseline language model. Experiments show that our topic adaptation technique leads to significant transcription quality gains.
Most of the Web-based methods for lexicon augmenting consist in capturing global semantic features of the targeted domain in order to collect relevant documents from the Web. We suggest that the local context of the out-of-vocabulary (OOV) words contains relevant information on the OOV words. With this information, we propose to use the Web to build locally-augmented lexicons which are used in a final local decoding pass. First, an automatic web based OOV word detection method is proposed. Then, we demonstrate the relevance of the Web for the OOV word retrieval. Different methods are proposed to retrieve the hypothesis words. We finally retrieve about 26{\%} of the OOV words with a lexicon increase of less than 1000 words using the reference context.
The download first, then process paradigm is still the predominant working method amongst the research community. The web-based paradigm, however, offers many advantages from a tool development and data management perspective as they allow a quick adaptation to changing research environments. Moreover, new ways of combining tools and data are increasingly becoming available and will eventually enable a true web-based workflow approach, thus challenging the download first, then process paradigm. The necessary infrastructure for managing, exploring and enriching language resources via the Web will need to be delivered by projects like CLARIN and DARIAH.
Nowadays portable devices such as smart phones can be used to capture the face of a user simultaneously with the voice input. Server based or even embedded dialogue system might utilize this additional information to detect whether the speaking user addresses the system or other parties or whether the listening user is focused on the display or not. Depending on these findings the dialogue system might change its strategy to interact with the user improving the overall communication between human and system. To develop and test methods for On/Off-Focus detection a multimodal corpus of user-machine interactions was recorded within the German SmartWeb project. The corpus comprises 99 recording sessions of a triad communication between the user, the system and a human companion. The user can address/watch/listen to the system but also talk to his companion, read from the display or simply talk to herself. Facial video is captured with a standard built-in video camera of a smart phone while voice input in being recorded by a high quality close microphone as well as over a realistic transmission line via Bluetooth and WCDMA. The resulting SmartWeb Video Corpus (SVC) can be obtained from the Bavarian Archive for Speech Signals.
This paper presents a corpus-based study of the discourse connective in contrast. The corpus data are drawn from the British National Corpus (BNC) and are analyzed at the levels of syntax, discourse structure, and compositional semantics. Following Webber et al. (2003), the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential.
We present initial results from an international and multi-disciplinary research collaboration that aims at the construction of a reference corpus of web genres. The primary application scenario for which we plan to build this resource is the automatic identification of web genres. Web genres are rather difficult to capture and to describe in their entirety, but we plan for the finished reference corpus to contain multi-level tags of the respective genre or genres a web document or a website instantiates. As the construction of such a corpus is by no means a trivial task, we discuss several alternatives that are, for the time being, mostly based on existing collections. Furthermore, we discuss a shared set of genre categories and a multi-purpose tool as two additional prerequisites for a reference corpus of web genres.
State-of-the-art coreference resolution engines show similar performance figures (low sixties on the MUC-7 data). Our system with a rich linguistically motivated feature set yields significantly better performance values for a variety of machine learners, but still leaves substantial room for improvement. In this paper we address a relatively unexplored area of coreference resolution - we present a detailed error analysis in order to understand the issues raised by corpus-based approaches to coreference resolution.
The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the discourse relations in Czech. We primarily focus on the description of the syntactically motivated relations in discourse, basing our findings on the theoretical background of the Prague Dependency Treebank 2.0 and the Penn Discourse Treebank 2. Our aim is to revisit the present-day syntactico-semantic (tectogrammatical) annotation in the Prague Dependency Treebank, extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new, discourse level of annotation. In this paper, we propose a feasible process of such a transfer, comparing the possibilities the Praguian dependency-based approach offers with the Penn discourse annotation based primarily on the analysis and classification of discourse connectives.
This paper describes a newly created text corpus of news articles that has been annotated for cross-document co-reference. Being able to robustly resolve references to entities across document boundaries will provide a useful capability for a variety of tasks, ranging from practical information retrieval applications to challenging research in information extraction and natural language understanding. This annotated corpus is intended to encourage the development of systems that can more accurately address this problem. A manual annotation tool was developed that allowed the complete corpus to be searched for likely co-referring entity mentions. This corpus of 257K words links mentions of co-referent people, locations and organizations (subject to some additional constraints). Each of the documents had already been annotated for within-document co-reference by the LDC as part of the ACE series of evaluations. The annotation process was bootstrapped with a string-matching-based linking procedure, and we report on some of initial experimentation with the data. The cross-document linking information will be made publicly available.
This paper presents the automatic extension of Princeton WordNet with Named Entities (NEs). This new resource is called Named Entity WordNet. Our method maps the noun is-a hierarchy of WordNet to Wikipedia categories, identifies the NEs present in the latter and extracts different information from them such as written variants, definitions, etc. This information is inserted into a NE repository. A module that converts from this generic repository to the WordNet specific format has been developed. The paper explores different aspects of our methodology such as the treatment of polysemous terms, the identification of hyponyms within the Wikipedia categorization system, the identification of Wikipedia articles which are NEs and the design of a NE repository compliant with the LMF ISO standard. So far, this procedure enriches WordNet with 310,742 NEs and 381,043 instance of relations.
This paper focuses on the influence of changing the text time frame on the performance of a named entity tagger. We followed a twofold approach to investigate this subject: on the one hand, we analyzed a corpus that spans 8 years, and, on the other hand, we assessed the performance of a name tagger trained and tested on that corpus. We created 8 samples from the corpus, each drawn from the articles for a particular year. In terms of corpus analysis, we calculated the corpus similarity and names shared between samples. To see the effect on tagger performance, we implemented a semi-supervised name tagger based on co-training; then, we trained and tested our tagger on those samples. We observed that corpus similarity, names shared between samples, and tagger performance all decay as the time gap between the samples increases. Furthermore, we observed that the corpus similarity and names shared correlate with the tagger F-measure. These results show that named entity recognition systems may become obsolete in a short period of time.
We discuss a named entity recognition system for Arabic, and show how we incorporated the information provided by MADA, a full morphological tagger which uses a morphological analyzer. Surprisingly, the relevant features used are the capitalization of the English gloss chosen by the tagger, and the fact that an analysis is returned (that a word is not OOV to the morphological analyzer). The use of the tagger also improves over a third system which just uses a morphological analyzer, yielding a 14{\textbackslash}{\%} reduction in error over the baseline. We conduct a thorough error analysis to identify sources of success and failure among the variations, and show that by combining the systems in simple ways we can significantly influence the precision-recall trade-off.
Foreign name expressions written in Chinese characters are difficult to recognize since the sequence of characters represents the Chinese pronunciation of the name. This paper suggests that known English or German person names can reliably be identified on the basis of the similarity between the Chinese and the foreign pronunciation. In addition to locating a person name in the text and learning that it is foreign, the corresponding foreign name is identified, thus gaining precious additional information for cross-lingual applications. This idea is implemented as a statistical module into the rule-based shallow parsing system SProUT, forming the HyFex system. The statistical component is invoked if a sequence of trigger characters is found that may correspond to a foreign name. Their phonetic Pinyin representation is produced and compared to the phonetic representations (SAMPA) of given foreign names, which are generated by the MARY TTS system for German and English pronunciations. This comparison is achieved by a hand-crafted metric that assigns costs to specific edit operations. The person name corresponding to the SAMPA representation with the lowest costs attached is returned as the most similar result, if a threshold is not exceeded. Our evaluation on publicly available data shows competitive results.
We introduce a low-complexity method for acquiring fine-grained classes of named entities from the Web. The method exploits the large amounts of textual data available on the Web, while avoiding the use of any expensive text processing techniques or tools. The quality of the extracted classes is encouraging with respect to both the precision of the sets of named entities acquired within various classes, and the labels assigned to the sets of named entities.
For a language pair such as Chinese and Korean that belong to entirely different language families in terms of typology and genealogy, finding the correspondences is quite obscure in word alignment. We present annotation guidelines for Chinese-Korean word alignment through contrastive analysis of morpho-syntactic encodings. We discuss the differences in verbal systems that cause most of linking obscurities in annotation process. Systematic comparison of verbal systems is conducted by analyzing morpho-syntactic encodings. The viewpoint of grammatical category allows us to define consistent and systematic instructions for linguistically distant languages such as Chinese and Korean. The scope of our guidelines is limited to the alignment between Chinese and Korean, but the instruction methods exemplified in this paper are also applicable in developing systematic and comprehensible alignment guidelines for other languages having such different linguistic phenomena.
This paper describes CzEng 0.7, a new release of Czech-English parallel corpus freely available for research and educational purposes. We provide basic statistics of the corpus and focus on data produced by a community of volunteers. Anonymous contributors manually correct the output of a machine translation (MT) system, generating on average 2000 sentences a month, 70{\%} of which are indeed correct translations. We compare the utility of community-supplied and of professionally translated training data for a baseline English-to-Czech MT system.
Data Selection has emerged as a common issue in language technologies. We define Data Selection as the choosing of a subset of training data that is most effective for a given task. This paper describes deductive feature detection, one component of a data selection system for machine translation. Feature detection determines whether features such as tense, number, and person are expressed in a language. The database of the The World Atlas of Language Structures provides a gold standard against which to evaluate feature detection. The discovered features can be used as input to a Navigator, which uses active learning to determine which piece of language data is the most important to acquire next.
This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data.
SECTra{\_}w is a web-oriented system mainly dedicated to the evaluation of MT systems. After importing a source corpus, and possibly reference translations, one can call various MT systems, store their results, and have a collection of human judges perform subjective evaluation online (fluidity, adequacy). It is also possible to perform objective, task-oriented evaluation by letting humans post-edit the MT results, using a web translation editor, and measuring an edit distance and/or the post-editing time. The post-edited results can be added to the set of reference translations, or constitute it if there were no references. SECTra{\_}w makes it possible to show not only tables of figures as results of an evaluation campaign, but also the real data (source, MT outputs, references, post-edited outputs), and to make the post-edition effort sensible by transforming the trace of the edit distance computation in an intuitive presentation, much like a revision presentation in Word. The system is written in java under Xwiki and uses the Ajax technique. It can handle large, multilingual and multimedia corpora: EuroParl, BTEC, ERIM (bilingual interpreted dialogues with audio and text), Unesco-B@bel, and a test corpus by France Telecom have been loaded together and used in tests.
We have analyzed system rankings for person name search algorithms using a data set for which several versions of ground truth were developed by employing different means of resolving adjudicator conflicts. Thirteen algorithms were ranked by F-score, using bootstrap resampling for significance testing, on a dataset containing 70,000 romanized names from various cultures. We found some disagreement among the four adjudicators, with kappa ranging from 0.57 to 0.78. Truth sets based on a single adjudicator, and on the intersection or union of positive adjudications produced sizeable variability in scoring sensitivity - and to a lesser degree rank order - compared to the consensus truth set. However, results on truth sets constructed by randomly choosing an adjudicator for each item were highly consistent with the consensus. The implication is that an evaluation where one adjudicator has judged each item is nearly as good as a more expensive and labor-intensive one where multiple adjudicators have judged each item and conflicts are resolved through voting.
We propose in this paper an automatic evaluation procedure based on a metric which could provide summary evaluation without human assistance. Our system includes two metrics, which are presented and discussed. The first metric is based on a known and powerful statistical test, the X2 goodness-of-fit test, and has been used in several applications. The second metric is derived from three common metrics used to evaluate Natural Language Processing (NLP) systems, namely precision, recall and f-measure. The combination of these two metrics is intended to allow one to assess the quality of summaries quickly, cheaply and without the need of human intervention, minimizing though, the role of subjective judgment and bias.
The availability of a huge mass of textual data in electronic format has increased the need for fast and accurate techniques for textual data processing. Machine learning and statistical approaches have been increasingly used in NLP since a decade, mainly because they are quick, versatile and efficient. However, despite this evolution of the field, evaluation still rely (most of the time) on a comparison between the output of a probabilistic or statistical system on the one hand, and a non-statistic, most of the time hand-crafted, gold standard on the other hand. In this paper, we take the example of the acquisition of subcategorization frames from corpora as a practical example. Our study is motivated by the fact that, even if a gold standard is an invaluable resource for evaluation, a gold standard is always partial and does not really show how accurate and useful results are.
This paper shows how a research and industry stimulation programme on human language technologies (HLT) for Dutch can be enhanced with more specific innovation policy aspects to support the take-up by the HLT industry in the Netherlands and Flanders. Important to note is the distinction between the HLT programme itself (called STEVIN) with its specific related committees and actions and the overall policy instruments (HLT Agency, HLT steering board?) that try to span the entire domain of HLT for Dutch and have a more permanent character. The establishment of a pricing committee and a PR {\&} communication working group is explained as a consequence of adopting the notion of innovation system as a theoretical framework. It means that a stronger emphasis is put on improving knowledge transfer and exchange amongst actors in the field. Therefore, the focus at the programme management level is shifting from the projects research activities producing results to gathering the results, making them available at a certain cost and advertising them through the appropriate channels to the appropriate potential customers. Our conclusion is that this policy stimulates the transfer from academia to industry though it is too soon for an in-depth assessment of the STEVIN programme and other HLT innovation policy instruments.
This paper, the fifth in a series of biennial progress reports, reviews the activities of the Linguistic Data Consortium with particular emphasis on general trends in the language resource landscape and on changes that distinguish the two years since LDCs last report at LREC from the preceding 8 years. After providing a perspective on the current landscape of language resources, the paper goes on to describe our vision of the role of LDC within the research communities it serves before sketching briefly specific publications and resources creations projects that have been the focus our attention since the last report.
Developing resources which can be used for Natural Language Processing is an extremely difficult task for any language, but is even more so for less privileged (or less computerized) languages. One way to overcome this difficulty is to adapt the resources of a linguistically close resource rich language. In this paper we discuss how the cost of such adaption can be estimated using subjective and objective measures of linguistic similarity for allocating financial resources, time, manpower etc. Since this is the first work of its kind, the method described in this paper should be seen as only a preliminary method, indicative of how better methods can be developed. Corpora of several less computerized languages had to be collected for the work described in the paper, which was difficult because for many of these varieties there is not much electronic data available. Even if it is, it is in non-standard encodings, which means that we had to build encoding converters for these varieties. The varieties we have focused on are some of the varieties spoken in the South Asian region.
This paper describes the latest developments in ELRAs services within the field of Language Resources (LR). These developments focus on 4 main groups of activities: the identification and distribution of Language Resources; the production of LRs; the evaluation of Human Language Technology (HLT), and the dissemination of information in the field. ELRAs initial work on the distribution of language resources has evolved throughout the years, currently covering a much wider range of activities that have been considered crucial for the current needs of the R{\&}D community and the good health of the LR world. Regarding distribution, considerable work has been done on a broader identification, which does not only consider resources to be immediately negotiated for distribution but which aims to inform on all available resources. This has been the seed for the Universal Catalogue. Furthermore, a Catalogue of LRs with favourable conditions for R{\&}D has also been created. Moreover, the different activities in what regards identification on demand, production within different frameworks, evaluation of language technologies and participation in evaluation campaigns, as well as our very specific focus on information dissemination are described in detail in this paper.
The importance of evaluation in promoting research and development in the information retrieval and natural language processing domains has long been recognised but is this sufficient? In many areas there is still a considerable gap between the results achieved by the research community and their implementation in commercial applications. This is particularly true for the cross-language or multilingual retrieval areas. Despite the strong demand for and interest in multilingual IR functionality, there are still very few operational systems on offer. The Cross Language Evaluation Forum (CLEF) is now taking steps aimed at changing this situation. The paper provides a critical assessment of the main results achieved by CLEF so far and discusses plans now underway to extend its activities in order to have a more direct impact on the application sector.
It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions.
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain. In our experiments, we built a simple machine learning-based pronoun resolution system, and evaluated the system on three different corpora: MUC, ACE, and GENIA. Comparative statistics not only reveal the noticeable issues in constructing an effective pronoun resolution system for a new domain, but also provides a comprehensive view of those corpora often used for this task.
This paper discusses the problem of utilising multiply annotated data in training biomedical information extraction systems. Two corpora, annotated with entities and relations, and containing a number of multiply annotated documents, are used to train named entity recognition and relation extraction systems. Several methods of automatically combining the multiple annotations to produce a single annotation are compared, but none produces better results than simply picking one of the annotated versions at random. It is also shown that adding extra singly annotated documents produces faster performance gains than adding extra multiply annotated documents.
We report the construction of a corpus for parser evaluation in the biomedical domain. A 50-abstract subset (492 sentences) of the GENIA corpus (Kim et al., 2003) is annotated with labeled head-dependent relations using the grammatical relations (GR) evaluation scheme (Carroll et al., 1998) ,which has been used for parser evaluation in the newswire domain.
In biomedical articles, terms with the same surface forms are often used to refer to different entities across a number of model organisms, in which case determining the species becomes crucial to term identification systems that ground terms to specific database identifiers. This paper describes a rule-based system that extracts species indicating words, such as human or murine, which can be used to decide the species of the nearby entity terms, and a machine-learning species disambiguation system that was developed on manually species-annotated corpora. Performance of both systems were evaluated on gold-standard datasets, where the machine-learning system yielded better overall results.
Many of the beliefs that one uses to reason about everyday entities and events are neither strictly true or even logically consistent. Rather, people appear to rely on a large body of folk knowledge in the form of stereotypical associations, clich{\'e}s and other kinds of naturalistic descriptions, many of which express views of the world that are second-hand, overly-simplified and, in some cases, non-literal to the point of being poetic. These descriptions pervade our language yet one rarely finds them in authoritative linguistic resources like dictionaries and encyclopaedias. We describe here how such naturalistic descriptions can be harvested from the web in the guise of explicit similes and related text patterns, and empirically demonstrate that these descriptions do broadly capture the way people see the world, at least from the perspective of category organization in an ontology.
We present and partially evaluate procedures for the extraction of noun+verb collocation candidates from German text corpora, along with their morphosyntactic preferences, especially for the active vs. passive voice. We start from tokenized, tagged, lemmatized and chunked text, and we use extraction patterns formulated in the CQP corpus query language. We discuss the results of a precision evaluation, on administrative texts from the European Union: we find a considerable amount of specialized collocations, as well as general ones and complex predicates; overall the precision is considerably higher than that of a statistical extractor used as a baseline.
In this paper we describe the construction of an illustrated Japanese Wordnet. We bootstrap the Wordnet using existing multiple existing wordnets in order to deal with the ambiguity inherent in translation. We illustrate it with pictures from the Open Clip Art Library.
The construction of a wordnet, a labour-intensive enterprise, can be significantly assisted by automatic grouping of lexical material and discovery of lexical semantic relations. The objective is to ensure high quality of automatically acquired results before they are presented for lexicographers approval. We discuss a software tool that suggests synset members using a measure of semantic relatedness with a given verb or adjective; this extends previous work on nominal synsets in Polish WordNet. Syntactically-motivated constraints are deployed on a large morphologically annotated corpus of Polish. Evaluation has been performed via the WordNet-Based Similarity Test and additionally supported by human raters. A lexicographer also manually assessed a suitable sample of suggestions. The results compare favourably with other known methods of acquiring semantic relations.
This paper introduces an ongoing work on developing verb frames for Hindi. Verb frames capture syntactic commonalities of semantically related verbs. The main objective of this work is to create a linguistic resource which will prove to be indispensable for various NLP applications. We also hope this resource to help us better understand Hindi verbs. We motivate the basic verb argument structure using relations as introduced by Panini. We show the methodology used in preparing these frames and the criteria followed for classifying Hindi verbs.
We present a corpus of spoken dialogues between students and an adaptive Wizard-of-Oz tutoring system, in which student uncertainty was manually annotated in real-time. We detail the corpus contents, including speech files, transcripts, annotations, and log files, and we discuss possible future uses by the computational linguistics community as a novel resource for studying naturally occurring user affect and adaptation in complex spoken dialogue systems.
This paper reports on the creation of the multimodal NIMITEK corpus of affected behavior in human-machine interaction and its role in the development of the NIMITEK prototype system. The NIMITEK prototype system is a spoken dialogue system for supporting users while they solve problems in a graphics system. The central feature of the system is adaptive dialogue management. The system dynamically defines a dialogue strategy according to the current state of the interaction (including also the emotional state of the user). Particular emphasis is devoted to the level of naturalness of interaction. We discuss that a higher level of naturalness can be achieved by combining a habitable natural language interface and an appropriate dialogue strategy. The role of the NIMITEK multimodal corpus in achieving these requirements is twofold: (1) in developing the model of attentional state on the level of users commands that facilitates processing of flexibly formulated commands, and (2) in defining the dialogue strategy that takes the emotional state of the user into account. Finally, we sketch the implemented prototype system and describe the incorporated dialogue management module. Whereas the prototype system itself is task-specific, the described underlying concepts are intended to be task-independent.
The goal of this work is to introduce an architecture to automatically detect the amount of stress in the speech signal close to real time. For this an experimental setup to record speech rich in vocabulary and containing different stress levels is presented. Additionally, an experiment explaining the labeling process with a thorough analysis of the labeled data is presented. Fifteen subjects were asked to play an air controller simulation that gradually induced more stress by becoming more difficult to control. During this game the subjects were asked to answer questions, which were then labeled by a different set of subjects in order to receive a subjective target value for each of the answers. A recurrent neural network was used to measure the amount of stress contained in the utterances after training. The neural network estimated the amount of stress at a frequency of 25 Hz and outperformed the human baseline.
In order to improve the flexibility and the precision of an automatic phone segmentation system for a type of expressive speech, the dubbing into French of fiction movies, we developed both the phonetic labeling process and the alignment process. The automatic labelling system relies on an automatic grapheme-to-phoneme conversion including all the variants of the phonetic chain and on HMM modeling. In this article, we will distinguish three sets of phone models: a set of context independent models, a set of left and right context dependant models and finally a mixing of the two that combines phone and triphone models according to the precision of alignment obtained for each phonetic broad-class. The three models are evaluated on a test corpus. On the one hand we notice a little decrease in the score of phonetic labelling mainly due to pauses insertions, but on the other hand the mixed set of models gives the best results for the score of precision of the alignment.
A Hungarian multimodal spontaneous expressive speech corpus was recorded following the methodology of a similar French corpus. The method relied on a Wizard of Oz scenario-based induction of varying affective states. The subjects were interacting with a supposedly voice-recognition driven computer application using simple command words. Audio and video signals were captured for the 7 recorded subjects. After the experiment, the subjects watched the video recording of their session and labelled the recorded corpus themselves, freely describing the evolution of their affective states. The obtained labels were later classified into one of the following broad emotional categories: satisfaction, dislike, stress, or other. A listening test was performed by 25 na{\"\i
Polarizing discussions about political and social issues are common in mass media. Annotations on the degree to which a sentence expresses an ideological perspective can be valuable for evaluating computer programs that can automatically identify strongly biased sentences, but such annotations remain scarce. We annotated the intensity of ideological perspectives expressed in 250 sentences by aggregating judgments from 18 annotators. We proposed methods of determining the number of annotators and assessing reliability, and showed the annotations were highly consistent across different annotator groups.
This paper introduces a method for creating a subjectivity lexicon for languages with scarce resources. The method is able to build a subjectivity lexicon by using a small seed set of subjective words, an online dictionary, and a small raw corpus, coupled with a bootstrapping process that ranks new candidate words based on a similarity measure. Experiments performed with a rule-based sentence level subjectivity classifier show an 18{\%} absolute improvement in F-measure as compared to previously proposed semi-supervised methods.
As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.
Fine-grained subjectivity analysis has been the subject of much recent research attention. As a result, the field has gained a number of working definitions, technical approaches and manually annotated corpora that cover many facets of subjectivity. Little work has been done, however, on one aspect of fine-grained opinions - the specification and identification of opinion topics. In particular, due to the difficulty of manual opinion topic annotation, no general-purpose opinion corpus with information about topics of fine-grained opinions currently exists. In this paper, we propose a methodology for the manual annotation of opinion topics and use it to annotate a portion of an existing general-purpose opinion corpus with opinion topic information. Inter-annotator agreement results according to a number of metrics suggest that the annotations are reliable.
In this paper, we investigate quasi-abstractive summaries, a new type of machine-generated summaries that do not use whole sentences, but only fragments from the source. Quasi-abstractive summaries aim at bridging the gap between human-written abstracts and extractive summaries. We present an approach that learns how to identify sets of sentences, where each set contains fragments that can be used to produce one sentence in the abstract; and then uses these sets to produce the abstract itself. Our experiments show very promising results. Importantly, we obtain our best results when the summary generation is anchored by the most salient Noun Phrases predicted from the text to be summarized.
Krahmer et al.s (2003) graph-based framework provides an elegant and flexible approach to the generation of referring expressions. In this paper, we present the first reported study that systematically investigates how to tune the parameters of the graph-based framework on the basis of a corpus of human-generated descriptions. We focus in particular on replicating the redundant nature of human referring expressions, whereby properties not strictly necessary for identifying a referent are nonetheless included in descriptions. We show how statistics derived from the corpus data can be integrated to boost the frameworks performance over a non-stochastic baseline.
Arrau is a new corpus annotated for anaphoric relations, with information about agreement and explicit representation of multiple antecedents for ambiguous anaphoric expressions and discourse antecedents for expressions which refer to abstract entities such as events, actions and plans. The corpus contains texts from different genres: task-oriented dialogues from the Trains-91 and Trains-93 corpus, narratives from the English Pear Stories corpus, newspaper articles from the Wall Street Journal portion of the Penn Treebank, and mixed text from the Gnome corpus.
In this paper we investigate the coverage of the two knowledge sources WordNet and Wikipedia for the task of bridging resolution. We report on an annotation experiment which yielded pairs of bridging anaphors and their antecedents in spoken multi-party dialog. Manual inspection of the two knowledge sources showed that, with some interesting exceptions, Wikipedia is superior to WordNet when it comes to the coverage of information necessary to resolve the bridging anaphors in our data set. We further describe a simple procedure for the automatic extraction of the required knowledge from Wikipedia by means of an API, and discuss some of the implications of the procedures performance.
We present the second version of the Penn Discourse Treebank, PDTB-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between PDTB-1.0 and PDTB-2.0. We present representative statistics for several aspects of the annotation in the corpus.
We present the main outcomes of the COREA project: a corpus annotated with coreferential relations and a coreference resolution system for Dutch. In the project we developed annotation guidelines for coreference resolution for Dutch and annotated a corpus of 135K tokens. We discuss these guidelines, the annotation tool, and the inter-annotator agreement. We also show a visualization of the annotated relations. The standard approach to evaluate a coreference resolution system is to compare the predictions of the system to a hand-annotated gold standard test set (cross-validation). A more practically oriented evaluation is to test the usefulness of coreference relation information in an NLP application. We run experiments with an Information Extraction module for the medical domain, and measure the performance of this module with and without the coreference relation information. We present the results of both this application-oriented evaluation of our system and of a standard cross-validation evaluation. In a separate experiment we also evaluate the effect of coreference information produced by a simple rule-based coreference module in a Question Answering application.
This paper describes an accurate, extensible method for automatically classifying unknown foreign words that requires minimal monolingual resources and no bilingual training data (which is often difficult to obtain for an arbitrary language pair). We use a small set of phonologically-based transliteration rules to generate a potentially unlimited amount of pseudo-data that can be used to train a classifier to distinguish etymological classes of actual words. We ran a series of experiments on identifying English loanwords in Korean, in order to explore the consequences of using pseudo-data in place of the original training data. Results show that a sufficient quantity of automatically generated training data, even produced by fairly low precision transliteration rules, can be used to train a classifier that performs within 0.3{\%} of one trained on actual English loanwords (96{\%} accuracy).
Semantic databases are a stable starting point in developing knowledge based systems. Since creating language resources demands many temporal, financial and human resources, a possible solution could be the import of a resource annotation from one language to another. This paper presents the creation of a semantic role database for Romanian, starting from the English FrameNet semantic resource. The intuition behind the importing program is that most of the frames defined in the English FN are likely to be valid cross-lingual, since semantic frames express conceptual structures, language independent at the deep structure level. The surface realization, the surface level, is realized according to each language syntactic constraints. In the paper we present the advantages of choosing to import the English FrameNet annotation, instead of annotating a new corpus. We also take into account the mismatches encountered in the validation process. The rules created to manage particular situations are used to improve the import program. We believe the information and argumentations in this paper could be of interest for those who wish develop FrameNet-like systems for other languages.
In this paper, we reported experiments of unsupervised automatic acquisition of Italian and English verb subcategorization frames (SCFs) from general and domain corpora. The proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about SCFs. Although preliminary, reported results are in line with state-of-the-art lexical acquisition systems. The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle (MDL). First experiments in this direction were carried out on Italian verbs with encouraging results.
Case frames are an important knowledge base for a variety of natural language processing (NLP) systems. For the practical use of these systems in the real world, wide-coverage case frames are required. In order to acquire such large-scale case frames, in this paper, we automatically compile case frames from a large corpus. The resultant case frames that are compiled from the English Gigaword corpus contain 9,300 verb entries. The case frames include most examples of normal usage, and are ready to be used in numerous NLP analyzers and applications.
This paper addresses a specific case of the task of lexical acquisition understood as the induction of information about the linguistic characteristics of lexical items on the basis of information gathered from their occurrences in texts. Most of the recent works in the area of lexical acquisition have used methods that take as much textual data as possible as source of evidence, but their performance decreases notably when only few occurrences of a word are available. The importance of covering such low frequency items lies in the fact that a large quantity of the words in any particular collection of texts will be occurring few times, if not just once. Our work proposes to compensate the lack of information resorting to linguistic knowledge on the characteristics of lexical classes. This knowledge, obtained from a lexical typology, is formulated probabilistically to be used in a Bayesian method to maximize the information gathered from single occurrences as to predict the full set of characteristics of the word. Our results show that our method achieves better results than others for the treatment of low frequency items.
In this paper we briefly describe the BioSec multimodal biometric database and analyze its use in automatic text-dependent speaker recognition research. The paper is structured into four parts: a short introduction to the problem of text-dependent speaker recognition; a brief review of other existing databases, including monomodal text-dependent speaker recognition databases and multimodal biometric recognition databases; a description of the BioSec database; and, finally, an experimental section in which speaker recognition results on some of these databases are presented and compared, using the same underlying speaker recognition technique in all cases.
Speaker identification and verification systems have a poor performance when model training is done in one language while the testing is done in another. This situation is not unusual in multilingual environments, where people should be able to access the system in any language he or she prefers in each moment, without noticing a performance drop. In this work we study the possibility of using features derived from prosodic parameters in order to reinforce the language robustness of these systems. First the features properties in terms of language and session variability are studied, predicting an increase in the language robustness when frame-wise intonation and energy values are combined with traditional MFCC features. The experimental results confirm that these features provide an improvement in the speaker recognition rates under language-mismatch conditions. The whole study is carried out in the Basque Country, a bilingual region in which Basque and Spanish languages co-exist.
In this paper, we describe NineOneOne (9-1-1), a system designed to recognize and translate Spanish emergency calls for better dispatching. We analyze the research challenges in adapting speech translation technology to 9-1-1 domain. We report our initial research towards building the system and the results of our initial experiments.
Recent years have seen increased interest within the speaker recognition community in high-level features including, for example, lexical choice, idiomatic expressions or syntactic structures. The promise of speaker recognition in forensic applications drives development toward systems robust to channel differences by selecting features inherently robust to channel difference. Within the language recognition community, there is growing interest in differentiating not only languages but also mutually intelligible dialects of a single language. Decades of research in dialectology suggest that high-level features can enable systems to cluster speakers according to the dialects they speak. The Phanotics (Phonetic Annotation of Typicality in Conversational Speech) project seeks to identify high-level features characteristic of American dialects, annotate a corpus for these features, use the data to dialect recognition systems and also use the categorization to create better models for speaker recognition. The data, once published, should be useful to other developers of speaker and dialect recognition systems and to dialectologists and sociolinguists. We expect the methods will generalize well beyond the speakers, dialects, and languages discussed here and should, if successful, provide a model for how linguists and technology developers can collaborate in the future for the benefit of both groups and toward a deeper understanding of how languages vary and change.
The original Mixer corpus was designed to satisfy developing commercial and forensic needs. The resulting Mixer corpora, Phases 1 through 5, have evolved to support and increasing variety of research tasks, including multilingual and cross-channel recognition. The Mixer Phases 4 and 5 corpora feature a wider variety of channels and greater variation in the situations under which the speech is recorded. This paper focuses on the plans, progress and results of Mixer 4 and 5.
To answer the critical need for sharable, reusable annotated resources with rich linguistic annotations, we are developing a Manually Annotated Sub-Corpus (MASC) including texts from diverse genres and manual annotations or manually-validated annotations for multiple levels, including WordNet senses and FrameNet frames and frame elements, both of which have become significant resources in the international computational linguistics community. To derive maximal benefit from the semantic information provided by these resources, the MASC will also include manually-validated shallow parses and named entities, which will enable linking WordNet senses and FrameNet frames within the same sentences into more complex semantic structures and, because named entities will often be the role fillers of FrameNet frames, enrich the semantic and pragmatic information derivable from the sub-corpus. All MASC annotations will be published with detailed inter-annotator agreement measures. The MASC and its annotations will be freely downloadable from the ANC website, thus providing maximum accessibility for researchers from around the globe.
We propose a set of heuristics for improving annotation quality of very large corpora efficiently. The Xinhua News portion of the Chinese Gigaword Corpus was tagged independently with both the Peking University ICL tagset and the Academia Sinica CKIP tagset. The corpus-based POS tags mapping will serve as the basis of the possible contrast in grammatical systems between PRC and Taiwan. And it can serve as the basic model for mapping between the CKIP and ICL tagging systems for any data.
We describe the creation of a corpus that supports a real-world hierarchical text categorization task in the domain of electronic rulemaking (eRulemaking). Features of the task and of the eRulemaking domain engender both a non-traditional text categorization corpus and a correspondingly difficult machine learning task. Interannotator agreement results are presented for a group of six annotators. We also briefly describe the results of experiments that apply standard and hierarchical text categorization techniques to the eRulemaking data sets. The corpus is the first in a series of related sentence-level text categorization corpora to be developed in the eRulemaking domain.
Pattern matching, or querying, over annotations is a general purpose paradigm for inspecting, navigating, mining, and transforming annotation repositories - the common representation basis for modern pipelined text-processing frameworks. Configurability of such frameworks and expressiveness of feature structure-based annotation schemes account for the high density of some such annotation repositories. This particular characteristic makes challenging the design of a pattern matching engine, capable of interpreting (or imposing) flat patterns over an arbitrarily dense annotation lattice. We present an approach where a finite state device carries out the application of (compiled) grammars over what is, in effect, a linearized projection of a unique route through the lattice; a route derived by a mix of static pattern (grammar) analysis and interpretation of navigational directives within the extended grammar formalism. Our approach achieves a mix of finite state scanning and lattice traversal for expressive and efficient pattern matching in dense annotations stores.
Many applications of computational linguistics are greatly influenced by the quality of corpora available and as automatically generated corpora continue to play an increasingly common role, it is essential that we not overlook the importance of well-constructed and homogeneous corpora. This paper describes an automatic approach to improving the homogeneity of corpora using an unsupervised method of statistical outlier detection to find documents and segments that do not belong in a corpus. We consider collections of corpora that are homogeneous with respect to topic (i.e. about the same subject), or genre (written for the same audience or from the same source) and use a combination of stylistic and lexical features of the texts to automatically identify pieces of text in these collections that break the homogeneity. These pieces of text that are significantly different from the rest of the corpus are likely to be errors that are out of place and should be removed from the corpus before it is used for other tasks. We evaluate our techniques by running extensive experiments over large artificially constructed corpora that each contain single pieces of text from a different topic, author, or genre than the rest of the collection and measure the accuracy of identifying these pieces of text without the use of training data. We show that when these pieces of text are reasonably large (1,000 words) we can reliably identify them in a corpus.
We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses. In a previous paper (Carl, 2007) we have described how the graph of hypotheses is generated through shallow transfer and chunk permutation rules, where nodes consist of vectors representing morpho-syntactic properties of words and phrases. This paper describes a number of methods to train statistical feature functions from some of the vectors components. The feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best translation paths in the graph. We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) models of lemma-based feature functions produce better results than token-based models, 2) adding PoS-tag feature function to the lemma models improves the output and 3) weights for lexical translations are suited if the training material is similar to the texts to be translated.
We report on an on-going research project aimed at increasing the range of translation equivalents which can be automatically discovered by MT systems. The methodology is based on semi-supervised learning of indirect translation strategies from large comparable corpora and applying them in run-time to generate novel, previously unseen translation equivalents. This approach is different from methods based on parallel resources, which currently can reuse only individual translation equivalents. Instead it models translation strategies which generalise individual equivalents and can successfully generate an open class of new translation solutions. The task of the project is integration of the developed technology into open-source MT systems.
A statistical machine translation (SMT) system requires homogeneous training data in order to get domain-sensitive (or context-sensitive) terminology translations. If the data contains various domains, it is difficult for an SMT to learn context-sensitive terminology mappings probabilistically. Yet, terminology translation accuracy is an important issue for MT users. This paper explores an approach to tackle this terminology translation problem for an SMT. We propose a way to identify terminology translations from MT output and automatically swap them with user-defined translations. Our approach is simple and can be applied to any type of MT system. We call our prototype Term Swapper. Term Swapper allows MT users to draw on their own dictionaries without affecting any parts of the MT output except for the terminology translation(s) in question. Using an SMT developed at Microsoft Research, called MSR-MT (Quirk et al., (2005); Menezes {\&} Quirk (2005)), we conducted initial experiments to investigate the coverage rate of Term Swapper and its impact on the overall quality of MT output. The results from our experiments show high coverage and positive impact on the overall MT quality.
An important problem when using Stochastic Inversion Transduction Grammars is their computational cost. More specifically, when dealing with corpora such as Europarl. only one iteration of the estimation algorithm becomes prohibitive. In this work, we apply a reduction of the cost by taking profit of the bracketing information in parsed corpora and show machine translation results obtained with a bracketed Europarl corpus, yielding interresting improvements when increasing the number of non-terminal symbols.
The number and sizes of parallel corpora keep growing, which makes it necessary to have automatic methods of processing them: combining, checking and improving corpora quality, etc. We here introduce a method which enables performing many of these by exploiting overlapping parallel corpora. The method finds the correspondence between sentence pairs in two corpora: first the corresponding language parts of the corpora are aligned and then the two resulting alignments are compared. The method takes into consideration slight differences in the source documents, different levels of segmentation of the input corpora, encoding differences and other aspects of the task. The paper describes two experiments conducted to test the method. In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts. In the second experiment alternatively aligned versions of the JRC-Acquis are compared to each other with the example of all language pairs between English, Estonian and Latvian. Several additional conclusions about the corpora can be drawn from the results. The method proves to be effective for several parallel corpora processing tasks.
We evaluate discriminative parse reranking and parser self-training on a new English test set using four versions of the Charniak parser and a variety of parser evaluation metrics. The new test set consists of 1,000 hand-corrected British National Corpus parse trees. We directly evaluate parser output using both the Parseval and the Leaf Ancestor metrics. We also convert the hand-corrected and parser output phrase structure trees to dependency trees using a state-of-the-art functional tag labeller and constituent-to-dependency conversion tool, and then calculate label accuracy, unlabelled attachment and labelled attachment scores over the dependency structures. We find that reranking leads to a performance improvement on the new test set (albeit a modest one). We find that self-training using BNC data leads to significantly better results. However, it is not clear how effective self-training is when the training material comes from the North American News Corpus.
This paper presents EASY, which has been the first campaign evaluating syntactic parsers on all the common syntactic phenomena and a large set of dependency relations. The language analyzed was French. During this campaign, an annotation scheme has been elaborated with the different actors: participants and corpus providers; then a corpus made of several syntactic materials has been built and annotated: it reflects a great variety of linguistic styles (from literature to oral transcriptions, and from newspapers to medical texts). Both corpus and annotation scheme are here briefly presented. Moreover, evaluation measures are explained and detailed results are given. The results of the 15 parsers coming from 12 teams are analyzed. To conclude, a first experiment aiming to combine the outputs of the different systems is shown.
Recent years have seen increasing attention in temporal processing of texts as well as a lot of standardization effort of temporal information in natural language. A central part of this information lies in the temporal relations between events described in a text, when their precise times or dates are not known. Reliable human annotation of such information is difficult, and automatic comparisons must follow procedures beyond mere precision-recall of local pieces of information, since a coherent picture can only be considered at a global level. We address the problem of evaluation metrics of such information, aiming at fair comparisons between systems, by proposing some measures taking into account the globality of a text.
In this paper we present two experiments conducted for comparison of different language identification algorithms. Short words-, frequent words- and n-gram-based approaches are considered and combined with the Ad-Hoc Ranking classification method. The language identification process can be subdivided into two main steps: first a document model is generated for the document and a language model for the language; second the language of the document is determined on the basis of the language model and is added to the document as additional information. In this work we present our evaluation results and discuss the importance of a dynamic value for the out-of-place measure.
In this paper we present the PASSAGE project which aims at building automatically a French Treebank of large size by combining the output of several parsers, using the EASY annotation scheme. We present also the results of the of the first evaluation campaign of the project and the preliminary results we have obtained with our ROVER procedure for combining parsers automatically.
Structural metadata extraction (MDE) research aims to develop techniques for automatic conversion of raw speech recognition output to forms that are more useful to humans and to downstream automatic processes. It may be achieved by inserting boundaries of syntactic/semantic units to the flow of speech, labeling non-content words like filled pauses and discourse markers for optional removal, and identifying sections of disfluent speech. This paper compares two Czech MDE speech corpora, one in the domain of broadcast news and the other in the domain of broadcast conversations. A variety of statistics about fillers, edit disfluencies, and syntactic/semantic units are presented. In addition, it is reported that disfluent portions of speech show differences in the distribution of parts of speech (POS) of their content in comparison with the general POS distribution. The two Czech corpora are not only compared with each other, but also with available numbers relating to English MDE corpora of broadcast news and telephone conversations.
Large speech and text corpora are crucial to the development of a state-of-the-art speech recognition system. This paper reports on the construction and evaluation of the first Thai broadcast news speech and text corpora. Specifications and conventions used in the transcription process are described in the paper. The speech corpus contains about 17 hours of speech data while the text corpus was transcribed from around 35 hours of television broadcast news. The characteristics of the corpus were analyzed and shown in the paper. The speech corpus was split according to the evaluation focus condition used in the DARPA Hub-4 evaluation. An 18K-word Thai speech recognition system was setup to test with this speech corpus as a preliminary experiment. Acoustic model adaptations were performed to improve the system performance. The best system yielded a word error rate of about 20{\%} for clean and planned speech, and below 30{\%} for the overall condition.
This paper describes the Norwegian broadcast news speech corpus RUNDKAST. The corpus contains recordings of approximately 77 hours of broadcast news shows from the Norwegian broadcasting company NRK. The corpus covers both read and spontaneous speech as well as spontaneous dialogues and multipart discussions, including frequent occurrences of non-speech material (e.g. music, jingles). The recordings have large variations in speaking styles, dialect use and recording/transmission quality. RUNDKAST has been annotated for research in speech technology. The entire corpus has been manually segmented and transcribed using hierarchical levels. A subset of one hour of read and spontaneous speech from 10 different speakers has been manually annotated using broad phonetic labels. We provide a description of the database content, the annotation tools and strategies, and the conventions used for the different levels of annotation. A corpus of this kind has up to this point not been available for Norwegian, but is considered a necessary part of the infrastructure for language technology research in Norway. The RUNDKAST corpus is planned to be included in a future national Norwegian language resource bank.
In this paper we present an overview on the development of a large vocabulary continuous speech recognition (LVCSR) system for Khmer, the official language of Cambodia, spoken by more than 15 million people. As an under-resourced language, develop a LVCSR system for Khmer is a challenging task. We describe our methodologies for quick language data collection and processing for language modeling and acoustic modeling. For language modeling, we investigate the use of word and sub-word as basic modeling unit in order to see the potential of sub-word units in the case of unsegmented language like Khmer. Grapheme-based acoustic modeling is used to quickly build our Khmer language acoustic model. Furthermore, the approaches and tools used for the development of our system are documented and made publicly available on the web. We hope this will contribute to accelerate the development of LVCSR system for a new language, especially for under-resource languages of developing countries where resources and expertise are limited.
This paper describes the collect and transcription of a large set of Arabic broadcast news speech data. A total of more than 2000 hours of data was transcribed. The transcription factor for transcribing the broadcast news data has been reduced using a method such as Quick Rich Transcription (QRTR) as well as reducing the number of quality controls performed on the data. The data was collected from several Arabic TV and radio sources and from both Modern Standard Arabic and dialectal Arabic. The orthographic transcriptions included segmentation, speaker turns, topics, sentence unit types and a minimal noise mark-up. The transcripts were produced as a part of the GALE project.
This paper presents a general methodology to mapping EuroWordNets (Vossen, 1998) to the Suggested Upper Merged Ontology (SUMO; Niles and Pease (2001)), and we show its application to the French EuroWordNet. The process makes use of existing work on mapping Princeton WordNet (Fellbaum, 1998) to SUMO (Niles and Pease, 2003). After a general discussion of the usefulness of our approach, we provide details on the procedure of mapping individual EuroWordNet synsets to SUMO conceptual classes, and discuss issues arising from a fully automatic mapping. In addition to this, we present a quantitative analysis of the thus created semantic resource and discuss how the accuracy in determining the correct SUMO class for a particular EuroWordNet synset might be improved. Finally, we briefly hint at how such resources may be used, e.g. in order to extract selectional preferences of verbal predicates with respect to the ontological categories of their syntactic arguments.
Named Entities (NE) are regarded as an important type of semantic knowledge in many natural language processing (NLP) applications. Originally, a limited number of NE categories were proposed. In MUC, it was 7 categories - people, organization, location, time, date, money and percentage expressions. However, it was noticed that such a limited number of NE categories is too small for many applications. The author has proposed Extended Named Entity (ENE), which has about 200 categories (Sekine and Nobata 04). During the development of ENE, we noticed that many ENE categories have specific attributes, and those provide very important information for the entities. For example, rivers have attributes like source location, outflow, and length. Some such information is essential to knowing about the river, while the name is only a label which can be used to refer to the river. Also, such attributes are important information for many NLP applications. In this paper, we report on the design of a set of attributes for ENE categories. We used a bottom up approach to creating the knowledge using a Japanese encyclopedia, which contains abundant descriptions of ENE instances.
The Semantic Web of the future will be characterized by using a very large number of ontologies embedded in ontology networks. It is important to provide strong methodological support for collaborative and context-sensitive development of networks of ontologies. This methodological support includes the identification and definition of which activities should be carried out when ontology networks are collaboratively built. In this paper we present the consensus reaching process followed within the NeOn consortium for the identification and definition of the activities involved in the ontology network development process. The consensus reaching process here presented produces as a result the NeOn Glossary of Activities. This work was conceived due to the lack of standardization in the Ontology Engineering terminology, which clearly contrasts with the Software Engineering field. Our future aim is to standardize the NeOn Glossary of Activities.
A core ontology is a mid-level ontology which bridges the gap between an upper ontology and a domain ontology. Automatic Chinese core ontology construction can help quickly model domain knowledge. A graph based core ontology construction algorithm (COCA) is proposed to automatically construct a core ontology from an English-Chinese bilingual term bank. This algorithm computes the mapping strength from a selected Chinese term to WordNet synset with association to an upper-level SUMO concept. The strength is measured using a graph model integrated with several mapping features from multiple information sources. The features include multiple translation feature between Chinese core term and WordNet, extended string feature and Part-of-Speech feature. Evaluation of COCA repeated on an English-Chinese bilingual Term bank with more than 130K entries shows that the algorithm is improved in performance compared with our previous research and can better serve the semi-automatic construction of mid-level ontology.
The multilingual European Thesaurus on International Relations and Area Studies (European Thesaurus) is a special subject thesaurus for the field of international affairs. It is intended for use in libraries and documentation centres of academic institutions and international organizations. The European Thesaurus was established in a collaborative project involving a number of leading European research institutes on international politics. It integrates the controlled terminologies of several existing thesauri. The European Thesaurus comprises about 8,200 terms and proper names from the 24 subject areas covered by the thesaurus. Because of its multilinguality, the European Thesaurus can not only be used for indexing, retrieval and terminological reference, but serves also as a translation tool for the languages represented. The establishment of cross-concordances to related thesauri extends the range of application of the European Thesaurus even further. They enable the treatment of semantic heterogeneity within subject gateways. The European Thesaurus is available both in a seven-lingual print-version as well as in an eight-lingual online-version. To reflect the changes in terminology the European Thesau-rus is regularly being amended and modified. Further languages are going to be included.
This paper proposes a method of increasing the size of a bilingual lexicon obtained from two other bilingual lexicons via a pivot language. When we apply this approach, there are two main challenges, ambiguity and mismatch of terms; we target the latter problem by improving the utilization ratio of the bilingual lexicons. Given two bilingual lexicons between language pairs Lf-Lp and Lp-Le, we compute lexical translation probabilities of word pairs by using a statistical word-alignment model, and term decomposition/composition techniques. We compare three approaches to generate the bilingual lexicon: exact merging, word-based merging, and our proposed alignment-based merging. In our method, we combine lexical translation probabilities and a simple language model for estimating the probabilities of translation pairs. The experimental results show that our method could drastically improve the number of translation terms compared to the two methods mentioned above. Additionally, we evaluated and discussed the quality of the translation outputs.
In current phrase-based Statistical Machine Translation systems, more training data is generally better than less. However, a larger data set eventually introduces a larger model that enlarges the search space for the decoder, and consequently requires more time and more resources to translate. This paper describes an attempt to reduce the model size by filtering out the less probable entries based on testing correlation using additional training data in an intermediate third language. The central idea behind the approach is triangulation, the process of incorporating multilingual knowledge in a single system, which eventually utilizes parallel corpora available in more than two languages. We conducted experiments using Europarl corpus to evaluate our approach. The reduction of the model size can be up to 70{\%} while the translation quality is being preserved.
In this paper, we propose a new phrase-based translation model based on inter-lingual triggers. The originality of our method is double. First we identify common source phrases. Then we use inter-lingual triggers in order to retrieve their translations. Furthermore, we consider the way of extracting phrase translations as an optimization issue. For that we use simulated annealing algorithm to find out the best phrase translations among all those determined by inter-lingual triggers. The best phrases are those which improve the translation quality in terms of Bleu score. Tests are achieved on movie subtitle corpora. They show that our phrase-based machine translation (PBMT) system outperforms a state-of-the-art PBMT system by almost 7 points.
We present new direct data analysis showing that dynamically-built context-dependent phrasal translation lexicons are more useful resources for phrase-based statistical machine translation (SMT) than conventional static phrasal translation lexicons, which ignore all contextual information. After several years of surprising negative results, recent work suggests that context-dependent phrasal translation lexicons are an appropriate framework to successfully incorporate Word Sense Disambiguation (WSD) modeling into SMT. However, this approach has so far only been evaluated using automatic translation quality metrics, which are important, but aggregate many different factors. A direct analysis is still needed to understand how context-dependent phrasal translation lexicons impact translation quality, and whether the additional complexity they introduce is really necessary. In this paper, we focus on the impact of context-dependent translation lexicons on lexical choice in phrase-based SMT and show that context-dependent lexicons are more useful to a phrase-based SMT system than a conventional lexicon. A typical phrase-based SMT system makes use of more and longer phrases with context modeling, including phrases that were not seen very frequently in training. Even when the segmentation is identical, the context-dependent lexicons yield translations that match references more often than conventional lexicons.
This work presents improvements of a large-scale Arabic to French statistical machine translation system over a period of three years. The development includes better preprocessing, more training data, additional genre-specific tuning for different domains, namely newswire text and broadcast news transcripts, and improved domain-dependent language models. Starting with an early prototype in 2005 that participated in the second CESTA evaluation, the system was further upgraded to achieve favorable BLEU scores of 44.8{\%} for the text and 41.1{\%} for the audio setting. These results are compared to a system based on the freely available Moses toolkit. We show significant gains both in terms of translation quality (up to +1.2{\%} BLEU absolute) and translation speed (up to 16 times faster) for comparable configuration settings.
This paper presents ongoing work dedicated to parsing the textual structure of procedural texts. We propose here a model for the intructional structure and criteria to identify its main components: titles, instructions, warnings and prerequisites. The main aim of this project, besides a contribution to text processing, is to be able to answer procedural questions (How-to? questions), where the answer is a well-formed portion of a text, not a small set of words as for factoid questions.
Morphological query expansion and language-filtering words have proved to be valid methods when searching the web for content in Basque via APIs of commercial search engines, as the implementation of these methods in recent IR and web-as-corpus tools shows, but no real analysis has been carried out to ascertain the degree of improvement, apart from a comparison of recall and precision using a classical web search engine and measured in terms of hit counts. This paper deals with a more theoretical study that confirms the validity of the combination of both methods. We have measured the increase in recall obtained by morphological query expansion and the increase in precision and loss in recall produced by language-filtering-words, but not only by searching the web directly and looking at the hit counts which are not considered to be very reliable at best, but also using both a Basque web corpus and a classical lemmatised corpus, thus providing more exact quantitative results. Furthermore, we provide various corpora-extracted data to be used in the aforementioned methods, such as lists of the most frequent inflections and declinations (cases, persons, numbers, times, etc.) for each POS the most interesting word forms for a morphologically expanded query, or a list of the most used Basque words with their frequencies and document-frequencies the ones that should be used as language-filtering words.
This paper describes the creation of a state-of-the-art answer type detection system capable of recognizing more than 200 different expected answer types with greater than 85{\%} precision and recall. After describing how we constructed a new, multi-tiered answer type hierarchy from the set of entity types recognized by Language Computer Corporations CICEROLITE named entity recognition system, we describe how we used this hierarchy to annotate a new corpus of more than 10,000 English factoid questions. We show how an answer type detection system trained on this corpus can be used to enhance the accuracy of a state-of-the-art question-answering system (Hickl et al., 2007; Hickl et al., 2006b) by more than 7{\%} overall.
Although answering list questions is not a new research area, answering them automatically still remains a challenge. The median F-score of systems that participated in TREC 2007 Question Answering track is still very low (0.085) while 74{\%} of the questions had a median F-score of 0. In this paper, we propose a novel approach to answering list questions. This approach is based on the hypothesis that answer instances of a list question co-occur in the documents and sentences related to the topic of the question. We use a clustering method to group the candidate answers that co-occur more often. To pinpoint the right cluster, we use the target and the question keywords as spies to return the cluster that contains these keywords.
Recently, collaboratively constructed resources such as Wikipedia and Wiktionary have been discovered as valuable lexical semantic knowledge bases with a high potential in diverse Natural Language Processing (NLP) tasks. Collaborative knowledge bases however significantly differ from traditional linguistic knowledge bases in various respects, and this constitutes both an asset and an impediment for research in NLP. This paper addresses one such major impediment, namely the lack of suitable programmatic access mechanisms to the knowledge stored in these large semantic knowledge bases. We present two application programming interfaces for Wikipedia and Wiktionary which are especially designed for mining the rich lexical semantic information dispersed in the knowledge bases, and provide efficient and structured access to the available knowledge. As we believe them to be of general interest to the NLP community, we have made them freely available for research purposes.
The Spoken Language Communication and Translation System for Tactical Use (TRANSTAC) program is a Defense Advanced Research Agency (DARPA) program to create bidirectional speech-to-speech machine translation (MT) that will allow U.S. Soldiers and Marines, speaking only English, to communicate, in tactical situations, with civilian populations who speak only other languages (for example, Iraqi Arabic). A key metric for the program is the odds of successfully transferring low-level concepts, defined as the source-language content words. The National Institute of Standards and Technology (NIST) has now carried out two large-scale evaluations of TRANSTAC systems, using that metric. In this paper we discuss the merits of that metric. It has proven to be quite informative. We describe exactly how we defined this metric and how we obtained values for it from panels of bilingual judges allowing others to do what we have done. We compare results on this metric to results on Likert-type judgments of semantic adequacy, from the same panels of bilingual judges, as well as to a suite of typical automated MT metrics (BLEU, TER, METEOR).
This paper reports on the QAST track of CLEF aiming to evaluate Question Answering on Speech Transcriptions. Accessing information in spoken documents provides additional challenges to those of text-based QA, needing to address the characteristics of spoken language, as well as errors in the case of automatic transcriptions of spontaneous speech. The framework and results of the pilot QAst evaluation held as part of CLEF 2007 is described, illustrating some of the additional challenges posed by QA in spoken documents relative to written ones. The current plans for future multiple-language and multiple-task QAst evaluations are described.
The re-use of spoken word audio collections maintained by audiovisual archives is severely hindered by their generally limited access. The CHoral project, which is part of the CATCH program funded by the Dutch Research Council, aims to provide users of speech archives with online, instead of on-location, access to relevant fragments, instead of full documents. To meet this goal, a spoken document retrieval framework is being developed. In this paper the evaluation efforts undertaken so far to assess and improve various aspects of the framework are presented. These efforts include (i) evaluation of the automatically generated textual representations of the spoken word documents that enable word-based search, (ii) the development of measures to estimate the quality of the textual representations for use in information retrieval, and (iii) studies to establish the potential user groups of the to-be-developed technology, and the first versions of the user interface supporting online access to spoken word collections.
Over the past five years, the Defense Advanced Research Projects Agency (DARPA) has funded development of speech translation systems for tactical applications. A key component of the research program has been extensive system evaluation, with dual objectives of assessing progress overall and comparing among systems. This paper describes the methods used to obtain BLEU, TER, and METEOR scores for two-way English-Iraqi Arabic systems. We compare the scores with measures based on human judgments and demonstrate the effects of normalization operations on BLEU scores. Issues that are highlighted include the quality of test data and differential results of applying automated metrics to Arabic vs. English.
We present work on a three-stage system to detect and classify disfluencies in multi party dialogues. The system consists of a regular expression based module and two machine learning based modules. The results are compared to other work on multi party dialogues and we show that our system outperforms previously reported ones.
This work studies the viability of performing heterogeneous automatic MT error analyses. Error analysis is, undoubtly, one of the most crucial stages in the development cycle of an MT system. However, often not enough attention is paid to this process. The reason is that performing an accurate error analysis requires intensive human labor. In order to speed up the error analysis process, we suggest partially automatizing it by having automatic evaluation metrics play a more active role. For that purpose, we have compiled a large and heterogeneous set of features at different linguistic levels and at different levels of granularity. Through a practical case study, we show how these features provide an effective means of ellaborating interpretable and detailed automatic reports of translation quality.
We report the results of our experiment on assessing the ability of automated MT evaluation metrics to remain sensitive to variations in MT quality as the average quality of the compared systems goes up. We compare two groups of metrics: those, which measure the proximity of MT output to some reference translation, and those which evaluate the performance of some automated process on degraded MT output. The experiment shows that proximity-based metrics (such as BLEU) loose sensitivity as the scores go up, but performance-based metrics (e.g., Named Entity recognition from MT output) remain sensitive across the scale. We suggest a model for explaining this result, which attributes stable sensitivity of performance-based metrics to measuring cumulative functional effect of different language levels, while proximity-based metrics measure structural matches on a lexical level and therefore miss higher-level errors that are more typical for better MT systems. Development of new automated metrics should take into account possible decline in sensitivity on higher-quality MT, which should be tested as part of meta-evaluation of the metrics.
Evaluation of Machine Translation (MT) technology is often tied to the requirement for tedious manual judgments of translation quality. While automated MT metrology continues to be an active area of research, a well known and often accepted standard metric is the manual human assessment of adequacy and fluency. There are several software packages that have been used to facilitate these judgments, but for the 2008 NIST Open MT Evaluation, NISTs Speech Group created an online software tool to accommodate the requirement for centralized data and distributed judges. This paper introduces the NIST TAP-ET application and reviews the reasoning underlying its design. Where available, analysis of data sets judged for Adequacy and Preference using the TAP-ET application will be presented. TAP-ET is freely available and ready to download, and contains a variety of customizable features.
The rapid growth of the Internet means that more information is available than ever before. Multilingual multi-document summarisation offers a way to access this information even when it is not in a language spoken by the reader by extracting the gist from related documents and translating it automatically. This paper presents an experiment in which Maximal Marginal Relevance (MMR), a well known multi-document summarisation method, is used to produce summaries from Romanian news articles. A task-based evaluation performed on both the original summaries and on their automatically translated versions reveals that they still contain a significant portion of the important information from the original texts. However, direct evaluation of the automatically translated summaries shows that they are not very legible and this can put off some readers who want to find out more about a topic.
We have integrated the RASP system with the UIMA framework (RASP4UIMA) and used this to parse the XML-encoded version of the British National Corpus (BNC). All original annotation is preserved, and parsing information, mainly in the form of grammatical relations, is added in an XML format. A few specific adaptations of the system to give better results with the BNC are discussed briefly. The RASP4UIMA system is publicly available and can be used to parse other corpora or document collections, and the final parsed version of the BNC will be deposited with the Oxford Text Archive.
In Japanese, the syntactic structure of a sentence is generally represented by the relationship between phrasal units, bunsetsus in Japanese, based on a dependency grammar. In many cases, the syntactic structure of a bunsetsu is not considered in syntactic structure annotation. This paper gives the criteria and definitions of dependency relationships between words in a bunsetsu and their applications. The target corpus for the word-level dependency annotation is a large spontaneous Japanese-speech corpus, the Corpus of Spontaneous Japanese (CSJ). One application of word-level dependency relationships is to find basic units for constructing accent phrases.
We describe the induction of lexical resources from unannotated corpora that are aligned with treebank grammars, providing a systematic correspondence between features in the lexical resource and a treebank syntactic resource. We first describe a methodology based on parsing technology for augmenting a treebank database with linguistic features. A PCFG containing these features is created from the augmented treebank. We then use a procedure based on the inside-outside algorithm to learn lexical resources aligned with the treebank PCFG from large unannotated corpora. The method has been applied in creating a feature-annotated English treebank based on the Penn Treebank. The unsupervised estimation procedure gives a substantial error reduction (up to 31.6{\%}) on the task of learning the subcategorization preference of novel verbs that are not present in the annotated training sample.
This paper describes a database of 11 dependency treebanks which were unified by means of a two-dimensional graph format. The format was evaluated with respect to storage-complexity on the one hand, and efficiency of data access on the other hand. An example of how the treebanks can be integrated within a unique interface is given by means of the DTDB interface.
The present communication brings to the fore the work undertaken at the Royal Institute of the Amazigh Culture (IRCAM, henceforth) within the Language Planning Center known as Centre de lAm{\'e}nagement Linguistique (CAL) within the framework of the language planning of Amazigh, particularly on the side of terminology. The focus will be on the concept of variation that affects different levels in the course of standardizing a language: orthography, spelling, grammar and lexis. Thus, after a brief survey of the main features of the Amazigh (Berber) language in general, the missions and the projects far achieved by CAL will be presented, particularly the objectives that relate to the work on the multiply varied corpus-based terminology. It appears that eliciting the pertinent information, for the most part, requires a whole amount of work on the re-writing of corpora so that the latter become exploitable in the standardization process. It should be pointed out that this stage of data homogenization, seemingly unwieldy for optimal exploitation, cannot be undertaken Amazighist linguists being involved in theoretical and methodological presuppositions that are at the root of this variation.
Existing techniques extract term candidates by looking for internal and contextual information associated with domain specific terms. The algorithms always face the dilemma that fewer features are not enough to distinguish terms from non-terms whereas more features lead to more conflicts among selected features. This paper presents a novel approach for term extraction based on delimiters which are much more stable and domain independent. The proposed approach is not as sensitive to term frequency as that of previous works. This approach has no strict limit or hard rules and thus they can deal with all kinds of terms. It also requires no prior domain knowledge and no additional training to adapt to new domains. Consequently, the proposed approach can be applied to different domains easily and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques which verifies its efficiency and domain independent nature. Experiments on new term extraction indicate that the proposed approach can also serve as an effective tool for domain lexicon expansion.
Terminology extraction commonly includes two steps: identification of term-like units in the texts, mostly multi-word phrases, and the ranking of the extracted term-like units according to their domain representativity. In this paper, we design a multi-word term extraction program for Arabic language. The linguistic filtering performs a morphosyntactic analysis and takes into account several types of variations. The domain representativity is measure thanks to statistical scores. We evalutate several association measures and show that the results we otained are consitent with those obtained for Romance languages.
In this paper we present an approach to terminology recognition whereby a sublanguage term (e.g. an aircraft engine component term extracted from a maintenance log) is matched to its corresponding term from a pre-defined list (such as a taxonomy representing the official break-down of the engine). Terminology recognition is addressed as a classification task whereby the extracted term is associated to one or more potential terms in the official description list via the application of string similarity metrics. The solution described in the paper uses dynamically computed similarity cut-off thresholds calculated on the basis of modeling a noise curve. Dissimilar string matches form a Gaussian distributed noise curve that can be identified and extracted leaving only mostly similar string matches. Dynamically calculated thresholds are preferable over fixed similarity thresholds as fixed thresholds are inherently imprecise, that is, there is no similarity boundary beyond which any two strings always describe the same concept.
This paper presents resources and strategies for persuasive natural language processing. After the introduction of a specifically tagged corpus, some techniques for affective language processing and for persuasive lexicon extraction are provided together with prospective scenarios of application.
Detecting the tone or emotive content of a text message is increasingly important in many natural language processing applications. While for the English language there exists a number of affect, emotive, opinion, or affect computer-usable lexicons for automatically processing text, other languages rarely possess these primary resources. Here we present a semi-automatic technique for quickly building a multidimensional affect lexicon for a new language. Most of the work consists of defining 44 paired affect directions (e.g. love-hate, courage-fear, etc.) and choosing a small number of seed words for each dimension. From this initial investment, we show how a first pass affect lexicon can be created for new language, using a SVM classifier trained on a feature space produced from Latent Semantic Analysis over a large corpus in the new language. We evaluate the accuracy of placing newly found emotive words in one or more of the defined semantic dimensions. We illustrate this technique by creating an affect lexicon for French, but the techniques can be applied to any language found on the Web and for which a large quantity of text exists.
The modelling of realistic emotional behaviour is needed for various applications in multimodal human-machine interaction such as the design of emotional conversational agents (Martin et al., 2005) or of emotional detection systems (Devillers and Vidrascu, 2007). Yet, building such models requires appropriate definition of various levels for representing the emotions themselves but also some contextual information such as the events that elicit these emotions. This paper presents a coding scheme that has been defined following annotations of a corpus of TV interviews (EmoTV). Deciding which events triggered or may trigger which emotion is a challenge for building efficient emotion eliciting protocols. In this paper, we present the protocol that we defined for collecting another corpus of spontaneous human-human interactions recorded in laboratory conditions (EmoTaboo). We discuss the events that we designed for eliciting emotions. Part of this scheme for coding emotional event is being included in the specifications that are currently defined by a working group of the W3C (the W3C Emotion Incubator Working group). This group is investigating the feasibility of working towards a standard representation of emotions and related states in technological contexts.
In this paper we describe the result of manually annotating I-CAB, the Italian Content Annotation Bank, by expressions of private state (EPSs), i.e., expressions that denote the presence of opinions, emotions, and other cognitive states. The aim of this effort was the generation of a standard resource for supporting the development of opinion extraction algorithms for Italian, and of a benchmark for testing such algorithms. To this end we have employed a previously existing annotation language (here dubbed WWC, from the initials of its proponents). We here describe the results of this annotation effort, including the results of a thorough inter-annotator agreement test. We conclude by discussing how WWC can be adapted to the specificities of a Romance language such as Italian.
The goal of this paper is to describe how adjectives are encoded in Cornetto, a semantic lexical database for Dutch. Cornetto combines two existing lexical resources with different semantic organisation, i.e. Dutch Wordnet (DWN) with a synset organisation and Referentie Bestand Nederlands (RBN) with an organisation in Lexical Units. Both resources will be aligned and mapped on the formal ontology SUMO. In this paper, we will first present details of the description of adjectives in each of the the two resources. We will then address the problems that are encountered during alignment to the SUMO ontology which are greatly due to the fact that SUMO has never been tested for its adequacy with respect to adjectives. We contrasted SUMO with an existing semantic classification which resulted in a further refined and extended SUMO geared for the description of adjectives.
We develop a method for detecting errors in semantic predicate-argument annotation, based on the variation n-gram error detection method. After establishing an appropriate data representation, we detect inconsistencies by searching for identical text with varying annotation. By remaining data-driven, we are able to detect inconsistencies arising from errors at lower layers of annotation.
Distributional, corpus-based descriptions have frequently been applied to model aspects of word meaning. However, distributional models that use corpus data as their basis have one well-known disadvantage: even though the distributional features based on corpus co-occurrence were often successful in capturing meaning aspects of the words to be described, they generally fail to capture those meaning aspects that refer to world knowledge, because coherent texts tend not to provide redundant information that is presumably available knowledge. The question we ask in this paper is whether dictionary and encyclopaedic resources might complement the distributional information in corpus data, and provide world knowledge that is missing in corpora. As test case for meaning aspects, we rely on a collection of semantic associates to German verbs and nouns. Our results indicate that a combination of the knowledge resources should be helpful in work on distributional descriptions.
Semantic annotation of text requires the dynamic merging of linguistically structured information and a world model, usually represented as a domain-specific ontology. On the other hand, the process of engineering a domain-ontology through semi-automatic ontology learning system requires the availability of a considerable amount of semantically annotated documents. Facing this bootstrapping paradox requires an incremental process of annotation-acquisition-annotation, whereby domain-specific knowledge is acquired from linguistically-annotated texts and then projected back onto texts for extra linguistic information to be annotated and further knowledge layers to be extracted. The presented methodology is a first step in the direction of a full virtuous circle where the semantic annotation platform and the evolving ontology interact in symbiosis. As a case study we have chosen the semantic annotation of product catalogues. We propose a hybrid approach, combining pattern matching techniques to exploit the regular structure of product descriptions in catalogues, and Natural Language Processing techniques which are resorted to analyze natural language descriptions. The semantic annotation involves the access to the ontology, semi-automatically bootstrapped with an ontology learning tool from annotated collections of catalogues.
This paper describes SW1, the first version of a semantically annotated snapshot of the English Wikipedia. In recent years Wikipedia has become a valuable resource for both the Natural Language Processing (NLP) community and the Information Retrieval (IR) community. Although NLP technology for processing Wikipedia already exists, not all researchers and developers have the computational resources to process such a volume of information. Moreover, the use of different versions of Wikipedia processed differently might make it difficult to compare results. The aim of this work is to provide easy access to syntactic and semantic annotations for researchers of both NLP and IR communities by building a reference corpus to homogenize experiments and make results comparable. These resources, a semantically annotated corpus and a entity containment derived graph, are licensed under the GNU Free Documentation License and available from http://www.yr-bcn.es/semanticWikipedia
This paper summarizes the annotation of fine-grained entailment relationships in the context of student answers to science assessment questions. We annotated a corpus of 15,357 answer pairs with 145,911 fine-grained entailment relationships. We provide the rationale for such fine-grained analysis and discuss its perceived benefits to an Intelligent Tutoring System. The corpus also has potential applications in other areas, such as question answering and multi-document summarization. Annotators achieved 86.2{\%} inter-annotator agreement (Kappa=0.728, corresponding to substantial agreement) annotating the fine-grained facets of reference answers with regard to understanding expressed in student answers and labeling from one of five possible detailed relationship categories. The corpus described in this paper, which is the only one providing such detailed entailment annotations, is available as a public resource for the research community. The corpus is expected to enable application development, not only for intelligent tutoring systems, but also for general textual entailment applications, that is currently not practical.
We discuss factors that affect human agreement on a semantic labeling task in the art history domain, based on the results of four experiments where we varied the number of labels annotators could assign, the number of annotators, the type and amount of training they received, and the size of the text span being labeled. Using the labelings from one experiment involving seven annotators, we investigate the relation between interannotator agreement and machine learning performance. We construct binary classifiers and vary the training and test data by swapping the labelings from the seven annotators. First, we find performance is often quite good despite lower than recommended interannotator agreement. Second, we find that on average, learning performance for a given functional semantic category correlates with the overall agreement among the seven annotators for that category. Third, we find that learning performance on the data from a given annotator does not correlate with the quality of that annotators labeling. We offer recommendations for the use of labeled data in machine learning, and argue that learners should attempt to accommodate human variation. We also note implications for large scale corpus annotation projects that deal with similarly subjective phenomena.
Semantic similarity is a key issue in many computational tasks. This paper goes into the development and evaluation of two common ways of automatically calculating the semantic similarity between two words. On the one hand, such methods may depend on a manually constructed thesaurus like (Euro)WordNet. Their performance is often evaluated on the basis of a very restricted set of human similarity ratings. On the other hand, corpus-based methods rely on the distribution of two words in a corpus to determine their similarity. Their performance is generally quantified through a comparison with the judgements of the first type of approach. This paper introduces a new Gold Standard of more than 5,000 human intra-category similarity judgements. We show that corpus-based methods often outperform (Euro)WordNet on this data set, and that the use of the latter as a Gold Standard for the former, is thus often far from ideal.
This paper presents an approach to annotation that BAE Systems has employed in the DARPA GALE Phase 2 Distillation evaluation. The purpose of the GALE Distillation evaluation is to quantify the amount of relevant and non-redundant information a distillation engine is able to produce in response to a specific, formatted query; and to compare that amount of information to the amount of information gathered by a bilingual human using commonly available state-of-the-art tools. As part of the evaluation, following NIST evaluation methodology of complex question answering (Voorhees, 2003), human annotators were asked to establish the relevancy of responses as well as the presence of atomic facts or information units, called nuggets of information. This paper discusses various challenges to the annotation of nuggets, called nuggetization, which include interaction between the granularity of nuggets and relevancy of these nuggets to the query in question. The approach proposed in the paper views nuggetization as a procedural task and allows annotators to revisit nuggetization based on the requirements imposed by the relevancy guidelines defined with a specific end-user in mind. This approach is shown in the paper to produce consistent annotations with high inter-annotator agreement scores.
We describe a methodology for evaluating the statistical performance of information distillation systems and apply it to a simple illustrative example. (An information distiller provides written English responses to English queries based on automated searches/transcriptions/translations of English and foreign-language sources. The sources include written documents and sound tracks.) The evaluation methodology extracts information nuggets from the distiller response texts and gathers them into fuzzy equivalence classes called nugs. Themethodology supports the usual performancemetrics, such as recall and precision, as well as a new information-theoretic metric called proficiency, which measures how much information a distiller provides relative to all of the information provided by a collection of distillers working on a common query and corpora. Unlike previous evaluation techniques, the methodology evaluates the relevance, granularity, and redundancy of information nuggets explicitly.
The ultimate goal when building dialogue systems is to satisfy the needs of real users, but quality assurance for dialogue strategies is a non-trivial problem. The applied evaluation metrics and resulting design principles are often obscure, emerge by trial-and-error, and are highly context dependent. This paper introduces data-driven methods for obtaining reliable objective functions for system design. In particular, we test whether an objective function obtained from Wizard-of-Oz (WOZ) data is a valid estimate of real users preferences. We test this in a test-retest comparison between the model obtained from the WOZ study and the models obtained when testing with real users. We can show that, despite a low fit to the initial data, the objective function obtained from WOZ data makes accurate predictions for automatic dialogue evaluation, and, when automatically optimising a policy using these predictions, the improvement over a strategy simply mimicking the data becomes clear from an error analysis.
This paper describes the building of a valency lexicon of Arabic verbs using a morphologically and syntactically annotated corpus, the Prague Arabic Dependency Treebank (PADT), as its primary source. We present the theoretical account on valency developed within the Functional Generative Description (FGD) theory. We apply the framework to Modern Standard Arabic and discuss various valency-related phenomena with respect to examples from the corpus. We then outline the methodology and the linguistic and technical resources used in the building of the lexicon. The key concept in our scenario is that of PDT-VALLEX of Czech. Our lexicon will be developed by linking the conceivable entries with their instances in the treebank. Conversely, the treebanks annotations will be linked to the lexicon. While a comparable scheme has been developed for Czech, our own contribution is to design and implement this model thoroughly for Arabic and the PADT data. The Arabic valency lexicon is intended for applications in computational parsing or language generation, and for use by human researchers. The proposed valency lexicon will be exploited in particular during further tectogrammatical annotations of PADT and might serve for enriching the expected second edition of the corpus-based Arabic-Czech Dictionary.
Terminologies and other knowledge resources are widely used to aid entity recognition in specialist domain texts. As well as providing lexicons of specialist terms, linkage from the text back to a resource can make additional knowledge available to applications. Use of such resources is especially pertinent in the biomedical domain, where large numbers of these resources are available, and where they are widely used in informatics applications. Terminology resources can be most readily used by simple lexical lookup of terms in the text. A major drawback with such lexical lookup, however, is poor precision caused by ambiguity between domain terms and general language words. We combine lexical lookup with simple filtering of ambiguous terms, to improve precision. We compare this lexical lookup with a statistical method of entity recognition, and to a method which combines the two approaches. We show that the combined method boosts precision with little loss of recall, and that linkage from recognised entities back to the domain knowledge resources can be maintained.
This paper presents a series of tools for the extraction of specialized corpora from the web and its subsequent analysis mainly with statistical techniques. It is an integrated system of original as well as standard tools and has a modular conception that facilitates its re-integration on different systems. The first part of the paper describes the original techniques, which are devoted to the categorization of documents as relevant or irrelevant to the corpus under construction, considering relevant a specialized document of the selected technical domain. Evaluation figures are provided for the original part, but not for the second part involving the analysis of the corpus, which is composed of algorithms that are well known in the field of Natural Language Processing, such as Kwic search, measures of vocabulary richness, the sorting of n-grams by frequency of occurrence or by measures of statistical association, distribution or similarity.
This paper presents a supervised method for the detection and extraction of Causal Relations from open domain text. First we give a brief outline of the definition of causation and how it relates to other Semantic Relations, as well as a characterization of their encoding. In this work, we only consider marked and explicit causations. Our approach first identifies the syntactic patterns that may encode a causation, then we use Machine Learning techniques to decide whether or not a pattern instance encodes a causation. We focus on the most productive pattern, a verb phrase followed by a relator and a clause, and its reverse version, a relator followed by a clause and a verb phrase. As relators we consider the words as, after, because and since. We present a set of lexical, syntactic and semantic features for the classification task, their rationale and some examples. The results obtained are discussed and the errors analyzed.
Morfette is a modular, data-driven, probabilistic system which learns to perform joint morphological tagging and lemmatization from morphologically annotated corpora. The system is composed of two learning modules which are trained to predict morphological tags and lemmas using the Maximum Entropy classifier. The third module dynamically combines the predictions of the Maximum-Entropy models and outputs a probability distribution over tag-lemma pair sequences. The lemmatization module exploits the idea of recasting lemmatization as a classification task by using class labels which encode mappings from word forms to lemmas. Experimental evaluation results and error analysis on three morphologically rich languages show that the system achieves high accuracy with no language-specific feature engineering or additional resources.
Ontology construction usually requires a domain-specific corpus for building corresponding concept hierarchy. The domain corpus must have a good coverage of domain knowledge. Wikipedia(Wiki), the worlds largest online encyclopaedic knowledge source, is open-content, collaboratively edited, and free of charge. It covers millions of articles and still keeps on expanding continuously. These characteristics make Wiki a good candidate as domain corpus resource in ontology construction. However, the selected article collection must have considerable quality and quantity. In this paper, a novel approach is proposed to identify articles in Wiki as domain-specific corpus by using available classification information in Wiki pages. The main idea is to generate a domain hierarchy from the hyperlinked pages of Wiki. Only articles strongly linked to this hierarchy are selected as the domain corpus. The proposed approach makes use of linked category information in Wiki pages to produce the hierarchy as a directed graph for obtaining a set of pages in the same connected branch. Ranking and filtering are then done on these pages based on the classification tree generated by the traversal algorithm. The experiment and evaluation results show that Wiki is a good resource for acquiring a relative high quality domain-specific corpus for ontology construction.
With the appearance of Semantic Web technologies, it becomes possible to develop novel, sophisticated question answering systems, where ontologies are usually used as the core knowledge component. In the EU-funded project, QALL-ME, a domain-specific ontology was developed and applied for question answering in the domain of tourism, along with the assistance of two upper ontologies for concept expansion and reasoning. This paper focuses on the development of the QALL-ME ontology in the tourism domain and its alignment with the upper ontologies - WordNet and SUMO. The design of the ontology is presented in the paper, and a semi-automatic alignment procedure is described with some alignment results given as well. Furthermore, the aligned ontology was used to semantically annotate original data obtained from the tourism web sites and natural language questions. The storage schema of the annotated data and the data access method for retrieving answers from the annotated data are also reported in the paper.
Acquiring knowledge from the Web to build domain ontologies has become a common practice in the Ontological Engineering field. The vast amount of freely available information allows collecting enough information about any domain. However, the Web usually suffers a lack of structure, untrustworthiness and ambiguity of the content. These drawbacks hamper the application of unsupervised methods of building ontologies demanded by the increasingly popular applications of the Semantic Web. We believe that the combination of several processing mechanisms and complementary information sources may potentially solve the problem. The analysis of different sources of evidence allows determining with greater reliability the validity of the detected knowledge. In this paper, we present GALeOn (General Architecture for Learning Ontologies) that combines sources and processing resources to provide complementary and redundant evidence for making better estimations about the relevance of the extracted knowledge and their relationships. Our goal in this paper is to show how combining several information sources and extraction mechanisms is possible to build a taxonomy of concepts with a higher accuracy than if only one of them is applied. The experimental results show how this combination notably increases the precision of the obtained results with minimum user intervention.
Automated extraction of ontological knowledge from text corpora is a relevant task in Natural Language Processing. In this paper, we focus on the problem of finding hypernyms for relevant concepts in a specific domain (e.g. Optical Recording) in the context of a concrete and challenging application scenario (patent processing). To this end information available on the Web is exploited. The extraction method includes four mains steps. Firstly, the Google search engine is exploited to retrieve possible instances of isa-patterns reported in the literature. Then, the returned snippets are filtered on the basis of lexico-syntactic criteria (e.g. the candidate hypernym must be expressed as a noun phrase without complex modifiers). In a further filtering step, only candidate hypernyms compatible with the target domain are kept. Finally a candidate ranking mechanism is applied to select one hypernym as output of the algorithm. The extraction method was evaluated on 100 concepts of the Optical Recording domain. Moreover, the reliability of isa-patterns reported in the literature as predictors of isa-relations was assessed by manually evaluating the template instances remaining after lexico-syntactic filtering, for 3 concepts of the same domain. While more extensive testing is needed the method appears promising especially for its portability across different domains.
When dealing with large, distributed systems that use state-of-the-art components, individual components are usually developed in parallel. As development continues, the decoupling invariably leads to a mismatch between how these components internally represent concepts and how they communicate these representations to other components: representations can get out of synch, contain localized errors, or become manageable only by a small group of experts for each module. In this paper, we describe the use of an ontology as part of a complex distributed virtual human architecture in order to enable better communication between modules while improving the overall flexibility needed to change or extend the system. We focus on the natural language understanding capabilities of this architecture and the relationship between language and concepts within the entire system in general and the ontology in particular.
This paper presents the results of a graph-based method for performing knowledge-based Word Sense Disambiguation (WSD). The technique exploits the structural properties of the graph underlying the chosen knowledge base. The method is general, in the sense that it is not tied to any particular knowledge base, but in this work we have applied it to the Multilingual Central Repository (MCR). The evaluation has been performed on the Senseval-3 all-words task. The main contributions of the paper are twofold: (1) We have evaluated the separate and combined performance of each type of relation in the MCR, and thus indirectly validated the contents of the MCR and their potential for WSD. (2) We obtain state-of-the-art results, and in fact yield the best results that can be obtained using publicly available data.
In this paper we present a Japanese-English Bilingual lexicon of technical terms. The lexicon was derived from the first and second NTCIR evaluation collections for research into cross-language information retrieval for Asian languages. While it can be utilized for translation between Japanese and English, the lexicon is also suitable for language research and language engineering. Since it is collection-derived, it contains instances of word variants and miss-spellings which make it eminently suitable for further research. For a subset of the lexicon we make available the collection statistics. In addition we make available a Katakana subset suitable for transliteration research.
This paper describes a novel methodology to perform bilingual terminology extraction, in which automatic alignment is used to improve the performance of terminology extraction for each language. The strengths of monolingual terminology extraction for each language are exploited to improve the performance of terminology extraction in the other language, thanks to the availability of a sentence-level aligned bilingual corpus, and an automatic noun phrase alignment mechanism. The experiment indicates that weaknesses in monolingual terminology extraction due to the limitation of resources in certain languages can be overcome by using another language which has no such limitation.
This paper reports an experience on producing manual word alignments over six different language pairs (all combinations between Portuguese, English, French and Spanish) (Gra{\c{c}}a et al., 2008). Word alignment of each language pair is made over the first 100 sentences of the common test set from the Europarl corpora (Koehn, 2005), corresponding to 600 new annotated sentences. This collection is publicly available at http://www.l2f.inesc- id.pt/resources/translation/. It contains, to our knowledge, the first word alignment gold set for the Portuguese language, with three other languages. Besides, it is to our knowledge, the first multi-language manual word aligned parallel corpus, where the same sentences are annotated for each language pair. We started by using the guidelines presented at (Mari{\~n}o, 2005) and performed several refinements: some due to under-specifications on the original guidelines, others because of disagreement on some choices. This lead to the development of an extensive new set of guidelines for multi-lingual word alignment annotation that, we believe, makes the alignment process less ambiguous. We evaluate the inter-annotator agreement obtaining an average of 91.6{\%} agreement between the different language pairs.
This paper presents the QALL-ME benchmark, a multilingual resource of annotated spoken requests in the tourism domain, freely available for research purposes. The languages currently involved in the project are Italian, English, Spanish and German. It introduces a semantic annotation scheme for spoken information access requests, specifically derived from Question Answering (QA) research. In addition to pragmatic and semantic annotations, we propose three QA-based annotation levels: the Expected Answer Type, the Expected Answer Quantifier and the Question Topical Target of a request, to fully capture the content of a request and extract the sought-after information. The QALL-ME benchmark is developed under the EU-FP6 QALL-ME project which aims at the realization of a shared and distributed infrastructure for Question Answering (QA) systems on mobile devices (e.g. mobile phones). Questions are formulated by the users in free natural language input, and the system returns the actual sequence of words which constitutes the answer from a collection of information sources (e.g. documents, databases). Within this framework, the benchmark has the twofold purpose of training machine learning based applications for QA, and testing their actual performance with a rapid turnaround in controlled laboratory setting.
This paper describes tools and techniques for accessing large quantities of speech data and for the visualisation of discourse interactions and events at levels above that of linguistic content. We are working with large quantities of dialogue speech including business meetings, friendly discourse, and telephone conversations, and have produced web-based tools for the visualisation of non-verbal and paralinguistic features of the speech data. In essence, they provide higher-level displays so that specific sections of speech, text, or other annotation can be accessed by the researcher and provide an interactive interface to the large amount of data through an Archive Browser.
While the Web is facing interesting new changes in the way users access, interact and even participate to its growth, the most traditional applications dedicated to its fruition: web browsers, are not responding with the same euphoric boost for innovation, mostly relying on third party or open-source community-driven extensions for addressing the new Social and Semantic Web trends and technologies. This technological and decisional gap, which is probably due to the lack of a strong standardization commitment on the one side (Web 2.0/Social Web) and in the delay of massive adherence to new officially approved standards (W3C approved Semantic Web languages), has to be filled by successful stories which could lay the path for the evolution of browsers. In this work we present a novel web browser extension which combines several features coming from the worlds of terminology and information extraction, semantic annotation and knowledge management, to support users in the process of both keeping track of interesting information they find on the web, and organizing its associated content following knowledge representation standards offered by the Semantic Web
Information extraction from large data repositories is critical to Information Management solutions. In addition to prerequisite corpus analysis, to determine domain-specific characteristics of text resources, developing, refining and evaluating analytics entails a complex and lengthy process, typically requiring more than just domain expertise. Modern architectures for text processing, while facilitating reuse and (re-)composition of analytical pipelines, do place additional constraints upon the analytics development, as domain experts need not only configure individual annotator components, but situate these within a fully functional annotator pipeline. We present the design, and current status, of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details, pipeline composition constraints, and data management. Instead, the tool embodies support for all stages of ontology-centric model development cycle from corpus analysis and concept definition, to model development and testing, to large scale evaluation, to easy and rapid composition of text applications deploying these concept models. With our design, we aim to meet the needs of domain experts, who are not necessarily expert NLP practitioners.
We present an approach for querying collections of heterogeneous linguistic corpora that are annotated on multiple layers using arbitrary XML-based markup languages. An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion. In addition, we present a highly flexible web-based graphical interface that can be used to query corpora with regard to several different linguistic properties such as, for example, syntactic tree fragments. This interface can also be used for ontology-based querying of multiple corpora simultaneously.
In this paper we describe an approach that both creates crosslingual acoustic monophone model sets for speech recognition tasks and objectively predicts their performance without target-language speech data or acoustic measurement techniques. This strategy is based on a series of linguistic metrics characterizing the articulatory phonetic and phonological distances of target-language phonemes from source-language phonemes. We term these algorithms the Combined Phonetic and Phonological Crosslingual Distance (CPP-CD) metric and the Combined Phonetic and Phonological Crosslingual Prediction (CPP-CP) metric. The particular motivations for this project are the current unavailability and often prohibitively high production cost of speech databases for many strategically important low- and middle-density languages. First, we describe the CPP-CD approach and compare the performance of CPP-CD-specified models to both native language models and crosslingual models selected by the Bhattacharyya acoustic-model distance metric in automatic speech recognition (ASR) experiments. Results confirm that the CPP-CD approach nearly matches those achieved by the acoustic distance metric. We then test the CPP-CP algorithm on the CPP-CD models by comparing the CPP-CP scores to the recognition phoneme error rates. Based on this comparison, we conclude that the CPP-CP algorithm is a reliable indicator of crosslingual model performance in speech recognition tasks.
In this paper, we present the collection and analysis of a spoken dialogue corpus obtained from interactions of older and younger users with a smart-home system. Our aim is to identify the amount and the origin of linguistic differences in the way older and younger users address the system. In addition, we investigate changes in the users linguistic behaviour after exposure to the system. The results show that the two user groups differ in their speaking style as well as their vocabulary. In contrast to younger users, who adapt their speaking style to the expected limitations of the system, older users tend to use a speaking style that is closer to human-human communication in terms of sentence complexity and politeness. However, older users are far less easy to stereotype than younger users.
In this paper we present a corpus of interactions of older and younger users with nine different dialogue systems. The corpus has been fully transcribed and annotated with dialogue acts and Information State Update (ISU) representations of dialogue context. Users not only underwent a comprehensive battery of cognitive assessments, but they also rated the usability of each dialogue system on a standardised questionnaire. In this paper, we discuss the corpus collection and outline the semi-automatic methods we used for discourse-level annotations. We expect that the corpus will provide a key resource for modelling older peoples interaction with spoken dialogue systems.
Within the framework of the Dutch-Flemish programme STEVIN, the JASMIN-CGN (Jongeren, Anderstaligen en Senioren in Mens-machine Interactie Corpus Gesproken Nederlands) project was carried out, which was aimed at collecting speech of children, non-natives and elderly people. The JASMIN-CGN project is an extension of the Spoken Dutch Corpus (CGN) along three dimensions. First, by collecting a corpus of contemporary Dutch as spoken by children of different age groups, elderly people and non-natives with different mother tongues, an extension along the age and mother tongue dimensions was achieved. In addition, we collected speech material in a communication setting that was not envisaged in the CGN: human-machine interaction. One third of the data was collected in Flanders and two thirds in the Netherlands. In this paper we report on our experiences in collecting this corpus and we describe some of the important decisions that we made in the attempt to combine efficiency and high quality.
The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German. It was recorded via the WWW in over 40 public schools in all dialect regions of Germany. In this paper, we present a cross-sectional study of f0 measurements on this database. The study documents the profound changes in male voices at the age 13-15. Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers. A closer analysis reveals that f0 variability is dependent on the speech style and both the length and the type of the utterance. The study provides statistically reliable voice parameters of adolescent speakers for German. The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability.
This paper describes part of the corpus collection efforts underway in the EC funded Companions project. The Companions project is collecting substantial quantities of dialogue a large part of which focus on reminiscing about photographs. The texts are in English and Czech. We describe the context and objectives for which this dialogue corpus is being collected, the methodology being used and make observations on the resulting data. The corpora will be made available to the wider research community through the Companions Project web site.
This paper describes a collection of correlated communicative samples collected from the same individuals across six diverse genres. Three of the genres were computer mediated: email, blog, and chat, and three non-computer-mediated: essay, interview, and discussion. Participants were drawn from a college student population with an equal number of males and females recruited. All communication expressed opinion on six pre-selected, current topics that had been determined to stimulate communication. The experimental design including methods of collection, randomization of scheduling of genre order and topic order is described. Preliminary results for two descriptive metrics, word count and Flesch readability, are presented. Interesting and, in some cases, significant effects were observed across genres by topic and by gender of participant. This corpus will provide a resource to investigate communication stylistics of individuals across genres, the identification of individuals from correlated data, as well as commonalities and differences across samples that agree in genre, topic, and/or gender of participant.
This paper discusses how Information Extraction is used to understand and manage Dialogue in the EU-funded Companions project. This will be discussed with respect to the Senior Companion, one of two applications under development in the EU-funded Companions project. Over the last few years, research in human-computer dialogue systems has increased and much attention has focused on applying learning methods to improving a key part of any dialogue system, namely the dialogue manager. Since the dialogue manager in all dialogue systems relies heavily on the quality of the semantic interpretation of the users utterance, our research in the Companions project, focuses on how to improve the semantic interpretation and combine it with knowledge from the Knowledge Base to increase the performance of the Dialogue Manager. Traditionally the semantic interpretation of a user utterance is handled by a natural language understanding module which embodies a variety of natural language processing techniques, from sentence splitting, to full parsing. In this paper we discuss the use of a variety of NLU processes and in particular Information Extraction as a key part of the NLU module in order to improve performance of the dialogue manager and hence the overall dialogue system.
We describe a new multimodal corpus currently under development. The corpus consists of videos of task-oriented dialogues that are annotated for speakers verbal requests and domain action executions. This resource provides data for new research on language production and comprehension. The corpus can be used to study speakers decisions as to how to structure their utterances given the complexity of the message they are trying to convey.
In this paper we present a novel approach to the incremental incorporation of semantic information in natural language processing which does not fall victim to the notorious problems of ambiguity and lack of robustness, namely through the formal interpretation of semantic annotation. We present a formal semantics for a language for the integrated annotation of several types of semantic information, such as (co-)reference relations, temporal information, and semantic roles. This semantics has the form of a compositional translation into second-order logic. We show that a truly semantic approach to the annotation of different types of semantic information raises interesting issues relating to the borders between these areas of semantics, and to the consistency of semantic annotations in multiple areas or in multiple annotation layers. The approach is compositional, in the sense that every well-formed subexpression of the annotation language can be translated to formal logic (and hence interpreted) independent of the rest of the annotation structure. The approach is also incremental in the sense that it is designed to be extendable to the semantic annotation of many other types of semantic information, such as spatial information, noun-noun relations, or quantification and modification structures.
In this paper, we present an original framework to model frame semantic resources (namely, FrameNet) using minimal supervision. This framework can be leveraged both to expand an existing FrameNet with new knowledge, and to induce a FrameNet in a new language. Our hypothesis is that a frame semantic resource can be modeled and represented by a suitable semantic space model. The intuition is that semantic spaces are an effective model of the notion of being characteristic of a frame for both lexical elements and full sentences. The paper gives two main contributions. First, it shows that our hypothesis is valid and can be successfully implemented. Second, it explores different types of semantic VSMs, outlining which one is more suitable for representing a frame semantic resource. In the paper, VSMs are used for modeling the linguistic core of a frame, the lexical units. Indeed, if the hypothesis is verified for these units, the proposed framework has a much wider application.
This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature (using text mining techniques on PubMed abstracts and fulltext papers) to help biomedical experts to interpret experimental results in hand. The infrastructural level bears on semantic-web technologies and standards that facilitate the actual fusion of the multi-source knowledge.
Vector-based models of lexical semantics retrieve semantically related words automatically from large corpora by exploiting the property that words with a similar meaning tend to occur in similar contexts. Despite their increasing popularity, it is unclear which kind of semantic similarity they actually capture and for which kind of words. In this paper, we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic properties of the nouns influence the results. In particular, we compare results from a dependency-based model with those from a 1st and 2nd order bag-of-words model and we examine the effect of the nouns frequency, semantic speficity and semantic class. We find that all three models find more synonyms for high-frequency nouns and those belonging to abstract semantic classses. Semantic specificty does not have a clear influence.
Within the scope of the SPACE project, the CHildrens Oral REading Corpus (CHOREC) is developed. This database contains recorded, transcribed and annotated read speech (42 GB or 130 hours) of 400 Dutch speaking elementary school children with or without reading difficulties. Analyses of inter- and intra-annotator agreement are carried out in order to investigate the consistency with which reading errors are detected, orthographic and phonetic transcriptions are made, and reading errors and reading strategies are labeled. Percentage agreement scores and kappa values both show that agreement between annotations, and therefore the quality of the annotations, is high. Taken all double or triple annotations (for 10{\%} resp. 30{\%} of the corpus) together, {\%} agreement varies between 86.4{\%} and 98.6{\%}, whereas kappa varies between 0.72 and 0.97 depending on the annotation tier that is being assessed. School type and reading type seem to account for systematic differences in {\%} agreement, but these differences disappear when kappa values are calculated that correct for chance agreement. To conclude, an analysis of the annotation differences with respect to the *s label (i.e. a label that is used to annotate undistinguishable spelling behaviour), phoneme labels, reading strategy and error labels is given.
This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English. Linkage is accomplished through the Inter-Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource, on the one hand, enables contrastive analysis of the linguistic phenomena surrounding events in both languages, and on the other hand, can be used to perform multilingual temporal analysis of texts. In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense.
The goal of the DARPA MADCAT (Multilingual Automatic Document Classification Analysis and Translation) Program is to automatically convert foreign language text images into English transcripts, for use by humans and downstream applications. The first phase the program focuses on translation of handwritten Arabic documents. Linguistic Data Consortium (LDC) is creating publicly available linguistic resources for MADCAT technologies, on a scale and richness not previously available. Corpora will consist of existing LDC corpora and data donations from MADCAT partners, plus new data collection to provide high quality material for evaluation and to address strategic gaps (for genre, dialect, image quality, etc.) in the existing resources. Training and test data properties will expand over time to encompass a wide range of topics and genres: letters, diaries, training manuals, brochures, signs, ledgers, memos, instructions, postcards and forms among others. Data will be ground truthed, with line, word and token segmentation and zoning, and translations and word alignments will be produced for a subset. Evaluation data will be carefully selected from the available data pools and high quality references will be produced, which can be used to compare MADCAT system performance against the human-produced gold standard.
Active learning (AL) is getting more and more popular as a methodology to considerably reduce the annotation effort when building training material for statistical learning methods for various NLP tasks. A crucial issue rarely addressed, however, is when to actually stop the annotation process to profit from the savings in efforts. This question is tightly related to estimating the classifier performance after a certain amount of data has already been annotated. While learning curves are the default means to monitor the progress of the annotation process in terms of classifier performance, this requires a labeled gold standard which - in realistic annotation settings, at least - is often unavailable. We here propose a method for committee-based AL to approximate the progression of the learning curve based on the disagreement among the committee members. This method relies on a separate, unlabeled corpus and is thus well suited for situations where a labeled gold standard is not available or would be too expensive to obtain. Considering named entity recognition as a test case we provide empirical evidence that this approach works well under simulation as well as under real-world annotation conditions.
Lexicon schemas and their use are discussed in this paper from the perspective of lexicographers and field linguists. A variety of lexicon schemas have been developed, with goals ranging from computational lexicography (DATR) through archiving (LIFT, TEI) to standardization (LMF, FSR). A number of requirements for lexicon schemas are given. The lexicon schemas are introduced and compared to each other in terms of conversion and usability for this particular user group, using a common lexicon entry and providing examples for each schema under consideration. The formats are assessed and the final recommendation is given for the potential users, namely to request standard compliance from the developers of the tools used. This paper should foster a discussion between authors of standards, lexicographers and field linguists.
This paper presents LexSchem - the first large, fully automatically acquired subcategorization lexicon for French verbs. The lexicon includes subcategorization frame and frequency information for 3297 French verbs. When evaluated on a set of 20 test verbs against a gold standard dictionary, it shows 0.79 precision, 0.55 recall and 0.65 F-measure. We have made this resource freely available to the research community on the web.
This presentation focuses on the semi-automatic extension of Arabic WordNet (AWN) using lexical and morphological rules and applying Bayesian inference. We briefly report on the current status of AWN and propose a way of extending its coverage by taking advantage of a limited set of highly productive Arabic morphological rules for deriving a range of semantically related word forms from verb entries. The application of this set of rules, combined with the use of bilingual Arabic-English resources and Princetons WordNet, allows the generation of a graph representing the semantic neighbourhood of the original word. In previous work, a set of associations between the hypothesized Arabic words and English synsets was proposed on the basis of this graph. Here, a novel approach to extending AWN is presented whereby a Bayesian Network is automatically built from the graph and then the net is used as an inferencing mechanism for scoring the set of candidate associations. Both on its own and in combination with the previous technique, this new approach has led to improved results.
This paper describes the evaluation process of an emotional speech database recorded for standard Basque, in order to determine its adequacy for the analysis of emotional models and its use in speech synthesis. The corpus consists of seven hundred semantically neutral sentences that were recorded for the Big Six emotions and neutral style, by two professional actors. The test results show that every emotion is readily recognized far above chance level for both speakers. Therefore the database is a valid linguistic resource for the research and development purposes it was designed for.
Recent years have seen an increasing interest in developing standards for linguistic annotation, with a focus on the interoperability of the resources. This effort, however, requires a profound knowledge of the advantages and disadvantages of linguistic annotation schemes in order to avoid importing the flaws and weaknesses of existing encoding schemes into the new standards. This paper addresses the question how to compare syntactically annotated corpora and gain insights into the usefulness of specific design decisions. We present an exhaustive evaluation of two German treebanks with crucially different encoding schemes. We evaluate three different parsers trained on the two treebanks and compare results using EvalB, the Leaf-Ancestor metric, and a dependency-based evaluation. Furthermore, we present TePaCoC, a new testsuite for the evaluation of parsers on complex German grammatical constructions. The testsuite provides a well thought-out error classification, which enables us to compare parser output for parsers trained on treebanks with different encoding schemes and provides interesting insights into the impact of treebank annotation schemes on specific constructions like PP attachment or non-constituent coordination.
The InFile project (INformation, FILtering, Evaluation) is a cross-language adaptive filtering evaluation campaign, sponsored by the French National Research Agency. The campaign is organized by the CEA LIST, ELDA and the University of Lille3-GERiiCO. It has an international scope as it is a pilot track of the CLEF 2008 campaigns. The corpus is built from a collection of about 1.4 million newswires (10 GB) in three languages, Arabic, English and French provided by the French news Agency Agence France Press (AFP) and selected from a 3-year period. The profiles corpus is made of 50 profiles from which 30 concern general news and events (national and international affairs, politics, sports?) and 20 concern scientific and technical subjects.
In languages that use diacritical characters, if these special signs are stripped-off from a word, the resulted string of characters may not exist in the language, and therefore its normative form is, in general, easy to recover. However, this is not always the case, as presence or absence of a diacritical sign attached to a base letter of a word which exists in both variants, may change its grammatical properties or even the meaning, making the recovery of the missing diacritics a difficult task, not only for a program but sometimes even for a human reader. We describe and evaluate an accurate knowledge-based system for automatic recovery of the missing diacritics in MS-Office documents written in Romanian. For the rare cases when the system is not able to make a reliable decision, it either provides the user a list of words with their recovery suggestions, or probabilistically chooses one of the possible changes, but leaves a trace (a highlighted comment) on each word the modification of which was uncertain.
This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data. We adapted the French Interlanguage Database FRIDA tagset (Granger, 2003a) to the data. We chose FRIDA in order to follow a known standard and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the distance between the two languages with respect to learner difficulty. The current collection of texts, which is constantly growing, contains intermediate and advanced-level student writings. We describe the need for such corpora, the learner data we have collected and the tagset we have developed. We also describe the error frequency distribution of both proficiency levels and the ongoing work.
Some time in the future, some spelling error correction system will correct all the errors, and only the errors. We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal. We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision, as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path. We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in ones evaluations. We finally contrast our preferred metrics to Accuracy, which is widely used in this field to this day and to the Area-Under-the-Curve, which is increasingly finding acceptance in other fields.
This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles. To create the corpus, we propose an algorithm based on Gale and Churchs sentence alignment algorithm(1993). However, our algorithm not only relies on character length information, but also uses subtitle-timing information, which is encoded in the subtitle files. Timing is highly correlated between subtitles in different versions (for the same movie), since subtitles that match should be displayed at the same time. However, the absolute time values cant be used for alignment, since the timing is usually specified by frame numbers and not by real time, and converting it to real time values is not always possible, hence we use normalized subtitle duration instead. This results in a significant reduction in the alignment error rate.
Research into spoken language has become more visual over the years. Both fundamental and applied research have progressively included gestures, gaze, and facial expression. Corpora of multi-modal conversational speech are rare and frequently difficult to use due to privacy and copyright restrictions. A freely available annotated corpus is presented, gratis and libre, of high quality video recordings of face-to-face conversational speech. Annotations include orthography, POS tags, and automatically generated phonemes transcriptions and word boundaries. In addition, labeling of both simple conversational function and gaze direction has been a performed. Within the bounds of the law, everything has been done to remove copyright and use restrictions. Annotations have been processed to RDBMS tables that allow SQL queries and direct connections to statistical software. From our experiences we would like to advocate the formulation of best practises for both legal handling and database storage of recordings and annotations.
This paper describes a multichannel acoustic data collection recorded under the European DICIT project, during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories. The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal processing front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms. In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations, allowing for repeatable experiments. To match the project requirements, the WOZ experiments were recorded in three languages: English, German and Italian. Besides the user inputs, the database also contains non-speech related acoustic events, room impulse response measurements and video data, the latter used to compute 3D labels. Sessions were manually transcribed and segmented at word level, introducing also specific labels for acoustic events.
The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research. The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects, so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project. The paper describes the project, and estimates its benefits and problems. It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project.
This paper presents AnCora, a multilingual corpus annotated at different linguistic levels consisting of 500,000 words in Catalan (AnCora-Ca) and in Spanish (AnCora-Es). At present AnCora is the largest multilayer annotated corpus of these languages freely available from http://clic.ub.edu/ancora. The two corpora consist mainly of newspaper texts annotated at different levels of linguistic description: morphological (PoS and lemmas), syntactic (constituents and functions), and semantic (argument structures, thematic roles, semantic verb classes, named entities, and WordNet nominal senses). All resulting layers are independent of each other, thus making easier the data management. The annotation was performed manually, semiautomatically, or fully automatically, depending on the encoded linguistic information. The development of these basic resources constituted a primary objective, since there was a lack of such resources for these languages. A second goal was the definition of a consistent methodology that can be followed in further annotations. The current versions of AnCora have been used in several international evaluation competitions
We introduce the corpus of United States Congressional bills from 1947 to 1998 for use by language research communities. The U.S. Policy Agenda Legislation Corpus Volume 1 (USPALCV1) includes more than 375,000 legislative bills annotated with a hierarchical policy area category. The human annotations in USPALCV1 have been reliably applied over time to enable social science analysis of legislative trends. The corpus is a member of an emerging family of corpora that are annotated by policy area to enable comparative parallel trend recognition across countries and domains (legislation, political speeches, newswire articles, budgetary expenditures, web sites, etc.). This paper describes the origins of the corpus, its creation, ways to access it, design criteria, and an analysis with common supervised machine learning methods. The use of machine learning methods establishes a baseline proposed modeling for the topic classification of legal documents.
This paper explores how a battery of unsupervised techniques can be used in order to create large, high-quality corpora for textual inference applications, such as systems for recognizing textual entailment (TE) and textual contradiction (TC). We show that it is possible to automatically generate sets of positive and negative instances of textual entailment and contradiction from textual corpora with greater than 90{\%} precision. We describe how we generated more than 1 million TE pairs - and a corresponding set of and 500,000 TC pairs - from the documents found in the 2 GB AQUAINT-2 newswire corpus.
Based on the idea that local contexts predict the same basic category across a language, we develop a simple method for comparing tagsets across corpora. The principle differences between tagsets are evidenced by variation in categories in one corpus in the same contexts where another corpus exhibits only a single tag. Such mismatches highlight differences in the definitions of tags which are crucial when porting technology from one annotation scheme to another.
The computational linguistics community in The Netherlands and Belgium has long recognized the dire need for a major reference corpus of written Dutch. In part to answer this need, the STEVIN programme was established. To pave the way for the effective building of a 500-million-word reference corpus of written Dutch, a pilot project was established. The Dutch Corpus Initiative project or D-Coi was highly successful in that it not only realized about 10{\%} of the projected large reference corpus, but also established the best practices and developed all the protocols and the necessary tools for building the larger corpus within the confines of a necessarily limited budget. We outline the steps involved in an endeavour of this kind, including the major highlights and possible pitfalls. Once converted to a suitable XML format, further linguistic annotation based on the state-of-the-art tools developed either before or during the pilot by the consortium partners proved easily and fruitfully applicable. Linguistic enrichment of the corpus includes PoS tagging, syntactic parsing and semantic annotation, involving both semantic role labeling and spatiotemporal annotation. D-Coi is expected to be followed by SoNaR, during which the 500-million-word reference corpus of Dutch should be built.
In this paper, we determine the relationships between nursing activities and nurseing conversations based on the principle of maximum entropy. For analysis of the features of nursing activities, we built nursing corpora from actual nursing conversation sets collected in hospitals that involve various information about nursing activities. Ex-nurses manually assigned nursing activity information to the nursing conversations in the corpora. Since it is inefficient and too expensive to attach all information manually, we introduced an automatic nursing activity determination method for which we built models of relationships between nursing conversations and activities. In this paper, we adopted a maximum entropy approach for learning. Even though the conversation data set is not large enough for learning, acceptable results were obtained.
Managing large groups of human judges to perform any annotation task is a challenge. Linguistic Data Consortium coordinated the creation of manual machine translation post-editing results for the DARPA Global Autonomous Language Exploration Program. Machine translation is one of three core technology components for GALE, which includes an annual MT evaluation administered by National Institute of Standards and Technology. Among the training and test data LDC creates for the GALE program are gold standard translations for system evaluation. The GALE machine translation system evaluation metric is edit distance, measured by HTER (human translation edit rate), which calculates the minimum number of changes required for highly-trained human editors to correct MT output so that it has the same meaning as the reference translation. LDC has been responsible for overseeing the post-editing process for GALE. We describe some of the accomplishments and challenges of completing the post-editing effort, including developing a new web-based annotation workflow system, and recruiting and training human judges for the task. In addition, we suggest that the workflow system developed for post-editing could be ported efficiently to other annotation efforts.
Linguists have long been producing grammatical decriptions of yet undescribed languages. This is a time-consuming process, which has already adapted to improved technology for recording and storage. We present here a novel application of NLP techniques to bootstrap analysis of collected data and speed-up manual selection work. To be more precise, we argue that unsupervised induction of morphology and part-of-speech analysis from raw text data is mature enough to produce useful results. Experiments with Latent Semantic Analysis were less fruitful. We exemplify this on Mpiemo, a so-far essentially undescribed Bantu language of the Central African Republic, for which raw text data was available.
This paper describes a method of readability measurement of Japanese texts based on a newly compiled textbook corpus. The textbook corpus consists of 1,478 sample passages extracted from 127 textbooks of elementary school, junior high school, high school, and university; it is divided into thirteen grade levels and the total size is about a million characters. For a given text passage, the readability measurement method determines the grade level to which the passage is the most similar by using character-unigram models, which are constructed from the textbook corpus. Because this method does not require sentence-boundary analysis and word-boundary analysis, it is applicable to texts that include incomplete sentences and non-regular text fragments. The performance of this method, which is measured by the correlation coefficient, is considerably high (R {\textgreater} 0.9); in case that the length of a text passage is limited in 25 characters, the correlation coefficient is still high (R = 0.83).
This paper reports on the design and construction of a bio-event annotated corpus which was developed with a specific view to the acquisition of semantic frames from biomedical corpora. We describe the adopted annotation scheme and the annotation process, which is supported by a dedicated annotation tool. The annotated corpus contains 677 abstracts of biomedical research articles.
Chemistry research papers are a primary source of information about chemistry, as in any scientific field. The presentation of the data is, predominantly, unstructured information, and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques. At one level, extracting the relevant information from research papers is a text mining task, requiring both extensive language resources and specialised knowledge of the subject domain. However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels. The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research. This relies on the cooperation of several journal publishers to provide papers in an appropriate form. The work is carried out as a collaboration involving the Computer Laboratory, Chemistry Department and eScience Centre at Cambridge University, and is funded under the UK eScience programme.
We provide an overview of corpus building efforts at the Jena University Language {\&} Information Engineering (JULIE) Lab which are focused on life science documents. Special emphasis is laid on semantic annotations in terms of a large amount of biomedical named entities (almost 100 entity types), semantic relations, as well as discourse phenomena, reference relations in particular.
This paper describes the design, implementation and population of a lexical resource for biology and bioinformatics (the BioLexicon) developed within an ongoing European project. The aim of this project is text-based knowledge harvesting for support to information extraction and text mining in the biomedical domain. The BioLexicon is a large-scale lexical-terminological resource encoding different information types in one single integrated resource. In the design of the resource we follow the ISO/DIS 24613 Lexical Mark-up Framework standard, which ensures reusability of the information encoded and easy exchange of both data and architecture. The design of the resource also takes into account the needs of our text mining partners who automatically extract syntactic and semantic information from texts and feed it to the lexicon. The present contribution first describes in detail the model of the BioLexicon along its three main layers: morphology, syntax and semantics; then, it briefly describes the database implementation of the model and the population strategy followed within the project, together with an example. The BioLexicon database in fact comes equipped with automatic uploading procedures based on a common exchange XML format, which guarantees that the lexicon can be properly populated with data coming from different sources.
We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax-based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection.
Large repositories of life science data in the form of domain-specific literature and large specialised textual collections increase on a daily basis to a level beyond the human mind can grasp and interpret. As the volume of data continues to increase, substantial support from new information technologies and computational techniques grounded in the mining paradigm is becoming apparent. These emerging technologies play a critical role in aiding research productivity, and they provide the means for reducing the workload for information access and decision support and for speeding up and enhancing the knowledge discovery process. In order to accomplish these higher level goals a fundamental and unavoidable starting point is the identification and mapping of terminology from unstructured data to biomedical knowledge sources and concept hierarchies. This paper provides a description of the work regarding terminology recognition using the Swedish MeSH{\copyright} thesaurus and its corresponding English source. The various transformation and refinement steps applied to the original database tables into a fully-fledged processing-oriented annotating resource are explained. Particular attention has been given to a number of these steps in order to automatically map the extensive variability of lexical terms to structured MeSH{\copyright} nodes. Issues on annotation and coverage are also discussed.
With the information overload in the life sciences there is an increasing need for annotated corpora, particularly with biological and biomedical entities, which is the driving force for data-driven language processing applications and the empirical approach to language study. Inspired by the work in the GENIA Corpus, which is one of the very few of such corpora, extensively used in the biomedical field, and in order to fulfil the needs of our research, we have collected a Swedish medical corpus, the MEDLEX Corpus. MEDLEX is a large structurally and linguistically annotated document collection, consisting of a variety of text documents related to various medical text subfields, and does not focus at a particular medical genre, due to the lack of large Swedish resources within a particular medical subdomain. Out of this collection we selected 300 documents which were manually examined by two human experts who inspected, corrected and/or accordingly modified the automatically provided annotations according to a set of provided labelling guidelines. The annotations consist of medical terminology provided by the Swedish and English MeSH{\copyright} (Medical Subject Headings) thesauri as well as named entity labels provided by an enhanced named entity recognition software.
In this article, we present a method for extracting automatically semantic relations from texts in the medical domain using linguistic patterns. These patterns refer to three levels of information about words: inflected form, lemma and part-of-speech. The method we present consists first in identifying the entities that are part of the relations to extract, that is to say diseases, exams, treatments, drugs or symptoms. Thereafter, sentences that contain couples of entities are extracted and the presence of a semantic relation is validated by applying linguistic patterns. These patterns were previously learnt automatically from a manually annotated corpus by relying onan algorithm based on the edit distance. We first report the results of an evaluation of our medical entity tagger for the five types of entities we have mentioned above and then, more globally, the results of an evaluation of our extraction method for four relations between these entities. Both evaluations were done for French.
Subcategorization is a kind of knowledge which can be considered as crucial in several NLP tasks, such as Information Extraction or parsing, but the collection of very large resources including subcategorization representation is difficult and time-consuming. Various experiences show that the automatic extraction can be a practical and reliable solution for acquiring such a kind of knowledge. The aim of this paper is to investigate the relationships between subcategorization frame extraction and the nature of data from which the frames have to be extracted, e.g. how much the task can be influenced by the richness/poorness of the annotation. Therefore, we present some experiments that apply statistical subcategorization extraction methods, known in literature, on an Italian treebank that exploits a rich set of dependency relations that can be annotated at different degrees of specificity. Benefiting from the availability of relation sets that implement different granularity in the representation of relations, we evaluate our results with reference to previous works in a cross-linguistic perspective.
We present an approach to creating a treebank of sentences using multiple notations or linguistic theories simultaneously. We illustrate the method by annotating sentences from the Penn Treebank II in three different theories in parallel: the original PTB notation, a Functional Dependency Grammar notation, and a Government and Binding style notation. Sentences annotated with all of these theories are represented in XML as a directed acyclic graph where nodes and edges may carry extra information depending on the theory encoded.
We report on an effort to build a corpus of Modern Hebrew tagged with part-of-speech and morphology. We designed a tagset specific to Hebrew while focusing on four aspects: the tagset should be consistent with common linguistic knowledge; there should be maximal agreement among taggers as to the tags assigned to maintain consistency; the tagset should be useful for machine taggers and learning algorithms; and the tagset should be effective for applications relying on the tags as input features. In this paper, we illustrate these issues by explaining our decision to introduce a tag for beinoni forms in Hebrew. We explain how this tag is defined, and how it helped us improve manual tagging accuracy to a high-level, while improving automatic tagging and helping in the task of syntactic chunking.
We present a study of the word interaction networks of Bengali in the framework of complex networks. The topological properties of these networks reveal interesting insights into the morpho-syntax of the language, whereas clustering helps in the induction of the natural word classes leading to a principled way of designing POS tagsets. We compare different network construction techniques and clustering algorithms based on the cohesiveness of the word clusters. Cohesiveness is measured against two gold-standard tagsets by means of the novel metric of tag-entropy. The approach presented here is a generic one that can be easily extended to any language.
Automatic tagging in Spanish has historically faced many problems because of some specific grammatical constructions. One of these traditional pitfalls is the se particle. This particle is a multifunctional and polysemous word used in many different contexts. Many taggers do not distinguish the possible uses of se and thus provide poor results at this point. In tune with the philosophy of free software, we have taken a free annotation tool as a basis, we have improved and enhanced its behaviour by adding new rules at different levels and by modifying certain parts in the code to allow for its possible implementation in other EAGLES-compliant tools. In this paper, we present the analysis carried out with different annotators for selecting the tool, the results obtained in all cases as well as the improvements added and the advantages of the modified tagger.
On many examples we present a query language of Netgraph - a fully graphical tool for searching in the Prague Dependency Treebank 2.0. To demonstrate that the query language fits the treebank well, we study an annotation manual for the most complex layer of the treebank - the tectogrammatical layer - and show that linguistic phenomena annotated on the layer can be searched for using the query language.
The PARC 700 dependency bank has a number of features that would seem to make it less than optimally suited for its intended purpose, parser evaluation. However, it is difficult to know precisely what impact these problems have on the evaluation results, and as a first step towards making comparison possible, a subset of the same sentences is presented here, marked up using a different format that avoids them. In this new representation, the tokens contain exactly the same sequence of characters as the original text, word order is encoded explicitly, and there is no artificial distinction between full tokens and attribute tokens. There is also a clear division between word tokens and empty nodes, and the token attributes are stored together with the word, instead of being spread out individually in the file. A standard programming language syntax is used for the data, so there is little room for markup errors. Finally, the dependency links are closer to standard grammatical terms, which presumably makes it easier to understand what they mean and to convert any particular parser output format to the Kalashnikov 691 representation. The data is provided both in machine-readable format and as graphical dependency trees.
Motivated by the expense in time and other resources to produce hand-crafted grammars, there has been increased interest in automatically obtained wide-coverage grammars from treebanks for natural language processing. In particular, recent years have seen the growth in interest in automatically obtained deep resources that can represent information absent from simple CFG-type structured treebanks and which are considered to produce more language-neutral linguistic representations, such as dependency syntactic trees. As is often the case in early pioneering work on natural language processing, English has provided the focus of first efforts towards acquiring deep-grammar resources, followed by successful treatments of, for example, German, Japanese, Chinese and Spanish. However, no comparable large-scale automatically acquired deep-grammar resources have been obtained for French to date. The goal of this paper is to present the application of treebank-based language acquisition to the case of French. We show that with modest changes to the established parsing architectures, encouraging results can be obtained for French, with an overall best dependency structure f-score of 86.73{\%}.
The paper presents a tool assisting manual annotation of linguistic data developed at the Department of Computational linguistics, IBL-BAS. Chooser is a general-purpose modular application for corpus annotation based on the principles of commonality and reusability of the created resources, language and theory independence, extendibility and user-friendliness. These features have been achieved through a powerful abstract architecture within the Model-View-Controller paradigm that is easily tailored to task-specific requirements and readily extendable to new applications. The tool is to a considerable extent independent of data format and representation and produces outputs that are largely consistent with existing standards. The annotated data are therefore reusable in tasks requiring different levels of annotation and are accessible to external applications. The tool incorporates edit functions, pass and arrangement strategies that facilitate annotators work. The relevant module produces tree-structured and graph-based representations in respective annotation modes. Another valuable feature of the application is concurrent access by multiple users and centralised storage of lexical resources underlying annotation schemata, as well as of annotations, including frequency of selection, updates in the lexical database, etc. Chooser has been successfully applied to a number of tasks: POS tagging, WS and syntactic annotation.
The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy users information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages.
Reported speech in the form of direct and indirect reported speech is an important indicator of evidentiality in traditional newspaper texts, but also increasingly in the new media that rely heavily on citation and quotation of previous postings, as for instance in blogs or newsgroups. This paper details the basic processing steps for reported speech analysis and reports on performance of an implementation in form of a GATE resource.
We outline work performed within the framework of a current EC project. The goal is to construct a language-independent information system for a specific domain (environment/ecology/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (Kybots). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here.
We describe methods for extracting interesting factual relations from scientific texts in computational linguistics and language technology taken from the ACL Anthology. We use a hybrid NLP architecture with shallow preprocessing for increased robustness and domain-specific, ontology-based named entity recognition, followed by a deep HPSG parser running the English Resource Grammar (ERG). The extracted relations in the MRS (minimal recursion semantics) format are simplified and generalized using WordNet. The resulting quriples are stored in a database from where they can be retrieved (again using abstraction methods) by relation-based search. The query interface is embedded in a web browser-based application we call the Scientists Workbench. It supports researchers in editing and online-searching scientific papers.
Discovering relations among Named Entities (NEs) from large corpora is both a challenging, as well as useful task in the domain of Natural Language Processing, with applications in Information Retrieval (IR), Summarization (SUM), Question Answering (QA) and Textual Entailment (TE). The work we present resulted from the attempt to solve practical issues we were confronted with while building systems for the tasks of Textual Entailment Recognition and Question Answering, respectively. The approach consists in applying grammar induced extraction patterns on a large corpus - Wikipedia - for the extraction of relations between a given Named Entity and other Named Entities. The results obtained are high in precision, determining a reliable and useful application of the built resource.
We describe and evaluate a prototype system for recognising person and place names in digitised records of British parliamentary proceedings from the late 17th and early 19th centuries. The output of an OCR engine is the input for our system and we describe certain issues and errors in this data and discuss the methods we have used to overcome the problems. We describe our rule-based named entity recognition system for person and place names which is implemented using the LT-XML2 and LT-TTT2 text processing tools. We discuss the annotation of a development and testing corpus and provide results of an evaluation of our system on the test corpus.
Entities - people, organizations, locations and the like - have long been a central focus of natural language processing technology development, since entities convey essential content in human languages. For multilingual systems, accurate translation of named entities and their descriptors is critical. LDC produced Entity Translation pilot data to support the ACE ET 2007 Evaluation and the current paper delves more deeply into the entity alignment issue across languages, combining the automatic alignment techniques developed for ACE-07 with manual alignment. Altogether 84{\%} of the Chinese-English entity mentions and 74{\%} of the Arabic-English entity mentions are perfect aligned. The results of this investigation offer several important insights. Automatic alignment algorithms predicted that perfect alignment for the ET corpus was likely to be no greater than 55{\%}; perfect alignment on the 15 pilot documents was predicted at 62.5{\%}. Our results suggest the actual perfect alignment rate is substantially higher (82{\%} average, 92{\%} for NAM entities). The careful analysis of alignment errors also suggests strategies for human translation to support the ET task; for instance, translators might be given additional guidance about preferred treatments of name versus nominal translation. These results can also contribute to refined methods of evaluating ET systems.
This paper addresses a novel approach that integrates two different types of information resources: the World Wide Web and libraries. This approach is based on a hypothesis: advantages and disadvantages of the Web and libraries are complemental. The integration is based on correspondent conceptual label names between the Wikipedia categories and subject headings of library materials. The method enables us to find locations of bookshelves in a library easily, using any query keywords. Any keywords which are registered as Wikipedia items are acceptable. The advantages of the method are: the integrative approach makes subject access of library resources have broader coverage than an approach which only uses subject headings; and the approach navigates us to reliable information resources. We implemented the proposed method into an application system, and are now operating the system at several university libraries in Japan. We are planning to evaluate the method based on the query logs collected by the system.
In this paper we present results from using Random indexing for Latent Semantic Analysis to handle Singular Value Decomposition tractability issues. In the paper we compare Latent Semantic Analysis, Random Indexing and Latent Semantic Analysis on Random Indexing reduced matrices. Our results show that Latent Semantic Analysis on Random Indexing reduced matrices provide better results on Precision and Recall than Random Indexing only. Furthermore, computation time for Singular Value Decomposition on a Random indexing reduced matrix is almost halved compared to Latent Semantic Analysis.
The paper presents a set of approaches to extend the automatically created Slovene wordnet with nominal multi-word expressions. In the first approach multi-word expressions from Princeton WordNet are translated with a technique that is based on word-alignment and lexico-syntactic patterns. This is followed by extracting new terms from a monolingual corpus using keywordness ranking and contextual patterns. Finally, the multi-word expressions are assigned a hypernym and added to our wordnet. Manual evaluation and comparison of the results shows that the translation approach is the most straightforward and accurate. However, it is successfully complemented by the two monolingual approaches which are able to identify more term candidates in the corpus that would otherwise go unnoticed. Some weaknesses of the proposed wordnet extension techniques are also addressed.
In this paper we present a new Document Management System called DrStorage. This DMS is multi-platform, JCR-170 compliant, supports WebDav, versioning, user authentication and authorization and the most widespread file formats (Adobe PDF, Microsoft Office, HTML,...). It is also easy to customize in order to enhance its search capabilities and to support automatic metadata assignment. DrStorage has been integrated with an automatic language guesser and with an automatic keyword extractor: these metadata can be assigned automatically to documents, because the DrStorages server part has benn modified to allow that metadata assignment takes place as documents are put in the repository. Metadata can greatly improve the search capabilites and the results quality of a search engine. DrStorages client has been customized with two search results view: the first, called timeline view, shows temporal trends of queries as an histogram, the second, keyword cloud, shows which words are correlated and how much are correlated with the results of a particular day.
Treatment of Multiword Expressions (MWEs) is one of the most complicated issues in natural language processing, especially in Machine Translation (MT). The paper presents dictionary of MWEs for a English-Latvian MT system, demonstrating a way how MWEs could be handled for inflected languages with rich morphology and rather free word order. The proposed dictionary of MWEs consists of two constituents: a lexicon of phrases and a set of MWE rules. The lexicon of phrases is rather similar to translation lexicon of the MT system, while MWE rules describe syntactic structure of the source and target sentence allowing correct transformation of different MWE types into the target language and ensuring correct syntactic structure. The paper demonstrates this approach on different MWE types, starting from simple syntactic structures, followed by more complicated cases and including fully idiomatic expressions. Automatic evaluation shows that the described approach increases the quality of translation by 0.6 BLEU points.
The project presented here is a part of a long term research program aiming at a full lexicon grammar for Polish (SyntLex). The main concern of this project is computer-assisted acquisition and morpho-syntactic description of verb-noun collocations in Polish. We present methodology and resources obtained in three main project phases which are: dictionary-based acquisition of collocation lexicon, feasibility study for corpus-based lexicon enlargement phase, corpus-based lexicon enlargement and collocation description. In this paper we focus on the results of the third phase. The presented here corpus-based approach permitted us to triple the size the verb-noun collocation dictionary for Polish. In the paper we describe the SyntLex Dictionary of Collocations and announce some future research intended to be a separate project continuation.
For compounding languages, a great part of the topical semantics is transported via nominal compounds. Various applications of natural language processing can profit from explicit access to these compounds, provided by a lexicon. The best way to acquire such a resource is to harvest corpora that represent the domain in question. For Chinese, a significant difficulty lies in the fact that the text comes as a string of characters, only segmented by sentence boundaries. Extraction algorithms that solely rely on context variety do not perform precisely enough. We propose a pipeline of filters that starts from a candidate set established by accessor variety and then employs several methods to improve precision. For the experiments the Xinhua part of the Chinese Gigaword Corpus was used. We extracted a random sample of 200 story texts with 119,509 Hanzi characters. All compound words of this evaluation corpus were tagged, segmented into their morphemes, and augmented with the POS-information of their segments. A cascade of filters applied to a preliminary set of compound candidates led to a very high precision of over 90{\%}, measured for the types. The result also holds for a small corpus where a solely contextual method introduces too much noise, even for the longer compounds. An introduction of MI into the basic candidacy algorithm led to a much higher recall with still reasonable precision for subsequent manual processing. Especially for the four-character compounds, that in our sample represent over 40{\%} of the target data, the method has sufficient efficacy to support the rapid construction of compound dictionaries from domain corpora.
We present an experiment in extracting collocations from the FrameNet corpus, specifically, support verbs such as direct in Environmentalists directed strong criticism at world leaders. Support verbs do not contribute meaning of their own and the meaning of the construction is provided by the noun; the recognition of support verbs is thus useful in text understanding. Having access to a list of support verbs is also useful in applications that can benefit from paraphrasing, such as generation (where paraphrasing can provide variety). This paper starts with a brief presentation of the notion of lexical function in Meaning-Text Theory, where they fall under the notion of lexical function, and then discusses how relevant information is encoded in the FrameNet corpus. We describe the resource extracted from the FrameNet corpus.
This paper describes Eksairesis, a system for learning economic domain knowledge automatically from Modern Greek text. The knowledge is in the form of economic terms and the semantic relations that govern them. The entire process in based on the use of minimal language-dependent tools, no external linguistic resources, and merely free, unstructured text. The methodology is thereby easily portable to other domains and other languages. The text is pre-processed with basic morphological annotation, and semantic (named and other) entities are identified using supervised learning techniques. Statistical filtering, i.e. corpora comparison is used to extract domain terms and supervised learning is again employed to detect the semantic relations between pairs of terms. Advanced classification schemata, ensemble learning, and one-sided sampling, are experimented with in order to deal with the noise in the data, which is unavoidable due to the low pre-processing level and the lack of sophisticated resources. An average 68.5{\%} f-score over all the classes is achieved when learning semantic relations. Bearing in mind the use of minimal resources and the highly automated nature of the process, classification performance is very promising, compared to results reported in previous work.
Although ontologies and linguistic resources play a key role in applied AI and NLP, they have not been developed in a common and systematic way. The lack of a systematic methodology for their development has lead to the production of resources that exhibit common flaws between them, and that, at least when it come to ontologies, negatively impact their results and reusability. In this paper, we introduce a software-engineering methodology for the construction of ontology-based linguistic resources, and present a sound conceptual schema that takes into account several considerations for the construction of software tools that allow the systematic and controlled construction of ontology-based linguistic resources.
OntoSelect is a dynamic web-based ontology library that harvests, analyzes and organizes ontologies published on the Semantic Web. OntoSelect allows searching as well as browsing of ontologies according to size (number of classes, properties), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies) and human languages used for class- and object property-labels. Ontology search in OntoSelect is based on a combined measure of coverage, structure and connectedness. Further, and in contrast to other ontology search engines, OntoSelect provides ontology search based on a complete web document instead of one or more keywords only.
In the field of ontology mapping, multilingual ontology mapping is an issue that is not well explored. This paper proposes a framework for mapping of multilingual Description Logics (DL) ontologies. First, the DL source ontology is translated to the target ontology language, using a lexical database or a dictionary, generating a DL translated ontology. The target and the translated ontologies are then used as input for the mapping process. The mappings are computed by specialized agents using different mapping approaches. Next, these agents use argumentation to exchange their local results, in order to agree on the obtained mappings. Based on their preferences and confidence of the arguments, the agents compute their preferred mapping sets. The arguments in such preferred sets are viewed as the set of globally acceptable arguments. A DL mapping ontology is generated as result of the mapping process. In this paper we focus on the process of generating the DL translated ontology.
This paper presents the process of acquiring a large, domain independent, taxonomy from the German Wikipedia. We build upon a previously implemented platform that extracts a semantic network and taxonomy from the English version of the Wikipedia. We describe two accomplishments of our work: the semantic network for the German language in which isa links are identified and annotated, and an expansion of the platform for easy adaptation for a new language. We identify the platforms strengths and shortcomings, which stem from the scarcity of free processing resources for languages other than English. We show that the taxonomy induction process is highly reliable - evaluated against the German version of WordNet, GermaNet, the resource obtained shows an accuracy of 83.34{\%}.
In this paper we present a Linguistic Meta-Model (LMM) allowing a semiotic-cognitive representation of knowledge. LMM is freely available and integrates the schemata of linguistic knowledge resources, such as WordNet and FrameNet, as well as foundational ontologies, such as DOLCE and its extensions. In addition, LMM is able to deal with multilinguality and to represent individuals and facts in an open domain perspective.
After a long history of compilation of our own lexical resources, EDR Japanese/English Electronic Dictionary, and discussions with major players on development of various WordNets, Japanese National Institute of Information and Communications Technology started developing the Japanese WordNet in 2006 and will publicly release the first version, which includes both the synset in Japanese and the annotated Japanese corpus of SemCor, in June 2008. As the first step in compiling the Japanese WordNet, we added Japanese equivalents to synsets of the Princeton WordNet. Of course, we must also add some synsets which do not exist in the Princeton WordNet, and must modify synsets in the Princeton WordNet, in order to make the hierarchical structure of Princeton synsets represent thesaurus-like information found in the Japanese language, however, we will address these tasks in a future study. We then translated English sentences which are used in the SemCor annotation into Japanese and annotated them using our Japanese WordNet. This article describes the overview of our project to compile Japanese WordNet and other resources which relate to our Japanese WordNet.
The majority of work described in this paper was conducted as part of the Recovering Evidence from Video by fusing Video Evidence Thesaurus and Video MetaData (REVEAL) project, sponsored by the UKs Engineering and Physical Sciences Research Council (EPSRC). REVEAL is concerned with reducing the time-consuming, yet essential, tasks undertaken by UK Police Officers when dealing with terascale collections of video related to crime-scenes. The project is working towards technologies which will archive video that has been annotated automatically based on prior annotations of similar content, enabling rapid access to CCTV archives and providing capabilities for automatic video summarisation. This involves considerations of semantic annotation relating, amongst other things, to content and to temporal reasoning. In this paper, we describe the ontology extraction components of the system in development, and its use in REVEAL for automatically populating a CCTV ontology from analysis of expert transcripts of the video footage.
An ontological knowledge management system requires dynamic and encapsulating operation in order to share knowledge among communities. The key to success of knowledge sharing in the field of agriculture is using and sharing agreed terminologies such as ontological knowledge especially in multiple languages. This paper proposes a workbench with three authoring tools for collaborative multilingual ontological knowledge construction and maintenance, in order to add value and support communities in the field of food and agriculture. The framework consists of the multilingual ontological knowledge construction and maintenance workbench platform, which composes of ontological knowledge management and user management, and three ontological knowledge authoring tools. The authoring tools used are two ontology extraction tools, ATOM and KULEX, and one ontology integration tool.
Lexical ontologies and semantic lexicons are important resources in natural language processing. They are used in various tasks and applications, especially where semantic processing is evolved such as question answering, machine translation, text understanding, information retrieval and extraction, content management, text summarization, knowledge acquisition and semantic search engines. Although there are a number of semantic lexicons for English and some other languages, Persian lacks such a complete resource to be used in NLP works. In this paper we introduce an ongoing project on developing a lexical ontology for Persian called FarsNet. We exploited a hybrid semi-automatic approach to acquire lexical and conceptual knowledge from resources such as WordNet, bilingual dictionaries, mono-lingual corpora and morpho-syntactic and semantic templates. FarsNet is an ontology whose elements are lexicalized in Persian. It provides links between various types of words (cross POS relations) and also between words and their corresponding concepts in other ontologies (cross ontologies relations). FarsNet aggregates the power of WordNet on nouns, the power of FrameNet on verbs and the wide range of conceptual relations from ontology community.
Rogets Thesaurus and WordNet are very widely used lexical reference works. We describe an automatic mapping procedure that effectively produces French translations of the terms in these two resources. Our approach to the challenging task of disambiguation is based on structural statistics as well as measures of semantic relatedness that are utilized to learn a classification model for associations between entries in the thesaurus and French terms taken from bilingual dictionaries. By building and applying such models, we have produced French versions of Rogets Thesaurus and WordNet with a considerable level of accuracy, which can be used for a variety of different purposes, by humans as well as in computational applications.
This paper is a contribution to formal ontology study. Some entities belong more or less to a class. In particular, some individual entities are attached to classes whereas they do not check all the properties of the class. To specify whether an individual entity belonging to a class is typical or not, we borrow the topological concepts of interior, border, closure, and exterior. We define a system of relations by adapting these topological operators. A scale of typicality, based on topology, is introduced. It enables to define levels of typicality where individual entities are more or less typical elements of a concept.
The measurement of conceptual similarity in a hierarchical structure has been proposed by studies such as Wu and Palmer (1994) which have been summarized and evaluated in Budanisky and Hirst (2006). The present study applies the measurement of conceptual similarity to conceptual metaphor research by comparing concreteness of ontological resource nodes to several prototypical concrete nodes selected by human subjects. Here, the purpose of comparing conceptual similarity between nodes is to select a concrete sense for a word which is used metaphorically. Through using WordNet-SUMO interface such as SinicaBow (Huang, Chang and Lee, 2004), concrete senses of a lexicon will be selected once its SUMO nodes have been compared in terms of conceptual similarity with the prototypical concrete nodes. This study has strong implications for the interaction of psycholinguistic and computational linguistic fields in conceptual metaphor research.
Many of the Japanese ideographs (Chinese characters) have a few meanings. Such ambiguities should be identified by using their contextual information. For example, we have an ideograph which has two pronunciations, /hitai/ and /gaku/, the former means a forehead of the human body and the latter has two meanings, an amount of money and a picture frame. Conventional methods for such a disambiguation problem have been using statistical methods with co-occurrence of words in their context. In this research, Contextual Dynamic Network Model is developed using the Associative Concept Dictionary which includes semantic relations among concepts/words and the relations can be represented with quantitative distances. In this model, an interactive activation method is used to identify a words meaning on the Contextual Semantic Network where the activation on the network is calculated using the distances. The proposed method constructs dynamically the Contextual Semantic Network according to the input words sequentially that appear in the sentence including an ambiguous word.
Generally, ontology learning and population is applied as a semi-automatic approach to knowledge acquisition in natural language understanding systems. That means, after the ontology is created or populated, an expert of the domain can still change or refine the newly acquired knowledge. In an incremental ontology learning framework (as e.g. applied for open-domain dialog systems) this approach is not sufficient as knowledge about the real world is dynamic and, therefore, has to be acquired and updated constantly. In this paper we propose the storing of newly acquired instances of an ontological concept in a separate database instead of integrating them directly into the systems knowledge base. The advantage is that possibly incorrect knowledge is not part of the systems ontology but stored aside. Furthermore, information about the confidence about the learned instances can be displayed and used for a final revision as well as a further automatic acquisition.
Computational terminology has notably evolved since the advent of computers. Regarding the extraction of terms in particular, a large number of resources have been developed: from very general tools to other much more specific acquisition methodologies. Such acquisition methodologies range from using simple linguistic patterns or frequency counting methods to using much more evolved strategies combining morphological, syntactical, semantical and contextual information. Researchers usually develop a term extractor to be applied to a given domain and, in some cases, some testing about the tool performance is also done. Afterwards, such tools may also be applied to other domains, though frequently no additional test is made in such cases. Usually, the application of a given tool to other domain does not require any tuning. Recently, some tools using semantic resources have been developed. In such cases, either a domain-specific or a generic resource may be used. In the latter case, some tuning may be necessary in order to adapt the tool to a new domain. In this paper, we present the task started in order to adapt YATE, a term extractor that uses a generic resource as EWN and that is already developed for the medical domain, into the economic one.
We present an approach to the discovery of semantically similar terms that utilizes a web search engine as both a source for generating related terms and a tool for estimating the semantic similarity of terms. The system works by associating with each document in the search engines index a weighted term vector comprising those phrases that best describe the documents subject matter. Related terms for a given seed phrase are generated by running the seed as a search query and mining the result vector produced by averaging the weights of terms associated with the top documents of the query result set. The degree of similarity between the seed term and each related term is then computed as the cosine of the angle between their respective result vectors. We test the effectiveness of this approach for building a term recommender system designed to help online advertisers discover additional phrases to describe their product offering. A comparison of its output with that of several alternative methods finds it to be competitive with the best known alternative.
The purpose of this paper is to clarify the temporal aspect of terminology focusing on the dictionarys impact on terms. We used womens studies terms as data and examined the changes of their values of five automatic term recognition (ATR) measures before and after dictionary publication. The changes of precision and recall of extraction based on these measures were also examined. The measures are TFIDF, C-value, MC-value, Nakagawas FLR, and simple document frequencies. We found that being listed in dictionaries gives longevity to terms and prevent them from losing termhood that is represented by these ATR measures. The peripheral or relatively less important terms are more likely to be influenced by dictionaries and their termhood increase after being listed in dictionaries. Among the termhood, the potential of word formation that can be measured by Nakagawas FLR seemed to be influenced most and the terms gradually gained it after being listed in dictionaries.
Automatic Term recognition (ATR) is a fundamental processing step preceding more complex tasks such as semantic search and ontology learning. From a large number of methodologies available in the literature only a few are able to handle both single and multi-word terms. In this paper we present a comparison of five such algorithms and propose a combined approach us{\neg}ing a voting mechanism. We evaluated the six approaches using two different corpora and show how the voting algo{\neg}rithm performs best on one corpus (a collection of texts from Wikipedia) and less well using the Genia corpus (a standard life science corpus). This indicates that choice and design of corpus has a major impact on the evaluation of term recog{\neg}nition algorithms. Our experiments also showed that single-word terms can be equally important and occupy a fairly large proportion in certain domains. As a result, algorithms that ignore single-word terms may cause problems to tasks built on top of ATR. Effective ATR systems also need to take into account both the unstructured text and the structured aspects and this means information extraction techniques need to be integrated into the term recognition process.
In this paper, we investigate the use of a machine-learning based approach to the specific problem of scientific term detection in patient information. Lacking lexical databases which differentiate between the scientific and popular nature of medical terms, we used local context, morphosyntactic, morphological and statistical information to design a learner which accurately detects scientific medical terms. This study is the first step towards the automatic replacement of a scientific term by its popular counterpart, which should have a beneficial effect on readability. We show a F-score of 84{\%} for the prediction of scientific terms in an English and Dutch EPAR corpus. Since recasting the term extraction problem as a classification problem leads to a large skewedness of the resulting data set, we rebalanced the data set through the application of some simple TF-IDF-based and Log-likelihood-based filters. We show that filtering indeed has a beneficial effect on the learners performance. However, the results of the filtering approach combined with the learning-based approach remain below those of the learning-based approach.
In this paper we describe the methodology and the first steps for the creation of WNTERM (from WordNet and Terminology), a specialized lexicon produced from the merger of the EuroWordNet-based Multilingual Central Repository (MCR) and the Basic Encyclopaedic Dictionary of Science and Technology (BDST). As an example, the ecology domain has been used. The final result is a multilingual (Basque and English) light-weight domain ontology, including taxonomic and other semantic relations among its concepts, which is tightly connected to other wordnets.
This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy/hyponymy relation.
In this paper we present an evaluation resource for geographic information retrieval developed within the Cross Language Evaluation Forum (CLEF). The GeoCLEF track is dedicated to the evaluation of geographic information retrieval systems. The resource encompasses more than 600,000 documents, 75 topics so far, and more than 100,000 relevance judgments for these topics. Geographic information retrieval requires an evaluation resource which represents realistic information needs and which is geographically challenging. Some experimental results and analysis are reported
Manual categorisation of documents is a time-consuming task that has been significantly alleviated with the deployment of automatic and machine-aided text categorisation systems. However, the proliferation of multilingual documentation has become a common phenomenon in many international organisations, while most of the current systems have focused on the categorisation of monolingual text. It has been recently shown that the inherent redundancy in bilingual documents can be effectively exploited by relatively simple, bilingual naive Bayes (multinomial) models. In this work, we present a refined version of these models in which this redundancy is explicitly captured by a combination of a unigram (multinomial) model and the well-known IBM 1 translation model. The proposed model is evaluated on two bilingual classification tasks and compared to previous work.
This paper proposes a ping-pong document clustering method using NMF and the linkage based refinement alternately, in order to improve the clustering result of NMF. The use of NMF in the ping-pong strategy can be expected effective for document clustering. However, NMF in the ping-pong strategy often worsens performance because NMF often fails to improve the clustering result given as the initial values. Our method handles this problem with the stop condition of the ping-pong process. In the experiment, we compared our method with the k-means and NMF by using 16 document data sets. Our method improved the clustering result of NMF significantly.
Spectral clustering is a powerful clustering method for document data set. However, spectral clustering needs to solve an eigenvalue problem of the matrix converted from the similarity matrix corresponding to the data set. Therefore, it is not practical to use spectral clustering for a large data set. To overcome this problem, we propose the method to reduce the similarity matrix size. First, using k-means, we obtain a clustering result for the given data set. From each cluster, we pick up some data, which are near to the central of the cluster. We take these data as one data. We call this data set as committee. Data except for committees remain one data. For these data, we construct the similarity matrix. Definitely, the size of this similarity matrix is reduced so much that we can perform spectral clustering using the reduced similarity matrix.
Accessing structured data in the form of ontologies requires training and learning formal query languages (e.g., SeRQL or SPARQL) which poses significant difficulties for non-expert users. One of the ways to lower the learning overhead and make ontology queries more straightforward is through a Natural Language Interface (NLI). While there are existing NLIs to structured data with reasonable performance, they tend to require expensive customisation to each new domain or ontology. Additionally, they often require specific adherence to a pre-defined syntax which, in turn, means that users still have to undergo training. In this paper we present Question-based Interface to Ontologies (QuestIO) - a tool for querying ontologies using unconstrained language-based queries. QuestIO has a very simple interface, requires no user training and can be easily embedded in any system or used with any ontology or knowledge base without prior customisation.
Catchwords refer to popular words or phrases within certain area in certain period of time. In this paper, we propose a novel approach for automatic Chinese catchwords extraction. At the beginning, we discuss the linguistic definition of catchwords and analyze the features of catchwords by manual evaluation. According to those features of catchwords, we define three aspects to describe Popular Degree of catchwords. To extract terms with maximum meaning, we adopt an effective ATE algorithm for multi-character words and long phrases. Then we use conic fitting in Time Series Analysis to build Popular Degree Curves of extracted terms. To calculate Popular Degree Values of catchwords, a formula is proposed which includes values of Popular Trend, Peak Value and Popular Keeping. Finally, a ranking list of catchword candidates is built according to Popular Degree Values. Experiments show that automatic Chinese catchword extraction is effective and objective in comparison with manual evaluation.
We describe ParsCit, a freely available, open-source implementation of a reference string parsing package. At the core of ParsCit is a trained conditional random field (CRF) model used to label the token sequences in the reference string. A heuristic model wraps this core with added functionality to identify reference strings from a plain text file, and to retrieve the citation contexts. The package comes with utilities to run it as a web service or as a standalone utility. We compare ParsCit on three distinct reference string datasets and show that it compares well with other previously published work.
Recently, language resources (LRs) are becoming indispensable for linguistic research. Unfortunately, it is not easy to find their usages by searching the web even though they must be described in the Internet or academic articles. This indicates that the intrinsic value of LRs is not recognized very well. In this research, therefore, we extract a list of usage information for each LR to promote the efficient utilization of LRs. In this paper, we proposed a method for extracting a list of usage information from academic articles by using rules based on syntactic information. The rules are generated by focusing on the syntactic features that are observed in the sentences describing usage information. As a result of experiments, we achieved 72.9{\%} in recall and 78.4{\%} in precision for the closed test and 60.9{\%} in recall and 72.7{\%} in precision for the open test.
One problem of data-driven answer extraction in open-domain factoid question answering is that the class distribution of labeled training data is fairly imbalanced. In an ordinary training set, there are far more incorrect answers than correct answers. The class-imbalance is, thus, inherent to the classification task. It has a deteriorating effect on the performance of classifiers trained by standard machine learning algorithms. They usually have a heavy bias towards the majority class, i.e. the class which occurs most often in the training set. In this paper, we propose a method to tackle class imbalance by applying some form of cost-sensitive learning which is preferable to sampling. We present a simple but effective way of estimating the misclassification costs on the basis of class distribution. This approach offers three benefits. Firstly, it maintains the distribution of the classes of the labeled training data. Secondly, this form of meta-learning can be applied to a wide range of common learning algorithms. Thirdly, this approach can be easily implemented with the help of state-of-the-art machine learning software.
The paper deals with the task of definition extraction from a small and noisy corpus of instructive texts. Three approaches are presented: Partial Parsing, Machine Learning and a sequential combination of both. We show that applying ML methods with the support of a trivial grammar gives results better than a relatively complicated partial grammar, and much better than pure ML approach.
The research field of extracting knowledge bases from text collections seems to be mature: its target and its working hypotheses are clear. In this paper we propose a platform, YAPEK, i.e., Yet Another Platform for Extracting Knowledge from corpora, that wants to be the base to collect the majority of algorithms for extracting knowledge bases from corpora. The idea is that, when many knowledge extraction algorithms are collected under the same platform, relative comparisons are clearer and many algorithms can be leveraged to extract more valuable knowledge for final tasks such as Textual Entailment Recognition. As we want to collect many knowledge extraction algorithms, YAPEK is based on the three working hypotheses of the area: the basic hypothesis, the distributional hypothesis, and the point-wise assertion patterns. In YAPEK, these three hypotheses define two spaces: the space of the target textual forms and the space of the contexts. This platform guarantees the possibility of rapidly implementing many models for extracting knowledge from corpora as the platform gives clear entry points to model what is really different in the different algorithms: the feature spaces, the distances in these spaces, and the actual algorithm.
In the context of ontology-based information extraction, identity resolution is the process of deciding whether an instance extracted from text refers to a known entity in the target domain (e.g. the ontology). We present an ontology-based framework for identity resolution which can be customized to different application domains and extraction tasks. Rules for identify resolution, which compute similarities between target and source entities based on class information and instance properties and values, can be defined for each class in the ontology. We present a case study of the application of the framework to the problem of multi-source job vacancy extraction
We have performed a set of experiments made to investigate the utility of morphological analysis to improve retrieval of documents written in languages with relatively large morphological variation in a practical commercial setting, using the SiteSeeker search system developed and marketed by Euroling Ab. The objective of the experiments was to evaluate different lemmatisers and stemmers to determine which would be the most practical for the task at hand: highly interactive, relatively high precision web searches in commercial customer-oriented document collections. This paper gives an overview of some of the results for Finnish and German, and describes specifically one experiment designed to investigate the case distribution of nouns in a highly inflectional language (Finnish) and the topicality of the nouns in target texts. We find that topical nouns taken from queries are distributed differently over relevant and non-relevant documents depending on their grammatical case.
We address here the need to assist users in rapidly accessing the most important or strategic information in the text corpus by identifying sentences carrying specific information. More precisely, we want to identify contribution of authors of scientific papers through a categorization of sentences using rhetorical and lexical cues. We built local grammars to annotate sentences in the corpus according to their rhetorical status: objective, new things, results, findings, hypotheses, conclusion, related{\_}word, future work. The annotation is automatically projected automatically onto two other corpora to test their portability across several domains. The local grammars are implemented in the Unitex system. After sentence categorization, the annotated sentences are clustered and users can navigate the result by accessing specific information types. The results can be used for advanced information retrieval purposes.
A frequent problem in automatic categorization applications involving Portuguese language is the absence of large corpora of previously classified documents, which permit the validation of experiments carried out. Generally, the available corpora are not classified or, when they are, they contain a very reduced number of documents. The general goal of this study is to contribute to the development of applications which aim at text categorization for Brazilian Portuguese. Specifically, we point out that keywords selection associated with neural networks can improve results in the categorization of Brazilian Portuguese texts. The corpus is composed of 30 thousand texts from the Folha de S{\~a}o Paulo newspaper, organized in 29 sections. In the process of categorization, the k-Nearest Neighbor (k-NN) algorithm and the Multilayer Perceptron neural networks trained with the backpropagation algorithm are used. It is also part of our study to test the identification of keywords parting from the log-likelihood statistical measure and to use them as features in the categorization process. The results clearly show that the precision is better when using neural networks than when using the k-NN.
In this paper we present an algorithm for automatic extraction of textual elements, namely titles and full text, associated with news stories in news web pages. We propose a supervised machine learning classification technique based on the use of a Support Vector Machine (SVM) classifier to extract the desired textual elements. The technique uses internal structural features of a webpage without relying on the Document Object Model to which many content authors fail to adhere. The classifier uses a set of features which rely on the length of text, the percentage of hypertext, etc. The resulting classifier is nearly perfect on previously unseen news pages from different sites. The proposed technique is successfully employed in Alzoa.com, which is the largest Arabic news aggregator on the web.
What kinds of lexical resources are helpful for extracting useful information from domain-specific documents? Although domain-specific documents contain much useful knowledge, it is not obvious how to extract such knowledge efficiently from the documents. We need to develop techniques for extracting hidden information from such domain-specific documents. These techniques do not necessarily use state-of-the-art technologies and achieve deep and accurate language understanding, but are based on huge amounts of linguistic resources, such as domain-specific lexical databases. In this paper, we introduce two techniques for extracting informative expressions from documents: the extraction of related words that are not only taxonomically related but also thematically related, and the acquisition of salient terms and phrases. With these techniques we then attempt to automatically and statistically extract domain-specific informative expressions in aviation documents as an example and evaluate the results.
Many systems have been developed in the past few years to assist researchers in the discovery of knowledge published as English text, for example in the PubMed database. At the same time, higher level collective knowledge is often published using a graphical notation representing all the entities in a pathway and their interactions. We believe that these pathway visualizations could serve as an effective user interface for knowledge discovery if they can be linked to the text in publications. Since the graphical elements in a Pathway are of a very different nature than their corresponding descriptions in English text, we developed a prototype system called PathText. The goal of PathText is to serve as a bridge between these two different representations. In this paper, we first describe the overall architecture and the interfaces of the PathText system, and then provide some details about the core Text Mining components.
We have analyzed the SPEX algorithm by Bernstein and Zobel (2004) for detecting co-derivative documents using duplicate n-grams. Although we totally agree with the claim that not using unique n-grams can greatly increase the efficiency and scalability of the process of detecting co-derivative documents, we have found serious bottlenecks in the way SPEX finds the duplicate n-grams. While the memory requirements for computing co-derivative documents can be reduced to up to 1{\%} by only using duplicate n-grams, SPEX needs about 40 times more memory for computing the list of duplicate n-grams itself. Therefore the memory requirements of the whole process are not reduced enough to make the algorithm practical for very large collections. We propose a solution for this problem using an external sort with the suffix array in-memory sorting and temporary file compression. The proposed algorithm for computing duplicate n-grams uses a fixed amount of memory for any input size.
We report about a project which brings together Natural Language Processing and eLearning. One of the functionalities developed within this project is the possibility to annotate learning objects semi-automatically with keywords. To this end, a keyword extractor has been created which is able to handle documents in 8 languages. The approach employed is based on a linguistic processing step which is followed by a filtering step of candidate keywords and their subsequent ranking based on frequency criteria. Three tests have been carried out to provide a rough evaluation of the performance of the tool, to measure inter annotator agreement in order to determine the complexity of the task and to evaluate the acceptance of the proposed keywords by users.
Relation extraction is the task of finding pre-defined semantic relations between two entities or entity mentions from text. Many methods, such as feature-based and kernel-based methods, have been proposed in the literature. Among them, feature-based methods draw much attention from researchers. However, to the best of our knowledge, existing feature-based methods did not explicitly incorporate the position feature and no in-depth analysis was conducted in this regard. In this paper, we define and exploit nine types of position information between two named entity mentions and then use it along with other features in a multi-class classification framework for Chinese relation extraction. Experiments on the ACE 2005 data set show that the position feature is more effective than the other recognized features like entity type/subtype and character-based N-gram context. Most important, it can be easily captured and does not require as much effort as applying deep natural language processing.
The release of the Enron corpus provided a unique resource for studying aspects of email use, because it is largely unfiltered, and therefore presents a relatively complete collection of emails for a reasonably large number of correspondents. This paper describes a newly created subcorpus of the Enron emails which we suggest can be used to test techniqes for authorship attribution, and further shows the application of three different classification methods to this task to present baseline results. Two of the classifiers used are are standard, and have been shown to perform well in the literature, and one of the classifiers is novel and based on concurrent work that proposes a Bayesian hierarchical distribution for word counts in documents. For each of the classifiers, we present results using six text representations, including use of linguistic structures derived from a parser as well as lexical information.
Each year NIST releases a set of question, document id, answer-triples for the factoid questions used in the TREC Question Answering track. While this resource is widely used and proved itself useful for many purposes, it also is too coarse a grain-size for a lot of other purposes. In this paper we describe how we have used Amazons Mechanical Turk to have multiple subjects read the documents and identify the sentences themselves which contain the answer. For most of the 1911 questions in the test sets from 2002 to 2006 and each of the documents said to contain an answer, the Question-Answer Sentence Pairs (QASP) corpus introduced in this paper contains the identified answer sentences. We believe that this corpus, which we will make available to the public, can further stimulate research in QA, especially linguistically motivated research, where matching the question to the answer sentence by either syntactic or semantic means is a central concern.
This paper presents various strategies for improving the extraction performance of less prominent relations with the help of the rules learned for similar relations, for which large volumes of data are available that exhibit suitable data properties. The rules are learned via a minimally supervised machine learning system for relation extraction called DARE. Starting from semantic seeds, DARE extracts linguistic grammar rules associated with semantic roles from parsed news texts. The performance analysis with respect to different experiment domains shows that the data property plays an important role for DARE. Especially the redundancy of the data and the connectivity of instances and pattern rules have a strong influence on recall. However, most real-world data sets do not possess the desirable small-world property. Therefore, we propose three scenarios to overcome the data property problem of some domains by exploiting a similar domain with better data properties. The first two strategies stay with the same corpus but try to extract new similar relations with learned rules. The third strategy adapts the learned rules to a new corpus. All three strategies show that frequently mentioned relations can help in the detection of less frequent relations.
This paper proposes an extension of Sumida and Torisawas method of acquiring hyponymy relations from hierachical layouts in Wikipedia (Sumida and Torisawa, 2008). We extract hyponymy relation candidates (HRCs) from the hierachical layouts in Wikipedia by regarding all subordinate items of an item x in the hierachical layouts as xs hyponym candidates, while Sumida and Torisawa (2008) extracted only direct subordinate items of an item x as xs hyponym candidates. We then select plausible hyponymy relations from the acquired HRCs by running a filter based on machine learning with novel features, which even improve the precision of the resulting hyponymy relations. Experimental results show that we acquired more than 1.34 million hyponymy relations with a precision of 90.1{\%}.
We present a topic boundary detection method that searches for connections between sequences of utterances in multi party dialogues. The connections are established based on word identity. We compare our method to a state-of-the art automatic Topic boundary detection method that was also used on multi party dialogues. We checked various methods of preprocessing of the data, including stemming, lemmatization and stopword filtering with a text-based as well as speech-based stopword lists. Using standard evaluation methods we found that our method outperformed the state-of-the art method.
In this paper Semantic Press, a tool for the automatic press review, is introduced. It is based on Text Mining technologies and is tailored to meet the needs of the eGovernment and eParticipation communities. First, a general description of the application demands emerging from the eParticipation and eGovernment sectors is offered. Then, an introduction to the framework of the automatic analysis and classification of newspaper content is provided, together with a description of the technologies underlying it.
In this paper, we describe an approach that aims to model heterogeneous resources for information extraction. Document is modeled in graph representation that enables better understanding of multi-media document and its structure which ultimately could result better cross-media information extraction. We also describe our proposed algorithm that segment document-based on the document modeling approach we described in this paper.
In this paper we present and discuss the results of a text coherence experiment performed on a small corpus of Romanian text from a number of alternative high school manuals. During the last 10 years, an abundance of alternative manuals for high school was produced and distributed in Romania. Due to the large amount of material and to the relative short time in which it was produced, the question of assessing the quality of this material emerged; this process relied mostly of subjective human personal opinion, given the lack of automatic tools for Romanian. Debates and claims of poor quality of the alternative manuals resulted in a number of examples of incomprehensible / incoherent paragraphs extracted from such manuals. Our goal was to create an automatic tool which may be used as an indication of poor quality of such texts. We created a small corpus of representative texts from Romanian alternative manuals. We manually classified the chosen paragraphs from such manuals into two categories: comprehensible/coherent text and incomprehensible/incoherent text. We then used different machine learning techniques to automatically classify them in a supervised manner. Our approach is rather simple, but the results are encouraging.
We aim to characterize the comparability of corpora, we address this issue in the trilingual context through the distinction of expert and non expert documents. We work separately with corpora composed of documents from the medical domain in three languages (French, Japanese and Russian) which present an important linguistic distance between them. In our approach, documents are characterized in each language by their topic and by a discursive typology positioned at three levels of document analysis: structural, modal and lexical. The document typology is implemented with two learning algorithms (SVMlight and C4.5). Evaluation of results shows that the proposed discursive typology can be transposed from one language to another, as it indeed allows to distinguish the two aimed discourses (science and popular science). However, we observe that performances vary a lot according to languages, algorithms and types of discursive characteristics.
This paper describes a syllabification based conversion method for converting romanized Persian text to the traditional Arabic-based writing system. The system is implemented in Xerox XFST and relies on rule based conversion of words rather than using morphological analysis. The paper presents a brief evaluation of the accuracy of the transcriptions generated by the method.
The growing dependence of modern society on the Web as a vital source of information and communication has become inevitable. However, the Web has become an ideal channel for various terrorist organisations to publish their misleading information and send unintelligible messages to communicate with their clients as well. The increase in the number of published anomalous misleading information on the Web has led to an increase in security threats. The existing Web security mechanisms and protocols are not appropriately designed to deal with such recently developed problems. Developing technology to detect anomalous textual information has become one of the major challenges within the NLP community. This paper introduces the problem of anomalous text detection by automatically extracting linguistic features from documents and evaluating those features for patterns of suspicious and/or inconsistent information in Arabic documents. In order to achieve that, we defined specific linguistic features that characterise various Arabic writing styles. Also, the paper introduces the main challenges in Arabic processing and describes the proposed unsupervised learning model for detecting anomalous Arabic textual information.
Text condensation aims at shortening the length of an utterance without losing essential textual information. In this paper, we report on the implementation and preliminary evaluation of a sentence condensation tool for Greek using a manually constructed table of 450 lexical paraphrases, and a set of rules that delete syntactic subtrees that carry minor semantic information. Evaluation on two-sentence sets show promising results regarding grammaticality and semantic acceptability of compressed versions.
Hardly any other kind of text structures is as notoriously difficult to read as patents. This is first of all due to their abstract vocabulary and their very complex syntactic constructions. Especially the claims in a patent are a challenge: in accordance with international patent writing regulations, each claim must be rendered in a single sentence. As a result, sentences with more than 200 words are not uncommon. Therefore, paraphrasing of the claims in terms the user can understand is of high demand. We present a rule-based paraphrasing module that realizes paraphrasing of patent claims in English as a rewriting task. Prior to the rewriting proper, the module implies the stages of simplification and discourse and syntactic analyses. The rewriting makes use of a full-fledged text generator and consists in a number of genuine generation tasks such as aggregation, selection of referring expressions, choice of discourse markers and syntactic generation. As generator, we use the MATE-work bench, which is based on the Meaning-Text Theory of linguistics.
The orthographical complexities of Chinese, Japanese, Korean (CJK) and Arabic pose a special challenge to developers of NLP applications. These difficulties are exacerbated by the lack of a standardized orthography in these languages, especially the highly irregular Japanese orthography and the ambiguities of the Arabic script. This paper focuses on CJK and Arabic orthographic variation and provides a brief analysis of the linguistic issues. The basic premise is that statistical methods by themselves are inadequate, and that linguistic knowledge supported by large-scale lexical databases should play a central role in achieving high accuracy in disambiguating and normalizing orthographic variants.
This paper focuses on automatically improving the readability of documents. We explore mechanisms relating to content control that could be used (i) by authors to improve the quality and consistency of the language used in authoring; and (ii) to find a means to demonstrate this to readers. To achieve this, we implemented and evaluated a number of software components, including those of the University of Surrey Department of Computings content analysis applications (System Quirk). The software integrates these components within the commonly available GATE software and incorporates language resources considered useful within the standards development process: a Plain English thesaurus; lookup of ISO terminology provided from a terminology management system (TMS) via ISO 16642; automatic terminology discovery using statistical and linguistic techniques; and readability metrics. Results lead us to the development of an assistive tool, initially for authors of standards but not considered to be limited only to such authors, and also to a system that provides automatic annotation of texts to help readers to understand them. We describe the system developed and made freely available under the auspices of the EU eContent project LIRICS.
Combinatorial Category Grammar is (CCG) a lexicalized grammar formalism which is expressed by syntactic category, a logical form representation. There are difficulties in representing CCG without any visualization tools. This paper presents a design framework of OpenCCG workbench and visualization tool which enables linguists to develop CCG based lexicons more easily. Our research is aimed to resolve these gaps by developing a user-friendly tool. OpenCCG Workbench, an open source web-based environment, was developed to enable multiple users to visually create and update grammars for using with the OpenCCG library. It was designed to streamline and speed-up the lexicon building process, and to free the linguists from writing XML files which is both cumbersome and error-prone. The system consists of three sub-systems: grammar management system, grammar validator system, and concordance retrieval system. In this paper we will mainly discuss the most important parts, grammar management and validation systems, which are directly related to a CCG lexicon construction. We support users in three levels; Expert linguists who play a role as lexical entry designer, normal linguists who adds or edits lexicons, and guests who requires an acquisition to the lexicon into their applications.
This paper presents an algorithm for correcting language errors typical of second-language learners. We focus on preposition errors, which are very common among second-language learners but are not addressed well by current commercial grammar correctors and editing aids. The algorithm takes as input a sentence containing a preposition error (and possibly other errors as well), and outputs the correct preposition for that particular sentence context. We use a two-phase hybrid rule-based and statistical approach. In the first phase, rule-based processing is used to generate a short expression that captures the context of use of the preposition in the input sentence. In the second phase, Web searches are used to evaluate the frequency of this expression, when alternative prepositions are used instead of the original one. We tested this algorithm on a corpus of 133 French sentences written by intermediate second-language learners, and found that it could address 69.9{\%} of those cases. In contrast, we found that the best French grammar and spell checker currently on the market, Antidote, addressed only 3{\%} of those cases. We also showed that performance degrades gracefully when using a corpus of frequent n-grams to evaluate frequencies.
This paper presents a context sensitive spell checking system that uses mixed trigram models, and introduces a new empirically grounded method for building confusion sets. The proposed method has been implemented, tested, and evaluated in terms of coverage, precision, and recall. The results show that the method is effective.
This paper presents a methodology for the design and implementation of user-centred language checking applications. The methodology is based on the separation of three critical aspects in this kind of application: functional purpose (educational or corrective goal), types of warning messages, and linguistic resources and computational techniques used. We argue that to assure a user-centred design there must be a clear-cut division between the error typology underlying the system and the software architecture. The methodology described has been used to implement two different user-driven spell, grammar and style checkers for Catalan. We discuss that this is an issue often neglected in commercial applications, and remark the benefits of such a methodology in the scalability of language checking applications. We evaluate our application in terms of recall, precision and noise, and compare it to the only other existing grammar checker for Catalan, to our knowledge.
The Internet has become the most popular platform for communication. However because most of the modern computer keyboard is Latin-based, Asian languages such as Chinese cannot input its characters (Hanzi) directly with these keyboards. As a result, methods for representing Chinese characters using Latin alphabets were introduced. The most popular method among these is the Pinyin input system. Pinyin is also called Romanised Chinese in that it phonetically resembles a Chinese character. Due to the highly ambiguous mapping from Pinyin to Chinese characters, word misuses can occur using standard computer keyboard, and more commonly so in internet chat-rooms or instant messengers where the language used is less formal. In this paper we aim to develop a system that can automatically identify such anomalies, whether they are simple typos or whether they are intentional. After identifying them, the system should suggest the correct word to be used.
Basque is a highly inflected and agglutinative language (Alegria et al., 1996). Two-level morphology has been applied successfully to this kind of languages and there are two-level based descriptions for very different languages. After doing the morphological description for a language, it is easy to develop a spelling checker/corrector for this language. However, what happens if we want to use the speller in the free world (OpenOffice, Mozilla, emacs, LaTeX, etc.)? Ispell and similar tools (aspell, hunspell, myspell) are the usual mechanisms for these purposes, but they do not fit the two-level model. In the absence of two-level morphology based mechanisms, an automatic conversion from two-level description to hunspell is described in this paper.
Patients require access to Electronic Patient Records, however medical language is often too difficult for patients to understand. Explaining records to patients is a time-consuming task, which we attempt to simplify by automating the translation procedure. This paper introduces a research project dealing with the automatic rewriting of medical narratives for the benefit of patients. We are looking at various ways in which technical language can be transposed into patient-friendly language by means of a comparison with patient information materials. The text rewriting procedure we describe could potentially have an impact on the quality of information delivered to patients. We report on some preliminary experiments concerning rewriting at lexical and paragaph level. This is an ongoing project which currently addresses a restricted number of issues, including target text modelling and text rewriting at lexical level.
Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort. Accordingly, there is very limited availability of off-the shelf tools for researchers whose interests are not primarily in coreference or others who want to concentrate on a specific aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of Soon et al.s proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers. BART has been released as open source software and is available from http://www.sfs.uni-tuebingen.de/{\textasciitilde}versley/BART
The ability to make progress in Computational Linguistics depends on the availability of large annotated corpora, but creating such corpora by hand annotation is very expensive and time consuming; in practice, it is unfeasible to think of annotating more than one million words. However, the success of Wikipedia and other projects shows that another approach might be possible: take advantage of the willingness of Web users to contribute to collaborative resource creation. AnaWiki is a recently started project that will develop tools to allow and encourage large numbers of volunteers over the Web to collaborate in the creation of semantically annotated corpora (in the first instance, of a corpus annotated with information about anaphora).
We report the results of a study that investigates the agreement of anaphoric annotations. The study focuses on the influence of the factors text length and text type on a corpus of scientific articles and newspaper texts. In order to measure inter-annotator agreement we compare existing approaches and we propose to measure each step of the annotation process separately instead of measuring the resulting anaphoric relations only. A total amount of 3,642 anaphoric relations has been annotated for a corpus of 53,038 tokens (12,327 markables). The results of the study show that text type has more influence on inter-annotator agreement than text length. Furthermore, the definition of well-defined annotation instructions and coder training is a crucial point in order to receive good annotation results.
In this paper we present an extension of the MATE/GNOME annotation scheme for anaphora (Poesio, 2004) which accounts for abstract anaphora in Danish and Italian. By abstract anaphora it is here meant pronouns whose linguistic antecedents are verbal phrases, clauses and discourse segments. The extended scheme, which we call the DAD annotation scheme, allows to annotate information about abstract anaphora which is important to investigate their use, see i.a. (Webber, 1988; Gundel et al., 2003; Navarretta, 2004; Navarretta, 2007) and which can influence their automatic treatment. Intercoder agreement scores obtained by applying the DAD annotation scheme on texts and dialogues in the two languages are given and show that the information proposed in the scheme can be recognised in a reliable way.
This paper describes a study of the levels at which different rhetorical relations occur in rhetorical structure trees. In a previous empirical study (Williams and Reiter, 2003) of the RST-DT (Rhetorical Structure Theory Discourse Treebank) Corpus (Carlson et al., 2003), we noticed that certain rhetorical relations tended to occur more frequently at higher levels in a rhetorical structure tree, whereas others seemed to occur more often at lower levels. The present study takes a closer look at the data, partly to test this observation, and partly to investigate related issues such as the relative complexity of satellite and nucleus for each type of relation. One practical application of this investigation would be to guide discourse planning in Natural Language Generation (NLG), so that it reflects more accurately the structures found in documents written by human authors. We present our preliminary findings and discuss their relevance for discourse planning.
We present a knowledge-based coreference resolution system for noun phrases in Hungarian texts. The system is used as a module in an automated psychological text processing project. Our system uses rules that rely on knowledge from the morphological, syntactic and semantic output of a deep parser and semantic relations form the Hungarian WordNet ontology. We also use rules that rely on Binding Theory, research results in Hungarian psycholinguistics, current research on proper name coreference identification and our own heuristics. We describe the constraints-and-preferences algorithm in detail that attempts to find coreference information for proper names, common nouns, pronouns and zero pronouns in texts. We present evaluation results for our system on a corpus manually annotated with coreference relations. Precision of the resolution of various coreference types reaches up to 80{\%}, while overall recall is 63{\%}. We also present an investigation of the various error types our system produced along with an analysis of the results.
The Italian particle ne exhibits interesting anaphoric properties that have not been yet explored in depth from a corpus and computational linguistic perspective. We provide: (i) an overview of the phenomenon; (ii) a set of annotation schemes for marking up occurrences of ne; (iii) the description of a corpus annotated for this phenomenon ; (iv) a first assessment of the resolution task. We show that the schemes we developed are reliable, and that the actual distribution of partitive and non-partitive uses of ne is inversely proportional to the amount of attention that the two different uses have received in the linguistic literature. As an assessment of the complexity of the resolution task, we find that a recency-based baseline yields an accuracy of less than 30{\%} on both development and test data.
This paper outlines the new resource technologies, products and applications that have been constructed during the development of a multi-modal (MM hereafter) corpus tool on the DReSS project (Understanding New Forms of the Digital Record for e-Social Science), based at the University of Nottingham, England. The paper provides a brief outline of the DRS (Digital Replay System, the software tool at the heart of the corpus), highlighting its facility to display synchronised video, audio and textual data and, most relevantly, a concordance tool capable of interrogating data constructed from textual transcriptions anchored to video or audio, and from coded annotations of specific features of gesture-in-talk. This is complemented by a real-time demonstration of the DRS interface in-use as part of the LREC 2008 conference. This will serve to show the manner in which a system such as the DRS can be used to facilitate the assembly, storage and analysis of multi modal corpora, supporting both qualitative and quantitative approaches to the analysis of collected data.
Web count statistics gathered from search engines have been widely used as a resource in a variety of NLP tasks. For some tasks, however, the information they exploit is not fine-grained enough. We propose an inverted index over grammatical relations as a fast and reliable resource to access more general and also more detailed frequency information. To build the index, we use a dependency parser to parse a large corpus. We extract binary dependency relations, such as he-subj-say (he is the subject of say) as index terms and construct the index using publicly available open-source indexing software. The unit we index over is the sentence. The index can be used to extract grammatical relations and frequency counts for these relations. The framework also provides the possibility to search for partial dependencies (say, the frequency of he occurring in subject position), words, strings and a combination of these. One possible application is the disambiguation of syntactic structures.
This paper presents a freely available evaluation tool for dependency parsing: MaltEval (http://w3.msi.vxu.se/users/jni/malteval). It is flexible and extensible, and provides functionality for both quantitative evaluation and visualization of dependency structure. The quantitative evaluation is compatible with other standard evaluation software for dependency structure which does not produce visualization of dependency structure, and can output more details as well as new types of evaluation metrics. In addition, MaltEval has generic support for confusion matrices. It can also produce statistical significance tests when more than one parsed file is specified. The visualization module also has the ability to highlight discrepancies between the gold-standard files and the parsed files, and it comes with an easy to use GUI functionality to search in the dependency structure of the input files.
The Berkeley FrameNet Project (BFN) is making an English lexical database called FrameNet, which describes syntactic and semantic properties of an English lexicon extracted from large electronic text corpora (Baker et al., 1998). Other projects dealing with Spanish, German and Japanese follow a similar approach and annotate large corpora. FrameSQL is a web-based application developed by the author, and it allows the user to search the BFN database in a variety of ways (Sato, 2003). FrameSQL shows a clear view of the headwords grammar and combinatorial properties offered by the FrameNet database. FrameSQL has been developing and new functions were implemented for processing the Spanish FrameNet data (Subirats and Sato, 2004). FrameSQL is also in the process of incorporating the data of the Japanese FrameNet Project (Ohara et al., 2003) and that of the Saarbr{\"u
In this paper, we describe a system that divides example sentences (data set) into clusters, based on the meaning of the target word, using a semi-supervised clustering technique. In this task, the estimation of the cluster number (the number of the meaning) is critical. Our system primarily concentrates on this aspect. First, a user assigns the system an initial cluster number for the target word. The system then performs general clustering on the data set to obtain small clusters. Next, using constraints given by the user, the system integrates these clusters to obtain the final clustering result. Our system performs this entire procedure with high precision and requiring only a few constraints. In the experiment, we tested the system for 12 Japanese nouns used in the SENSEVAL2 Japanese dictionary task. The experiment proved the effectiveness of our system. In the future, we will improve sentence similarity measurements.
This paper describes an ongoing project Japanese FrameNet (JFN), a corpus-based lexicon of Japanese in the FrameNet style. This paper focuses on the set of software tools tailored for the JFN annotation process. As the first step in the annotation, annotators select target sentences from the JFN corpus using the JFN kwic search tool, where they can specify cooccurring words and/or the part of speech of collocates. Our search tool is capable of displaying the parsed tree of a target sentence and its neigbouring sentences. The JFN corpus mainly consists of balanced and copyright-free Japanese Corpus which is being built as a national project. After the sentence to be annotated is chosen, the annotator labels syntactic and semantic tags to the appropriate phrases in the sentence. This work is performed on an annotation platform called JFNDesktop, in which the functions of labeling assist and consistency checking of annotations are available. Preliminary evaluation of our platform shows such functions accelerate the annotation process.
In this paper we present JMWNL, a multilingual extension of the JWNL java library, which was originally developed for accessing Princeton WordNet dictionaries. JMWNL broadens the range of JWNLs accessible resources by covering also dictionaries produced inside the EuroWordNet project. Specific resources, such as language-dependent algorithmic stemmers, have been adopted to cover the diversities in the morphological nature of words in the addressed idioms. New semantic and lexical relations have been included to maximize compatibility with new versions of the original Princeton WordNet and to include the whole range of relations from EuroWordNet. Relations from Princeton WordNet on one side and EuroWordNet on the other one have in some cases been mapped to provide a uniform reference for coherent cross-linguistic use of the library.
This paper investigates the state of the art in automatic textual annotation tools, and examines the extent to which they are ready for use in the real world. We define some benchmarking criteria for measuring the usability of annotation tools, and examine those factors which are particularly important for a real user to be able to determine which is the most suitable tool for their use. We discuss factors such as usability, accessibility, interoperability and scalability, and evaluate a set of annotation tools according to these factors. Finally, we draw some conclusions about the current state of research in annotation and make some suggestions for the future.
In this work we propose a new strategy for the authorship identification problem and we test it on an example from Romanian literature: did Radu Albala found the continuation of Mateiu Caragiales novel Sub pecetea tainei, or did he write himself the respective continuation? The proposed strategy is based on the similarity of rankings of function words; we compare the obtained results with the results obtained by a learning method (namely Support Vector Machines -SVM- with a string kernel).
In this paper, we describe a unifying approach to tackle data heterogeneity issues for lexica and related resources. We present LEXUS, our software that implements the Lexical Markup Framework (LMF) to uniformly describe and manage lexica of different structures. LEXUS also makes use of a central Data Category Registry (DCR) to address terminological issues with regard to linguistic concepts as well as the handling of working and object languages. Finally, we report on ViCoS, a LEXUS extension, providing support for the definition of arbitrary semantic relations between lexical entries or parts thereof.
GermaNet is regarded to be a valuable resource for many German NLP applications, corpus research, and teaching. This demo presents three GUI-based tools meant to facilitate the exploration of and navigation through GermaNet. The GermaNet Explorer exhibits various retrieval, sort, filter and visualization functions for words/synsets and also provides an insight into the modeling of GermaNets semantic relations as well as its representation as a graph. The GermaNet-Measure-API and GermaNet Pathfinder offer methods for the calculation of semantic relatedness based on GermaNet as a resource and the visualization of (semantic) paths between words/synsets. The GermaNet-Measure-API furthermore features a flexible interface, which facilitates the integration of all relatedness measures provided into user-defined applications. We have already used the three tools in our research on thematic chaining and thematic indexing, as a tool for the manual annotation of lexical chains, and as a resource in our courses on corpus linguistics and semantics.
Development of lexical resources is, along with grammar development, one of the main efforts when building multilingual NLP applications. In this paper, we present a tool-based approach for more efficient manual lexicon development for a spoken language translation system. The approach in particular addresses the common problems of multilingual lexica including the redundancy of encoded information and inconsistency of lexica of different languages. The general benefits of this practical tool-based approach are clear and user-friendly lexicon structure, inheritance of information inside of a language and between different system languages, and transparency and consistency of coverage between system languages. The visual tool-based approach is user-friendly to linguistic informants that dont have previous experience of lexicon development, while at the same time, it still is a powerful tool for expert system developers.
This paper describes ODL, a description language for lexical information that is being developed within the context of a national project called MLRS (Maltese Language Resource Server) whose goal is to create a national corpus and computational lexicon for the Maltese language. The main aim of ODL is to make the task of the lexicographer easier by allowing lexical specifications to be set out formally so that actual entries will conform to them. The paper describes some of the background motivation, the ODL language itself, and concludes with a short example of how lexical values expressed in ODL can be mapped to an existing tagset together with some speculations about future work.
This paper focuses on different aspects of collaborative work used to create the electronic version of a dictionary in paper format, edited and printed by the Romanian Academy during the last century. In order to ensure accuracy in a reasonable amount of time, collaborative proofreading of the scanned material, through an on-line interface has been initiated. The paper details the activities and the heuristics used to maximize accuracy, and to evaluate the work of anonymous contributors with diverse backgrounds. Observing the behaviour of the enterprise for a period of 6 months allows estimating the feasibility of the approach till the end of the project.
This paper presents a feasibility study of a merge between SprogTeknologisk Ordbase (STO), which contains morphological and syntactic information, and DanNet, which is a Danish WordNet containing semantic information in terms of synonym sets and semantic relations. The aim of the merge is to develop a richer, composite resource which we believe will have a broader usage perspective than the two seen in isolation. In STO, the organizing principle is based on the observable syntactic features of a lemmas near context (labeled syntactic units or SynUs). In contrast, the basic unit in DanNet is constituted by semantic senses or - in wordnet terminology - synonym sets (synsets). The merge of the two resources is thus basically to be understood as a linking between SynUs and synsets. In the paper we discuss which parts of the merge can be performed semi-automatically and which parts require manual linguistic matching procedures. We estimate that this manual work will amount to approx. 39{\%} of the lexicon material.
This paper presents a multipurpose system for wordnet (WN) development, named Hydra. Hydra is an application for data editing and validation, as well as for data retrieval and synchronization between wordnets for different languages. The use of modal language for wordnet, the representation of wordnet as a relational database and the concurrent access are among its main advantages.
Multilingual Automatic Speech Recognition (ASR) systems are of great interest in multilingual environments. We studied the case of the Comunitat Valenciana where the two official languages are Spanish and Valencian. These two languages share most of their phonemes, and their syntax and vocabulary are also quite similar since they have influenced each other for many years. We constructed a system, and trained its acoustic models with a small corpus of Spanish and Valencian, which has produced poor results due to the lack of data. Adaptation techniques can be used to adapt acoustic models that are trained with a large corpus of a language inr order to obtain acoustic models for a phonetically similar language. This process is known as language adaptation. The Maximum Likelihood Linear Regression (MLLR) technique has commonly been used in speaker adaptation; however we have used MLLR in language adaptation. We compared several MLLR variants (mean square, diagonal matrix and full matrix) for language adaptation in order to choose the best alternative for our system.
When a user cannot find a word, he may think of semantically related words that could be used into an automatic process to help him. This paper presents an evaluation of lexical resources and semantic networks for modelling mental associations. A corpus of associations has been constructed for its evaluation. It is composed of 20 low frequency target words each associated 5 times by 20 users. In the experiments we look for the target word in propositions made from the associated words thanks to 5 different resources. The results show that even if each resource has a useful specificity, the global recall is low. An experiment to extract common semantic features of several associations showed that we cannot expect to see the target word below a rank of 20 propositions.
Terms, term relevances, and sentence relevances are concepts that figure in many NLP applications, such as Text Summarization. These concepts are implemented in various ways, though. In this paper, we want to shed light on the impact that different implementations can have on the overall performance of the systems. In particular, we examine the interplay between term definitions and sentence-scoring functions. For this, we define a gold standard that ranks sentences according to their significance and evaluate a range of relevant parameters with respect to the gold standard.
We report on the evaluation of information structural annotation according to the Linguistic Information Structure Annotation Guidelines (LISA, (Dipper et al., 2007)). The annotation scheme differentiates between the categories of information status, topic, and focus. It aims at being language-independent and has been applied to highly heterogeneous data: written and spoken evidence from typologically diverse languages. For the evaluation presented here, we focused on German texts of different types, both written texts and transcriptions of spoken language, and analyzed the annotation quantitatively and qualitatively.
We present in this paper a comparison between three segmentation systems for the Vietnamese language. Indeed, the majority of Vietnamese words is built by semantic composition from about 7,000 syllables, which also have a meaning as isolated words. So the identification of word boundaries in a text is not a simple task, and ambiguities often appear. Beyond the presentation of the tested systems, we also propose a standard definition for word segmentation in Vietnamese, and introduce a reference corpus developed for the purpose of evaluating such a task. The results observed confirm that it can be relatively well treated by automatic means, although a solution needs to be found to take into account out-of-vocabulary words.
The paper presents a comparative study of semantic and lexical relations defined and adopted in WordNet and EuroWordNet. This document describes the experimental observations achieved through the analysis of data from different WordNet versions and EuroWordNet distributions for different languages, during the development of JMWNL (Java Multilingual WordNet Library), an extensible multilingual library for accessing WordNet-like resources in different languages and formats. The goal of this work was to realize an operative mapping between the relations defined in the two lexical resources and to unify library access and content navigation methods for both WordNet and EuroWordNet. The analysis focused on similarities, differences, semantic overlaps or inclusions, factual misinterpretations and inconsistencies between the intended and practical use of each single relation defined in these two linguistic resources. The paper details with examples the produced mapping, discussing required operations which implied merging, extending or simply keeping separate the examined relations
The lack of structure in the content of email messages makes it very hard for data channelled between the sender and the recipient to be correctly interpreted and acted upon. As a result, the purposes of messages frequently end up not being fulfilled, prompting prolonged communication and stalling the disconnected workflow that is characteristic of email. This problem could be partially solved by extending the current email model to support light-weight semantics pertaining to the intents of the sender and the expectations from the recipient(s), thus leaving no room for ambiguity. Semantically-aware email clients will then be able to support the user with the workflow of email-generated tasks. In line with this thinking, we present the sMail Conceptual Framework. At its core, this framework has an Email Speech Act Model. Given this model, email content can be categorized into a set of speech acts, each carrying specific expectations. In this paper we present and discuss the methodology and results of this model?s statistical evaluation. By performing the same evaluation on another existing model, we demonstrate our model?s higher sophistication. After careful observations, we perform changes to the model and subsequently accommodate the changes in the revised sMail Conceptual Framework.
We present an evaluation of inter-sentential coreference annotation in the context of manually created semantic networks. The semantic networks are constructed independently be each annotator and require an entity mapping priori to evaluating the coreference. We introduce a model used for mapping the semantic entities as well as an algorithm used for our evaluation task. Finally, we report the raw statistics for inter-annotator agreement and describe the inherent difficulty in evaluating coreference in semantic networks.
The Arabic Treebank (ATB), released by the Linguistic Data Consortium, contains multiple annotation files for each source file, due in part to the role of diacritic inclusion in the annotation process. The data is made available in both vocalized and unvocalized forms, with and without the diacritic marks, respectively. Much parsing work with the ATB has used the unvocalized form, on the basis that it more closely represents the real-world situation. We point out some problems with this usage of the unvocalized data and explain why the unvocalized form does not in fact represent real-world data. This is due to some aspects of the treebank annotation that to our knowledge have never before been published.
West African languages are written with alphabets that comprize non classical Latin characters. It is possible to design virtual keyboards which allow the writing of such special characters with a combination of keys. During the last decade, many different virtual keyboards had been created, without any standardization to fix the correspondence between each character and the keys to press to obtain it. We define a grid to evaluate such keyboards and apply it to five virtual keyboards in relation with the five main languages of Niger (Fulfulde, Hausa, Kanuri, Songhai-Zarma, Tamashek), Bambara and Soninke from Mali and Dyoula from Burkina Faso. We conclude hat the African LLACAN keyboard should be recommended in Niger because it covers all the characters used in the alphabets of the main languages of this country, it produces valid Unicode codes and it minimizes the number of keys to be pressed.
Evaluation campaigns have become an established way to evaluate automatic systems which tackle the same task. This paper presents the first edition of the Anaphora Resolution Exercise (ARE) and the lessons learnt from it. This first edition focused only on English pronominal anaphora and NP coreference, and was organised as an exploratory exercise where various issues were investigated. ARE proposed four different tasks: pronominal anaphora resolution and NP coreference resolution on a predefined set of entities, pronominal anaphora resolution and NP coreference resolution on raw texts. For each of these tasks different inputs and evaluation metrics were prepared. This paper presents the four tasks, their input data and evaluation metrics used. Even though a large number of researchers in the field expressed their interest to participate, only three institutions took part in the formal evaluation. The paper briefly presents their results, but does not try to interpret them because in this edition of ARE our aim was not about finding why certain methods are better, but to prepare the ground for a fully-fledged edition.
In this paper we describe some studies of Portuguese-English word alignment, focusing on (i) measuring the importance of the coupling between dictionaries and corpus; (ii) assessing the relevance of using syntactic information (POS and lemma) or just word forms, and (iii) taking into account the direction of translation. We first provide some motivation for the studies, as well as insist in separating type from token anlignment. We then briefly describe the resources employed: the EuroParl and COMPARA corpora, and the alignment tools, NATools, introducing some measures to evaluate the two kinds of dictionaries obtained. We then present the results of several experiments, comparing sizes, overlap, translation fertility and alignment density of the several bilingual resources built. We also describe preliminary data as far as quality of the resulting dictionaries or alignment results is concerned.
This paper presents the evaluation of the dictionary look-up component of Mayo Clinics Information Extraction system. The component was tested on a corpus of 160 free-text clinical notes which were manually annotated with the named entity disease. This kind of clinical text presents many language challenges such as fragmented sentences and heavy use of abbreviations and acronyms. The dictionary used for this evaluation was a subset of SNOMED-CT with semantic types corresponding to diseases/disorders without any augmentation. The algorithm achieves an F-score of 0.56 for exact matches and F-scores of 0.76 and 0.62 for right and left-partial matches respectively. Machine learning techniques are currently under investigation to improve this task.
We report on the construction of a gold-standard dataset consisting of annotated clinical notes suitable for evaluating our biomedical named entity recognition system. The dataset is the result of consensus between four human annotators and contains 1,556 annotations on 160 clinical notes using 658 unique concept codes from SNOMED-CT corresponding to human disorders. Inter-annotator agreement was calculated on annotations from 100 of the documents for span (90.9{\%}), concept code (81.7{\%}), context (84.8{\%}), and status (86.0{\%}) agreement. Complete agreement for span, concept code, context, and status was 74.6{\%}. We found that creating a consensus set based on annotations from two independently-created annotation sets can reduce inter-annotator disagreement by 32.3{\%}. We found little benefit to pre-annotating the corpus with a third-party named entity recognizer.
Fixed, limited budgets often constrain the amount of expert annotation that can go into the construction of annotated corpora. Estimating the cost of annotation is the first step toward using annotation resources wisely. We present here a study of the cost of annotation. This study includes the participation of annotators at various skill levels and with varying backgrounds. Conducted over the web, the study consists of tests that simulate machine-assisted pre-annotation, requiring correction by the annotator rather than annotation from scratch. The study also includes tests representative of an annotation scenario involving Active Learning as it progresses from a na{\"\i
The explicit introduction of morphosyntactic information into statistical machine translation approaches is receiving an important focus of attention. The current freely available Part of Speech (POS) taggers for the French language are based on a limited tagset which does not account for some flectional particularities. Moreover, there is a lack of a unified framework of training and evaluation for these kinds of linguistic resources. Therefore in this paper, three standard POS taggers (Treetagger, Brills tagger and the standard HMM POS tagger) are trained and evaluated in the same conditions on the French MULTITAG corpus. This POS-tagged corpus provides a tagset richer than the usual ones, including gender and number distinctions, for example. Experimental results show significant differences of performance between the taggers. According to the tagging accuracy estimated with a tagset of 300 items, taggers may be ranked as follows: Treetagger (95.7{\%}), Brills tagger (94.6{\%}), HMM tagger (93.4{\%}). Examples of translation outputs illustrate how considering gender and number distinctions in the POS tagset can be relevant.
This paper describes the development of a ground truth dataset of culturally diverse Romanized names in which approximately 70,000 names are matched against a subset of 700. We ran the subset as queries against the complete list using several matchers, created adjudication pools, adjudicated the results, and compiled two versions of ground truth based on different sets of adjudication guidelines and methods for resolving adjudicator conflicts. The name list, drawn from publicly available sources, was manually seeded with over 1500 name variants. These names include transliteration variation, database fielding errors, segmentation differences, incomplete names, titles, initials, abbreviations, nicknames, typos, OCR errors, and truncated data. These diverse types of matches, along with the coincidental name similarities already in the list, make possible a comprehensive evaluation of name matching systems. We have used the dataset to evaluate several open source and commercial algorithms and provide some of those results.
In aiming at research and development on machine translation, we produced a test collection for Japanese-English machine translation in the seventh NTCIR Workshop. This paper describes details of our test collection. From patent documents published in Japan and the United States, we extracted patent families as a parallel corpus. A patent family is a set of patent documents for the same or related invention and these documents are usually filed to more than one country in different languages. In the parallel corpus, we aligned Japanese sentences with their counterpart English sentences. Our test collection, which includes approximately 2,000,000 sentence pairs, can be used to train and test machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval and the contribution of machine translation to a patent retrieval task can also be evaluated. Our test collection will be available to the public for research purposes after the NTCIR final meeting.
Recently, most of the research in NLP has concentrated on the creation of applications coping with textual entailment. However, there still exist very few resources for the evaluation of such applications. We argue that the reason for this resides not only in the novelty of the research field but also and mainly in the difficulty of defining the linguistic phenomena which are responsible for inference. As the TSNLP project has shown test suites provide optimal diagnostic and evaluation tools for NLP applications, as contrary to text corpora they provide a deep insight in the linguistic phenomena allowing control over the data. Thus in this paper, we present a test suite specifically developed for studying inference problems shown by English adjectives. The construction of the test suite is based on the deep linguistic analysis and following classification of entailment patterns of adjectives and follows the TSNLP guidelines on linguistic databases providing a clear coverage, systematic annotation of inference tasks, large reusability and simple maintenance. With the design of this test suite we aim at creating a resource supporting the evaluation of computational systems handling natural language inference and in particular at providing a benchmark against which to evaluate and compare existing semantic analysers.
Recently, speech recognition performance has been drastically improved by statistical methods and huge speech databases. Now performance improvement under such realistic environments as noisy conditions is being focused on. Since October 2001, we from the working group of the Information Processing Society in Japan have been working on evaluation methodologies and frameworks for Japanese noisy speech recognition. We have released frameworks including databases and evaluation tools called CENSREC-1 (Corpus and Environment for Noisy Speech RECognition 1; formerly AURORA-2J), CENSREC-2 (in-car connected digits recognition), CENSREC-3 (in-car isolated word recognition), and CENSREC-1-C (voice activity detection under noisy conditions). In this paper, we newly introduce a collection of databases and evaluation tools named CENSREC-4, which is an evaluation framework for distant-talking speech under hands-free conditions. Distant-talking speech recognition is crucial for a hands-free speech interface. Therefore, we measured room impulse responses to investigate reverberant speech recognition. The results of evaluation experiments proved that CENSREC-4 is an effective database suitable for evaluating the new dereverberation method because the traditional dereverberation process had difficulty sufficiently improving the recognition performance. The framework was released in March 2008, and many studies are being conducted with it in Japan.
This paper describes the evaluation methodology used to evaluate the TC-STAR speech-to-speech translation (SST) system and the results from the third year of the project. It follows the results presented in Hamon (2007), dealing with the first end-to-end evaluation of the project. In this paper, we try to experiment with the methodology and the protocol during a second end-to-end evaluation, by comparing outputs from the TC-STAR system with interpreters from the European parliament. For this purpose, we test different criteria of evaluation and type of questions within a comprehension test. The results show that interpreters do not translate all the information (as opposed to the automatic system), but the quality of SST is still far from that of human translation. The experimental comprehension test used provides new information to study the quality of automatic systems, but without settling the issue of which protocol is the best. This depends on what the evaluator wants to know about the SST: either to have a subjective end-user evaluation or a more objective one.
In dialogue systems, it is necessary to decode the user input into semantically meaningful units. These semantical units, usually Dialogue Acts (DA), are used by the system to produce the most appropriate response. The user turns can be segmented into utterances, which are meaningful segments from the dialogue viewpoint. In this case, a single DA is associated to each utterance. Many previous works have used DA assignation models on segmented dialogue corpora, but only a few have tried to perform the segmentation and assignation at the same time. The knowledge of the segmentation of turns into utterances is not common in dialogue corpora, and knowing the quality of the segmentations provided by the models that simultaneously perform segmentation and assignation would be interesting. In this work, we evaluate the accuracy of the segmentation offered by this type of model. The evaluation is done on a Spanish dialogue system on a railway information task. The results reveal that one of these techniques provides a high quality segmentation for this corpus.
In this paper, we present a comparison between two corpora acquired by means of two different techniques. The first corpus was acquired by means of the Wizard of Oz technique. A dialog simulation technique has been developed for the acquisition of the second corpus. A random selection of the user and system turns has been used, defining stop conditions for automatically deciding if the simulated dialog is successful or not. We use several evaluation measures proposed in previous research to compare between our two acquired corpora, and then discuss the similarities and differences between the two corpora with regard to these measures.
Embodied Conversational Agents have typically been constructed for use in limited domain applications, and tested in very specialized environments. Only in recent years have there been more cases of moving agents into wider public applications (e.g.Bell et al., 2003; Kopp et al., 2005). Yet little analysis has been done to determine the differing needs, expectations, and behavior of human users in these environments. With an increasing trend for virtual characters to go public, we need to expand our understanding of what this entails for the design and capabilities of our characters. This paper explores these issues through an analysis of a corpus that has been collected since December 2006, from interactions with the virtual character Sgt Blackwell at the Cooper Hewitt Museum in New York. The analysis includes 82 hierarchical categories of user utterances, as well as specific observations on user preferences and behaviors drawn from interactions with Blackwell.
The RITEL project aims to integrate a spoken language dialogue system and an open-domain information retrieval system in order to enable human users to ask a general question and to refine their search for information interactively. This type of system is often referred to as an Interactive Question Answering (IQA) system. In this paper, we present an evaluation of how the performance of the RITEL system differs when users interact with it using spoken versus textual input and output. Our results indicate that while users do not perceive the two versions to perform significantly differently, many more questions are asked in a typical text-based dialogue.
We outline a methodological classification for evaluation approaches of software in general. This classification was initiated partly owing to involvement in a biennial European competition (the European Academic Software Award, EASA) which was held for over a decade. The evaluation grid used in EASA gradually became obsolete and inappropriate in recent years, and therefore needed to be revised. In order to do this, it was important to situate the competition in relation to other software evaluation procedures. A methodological perspective for the classification is adopted rather than a conceptual one, since a number of difficulties arise with the latter. We focus on three main questions: What to evaluate? How to evaluate? and Who does evaluate? The classification is therefore hybrid: it allows one to account for the most common evaluation approaches and is also an observatory. Two main approaches are differentiated: system and usage. We conclude that any evaluation always constructs its own object, and the objects to be evaluated only partially determine the evaluation which can be applied to them. Generally speaking, this allows one to begin apprehending what type of knowledge is objectified when one or another approach is chosen.
We present the procedures we implemented to carry out system oriented evaluation of a syntax-based word aligner, ALIBI. While cross-corpus evaluation is still relatively rare in NLP, we take the approach of regarding cross-corpus evaluation as part of system oriented evaluation. Our hypothesis is that the granularity of alignments and the level of syntactic correspondence depend on corpus type; our objective is to assess how this impacts on alignment quality. We test our system on three English-French parallel corpora. The evaluation procedures are defined in accordance with state-of-the-art word alignment evaluation principles. They include, for each corpus, the creation of a reference set containing multiple annotations of the same data, the assessment of inter-annotator agreement rates and an analysis of the reference set obtained. We show that alignment performance varies across corpora according to the multiple reference annotations produced and further motivate our choice of preserving all reference annotations without solving disagreements between annotators.
In this paper, we discuss methods of measuring the performance of ontology-based information extraction systems. We focus particularly on the Balanced Distance Metric (BDM), a new metric we have proposed which aims to take into account the more flexible nature of ontologically-based applications. We first examine why traditional Precision and Recall metrics, as used for flat information extraction tasks, are inadequate when dealing with ontologies. We then describe the Balanced Distance Metric (BDM) which takes ontological similarity into account. Finally, we discuss a range of experiments designed to test the accuracy and usefulness of the BDM when compared with traditional metrics and with a standard distance-based metric.
Some alternatives to the standard evalb measures for parser evaluation are considered, principally the use of a tree-distance measure, which assigns a score to a linearity and ancestry respecting mapping between trees, in contrast to the evalb measures, which assign a score to a span preserving mapping. Additionally, analysis of the evalb measures suggests some further variants, concerning different normalisations, the portions of a tree compared and whether scores should be micro or macro averaged. The outputs of 6 parsing systems on Section 23 of the Penn Treebank were taken. It is shown that the ranking of the parsing systems varies as the alternative evaluation measures are used. For a fixed parsing system, it is also shown that the ranking of the parses from best to worst will vary according to whether the evalb or tree-distance measure is used. It is argued that the tree-distance measure ameliorates a problem that has been noted concerning over-penalisation of attachment errors.
We present a tool, BLEU+, which implements various extension to BLEU computation to allow for a better understanding of the translation performance, especially for morphologically complex languages. BLEU+ takes into account both closeness in morphological structure, closeness of the root words in the WordNet hierarchy while comparing tokens in the candidate and reference sentence. In addition to gauging performance at a finer level of granularity, BLEU+ also allows the computation of various upper bound oracle scores: comparing all tokens considering only the roots allows us to get an upper bound when all errors due to morphological structure are fixed, while comparing tokens in an error-tolerant way considering minor morpheme edit operations, allows us to get a (more realistic) upper bound when tokens that differ in morpheme insertions/deletions and substitutions are fixed. We use BLEU+ in the fine-grained evaluation of the output of our English-to-Turkish statistical MT system.
This paper discusses development and evaluation of a practical, valid and reliable instrument for evaluating the spoken language abilities of second-language (L2) learners of English. First we sketch the theory and history behind elicited imitation (EI) tests and the renewed interest in them. Then we present how we developed a new test based on various language resources, and administered it to a few hundred students of varying levels. The students were also scored using standard evaluation techniques, and the EI results were compared to more traditionally derived scores. We also sketch how we developed a new integrated tool that allows the session recordings of the EI data to be analyzed with a widely-used automatic speech recognition (ASR) engine. We discuss the promising results of the ASR engines processing of these files and how they correlated with human scoring of the same items. We indicate how the integrated tool will be used in the future. Further development plans and prospects for follow-on work round out the discussion.
In this paper we present a usability measure adapted to mobile services, which is based on the well-known theoretical framework defined in the ISO 9241-11 standard. This measure is then applied to a representative set of services of the Telef{\'o}nicas portfolio for residential customers. The user tests that we present were carried out by a total of 327 people. Additionally, we describe the detailed application of the methodology to a particular service and present the results of all the experiments that were carried out with the different services. These results show highly significant differences in the three usability measures considered (effectiveness, efficiency and satisfaction), though all of them have the same trend. The worst performers in all cases were the WAP and i-mode user interfaces (UI), while the best performers were the SMS and web based UIs closely followed by the voice UI. Finally, we also analyse the results and present our conclusions.
This paper analyses some general issues about human language technology evaluation, focusing on economic aspects. It first provides a scientific rationale for the need to organize evaluation in the form of campaigns, by relating this need to some basic characteristics of human language technologies, namely that they involve learning to process information in a way which reproduces human capabilities. It then reviews the benefits and constraints of these evaluation campaigns. Borrowing concepts from the field of economics, it also provides an analysis of the economic incentives to organize evaluation campaigns. It entails from this analysis that fitting evaluation campaigns to the needs of scientific research requires a strong implication in term of research policy and public funding.
The influence of English as a global language continues to grow to an extent that its words and expressions permeate the original forms of other languages. This paper evaluates a modular Web-based sub-component of an existing English inclusion classifier and compares it to a corpus-based lookup technique. Both approaches are evaluated on a German gold standard data set. It is demonstrated to what extent the Web-based approach benefits from the amount of data available online and the fact that this data is constantly updated.
This paper investigates a new evaluation method for assessing the coherence of computer-aided summaries, justified by the inappropriacy of existing evaluation methods for this task. It develops a metric for Centering Theory (CT), a theory of local coherence and salience, to measure coherence in pairs of extracts and abstracts produced in a computer-aided summarisation environment. 100 news text summaries (50 pairs of extracts and their corresponding abstracts) are analysed using CT and the metric is applied to obtain a score for each summary; the summary with the higher score out of a pair is considered more coherent. Human judgement is also obtained to allow a comparison with the CT evaluation to assess the validity of the development of CT as a useful evaluation metric in computer-aided summarisation.
The NIST Automatic Content Extraction (ACE) Evaluation expands its focus in 2008 to encompass the challenge of cross-document and cross-language global integration and reconciliation of information. While past ACE evaluations have been limited to local (within-document) detection and disambiguation of entities, relations and events, the current evaluation adds global (cross-document and cross-language) entity disambiguation tasks for Arabic and English. This paper presents the 2008 ACE XDoc evaluation task and associated infrastructure. We describe the linguistic resources created by LDC to support the evaluation, focusing on new approaches required for data selection, data processing, annotation task definitions and annotation software, and we conclude with a discussion of the metrics developed by NIST to support the evaluation.
Whats the best way to assess the performance of a semantic component in an NLP system? Tradition in NLP evaluation tells us that comparing output against a gold standard is a good idea. To define a gold standard, one first needs to decide on the representation language, and in many cases a first-order language seems a good compromise between expressive power and efficiency. Secondly, one needs to decide how to represent the various semantic phenomena, in particular the depth of analysis of quantification, plurals, eventualities, thematic roles, scope, anaphora, presupposition, ellipsis, comparatives, superlatives, tense, aspect, and time-expressions. Hence it will be hard to come up with an annotation scheme unless one permits different level of semantic granularity. The alternative is a theory-neutral black-box type evaluation where we just look at how systems react on various inputs. For this approach, we can consider the well-known task of recognising textual entailment, or the lesser-known task of textual model checking. The disadvantage of black-box methods is that it is difficult to come up with natural data that cover specific semantic phenomena.
Evaluating the output of NLG systems is notoriously difficult, and performing assessments of text quality even more so. A range of automated and subject-based approaches to the evaluation of text quality have been taken, including comparison with a putative gold standard text, analysis of specific linguistic features of the output, expert review and task-based evaluation. In this paper we present the results of a variety of such approaches in the context of a case study application. We discuss the problems encountered in the implementation of each approach in the context of the literature, and propose that a test based on the Turing test for machine intelligence offers a way forward in the evaluation of the subjective notion of text quality.
This paper describes a Name Matching Evaluation Laboratory that is a joint effort across multiple projects. The lab houses our evaluation infrastructure as well as multiple name matching engines and customized analytical tools. Included is an explanation of the methodology used by the lab to carry out evaluations. This methodology is based on standard information retrieval evaluation, which requires a carefully-constructed test data set. The paper describes how we created that test data set, including the ground truth used to score the systems performance. Descriptions and snapshots of the labs various tools are provided, as well as information on how the different tools are used throughout the evaluation process. By using this evaluation process, the lab has been able to identify strengths and weaknesses of different name matching engines. These findings have led the lab to an ongoing investigation into various techniques for combining results from multiple name matching engines to achieve optimal results, as well as into research on the more general problem of identity management and resolution.
This paper presents the sequential evaluation of the question answering system SQuaLIA. This system is based on the same sequential process as most statistical question answering systems, involving 4 main steps from question analysis to answer extraction.The evaluation is based on a corpus made from 20 questions taken in the set of an evaluation campaign and which were well answered by SQuaLIA. Each of the 20 questions has been typed by 17 native participants, non natives and dyslexics. They were vocally instructed the target of each question. Each of the 4 analysis steps of the system involves a loss of accuracy, until an average of 60 of right answers at the end of the process. The main cause of this loss seems to be the orthographic mistakes users make on nouns.
The field of automated sentiment analysis has emerged in recent years as an exciting challenge to the computational linguistics community. Research in the field investigates how emotion, bias, mood or affect is expressed in language and how this can be recognised and represented automatically. To date, the most successful applications have been in the classification of product reviews and editorials. This paper aims to open a discussion about alternative evaluation methodologies for sentiment analysis systems that broadens the scope of this new field to encompass existing work in other domains such as psychology and to exploit existing resources in diverse domains such as finance or medicine. We outline some interesting avenues for research which investigate the impact of affective text content on the human psyche and on external factors such as stock markets.
We present in this article the methods we used for obtaining measures to ensure the quality and well-formedness of a text corpus. These measures allow us to determine the compatibility of a corpus with the treatments we want to apply on it. We called this method certification of corpus. These measures are based upon the characteristics required by the linguistic treatments we have to apply on the corpus we want to certify. Since the certification of corpus allows us to highlight the errors present in a text, we developed modules to carry out an automatic correction. By applying these modules, we reduced the number of errors. In consequence, it increases the quality of the corpus making it possible to use a corpus that a first certification would not have admitted.
In this article, we propose a web based listening test system that can be used with a large range of listeners. Our main goals were to make the configuration of the tests as simple and flexible as possible, to simplify the recruiting of the testees and, of course, to keep track of the results using a relational database. This first version of our system can perform the most widely used listening tests in the speech processing community (AB-BA, ABX and MOS tests). It can also easily evolve and propose other tests implemented by the tester by means of a module interface. This scenario is explored in this article which proposes an implementation of a module for Comparison Mean Opinion Score (CMOS) tests and conduct of such an experiment. This test allowed us to extract from the BREF120 corpus a couple of voices of distinct supra-segmental characteristics. This system is offered to the speech synthesis and speech conversion community under free license.
The objective of the Semiotic-based Ontology Evaluation Tool (S-OntoEval) is to evaluate and propose improvements to a given ontological model. The evaluation aims at assessing the quality of the ontology by drawing upon semiotic theory, taking several metrics into consideration for assessing the syntactic, semantic, and pragmatic aspects of ontology quality. We consider an ontology to be a semiotic object and we identify three main types of semiotic ontology evaluation levels: the structural level, assessing the ontology syntax and formal semantics; the functional level, assessing the ontology cognitive semantics and; the usability-related level, assessing the ontology pragmatics. The Ontology Evaluation Tool implements metrics for each semiotic ontology level: on the structural level by making use of reasoner such as the RACER System and Pellet to check the logical consistency of our ontological model (TBoxes and ABoxes) and graph-theory measures such as Depth; on the functional level by making use of a task-based evaluation approach which measures the quality of the ontology based on the adequacy of the ontological model for a specific task; and on the usability-profiling level by applying a quantitative analysis of the amount of annotation. Other metrics can be easily integrated and added to the respective evaluation level. In this work, the Ontology Evaluation Tool is used to test and evaluate the SWIntO Ontology of the SmartWeb project.
In this paper we describe ANNALIST (Annotation, Alignment and Scoring Tool), a scoring system for the evaluation of the output of semantic annotation systems. ANNALIST has been designed as a system that is easily extensible and configurable for different domains, data formats, and evaluation tasks. The system architecture enables data input via the use of plugins and the users can access the systems internal alignment and scoring mechanisms without the need to convert their data to a specified format. Although developed for evaluation tasks that involve the scoring of entity mentions and relations primarily, ANNALISTs generic object representation and the availability of a range of criteria for the comparison of annotations enable the system to be tailored to a variety of scoring jobs. The paper reports on results from using ANNALIST in real-world situations in comparison to other scorers which are more established in the literature. ANNALIST has been used extensively for evaluation tasks within the VIKEF (EU FP6) and CLEF (UK MRC) projects.
This paper presents recent results of the application of the task-based Browser Evaluation Test (BET) to meeting browsers, that is, interfaces to multimodal databases of meeting recordings. The tasks were defined by browser-neutral BET observers. Two groups of human subjects used the Transcript-based Query and Browsing interface (TQB), and attempted to solve as many BET tasks - pairs of true/false statements to disambiguate - as possible in a fixed amount of time. Their performance was measured in terms of precision and speed. Results indicate that the browsers annotation-based search functionality is frequently used, in particular the keyword search. A more detailed analysis of each test question for each participant confirms that despite considerable variation across strategies, the use of queries is correlated to successful performance.
The Framework for the Evaluation for Machine Translation (FEMTI) contains guidelines for building a quality model that is used to evaluate MT systems in relation to the purpose and intended context of use of the systems. Contextual quality models can thus be constructed, but entering into FEMTI the knowledge required for this operation is a complex task. An experiment has been set up in order to transfer knowledge from MT evaluation experts into the FEMTI guidelines, by polling experts about the evaluation methods they would use in a particular context, then inferring from the results generic relations between characteristics of the context of use and quality characteristics. The results of this hands-on exercise, carried out as part of a conference tutorial, have served to refine FEMTIs generic contextual quality model and to obtain feedback on the FEMTI guidelines in general.
One of the most challenging tasks for uniformed service personnel serving in foreign countries is effective verbal communication with the local population. To remedy this problem, several companies and academic institutions have been funded to develop machine translation systems as part of the DARPA TRANSTAC (Spoken Language Communication and Translation System for Tactical Use) program. The goal of this program is to demonstrate capabilities to rapidly develop and field free-form, two-way translation systems that would enable speakers of different languages to communicate with one another in real-world tactical situations. DARPA has mandated that each TRANSTAC technology be evaluated numerous times throughout the life of the program and has tasked the National Institute of Standards and Technology (NIST) to lead this effort. This paper describes the experimental design methodology and test procedures from the most recent evaluation, conducted in July 2007, which focused on English to/from Iraqi Arabic.
Evaluation of machine translation (MT) output is a challenging task. In most cases, there is no single correct translation. In the extreme case, two translations of the same input can have completely different words and sentence structure while still both being perfectly valid. Large projects and competitions for MT research raised the need for reliable and efficient evaluation of MT systems. For the funding side, the obvious motivation is to measure performance and progress of research. This often results in a specific measure or metric taken as primarily evaluation criterion. Do improvements in one measure really lead to improved MT performance? How does a gain in one evaluation metric affect other measures? This paper is going to answer these questions by a number of experiments.
Nowadays, there are hundreds of Natural Language Processing applications and resources for different languages that are developed and/or used, almost exclusively with a few but notable exceptions, by their creators. Assuming that the right to use a particular application or resource is licensed by the rightful owner, the user is faced with the often not so easy task of interfacing it with his/her own systems. Even if standards are defined that provide a unified way of encoding resources, few are the cases when the resources are actually coded in conformance to the standard (and, at present time, there is no such thing as general NLP application interoperability). Semantic Web came with the promise that the web will be a universal medium for information exchange whatever its content. In this context, the present article outlines a collection of linguistic web services for Romanian and English, developed at the Research Institute for AI for the Romanian Academy (RACAI) which are ready to provide a standardized way of calling particular NLP operations and extract the results without caring about what exactly is going on in the background.
In this paper two highly innovative digital editions will be presented. For the creation and the implementation of these editions the latest developments within corpus research have been taken into account. The digital editions of the historical literary journals Die Fackel (published by Karl Kraus in Vienna from 1899 to 1936) and Der Brenner (published by Ludwig Ficker in Innsbruck from 1910 to 1954) have been developed within the corpus research framework of the AAC - Austrian Academy Corpus at the Austrian Academy of Sciences in collaboration with other researchers and programmers in the AAC from Vienna together with the graphic designer Anne Burdick from Los Angeles. For the creation of these scholarly digital editions the AAC edition philosophy and edition principles have been applied whereby new corpus research methods have been made use of for questions of computational philology and textual studies in a digital environment. The examples of the digital online editions of the literary journals Die Fackel and Der Brenner will give insights into the potentials and the benefits of making corpus research methods and techniques available for scholarly research into language and literature.
ASV Toolbox is a modular collection of tools for the exploration of written language data both for scientific and educational purposes. It includes modules that operate on word lists or texts and allow to perform various linguistic annotation, classification and clustering tasks, including language detection, POS-tagging, base form reduction, named entity recognition, and terminology extraction. On a more abstract level, the algorithms deal with various kinds of word similarity, using pattern-based and statistical approaches. The collection can be used to work on large real-world data sets as well as for studying the underlying algorithms. Each module of the ASV Toolbox is designed to work either on a plain text files or with a connection to a MySQL database. While it is especially designed to work with corpora of the Leipzig Corpora Collection, it can easily be adapted to other sources.
In the present paper we report on the development of a cluster of web services of language technology for Portuguese that we named as LXService. These web services permit the direct interaction of client applications with language processing tools via the Internet. This way of making available language technology was motivated by the need of its integration in an eLearning environment. In particular, it was motivated by the development of new multilingual functionalities that were aimed at extending a Learning Management System and that needed to resort to the outcome of some of those tools in a distributed and remote fashion. This specific usage situation happens however to be representative of a typical and recurrent set up in the utilization of language processing tools in different settings and projects. Therefore, the approach reported here offers not only a solution for this specific problem, which immediately motivated it, but contributes also some first steps for what we see as an important paradigm shift in terms of the way language technology can be distributed and find a better way to unleash its full potential and impact.
We present TextPro, a suite of modular Natural Language Processing (NLP) tools for analysis of Italian and English texts. The suite has been designed so as to integrate and reuse state of the art NLP components developed by researchers at FBK. The current version of the tool suite provides functions ranging from tokenization to chunking and Named Entity Recognition (NER). The systems architecture is organized as a pipeline of processors wherein each stage accepts data from an initial input or from an output of a previous stage, executes a specific task, and sends the resulting data to the next stage, or to the output of the pipeline. TextPro performed the best on the task of Italian NER and Italian PoS Tagging at EVALITA 2007. When tested on a number of other standard English benchmarks, TextPro confirms that it performs as state of the art system. Distributions for Linux, Solaris and Windows are available, for both research and commercial purposes. A web-service version of the system is under development.
This paper describes experimental use of the multi-agent architecture to integrate Natural Language and Information Systems research and teaching, by casting a group of students as intelligent agents to collect and analyse English language resources from around the world. Section 2 and section 3 describe the hybrid intelligent information systems experiments at the University of Leeds and the results generated, including several research papers accepted at international conferences, and a finalist entry in the British Computer Society Machine Intelligence contest. Our proposals for applying the multi-agent idea in other universities such as the Arab Open University are presented in section 4. The conclusion is presented in section 5: the success of hybrid intelligent information systems experiments in generating research papers within a limited time.
Language resources and tools to create and process these resources are necessary components in human language technology and natural language applications. In this paper, we describe a survey of existing language resources for Swedish, and the need for Swedish language resources to be used in research and real-world applications in language technology as well as in linguistic research. The survey is based on a questionnaire sent to industry and academia, institutions and organizations, and to experts involved in the development of Swedish language resources in Sweden, the Nordic countries and world-wide.
We describe a web-based corpus query system, Glossa, which combines the expressiveness of regular query languages with the user-friendliness of a graphical interface. Since corpus users are usually linguists with little interest in technical matters, we have developed a system where the user need not have any prior knowledge of the search system. Furthermore, no previous knowledge of abbreviations for metavariables such as part of speech and source text is needed. All searches are done using checkboxes, pull-down menus, or writing simple letters to make words or other strings. Querying for more than one word is simply done by adding an additional query box, and for parts of words by choosing a feature such as start of word. The Glossa system also allows a wide range of viewing and post-processing options. Collocations can be viewed and counted in a number of ways, and be viewed as different kinds of graphical charts. Further annotation and deletion of single results for further processing is also easy. The Glossa system is already in use for a number of corpora. Corpus administrators can easily adapt the system to a wide range of corpora, including multilingual corpora and corpora with audio and video content.
The high level of heterogeneity between linguistic annotations usually complicates the interoperability of processing modules within an NLP pipeline. In this paper, a framework for the interoperation of NLP components, based on a data-driven architecture, is presented. Here, ontologies of linguistic annotation are employed to provide a conceptual basis for the tagset-neutral processing of linguistic annotations. The framework proposed here is based on a set of structured OWL ontologies: a reference ontology, a set of annotation models which formalize different annotation schemes, and a declarative linking between these, specified separately. This modular architecture is particularly scalable and flexible as it allows for the integration of different reference ontologies of linguistic annotations in order to overcome the absence of a consensus for an ontology of linguistic terminology. Our proposal originates from three lines of research from different fields: research on annotation type systems in UIMA; the ontological architecture OLiA, originally developed for sustainable documentation and annotation-independent corpus browsing, and the ontologies of the OntoTag model, targeted towards the processing of linguistic annotations in Semantic Web applications. We describe how UIMA annotations can be backed up by ontological specifications of annotation schemes as in the OLiA model, and how these are linked to the OntoTag ontologies, which allow for further ontological processing.
Within the CLARIN e-science infrastructure project it is foreseen to develop a component-based registry for metadata for Language Resources and Language Technology. With this registry it is hoped to overcome the problems of the current available systems with respect to inflexible fixed schema, unsuitable terminology and interoperability problems. The registry will address interoperability needs by refering to a shared vocabulary registered in data category registries as they are suggested by ISO.
The DAM-LR project aims at virtually integrating various European language resource archives that allow users to navigate and operate in a single unified domain of language resources. This type of integration introduces Grid technology to the humanities disciplines and forms a federation of archives. The complete architecture is designed based on a few well-known components .This is considered the basis for building a research infrastructure for Language Resources as is planned within the CLARIN project. The DAM-LR project was purposefully started with only a small number of participants for flexibility and to avoid complex contract negotiations with respect to legal issues. Now that we have gained insights into the basic technology issues and organizational issues, it is foreseen that the federation will be expanded considerably within the CLARIN project that will also address the associated legal issues.
About two years ago, the Max Planck Institute for Psycholinguistics in Nijmegen, The Netherlands, started an initiative to install regional language archives in various places around the world, particularly in places where a large number of endangered languages exist and are being documented. These digital archives make use of the LAT archiving framework that the MPI has developed over the past nine years. This framework consists of a number of web-based tools for depositing, organizing and utilizing linguistic resources in a digital archive. The regional archives are in principle autonomous archives, but they can decide to share metadata descriptions and language resources with the MPI archive in Nijmegen and become part of a grid of linked LAT archives. By doing so, they will also take advantage of the long-term preservation strategy of the MPI archive. This paper describes the reasoning behind this initiative and how in practice such an archive is set up.
Corpus-based approaches and statistical approaches have been the main stream of natural language processing research for the past two decades. Language resources play a key role in such approaches, but there is an insufficient amount of language resources in many Asian languages. In this situation, standardisation of language resources would be of great help in developing resources in new languages. This paper presents the latest development efforts of our project which aims at creating a common standard for Asian language resources that is compatible with an international standard. In particular, the paper focuses on i) lexical specification and data categories relevant for building multilingual lexical resources for Asian languages; ii) a core upper-layer ontology needed for ensuring multilingual interoperability and iii) the evaluation platform used to test the entire architectural framework.
In recent years, language resources acquired from theWeb are released, and these data improve the performance of applications in several NLP tasks. Although the language resources based on the web page unit are useful in NLP tasks and applications such as knowledge acquisition, document retrieval and document summarization, such language resources are not released so far. In this paper, we propose a data format for results of web page processing, and a search engine infrastructure which makes it possible to share approximately 100 million Japanese web data. By obtaining the web data, NLP researchers are enabled to begin their own processing immediately without analyzing web pages by themselves.
In this paper we address the issue of developing an interoperable infrastructure for language resources and technologies. In our approach, called UFRA, we extend the Federate Database Architecture System adding typical functionalities caming from UIMA. In this way, we capitalize the advantages of a federated architecture, such as autonomy, heterogeneity and distribution of components, monitored by a central authority responsible for checking both the integration of components and user rights on performing different tasks. We use the UIMA approach to manage and define one common front-end, enabling users and clients to query, retrieve and use language resources and technologies. The purpose of this paper is to show how UIMA leads from a Federated Database Architecture to a Federated Resource Architecture, adding to a registry of available components both static resources such as lexicons and corpora and dynamic ones such as tools and general purpose language technologies. At the end of the paper, we present a case-study that adopts this framework to integrate the SIMPLE lexicon and TIMEML annotation guidelines to tag natural language texts.
Our goal is to provide a web-based platform for the long-term preservation and distribution of a heterogeneous collection of linguistic resources. We discuss the corpus preprocessing and normalisation phase that results in sets of multi-rooted trees. At the same time we transform the original metadata records, just like the corpora annotated using different annotation approaches and exhibiting different levels of granularity, into the all-encompassing and highly flexible format eTEI for which we present editing and parsing tools. We also discuss the architecture of the sustainability platform. Its primary components are an XML database that contains corpus and metadata files and an SQL database that contains user accounts and access control lists. A staging area, whose structure, contents, and consistency can be checked using tools, is used to make sure that new resources about to be imported into the platform have the correct structure.
We describe the process of converting plain text cultural heritage data to elements of a domain-specific knowledge base, using general machine learning techniques. First, digitised expedition field notes are segmented and labelled automatically. In order to obtain perfect records, we create an annotation tool that features selective sampling, allowing domain experts to validate automatically labelled text, which is then stored in a database. Next, the records are enriched with semi-automatically derived secondary metadata. Metadata enable fine-grained querying, the results of which are additionally visualised using maps and photos.
The National Institute of Information and Communications Technology (NICT) and Nagoya University have been jointly constructing a large scale database named SHACHI by collecting detailed meta-information on language resources (LRs) in Asia and Western countries, for the purpose of effectively combining LRs. The purpose of this project is to investigate languages, tag sets, and formats compiled in LRs throughout the world, to systematically store LR metadata, to create a search function for this information, and to ultimately utilize all this for a more efficient development of LRs. This metadata database contains more than 2,000 compiled LRs such as corpora, dictionaries, thesauruses and lexicons, forming a large scale metadata of LRs archive. Its metadata, an extended version of OLAC metadata set conforming to Dublin Core, which contain detailed meta-information, have been collected semi-automatically. This paper explains the design and the structure of the metadata database, as well as the realization of the catalogue search tool. Additionally, the website of this database is now open to the public and accessible to all Internet users.
Metadata registries comprising sets of categories to be used in data collections exist in many fields. The purpose of a metadata registry is to facilitate data exchange and interoperability within a domain, and registries often contain definitions and examples. In this paper we will argue that in order to ensure completeness, consistency, user-friendliness and extensibility, metadata registries should be structured as taxonomies. Furthermore we will illustrate the usefulness of using terminological ontologies as the basis for developing metadata taxonomies. In this connection we will discuss the principles of developing ontologies and the differences between taxonomies and ontologies. The paper includes examples of initiatives for developing metadata standards within the field of language resources, more specifically lexical data categories, elaborated at international and national level. However, the principles that we introduce for the development of data category registries are relevant not only for metadata registries for lexical resources, but for all kinds of metadata registries.
The purpose of Oriental COCOSDA is to provide the Asian community a platform to exchange ideas, to share information and to discuss regional matters on creation, utilization, dissemination of spoken language corpora of oriental languages and also on the assessment methods of speech recognition/synthesis systems as well as to promote speech research on oriental languages. Since its preparatory meeting in Hong Kong in 1997, annual workshops have been organized and held in Japan, Taiwan, China, Korea, Thailand, Singapore, India, Indonesia, Malaysia, and Vietnam from 1998 onwards. The organization is managed by a convener, three advisory members, and 26 committee members from 13 regions in Oriental area. In order to commemorate 10 years of continued activities, the members have decided to publish a book which covers a wide range of speech research. Special focus will be on speech resources or speech corpora in Oriental countries and standardization of speech input/output systems performance evaluation methods on which key technologies for speech systems development are based. The book will also include linguistic outlines of oriental languages, annotation, labeling, and software tools for speech processing.
This paper presents a new corpus project, aiming at building a national corpus of Polish. What makes it different from a typical YACP (Yet Another Corpus Project) is 1) the fact that all four partners in the project have in the past constructed corpora of Polish, sometimes in the spirit of collaboration, at other times - in the spirit of competition, 2) the partners bring into the project varying areas of expertise and experience, so the synergy effect is anticipated, 3) the corpus will be built with an eye on specific applications in various fields, including lexicography (the corpus will be the empirical basis of a new large general dictionary of Polish) and natural language processing (a number of NLP tools will be constructed within the project).
The paper will give an overview of developments in Estonia in the field of Human Language Technologies. Despite of the fact that Estonian is one of the smallest official languages in EU and therefore in less favourable position in the HLT-market, the national initiatives are undertaken in order to promote HLT development in Estonia. The paper will introduce recent activities in Estonia, including National Programme for Estonian Language Technology (2006-2010).
After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators? conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.
The paper describes the project whose main purpose is the creation of the Slovene terminology web portal, funded by the Slovene Research Agency and the Amebis software company. It focuses on the DTD/schema used for the unification of different terminology resources in different input formats into one database available on the web. Two projects involving unification DTD/schemas were taken as the model for the resulting DTD/schema: the CONCEDE project and the TMF project. The final DTD/schema was tested on twenty different specialized dictionaries, both monolingual and bilingual, in various formats either without any existing markup or with complex XML structure. The result of the project will be an on-line terminology resource for Slovenian which will also include didactic material on terminology and free tools for uploading domain-specific text collections to be processed with NLP software, including a term extractor.
Semantic roles have often proved to be useful labels for stating linguistic generalisations of various sorts. There is, however, a lack of agreement on their defining criteria, which causes serious problems for semantic roles to be a useful classificatory device for predicate-argument relations. These criteria should (a) support the design of a semantic role set which is complete but does not contain redundant relations; (b) be based on semantic rather than morphological, lexical or syntactic properties; and (c) enable formal interpretation. In this paper we report on the analyses of alternative approaches to annotation and representation of semantic role information (such as FrameNet, PropBank and VerbNet) with respect to their models of description, granularity of semantic role sets, definitions of semantic roles concepts, consistency and reliability of annotations. We present methodological principles for characterising well-defined concepts which were developed within the LIRICS (Linguistic InfRastructure for Interoperable ResourCes and Systems; see http://lirics.loria.fr) project, as well as the designed set of semantic roles and their definitions in ISO 12620 format. We discuss evaluation results of the defined concepts for semantic role annotation concerning the redundancy and completeness of the tagset and the reliability of annotations in terms of inter-annotator agreement.
Part-of-speech or morphological tags are important means of annotation in a vast number of corpora. However, different sets of tags are used in different corpora, even for the same language. Tagset conversion is difficult, and solutions tend to be tailored to a particular pair of tagsets. We propose a universal approach that makes the conversion tools reusable. We also provide an indirect evaluation in the context of a parsing task.
Times have changed over the last ten years in terms of dictionary production. With the introduction of digital support and networking, the lifespan of dictionaries has been considerably extended. The dictionary manuscript has become a unique data-source that can be reused and manipulated many times by numerous in-house and external experts. The traditional relationship between author, publisher and user has now been extended to include other partners: data-providers - either other publishers or institutions or industry-partners - , software developers, language-tool providers, etc. All these dictionary experts need a basic common language to optimize their work flow and to be able to co-operate in developing new products while avoiding time-consuming and expensive data manipulations. In this paper we will first of all present the ISO standardization for Lexicography which takes these new market needs into account, and then go on to describe the new standard ISO 1951: -Presentation/Representation of entries in dictionaries- which was published in March 2007. In conclusion, we will outline the benefits of standardization for the dictionary publishing industry.
To achieve true interoperability for valuable linguistic resources different levels of variation need to be addressed. ISO Technical Committee 37, Terminology and other language and content resources, is developing a Data Category Registry. This registry will provide a reusable set of data categories. A new implementation, dubbed ISOcat, of the registry is currently under construction. This paper shortly describes the new data model for data categories that will be introduced in this implementation. It goes on with a sketch of the standardization process. Completed data categories can be reused by the community. This is done by either making a selection of data categories using the ISOcat web interface, or by other tools which interact with the ISOcat system using one of its various Application Programming Interfaces. Linguistic resources that use data categories from the registry should include persistent references, e.g. in the metadata or schemata of the resource, which point back to their origin. These data category references can then be used to determine if two or more resources share common semantics, thus providing a level of interoperability close to the source data and a promising layer for semantic alignment on higher levels.
The Dutch HLT agency for language and speech technology (known as TST-centrale) at the Institute for Dutch Lexicology is responsible for the maintenance, distribution and accessibility of (Dutch) digital language resources. In this paper we present a project which aims to standardise the format of a set of bilingual lexicons in order to make them available to potential users, to facilitate the exchange of data (among the resources and with other (monolingual) resources) and to enable reuse of these lexicons for NLP applications like machine translation and multilingual information retrieval. We pay special attention to the methods and tools we used and to some of the problematic issues we encountered during the conversion process. As these problems are mainly caused by the fact that the standard LMF model fails in representing the detailed semantic and pragmatic distinctions made in our bilingual data, we propose some modifications to the standard. In general, we think that a standard for lexicons should provide a model for bilingual lexicons that is able to represent all detailed and fine-grained translation information which is generally found in these types of lexicons.
This poster presents an ISO framework for the standardization of syntactic annotation (SynAF). The normative part SynAF is concerned with a metamodel for syntactic annotation that covers both dimensions of constituency and dependency, and propose thus a multi-layered annotation framework that allows the combined and interrelated annotation of language data along both lines of consideration. This standard is designed to be used in close conjuncion with the metamodel presented in the Linguistic Annotation Framework (LAF) and with ISO 12620, Terminology and other language resources - Data categories.
The project described in this paper is funded by the French Ministry of Research. It aims at providing producers of Language Resources, and HLT players in general, with a guide which offers technical, legal and strategic recommendations/guidelines for the reuse of their Language Resources. The guide is dedicated in particular to academic laboratories which produce Language Resources and may benefit from further advice to start development, but also to any HLT player who wishes to follow the best practices in this field. The guidelines focus on different steps of a Language Resources life, i.e. specifications, production, validation, distribution, and maintenance. This paper gives a brief overview of the guide, and describes a) technical formats, standards and best practices which correspond to the current state of the art, for different types of resources, whether written or spoken, at different steps of the production line, b) legal issues and models/templates which can be used for the dissemination of Language Resources as widely as possible, c) strategic issues, by offering a dissemination plan which takes into account all types of constraints faced by HLT community players.
This paper deals with a multilingual relational lexical database of proper name, Prolexbase, a free resource available on the CNRTL website. The Prolex model is based on two main concepts: firstly, a language independent pivot and, secondly, the prolexeme (the projection of the pivot onto particular language), that is a set of lemmas (names and derivatives). These two concepts model the variations of proper name: firstly, independent of language and, secondly, language dependent by morphology or knowledge. Variation processing is very important for NLP: the same proper name can be written in different instances, maybe in different parts of speech, and it can also be replaced by another one, a lexical anaphora (that reveals semantic link). The pivot represents different referents points of view, i.e. language independent variations of name. Pivots are linked by three semantic relations (quasi-synonymy, partitive relation and associative relation). The prolexeme is a set of variants (aliases), quasi-synonyms and morphosemantic derivatives. Prolexemes are linked to classifying contexts and reliability code.
This paper discusses ontologization of lexicon access functions in the context of a service-oriented language infrastructure, such as the Language Grid. In such a language infrastructure, an access function to a lexical resource, embodied as an atomic Web service, plays a crucially important role in composing a composite Web service tailored to a users specific requirement. To facilitate the composition process involving service discovery, planning and invocation, the language infrastructure should be ontology-based; hence the ontologization of a range of lexicon functions is highly required. In a service-oriented environment, lexical resources however can be classified from a service-oriented perspective rather than from a lexicographically motivated standard. Hence to address the issue of interoperability, the taxonomy for lexical resources should be ground to principled and shared lexicon ontology. To do this, we have ontologized the standardized lexicon modeling framework LMF, and utilized it as a foundation to stipulate the service-oriented lexicon taxonomy and the corresponding ontology for lexicon access functions. This paper also examines a possible solution to fill the gap between the ontological descriptions and the actual Web service API by adopting a W3C recommendation SAWSDL, with which Web service descriptions can be linked with the domain ontology.
This paper presents two lexical data bases for Romanian: RoMorphoDict, a dictionary of inflected forms and RoSyllabiDict, a dictionary of syllabified inflected forms. Each data basis is available in two Unicode formats: text and XML. An entry of RoMorphoDict, in text format, contains information on inflected form, its lemma, its morpho-syntactic description and the marking of the stressed vowel in pronunciation, while in XML format, an entry, representing the whole paradigm of a word, contains further informations about roots and paradigm class. An entry of RoSyllabiDict, in both formats, contains information about unsyllabified word, its syllabified correspondent, grammatical information and/or type of syllabification, if it is the case. The stressed vowel is also marked on the syllabified form. Each lexical data base includes the corresponding inflected forms of about 65,000 lemmas, that is, over 700,000 entries in RoMorphoDict, and over 500,000 entries in RoSyllabiDict. Both resources are available for free. The paper describes in detail the content of these data bases and the procedure of building them.
Although the World Wide Web has late become an important source to consult for the meaning of words, a number of technical terms related to high technology are not found on the Web. This paper describes a method to produce an encyclopedic dictionary for high-tech terms from patent information. We used a collection of unexamined patent applications published by the Japanese Patent Office as a source corpus. Given this collection, we extracted terms as headword candidates and retrieved applications including those headwords. Then, we extracted paragraph-style descriptions and categorized them into technical domains. We also extracted related terms for each headword. We have produced a dictionary including approximately 400,000 Japanese terms as headwords. We have also implemented an interface with which users can explore our dictionary by reading text descriptions and viewing a related-term graph.
In this paper we discuss how linguistic and geographic distances can be related using a 3D visualization. We will convert linguistic data for locations along the German-Dutch border to linguistic distances that can be compared directly to geographic distances. This enables us to visualize linguistic distances as real distances with the use of the third dimension available in 3D modelling software. With such a visualization we will test if descriptive dialect data support the hypothesis that the German-Dutch state border became a linguistic border between the German and Dutch dialects. Our visualization is implemented in the 3D modelling software SketchUp.
This paper describes a project aimed at converting a legacy representation of English idioms into an XML-based format. The project is set in the context of a large electronic English-Polish dictionary which contains several hundred formalized idiom descriptions and which has been released under the terms of a free license. In short, the project consists of three phases: cleaning up the dictionary markup, extracting the legacy idiom representations, and converting them into TEI P5 XML constrained by a RelaxNG grammar created for this purpose and constituting a module that can be included as part of the TEI P5 schema. The paper contains general descriptions of the individual phases and several examples of XML-encoded idioms. It also suggests some directions for further research, which include abstracting the XML-ized idiom representations into general syntactic patterns and using the representations to automatically identify idioms in tagged corpora.
ProPOSEL is a prototype prosody and PoS (part-of-speech) English lexicon for Language Engineering, derived from the following language resources: the computer-usable dictionary CUVPlus, the CELEX-2 database, the Carnegie-Mellon Pronouncing Dictionary, and the BNC, LOB and Penn Treebank PoS-tagged corpora. The lexicon is designed for the target application of prosodic phrase break prediction but is also relevant to other machine learning and language engineering tasks. It supplements the existing record structure for wordform entries in CUVPlus with syntactic annotations from rival PoS-tagging schemes, mapped to fields for default closed and open-class word categories and for lexical stress patterns representing the rhythmic structure of wordforms and interpreted as potential new text-based features for automatic phrase break classifiers. The current version of the lexicon comes as a textfile of 104052 separate entries and is intended for distribution with the Natural Language ToolKit; it is therefore accompanied by supporting Python software for manipulating the data so that it can be used for Natural Language Processing (NLP) and corpus-based research in speech synthesis and speech recognition.
One of the aims of the Language Technology for eLearning project is to show that Natural Language Processing techniques can be employed to enhance the learning process. To this end, one of the functionalities that has been developed is a pattern-based glossary candidate detector which is capable of extracting definitions in eight languages. In order to improve the results obtained with the pattern-based approach, machine learning techniques are applied on the Dutch results to filter out incorrectly extracted definitions. In this paper, we discuss the machine learning techniques used and we present the results of the quantitative evaluation. We also discuss the integration of the tool into the Learning Management System ILIAS.
Deep processing of natural language requires large scale lexical resources that have sufficient coverage at a sufficient level of detail and accuracy (i.e. both recall and precision). Hand-crafted lexicons are extremely labour-intensive to create and maintain, and require continuous updating and extension to retain their level of usability. In this paper we present a technique for extending lexicons using similarity measures that can be extracted from corpora. The technique involves creating lexical entries for unknown words based on entries for words that are known and that are deemed to be distributionally similar. We demonstrate the applicability of the approach by providing an extended lexicon for the LinGO system using similarity measures extracted from the BNC. We also discuss the advantages and disadvantages of using such lexical extensions in different ways: principally either as part of the main lexicon or as a separate resource used only for last resort use.
Many NLP modules and applications require the availability of a module for wide-coverage inflectional analysis. One way to obtain such analyses is to use a morphological analyser in combination with an inflectional lexicon. Since large text corpora nowadays are easily available and inflectional systems are in general well understood, it seems feasible to acquire lexical data from raw texts, guided by our knowledge of inflection. I present an acquisition method along these lines for German. The general idea can be roughly summarised as follows: first, generate a set of lexical entry hypotheses for each word-form in the corpus; then, select hypotheses that explain the word-forms found in the corpus best. To this end, I have turned an existing morphological grammar, cast in finite-state technology (Schmid et al. 2004), into a hypothesiser for lexical entries. Irregular forms are simply listed so that they do not interfere with the regular rules used in the hypothesiser. Running the hypothesiser on a text corpus yields a large number of lexical entry hypotheses. These are then ranked according to their validity with the help of a statistical model that is based on the number of attested and predicted word forms for each hypothesis.
Despite of the importance of lexical resources for a number of NLP applications (Machine Translation, Information Extraction, Question Answering, among others), there has been a traditional lack of generic tools for the creation, maintenance and management of computational lexica. The most direct obstacle for the development of generic tools, independent of any particular application format, was the lack of standards for the description and encoding of lexical resources. The availability of the Lexical Markup Framework (LMF) has changed this scenario and has made it possible the development of generic lexical platforms. COLDIC is a generic platform for working with computational lexica. The system has been designed to let the user concentrate on lexicographical tasks, but still being autonomous in the management of the tools. The creation and maintenance of the database, which is the core of the tool, demand no specific training in databases. A LMF compliant schema implemented in a Document Type Definition (DTD) describing the lexical resources is taken by the system to automatically configure the platform. Besides, the most standard web services for interoperability are also generated automatically. Other components of the platform include build-in functions supporting the most common tasks of the lexicographic work.
The paper describes the treatment of some specific syntactic constructions in two treebanks of Latin according to a common set of annotation guidelines. Both projects work within the theoretical framework of Dependency Grammar, which has been demonstrated to be an especially appropriate framework for the representation of languages with a moderately free word order, where the linear order of constituents is broken up with elements of other constituents. The two projects are the first of their kind for Latin, so no prior established guidelines for syntactic annotation are available to rely on. The general model for the adopted style of representation is that used by the Prague Dependency Treebank, with departures arising from the Latin grammar of Pinkster, specifically in the traditional grammatical categories of the ablative absolute, the accusative + infinitive, and gerunds/gerundives. Sharing common annotation guidelines allows us to compare the datasets of the two treebanks for tasks such as mutually checking annotation consistency, diachronically studying specific syntactic constructions, and training statistical dependency parsers.
It is known that POS tagging is not very accurate for unknown words (words which the POS tagger has not seen in the training corpora). Thus, a first step to improve the tagging accuracy would be to extend the coverage of the taggers learned lexicon. It turns out that, through the use of a simple procedure, one can extend this lexicon without using additional, hard to obtain, hand-validated training corpora. The basic idea consists of merely adding new words along with their (correct) POS tags to the lexicon and trying to estimate the lexical distribution of these words according to similar ambiguity classes already present in the lexicon. We present a method of automatically acquire high quality POS tagging lexicons based on morphologic analysis and generation. Currently, this procedure works on Romanian for which we have a required paradigmatic generation procedure but the architecture remains general in the sense that given the appropriate substitutes for the morphological generator and POS tagger, one should obtain similar results.
We present the main findings and preliminary results of an ongoing project aimed at developing a system for collocation extraction based on contextual morpho-syntactic properties. We explored two hybrid extraction methods: the first method applies language-indepedent statistical techniques followed by a linguistic filtering, while the second approach, available only for German, is based on a set of lexico-syntactic patterns to extract collocation candidates. To define extraction and filtering patterns, we studied a specific collocation category, the Verb-Noun constructions, using a model inspired by the systemic functional grammar, proposing three level analysis: lexical, functional and semantic criteria. From tagged and lemmatized corpus, we identify some contextual morpho-syntactic properties helping to filter the output of the statistical methods and to extract some potential interesting VN constructions (complex predicates vs complex predicators). The extracted candidates are validated and classified manually.
In this paper we discuss an approach to the semi-automatic extraction and classification of the compounds extracted from German corpora. Compound nominals are semi-automatically extracted from text corpora along with their sentential complements. In this study we concentrate on that{\-}, wh{\-} or if subclauses although our methods can be applied to other complements as well. We elaborate an architecture using linguistic knowledge about the phenomena we extract, and aim at answering the following questions: how can data about subcategorisation properties of nominal compounds be extracted from text corpora, and how can compounds be classified according to their subcategorisation properties? Our classification is based on the relationships between the subcategorisation of nominal compounds, e.g. Grundfrage, Wettstreit and Beweismittel, and that of their constituent parts, such as Frage, Streit, Beweis, etc. We show that there are cases which do not match the commonly accepted assumption that the head of a compound is its valency bearer. Such cases should receive a specific treatment in NLP dictionary building. This calls for tools to identify and classify such cases by means of data extraction from corpora. We propose precision-oriented semi{\-}automatic extraction which can operate on tokenized, tagged and lemmatized texts. In the future, we are going to extend the kinds of extracted complements beyond subclauses and analyze the nature of the non-head valency-bearer of compounds, as well as an extension of the kinds of extracted complements beyond subclauses.
Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity; underspecified representations are particularly well suited for the representation of ambiguous data because they allow for high informational efficiency. We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses. The main topic of this article is a data model and an encoding scheme based on LAF/GrAF (Ide and Romary, 2006; Ide and Suderman, 2007) which provides a flexible framework for encoding underspecified representations. We show how a set of dependency structures and a set of TiGer graphs (Brants et al., 2002) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.
The JOSmorphosyntactic resources for Slovene consist of the specifications, lexicon, and two corpora: jos100k, a 100,000 word balanced monolingual sampled corpus annotated with hand validated morphosyntactic descriptions (MSDs) and lemmas, and jos1M, the 1 million-word partially hand validated corpus. The two corpora have been sampled from the 600M-word Slovene reference corpus FidaPLUS. The JOS resources have a standardised encoding, with the MULTEXT-East-type morphosyntactic specifications and the corpora encoded according to the Text Encoding Initiative Guidelines P5. JOS resources are available as a dataset for research under the Creative Commons licence and are meant to facilitate developments of HLT for Slovene.
The paper presents Spejd, an Open Source Shallow Parsing and Disambiguation Engine. Spejd (abbreviated to ♠) is based on a fully uniform formalism both for constituency partial parsing and for morphosyntactic disambiguation - the same grammar rule may contain structure-building operations, as well as morphosyntactic correction and disambiguation operations. The formalism and the engine are more flexible than either the usual shallow parsing formalisms, which assume disambiguated input, or the usual unification-based formalisms, which couple disambiguation (via unification) with structure building. Current applications of Spejd include rule-based disambiguation, detection of multiword expressions, valence acquisition, and sentiment analysis. The functionality can be further extended by adding external lexical resources. While the examples are based on the set of rules prepared for the parsing of the IPI PAN Corpus of Polish, ♠ is fully language-independent and we hope it will also be useful in the processing of other languages.
This paper describes a three-part annotation scheme for superlatives. The first identifies syntactic classes, since superlatives can serve different semantic purposes. The second and third only apply to superlatives that express straight-forward comparisons between targets and their comparison sets. The second form of annotation identifies the spans of each target and comparison set, which is of interest for relation extraction. The third form labels superlatives as facts or opinions, which has not yet been undertaken in the area of sentiment detection. The annotation scheme has been tested and evaluated on 500 tokens of superlatives, the results of which are presented in Section 5. In addition to providing a platform for investigating superlatives on a larger scale, this research also introduces a new text-based Wikipedia corpus which is especially suitable for linguistic research.
Part-of-Speech tagging is generally performed by Markov models, based on bigram or trigram models. While Markov models have a strong concentration on the left context of a word, many languages require the inclusion of right context for correct disambiguation. We show for German that the best results are reached by a combination of left and right context. If only left context is available, then changing the direction of analysis and going from right to left improves the results. In a version of MBT with default parameter settings, the inclusion of the right context improved POS tagging accuracy from 94.00{\%} to 96.08{\%}, thus corroborating our hypothesis. The version with optimized parameters reaches 96.73{\%}.
Based on simple methods such as observing word and part of speech tag co-occurrence and clustering, we generate syntactic parses of sentences in an entirely unsupervised and self-inducing manner. The parser learns the structure of the language in question based on measuring breaking points within sentences. The learning process is divided into two phases, learning and application of learned knowledge. The basic learning works in an iterative manner which results in a hierarchical constituent representation of the sentence. Part-of-Speech tags are used to circumvent the data sparseness problem for rare words. The algorithm is applied on untagged data, on manually assigned tags and on tags produced by an unsupervised part of speech tagger. The results are unsurpassed by any self-induced parser and challenge the quality of trained parsers with respect to finding certain structures such as noun phrases.
In this paper we propose a rule-based approach to extract dependency and grammatical functions from the Venice Italian Treebank, a Treebank of written text with PoS and constituent labels consisting of 10,200 utterances and about 274,000 tokens. As manual corpus annotation is expensive and time-consuming, we decided to exploit this existing constituency-based Treebank to derive dependency structures with lower effort. After describing the procedure to extract heads and dependents, based on a head percolation table for Italian, we introduce the rules adopted to add grammatical relation labels. To this purpose, we manually relabeled all non-canonical arguments, which are very frequent in Italian, then we automatically labeled the remaining complements or arguments following some syntactic restrictions based on the position of the constituents w.r.t to parent and sibling nodes. The final section of the paper describes evaluation results. Evaluation was carried out in two steps, one for dependency relations and one for grammatical roles. Results are in line with similar conversion algorithms carried out for other languages, with 0.97 precision on dependency arcs and F-measure for the main grammatical functions scoring 0.96 or above, except for obliques with 0.75.
The paper presents two experiments of unsupervised classification of Italian noun phrases. The goal of the experiments is to identify the most prominent contextual properties that allow for a functional classification of noun phrases. For this purpose, we used a Self Organizing Map is trained with syntactically-annotated contexts containing noun phrases. The contexts are defined by means of a set of features representing morpho-syntactic properties of both nouns and their wider contexts. Two types of experiments have been run: one based on noun types and the other based on noun tokens. The results of the type simulation show that when frequency is the most prominent classification factor, the network isolates idiomatic or fixed phrases. The results of the token simulation experiment, instead, show that, of the 36 attributes represented in the original input matrix, only a few of them are prominent in the re-organization of the map. In particular, key features in the emergent macro-classification are the type of determiner and the grammatical number of the noun. An additional but not less interesting result is an organization into semantic/pragmatic micro-classes. In conclusions, our result confirm the relative prominence of determiner type and grammatical number in the task of noun (phrase)categorization.
This paper presents a corpus study of parenthetical constructions in two different corpora: the Penn Discourse Treebank (PDTB, (PDTBGroup, 2008)) and the RST Discourse Treebank (Carlson et al., 2001). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output. We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories: ELABORATION/EXPANSION-type NP-modifier parentheticals and NON-ELABORATION/EXPANSION-type VP- or S-modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines.
The Arabic Treebank team at the Linguistic Data Consortium has significantly revised and enhanced its annotation guidelines and procedure over the past year. Improvements were made to both the morphological and syntactic annotation guidelines, and annotators were trained in the new guidelines, focusing on areas of low inter-annotator agreement. The revised guidelines are now being applied in annotation production, and the combination of the revised guidelines and a period of intensive annotator training has raised inter-annotator agreement f-measure scores already and has also improved parsing results.
In this paper, we present the details of creating a pilot Arabic proposition bank (Propbank). Propbanks exist for both English and Chinese. However the morphological and syntactic expression of linguistic phenomena in Arabic yields a very different type of process in creating an Arabic propbank. Hence, we highlight those characteristics of Arabic that make creating a propbank for the language a different challenge compared to the creation of an English Propbank.We believe that many of the lessons learned in dealing with Arabic could generalise to other languages that exhibit equally rich morphology and relatively free word order.
This paper introduces Saxon, a rule-based document annotator that is capable of processing and annotating several document formats and media, both within and across documents. Furthermore, Saxon is readily extensible to support other input formats due to both its flexible rule formalism and the modular plugin architecture of the Runes framework upon which it is built. In this paper we introduce the Saxon rule formalism through examples aimed at highlighting its power and flexibility.
We present a new coding mechanism, spatiotemporal coding, that allows coders to annotate points and regions in the video frame by drawing directly on the screen. Coders can not only attach labels to time intervals in the video but can specify a possibly moving region on the video screen. This opens up the spatial dimension for multi-track video coding and is an essential asset in almost every area of video coding, e.g. gesture coding, facial expression coding, encoding semantics for information retrieval etc. We discuss conceptual variants, design decisions and the relation to the MPEG-7 standard and tools.
In human face-to-face interaction, participants can rely on a number of audio-visual information for interpreting interlocutors communicative intentions, such information strongly contributing to the successfulness of communication. Modelling these typical human abilities represents a main objective in human communication research, including technological applications like human-machine interaction. In this pilot study we explore the possibility of using audio-visual parameters for describing/measuring the differences perceived in interlocutors communicative behaviours. Preliminary results derived from the multimodal analysis of a single subject seem to indicate that measuring the distribution of some prosodic and hand gesture events which are temporally co-occurring contribute to the accounting of such perceived differences. Moreover, as far as gesture events are concerned, it has been observed that relevant information are not simply to be found in the occurences of single gestures, but mainly in some gesture modalities (for example, single stroke vs multiple stroke gestures, one-hand vs both-hands gestures, etc?). In this paper we also introduce and describe a software package, ViSuite, we developed for multimodal processing and used for the work described in his paper.
The Linguistic Data Consortium (LDC) creates a variety of linguistic resources - data, annotations, tools, standards and best practices - for many sponsored projects. The programming staff at LDC has created the tools and technical infrastructures to support the data creation efforts for these projects, creating tools and technical infrastructures for all aspects of data creation projects: data scouting, data collection, data selection, annotation, search, data tracking and worklow management. This paper introduces a number of samples of LDC programming staffs work, with particular focus on the recent additions and updates to the suite of software tools developed by LDC. Tools introduced include the GScout Web Data Scouting Tool, LDC Data Selection Toolkit, ACK - Annotation Collection Kit, XTrans Transcription and Speech Annotation Tool, GALE Distillation Toolkit, and the GALE MT Post Editing Workflow Management System.
In this paper we discuss the design, acquisition and preprocessing of a Czech audio-visual speech corpus. The corpus is intended for training and testing of existing audio-visual speech recognition system. The name of the database is UWB-07-ICAVR, where ICAVR stands for Impaired Condition Audio Visual speech Recognition. The corpus consists of 10,000 utterances of continuous speech obtained from 50 speakers. The total length of the database is 25 hours. Each utterance is stored as a separate sentence. The corpus extends existing databases by covering condition of variable illumination. We acquired 50 speakers, where half of them were men and half of them were women. Recording was done by two cameras and two microphones. Database introduced in this paper can be used for testing of visual parameterization in audio-visual speech recognition (AVSR). Corpus can be easily split into training and testing part. Each speaker pronounced 200 sentences: first 50 were the same for all, the rest of them were different. Six types of illumination were covered. Session for one speaker can fit on one DVD disk. All files are accompanied by visual labels. Labels specify region of interest (mouth and area around them specified by bounding box). Actual pronunciation of each sentence is transcribed into the text file.
The availability of large amounts of data is a fundamental prerequisite for building handwriting recognition systems. Any system needs a test set of labelled samples for measuring its performance along its development and guiding it. Moreover, there are systems that need additional samples for learning the recognition task they have to cope with later, i.e. a training set. Thus, the acquisition and distribution of standard databases has become an important issue in the handwriting recognition research community. Examples of widely used databases in the online domain are UNIPEN, IRONOFF, and Pendigits. This paper describes the current state of our own database, UJIpenchars, whose first version contains online representations of 1,364 isolated handwritten characters produced by 11 writers and is freely available at the UCI Machine Learning Repository. Moreover, we have recently concluded a second acquisition phase, totalling more than 11,000 samples from 60 writers to be made available in short as UJIpenchars2.
This paper deals with non manual gestures annotation involved in Sign Language within the context of automatic generation of Sign Language. We will tackle linguistic researches in sign language, present descriptions of non manual gestures and problems lead to movement description. Then, we will propose a new annotation methodology, which allows non manual gestures description. This methodology can describe all Non Manual Gestures with precision, economy and simplicity. It is based on four points: Movement description (instead of position description); Movement decomposition (the diagonal movement is described with horizontal movement and vertical movement separately); Element decomposition (we separate higher eyelid and lower eyelid); Use of a set of symbols rather than words. One symbol can describe many phenomena (with use of colours, height...). First analysis results allow us to define precisely the structure of eye blinking and give the very first ideas for the rules to be designed. All the results must be refined and confirmed by extending the study on the whole corpus. In a second step, our annotation will be used to produce analyses in order to define rules and structure definition of Non Manual Gestures that will be evaluate in LIMSIs automatic French Sign Language generation system.
A new, linguistically annotated, video database for automatic sign language recognition is presented. The new RWTH-BOSTON-400 corpus, which consists of 843 sentences, several speakers and separate subsets for training, development, and testing is described in detail. For evaluation and benchmarking of automatic sign language recognition, large corpora are needed. Recent research has focused mainly on isolated sign language recognition methods using video sequences that have been recorded under lab conditions using special hardware like data gloves. Such databases have often consisted generally of only one speaker and thus have been speaker-dependent, and have had only small vocabularies. A new database access interface, which was designed and created to provide fast access to the database statistics and content, makes it possible to easily browse and retrieve particular subsets of the video database. Preliminary baseline results on the new corpora are presented. In contradistinction to other research in this area, all databases presented in this paper will be publicly available.
Systems that automatically process sign language rely on appropriate data. We therefore present the ATIS sign language corpus that is based on the domain of air travel information. It is available for five languages, English, German, Irish sign language, German sign language and South African sign language. The corpus can be used for different tasks like automatic statistical translation and automatic sign language recognition and it allows the specific modeling of spatial references in signing space.
This paper discusses the design, recording and preprocessing of a Czech sign language corpus. The corpus is intended for training and testing of sign language recognition (SLR) systems. The UWB-07-SLR-P corpus contains video data of 4 signers recorded from 3 different perspectives. Two of the perspectives contain whole body and provide 3D motion data, the third one is focused on signers face and provide data for face expression and lip feature extraction. Each signer performed 378 signs with 5 repetitions. The corpus consists of several types of signs: numbers (35 signs), one and two-handed finger alphabet (64), town names (35) and other signs (244). Each sign is stored in a separate AVI file. In total the corpus consists of 21853 video files in total length of 11.1 hours. Additionally each sign is preprocessed and basic features such as 3D hand and head trajectories are available. The corpus is mainly focused on feature extraction and isolated SLR rather than continuous SLR experiments.
We have obtained the valuable findings about the developmental processes of demonstrative expression skills, which is concerned with the fundamental commonsense of human knowledge, such as to get an object and to catch someones attention. We have already developed a framework to record genuine spontaneous speech of infants. We are constructing a multimodal infant behavior corpus, which enables us to elucidate human commonsense knowledge and its acquisition mechanism. Based on the observation of the corpus, we proposed a multimodal behavior description for observation of demonstrative expressions. We proved that the proposed model has the nearly 90{\%} coverage in an open test of the behavior description task. The analysis using the model produced many valuable findings from multimodal viewpoints; for example, the change of line of sight from object to person to person to object means that the infant has obtained a better way to catch someones attention. Our intention-based analysis provided us with an infant behavior model that may apply to a likely behavior simulation system.
This paper describes a method of automatic emotional degree labeling for speakers anger utterances during natural Japanese dialog. First, we explain how to record anger utterance naturally appeared in natural Japanese dialog. Manual emotional degree labeling was conducted in advance to grade the utterances by a 6 Likert scale to obtain a correct anger degree. Then experiments of automatic anger degree estimation were conducted to label an anger degree with each utterance by its acoustic features. Also estimation experiments were conducted with speaker-dependent datasets to find out any influence of individual emotional expression on automatic emotional degree labeling. As a result, almost all the speakers models show higher adjusted R square so that those models are superior to the speaker-independent model in those estimation capabilities. However, a residual between automatic emotional degree and manual emotional degree (0.73) is equivalent to those of speakers models. There still has a possibility to label utterances with the speaker-independent model.
The present paper deals with the design and the annotation of a Greek real-world emotional speech corpus. The speech data consist of recordings collected during the interaction of na{\"\i
This paper presents an annotation scheme for marking subjective content in meetings, specifically the opinions and sentiments that participants express as part of their discussion. The scheme adapts concepts from the Multi-perspective Question Answering (MPQA) Annotation Scheme, an annotation scheme for marking opinions and attributions in the news. The adaptations reflect the differences in multiparty conversation as compared to text, as well as the overall goals of our project.
In the Autonomata project we have collected a corpus of spoken name utterances with manually corrected phonemic transcriptions of these utterances. The corpus was designed with the intention to become a major resource for the development of automatic speech recognition engines that can achieve a high accuracy on the recognition of person and geographical names spoken in Dutch. The recorded names were selected so as to reveal the major pronunciation variations that a speech recognizer of e.g. a navigation system with speech input is going to be confronted with. This includes native speakers speaking foreign names and vice versa.
A new procedure is described for generating pronunciations for a dictionary of place-names in a less-resourced language (Welsh, spoken in Wales, UK). The method is suitable for use in a situation where there is a lack of skilled phoneticians with expertise in the language, but where there are native speakers available, as well as a text-to-speech synthesiser for the language. The lack of skilled phoneticians will make it impossible to carry out direct editing of pronunciations, and so a method has been devised that makes it possible for non-phonetician native speakers to edit pronunciations without knowledge of the phonology of the language. The key advance in this method is the use of re-spelling to indicate pronunciation in a linguistically-na{\"\i
In this paper, we investigated how foreign language speakers pronounce Japanese words transliterated using two major Romanization systems, Hepburn and Kunrei. First, we recorded foreign language speakers pronouncing Romanized Japanese words. Next, Japanese speakers listened to the recordings and wrote down the words in Japanese Kana. Sets of each Romanized Japanese word, its correct Kana expression, its recorded reading, and the Kana dictated from the recording were stored in our database. We also investigated which of the two Romanization systems was pronounced more correctly by foreign language speakers by comparing the correctness of their respective readings. We also investigated which systems pronunciation by foreign language speakers was judged as more acceptable by Japanese speakers.
Large vocabulary automatic speech recognition (ASR) technologies perform well in known, controlled contexts. However recognition of proper nouns is commonly considered as a difficult task. Accurate phonetic transcription of a proper noun is difficult to obtain, although it can be one of the most important resources for a recognition system. In this article, we propose methods of automatic phonetic transcription applied to proper nouns. The methods are based on combinations of the rule-based phonetic transcription generator LIA{\_}PHON and an acoustic-phonetic decoding system. On the ESTER corpus, we observed that the combined systems obtain better results than our reference system (LIA{\_}PHON). The WER (Word Error Rate) decreased on segments of speech containing proper nouns, without affecting negatively the results on the rest of the corpus. On the same corpus, the Proper Noun Error Rate (PNER, which is a WER computed on proper nouns only), decreased with our new system.
The consortium ECESS (European Center of Excellence for Speech Synthesis) has set up a framework for evaluation of software modules and tools relevant for speech synthesis. Till now two lines of evaluation campaigns have been established: (1) Evaluation of the ECESS TTS modules (text processing, prosody, acoustic synthesis). (2) Evaluation of ECESS tools (pitch extraction, voice activity detection, phonetic segmentation). The functionality and interfaces of the ECESS TTS have been developed by a joint effort between ECESS and the EC-funded project TC-STAR . First evaluation campaigns were conducted within TC-STAR using the ECESS framework. As TC-STAR finished in March 2007, ECESS continued and extended the evaluation of ECESS TTS modules and tools by its own. Within the paper we describe a novel framework which allows performing remote evaluation for modules via the web. First experimental results are reported. Further the result of several evaluation campaigns for tools handling pitch extraction and voice activity detection are presented.
The production of rich multilingual speech corpus resources on a large scale is a requirement for many linguistic, phonetic and technological tasks, in both research and application domains. It is also time-consuming and therefore expensive. The human component in the resource creation process is also prone to inconsistencies, a situation frequently documented in cross-transcriber consistency studies. In the present case, corpora of three languages were to be evaluated and corrected: (1) Polish, a large automatically annotated and manually corrected single-speaker TTS unit-selection corpus in the BOSS Label File (BLF) format, (2) German and (3) English, the second and third being manually annotated multi-speaker story-telling learner corpora in Praat TextGrid format. A method is provided for supporting the evaluation and correction of time-aligned annotations for the three corpora by permitting a rapid audio screening of the annotations by an expert listener for the detection of perceptually conspicuous systematic or isolated errors in the annotations. The criterion for perceptual conspicuousness was provided by converting the annotation formats into the interface format required by the MBROLA speech synthesiser. The audio screening procedure is complementary to other methods of corpus evaluation and does not replace them.
This paper presents a freely-available, and flexible Wizard of Oz environment for rapid prototyping. The system is designed to investigate the required features of a dialog system using the commonly used Wizard of Oz approach. The idea is that the time consuming design of such a tool can be avoided by using the provided architecture. The developers can easily adapt the database and extend the tool to the individual needs of the targeted dialog system. The tool is designed as a client-server architecture and provides efficient input features and versatile output types including voice, or an avatar as visual output. Furthermore, a scenario, namely restaurant selection, is introduced in order to give an example application for a dialog system.
The paper deals with the process of designing a phonetically and prosodically rich speech corpus for unit selection speech synthesis. The attention is given mainly to the recording and verification stage of the process. In order to ensure as high quality and consistency of the recordings as possible, a special recording environment consisting of a recording session management and pluggable chain of checking modules was designed and utilised. Other stages, namely text collection (including) both phonetically and prosodically balanced sentence selection and a careful annotation on both orthographic and phonetic level are also mentioned.
In this paper we share our experience and describe the methodologies that we have used in designing and recording large speech databases for applications requiring speech synthesis. Given the growing demand for customized and domain specific voices for use in corpus based synthesis systems, we believe that good practices should be established for the creation of these databases which are a key factor in the quality of the resulting speech synthesizer. We will focus on the designing of the recording prompts, on the speaker selection procedure, on the recording setup and on the quality control of the resulting database. One of the major challenges was to assure the uniformity of the recordings during the 20 two-hour recording sessions that each speaker had to perform, to produce a total of 13 hours of recorded speech for each of the four speakers. This work was conducted in the scope of the Tecnovoz project that brought together 4 speech research centers and 9 companies with the goal of integrating speech technologies in a wide range of applications.
This paper presents MISTRAL, an open source statistical machine translation decoder dedicated to spoken language translation. While typical machine translation systems take a written text as input, MISTRAL translates word lattices produced by automatic speech recognition systems. The lattices are translated in two passes using a phrase-based model. Our experiments reveal an improvement in BLEU when translating lattices instead of sentences returned by a speech recognition system.
LC-STAR II is a follow-up project of the EU funded project LC-STAR (Lexica and Corpora for Speech-to-Speech Translation Components, IST-2001-32216). LC-STAR II develops large lexica containing information for speech processing in ten languages targeting especially automatic speech recognition and text to speech synthesis but also other applications like speech-to-speech translation and tagging. The project follows by large the specifications developed within the scope of LC-STAR covering thirteen languages: Catalan, Finnish, German, Greek, Hebrew, Italian, Mandarin Chinese, Russian, Turkish, Slovenian, Spanish, Standard Arabic and US-English. The ten new LC-STAR II languages are: Brazilian-Portuguese, Cantonese, Czech, English-UK, French, Hindi, Polish, Portuguese, Slovak, and Urdu. The project started in 2006 with a lifetime of two years. The project is funded by a consortium, which includes Microsoft (USA), Nokia (Finland), NSC (Israel), Siemens (Germany) and Harmann/Becker (Germany). The project is coordinated by UPC (Spain) and validation is performed by SPEX (The Netherlands), and CST (Denmark). The developed language resources will be shared among partners.This paper presents a summary of the creation of word lists and lexica and an overview of adaptations of the specifications and conceptual representation model from LC-STAR to the new languages. The validation procedure will be presented too.
A new approach to handle unknown words in machine translation is presented. The basic idea is to find definitions for the unknown words on the source language side and translate those definitions instead. Only monolingual resources are required, which generally offer a broader coverage than bilingual resources and are available for a large number of languages. In order to use this in a machine translation system definitions are extracted automatically from online dictionaries and encyclopedias. The translated definition is then inserted and clearly marked in the original hypothesis. This is shown to lead to significant improvements in (subjective) translation quality.
We describe recent work on MedSLT, a medium-vocabulary interlingua-based medical speech translation system, focussing on issues that arise when handling languages of which the grammar engineer has little or no knowledge. We show how we can systematically create and maintain multiple forms of grammars, lexica and interlingual representations, with some versions being used by language informants, and some by grammar engineers. In particular, we describe the advantages of structuring the interlingua definition as a simple semantic grammar, which includes a human-readable surface form. We show how this allows us to rationalise the process of evaluating translations between languages lacking common speakers, and also makes it possible to create a simple generic tool for debugging to-interlingua translation rules. Examples presented focus on the concrete case of translation between Japanese and Arabic in both directions.
We present an approach for the cross-lingual induction of speech recognition grammars that separates the task of translation from the task of grammar generation. The source speech recognition grammar is used to generate phrases, which are translated by a common translation service. The target recognition grammar is induced by using the production rules of the source language, manually translated sentences and a statistical word alignment tool. We induce grammars for the target languages Spanish and Japanese. The coverage of the resulting grammars is evaluated on two corpora and compared quantitatively and qualitatively to a grammar induced with unsupervised monolingual grammar induction.
In this paper, quantitative analyses of the delay in Japanese-to-English (J-E) and English-to-Japanese (E-J) interpretations are described. The Simultaneous Interpretation Database of Nagoya University (SIDB) was used for the analyses. Beginning time and end time of each word were provided to the corpus using HMM-based phoneme segmentation, and the time lag between the corresponding words was calculated as the word-level delay. Word-level delay was calculated for 3,722 pairs and 4,932 pairs of words for J-E and E-J interpretations, respectively. The analyses revealed that J-E interpretation has much larger delay than E-J interpretation and that the difference of word order between Japanese and English affect the degree of delay.
This paper introduces a knowledge representation formalism used for annotation of the French MEDIA dialogue corpus in terms of high level semantic structures. The semantic annotation, worked out according to the Berkeley FrameNet paradigm, is incremental and partially automated. We describe an automatic interpretation process for composing semantic structures from basic semantic constituents using patterns involving words and constituents. This process contains procedures which provide semantic compositions and generating frame hypotheses by inference. The MEDIA corpus is a French dialogue corpus recorded using a Wizard of Oz system simulating a telephone server for tourist information and hotel booking. It had been manually transcribed and annotated at the word and semantic constituent levels. These levels support the automatic interpretation process which provides a high level semantic frame annotation. The Frame based Knowledge Source we composed contains Frame definitions and composition rules. We finally provide some results obtained on the automatically-derived annotation.
We present recent work in the area of Cross-Domain Dialogue Act (DA) tagging. We have previously reported on the use of a simple dialogue act classifier based on purely intra-utterance features - principally involving word n-gram cue phrases automatically generated from a training corpus. Such a classifier performs surprisingly well, rivalling scores obtained using far more sophisticated language modelling techniques. In this paper, we apply these automatically extracted cues to a new annotated corpus, to determine the portability and generality of the cues we learn.
Regulus is an Open Source platform that supports construction of rule-based medium-vocabulary spoken dialogue applications. It has already been used to build several substantial speech-enabled applications, including NASAs Clarissa procedure navigator and Geneva Universitys MedSLT medical speech translator. System like these would be far more useful if they were available on a hand-held device, rather than, as with the present version, on a laptop. In this paper we describe the Open Source framework we have developed, which makes it possible to run Regulus applications on generally available mobile devices, using a distributed client-server architecture that offers transparent and reliable integration with different types of ASR systems. We describe the architecture, an implemented calendar application prototype hosted on a mobile device, and an evaluation. The evaluation shows that performance on the mobile device is as good as performance on a normal desktop PC.
In this paper we present an active approach to annotate with lexical and semantic labels an Italian corpus of conversational human-human and Wizard-of-Oz dialogues. This procedure consists in the use of a machine learner to assist human annotators in the labeling task. The computer assisted process engages human annotators to check and correct the automatic annotation rather than starting the annotation from un-annotated data. The active learning procedure is combined with an annotation error detection to control the reliablity of the annotation. With the goal of converging as fast as possible to reliable automatic annotations minimizing the human effort, we follow the active learning paradigm, which selects for annotation the most informative training examples required to achieve a better level of performance. We show that this procedure allows to quickly converge on correct annotations and thus minimize the cost of human supervision.
The extraction of flat concepts out of a given word sequence is usually one of the first steps in building a spoken language understanding (SLU) or dialogue system. This paper explores five different modelling approaches for this task and presents results on a French state-of-the-art corpus, MEDIA. Additionally, two log-linear modelling approaches could be further improved by adding morphologic knowledge. This paper goes beyond what has been reported in the literature. We applied the models on the same training and testing data and used the NIST scoring toolkit to evaluate the experimental results to ensure identical conditions for each of the experiments and the comparability of the results. Using a model based on conditional random fields, we achieve a concept error rate of 11.8{\%} on the MEDIA evaluation corpus.
Texts generated by automatic speech recognition (ASR) systems have some specificities, related to the idiosyncrasies of oral productions or the principles of ASR systems, that make them more difficult to exploit than more conventional natural language written texts. This paper aims at studying the interest of morphosyntactic information as a useful resource for ASR. We show the ability of automatic methods to tag outputs of ASR systems, by obtaining a tag accuracy similar for automatic transcriptions to the 95-98 {\%} usually reported for written texts, such as newspapers. We also demonstrate experimentally that tagging is useful to improve the quality of transcriptions by using morphosyntactic information in a post-processing stage of speech decoding. Indeed, we obtain a significant decrease of the word error rate with experiments done on French broadcast news from the ESTER corpus; we also notice an improvement of the sentence error rate and observe that a significant number of agreement errors are corrected.
This paper describes a new speech corpus, STC-TIMIT, and discusses the process of design, development and its distribution through LDC. The STC-TIMIT corpus is derived from the widely used TIMIT corpus by sending it through a real and single telephone channel. TIMIT is phonetically balanced, covers the dialectal diversity in continental USA and has been extensively used as a benchmark for speech recognition algorithms, especially in early stages of development. The experimental usability of TIMIT has been increased eventually with the creation of derived corpora, passing the original data through different channels. One such example is the well-known NTIMIT corpus, where the original files in TIMIT are re-recorded after being sent through different telephone calls, resulting in a corpus that characterizes telephone channels in a wide sense. In STC-TIMIT, we followed a similar procedure, but the whole corpus was transmitted in a single telephone call with the goal of obtaining data from a real and yet highly stable telephone channel across the whole corpus. Files in STC-TIMIT are aligned to those of TIMIT with a theoretical precision of 0.125 ms, making TIMIT labels valid for the new corpus. The experimental section presents several results on speech recognition accuracy.
The goal of the LILA project was the collection of speech databases over cellular telephone networks of five languages in three Asian countries. Three languages were recorded in India: Hindi by first language speakers, Hindi by second language speakers and Indian English. Furthermore, Mandarin was recorded in China and Korean in South-Korea. The databases are part of the SpeechDat-family and follow the SpeechDat rules in many respects. All databases have been finished and have passed the validation tests. Both Hindi databases and the Korean database will be available to the public for sale.
The paper provides an overview of the Polish Speech Database for taking dictation of legal texts, created for the purpose of LVCSR system for Polish. It presents background information about the design of the database and the requirements coming from its future uses. The applied method of the text corpora construction is presented as well as the database structure and recording scenarios. The most important details on the recording conditions and equipment are specified, followed by the description of the assessment methodology of recording quality, and the annotation specification and evaluation. Additionally, the paper contains current statistics from the database and the information about both the ongoing and planned stages of the database development process.
In this paper, we present a database with speech in different types of background noises. The speech and noise were recorded with a set of different microphones and including some sensors that pick up the speech vibrations by making contact with the skull, the throat and the ear canal, respectively. As these sensors should be less sensitive to noise sources, our database can be especially useful for investigating the properties of these special microphones and comparing them to those of conventional microphones for applications requiring noise robust speech capturing and processing. In this paper we describe some experiments that were carried out using this database in the field of Voice Activity Detection (VAD). It is shown that the signals of a special microphone such as the throat microphone exhibit a high signal to noise ratio and that this property can be exploited to significantly improve the accuracy of a VAD algorithm.
This paper describes the corpus of university lectures that has been recorded in European Portuguese, and some of the recognition experiments we have done with it. The highly specific topic domain and the spontaneous speech nature of the lectures are two of the most challenging problems. Lexical and language model adaptation proved difficult given the scarcity of domain material in Portuguese, but improvements can be achieved with unsupervised acoustic model adaptation. From the point of view of the study of spontaneous speech characteristics, namely disflluencies, the LECTRA corpus has also proved a very valuable resource.
A number of forensic studies published during the last 50 years report that intoxication with alcohol influences speech in a way that is made manifest in certain features of the speech signal. However, most of these studies are based on data that are not publicly available nor of statistically sufficient size. Furthermore, in spite of the positive reports nobody ever successfully implemented a method to detect alcoholic intoxication from the speech signal. The Alcohol Language Corpus (ALC) aims to answer these open questions by providing a publicly available large and statistically sound corpus of intoxicated and sober speech. This paper gives a detailed description of the corpus features and methodology. Also, we will present some preliminary results on a series of verifications about reported potential features that are claimed to reliably indicate alcoholic intoxication.
The aim of this paper is to present the design of a multimodal database suitable for research on new possibilities for automatic diagnosis of patients with severe obstructive sleep apnoea (OSA). Early detection of severe apnoea cases can be very useful to give priority to their early treatment optimizing the expensive and time-consuming tests of current diagnosis methods based on full overnight sleep in a hospital. This work is part of an on-going collaborative project between medical and signal processing groups towards the design of a multimodal database as an innovative resource to promote new research efforts on automatic OSA diagnosis through speech and image processing technologies. In this contribution we present the multimodal design criteria derived from the analysis of specific voice properties related to OSA physiological effects as well as from the morphological facial characteristics in apnoea patients. Details on the database structure and data collection methodology are also given as it is intended to be an open resource to promote further research in this field. Finally, preliminary experimental results on automatic OSA voice assessment are presented for the collected speech data in our OSA multimodal database. Standard GMM speaker recognition techniques obtain an overall correct classification rate of 82{\%}. This represents an initial promising result underlining the interest of this research framework and opening further perspectives for improvement using more specific speech and image recognition technologies.
The Spoken Document Processing Working Group, which is part of the special interest group of spoken language processing of the Information Processing Society of Japan, is developing a test collection for evaluation of spoken document retrieval systems. A prototype of the test collection consists of a set of textual queries, relevant segment lists, and transcriptions by an automatic speech recognition system, allowing retrieval from the Corpus of Spontaneous Japanese (CSJ). From about 100 initial queries, application of the criteria that a query should have more than five relevant segments that consist of about one minute speech segments yielded 39 queries. Targeting the test collection, an ad hoc retrieval experiment was also conducted to assess the baseline retrieval performance by applying a standard method for spoken document retrieval.
In this paper, a large-scale real-world speech database is introduced along with other multimedia driving data. We designed a data collection vehicle equipped with various sensors to synchronously record twelve-channel speech, three-channel video, driving behavior including gas and brake pedal pressures, steering angles, and vehicle velocities, physiological signals including driver heart rate, skin conductance, and emotion-based sweating on the palms and soles, etc. These multimodal data are collected while driving on city streets and expressways under four different driving task conditions including two kinds of monologues, human-human dialog, and human-machine dialog. We investigated the response timing of drivers against navigator utterances and found that most overlapped with the preceding utterance due to the task characteristics and the features of Japanese. When comparing utterance length, speaking rate, and the filler rate of driver utterances in human-human and human-machine dialogs, we found that drivers tended to use longer and faster utterances with more fillers to talk with humans than machines.
This paper explains our developing Corpus of Japanese classroom Lecture speech Contents (henceforth, denoted as CJLC). Increasing e-Learning contents demand a sophisticated interactive browsing system for themselves, however, existing tools do not satisfy such a requirement. Many researches including large vocabulary continuous speech recognition and extraction of important sentences against lecture contents are necessary in order to realize the above system. CJLC is designed as their fundamental basis, and consists of speech, transcriptions, and slides that were collected in real university classroom lectures. This paper also explains the difference about disfluency acts between classroom lectures and academic presentations.
Air traffic control (ATC) is based on voice communication between pilots and controllers and uses a highly task and domain specific language. Due to this very reason, spoken language technologies for ATC require domain-specific corpora, of which only few exist to this day. The ATCOSIM Air Traffic Control Simulation Speech corpus is a speech database of non-prompted and clean ATC operator speech. It consists of ten hours of speech data, which were recorded in typical ATC control room conditions during ATC real-time simulations. The database includes orthographic transcriptions and additional information on speakers and recording sessions. The ATCOSIM corpus is publicly available and provided online free of charge. In this paper, we first give an overview of ATC related corpora and their shortcomings. We then show the difficulties in obtaining operational ATC speech recordings and propose the use of existing ATC real-time simulations. We describe the recording, transcription, production and validation process of the ATCOSIM corpus, and outline an application example for automatic speech recognition in the ATC domain.
A speech and noise corpus dealing with the extreme conditions of the motorcycle environment is developed within the MoveOn project. Speech utterances in British English are recorded and processed approaching the issue of command and control and template driven dialog systems on the motorcycle. The major part of the corpus comprises noisy speech and environmental noise recorded on a motorcycle, but several clean speech recordings in a silent environment are also available. The corpus development focuses on distortion free recordings and accurate descriptions of both recorded speech and noise. Not only speech segments are annotated but also annotation of environmental noise is performed. The corpus is a small-sized speech corpus with about 12 hours of clean and noisy speech utterances and about 30 hours of segments with environmental noise without speech. This paper addresses the motivation and development of the speech corpus and finally presents some statistics and results of the database creation.
This paper describes a corpus consisting of audio data for automatic space monitoring based solely on the perceived acoustic information. The particular database is created as part of a project aiming at the detection of abnormal events, which lead to life-threatening situations or property damage. The audio corpus is composed of vocal reactions and environmental sounds that are usually encountered in atypical situations. The audio data is composed of three parts: Phase I - professional sound effects collections, Phase II recordings obtained from action and drama movies and Phase III - vocal reactions related to real-world emergency events as retrieved from television, radio broadcast news, documentaries etc. The annotation methodology is given in details along with preliminary classification results and statistical analysis of the dataset regarding Phase I. The main objective of such a dataset is to provide training data for automatic recognition machines that detect hazardous situations and to provide security enhancement in public environments, which otherwise require human supervision.
Being the clients first interface, call centres worldwide contain a huge amount of information of all kind under the form of conversational speech. If accessible, this information can be used to detect eg. major events and organizational flaws, improve customer relations and marketing strategies. An efficient way to exploit the unstructured data of telephone calls is data-mining, but current techniques apply on text only. The CallSurf project gathers a number of academic and industrial partners covering the complete platform, from automatic transcription to information retrieval and data mining. This paper concentrates on the speech recognition module as it discusses the collection, the manual transcription of the training corpus and the techniques used to build the language model. The NLP techniques used to pre-process the transcribed corpus for data mining are POS tagging, lemmatization, noun group and named entity recognition. Some of them have been especially adapted to the conversational speech characteristics. POS tagging and preliminary data mining results obtained on the manually transcribed corpus are briefly discussed.
This paper presents the results of the NEOLOGOS project: a children database and an optimized adult database for the French language. A new approach was adopted for the collection of the adult database in order to enable the development of new algorithms in the field of speech processing (study of speaker characteristics, speakers similarity, speaker selection algorithms, etc.) The objective here was to define and to carry out a new methodology for collecting significant quantities of speaker dependent data, for a significant number of speakers, as was done for several databases oriented towards speaker verification, but with the additional constraint of maximising the coverage of the space of all speakers. The children database is made of 1,000 sessions recorded by children between 7 and 16 years old. Both speech databases are SpeehDat-compliant meaning that they can be easily used for research and development in the field of speech technology.
Spoken corpora provide a critical resource for research, development and evaluation of spoken dialog systems. This paper describes the telephone spoken dialog corpus for Polish created by Polish-Japanese Institute of Information Technology team within the LUNA project (IST 033549). The main goal of this project is to create a robust natural spoken language understanding (SLU) toolkit, which can be used to improve the speech-enabled telecom services in multilingual context (Italian, French and Polish). The corpus has been collected at the call center of Warsaw Transport Authority, manually transcribed and richly annotated on acoustic, syntactic and semantic levels. The most frequent users requests concern city traffic information (public transportation stops, routes, schedules, trip planning etc.). The collected database consists of two parts: 500 human-human dialogs of approx. 670 minutes long with a vocabulary of ca. 8,000 words and 500 human-machine dialogs recorded via the use of Wizard-of-Oz paradigm. The syntactic and semantic annotation is carried out by another team (Mykowiecka et al., 2007). This database is the first one collected for spontaneous Polish speech recorded through telecommunication lines and will be used for development and evaluation of automatic speech recognition (ASR) and robust natural spoken language understanding (SLU) components.
Research activity on the Portuguese language for speech synthesis and recognition has suffered from a considerable lack of human and material resources. This has raised some obstacles to the development of speech technology and speech interface platforms. One of the most significant obstacles is the lack of spontaneous speech corpora for the creation, training and further improvement of speech synthesis and recognition programs. It was in order to suppress this gap that the CORP-ORAL project was planned. The aim of the project is to build a corpus of spontaneous EP available for the training of speech synthesis and recognition systems as well as phonetic, phonological, lexical, morphological and syntactic studies. Further possibilities of enquiry such as sociolinguistic and pragmatic research are also covered in the corpus design. The data consist of unscripted and unprompted face-to-face dialogues between family, friends, colleagues and unacquainted participants. All recordings are orthographically transcribed and prosodically annotated. CORP-ORAL is built from scratch with the explicit goal of becoming entirely available on the internet to the scientific community and the public in general.
We argue for the necessity of studying human-human spoken conversations of various kinds in order to create user interfaces to databases. An efficient user interface benefits from a well-organized corpus that can be used for investigating the strategies people use in conversations in order to be efficient and to handle the spoken communication problems. For modeling the natural behaviour and testing the model we need a dialogue corpus where the roles of participants are close to the roles of the dialogue system and its user. For that reason, we collect and investigate the Corpus of the Spoken Estonian and the Estonian Dialogue Corpus as the sources for human-human interaction investigation. The transcription conventions and annotation typology of spoken human-human dialogues in Estonian are introduced. For creating a user interface the corpus of one institutional conversation type is insufficient, since we need to know what phenomena are inherent for the spoken language in general, what means are used only in certain types of the conversations and what are the differences.
The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German ({\"O
The research project German Today aims to determine the amount of regional variation in (near-)standard German spoken by young and older educated adults and to identify and locate regional features. To this end, we compile an areally extensive corpus of read and spontaneous German speech. Secondary school students and 50-to-60-year-old locals are recorded in 160 cities throughout the German speaking area of Europe. All participants read a number of short texts and a word list, name pictures, translate words and sentences from English, answer questions in a sociobiographic interview, and take part in a map task experiment. The resulting corpus comprises over 1,000 hours of speech, which is transcribed orthographically. Automatically derived broad phonetic transcriptions, selective manual narrow phonetic transcriptions, and variationalist annotations are added. Focussing on phonetic variation we aim to show to what extent national or regional standards exist in spoken German. Furthermore, the linguistic variation due to different contextual styles (read vs. spontaneous speech) shall be analysed. Finally, the corpus enables us to investigate whether linguistic change has occurred in spoken (near-)standard German.
In this paper we describe the design and production of Catalan database for building synthetic voices. Two speakers, with 10 hours per speaker, have recorded 10 hours of speech. The speaker selection and the corpus design aim to provide resources for high quality synthesis. The resources have been used to build voices for the Festival TTS. Both the original recordings and the Festival databases are freely available for research and for commertial use.
In the present contribution we start with an overview of the linguistic situation of Luxembourg. We then describe specificities of spoken and written L{\"e
The present contribution aims at increasing our understanding of automatic speech recognition (ASR) errors involving frequent homophone or almost homophone words by confronting them to perceptual results. The long-term aim is to improve acoustic modelling of these items to reduce automatic transcription errors. A first question of interest addressed in this paper is whether homophone words such as et (and); and est (to be), for which ASR systems rely on language model weights, can be discriminated in a perceptual transcription test with similar n-gram constraints. A second question concerns the acoustic separability of the two homophone words using appropriate acoustic and prosodic attributes. The perceptual test reveals that even though automatic and perceptual errors correlate positively, human listeners deal with local ambiguity more efficiently than the ASR system in conditions which attempt to approximate the information available for decision for a 4-gram language model. The corresponding acoustic analysis shows that the two homophone words may be distinguished thanks to some relevant acoustic and prosodic attributes. A first experiment in automatic classification of the two words using data mining techniques highlights the role of the prosodic (duration and voicing) and contextual information (pauses co-occurrence) in distinguishing the two words. Current results, even though preliminary, suggests that new levels of information, so far unexplored in pronunciations modelling for ASR, may be considered in order to efficiently factorize the word variants observed in speech and to improve the automatic speech transcription.
Some big languages like English are spoken by a lot of people whose mother tongues are different from. Their second languages often have not only distinct accent but also different lexical and syntactic characteristics. Speech recognition performance is severely affected when the lexical, syntactic, or semantic characteristics in the training and recognition tasks differ. Language model of a speech recognition system is usually trained with transcribed speech data or text data collected in English native countries, therefore, speech recognition performance is expected to be degraded by mismatch of lexical and syntactic characteristics between native speakers and second language speakers as well as the distinction between their accents. The aim of language model adaptation is to exploit specific, albeit limited, knowledge about the recognition task to compensate for mismatch of the lexical, syntactic, or semantic characteristics. This paper describes whether the language model adaptation is effective for compensating for the mismatch between the lexical, syntactic, or semantic characteristics of native speakers and second language speakers.
This paper describes the use of the CasSys platform in order to achieve the chunking of conversational speech transcripts by means of cascades of Unitex transducers. Our system is involved in the EPAC project of the French National agency of Research (ANR). The aim of this project is to develop robust methods for the annotation of audio/multimedia document collections which contains conversational speech sequences such as TV or radio programs. At first, this paper presents the EPAC project and the adaptation of a former chunking system (Romus) which was developed in the restricted framework of dedicated spoken man-machine dialogue. Then, it describes the problems that are arising due to 1) spontaneous speech disfluencies and 2) errors for the previous stages of processing (automatic speech recognition and POS tagging).
Our paper focuses on the gain which can be achieved on human transcription of spontaneous and prepared speech, by using the assistance of an ASR system. This experiment has shown interesting results, first about the duration of the transcription task itself: even with the combination of prepared speech + ASR, an experimented annotator needs approximately 4 hours to transcribe 1 hours of audio data. Then, using an ASR system is mostly time-saving, although this gain is much more significant on prepared speech: assisted transcriptions are up to 4 times faster than manual ones. This ratio falls to 2 with spontaneous speech, because of ASR limits for these data. Detailed results reveal interesting correlations between the transcription task and phenomena such as Word Error Rate, telephonic or non-native speech turns, the number of fillers or propers nouns. The latter make spelling correction very time-consuming with prepared speech because of their frequency. As a consequence, watching for low averages of proper nouns may be a way to detect spontaneous speech.
In this paper we present our recent work to develop phonemic and syllabic inventories for Castilian Spanish based on the C-ORAL-ROM corpus, a spontaneous spoken resource with varying degrees of naturalness and in different communicative contexts. These inventories have been developed by means of a phonemic and syllabic automatic transcriptor whose output has been assessed by manually reviewing most of the transcriptions. The inventories include absolute frequencies of occurrence of the different phones and syllables. These frequencies have been contrasted against an inventory extracted from a comparable textual corpus, finding evidence that the available inventories, based mainly on text, do not provide an accurate description of spontaneously spoken Castilian Spanish.
Phonetic segmentation is the procedure which is used in many applications of speech processing, both as a subpart of automated systems or as the tool for an interactive work. In this paper we are presenting the latest development in our tool of automated phonetic segmentation. The tool is based on HMM forced alignment realized by publicly available HTK toolkit. It is implemented into the environment of Praat application and it can be used with several optional settings. The tool is designed for segmentation of the utterances with known orthographic records while phonetic contents are obtained from the pronunciation lexicon or from orthoepic record generated by rules for new unknown words. Second part of this paper describes small Czech reference database precisely labelled on phonetic level which is supposed to be used for the analysis of the accuracy of automatic phonetic segmentation.
The paper describes a method of word phonosemantics estimation. We treat phonosemantics as a subconscious emotional perception of word sounding independent on the word meaning. The method is based on the data about emotional perception of sounds obtained from a number of respondents. A program estimates words emotional characteristics using the data about sounds. The program output was compared with humans judgment. The results of the experiments showed that in most cases computer description of a word based on phonosemantic calculations is similar with our own impressions of the words sounding. On the other hand the word meaning dominates in emotional perception of the word and phonosemantic part comes out for the words with unknown meaning.
The metadata management system for speech corpora memasysco has been developed at the Institut f{\"u
This article is interested in the problem of the linguistic content of a speech corpus. Depending on the target task, the phonological and linguistic content of the corpus is controlled by collecting a set of sentences which covers a preset description of phonological attributes under the constraint of an overall duration as small as possible. This goal is classically achieved by greedy algorithms which however do not guarantee the optimality of the desired cover. In recent works, a lagrangian-based algorithm, called LamSCP, has been used to extract coverings of diphonemes from a large corpus in French, giving better results than a greedy algorithm. We propose to keep comparing both algorithms in terms of the shortest duration, stability and robustness by achieving multi-represented diphoneme or triphoneme covering. These coverings correspond to very large scale optimization problems, from a corpus in English. For each experiment, LamSCP improves the greedy results from 3.9 to 9.7 percent.
Speech synthesis by unit selection requires the segmentation of a large single speaker high quality recording. Automatic speech recognition techniques, e.g. Hidden Markov Models (HMM), can be optimised for maximum segmentation accuracy. This paper presents the results of tuning such a phoneme segmentation system. Firstly, using no text transcription, the design of an HMM phoneme recogniser is optimised subject to a phoneme bigram language model. Optimal performance is obtained with triphone models, 7 states per phoneme and 5 Gaussians per state, reaching 94.4{\%} phoneme recognition accuracy with 95.2{\%} of phoneme boundaries within 70 ms of hand labelled boundaries. Secondly, using the textual information modeled by a multi-pronunciation phonetic graph built according to errors found in the first step, the reported phoneme recognition accuracy increases to 96.8{\%} with 96.1{\%} of phoneme boundaries within 70 ms of hand labelled boundaries. Finally, the results from these two segmentation methods based on different phonetic graphs, the evaluation set, the hand labelling and the test procedures are discussed and possible improvements are proposed.
Corpus based methods are increasingly used for speech technology applications and for the development of theoretical or computer models of spoken languages. These usages range from unit selection speech synthesis to statistical modeling of speech phenomena like prosody or expressivity. In all cases, these usages require a wide range of tools for corpus creation, labeling, symbolic and acoustic analysis, storage and query. However, if a variety of tools exists for each of these individual tasks, they are rarely integrated into a single platform made available to a large community of researchers. In this paper, we propose IrcamCorpusTools, an open and easily extensible platform for analysis, query and visualization of speech corpora. It is already used for unit selection speech synthesis, for prosody and expressivity studies, and to exploit various corpora of spoken French or other languages.
The output of a speech recognition system is not always ideal for subsequent downstream processing, in part because speakers themselves often make mistakes. A system would accomplish speech reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and content-preserving English, the message that the speaker intended to convey. These cleaner speech transcripts would allow for more accurate language processing as needed for NLP tasks such as machine translation and conversation summarization, which often rely on grammatical input. Recognizing that supervised statistical methods to identify and transform ill-formed areas of the transcript will require richly labeled resources, we have built the Spontaneous Speech Reconstruction corpus. This small corpus of reconstructed and aligned conversational telephone speech transcriptions for the Fisher conversational telephone speech corpus (Strassel and Walker, 2004) was annotated on several levels including string transformations and predicate-argument structure, and will be shared with the linguistic research community.
Spock is an open source tool for the easy deployment of time-aligned corpora. It is fully web-based, and has very limited server-side requirements. It allows the end-user to search the corpus in a text-driven manner, obtaining both the transcription and the corresponding sound fragment in the result page. Spock has an administration environment to help manage the sound files and their respective transcription files, and also provides statistical data about the files at hand. Spock uses a proprietary file format for storing the alignment data but the integrated admin environment allows you to import files from a number of common file formats. Spock is not intended as a transcriber program: it is not meant as an alternative to programs such as ELAN, Wavesurfer, or Transcriber, but rather to make corpora created with these tools easily available on line. For the end user, Spock provides a very easy way of accessing spoken corpora, without the need of installing any special software, which might make time-aligned corpora corpora accessible to a large group of users who might otherwise never look at them.
There are conflicting views in the literature as to the role of listener-adaptive processes in language production in general and articulatory reduction in particular. We present two novel pieces of corpus evidence that corroborate the hypothesis that non-lexical variation of durations is related to the speed of retrieval of stored motor code chunks and durational reduction is the result of facilitatory priming.
Question Answering systems are systems that enable the user to ask questions in natural language and to also receive an answer in natural language. Most existing systems, however, are constructed for the English language, and it is not clear in how far these approaches are also applicable to other languages. A richer morphology, greater syntactic variability, and smaller fraction of webpages available in the language are just some issues that complicate the construction of systems for German. In this paper, we present a modular Question Answering System for German which uses several morphological resources to increase recall. Nouns are converted into verbs, verbs into nouns, and the tenses of verbs are modified. We use a web search engine as a back end to allow for open-domain Question Answering. A POS-tagger is employed to identify answer candidates which are then filtered and tiled. The system is shown to achieve a higher recall than other systems for German.
This paper deals with the treatment of constructed neologisms in a machine translation system. It focuses on a particular issue in Romance languages: relational adjectives and the role they play in prefixation. Relational adjectives are formally adjectives but are semantically linked to their base-noun. In prefixation processes, the prefix is formally attached to the adjective, but its semantic value(s) is applied to the semantic features of the base-noun. This phenomenon has to be taken into account by any morphological analyser or generator. Moreover, in a contrastive perspective, the possibilities of creating adjectives out of nouns are not the same in every language. We present the special mechanism we put in place to deal with this type of prefixation, and the automatic method we used to extend lexicons, so that they can retrieve the base-nouns of prefixed relational adjectives, and improve the translation quality.
In this paper, we discuss lemma identification in Japanese morphological analysis, which is crucial for a proper formulation of morphological analysis that benefits not only NLP researchers but also corpus linguists. Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings. The mapping from a writing form onto a lemma is important in linguistic analysis of corpora. The current study focuses on disambiguation of heteronyms, words with the same writing form but with different word forms. To resolve heteronym ambiguity, we make use of goshu information, the classification of words based on their origin. Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information. Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.
Morphologically rich languages pose a challenge to the annotators of treebanks with respect to the status of orthographic (space-delimited) words in the syntactic parse trees. In such languages an orthographic word may carry various, distinct, sorts of information and the question arises whether we should represent such words as a sequence of their constituent morphemes (i.e., a Morpheme-Based annotation strategy) or whether we should preserve their special orthographic status within the trees (i.e., a Word-Based annotation strategy). In this paper we empirically address this challenge in the context of the development of Language Resources for Modern Hebrew. We compare and contrast the Morpheme-Based and Word-Based annotation strategies of pronominal clitics in Modern Hebrew and we show that the Word-Based strategy is more adequate for the purpose of training statistical parsers as it provides a better PP-attachment disambiguation capacity and a better alignment with initial surface forms. Our findings in turn raise new questions concerning the interaction of morphological and syntactic processing of which investigation is facilitated by the parallel treebank we made available.
The development of natural language processing (NLP) components is resource-intensive and therefore justifies exploring ways of reducing development time and effort when building NLP components. This paper addresses the experimental fast-tracking of the development of finite-state morphological analysers for Xhosa, Swati and (Southern) Ndebele by using an existing morphological analyser prototype for Zulu. The research question is whether fast-tracking is feasible across the language boundaries between these closely related varieties. The objective is a thorough assessment of recognition rates yielded by the Zulu morphological analyser for the three related languages. The strategy is to use techniques comprising several cycles of the following steps: applying the analyser to corpus data from all languages, identifying failures, and implementing the respective changes in the analyser. Tests show that the high degree of shared typological properties and formal similarities among the Nguni varieties warrants a modular fast-tracking approach. Word forms recognized by the Zulu analyser were mostly adequately interpreted. Therefore, the focus lies on providing adaptations based on failure output analysis for each language. As a result, the development of analysers for Xhosa, Swati and Ndebele is considerably faster than the creation of the Zulu prototype. The paper concludes with comments on the feasibility of the experiment, and the results of the evaluation.
This paper describes methods used for generating a morphological lexicon of organization entity names in Croatian. This resource is intended for two primary tasks: template-based natural language generation and named entity identification. The main problems concerning the lexicon generation are high level of inflection in Croatian and low linguistic quality of the primary resource containing named entities in normal form. The problem is divided into two subproblems concerning single-word and multi-word expressions. The single-word problem is solved by training a supervised learning algorithm called linear successive abstraction. With existing common language morphological resources and two simple hand-crafted rules backing up the algorithm, accuracy of 98.70{\%} on the test set is achieved. The multi-word problem is solved through a semi-automated process for multi-word entities occurring in the first 10,000 named entities. The generated multi-word lexicon will be used for natural language generation only while named entity identification will be solved algorithmically in forthcoming research. The single-word lexicon is capable of handling both tasks.
This paper reports the principles behind designing a tagset to cover Russian morphosyntactic phenomena, modifications of the core tagset, and its evaluation. The tagset is based on the MULTEXT-East framework, while the decisions in designing it were aimed at achieving a balance between parameters important for linguists and the possibility to detect and disambiguate them automatically. The final tagset contains about 500 tags and achieves about 95{\%} accuracy on the disambiguated portion of the Russian National Corpus. We have also produced a test set that can be shared with other researchers.
In this paper we deal with a recently developed large Czech MWE database containing at the moment 160,000 MWEs (treated as lexical units). It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others. We describe the structure of the database and compare the built MWEs database with the corpus data from Czech National Corpus SYN2000 (approx. 100 mil. tokens) and present results of this comparison in the paper. These MWEs have not been obtained from the corpus since their frequencies in it are rather low. To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs. We also discuss exploitation of the database for working out a more adequate tagging and lemmatization. The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units, i.e. to make tagging and lemmatization more adequate.
In this paper, we define the task of Number Identification in natural context. We present and validate a language-independent semi-automatic approach to quickly building a gold standard for evaluating number identification systems by exploiting hand-aligned parallel data. We also present and extensively evaluate a robust rule-based system for number identification in natural context for Arabic for a variety of number formats and types. The system is shown to have strong performance, achieving, on a blind test, a 94.8{\%} F-score for the task of correctly identifying number expression spans in natural text, and a 92.1{\%} F-score for the task of correctly determining the core numerical value.
This paper presents some preliminary results of our dependency parser for Thai. It is part of an ongoing project in developing a syntactically annotated Thai corpus. The parser has been trained and tested by using the complete part of the corpus. The parser achieves 83.64{\%} as the root accuracy, 78.54{\%} as the dependency accuracy and 53.90{\%} as the complete sentence accuracy. The trained parser will be used as a preprocessing step in our corpus annotation workflow in order to accelerate the corpus development.
In many applications of natural language processing (NLP) grammatically tagged corpora are needed. Thus Part of Speech (POS) Tagging is of high importance in the domain of NLP. Many taggers are designed with different approaches to reach high performance and accuracy. These taggers usually deal with inter-word relations and they make use of lexicons. In this paper we present a new tagging algorithm with a hybrid approach. This algorithm combines the features of probabilistic and rule-based taggers to tag Persian unknown words. In contrast with many other tagging algorithms this algorithm deals with the internal structure of the words and it does not need any built in knowledge. The introduced tagging algorithm is domain independent because it uses morphological rules. In this algorithm POS tags are assigned to unknown word with a probability which shows the accuracy of the assigned POS tag. Although this tagger is proposed for Persian, it can be adapted to other languages by applying their morphological rules.
We present a universal Parts-of-Speech (POS) tagset framework covering most of the Indian languages (ILs) following the hierarchical and decomposable tagset schema. In spite of significant number of speakers, there is no workable POS tagset and tagger for most ILs, which serve as fundamental building blocks for NLP research. Existing IL POS tagsets are often designed for a specific language; the few that have been designed for multiple languages cover only shallow linguistic features ignoring linguistic richness and the idiosyncrasies. The new framework that is proposed here addresses these deficiencies in an efficient and principled manner. We follow a hierarchical schema similar to that of EAGLES and this enables the framework to be flexible enough to capture rich features of a language/ language family, even while capturing the shared linguistic structures in a methodical way. The proposed common framework further facilitates the sharing and reusability of scarce resources in these languages and ensures cross-linguistic compatibility.
In this paper, we report our work on the creation of a number of lexical resources that are crucial for an interlingua based MT from English to other languages. These lexical resources are in the form of sub-categorization frames, verb knowledge bases and rule templates for establishing semantic relations and speech act like attributes. We have created these resources over a long period of time from Oxford Advanced Learners Dictionary (OALD) [1], VerbNet [2], Princeton WordNet 2.1 [3], LCS database [4], Penn Tree Bank [5], and XTAG lexicon [6]. On the challenging problem of generating interlingua from domain and structure unrestricted English sentences, we are able to demonstrate that the use of these lexical resources makes a difference in terms of accuracy figures.
In the field of Natural Language Processing, in order to work out a thematic representation system of general knowledge, methods relying on thesaurus have been used for about twenty years. A thesaurus consists of a set of concepts which define a generating system of a vector space modelling general knowledge. These concepts, often organized in a treelike structure, constitute a fundamental, but completely fixed tool. Even if the concepts evolve (we think for example of the technical fields), a thesaurus as for it can evolve only at the time of a particularly heavy process, because it requires the collaboration of human experts. After detailing the characteristics which a generating system of the vector space of knowledge modelling must have, we define the basic notions. Basic notions, whose construction is initially based on the concepts of a thesaurus, constitute another generating system of this vector space. We then approach the determination of the acceptions expressing the basic notions. Lastly, we clarify how, being freed from the concepts of the thesaurus, the basic notions evolve progressively with the analysis of new texts by an iterative process.
Chinese writing system is not only used by Chinese but also used by Japanese. The motivation of this paper is to extend the architecture of Hantology which describes the features of Chinese writing system to integrate Japan Kanji and Chinese characters into the same ontology. The problem is Chinese characters adopted by Japan have been changed, thus, the modification of the original architecture of Hantology is needed. A extended architecture consists orthographic, pronunciation, sense and derived lexicon dimensions. is proposed in this paper. The contribution of this study is that the extension architecture of Hantology provides a platform to analyze the variation of Chinese characters used in Japan. The analytic results of variation for a specific Kanji can be integrated into Hantology, so it is easier to study the variation of Chinese characters systematically
This paper describes the interaction among language resources for an adequate concept annotation of domain texts in several languages. The architecture includes domain ontology, domain texts, language specific lexicons, regular grammars and disambiguation rules. Ontology plays a central role in the architecture. We assume that it represents the meaning of the terms in the lexicons. Thus, the lexicons for the languages of the project (http://www.lt4el.eu/ - the LT4eL (Language Technology for eLearning) project is supported by the European Community under the Information Society and Media Directorate, Learning and Cultural Heritage Unit.) are constructed on the base of the ontology. The grammars and disambiguation rules facilitate the annotation of the text with concepts from the ontology. The established in this way relation between ontology and text supports different searches for content in the annotated documents. This is considered the preparatory phase for the integration of a semantic search facility in Learning Management Systems. The implementation and performance of this search are discussed in the context of related work as well as other types of searches. Also the results from some preliminary steps towards evaluation of the concept-based and text-based search are presented.
This paper proposes a distributional model of word use and word meaning which is derived purely from a body of text, and then applies this model to determine whether certain words are used in or out of context. We suggest that we can view the contexts of words as multinomially distributed random variables. We illustrate how using this basic idea, we can formulate the problem of detecting whether or not a word is used in context as a likelihood ratio test. We also define a measure of semantic relatedness between a word and its context using the same model. We assume that words that typically appear together are related, and thus have similar probability distributions and that words used in an unusual way will have probability distributions which are dissimilar from those of their surrounding context. The relatedness of a word to its context is based on Kullback-Leibler divergence between probability distributions assigned to the constituent words in the given sentence. We employed our methods on a defense-oriented application where certain words are substituted with other words in an intercepted communication.
We describe an automatic projection algorithm for transferring frame-semantic information from English to Italian texts as a first sep towards the creation of Italian FrameNet. Given an English text with frame information and its Italian translation, we project the annotation in four steps: first the Italian text is parsed, then English-Italian alignment is automatically carried out at word level, then we extract the semantic head for every annotated constituent on the English corpus side and finally we project annotation from English to Italian using aligned semantic heads as bridge. With our work, we point out typical features of the Italian language as regards frame-semantic annotation, in particular we describe peculiarities of Italian that at the moment make the projection task more difficult than in the above-mentioned examples. Besides, we created a gold standard with 987 manually annotated sentences to evaluate the algorithm.
We present the results of an agreement task carried out in the framework of the KNOW Project and consisting in manually annotating an agreement sample totaling 50 sentences extracted from the SenSem corpus. Diambiguation was carried out for all nouns, proper nouns and adjectives in the sample, all of which were assigned EuroWordNet (EWN) synsets. As a result of the task, Spanish WN has been shown to exhibit 1) lack of explanatory clarity (it does not define word meanings, but glosses and examplifies them instead; it does not systematically encode metaphoric meanings, either); 2) structural inadequacy (some words appear as hyponyms of another sense of the same word; sometimes there even coexist in Spanish WN a general sense and a specific one related to the same concept, but with no structural link in between; hyperonymy relationships have been detected that are likely to raise doubts to human annotators; there can even be found cases of auto-hyponymy); 3) cross-linguistic inconsistency (there exist in English EWN concepts whose lexical equivalent is missing in Spanish WN; glosses in one language more often than not contradict or diverge from glosses in another language).
The work described in this paper aims to enrich the noun classifications of an existing database of lexical resources (de Matos and Ribeiro, 2004) adding missing information such as semantic relations. Relations are extracted from an annotated and manually corrected corpus. Semantic relations added to the database are retrieved from noun-appositive relations found in the corpus. The method uses clustering to generate labeled sets of words with hypernym relations between set label and set elements.
This paper describes the development of a written corpus of argumentative reasoning. Arguments in the corpus have been analysed using state of the art techniques from argumentation theory and have been marked up using an open, reusable markup language. A number of the key challenges enountered during the process are explored, and preliminary observations about features such as inter-coder reliability and corpus statistics are discussed. In addition, several examples are offered of how this kind of language resource can be used in linguistic, computational and philosophical research, and in particular, how the corpus has been used to initiate a programme investigating the automatic detection of argumentative structure.
In this paper, we present a linguistic resource that annotates event structures in texts. We consider an event structure as a collection of events that interact with each other in a given situation. We interpret the interactions between events as event relations. In this regard, we propose and annotate a set of six relations that best capture the concept of event structure. These relations are: subevent, reason, purpose, enablement, precedence and related. A document from this resource can encode multiple event structures and an event structure can be described across multiple documents. In order to unify event structures, we also annotate inter- and intra-document event coreference. Moreover, we provide methodologies for automatic discovery of event structures from texts. First, we group the events that constitute an event structure into event clusters and then, we use supervised learning frameworks to classify the relations that exist between events from the same cluster
This paper discusses findings of a frame-based contrastive text analysis, using the large-scale and precise descriptions of semantic frames provided by the FrameNet project (Baker, 2006; Fillmore, 2006). It points out that even though the existing FrameNet methodology allows us to compare languages at a more detailed level than previous studies (e.g. Talmy, 2003; Slobin, 2004), in order to investigate how different languages encode the same events, it is also necessary to make cross-references to grammatical constructions rather than limiting ourselves to analyzing the semantics of frame-bearing predicates. Based on a contrastive text analysis of an English-Japanese aligned parallel corpus and on the lexicon-building project of Japanese FrameNet (Ohara et al., 2006), the paper attempts to represent interactions between lexical units and constructions of Japanese sentences in terms of the combined lexicon and constructicon, currently being developed in FrameNet (Fillmore, 2006). By applying the idea to the analysis of Japanese in Japanese FrameNet, it is hoped that the study will give support to working out the details of the new FrameNet directions.
Notwithstanding its acknowledged richness, the SIMPLE semantic model does not offer the representational vocabulary for encoding some conceptual links holding between events and their participants and among co-participants in events. Although critical for boosting performance in many NLP application tasks, such deep lexical information is therefore only partially encoded in the SIMPLE-CLIPS Italian semantic database. This paper reports on the enrichment of the SIMPLE relation set by some expressive means, namely semantic relations, borrowed from the EuroWordNet model and their implementation in the SIMPLE-CLIPS lexicon. The original situation existing in the database, as to the expression of this type of information is described and the loan descriptive vocabulary presented. Strategies based on the exploitation of the source lexicon data were adopted to induce new information: a wide range of semantic - but also syntactic - information was investigated for singling out word senses candidate to be linked by the new relations. The lexicon enrichment by 5,000 new relations instantiated so far has therefore been carried out as a largely automated, low-effort and cost-free process, with no heavy human intervention. The redundancy set off by such an extension of information is being addressed by the implementation of inheritance in the SIMPLE-CLIPS database (Del Gratta et al., 2008).
This paper presents the application of inheritance to the formal taxonomy (is-a) of a semantically rich Language Resource based on the Generative Lexicon theory, SIMPLE-CLIPS. The aim is to lighten the representation of its semantic layer by reducing the number of encoded relations. A prediction calculation on the impact of introducing inheritance regarding space occupancy is carried out, yielding a significant space reduction of 22{\%}. This is corroborated by its actual application, which reduces the number of explicitly encoded relations in this lexicon by 18.4{\%}. Later on, we study the issues that inheritance poses to the Language Resources, and discuss sensitive solutions to tackle each of them, including examples. Finally, we present a discussion on the application of inheritance, from which two side effect advantages arise: consistency enhancement and inference capabilities.
The identification of class instances within unstructured text for either the purposes of Ontology population or semantic annotation are usually limited to term mentions of Proper Noun and Personal Noun or fixed Key Phrases within Text Analytics or Ontology based Information Extraction(OBIE) applications. These systems do not generalize to cope with compound nominal classes of multi word expressions. Computational Linguistics approaches involving deep analysis tend to suffer from idiomaticity and overgeneration problems while the shallower words with spaces approach frequently employed in Information Extraction(IE) and Industrial Text Analytics systems lacks flexibility and is prone to lexical proliferation. We outline a representation for encoding light linguistic features of Compound Nominal term mentions of Concepts within an Ontology as well as a lightweight semantic annotator which complies the above linguistic information into efficient Dictionary formats to drive large scale identification and semantic annotation of the aforementioned concepts.
This paper presents our work on the detection of temporal information in web pages. The pages examined within the scope of this study were taken from the tourism sector and the temporal information in question is thus particular to this area. The differences that exist between extraction from plain textual data and extraction from the web are brought to light. These differences mainly concern the spatial arrangement of the text, the use of punctuation and the respect of traditional syntactic rules. The temporal expressions to be extracted are classified into two kinds: temporal information that concerns one particular event and repetitive temporal information. We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers. First evaluations have shown promising results. Since the visual structure of a web page is very important and often informs the user before he has even read the text, a semiotic study is also presented in this paper.
WCTAnalyze is a tool for storing, accessing and visually analyzing huge collections of temporally indexed data. It is motivated by applications in media analysis, business intelligence etc. where higher level analysis is performed on top of linguistically and statistically processed unstructured textual data. WCTAnalyze combines fast access with economically storage behaviour and appropriates a lot of built in visualization options for result presentation in detail as well as in contrast. So it enables an efficient and effective way to explore chronological text patterns of word forms, their co-occurrence sets and co-occurrence set intersections. Digging deep into co-occurrences of the same semantic or syntactic describing wordforms, some entities can be recognized as to be temporal related, whereas other differ significantly. This behaviour motivates approaches in interactive discovering events based on co-occurrence subsets.
We are currently developing MiniSTEx, a spatiotemporal annotation system to handle temporal and/or geospatial information directly and indirectly expressed in texts. In the end the aim is to locate all eventualities in a text on a time axis and/or a map to ensure an optimal base for automatic temporal and geospatial reasoning. MiniSTEx was originally developed for Dutch, keeping in mind that it should also be useful for other European languages, and for multilingual applications. In order to meet these desiderata we need the MiniSTEx system to be able to draw the conclusions human readers would also draw, e.g. based on their (spatiotemporal) world knowledge, i.e. the common knowledge such readers share. Therefore, notions like background knowledge, intended audience, and present-day user play a major role in our approach. The world knowledge MiniSTEx uses is contained in interconnected tables in a database. At the moment it is used for Dutch and English. Special attention will be paid to the problems we face when looking at older texts or recent historical or encyclopedic texts, i.e. texts with lots of references to times and locations that are not compatible with our current maps and calendars.
Dating of contents is relevant to multiple advanced Natural Language Processing (NLP) applications, such as Information Retrieval or Question Answering. These could be improved by using techniques that consider a temporal dimension in their processes. To achieve it, an accurate detection of temporal expressions in data sources must be firstly done, dealing with them in an appropriated standard format that captures the time value of the expressions once resolved, and allows reasoning without ambiguity, in order to increase the range of search and the quality of the results to be returned. These tasks are completely necessary for NLP applications if an efficient temporal reasoning is afterwards expected. This work presents a typology of time expressions based on an empirical inductive approach, both from a structural perspective and from the point of view of their resolution. Furthermore, a method for the automatic recognition and resolution of temporal expressions in Spanish contents is provided, obtaining promising results when it is tested by means of an evaluation corpus.
This paper reports on the annotation of all English verbs included in WordNet 2.0 with TimeML event classes. Two annotators assign each verb present in WordNet the most relevant event class capturing most of that verbs meanings. At the end of the annotation process, inter-annotator agreement is measured using kappa statistics, yielding a kappa value of 0.87. The cases of disagreement between the two independent annotations are clarified by obtaining a third, and in some cases, a fourth opinion, and finally each of the 11,306 WordNet verbs is mapped to a unique event class. The resulted annotation is then employed to automatically assign the corresponding class to each occurrence of a finite or non-finite verb in a given text. The evaluation performed on TimeBank reveals an F-measure of 86.43{\%} achieved for the identification of verbal events, and an accuracy of 85.25{\%} in the task of classifying them into TimeML event classes.
In this paper, we present a simple yet efficient automatic system to translate biomedical terms. It mainly relies on a machine learning approach able to infer rewriting rules from pair of terms in two languages. Given a new term, these rules are then used to transform the initial term into its translation. Since conflicting rules may produce different translations, we also use language modeling to single out the best candidate. We report experiments on different language pairs (including Czech, English, French, Italian, German, Portuguese, Spanish and even Russian); our approach yields good results (varying according to the considered languages) and outperforms existing ones for the French-English pair.
We show here the viability of a rapid deployment of a new language pair within the METIS architecture. In order to do it, we have benefited from the approach of our existing Spanish-English system, which is particularly generation intensive. Contrarily to other SMT or EBMT systems, the METIS architecture allows us to forgo parallel texts, which for many language pairs, such as Catalan-English are hard to obtain. In this experiment, we have successfully built a Catalan-English prototype by simply plugging a POS tagger for Catalan and a bilingual Catalan-English dictionary to the English generation part of the system already developed for other language pairs.
In this paper we describe the METIS-II system and its evaluation on each of the language pairs: Dutch, German, Greek, and Spanish to English. The METIS-II system envisaged developing a data-driven approach in which no parallel corpus is required, and in which no full parser or extensive rule sets are needed. We describe evalution on a development test set and on a test set coming from Europarl, and compare our results with SYSTRAN. We also provide some further analysis, researching the impact of the number and source of the reference translations and analysing the results according to test text type. The results are expectably lower for the METIS system, but not at an unatainable distance from a mature system like SYSTRAN.
Statistical Machine Translation (SMT) is based on alignment models which learn from bilingual corpora the word correspondences between source and target language. These models are assumed to be capable of learning reorderings of sequences of words. However, the difference in word order between two languages is one of the most important sources of errors in SMT. This paper proposes a Recursive Alignment Block Classification algorithm (RABCA) that can take advantage of inductive learning in order to solve reordering problems. This algorithm should be able to cope with swapping examples seen during training; it should infer properties that might permit to reorder pairs of blocks (sequences of words) which did not appear during training; and finally it should be robust with respect to training errors and ambiguities. Experiments are reported on the EuroParl task and RABCA is tested using two state-of-the-art SMT systems: a phrased-based and an Ngram-based. In both cases, RABCA improves results.
We report on the evaluation of the Norwegian-English MT prototype system LOGON. The system is rule-based and makes use of well-established frameworks for analysis and generation (LFG and HPSG). Minimal Recursion Semantics is the glue which performs transfer from source to target language and serves as the information vehicle between LFG and HPSG. The project-internal testing uses material from the training data sources from the domain guidebooks for mountain hiking in the summer season in Southern Norway. This testing, involving eight external assessors, yielded 57 {\%} translated sentences, with acceptable fidelity measures, but with less than acceptable fluency measures. Additional test 1: The LOGON system is sensitive to vocabulary, so we were interested to see to what extent the system would be able to carry over to new texts from the same narrow domain. With only 22 {\%} acceptable translations, this test had disappointing results. Additional test 2: Given the grammatical backbone of the system, we found it important to test it on a syntactic test-suite with only known vocabulary. Here, 55 {\%} of the sentences had good translations. The tests show that even within a very narrow semantic domain, vocabulary sensitivity is the most crucial obstacle for this approach.
Parallel corpora are critical resources for machine translation research and development since parallel corpora contain translation equivalences of various granularities. Manual annotation of word {\&} phrase alignment is of significance to provide gold-standard for developing and evaluating both example-based machine translation model and statistical machine translation model. This paper presents the work of word {\&} phrase alignment annotation in the NICT Japanese-Chinese parallel corpus, which is constructed at the National Institute of Information and Communications Technology (NICT). We describe the specification of word alignment annotation and the tools specially developed for the manual annotation. The manual annotation on 17,000 sentence pairs has been completed. We examined the manually annotated word alignment data and extracted translation knowledge from the word {\&} phrase aligned corpus.
As a first step to developing systems that enable non-native speakers to output near-perfect English sentences for given mixed English-Japanese sentences, we propose new approaches for selecting English equivalents by using the number of hits for various contexts in large English corpora. As the large English corpora, we not only used the huge amounts of Web data but also the manually compiled large, high-quality English corpora. Using high-quality corpora enables us to accurately select equivalents, and using huge amounts of Web data enables us to resolve the problem of the shortage of hits that normally occurs when using only high-quality corpora. The types and lengths of contexts used to select equivalents are variable and optimally determined according to the number of hits in the corpora, so that performance can be further refined. Computer experiments showed that the precision of our methods was much higher than that of the existing methods for equivalent selection.
In this paper, we describe our work on building a parallel treebank for a less studied and typologically dissimilar language pair, namely Swedish and Turkish. The treebank is a balanced syntactically annotated corpus containing both fiction and technical documents. In total, it consists of approximately 160,000 tokens in Swedish and 145,000 in Turkish. The texts are linguistically annotated using different layers from part of speech tags and morphological features to dependency annotation. Each layer is automatically processed by using basic language resources for the involved languages. The sentences and words are aligned, and partly manually corrected. We create the treebank by reusing and adjusting existing tools for the automatic annotation, alignment, and their correction and visualization. The treebank was developed within the project supporting research environment for minor languages aiming at to create representative language resources for language pairs dissimilar in language structure. Therefore, efforts are put on developing a general method for formatting and annotation procedure, as well as using tools that can be applied to other language pairs easily.
A wide spectrum of multilingual applications have aligned parallel corpora as their prerequisite. The aim of the project described in this paper is to build a multilingual corpus where all sentences are aligned at very high precision with a minimal human effort involved. The experiments on a combination of sentence aligners with different underlying algorithms described in this paper showed that by verifying only those links which were not recognized by at least two aligners, an error rate can be reduced by 93.76{\%} as compared to the performance of the best aligner. Such manual involvement concerned only a small portion of all data (6{\%}). This significantly reduces a load of manual work necessary to achieve nearly 100{\%} accuracy of alignment.
This paper proposes a method of constructing a dictionary for a pair of languages from bilingual dictionaries between each of the languages and a third language. Such a method would be useful for language pairs for which wide-coverage bilingual dictionaries are not available, but it suffers from spurious translations caused by the ambiguity of intermediary third-language words. To eliminate spurious translations, the proposed method uses the monolingual corpora of the first and second languages, whose availability is not as limited as that of parallel corpora. Extracting word associations from the corpora of both languages, the method correlates the associated words of an entry word with its translation candidates. It then selects translation candidates that have the highest correlations with a certain percentage or more of the associated words. The method has the following features. It first produces a domain-adapted bilingual dictionary. Second, the resulting bilingual dictionary, which not only provides translations but also associated words supporting each translation, enables contextually based selection of translations. Preliminary experiments using the EDR Japanese-English and LDC Chinese-English dictionaries together with Mainichi Newspaper and Xinhua News Agency corpora demonstrate that the proposed method is viable. The recall and precision could be improved by optimizing the parameters.
We present the machine learning framework that we are developing, in order to support explorative search for non-trivial linguistic configurations in low-density languages (languages with no or few NLP tools). The approach exploits advanced existing analysis tools for high-density languages and word-aligned multi-parallel corpora to bridge across languages. The goal is to find a methodology that minimizes the amount of human expert intervention needed, while producing high-quality search and annotation tools. One of the main challenges is the susceptibility of a complex system combining various automatic analysis components to hard-to-control noise from a number of sources. We present systematic experiments investigating to what degree the noise issue can be overcome by (i) exploiting more than one perspective on the target language data by considering multiple translations in the parallel corpus, and (ii) using minimally supervised learning techniques such as co-training and self-training to take advantage of a larger pool of data for generalization. We observe that while (i) does help in the training individual machine learning models, a cyclic bootstrapping process seems to suffer too much from noise. A preliminary conclusion is that in a practical approach, one has to rely on a higher degree of supervision or on noise detection heuristics.
This paper presents an approach to computer-assisted teaching of reading abilities using corpus data. The approach is supported by a set of tools for automatically selecting and classifying texts retrieved from the Internet. The approach is based on a linguistic model of textual cohesion which describes relations between larger textual units that go beyond the sentence level. We show that textual connectors that link such textual units reliably predict different types of texts, such as information and opinion: using only textual connectors as features, an SVM classifier achieves an F-score of between 0.85 and 0.93 for predicting these classes. The tools are used in our project on teaching reading skills in a cognate foreign language (L3) which is cognate to a known foreign language (L2).
This paper addresses the problem of synchronizing movie subtitles, which is necessary to improve alignment quality when building a parallel corpus out of translated subtitles. In particular, synchronization is done on the basis of aligned anchor points. Previous studies have shown that cognate filters are useful for the identification of such points. However, this restricts the approach to related languages with similar alphabets. Here, we propose a dictionary-based approach using automatic word alignment. We can show an improvement in alignment quality even for related languages compared to the cognate-based approach.
In human translation, translators first make draft translations and then modify and edit them. In the case of experienced translators, this process involves the use of wide-ranging expert knowledge, which has mostly remained implicit so far. Describing the difference between draft and final translations, therefore, should contribute to making this knowledge explicit. If we could clarify the expert knowledge of translators, hopefully in a computationally tractable way, we would be able to contribute to the automatic notification of awkward translations to assist inexperienced translators, improving the quality of MT output, etc. Against this backdrop, we have started constructing a corpus that indicates patterns of modification between draft and final translations made by human translators. This paper reports on our progress to date.
This paper describes a solution to lexical transfer as a trade-off between a dictionary and an ontology. It shows its association to a translation tool based on morpho-syntactical parsing of the source language. It is based on the English Roget Thesaurus and its equivalent, the French Larousse Thesaurus, in a computational framework. Both thesaurii are transformed into vector spaces, and all monolingual entries are represented as vectors, with 1,000 components for English and 873 for French. The indexing concepts of the respective thesaurii are the generation families of the vector spaces. A bilingual data structure transforms French entries into vectors in the English space, by using their equivalencies representations. Word sense disambiguation consists in choosing the appropriate vector among these bilingual vectors, by computing the contextualized vector of a given word in its source sentence, wading it in the English vector space, and computing the closest distance to the different entries in the bilingual data structure beginning with the same source string (i.e. French word). The process has been experimented on a 20,000 words extract of a French novel, Le Petit Prince, and lexical transfer results were found quite encouraging with a recall of 71{\%} and a precision of 86{\%}.
Recently the LATL has undertaken the development of a multilingual translation system based on a symbolic parsing technology and on a transfer-based translation model. A crucial component of the system is the lexical database, notably the bilingual dictionaries containing the information for the lexical transfer from one language to another. As the number of necessary bilingual dictionaries is a quadratic function of the number of languages considered, we will face the problem of getting a large number of dictionaries. In this paper we discuss a solution to derive a bilingual dictionary by transitivity using existing ones and to check the generated translations in a parallel corpus. Our first experiments concerns the generation of two bilingual dictionaries and the quality of the entries are very promising. The number of generated entries could however be improved and we conclude the paper with the possible ways we plan to explore.
Recently, there has been an emphasis on creating shared resources for natural language processing applications. This has resulted in the development of high-quality tools and data, which can then be leveraged by the research community as components for novel systems. In this paper, we reuse an open source machine translation framework to create an Arabic-to-English entity translation system. The system first translates known entity mentions using a standard phrase-based statistical machine translation framework, which is then reused to perform name transliteration on unknown mentions. In order to transliterate names more accurately, we introduce an algorithm to augment a names database with name origin and frequency information from existing data resources. Origin information is used to learn name origin classifiers and origin-specific transliteration models, while frequency information is used to select amongst n-best transliteration candidates. This work demonstrates the feasibility and benefit of adapting such data resources and shows how off-the-shelf tools and data resources can be repurposed to rapidly create a system outside their original domain.
Producing machine translation (MT) for the many minority languages in the world is a serious challenge. Minority languages typically have few resources for building MT systems. For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development. For these reasons, our research programs on minority language MT have focused on leveraging to the maximum extent two resources that are available for minority languages: linguistic structure and bilingual informants. All natural languages contain linguistic structure. And although the details of that linguistic structure vary from language to language, language universals such as context-free syntactic structure and the paradigmatic structure of inflectional morphology, allow us to learn the specific details of a minority language. Similarly, most minority languages possess speakers who are bilingual with the major language of the area. This paper discusses our efforts to utilize linguistic structure and the translation information that bilingual informants can provide in three sub-areas of our rapid development MT program: morphology induction, syntactic transfer rule learning, and refinement of imperfect learned rules.
Parallel text is one of the most valuable resources for development of statistical machine translation systems and other NLP applications. The Linguistic Data Consortium (LDC) has supported research on statistical machine translations and other NLP applications by creating and distributing a large amount of parallel text resources for the research communities. However, manual translations are very costly, and the number of known providers that offer complete parallel text is limited. This paper presents a cost effective approach to identify parallel document pairs from sources that provide potential parallel text - namely, sources that may contain whole or partial translations of documents in the source language - using the BITS and Champollion parallel text alignment systems developed by LDC.
As huge quantities of documents have become available, services using natural language processing technologies trained by huge corpora have emerged, such as information retrieval and information extraction. In this paper we verify the usefulness of resource-based, or corpus-based, translation in the aviation domain as a real business situation. This study is important from both a business perspective and an academic perspective. Intuitively, manuals for similar products, or manuals for different versions of the same product, are likely to resemble each other. Therefore, even with only a small training data, a corpus-based MT system can output useful translations. The corpus-based approach is powerful when the target is repetitive. Manuals for similar products, or manuals for different versions of the same product, are real-world documents that are repetitive. Our experiments on translation of manual documents are still in a beginning stage. However, the BLEU score from very small number of training sentences is already rather high. We believe corpus-based machine translation is a player full of promise in this kind of actual business scene.
In this paper, we present HeiNER, the multilingual Heidelberg Named Entity Resource. HeiNER contains 1,547,586 disambiguated English Named Entities together with translations and transliterations to 15 languages. Our work builds on the approach described in (Bunescu and Pasca, 2006), yet extends it to a multilingual dimension. Translating Named Entities into the various target languages is carried out by exploiting crosslingual information contained in the online encyclopedia Wikipedia. In addition, HeiNER provides linguistic contexts for every NE in all target languages which makes it a valuable resource for multilingual Named Entity Recognition, Disambiguation and Classification. The results of our evaluation against the assessments of human annotators yield a high precision of 0.95 for the NEs we extract from the English Wikipedia. These source language NEs are thus very reliable seeds for our multilingual NE translation method.
Word Sense Disambiguation (WSD) is an intermediate task that serves as a means to an end defined by the application in which it is to be used. However, different applications have varying disambiguation needs which should have an impact on the choice of the method and of the sense inventory used. The tendency towards application-oriented WSD becomes more and more evident, mostly because of the inadequacy of predefined sense inventories and the inefficacy of application-independent methods in accomplishing specific tasks. In this article, we present a data-driven method of sense induction, which combines contextual and translation information coming from a bilingual parallel training corpus. It consists of an unsupervised method that clusters semantically similar translation equivalents of source language (SL) polysemous words. The created clusters are projected on the SL words revealing their sense distinctions. Clustered equivalents describing a sense of a polysemous word can be considered as more or less commutable translations for an instance of the word carrying this sense. The resulting sense clusters can thus be used for WSD and sense annotation, as well as for lexical selection in translation applications.
This paper discusses a framework for development of bilingual and multilingual comprehension assistants and presents a prototype implementation of an English-Bulgarian comprehension assistant. The framework is based on the application of advanced graphical user interface techniques, WordNet and compatible lexical databases as well as a series of NLP preprocessing tasks, including POS-tagging, lemmatisation, multiword expressions recognition and word sense disambiguation. The aim of this framework is to speed up the process of dictionary look-up, to offer enhanced look-up functionalities and to perform a context-sensitive narrowing-down of the set of translation alternatives proposed to the user.
For increased speed in developing gigaword language resources for medium resource density languages we integrated several FOSS tools in the HUN* toolkit. While the speed and efficiency of the resulting pipeline has surpassed our expectations, our experience in developing LDC-style resource packages for Uzbek and Kurdish makes clear that neither the data collection nor the subsequent processing stages can be fully automated.
Progress in the Machine Translation (MT) research community, particularly for statistical approaches, is intensely data-driven. Acquiring source language documents for testing, creating training datasets for customized MT lexicons, and building parallel corpora for MT evaluation require translators and non-native speaking analysts to handle large document collections. These collections are further complicated by differences in format, encoding, source media, and access to metadata describing the documents. Automated tools that allow language professionals to quickly annotate, translate, and evaluate foreign language documents are essential to improving MT quality and efficacy. The purpose of this paper is present our research approach to improving MT through pre-processing source language documents. In particular, we will discuss the development and use of MTriage, an application environment that enables the translator to markup documents with metadata for MT parameterization and routing. The use of MTriage as a web-enabled front end to multiple MT engines has leveraged the capabilities of our human translators for creating lexicons from NFW (Not-Found-Word) lists, writing reference translations, and creating parallel corpora for MT development and evaluation.
Voss et al. (2006) analyzed newswire translations of three DARPA GALE Arabic-English MT systems at the segment level in terms of subjective judgmen+F925t scores, automated metric scores, and correlations among these different score types. At this level of granularity, the correlations are weak. In this paper, we begin to reconcile the subjective and automated scores that underlie these correlations by explicitly grounding MT output with its Reference Translation (RT) prior to subjective or automated evaluation. The first two phases of our approach annotate {MT, RT} pairs with the same types of textual comparisons that subjects intuitively apply, while the third phase (not presented here) entails scoring the pairs: (i) automated calculation of MT-RT hits using CMU aligner from METEOR, (ii) an extension phase where our Buckwalter-based Lookup Tool serves to generate six other textual comparison categories on items in the MT output that the CMU aligner does not identify, and (iii) given the fully categorized RT {\&} MT pair, a final adequacy score is assigned to the MT output, either by an automated metric based on weighted category counts and segment length, or by a trained human judge.
Tokenization is one of the initial steps done for almost any text processing task. It is not particularly recognized as a challenging task for English monolingual systems but it rapidly increases in complexity for systems that apply it for different languages. This article proposes a supervised learning approach to perform the tokenization task. The method presented in this article is based on character transitions representation, a representation that allows compound expressions to be recognized as a single token. Compound tokens are identified independent of the character that creates the expression. The method automatically learns tokenization rules from a pre-tokenized corpus. The results obtained using the trainable system show that for Romanian and English a statistical significant improvement is obtained over a baseline system that tokenizes texts on every non-alphanumeric character.
Low-density languages raise difficulties for standard approaches to natural language processing that depend on large online corpora. Using Persian as a case study, we propose a novel method for bootstrapping MT capability for a low-density language in the case where it relates to a higher density variant. Tajiki Persian is a low-density language that uses the Cyrillic alphabet, while Iranian Persian (Farsi) is written in an extended version of the Arabic script and has many computational resources available. Despite the orthographic differences, the two languages have literary written forms that are almost identical. The paper describes the development of a comprehensive finite-state transducer that converts Tajik text to Farsi script and runs the resulting transliterated document through an existing Persian-to-English MT system. Due to divergences that arise in mapping the two writing systems and phonological and lexical distinctions, the system uses contextual cues (such as the position of a phoneme in a word) as well as available Farsi resources (such as a morphological analyzer to deal with differences in the affixal structures and a lexicon to disambiguate the analyses) to control the potential combinatorial explosion. The results point to a valuable strategy for the rapid prototyping of MT packages for languages of similar uneven density.
In this paper we will focus on the lexical-semantic relations in the German wordnet GermaNet. It has been shown that wordnets suffer from the relatively small number of relations between their lexical objects. It is assumed that applications in NLP and IR, in particular those relying on word sense disambiguation, can be boosted by a higher relational density of the lexical resource. We report on research and experiments in the lexical acquisition of a new type of relation from a large annotated German newspaper corpus, i.e. the relation between the verbal head of a predicate and the nominal head of its argument. We investigate how the insertion of instances of this relation into the German wordnet GermaNet affects the overall structure of the wordnet as well as the neighbourhood of the nodes which are connected by an instance of the new relation.
In this paper we present contrastive colour studies done using COMPARA, the largest edited parallel corpus in the world (as far as we know). The studies were the result of semantic annotation of the corpus in this domain. We chose to start with colour because it is a relatively contained lexical category and the subject of many arguments in linguistics. We begin by explaining the criteria involved in the annotation process, not only for the colour categories but also for the colour groups created in order to do finer-grained analyses, presenting also some quantitative data regarding these categories and groups. We proceed to compare the two languages according to the diversity of available lexical items, morphological and syntactic properties, and then try to understand the translation of colour. We end by explaining how any user who wants to do serious studies using the corpus can collaborate in enhancing the corpus and making their semantic annotations widely available as well.
This paper presents three electronic collections of polarity items: (i) negative polarity items in Romanian, (ii) negative polarity items in German, and (iii) positive polarity items in German. The presented collections are a part of a linguistic resource on lexical units with highly idiosyncratic occurrence patterns. The motivation for collecting and documenting polarity items was to provide a solid empirical basis for linguistic investigations of these expressions. Our databe provides general information about the collected items, specifies their syntactic properties, and describes the environment that licenses a given item. For each licensing context, examples from various corpora and the Internet are introduced. Finally, the type of polarity (negative or positive) and the class (superstrong, strong, weak or open) associated with a given item is specified. Our database is encoded in XML and is available via the Internet, offering dynamic and flexible access.
In this paper, we discuss the integration of metaphor information into the RDF/OWL representation of EuroWordNet. First, the lexical database WordNet and its variants are presented. After a brief description of the Hamburg Metaphor Database, examples of its conversion into the RDF/OWL representation of EuroWordNet are discussed. The metaphor information is added to the general EuroWordNet data and the new resulting RDF/OWL structure is shown in LexiRes, a visualization tool developed and adapted for handling structures of ontological and lexical databases. We show how LexiRes can be used to further edit the newly added metaphor information, and explain some problems with this new type of information on the basis of examples.
We address the question of which syntactic representation is best suited for role-semantic analysis of English in the FrameNet paradigm. We compare systems based on dependencies and constituents, and a dependency syntax with a rich set of grammatical functions with one with a smaller set. Our experiments show that dependency-based and constituent-based analyzers give roughly equivalent performance, and that a richer set of functions has a positive influence on argument classification for verbs.
In this paper we present two large-scale verbal lexicons, AnCora-Verb-Ca for Catalan and AnCora-Verb-Es for Spanish, which are the basis for the semantic annotation with arguments and thematic roles of AnCora corpora. In AnCora-Verb lexicons, the mapping between syntactic functions, arguments and thematic roles of each verbal predicate it is established taking into account the verbal semantic class and the diatheses alternations in which the predicate can participate. Each verbal predicate is related to one or more semantic classes basically differentiated according to the four event classes -accomplishments, achievements, states and activities-, and on the diatheses alternations in which a verb can occur. AnCora-Verb-Es contains a total of 1,965 different verbs corresponding to 3,671 senses and AnCora-Verb-Ca contains 2,151 verbs and 4,513 senses. These figures correspond to the total of 500,000 words contained in each corpus, AnCora-Ca and AnCora-Es. The lexicons and the annotated corpora constitute the richest linguistic resources of this kind freely available for Spanish and Catalan. The big amount of linguistic information contained in both resources should be of great interest for computational applications and linguistic studies. Currently, a consulting interface for these lexicons is available at (http://clic.ub.edu/ancora/).
WordNet has been used extensively as a resource for the Word Sense Disambiguation (WSD) task, both as a sense inventory and a repository of semantic relationships. Recently, we investigated the possibility to use it as a resource for the Geographical Information Retrieval task, more specifically for the toponym disambiguation task, which could be considered a specialization of WSD. We found that it would be very useful to assign to geographical entities inWordNet their coordinates, especially in order to implement geometric shapebased disambiguation methods. This paper presents Geo-WordNet, an automatic annotation of WordNet with geographical coordinates. The annotation has been carried out by extracting geographical synsets from WordNet, together with their holonyms and hypernyms, and comparing them to the entries in the Wikipedia-World geographical database. A weight was calculated for each of the candidate annotations, on the basis of matches found between the database entries and synset gloss, holonyms and hypernyms. The resulting resource may be used in Geographical Information Retrieval related tasks, especially for toponym disambiguation.
This paper is motivated by the demand for more linguistic resources for the study of languages and the improvement of those already existing. The first step in our work is the selection of the most significant frames in the English FrameNet according to a representative medical corpus. These frames were subsequently attached to different EuroWordNet synsets and translated into Spanish. Results show how the translation was made with high accuracy (95.9 {\%} of correct words). In addition to that, the original English lexical units were augmented with new units by 120{\%}
We propose two general and robust methods for enriching resources annotated in the Frame Semantic paradigm with syntactic dependency graphs, which can provide useful additional information for applications such as semantic role labeling methods. One method incorporates information of a dependency parser, while the other one assumes the resource to be based on a treebank and uses dependency graphs converted from phrase structure trees. Coverage and accuracy of the methods are evaluated on the English FrameNet and German SALSA corpora. It is shown that large proportions of those resources can be accurately enriched by mapping their annotations onto dependency graphs. Failures to do so are found to be largely due to parser errors and can therefore be seen as an indicator of incorrect parses, which helps to improve parse selection. The remaining failures are analyzed and an outlook on ways of improving the results by adaptation to specific resources is given.
Princeton WordNet (WN.Pr) lexical database has motivated efficient compilations of bulky relational lexicons since its inception in the 1980{\'{}}s. The EuroWordNet project, the first multilingual initiative built upon WN.Pr, opened up ways of building individual wordnets, and inter-relating them by means of the so-called Inter-Lingual-Index, an unstructured list of the WN.Pr synsets. Other important initiative, relying on a slightly different method of building multilingual wordnets, is the MultiWordNet project, where the key strategy is building language specific wordnets keeping as much as possible of the semantic relations available in the WN.Pr. This paper, in particular, stresses that the additional advantage of using WN.Pr lexical database as a resource for building wordnets for other languages is to explore possibilities of implementing an automatic procedure to map the WN.Pr conceptual relations as hyponymy, co-hyponymy, troponymy, meronymy, cause, and entailment onto the lexical database of the wordnet under construction, a viable possibility, for those are language-independent relations that hold between lexicalized concepts, not between lexical units. Accordingly, combining methods from both initiatives, this paper presents the ongoing implementation of the WN.Br lexical database and the aforementioned automation procedure illustrated with a sample of the automatic encoding of the hyponymy and co-hyponymy relations.
In this paper we present the Cast3LB-CoNLL-SemRol corpus, currently the only corpus of Spanish annotated with dependency syntax and semantic roles, and the tools that have been trained on the corpus: an ensemble of parsers and two dependency-based semantic role labelers that are the only semantic role labelers based on dependency syntax available for Spanish at this moment. One of the systems uses information from gold standard syntax, whereas the other one uses information from predicted syntax. The results of the first system (86 F1) are comparable to current state of the art results for constituent-based semantic role labeling of Spanish. The results of the second are 11 points lower. This work has been carried out as part of the project T{\'e}cnicas semiautom{\'a}ticas para el etiquetado de roles sem{\'a}nticos en corpus del espa{\~n}ol.
The paper reports on completed work aimed at the creation of a resource, namely, the Greek Textual Entailment Corpus (GTEC) that is appropriate for guiding training and evaluation of a system that recognizes Textual Entailment in Greek texts. The corpus of textual units was collected in view of a range of NLP applications, where semantic interpretation is of paramount importance, and it was manually annotated at the level of Textual Entailment. Moreover, a number of linguistic annotations were also integrated that were deemed useful for prospect system developers. The critical issue was the development of a final resource that is re-usable and adaptable to different NLP systems, in order to either enhance their accuracy or to evaluate their output. We are hereby focusing on the methodological issues underpinning data selection and annotation. An initial approach towards the development of a system catering for the automatic Recognition of Textual Entailment in Greek is also presented and preliminary results are reported.
We describe various syntactic and semantic conditions for finding abstractnouns which refer to concepts of adjectives from a text, in an attempt to explore the creation of a thesaurus from text. Depending on usages, six kinds of syntactic patterns are shown. In the syntactic and semantic conditions an omission of an abstract noun is mainly used, but in addition, various linguistic clues are needed. We then compare our results with synsets of Japanese WordNet. From a viewpoint of Japanese WordNet, the degree of agreement of ?Attribute? between our data and Japanese WordNet was 22{\%}. On the other hand, the total number of differences of obtained abstract nouns was 267. From a viewpoint of our data,the degree of agreement of abstract nouns between our data and Japanese WordNet was 54{\%}.
In the few last years, due to the increasing importance of the web, both computational tools and resources need to be more and more visible and easily accessible to a vast community of scholars, students and researchers. Furthermore, high quality lexical resources are crucially required for a wide range of HLT-NLP applications, among which word sense disambiguation. Vast and consistent electronic lexical resources do exist which can be further enhanced and enriched through their linking and integration. An ILC project dealing with the link of two large lexical semantic resources for the Italian language, namely ItalWordNet and PAROLE-SIMPLE-CLIPS, fits this trend. Concrete entities were already linked and this paper addresses the semi-automatic mapping of events and abstract entities. The lexical models of the two resources, the mapping strategy and the tool that was implemented to this aim are briefly outlined. Special focus is put on the results of the linking process: figures are reported and examples are given which illustrate both the linking and harmonization of the resources but also cases of discrepancies, mainly due to the different underlying semantic models.
In this paper we present the procedure we followed to develop the Italian Super Sense Tagger. In particular, we adapted the English SuperSense Tagger to the Italian Language by exploiting a parallel sense labeled corpus for training. As for English, the Italian tagger uses a fixed set of 26 semantic labels, called supersenses, achieving a slightly lower accuracy due to the lower quality of the Italian training data. Both taggers accomplish the same task of identifying entities and concepts belonging to a common set of ontological types. This parallelism allows us to define effective methodologies for a broad range of cross-language knowledge acquisition tasks
Building a Linguistic Resource (LR) is a task requiring a huge quantitative of means, human resources and funds. Though finalization of the development phase and assessment of the produced resource, necessarily require human involvement, a computer aided process for building the resources initial structure would greatly reduce the overall effort to be undertaken. We present here a novel approach for automatizing the process of building structured (possibly multilingual) LRs, starting from already available LRs and exploiting simple vocabularies of synonyms and/or translations for different languages. A simple algorithm for clustering terms, according to their shared senses, is presented in two versions, both for separating flat list of synonyms and flat lists of translations. The algorithm is then motivated against two possible exploitations: reducing the cost for producing new LRs, and linguistically enriching the content of existing semantic resources, like SW ontologies and knowledge bases. Empirical results are provided for two experimental setups: automatic term clustering for English synonyms list, and for Italian translations of English terms
This paper discusses the use of computational linguistic technology to extract definitions from a large corpus of German court decisions. We present a corpus-based survey of definition structures used in this kind of document. We then evaluate the results of a definition extraction system that uses patterns identified in this survey to extract from dependency parsed text. We show how an automatically induced ranking function improves the quality of the search results of this system, and we discuss methods for the acquisition of further extraction rules.
The paper describes the project held within Russian National Corpus (http://www.ruscorpora.ru). Beside such obligatory constituents of a linguistic corpus as POS (parts of speech) and morphological tagging RNC contains semantic annotation. Six classifications are involved in the tagging: category, taxonomy, mereology, topology, evaluation and derivational classes. The operating of the context semantic rules is shown by applying them to various polysemous nouns and adjectives. Our results demonstrate semantic tags incorporated in the context to be highly effective for WSD.
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text. Lexical semantic factorization is very important in that aspect due to its feasibility, high level of abstraction, and the language independence of its output. In the core of such a factorization lies an Arabic lexical semantic DB. While building this LR, we had to go beyond the conventional exclusive collection of words from dictionaries and thesauri that cannot alone produce a satisfactory coverage of this highly inflective and derivative language. This paper is hence devoted to the design and implementation of an Arabic lexical semantics LR that enables the retrieval of the possible senses of any given Arabic word at a high coverage. Instead of tying full Arabic words to their possible senses, our LR flexibly relates morphologically and PoS-tags constrained Arabic lexical compounds to a predefined limited set of semantic fields across which the standard semantic relations are defined. With the aid of the same large-scale Arabic morphological analyzer and PoS tagger in the runtime, the possible senses of virtually any given Arabic word are retrievable.
Discourse structure and coherence relations are one of the main inferential challenges addressed by computational pragmatics. The present study focuses on discourse markers as key elements in guiding the inferences of the statements in natural language. Through a rule-based approach for the automatic identification, classification and annotation of the discourse markers in a multilingual parallel corpus (Arabic-Spanish-English), this research provides a valuable resource for the community. Two main aspects define the novelty of the present study. First, it offers a multilingual computational processing of discourse markers, grounded on a theoritical framework and implemented in a XML tagging scheme. The XML scheme represents a set of pragmatic and grammatical attributes, considered as basic features for the different kinds of discourse markers. Besides, the scheme provides a typology of discourse markers based on their discursive functions including hypothesis, co-argumentation, cause, consequence, concession, generalization, topicalization, reformulation, enumeration, synthesis, etc. Second, Arabic language is addressed from a computational pragmatic perspective where the identification, classification and annotation processes are carried out using the information provided from the tagging of Spanish discourse markers and the alignments.
We describe ongoing work in semi-automatic annotating corpus, with the goal to answer why-question in question answering system and give a construction of the coherent tree for text summarization. In this paper we present annotation schemas for identifying the discourse relations that hold between the parts of text as well as the particular textual of span that are related via the discourse relation. Furthermore, we address several tasks in building the annotated corpus in discourse level, namely creating annotated guidelines, ensuring annotation accuracy and evaluating.
We present a multi-lingual dictionary of dirty words. We have collected about 3,200 dirty words in several languages and built a database of these. The language with the most words in the database is English, though there are several hundred dirty words in for instance Japanese too. Words are classified into their general meaning, such as what part of the human anatomy they refer to. Words can also be assigned a nuance label to indicate if it is a cute word used when speaking to children, a very rude word, a clinical word etc. The database is available online and will hopefully be enlarged over time. It has already been used in research on for instance automatic joke generation and emotion detection.
We implement several different methods for generating jokes in English. The common theme is to intentionally produce poor utterances by breaking Grices maxims of conversation. The generated jokes are evaluated and compared to human made jokes. They are in general quite weak jokes, though there are a few high scoring jokes and many jokes that score higher than the most boring human joke.
Automatic sentiment analysis in texts has attracted considerable attention in recent years. Most of the approaches developed to classify texts or sentences as positive or negative rest on a very specific kind of language resource: emotional lexicons. To build these resources, several automatic techniques have been proposed. Some of them are based on dictionaries while others use corpora. One of the main advantages of the corpora techniques is that they can build lexicons that are tailored for a specific application simply by using a specific corpus. Currently, only anecdotal observations and data from other areas of language processing plead in favour of the utility of specific corpora. This research aims to test this hypothesis. An experiment based on 702 sentences evaluated by judges shows that automatic techniques developed for estimating the valence from relatively small corpora are more efficient if the corpora used contain texts similar to the one that must be evaluated.
This paper presents the design and construction of a Chinese opinion corpus based on the online product reviews. Based on the observation on the characteristics of opinion expression in Chinese online product reviews, which is quite different from in the formal texts such as news, an annotation framework is proposed to guide the construction of the first Chinese opinion corpus based on online product reviews. The opinionated sentences are manually identified from the review text. Furthermore, for each comment in the opinionated sentence, its 13 describing elements are annotated including the expressions related to the interested product attributes and user opinions as well as the polarity and degree of the opinions. Currently, 12,724 comments are annotated in 10,935 sentences from review text. Through statistical analysis on the opinion corpus, some interesting characteristics of Chinese opinion expression are presented. This corpus is shown helpful to support systematic research on Chinese opinion analysis.
This paper presents OMINE, an opinion mining system which aims to identify concepts such as products and their attributes, and analyze their corresponding polarities. Our work pioneers at linking extracted topic terms with domain-specific concepts. Compared with previous work, taking advantage of ontological techniques, OMINE achieves 10{\%} higher recall with the same level precision on the topic extraction task. In addition, making use of opinion patterns for sentiment analysis, OMINE improves the performance of the backup system (NGram) around 6{\%} for positive reviews and 8{\%} for negative ones.
This paper proposes a new method of the sentiment analysis utilizing inter-sentence structures especially for coping with reversal phenomenon of word polarity such as quotation of others opinions on an opposite side. We model these phenomenon using Hidden Conditional Random Fields(HCRFs) with three kinds of features: transition features, polarity features and reversal (of polarity) features. Polarity features and reversal features are doubly added to each word, and each weight of the features are trained by the common structure of positive and negative corpus in, for example, assuming that reversal phenomenon occured for the same reason (features) in both polarity corpus. Our method achieved better accuracy than the Naive Bayes method and as good as SVMs.
In this paper a first implementation of a tool for valence shifting of natural language texts, named Valentino (VALENced Text INOculator), is presented. Valentino can modify existing textual expressions towards more positively or negatively valenced versions. To this end we built specific resources gathering various valenced terms that are semantically or contextually connected, and implemented strategies that uses these resources for substituting input terms.
Submission received: 18 November 2005; revised submission received: 18 October 2006; accepted for publication: 21 September 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 4  existence of such data, it should be possible to exploit machine learning methods to automatically build and optimize a new dialogue system. This objective poses two questions: what machine learning methods are effective for this problem? and how can we encode the task in a way which is appropriate for these methods? For the latter challenge, we exploit the Information State Update (ISU) approach to dialogue systems (Bohlin et al. 1999; Larsson and Traum 2000), which provides the kind of rich and ﬂexible feature-based representations of context that are used with many recent machine learning methods, including the linear function approximation method we use here. For the former challenge, we propose a novel hybrid method that combines reinforcement learning (RL) with supervised learning (SL). The focus of this article is to establish effective methods for using ﬁxed corpora of dialogues to automatically optimize complex dialogue systems. To avoid the need for extensive hand-crafting, we allow rich representations of context that include all the features that might be relevant to dialogue management decisions, and we allow a broad set of dialogue management decisions with very few constraints on when a decision is applicable. This ﬂexibility simpliﬁes system design, but it leads to a huge space of possible dialogue management policies, which poses severe difﬁculties for existing approaches to machine learning for dialogue systems (see Section 1.1). Our proposed method addresses these difﬁculties without the use of user simulations, feature engineering, or further data collections. We demonstrate the effectiveness of the proposed method on the COMMUNICATOR corpora of ﬂight-booking dialogues. Our method (“hybrid learning” with linear function approximation) can learn dialogue strategies that are better than those learned by standard learning methods, and that are better than the (in this case hand-coded) strategies present in the original corpora, according to a variety of metrics. To evaluate learned strategies we run them with simulated users that are also trained on (different parts of) the COMMUNICATOR corpora, and automatically score the simulated dialogues based on how many information “slots” they manage to collect from users (“ﬁlled slots”), whether those slots were conﬁrmed (“conﬁrmed slots”), and how many dialogue turns were required to do so. Later work has shown these metrics to correlate strongly with task completion for real users of the different policies (Lemon, Georgila, and Henderson 2006). The main contributions of the work are therefore in empirically demonstrating that: r limited initial data sets can be used to train complex dialogue policies, using a novel combination of supervised and reinforcement learning; and r large, feature-based representations of dialogue context can be used in tractable learning of dialogue policies, using linear function approximation. In this article, after a discussion of related work, we outline the annotations we have added to the COMMUNICATOR data, then present the proposed learning method, and describe our evaluation method. Finally, we present the evaluation results and discuss their implications. 1.1 Related Work As in previous work on learning for dialogue systems, in this article we focus on learning dialogue management policies. Formally, a dialogue management policy is a 488  Henderson, Lemon, and Georgila  Hybrid Reinforcement/Supervised Learning  mapping from a dialogue context (a.k.a. a state) to an action that the system should take in that context. Because most previous work on dialogue systems has been done in the context of hand-crafted systems, we use representations of the dialogue context and the action set based on previous work on hand-crafted dialogue systems. Our main novel contribution is in the area of learning, where we build on previous work on automatically learning dialogue management policies, discussed subsequently. The ISU approach to dialogue (Bohlin et al. 1999; Larsson and Traum 2000) employs rich representations of dialogue context for ﬂexible dialogue management. Information States are feature structures intended to record all the information about the preceding portion of the dialogue that is relevant to making dialogue management decisions. An example of some of the types of information recorded in our Information States is shown in Figure 1, including ﬁlled slots, conﬁrmed slots, and previous speech acts. Previous work has raised the question of whether dialogue management policies can be learned (Levin and Pieraccini 1997) for systems that have only a limited view of the dialogue context, for example, not including prior speech act history (see the following). One prominent representation of the set of possible system actions is the DATE scheme (Walker and Passonneau 2001). In particular, this representation is used in the COMMUNICATOR corpus annotation (Walker, Passonneau, and Boland 2001), discussed herein. The DATE scheme classiﬁes system actions in terms of their Conversational Domain, Speech Act, and Task. For example, one possible system action is about task,  Figure 1 Example ﬁelds from an Information State annotation. User-provided information is in square brackets. 489  Computational Linguistics  Volume 34, Number 4  request info, dest city , which corresponds to a system utterance such as What is your destination city? The speciﬁc instantiation of this scheme, and our extensions to it, are discussed in Section 1.2. Machine-learning approaches to dialogue management attempt to learn optimal dialogue policies from corpora of simulated or real dialogues, or by generating such data during automatic trial-and-error exploration of possible policies. Automatic optimization is desirable because of the high cost of developing and maintaining handcoded dialogue managers, and because there is no guarantee that hand-coded dialogue management strategies are good. Several research groups have developed reinforcement learning approaches to dialogue management, starting with Levin and Pieraccini (1997) and Walker, Fromer, and Narayanan (1998). Previous work has been restricted to limited dialogue context representations and limited sets of actions to choose among (Walker, Fromer, and Narayanan 1998; Goddeau and Pineau 2000; Levin, Pieraccini, and Eckert 2000; Roy, Pineau, and Thrun 2000; Schefﬂer and Young 2002; Singh et al. 2002; Williams and Young 2005; Williams, Poupart, and Young 2005a). Much of the prior work in RL for dialogue management focuses on the problem of choosing among a particular limited set of actions (e.g., conﬁrm, don’t conﬁrm) in speciﬁc problematic states (see, e.g., Singh et al. 2000a). This approach augments, rather than replaces, hand-crafted dialogue systems, because the vast majority of decisions, which are not learned, need to be speciﬁed by hand. In contrast, we tackle the problem of learning to choose among any possible dialogue actions for almost every possible state. In addition, all prior work has used only a limited representation of the dialogue context, often consisting only of the states of information slots (e.g., destination city ﬁlled with high conﬁdence) in the application (Goddeau and Pineau 2000; Levin, Pieraccini, and Eckert 2000; Singh et al. 2000a, 2000b, 2002; Young 2000; Schefﬂer and Young 2002; Williams, Poupart, and Young 2005a, 2005b; Williams and Young 2005; Pietquin and Dutoit 2006b), with perhaps some additional low-level information (such as acoustic features [Pietquin 2004]). Only recently have researchers experimented with using enriched representations of dialogue context (Gabsdil and Lemon 2004; Lemon et al. 2005; Frampton and Lemon 2006; Rieser and Lemon 2006c), as we do in this article. From this work it is known that adding context features leads to better dialogue strategies, compared to, for example, simply using the status of ﬁlled or conﬁrmed information slots as has been studied in all prior work (Frampton and Lemon 2006). In this article we explore methods for scalable, tractable learning when using all the available context features. Reinforcement Learning requires estimating how good different actions will be in different dialogue contexts. Because most previous work has only differentiated between a small number of possible dialogue contexts, they have been able to perform these estimates for each state independently (e.g., Singh et al. 2002; Pietquin 2004). In contrast, we use function approximation to allow generalization to states that were not in the training data. Function approximation was also applied to RL by Denecke, Dohsaka, and Nakano (2005), but they still use a relatively small state space (6 features, 972 possible states). They also only exploit data for the 50 most frequent states, using what is in effect a Gaussian kernel to compute estimates for the remaining states from these 50 states. This is a serious limitation to their method, because a large percentage of the data is likely to be from less frequent states, and thus would be ignored. In our data set, we found that state frequencies followed a Zipﬁan (i.e., largetailed) distribution, with 61% of the system turns having states that only occurred once in the data. 490  Henderson, Lemon, and Georgila  Hybrid Reinforcement/Supervised Learning  Another source of variation between learning approaches is the extent to which they train on data from simulated users of different kinds, rather than train on data gathered from real user interactions (as is done in this article). Simulated users are generally preferred due to the much smaller development effort involved, and the fact that trialand-error training with humans is tedious for the users. However, the issues of how to construct and then evaluate simulated users are open problems. Clearly there is a dependency between the accuracy of the simulation used for training and the eventual dialogue policy that is learned (Schatzmann et al. 2005). Current research attempts to develop metrics for user simulation that are predictive of the overall quality of the ﬁnal learned dialogue policy (Schatzmann, Georgila, and Young 2005; Schatzmann et al. 2005; Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon 2006; Rieser and Lemon 2006a; Schatzmann et al. 2006; Williams 2007). Furthermore, several approaches use simple probabilistic simulations encoded by hand, using intuitions about reasonable user behaviors (e.g., Pietquin 2004; Frampton and Lemon 2005; Pietquin and Dutoit 2006a), whereas other work (e.g., Schefﬂer and Young 2001, 2002; Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon 2006; Rieser and Lemon 2006a) builds simulated users from dialogue corpora. We use the latter approach, but only in the evaluation of our learned policies. No matter which method is chosen for user simulation, a simulated user is still clearly different from a human user. Therefore, it is important to learn as much as possible from the data we have from human users. In addition, the huge policy space makes policy exploration with simulated users intractable, unless we can initialize the system with a good policy and constrain the policy exploration. This also requires learning as much as possible from the initial set of data. Therefore, in this article we investigate using a ﬁxed corpus of dialogues to automatically optimize dialogue systems. No user simulation is involved in training, thus avoiding the issue of dependency on the quality and availability of user simulations. Previous work on RL has made use of policy exploration (Sutton and Barto 1998), where new data is generated for each policy that is considered during the course of learning (for example using simulated users). Indeed, this is often considered an integral part of RL. In contrast, we choose to learn from a ﬁxed data set, without policy exploration. This is motivated by the fact that real dialogue corpora are very expensive to produce, and it is often not practical to produce new real dialogues during the course of learning. Singh et al. (2002) manage to perform one iteration of policy exploration with real data, but most work on RL requires many thousands of iterations. As discussed previously, this motivates using simulated data for training, but even if accurate dialogues can be automatically generated with simulated users, training on simulated dialogues does not replace the need to fully exploit the real data, and does not solve the sparse data problems that we address here. With a very large state space, it will never be tractable for policy exploration to test a new policy on even a reasonable proportion of the states. Thus we will inevitably need to stop policy exploration with a policy that has not been sufﬁciently tested. In this sense, we will be in a very similar situation to learning from a ﬁxed data set, where we don’t have the option of generating new data for new states. For this reason, the solution we propose for learning from ﬁxed data sets is also useful for learning with policy exploration. There have been some proposals in RL for learning a policy that is different from that used to generate the data (called “off-policy” learning), but these methods have been found not to work well with linear function approximation (Sutton and Barto 1998). They also do not solve the problem of straying from the region of state space that has been observed in the data, discussed subsequently. 491  Computational Linguistics  Volume 34, Number 4  1.2 The COMMUNICATOR Domain and Data Annotation To empirically evaluate our proposed learning method, we apply it to the COMMUNICATOR domain using the COMMUNICATOR corpora. The COMMUNICATOR corpora (2000 [Walker et al. 2001] and 2001 [Walker et al. 2002b]) consist of human–machine dialogues (approximately 2,300 dialogues in total). The users always try to book a ﬂight, but they may also try to select a hotel or car rental. The dialogues are primarily “slotﬁlling” dialogues, with some information being presented to the user after the system thinks it has ﬁlled (or conﬁrmed) the relevant slots. These corpora have been previously annotated using the DATE scheme, for the Conversational Domain, Speech Act, and Task of each system utterance (Walker and Passonneau 2001; Walker, Passonneau, and Boland 2001). In addition, the results of user questionnaires are available, but only for the 2001 corpus. Table 1 shows some statistics for the two collections. In the 2000 collection each turn contains only one utterance but in the 2001 corpus a turn may contain more than one utterance. More details about the COMMUNICATOR corpora can be found in Walker, Passonneau, and Boland (2001) and Walker et al. (2001, 2002a). We used a hand-crafted automatic system (Georgila, Lemon, and Henderson 2005; Georgila et al., submitted) to assign Speech Acts and Tasks to the user utterances, and to compute state representations for each point in the dialogue (i.e., after every utterance). Although we annotated the whole 2000 and 2001 corpora, because we need the results of user questionnaires (as discussed subsequently), we only make use of the 2001 data for the experiments reported here. The 2001 data has eight systems, 1,683 dialogues, and 125,388 total states, two thirds of which result from system actions and one third from user actions. The annotation system is implemented using DIPPER (Bos et al. 2003) and OAA (Cheyer and Martin 2001), using several OAA agents (see Georgila, Lemon, and Henderson, 2005, and Georgila et al., submitted, for more details). Following the ISU approach, we represented states using Information States, which are feature structures intended to record all the information about the preceding portion of the dialogue that is relevant to making dialogue management decisions. An example of some of the types of information recorded in an Information State is shown in Figure 1, including ﬁlled slots, conﬁrmed slots, and previous speech acts. Given this corpus, we need to learn a dialogue management policy that maps these state representations to effective system actions. As the example in Figure 1 illustrates, there are a large number of features in dialogue states that are potentially relevant to  Table 1 Statistics for the 2000 and 2001 COMMUNICATOR data.  Year  2000 2001 Total  Number of dialogues Number of turns Number of system turns Number of user turns Number of utterances Number of system utterances Number of user utterances Number of system dialogue acts  648 24,728 13,013 11,715 24,728 13,013 11,715 22,752  1683 78,718 39,419 39,299 89,666 50,159 39,507 85,881  2331 103,446 52,432 51,014 114,394 63,172 51,222 108,633  492  Henderson, Lemon, and Georgila  Hybrid Reinforcement/Supervised Learning  dialogue management, and thus should not be excluded from the state representations we use in learning. This leads to a very large space of possible states (over 10386 states are theoretically possible in our model), with a very high chance that a state encountered in testing will not be exactly the same as any state encountered in training. This fact motivates, if not requires, the use of approximation methods. The complexity of the COMMUNICATOR domain is also manifested in the large number of system actions that the dialogue management policy needs to choose between. The DATE scheme representation of system actions implies that each possible triple of values for the Conversational Domain, Speech Act, and Task is a different action. In addition, we have added release turn and end dialogue actions. There are a total of 74 system actions that occur in the annotated COMMUNICATOR data. 2. Reinforcement Learning with a Fixed Data Set We use the annotated COMMUNICATOR data to train a Reinforcement Learning system. In RL, the objective of the system is to maximize the reward it gets during entire dialogues. Rewards are deﬁned to reﬂect how successful a dialogue was, so by maximizing the total reward the system optimizes the quality of dialogues. The difﬁculty is that, at any point in the dialogue, the system cannot be sure what will happen in the remainder of the dialogue, and thus cannot be sure what effect its actions will have on the total reward at the end of the dialogue. Thus the system must choose an action based on the average reward it has observed previously after it has performed that action in states similar to the current one. This average is the expected future reward. The core component of any RL system is the estimation of the expected future reward (called the Q-function). Given a state and an action that could be taken in that state, the Q-function tells us what total reward, on average, we can expect between taking that action and the end of the dialogue.1 Once we have this function, the optimal dialogue management policy reduces to simply choosing the action that maximizes the expected future reward for the current state. Our proposal for RL with ﬁxed data sets uses two main techniques. The ﬁrst is the use of function approximation to estimate the expected future reward. We claim that linear function approximation is an effective way to generalize from a limited data set to a large space of state–action pairs. The second technique is a novel hybrid learning method that combines RL with supervised learning (SL). SL is used to characterize how much data we have for each area of the state–action space (also using linear function approximation). Our hybrid policy uses SL to avoid state–action pairs for which we do not have enough data, while using RL to maximize reward within the parts of the space where we do have enough data. We claim that this is an effective solution to the problem of learning complex tasks from ﬁxed data sets. 2.1 Deﬁning Dialogue Reward To apply RL to the COMMUNICATOR data, we ﬁrst have to deﬁne a mapping r(d, i) from a dialogue d and a position in that dialogue i to a reward value. This reward function is computed using the reward level of annotation in the COMMUNICATOR data, which was  
 Volume 34, Number 4  programming languages. Thus, the mainstream approach to natural language parsing uses algorithms that efﬁciently derive a potentially very large set of analyses in parallel, typically making use of dynamic programming and well-formed substring tables or charts. When disambiguation is required, this approach can be coupled with a statistical model for parse selection that ranks competing analyses with respect to plausibility. Although it is often necessary, for efﬁciency reasons, to prune the search space prior to the ranking of complete analyses, this type of parser always has to handle multiple analyses. By contrast, parsers for formal languages are usually based on deterministic parsing techniques, which are maximally efﬁcient in that they only derive one analysis. This is possible because the formal language can be deﬁned by a non-ambiguous formal grammar that assigns a single canonical derivation to each string in the language, a property that cannot be maintained for any realistically sized natural language grammar. Consequently, these deterministic parsing techniques have been much less popular for natural language parsing, except as a way of modeling human sentence processing, which appears to be at least partly deterministic in nature (Marcus 1980; Shieber 1983). More recently, however, it has been shown that accurate syntactic disambiguation for natural language can be achieved using a pseudo-deterministic approach, where treebank-induced classiﬁers are used to predict the optimal next derivation step when faced with a nondeterministic choice between several possible actions. Compared to the more traditional methods for natural language parsing, this can be seen as a severe form of pruning, where parse selection is performed incrementally so that only a single analysis is derived by the parser. This has the advantage of making the parsing process very simple and efﬁcient but the potential disadvantage that overall accuracy suffers because of the early commitment enforced by the greedy search strategy. Somewhat surprisingly, though, research has shown that, with the right choice of parsing algorithm and classiﬁer, this type of parser can achieve state-of-the-art accuracy, especially when used with dependency-based syntactic representations. Classiﬁer-based dependency parsing was pioneered by Kudo and Matsumoto (2002) for unlabeled dependency parsing of Japanese with head-ﬁnal dependencies only. The algorithm was generalized to allow both head-ﬁnal and head-initial dependencies by Yamada and Matsumoto (2003), who reported very good parsing accuracy for English, using dependency structures extracted from the Penn Treebank for training and testing. The approach was extended to labeled dependency parsing by Nivre, Hall, and Nilsson (2004) (for Swedish) and Nivre and Scholz (2004) (for English), using a different parsing algorithm ﬁrst presented in Nivre (2003). At a recent evaluation of data-driven systems for dependency parsing with data from 13 different languages (Buchholz and Marsi 2006), the deterministic classiﬁer-based parser of Nivre et al. (2006) reached top performance together with the system of McDonald, Lerman, and Pereira (2006), which is based on a global discriminative model with online learning. These results indicate that, at least for dependency parsing, deterministic parsing is possible without a drastic loss in accuracy. The deterministic classiﬁer-based approach has also been applied to phrase structure parsing (Kalt 2004; Sagae and Lavie 2005), although the accuracy for this type of representation remains a bit below the state of the art. In this setting, more competitive results have been achieved using probabilistic classiﬁers and beam search, rather than strictly deterministic search, as in the work by Ratnaparkhi (1997, 1999) and Sagae and Lavie (2006). A deterministic classiﬁer-based parser consists of three essential components: a parsing algorithm, which deﬁnes the derivation of a syntactic analysis as a sequence 514  Nivre  Deterministic Incremental Dependency Parsing  of elementary parsing actions; a feature model, which deﬁnes a feature vector representation of the parser state at any given time; and a classiﬁer, which maps parser states, as represented by the feature model, to parsing actions. Although different types of parsing algorithms, feature models, and classiﬁers have been used for deterministic dependency parsing, there are very few studies that compare the impact of different components. The notable exceptions are Cheng, Asahara, and Matsumoto (2005), who compare two different algorithms and two types of classiﬁer for parsing Chinese, and Hall, Nivre, and Nilsson (2006), who compare two types of classiﬁers and several types of feature models for parsing Chinese, English, and Swedish. In this article, we focus on parsing algorithms. More precisely, we describe two families of algorithms that can be used for deterministic dependency parsing, supported by classiﬁers for predicting the next parsing action. The ﬁrst family uses a stack to store partially processed tokens and is restricted to the derivation of projective dependency structures. The algorithms of Kudo and Matsumoto (2002), Yamada and Matsumoto (2003), and Nivre (2003, 2006b) all belong to this family. The second family, represented by the algorithms described by Covington (2001) and recently explored for classiﬁerbased parsing in Nivre (2007), instead uses open lists for partially processed tokens, which allows arbitrary dependency structures to be processed (in particular, structures with non-projective dependencies). We provide a detailed analysis of four different algorithms, two from each family, and give proofs of correctness and complexity for each algorithm. In addition, we perform an experimental evaluation of accuracy and efﬁciency for the four algorithms, combined with state-of-the-art classiﬁers, using data from 13 different languages. Although variants of these algorithms have been partially described in the literature before, this is the ﬁrst comprehensive analysis and evaluation of the algorithms within a uniﬁed framework. The remainder of the article is structured as follows. Section 2 deﬁnes the task of dependency parsing and Section 3 presents a formal framework for the characterization of deterministic incremental parsing algorithms. Sections 4 and 5 contain the formal analysis of four different algorithms, deﬁned within the formal framework, with proofs of correctness and complexity. Section 6 presents the experimental evaluation; Section 7 reports on related work; and Section 8 contains our main conclusions.  2. Dependency Parsing Dependency-based syntactic theories are based on the idea that syntactic structure can be analyzed in terms of binary, asymmetric dependency relations holding between the words of a sentence. This basic conception of syntactic structure underlies a variety of different linguistic theories, such as Structural Syntax (Tesnie`re 1959), Functional Generative Description (Sgall, Hajicˇova´, and Panevova´ 1986), Meaning-Text Theory (Mel’cˇuk 1988), and Word Grammar (Hudson 1990). In computational linguistics, dependencybased syntactic representations have in recent years been used primarily in data-driven models, which learn to produce dependency structures for sentences solely from an annotated corpus, as in the work of Eisner (1996), Yamada and Matsumoto (2003), Nivre, Hall, and Nilsson (2004), and McDonald, Crammer, and Pereira (2005), among others. One potential advantage of such models is that they are easily ported to any domain or language in which annotated resources exist. In this kind of framework the syntactic structure of a sentence is modeled by a dependency graph, which represents each word and its syntactic dependents through labeled directed arcs. This is exempliﬁed in Figure 1, for a Czech sentence taken from the Prague 515  Computational Linguistics  Volume 34, Number 4  ✞ ✞ ROOT0  AuxK  ✞  AuxP  ¤  ✞  AuxP  Pred  ¤✞  Sb  ¤  ✞ Atr ¤  ✞ AuxZ ¤  ❄ Z1 (Out-of  ❄ nich2 them  ❄  ❄  ❄  je3  jen4  jedna5  is  only one-FEM-SG  (“Only one of them concerns quality.”)  ¤  ¤  ✞ Adv ¤  ❄  ❄❄  na6 kvalitu7 .8  to quality .)  Figure 1 Dependency graph for a Czech sentence from the Prague Dependency Treebank.  ✞ ROOT0  ✞  P  ¤  ROOT  ¤ ✞ OBJ  ¤  ✞  PMOD  ¤  ✞ NMOD ¤✞SBJ ¤  ✞NMOD ¤N✞MOD¤  ✞ NMOD ¤  ❄  ❄❄  ❄  ❄❄  ❄  ❄❄  Economic1 news2 had3 little4 effect5 on6 ﬁnancial7 markets8 .9  Figure 2 Dependency graph for an English sentence from the Penn Treebank.  Dependency Treebank (Hajicˇ et al. 2001; Bo¨ hmova´ et al. 2003), and in Figure 2, for an English sentence taken from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994).1 An artiﬁcial word ROOT has been inserted at the beginning of each sentence, serving as the unique root of the graph. This is a standard device that simpliﬁes both theoretical deﬁnitions and computational implementations. Deﬁnition 1 Given a set L = {l1, . . . , l|L|} of dependency labels, a dependency graph for a sentence x = (w0, w1, . . . , wn) is a labeled directed graph G = (V, A), where 1. V = {0, 1, . . . , n} is a set of nodes, 2. A ⊆ V × L × V is a set of labeled directed arcs. The set V of nodes (or vertices) is the set of non-negative integers up to and including n, each corresponding to the linear position of a word in the sentence (including ROOT). The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and j are nodes and l is a dependency label. Because arcs are used to represent dependency relations, we will say that i is the head and l is the dependency type of j. Conversely, we say that j is a dependent of i.  
E-mail: ccb@cs.jhu.edu. † School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for publication: 26 March 2008. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 4  syntax trees—they all rely on some form of alignment for extracting paraphrase pairs. In its simplest form, the alignment can range over individual words, as is often done in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay and Lee 2003). The obtained paraphrases are typically evaluated via human judgments. Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In some cases the automatically acquired paraphrases are compared against manually generated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance increase for a speciﬁc application, such as machine translation (Callison-Burch, Koehn, and Osborne 2006). Unfortunately, manually evaluating paraphrases in this way has at least three drawbacks. First, it is infeasible to perform frequent evaluations when assessing incremental system changes or tuning system parameters. Second, it is difﬁcult to replicate results presented in previous work because there is no standard corpus, and no standard evaluation methodology. Consequently comparisons across systems are few and far between. The third drawback concerns the evaluation studies themselves, which primarily focus on precision. Recall is almost never evaluated directly in the literature. And this is for a good reason: There is no guarantee that participants will identify the same set of paraphrases as each other or with a computational model. The problem relates to the nature of the paraphrasing task, which has so far eluded formal deﬁnition (see the discussion in Barzilay [2003]). Such a deﬁnition is not so crucial when assessing precision, because subjects are asked to rate the paraphrases without actually having to identify them. However, recall might be measured with respect to some set of “goldstandard” paraphrases which will have to be collected according to some concrete deﬁnition. In this article we present a resource that could potentially be used to address these problems. Speciﬁcally, we create a monolingual parallel corpus with human paraphrase annotations. Our working deﬁnition of paraphrase is based on word and phrase1 alignments between semantically equivalent sentences. Other deﬁnitions are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences.  
Computational Linguistics  Volume 34, Number 4  be seen insofar as the notion “degree of grammaticality” has never been rigorously deﬁned. Chapter 6, on Semantics, starts with a discussion of various standard paradoxes such as the Liar, and then moves on to an overview of Montague’s theory, type theory, and grammatical semantics. Throughout the discussion, Kornai underscores the fundamental limitations of theories of semantics that are based purely upon evaluation of truth conditions for artiﬁcial fragments, an important point for anyone who wants to go beyond theoretical philosophically inspired models and consider semantic interpretation in the real world. Complexity is the topic of Chapter 7. This is not the Chomsky-hierarchy notion of complexity, but rather deals with information theory, in particular entropy, Kolmogorov complexity, and a short section on learning, including identiﬁcation in the limit and PAC learning. Pattern recognition is divided across two chapters, with Chapter 8 laying the essential groundwork of linguistic pattern recognition, and Chapter 9 presenting details on speech processing and handwriting recognition. This includes feature extraction: In the case of speech recognition, Kornai reviews the frequency representation of speech signals, and deﬁnes the cepstrum. Discussion of acoustic models leads us to phonemes as hidden units, with a slight detour into the ﬁne-grained distinctions between different levels of phonemic analysis in the once popular but now largely discredited theory of Lexical Phonology. Each chapter ends with a section entitled “Further Reading,” and the texts referred to are generally quite useful as material for readers who wish to explore the issues further. According to Wikipedia, Kornai is a “well-known mathematical linguist” whose Erdo˝s number is 2. Unfortunately, neither of us can claim Kornai’s mathematical sophistication or stature, but on the other hand this makes us good judges of the book’s potential audience; and herein lies a problem. Kornai’s target is “anyone with sufﬁcient general mathematical maturity” with “[n]o prior knowledge of linguistics or languages . . . assumed on the part of the reader” (page viii). This suggests that the book is not primarily aimed at linguists, and certainly the mathematical maturity assumed puts this book well beyond the reach of most linguists, so that it could not easily be used in an introductory course on mathematical linguistics in a linguistics program. It is probably beyond the reach of many computer science students as well. What about those who do have the mathematical maturity, but know nothing about linguistics? The problem here is that in many cases Kornai does not give enough background (or any background) to appreciate the signiﬁcance of the particular issues being discussed. For example, on page 77 Kornai gives weak crossover and heavy NP shift as examples of phenomena that have ‘weak’ effects on grammaticality, and resumptive pronouns as examples of phenomena that are marginal in some languages (such as English). But nowhere does he explain what these terms denote, which means that these are throw-away comments for anyone who does not already know. Section 3.2 introduces phonological features and feature geometry and sketches some of the mathematical properties of systems with features; but very little background is given on what features are supposed to represent. The short discussion of Optimality Theory (pages 67–69) hardly gives enough background to give a feel for the main points of that approach. In other cases, topics are introduced but their importance to surrounding topics is hard to fathom. For example, in Section 6.1.3 a discussion of the Berry paradox leads into a digression on how to implement digit-sequence-to-number-name mappings as ﬁnitestate transducers. Apart from giving Kornai an opportunity to emphasize that this is 616  
Submission received: 4 September 2007; revised submission received: 20 December 2007; accepted for publication: 6 April 2008. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 3  training and test material for automatic classiﬁers. The 0.8 rule of thumb is irrelevant for this purpose, because classiﬁers will be affected by disagreement differently than correlations. Furthermore, Krippendorff’s argument comes with a caveat: the disagreement must be due to random noise. For his case of correlations, any patterns in the disagreement could accidentally bolster the relationship perceived in the data, leading to false results. To be sure that data is ﬁt for the intended purpose, Krippendorff advises the analyst to look for structure in the disagreement and consider how it might affect data use. Although computational linguists have rarely followed this advice, it is just as relevant to us. Machine-learning algorithms are designed speciﬁcally to look for, and predict, patterns in noisy data. In theory, this makes random disagreement unimportant. More data will yield more signal and the learner will ignore the noise. However, as Craggs and McGee Wood (2005) suggest, this also makes systematic disagreement dangerous, because it provides an unwanted pattern for the learner to detect. We demonstrate that machine learning can tolerate data with a low reliability measurement as long as the disagreement looks like random noise, and that when it does not, data can have a reliability measure commonly held to be acceptable but produce misleading results. 2. Method To explain what is wrong with using 0.8 as a cut-off, we need to think about how data is used for classiﬁcation tasks. Consider Figure 1, which shows a relation between some features A and a class label B. Learning labels from a set of features is a common task in computational linguistics; for instance, in Shriberg et al. (1998), which assumes a pre-existing dialogue act segmentation, the labels are dialogue act types, and they are learned from automatically derived prosodic features. In this way of using data, only one of the variables—the output dialogue act label—is hand-coded. In the ﬁgure, the real relationship between prosody and dialogue act label is shown on the left; R relates the prosodic features A to the output act B.  Figure 1 Hand-coded target labels are used to train classiﬁers to automatically predict those labels from features. 320  Reidsma and Carletta  Reliability Measurement without Limits  In theory, there is one correct label for any given act. However, in practice human coders disagree, choosing different labels for the same act (sometimes even with divergences that make one question whether there is one correct answer). The data actually available for analysis is shown in the middle of the ﬁgure. Here, the automatic features, A, are the same as before, but there are multiple, possibly differing labels for the same act, Bobs, coming from different human annotators. Finally, on the right the ﬁgure shows the classiﬁer. It takes the same prosodic features A and uses them to predict a dialogue act label Bpred on new data, using the relationship learned from the observed data, RML. Projects vary in how they choose data from which to build the classiﬁer when coders disagree, but whatever they do is colored by the observations they have available to them. We often think of reliability assessment as telling us how much disagreement there is among the human coders, but the real issue is how their individual interpretations of the coding scheme make RML differ from R. There is a problem that arises for anyone using this methodology. Without the “real” data, it is impossible to judge how well the learned relationship reﬂects the real one. Classiﬁcation performance for Bpred can only be calculated with respect to the “observed” data Bobs. In this article, we surmount this problem by simulating the real world so that we can measure the differences between this “observed” performance and the “real” performance. Our simulation uses a Bayesian network (Pearl 1988) to create an initial, “real” data set with 3,000 samples of features (A) and their corresponding target labels (B). For simplicity, we use a single ﬁve-valued feature and ﬁve possible labels. The relative label frequencies vary between 17% and 25%. This gives us a small amount of variation around what is essentially equally distributed data. We corrupt the labels (B) to simulate the “hand-coded” observed data (Bobs) corresponding to the output of a human coder, and then train a neural network constructed using the WEKA toolkit (Witten and Frank 2005) on 2,000 samples from Bobs. Finally, we calculate the neural network’s performance twice, using as test data either the remaining 1,000 samples from Bobs or the initial, “real” versions of those same 1,000 samples. There are three ways in which we need to vary our simulation in order to be systematic. The ﬁrst is in the strength of the relationship between the features the machine learner takes as input and the target labels, which we achieve simply by changing the probabilities in the Bayesian network that creates the data set. In the simulation, we vary the strength of the relationship in eight graded steps.1 The second is in the amount of disagreement we introduce when we create the observed data (Bobs). We create 200 different versions of the hand-coded data that cover a range of values from κ = 0 to  
Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 3  years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each of its candidates, with features describing the properties of the anaphor and the individual candidate. During resolution, the antecedent of an anaphor is selected based on the classiﬁcation results for each candidate. One assumption behind the single-candidate model is that whether a candidate is the antecedent of an anaphor is completely independent of the other competing candidates. However, anaphora resolution can be more accurately represented as a ranking problem in which candidates are ordered based on their preference and the best one is the antecedent of the anaphor (Jurafsky and Martin 2000). The single-candidate model, which only considers the candidates of an anaphor in isolation, is incapable of effectively capturing the preference relationship between candidates for its training. Consequently, the learned classiﬁer cannot produce reliable results for preference determination during resolution. To deal with this problem, we propose a twin-candidate learning model for anaphora resolution. The main idea behind the model is to recast anaphora resolution as a preference classiﬁcation problem. The purpose of the classiﬁcation is to determine the preference between two competing candidates for the antecedent of a given anaphor. In the model, an instance is formed by an anaphor and two of its antecedent candidates, with features used to describe their properties and relationships. The antecedent is selected based on the judged preference among the candidates. In the article we focus on two issues about the twin-candidate model. In the ﬁrst part, we will introduce the framework of the twin-candidate model for anaphora resolution, including detailed training procedures and resolution schemes. In the second part, we will further explore how to deploy the twin-candidate model in the more complicated task of coreference resolution. We will present an empirical evaluation of the twin-candidate model in different domains, using the Automatic Content Extraction (ACE) data sets. The experimental results indicate that the twin-candidate model is superior to the single-candidate model for the task of pronominal anaphora resolution. For the coreference resolution task, it also performs equally well, or better. 2. Related Work To our knowledge, the ﬁrst work on the twin-candidate model for anaphora resolution was proposed by Connolly, Burger, and Day (1997). Their work relied on a set of features that included lexical type, grammatical role, recency, and number/gender/semantic agreement, and employed a simple linear search scheme to choose the most preferred candidate. Their system produced a relatively low accuracy rate for pronoun resolution (55.3%) and deﬁnite NP resolution (37.4%) on a set of selected news articles. Iida et al. (2003) used the twin-candidate model (called the tournament model in their work) to perform Japanese zero-anaphora resolution. They utilized the same linear 328  Yang, Su, and Tan  A Twin-Candidate Model for AR  scheme to search for antecedents. Compared with Connolly, Burger, and Day (1997), they adopted richer features in which centering information was incorporated to capture contextual knowledge. Their system achieved an accuracy of around 70% on a data set drawn from a corpus of newspaper articles. Both of these studies were carried out on uncommon data sets, which makes it difﬁcult to compare their results with other baseline systems. In contrast to the previous work, we will explore the twincandidate model comprehensively by describing the model in more detail, trying more effective resolution schemes, deploying the model in the more complicated coreference resolution task, performing more extensive experiments, and evaluating the model in more depth. Denis and Baldridge (2007) proposed a pronoun resolution system that directly used a ranking learning algorithm (based on Maximal Entropy) to train a preference classiﬁer for antecedent selection. They reported an accuracy of around 72–76% for the different domains in the ACE data set. In our study, we will also investigate the solution of using a general ranking learner (e.g., Ranking-SVM). By comparison, the twin-candidate model is applicable to any discriminative learning algorithm, no matter whether it is capable of ranking learning or not. Moreover, as the model is trained and tested on pairwise candidates, it can effectively capture various relationships between candidates for better preference learning and determination. Ng (2005) presented a ranking model for coreference resolution. The model focused on the preference between the potential partitions of NPs, instead of the potential antecedents of an NP as in our work. Given an input document, the model ﬁrst employed n pre-selected coreference resolution systems to generate n candidate partitions of NPs. The model learned a preference classiﬁer (trained using Ranking-SVM) that could distinguish good and bad partitions during testing. The best rank partition would be selected as the resolution output of the current text. The author evaluated the model on the ACE data set and reported an F-measure of 55–69% for the different domains. Although ranking-based, Ng’s model is quite different from ours as it operates at the cluster-level whereas ours operates at the mention-level. In fact, the result of our twincandidate system can be used as an input to his model.  3. The Twin-Candidate Model for Anaphora Resolution 3.1 The Single-Candidate Model Learning-based anaphora resolution uses a machine learning method to obtain p(ante (Ck)|ana, C1, C2, . . . , Cn), the probability that a candidate Ck is the antecedent of the anaphor ana in the context of its antecedent candidates, C1, C2, . . . , Cn. The singlecandidate model assumes that the probability that Ck is the antecedent is only dependent on the anaphor ana and Ck, and independent of all the other candidates. That is:  p (ante(Ck) | ana, C1, C2, . . . , Cn) = p (ante(Ck) | ana, Ck)  (1)  Thus, the probability of a candidate Ck being the antecedent can be approximated using the classiﬁcation result on the instance describing the anaphor and Ck alone. The single-candidate model is widely used in most anaphora resolution systems (Aone and Bennett 1995; Ge, Hale, and Charniak 1998; Preiss 2001; Strube and Mueller 2003; Kehler et al. 2004; Ng et al. 2005). In our study, we also build as the  329  Computational Linguistics  Volume 34, Number 3  Table 1 A sample text for anaphora resolution. [1 Those ﬁgures] are almost exactly what [2 the government] proposed to [3 legislators] in [4 September]. If [5 the government] can stick with [6 them], [7 it] will be able to halve this year’s 120 billion ruble (US $193 billion) deﬁcit.  Table 2 Training instances generated under the single-candidate model for anaphora resolution.  Anaphor  Training Instance  Label  i{[6 them] , [1 Those ﬁgures]}  
† Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. 1 www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accepted for publication: 20 October 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 3  and Manning 2003). It is useful to understand generic algorithms that may support all these tasks and more. Rounds (1970) and Thatcher (1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language: Recent developments in the theory of automata have pointed to an extension of the domain of deﬁnition of automata from strings to trees . . . parts of mathematical linguistics can be formalized easily in a tree-automaton setting . . . We investigate decision problems and closure properties. Our results should clarify the nature of syntax-directed translations and transformational grammars . . . (Rounds 1970) The Rounds/Thatcher tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees independently, with each subtree transformed depending only on its own passed-down state. This class of transducer, called R in earlier works (Gécseg and Steinby 1984; Graehl and Knight 2004) for “root-to-frontier,” is often nowadays called T, for “top-down”. Rounds uses a mathematics-oriented example of a T transducer, which we repeat in Figure 1. At each point in the top-down traversal, the transducer chooses a production to apply, based only on the current state and the current root symbol. The traversal continues until there are no more state-annotated nodes. Non-deterministic transducers may have several productions with the same left-hand side, and therefore some free choices to make during transduction. A T transducer compactly represents a potentially inﬁnite set of input/output tree pairs: exactly those pairs (T1, T2) for which some sequence of productions applied to T1 (starting in the initial state) results in T2. This is similar to an FST, which compactly represents a set of input/output string pairs; in fact, T is a generalization of FST. If we think of strings written down vertically, as degenerate trees, we can convert any FST into a T transducer by automatically replacing FST transitions with T productions, as follows: If an FST transition from state q to state r reads input symbol A and outputs symbol B, then the corresponding T production is q A(x0) → B(r x0). If the FST transition output is epsilon, then we have instead q A(x0) → r x0, or if the input is epsilon, then q x0 → B(r x0). Figure 2 depicts a sample FST and its equivalent T transducer. T does have some extra power beyond path following and state-based recordkeeping. It can copy whole subtrees, and transform those subtrees differently. It can also delete subtrees without inspecting them (imagine by analogy an FST that quits and accepts right in the middle of an input string). Variants of T that disallow copying and deleting are called LT (for linear) and NT (for nondeleting), respectively. One advantage to working with tree transducers is the large and useful body of literature about these automata; two excellent surveys are Gécseg and Steinby (1984) and Comon et al. (1997). For example, it is known that T is not closed under composition (Rounds 1970), and neither are LT or B (the “bottom-up” cousin of T), but the noncopying LB is closed under composition. Many of these composition results are ﬁrst found in Engelfriet (1975). The power of T to change the structure of an input tree is surprising. For example, it may not be initially obvious how a T transducer can transform the English structure S(PRO, VP(V, NP)) into the Arabic equivalent S(V, PRO, NP), as it is difﬁcult to move the subject PRO into position between the verb V and the direct object NP. First, T productions have no lookahead capability—the left-hand-side of the S production 392  Graehl, Knight, and May  Training Tree Transducers  Figure 1 Part of a sample T tree transducer, adapted from Rounds (1970). consists only of q S(x0, x1), although we want the English-to-Arabic transformation to apply only when it faces the entire structure q S(PRO, VP(V, NP)). However, we can simulate lookahead using states, as in these productions: q S(x0, x1) → S(qpro x0, qvp.v.np x1) qpro PRO → PRO qvp.v.np VP(x0, x1) → VP(qv x0, qnp x1) 393  Computational Linguistics  Volume 34, Number 3  Figure 2 An FST and its equivalent T transducer. By omitting rules like qpro NP → ..., we ensure that the entire production sequence will dead-end unless the ﬁrst child of the input tree is in fact PRO. So ﬁnite lookahead (into inputs we don’t delete) is not a problem. But these productions do not actually move the subtrees around. The next problem is how to get the PRO to appear between the V and NP, as in Arabic. This can be carried out using copying. We make two copies of the English VP, and assign them different states, as in the following productions. States encode instructions for extracting/positioning the relevant portions of the VP. For example, the state qleft.vp.v means “assuming this tree is a VP whose left child is V, output only the V, and delete the right child”: q S(x0, x1) → S(qleft.vp.v x1, qpro x0, qright.vp.np x1) qpro PRO → PRO qleft.vp.v VP(x0, x1) → qv x0 qright.vp.np VP(x0, x1) → qnp x1 With these rules, the transduction proceeds as in Figure 3. This ends our informal presentation of tree transducers. Although general properties of T are understood, there are many algorithmic questions. In this article, we take on the problem of training probabilistic T transducers. For many language problems (machine translation, paraphrasing, text compression, etc.), it is possible to collect training data in the form of tree pairs and to distill linguistic knowledge automatically. Our problem statement is: Given (1) a particular transducer 394  Graehl, Knight, and May  Training Tree Transducers  Figure 3 Multilevel re-ordering of nodes in a T-transducer.  with rules R, and (2) a ﬁnite training set of sample input/output tree pairs, we want to produce (3) a probability estimate for each rule in R such that we maximize the probability of the output trees given the input trees. As with the forward–backward algorithm, we seek at least a local maximum. Tree transducers with weights have been studied (Kuich 1999; Engelfriet, Fülöp, and Vogler 2004; Fülöp and Vogler 2004) but we know of no existing training procedure. Sections 2–4 of this article deﬁne basic concepts and recall the notions of relevant automata and grammars. Sections 5–7 describe a novel tree transducer training algorithm, and Sections 8–10 describe a variant of that training algorithm for trees and strings. Section 11 presents an example linguistic tree transducer and provides empirical evidence of the feasibility of the training algorithm. Section 12 describes how the training algorithm may be used for training context-free grammars. Section 13 discusses related and future work.  2. Trees  TΣ is the set of (rooted, ordered, labeled, ﬁnite) trees over alphabet Σ. An alphabet is a ﬁnite  set. (see Table 1)  TΣ(X) are the trees over alphabet Σ, indexed by X—the subset of TΣ∪X where only  leaves may be labeled by X (TΣ(∅) = TΣ). Leaves are nodes with no children.  The  ∞ i=0  Ni  nodes (N0 ≡  of a tree t {()}). The  are identiﬁed size of a tree  one-to-one with its paths: pathst ⊂ paths ≡ is the number of nodes: |t| = |pathst|. The  N∗ ≡ path  to the root is the empty sequence (), and p1 extended by p2 is p1 · p2, where · is the  concatenation operator:  (a1, . . . , an) · (b1, . . . , bm) ≡ (a1, . . . , an, b1, . . . , bm) For p ∈ pathst, rankt(p) is the number of children, or rank, of the node at p, and labelt(p) ∈ Σ is its label. The ranked label of a node is the pair labelandrankt(p) ≡ (labelt(p), rankt(p)). For 1 ≤ i ≤ rankt(p), the ith child of the node at p is located at  395  Computational Linguistics  Volume 34, Number 3  Table 1 Notation guide. Notation (w)FS(T, A) (w)RTG (x)(L)(N)T(s) (S)T(A, S)G (S, P)CFG R+ N ∅ ≡ |A| X∗ a·b <lex Σ t ∈ TΣ TΣ (X ) A(t) A((x1, . . . , xn )) p paths pathst pathst({A, B}) t↓p rankt (p) labelt (p) labelandrankt (p) t[p ← t ] t[p ← tp, ∀p ∈ P] yieldt (X ) S∈N P, R D(M) LD(M) wM(d ∈ D(M)) WM (x) L(M) ∆ Qi ∈ Q λ ∈ xTPATΣ True s ∈ Σ∗ s[i] indicess letterss |s| spanss s ↓ (i, j) s ↓ [i] s[p ← s ] s[p ← sp, ∀p ∈ P]  Meaning (weighted) ﬁnite-state string (transducers,acceptors) (weighted) regular tree grammars (generalizes PCFG) (extended) (linear) (nondeleting) top–down tree(-to-string) transducers (synchronous) tree (adjoining,substitution) grammars (synchronous,probabilistic) context-free grammars positive real numbers natural numbers: {1, 2, 3, . . .} empty set equals (by deﬁnition) size of ﬁnite set A Kleene star of X, i.e., strings over alphabet X: {(x1, . . . , xn) | n ≥ 0} String concatenation: (1) · (2, 3) = (1, 2, 3) lexicographic (dictionary) order: () < (1) < (1, 1) < . . . < (1, 2) < . . . alphabet (set of symbols) (commonly: input tree alphabet) t is a tree with label alphabet Σ ... and with variables from additional leaf label alphabet X tree constructed by placing a unary A above tree t tree constructed by placing an n-ary A over leaves (x1, . . . , xn) tree path, e.g., (a, b) is the bth child of the ath child of root the set of all tree paths (≡ N∗) subset of paths that lead to actual nodes in t paths that lead to nodes labeled A or B in t the subtree of t with root at p, so that (t ↓ p) ↓ q = t ↓ (p · q) the number of children of the node p of t the label of node p of t the pair (labelt(p), rankt(p)) substitution of tree t for the subtree t ↓ p parallel substitution of tree tp for each t ↓ p the left → right concatenation of the X labels of the leaves of t start nonterminal of a regular tree grammar productions of a regular tree grammar, rules of a tree transducer derivations (keeping a list of applied rewrites) of M leftmost derivations of M weight of a derivation d: product of weight of each rule usage total weight of x in M: sum of weight of all LD(M) producing x weighted tree set, tree relation, or tree-to-string relation of M output tree alphabet initial (start) state of a transducer functions from TΣ to {0, 1} that examine ﬁnitely many paths the tree pattern True(t) ≡ 1, ∀t s is a string from alphabet Σ, e.g., () the empty string ith letter of string s - the ith projection πi i such that s[i] exists: (1, . . . , |s|) set of all letters s[i] in s length of string; |s| = |indicess|, not |letterss| Analogous to tree paths, pairs (i,j) denoting substrings The substring (s[i], . . . , s[j − 1]) indicated by the span (i, j) ∈ spanss same as s[i]; [i] stands for the span (i, i + 1) Substitution of string s for span p of s Parallel (non-overlapping) substitution of string sp for each s ↓ p  396  Graehl, Knight, and May  Training Tree Transducers  path p · (i). The subtree at path p of t is t ↓ p, deﬁned by pathst↓p ≡ {q | p · q ∈ pathst} and labelandrankt↓p(q) ≡ labelandrankt(p · q). The paths to X in t are pathst(X) ≡ {p ∈ pathst | labelt(p) ∈ X}. A set of paths F ⊂ paths is a frontier iff it is pairwise preﬁx-independent:  ∀p1, p2 ∈ F, p ∈ paths : p1 = p2 · p =⇒ p1 = p2 We write F for the set of all frontiers. F is a frontier of t, if F ⊂ Ft is a frontier whose paths are all valid for t—Ft ≡ F ∩ pathst. For t, s ∈ TΣ(X), p ∈ pathst, t[p ← s] is the substitution of s for p in t, where the subtree at path p is replaced by s. For a frontier F of t, the parallel substitution of tp for the frontier F ∈ Ft in t is written t[p ← tp, ∀p ∈ F], where there is a tp ∈ TΣ(X) for each path p. The result of a parallel substitution is the composition of the serial substitutions for all p ∈ F, replacing each t ↓ p with tp. (If F were not a frontier, the result would vary with the order of substitutions sharing a common preﬁx.) For example: t[p ← t ↓ p · (1), ∀p ∈ F] would splice out each node p ∈ F, replacing it by its ﬁrst subtree. Trees may be written as strings over Σ ∪ {(, )} in the usual way. For example, the tree t = S(NP, VP(V, NP)) has labelandrankt((2)) = (VP, 2) and labelandrankt((2, 1)) = (V, 0). Commas, written only to separate symbols in Σ composed of several typographic letters, should not be considered part of the string. For example, if we write σ(t) for σ ∈ Σ, t ∈ TΣ, we mean the tree with labelσ(t)(()) ≡ σ, rankσ(t)(()) ≡ 1 and σ(t) ↓ (1) ≡ t. Using this notation, we can give a deﬁnition of TΣ(X):  If x ∈ X, then x ∈ TΣ(X)  (1)  If σ ∈ Σ, then σ ∈ TΣ(X)  (2)  If σ ∈ Σ and t1, . . . , tn ∈ TΣ(X), then σ(t1, . . . , tn) ∈ TΣ(X)  (3)  The yield of X in t is yieldt(X), the concatenation (in lexicographic order2) over paths to leaves l ∈ pathst (such that rankt(l) = 0) of labelt(l) ∈ X—that is, the string formed by reading out the leaves labeled with X in left-to-right order. The usual case (the yield of t)  is yieldt ≡ yieldt(Σ). More precisely,    l  if r = 0 ∧ l ∈ X where (l, r) ≡ labelandrankt(())  yieldt (X )  ≡    () •ri=1 yieldt↓(i) (X )  if r = 0 ∧ l otherwise  ∈  X  where  •ri=1si  ≡  s1  ·  .  .  .  ·  sr  3. Regular Tree Grammars In this section, we describe the regular tree grammar, a common way of compactly representing a potentially inﬁnite set of trees (similar to the role played by the regular grammar for strings). We describe the version where trees in a set have different weights, in the same way that a weighted ﬁnite-state acceptor gives weights for strings  2 () <lex (a), (a1 ) <lex (a2 ) iff a1 < a2, (a1 ) · b1 <lex (a2 ) · b2 iff a1 < a2 ∨ (a1 = a2 ∧ b1 <lex b2 ).  397  Computational Linguistics  Volume 34, Number 3  Σ = {S, NP, VP, PP, PREP, DET, N, V, run, the, of, sons, daughters} N = {qnp, qpp, qdet, qn, qprep} S=q P = {q →1.0 S(qnp, VP(VB(run))), qnp →0.6 NP(qdet, qn), qnp →0.4 NP(qnp, qpp), qpp →1.0 PP(qprep, np), qdet →1.0 DET(the), qprep →1.0 PREP(of), qn →0.5 N(sons), qn →0.5 N(daughters)} Sample generated trees: S(NP(DT(the), N(sons)), VP(V(run))) (with probability 0.3) S(NP(NP(DT(the), N(sons)), PP(PREP(of), NP(DT(the), N(daughters)))), VP(V(run))) (with probability 0.036) Figure 4 A sample weighted regular tree grammar (wRTG).  in a regular language; when discussing weights, we assume the commutative semiring ({r ∈ R | r ≥ 0}, +, ·, 0, 1) of nonnegative reals with the usual sum and product. A weighted regular tree grammar (wRTG) G is a quadruple (Σ, N, S, P), where Σ is the alphabet, N is the ﬁnite set of nonterminals, S ∈ N is the start (or initial) nonterminal, and P ⊆ N × TΣ(N) × R+ is a ﬁnite set of weighted productions (R+ ≡ {r ∈ R | r > 0}). A production (lhs, rhs, w) is written lhs →w rhs (if w is omitted, the multiplicative identity 1 is assumed). Productions whose rhs contains no nonterminals (rhs ∈ TΣ) are called terminal productions, and rules of the form A →w B, for A, B ∈ N are called -productions, or state-change productions, and can be used in lieu of multiple initial nonterminals. Figure 4 shows a sample wRTG. This grammar generates an inﬁnite number of trees. We deﬁne the binary derivation relation on terms TΣ(N) and derivation histories (TΣ(N × (paths × P)∗): ⇒G≡ ((a, h), (b, h · (p, (l, r, w)))) | (l, r, w) ∈ P∧ p ∈ pathsa({l})∧ b = a[p ← r]  398  Graehl, Knight, and May  Training Tree Transducers  That is, (a, h) ⇒G (b, h · (p, (l, r, w))) iff b may be derived from a by using the rule l →w r to replace the nonterminal leaf l at path p with r. The reﬂexive, transitive closure of ⇒G is written ⇒∗G, and the derivations of G, written D(G), are the ways the start nonterminal may be expanded into entirely terminal trees: D(G) ≡ (t, h) ∈ TΣ × (paths × P)∗ | (S, ()) ⇒∗G (t, h) We also project the ⇒∗G relation so that it refers only to trees: t ⇒∗G t iff ∃h , h ∈ (paths × P)∗ : (t , h ) ⇒∗G (t, h). We take the product of the used weights to get the weight of a derivation d ∈ D(G): n wG((t, (h1, . . . , hn)) ∈ D(G)) ≡ wi where hi = (pi, (li, ri, wi)) i=1 The leftmost derivations of G build a tree preorder from left to right (always expanding the leftmost nonterminal in its string representation): LD(G) ≡ (t, ((p1, r1), . . . , (pn, rn))) ∈ DG | ∀1 ≤ i < n : pi+1 ≮lex pi The total weight of t in G is given by WG : TΣ → R, the sum of the weights of leftmost derivations producing t: WG(t) ≡ (t,h)∈LD(G) wG((t, h)). Collecting the total weight of every possible (nonzero weight) output tree, we call L(G) the weighted tree language of G, where L(G) = {(t, w) | WG(t) = w ∧ w > 0} (the unweighted tree language is simply the ﬁrst projection). For every weighted context-free grammar, there is an equivalent wRTG that generates its weighted derivation trees (whose yield is a string in the context-free language), and the yield of any regular tree language is a context-free string language (Gécseg and Steinby 1984). We can also interpret a regular tree grammar as a context-free string grammar with alphabet Σ ∪ {(, )}. wRTGs generate (ignoring weights) exactly the recognizable tree languages, which are sets of trees accepted by a non-transducing automaton version of T. This acceptor automaton is described in Doner (1970) and is actually a closer mechanical analogue to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection (Gécseg and Steinby 1984), and the constructive proof also applies to weighted wRTG intersection. There is a normal form for wRTGs analogous to that of regular grammars: Right-hand sides are a single terminal root with (optional) nonterminal children. What is sometimes called a forest in natural language generation (Langkilde 2000; Nederhof and Satta 2002) is a ﬁnite wRTG without loops—for all valid derivation trees, each nonterminal may only occur once in any path from root to a leaf: ∀n ∈ N, t ∈ TΣ(N), h ∈ (paths × P)∗ : (n, ()) ⇒∗G (t, h) =⇒ pathst({n}) = ∅ RTGs produce tree sets equivalent to those produced by tree substitution grammars (TSGs) (Schabes 1990) up to relabeling. The relabeling is necessary because RTGs distinguish states and tree symbols, which are conﬂated in TSGs at the elementary tree root. Regular tree languages are strictly contained in tree sets of tree adjoining grammars (TAG; Joshi and Schabes 1997), which generate string languages strictly between the context-free and indexed languages. RTGs are essentially TAGs without auxiliary trees  399  Computational Linguistics  Volume 34, Number 3  and their adjunction operation; the productions correspond exactly to TAG’s initial trees and the elementary tree substitution operation.  4. Extended-LHS Tree Transducers (xT) Section 1 informally described the root-to-frontier transducer class T. We saw that T allows, by use of states, ﬁnite lookahead and arbitrary rearrangement of non-sibling input subtrees removed by a ﬁnite distance. However, it is often easier to write rules that explicitly represent such lookahead and movement, relieving the burden on the user to produce the requisite intermediary rules and states. We deﬁne xT, a generalization of weighted T. Because of its good ﬁt to natural language problems, xT is already brieﬂy touched on, though not deﬁned, in Section 4 of Rounds (1970). A weighted extended-lhs top-down tree transducer M is a quintuple (Σ, ∆, Q, Qi, R) where Σ is the input alphabet, and ∆ is the output alphabet, Q is a ﬁnite set of states, Qi ∈ Q is the initial (or start, or root) state, and R ⊆ Q × xTPATΣ × T∆(Q × paths) × R+ is a ﬁnite set of weighted transformation rules. xTPATΣ is the set of ﬁnite tree patterns: predicate functions f : TΣ → {0, 1} that depend only on the label and rank of a ﬁnite number of ﬁxed paths of their input. A rule (q, λ, rhs, w) is written q λ →w rhs, meaning that an input subtree matching λ while in state q is transformed into rhs, with Q × paths leaves replaced by their (recursive) transformations. The Q × paths leaves of a rhs are called nonterminals (there may also be terminal leaves labeled by the output tree alphabet ∆). xT is the set of all such transducers T; the set of conventional top-down transducers, is a subset of xT where the rules are restricted to use ﬁnite tree patterns that depend only on the root: TPATΣ ≡ {pσ,r(t)} where pσ,r(t) ≡ (labelt(()) = σ ∧ rankt(()) = r). Rules whose rhs are a pure T∆ with no states/paths for further expansion are called terminal rules. Rules of the form q λ →w q () are -rules, or state-change rules, which substitute state q for state q without producing output, and stay at the current input subtree. Multiple initial states are not needed: we can use a single start state Qi, and instead of each initial state q with starting weight w add the rule Qi True →w q () (where True(t) ≡ 1, ∀t). We deﬁne the binary derivation relation for xT transducer M on partially transformed terms and derivation histories TΣ∪∆∪Q × (paths × R)∗: ⇒M≡ ((a, h), (b, h · (i, (q, λ, rhs, w)))) | (q, λ, rhs, w) ∈ R ∧ i ∈ pathsa ∧ q = labela(i) ∧ λ(a ↓ (i · (1))) = 1 ∧ p ← q (a ↓ (i · (1) · i )), b = a i ← rhs ∀p ∈ pathsrhs : labelrhs(p) = (q , i ) That is, b is derived from a by application of a rule q λ →w rhs to an unprocessed input subtree a ↓ i which is in state q, replacing it by output given by rhs with variables (q , i ) replaced by the input subtree at relative path i in state q .3 Let ⇒∗M, D(M), LD(M), wM, WM, and L(M) (the weighted tree relation of M) follow from the single-step ⇒M exactly as they did in Section 3, except that the arguments are 3 Recall that q(a) is the tree whose root is labeled q and whose single child is the tree a. 400  Graehl, Knight, and May  Training Tree Transducers  input and output instead of just output, with initial terms Qi(t) for each input t ∈ TΣ in place of S: D(M) ≡ (t, t , h) ∈ TΣ × T∆ × (paths × R)∗ | (Qi(t), ()) ⇒∗M (t , h) We have given a rewrite semantics for our transducer, similar to wRTG. In the intermediate terms of a derivation, the active frontier of computation moves top-down, with everything above that frontier forming the top portion of the ﬁnal output. The next rewrite always occurs somewhere on the frontier, and in a complete derivation, the frontier ﬁnally shrinks and disappears. In wRTG, the frontier consisted of the nonterminallabeled leaves. In xT, the frontier items are not nonterminals, but pairs of state and input subtrees. We choose to represent these pairs as subtrees of terms with labels taken from Σ ∪ ∆ ∪ Q, where the state is the parent of the input subtree. In fact, given an M ∈ xT and an input tree t, we can take all the (ﬁnitely many) pairs of input subtrees and states as nonterminals in a wRTG G, with all the (ﬁnitely many) possible single-step derivation rewrites of M applied to t as productions (taking the weight of the xT rule used), and the initial term Qi(t) as the start nonterminal, isomorphic to the derivations of the M which start with Qi(t): (d, h) ∈ D(G) iff (t, d, h) ∈ D(M). Such derivations are exactly how all the outputs of an input tree t are produced: when the resulting term d is in T∆, we say that (t, d) is in the tree relation and that d is an output of t. Naturally, there may be input trees for which no complete derivation exists—such inputs are not in the domain of the weighted tree relation, having no output. It is known that domain(M) ≡ {i | ∃o, w : (i, o, w) ∈ L(M)}, the set of inputs that produce any output, is always a recognizable tree language (Rounds 1970). The sources of a rule r = (q, l, rhs, w) ∈ R are the input-paths in the rhs: sources(r) ≡ {i | ∃p ∈ pathsrhs(Q × paths), q ∈ Q : labelrhs(p) = (q , i )} If the sources of a rule refer to input paths that do not exist in the input, then the rule cannot apply (because a ↓ (i · (1) · i ) would not exist). In the traditional statement of T, sources(r) are the variables xi, standing for the ith child of the root at path (i), and the right hand sides of rules refer to them by name: (qi, xi). In xT, however, we refer to the mapped input subtrees by path (and we are not limited to the immediate children of the root of the subtree under transformation, but may choose any frontier of it). A transducer is linear if for all its rules r, sources(r) are a frontier and occur at most once: ∀p1, p2 ∈ pathsrhs(Q × paths), p ∈ paths − {()} : p1 = p2 · p. A transducer is deterministic if for any input, at most one rule matches per state: ∀q ∈ Q, t ∈ TΣ, r = (q, p, r, w), r = (q , p , r , w ) ∈ R : p(t) = 1 ∧ p (t) = 1 =⇒ r = r or in other words, the rules for a given state have patterns that partition possible input trees. A transducer is deleting if there are rules in which (for some matching inputs) entire subtrees are not used in their rhs. In practice, we will be interested mostly in concrete transducers, where the patterns fully specify the labels and ranks of an input subtree including all the ancestors of sources(r). Naturally, T are concrete. We have taken to writing concrete rules’ patterns as trees with variables X in the leaves (at the sources), and using those same  401  Computational Linguistics  Volume 34, Number 3  variables in the rhs instead of writing the corresponding path in the lhs. For example: q A(x0:B, C) →w q x0 means a xT rule (q, λ, rhs, w) with rhs = (q , (1)) and λ ≡ (labelandrankt(()) = (A, 1) ∧ labelt((1)) = B ∧ labelandrankt((2)) = (C, 0)) It might be convenient to convert any xT transducer to an equivalent T transducer, then process it with T-based algorithms—in such a case, xT would just be syntactic sugar for T. We can automatically generate T productions that use extra states to emulate the ﬁnite lookahead and movement available in xT (as demonstrated in Section 1), but with one fatal ﬂaw: Because of the deﬁnition of ⇒M, xT (and thus T) only has the ability to process input subtrees that produce corresponding output subtrees (alas, there is no such thing as an empty tree), and because TPAT can only inspect the root node while deriving replacement subtrees, T can check only the parts of the input subtree that lie along paths that are referenced in the rhs of the xT rule. For example, suppose we want to transform NP(DET, N) (but not, say, NP(ADJ, N)) into the tree N using rules in T. Although this is a simple xT rule, the closest we can get with T would be q NP(x0, x1) → q.N x1, but we cannot check both subtrees without emitting two independent subtrees in the output (which rules out producing just N). Thus, xT is a bit more powerful than T. 5. Parsing an xT Tree Relation Derivation trees for a transducer M = (Σ, ∆, Q, Qi, R) are TR (trees labeled by rules) isomorphic to complete leftmost M-derivations. Figure 5 shows derivation trees for a particular transducer. In order to generate derivation trees for M automatically, we build a modiﬁed transducer M . This new transducer produces derivation trees on its output instead of normal output trees. M is (Σ, R, Q, Qi, R ), with4 R ≡ {(q, λ, r(yieldrhs(Q × paths)), w) | r = (q, λ, rhs, w) ∈ R} That is, the original rhs of rules are ﬂattened into a tree of depth 1, with the root labeled by the original rule, and all the non-expanding ∆-labeled nodes of the rhs removed, so that the remaining children are the nonterminal yield in left to right order. Derivation trees deterministically produce a single weighted output tree, and for concrete transducers, a single input tree. For every leftmost derivation there is exactly one corresponding derivation tree: We start with a sequence of leftmost derivations and promote rules applied to paths that are preﬁxes of rules occurring later in the sequence (the ﬁrst will always be the root), or, in the other direction, list out the rules of the derivation tree in order.5 The weights of derivation trees are, of course, just the product of the weights of the rules in them.6 The derived transducer M nicely produces derivation trees for a given input, but in explaining an observed (input/output) pair, we must restrict the possibilities further. Because the transformations of an input subtree depend only on that subtree and its state, we can build a compact wRTG that produces exactly the weighted derivation trees corresponding to M-transductions (I, ()) ⇒∗M (O, h) (Algorithm 1). 4 By r((t1, . . . , tn )), we mean the tree r(t1, . . . , tn ). 5 Some path concatenation is required, because paths in histories are absolute, whereas the paths in rule rhs are relative to the input subtree. 6 Because our product is commutative, the order does not matter. 402  Graehl, Knight, and May  Training Tree Transducers  Figure 5 Derivation trees for a T tree transducer. Algorithm 1 makes use of memoization—the possible derivations for a given (q, i, o) are constant, so we store answers for all past queries in a lookup table and return them, avoiding needless recomputation. Even if we prove that there are no derivations for some (q, i, o), successful subhypotheses met during the proof may recur and are kept, but we do avoid adding productions we know can’t succeed. We have in the worst case to visit all |Q| · |I| · |O| (q, i, o) pairs and apply all |R| transducer rules successfully at each of them, so time and space complexity, proportional to the size of the (unpruned) output wRTG, are both O(|Q| · |I| · |O| · |R|), or O(Gn2), where n is the total size of the 403  Computational Linguistics  Volume 34, Number 3  Algorithm 1. Deriv (derivation forest for I ⇒∗xT O) Input: xT transducer M = (Σ, ∆, Q, Qi, R) and observed tree pair I ∈ TΣ, O ∈ T∆. Output: derivation wRTG G = (R, N ⊆ Q × pathsI × pathsO, S, P) generating all weighted derivation trees for M that produce O from I. Returns false instead if there are no such trees. O(G|I||O|) time and space complexity, where G is a grammar constant. begin S ← (Qi, (), ()), N ← ∅, P ← ∅, memo ← ∅ if PRODUCEI,O(S) then N ← {n | ∃(n , rhs, w) ∈ P : n = n ∨ n ∈ yieldrhs(Q × pathsI × pathsO)} return G = (R, N, S, P) else return false end PRODUCEI,O(α = (q, i, o) ∈ Q × pathsI × pathsO) returns boolean ≡ begin if ∃(α, r) ∈ memo then return r memo ← memo ∪ {(α, true)} anyrule? ← false for r = (q, λ, rhs, w) ∈ R : λ(I ↓ i) = 1 ∧ MatchO,∆(rhs, o) do (o1, . . . , on) ← pathsrhs(Q × paths) sorted by o1 <lex . . . <lex on //n = 0 if there are no rhs variables labelandrankderivrhs(()) ← (r, n) //derivrhs is a newly created tree for j ← 1 to n do (q , i ) ← labelrhs(oj) β ← (q , i · i , o · oj) if ¬PRODUCEI,O(β) then next r labelandrankderivrhs((j)) ← (β, 0) anyrule? ← true P ← P ∪ {(α, derivrhs, w)} memo ← memo ∪ {(α,anyrule?)} return anyrule? end Matcht,Σ(t , p) ≡ ∀p ∈ path(t ) : label(t , p ) ∈ Σ =⇒ labelandrankt (p ) = labelandrankt(p · p )  input and output trees, and G is the grammar constant accounting for the states and rules (and their size). If the transducer contains cycles of state-change rules, then the generated derivation forest may have inﬁnitely many trees in it, and thus the memoization of PRODUCE must temporarily assume that the alignment (q, i, o) under consideration will succeed upon reaching itself, through such a cycle, even though the answer is not yet conclusive (it may be conclusively true, but not false). Although it would be possible to detect these cycles (setting “pending” rather than true for the interim in memo) and deal with them more severely, we can just remove the surplus later in linear time, using Algorithm 2, which is an implementation (for wRTG) of a well-known method of pruning useless 404  Graehl, Knight, and May  Training Tree Transducers  Algorithm 2. RTGPrune (wRTG useless nonterminal/production identiﬁcation)  Input: wRTG G = (Σ, N, S, P), with P = (p1, . . . , pm) and pi = (qi, ti, wi). Output: For all n ∈ N, B[n] = (∃t ∈ TΣ : n ⇒∗G t) (true if n derives some output tree t with no remaining nonterminals, false if it’s useless), and A[n] = (∃t ∈ TΣ, t ∈ TΣ({n}) : S ⇒∗G t ⇒∗G t) (n additionally can be produced from an S using only productions that can appear in complete derivations).  Time and space complexity are linear in the total size of the input:  O(|N| +  m i=1  (1  +  |pathsti  |)  begin  M←∅  for n ∈ N do B[n] ← false, Adj[n] ← ∅  for i ← 1 to m do  Y ← {labelti (p) | p ∈ pathsti (N)} // Y are the unique N in rhs of rule i  for n ∈ Y do Adj[n] ← Adj[n] ∪ {i}  if |Y| = 0 then M ← M ∪ {i}  r[i] ← |Y|  for n ∈ M do REACH(n)  /* Now that B[n] are decided, compute A[n]  */  for n ∈ N do A[n] ← false  USE(S)  end  REACH(n) ≡ begin B[n] ← true for i ∈ Adj[n] do if ¬B[qi] then r[i] ← r[i] − 1 if r[i] = 0 then REACH(qi)  end  USE(n) ≡ begin  A[n] ← true  for n s.t. ∃(n, t, w) ∈ R : n ∈ yieldt(N) do  /* for n that are in the rhs of rules whose lhs is n  */  if ¬A[n ] ∧ B[n ] then USE(n )  end  productions from a CFG (Hopcroft and Ullman 1979).7 We eliminate all the remains of failed subforests, by removing all nonterminals n, and any productions involving n, where Algorithm 2 gives A[n] = false. In the next section, we show how to compute the contribution of a nonterminal to the weighted trees produced by a wRTG, in a generalization of Algorithm 2 that gives us weights that we accumulate per rule over the training examples, for EM training. 7 The idea is to ﬁrst remove all nonterminals (and productions referring to them) that don’t yield any terminal string, and after that, to remove those which are not reachable top-down from S. 405  Computational Linguistics  Volume 34, Number 3  6. Inside–Outside for wRTG Given a wRTG G = (Σ, N, S, P), we can compute the sums of weights of trees derived using each production by adapting the well-known inside–outside algorithm for weighted context-free (string) grammars (Lari and Young 1990). Inside weights βG for a nonterminal or production are the sum of weights of all trees that can be derived from it:  βG(n ∈ N) ≡  w · βG(r)  (n,r,w)∈P  βG(r ∈ TΣ(N) | (n, r, w) ∈ P}) ≡  βG (labelr (p ) )  p∈pathsr (N )  By deﬁnition, βG(S) gives the sum of the weights of all trees generated by G. For the wRTG generated by Deriv(M, I, O), this is exactly WM(I, O). The recursive deﬁnition of β does not assume a non-recursive wRTG. In the presence of derivation cycles with weights less than 1, β can still be evaluated as a convergent sum over an inﬁnite number of trees. The output of Deriv will always be non-recursive provided there are no cycles of -rules in the transducer. There is usually no reason to build such cycles, as the effect (in the unweighted case) is just to make all implicated states equivalent. Outside weights αG are for each nonterminal the sums over all its occurrences in complete derivations in the wRTG of the weight of the whole tree, excluding the occurrence subtree weight (we deﬁne this without resorting to division for cancellation, but in practice we may use division by βG(n) to achieve the same result).  αG(n ∈ N) ≡    uses of n in productions  
this distinction is irrelevant. Submission received: 19 June 2006; revised submission received: 30 May 2007; accepted for publication: 12 October 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 3  into patterns: The ﬁrst radical is inserted into the ﬁrst consonantal slot of the pattern, the second ﬁlls the second slot, and the third ﬁlls the last slot. See Shimron (2003) for a survey. We present a machine learning approach, augmented by limited linguistic knowledge, to the problem of identifying the roots of Semitic words. To the best of our knowledge, this is the ﬁrst application of machine learning to this problem, and one of the few attempts to directly address the non-concatenative morphology of Semitic languages using machine learning. Although there exist programs which can extract the roots of words in Arabic (Beesley 1998a, 1998b) and Hebrew (Choueka 1990), they are all dependent on labor-intensive construction of large-scale lexicons which are components of full-scale morphological analyzers. Note that the Arabic morphological analyzer of Buckwalter (2002, software documentation) only uses “word stems—rather than root and pattern morphemes—to identify lexical items.” Buckwalter further notes that “The information on root and pattern morphemes could be added to each stem entry if this were desired.” The challenge of our work is to automate this process, avoiding the bottleneck of having to laboriously list the root and pattern of each lexeme in the language. Identifying the root of a given word is a non-trivial problem, due to the complex nature of Semitic derivational and inﬂectional morphology and the peculiarities of the orthography. It is also an important task. Although existing morphological analyzers for Hebrew only provide a lexeme (which is a combination of a root and a pattern), for other Semitic languages, notably Arabic, the root is an essential part of any morphological analysis simply because traditional dictionaries are organized by root, rather than by lexeme (Owens 1997). Information on roots is important for linguistic research, because roots can shed light on etymological processes, both within a single language and across languages. Furthermore, roots are known to carry some meaning, albeit vague. This information can be useful for computational applications: For example, several studies show that indexing Arabic documents by root improves the performance of information retrieval systems (Al-Kharashi and Evens 1994; Abu-Salem, Al-Omari, and Evens 1999; Larkey, Ballesteros, and Connell 2002).2 The contributions of this article are manifold. First and foremost, we report on a practical system which can be used to extract roots in Hebrew and Arabic (the system is freely available; an on-line demo is provided at http://cl.haifa.ac.il/ projects/roots/index.shtml). The system can be used for practical applications or for scientiﬁc (linguistic) research, and constitutes an important addition to the growing set of resources dedicated to Semitic languages. It is one of the few attempts to directly address the non-concatenative morphology of Semitic languages and extract non-contiguous morphemes from surface forms. As a machine learning application, this work describes a set of experiments in combination of classiﬁers under constraints. The resulting insights can be used for other applications of the same techniques for similar problems (see, e.g., Habash and Rambow 2005). Furthermore, this work demonstrates that providing a data-driven classiﬁer with limited linguistic knowledge signiﬁcantly improves the classiﬁcation results. We focus on Hebrew in the ﬁrst part of this article. After sketching the linguistic data in Section 2 and our methodology in Section 3, we discuss in Section 4 a simple, baseline, learning approach. We then propose several methods for combining the results  2 These results are challenged by Darwish and Oard (2002), who conclude that roots are inferior to character n-grams for this task. 430  Daya, Roth, and Wintner  Identifying Semitic Roots  of interdependent classiﬁers in Section 5 and demonstrate the beneﬁts of using limited linguistic knowledge in the inference procedure. Then, the same technique is applied to Arabic in Section 6 and we demonstrate comparable improvements. In Section 7 we discuss the inﬂuence of global constraints on local classiﬁers. We conclude with suggestions for future research.  2. Linguistic Background Root and pattern morphology is the major word formation device of Semitic languages. As an example, consider the Hebrew roots g.d.l, k.t.b, and r.$.m and the patterns haCCaCa, hitCaCCut, and miCCaC, where the “C”s indicate the slots.3 When the roots combine with these patterns the resulting lexemes are hagdala, hitgadlut, migdal, haktaba, hitkatbut, miktab, har$ama, hitra$mut, and mir$am, respectively. After the root combines with the pattern, some morpho-phonological alternations take place, which may be non-trivial: For example, the hitCaCCut pattern triggers assimilation when the ﬁrst consonant of the root is t or d : thus, d.r.$+hitCaCCut yields hiddar$ut. The same pattern triggers metathesis when the ﬁrst radical is s or $ : s.d.r+hitCaCCut yields histadrut rather than the expected *hitsadrut. Semi-vowels such as w or y in the root are frequently combined with the vowels of the pattern, so that q.w.m+haCCaCa yields haqama, and so on. Frequently, root consonants such as w or y are altogether missing from the resulting form. These matters are complicated further due to two sources: First, the standard Hebrew orthography leaves most of the vowels unspeciﬁed.4 It does not explicate a and e vowels, does not distinguish between o and u vowels and leaves many of the i vowels unspeciﬁed. Furthermore, the single letter w is used both for the vowels o and u and for the consonant v, whereas i is similarly used both for the vowel i and for the consonant y. On top of that, the script dictates that many particles, including four of the most frequent prepositions, the deﬁnite article, the coordinating conjunction, and some subordinating conjunctions, all attach to the words which immediately follow them. Thus, a form such as mhgr can be read as a lexeme (‘immigrant’), as m-hgr (‘from Hagar’), or even as m-h-gr (‘from the foreigner’). Note that there is no deterministic way to tell whether the ﬁrst m of the form is part of the pattern, the root, or a preﬁxing particle (the preposition m ‘from’). The Hebrew script has 22 letters, all of which can be considered consonants. The number of tri-consonantal roots is thus theoretically bounded by 223, although several phonological constraints limit this number to a much smaller value. For example, although roots whose second and third radicals are identical abound in Semitic languages, roots whose ﬁrst and second radicals are identical are extremely rare (see McCarthy 1981 for a theoretical explanation). To estimate the number of roots in Hebrew we compiled a list of roots from two sources: a dictionary (Even-Shoshan 1993) and the verb paradigm tables of Zdaqa (1974). The union of these yields a list of 2,152 roots.5  3 To facilitate readability we use a straight-forward transliteration of Hebrew using ASCII characters, where the characters (in Hebrew alphabetic order) are: ’bgdhwzxviklmnsypcqr$t. 4 In this work we consider only texts in undotted, or unvocalized script. This is the standard script of both Hebrew and Arabic. 5 Only tri-consonantal roots are counted. Ornan (2003) mentions 3,407 roots, whereas the number of roots in Arabic is estimated to be 10,000 (Darwish 2002). We do not know why Arabic should have so many more roots than Hebrew. 431  Computational Linguistics  Volume 34, Number 3  Whereas most Hebrew roots are regular, many belong to weak paradigms, which means that root consonants undergo changes in some patterns. Examples include i or n as the ﬁrst root consonant, w or i as the second, i as the third, and roots whose second and third consonants are identical. For example, consider the pattern hCCCh. Regular roots such as p.s.q yield forms such as hpsqh. However, the irregular roots n.p.l, i.c.g, q.w.m, and g.n.n in this pattern yield the seemingly similar forms hplh, hcgh, hqmh, and hgnh, respectively. Note that in the ﬁrst and second examples, the ﬁrst radical (n or i ) is missing, in the third the second radical (w) is omitted, and in the last example one of the two identical radicals is omitted. Consequently, a form such as hC1C2h can have any of the roots n.C1.C2, C1.w.C2, C1.i.C2, C1.C2.C2 and even, in some cases, i.C1.C2. Although root and pattern morphology is the major word formation device of Semitic languages, both Hebrew and Arabic have words which are not generated through this mechanism, and therefore have no root. These are either loan words (which are oftentimes longer than originally Semitic words, in particular in the case of proper names) or short functional or frequent words whose origin is more ancient. For example, the most frequent token in Hebrew texts is the accusative preposition ’t, which is not formed through root and pattern processes. Of course, there may be surface forms which are ambiguous: one reading based on root and pattern morphology, the other a loan word, for example, npl (either ‘Nepal’ or ‘fall’ (past, 3rd person masculine singular)). Although the Hebrew script is highly ambiguous, ambiguity is somewhat reduced for the task we consider here, as many of the possible lexemes of a given form share the same root. Still, in order to correctly identify the root of a given word, context must be taken into consideration. For example, the form $mnh has more than a dozen readings, including the adjective ‘fat’ (feminine singular), which has the root $.m.n, and the verb ‘count,’ whose root is m.n.i, preceded by a subordinating conjunction. In the experiments we describe herein we ignore context completely, so our results are handicapped by design. Adding contextual information renders the problem very similar to that of word sense disambiguation (as different roots denote distinct senses), and we opted to focus only on morphology here. 3. Data and Methodology 3.1 Machine-Learning Framework In this work we apply several machine-learning techniques to the problem of root identiﬁcation. In all the experiments described in this article we use SNoW (Roth 1998; Carlson et al. 1999) as the learning environment, with Winnow as the update rule (using Perceptron yielded comparable results). SNoW is a multi-class classiﬁer that is speciﬁcally tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large. It is an on-line linear classiﬁer, as are most of the classiﬁers currently used in NLP, over a variable number of expressive features. In addition to the “standard” perceptron-like algorithms, SNoW has a number of extensions such as regularization and good treatment of multiclass classiﬁcation. SNoW provides, in addition to classiﬁcation, a reliable conﬁdence in the instance prediction which facilitates its use in an inference algorithm that combines predictors to produce a coherent inference. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks, including part-of-speech tagging, shallow parsing, information extraction tasks, and so forth, and compared favorably with other classiﬁers (Roth 1998; Golding and Roth 1999; Banko and Brill 2001; Punyakanok and  432  Daya, Roth, and Wintner  Identifying Semitic Roots  Roth 2001; Florian 2002; Punyakanok, Roth, and Yih 2005). The choice of SNoW as the learning algorithm in this work is motivated by its good performance on other, similar tasks and on the availability in SNoW of easy to tune state-of-the-art versions of three linear algorithms (Perceptron, Winnow, and naive Bayes) in a single package. As was shown by Roth (1998), Golding and Roth (1999), and in countless experimental papers thereafter, most algorithms used today, from on-line variations of Winnow and Perceptron to maximum entropy algorithms to SVMs, perform comparably if tuned properly, and the eventual performance depends mostly on the selection of features. 3.2 Data and Evaluation For training and testing, a Hebrew linguist manually tagged a corpus of 15,000 words (from a set of newspaper articles). Of these, only 9,752 were annotated; the reason for the gap is that some Hebrew words, mainly borrowed but also some frequent words such as prepositions, are not formed by the root and pattern paradigm. Such words are excluded from our experiments in this work; in an application, such words have to be identiﬁed and handled separately. This can be rather easily done using simple heuristics and a small list of frequent closed-class words, because words which do not conform to the root and pattern paradigm are either (short, functional) closed-class words, or loan words which tend to be longer and, in many cases, involve “foreign” characters (typically proper names). This problem is orthogonal to the problem of identifying the root, and hence a pipeline approach is reasonable. We further eliminated 168 roots with more than three consonants and were left with 5,242 annotated word types, exhibiting 1,043 different roots. Table 1 shows the distribution of word types according to root ambiguity. Table 2 provides the distribution of the roots of the 5,242 word types in our corpus according to root type, where Ri is the ith radical (note that some roots may belong to more than one group).  Table 1 Root ambiguity in the corpus.  Number of roots  
Computational Linguistics  Volume 34, Number 3  Chapter 2 provides a laundry list of morphological phenomena, arguing that ﬁnitestate composition captures each of them, even in cases where there is a more obvious solution (e.g., ﬁnite-state concatenation for concatenative phenomena). Examples of many kinds of phenomena are given from diverse languages: prosodic restrictions in Yowulmne, phonological effects of German afﬁxes, and subsegmental morphology in Welsh, to name a few. Importantly, the compile-reduce and merge operations are argued to be syntactic sugar for effects achievable by ﬁnite-state composition, so that even rootand-pattern Arabic morphology is explained in the same algebraic framework. Reduplication effects, of course, challenge ﬁnite-state explanations, and so receive their own section. Extended (non-regular) computational models are presented alongside data from Gothic, Dakota, and Sye. The authors speculate that, in contrast with the commonly accepted Correspondence theory, Morphological Doubling theory (Inkelas and Zoll 1999), if correct, would imply that a non-regular “copying” process is not at work in reduplication. It is at this point that the reader may experience some discomfort; should the reduplication problem be addressed in syntax rather than morphology? Where exactly does the boundary lie? Readers hoping for a reassessment of this boundary, or even a new bridge over it, will not ﬁnd it here. Chapter 3 begins with Stump’s (2001) two-dimensional taxonomy of morphological theories, which appears rather divorced from the rich work on ﬁnite-state computational morphology in Chapter 2. The subtleties among the four types of theories (lexical vs. inferential and incremental vs. realizational, a more nuanced breakdown of the debate over “item-and-arrangement” vs. “item-and-process”) may be difﬁcult to understand for the reader not trained in morphological theory, but resolution comes quickly. We are presented with a series of examples showing “proof-of-concept” fragmentary implementations (in AT&T’s lextools) of phenomena in Sanskrit, Swahili, and Breton to argue that lexical-incrementalist and inferential-realizational theories are computationally equivalent; both can be implemented using FSTs and can lead to the same models. Chapter 4 gives an algebraic analysis of Koskenniemi’s (1983) “KIMMO” twolevel morphological analysis system. Koskenniemi’s hand-coded morphology rules are argued to be a historical accident; if only computers had been more powerful in the 1980s, compilation of those rules into FSTs might have been automated, and in fact Kaplan and Kay had already developed the algorithms.1 In the spirit of the previous chapter, Sproat and Roark also note that morphological accounts that use one, two, or more “cascaded” levels are all computationally equivalent rational relations under the ﬁnite-state approach, and that Optimality Theory can (under certain assumptions about constraints) be implemented with ﬁnite-state operations as well (Ellison 1994). Chapter 5, “Machine learning of morphology,” focuses on unsupervised morphology induction methods. There is about a page of discussion on statistical language modeling approaches for disambiguation in agglutinative languages; no mention is made of the more recent use of discriminative machine learning in morphological disambiguation (Kudo, Yamamoto, and Matsumoto 2004; Habash and Rambow 2005; Smith, Smith, and Tromble 2005). The chapter focuses on the approaches of Goldsmith (2001), Schone and Jurafsky (2001), and Yarowsky and Wicentowski (2001). Although each approach is interesting on its own, little effort is made to unify work in this area, and none to bring the reader back full circle to ﬁnite-state models or the problem of inducing from data regular grammars (Stolcke and Omohundro 1993) or their weights  
 Computational Linguistics  Volume 34, Number 3  and then for a few months, but eventually you give up, and go back to using the same old tagger you used before. 2. The Paradox of Faith-Based Empiricism The tale of the Zigglebottom Tagger is one of disappointment, not just for you but also for Zigglebottom himself. While his work achieved publication, it must gnaw at his scientiﬁc conscience that he can’t reproduce his own results. The fact that you can’t reproduce those results either raises questions, but those are resolved with a shrug of your shoulders and by giving the beneﬁt of the doubt to Zigglebottom. He’s not a fraud; there’s just some crucial detail that is neither recorded in the article nor in the software, which can’t be installed and run in any case. The problem here is not the Zigglebottom article; as a community we accept that our publications don’t provide enough space to describe our elaborate 21st century empirical methods in sufﬁcient detail to allow for re-implementation and reproduction of results. This is true despite the generous page allowances in Computational Linguistics and even more so in our much more constrained conference proceedings. What’s really missing is the software that produced the results that convinced the reviewers the article should be published. This is particularly troubling given the highly empirical nature of the work reported in so many of our publications. We publish page after page of experimental results where apparently small differences determine the perceived value of the work. In this climate, convenient reproduction of results establishes a vital connection between authors and readers. Our community expects published papers to be rigorously reviewed and made available via open access as soon as possible (e.g., via the ACL Anthology1). We expect the supporting corpora and lexical resources will be made available even if at some cost (e.g., via the Linguistic Data Consortium2). Yet, we do not have the same expectations regarding our software. While we have table after table of results to pore over, we usually don’t have access to the software that would allow us to reproduce those results. This cuts to the core of whether we are engaged in science, engineering, or theology: Scientists reproduce results; engineers build impressive and enduring artifacts; and theologians muse about what they believe but can’t see or prove. Before you judge the analogy with theology as being too harsh, conduct the following experiment. Randomly select one of your own publications from a year or two ago and think about what would be involved in reproducing the results. How long would it take, assuming you would be able to do it? If you can’t reproduce those results, why do you believe them? Why should your readers? Our inability to reproduce results leads to a debilitating paradox, where we as reviewers and readers accept highly empirical results on faith. We do this routinely, to the point where we seem to have given up on the idea of being able to reproduce results. This is the natural consequence of faith-based empiricism, and the only way to ﬁght that movement is with a little bit of heresy. Let’s not accept large tables of empirical results on faith, let’s insist that we be able to reproduce them exactly and conveniently. Let’s insist that we are scientists ﬁrst and foremost, and agree that this means that we must be able to reproduce each other’s results.  
∗ One Microsoft Way, Redmond, WA 98052, USA. E-mail: kristout@microsoft.com. ∗∗ Department of Electrical Engineering and Computer Sciences, Soda Hall, Berkeley, CA 94720, USA. E-mail: aria42@cs.berkeley.edu. † Department of Computer Science, Gates Building 1A, 353 Serra Mall, Stanford CA 94305, USA. E-mail: manning@cs.stanford.edu. Submission received: 15 July 2006; Revised submission received: 1 May 2007; Accepted for publication: 19 June 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 2  [Final-hour trading]THEME accelerated [to 108.1 million shares]TARGET [yesterday]ARGM-TMP, the ﬁrst argument is the subject noun phrase ﬁnal-hour trading of the active verb accelerated. If we did not consider the rest of the sentence, it would look more like an AGENT argument, but when we realize that there is no other good candidate for a THEME argument, because to 108.1 million shares must be a TARGET and yesterday is most likely ARGM-TMP, we can correctly label it THEME. Even though previous work has modeled some correlations between the labels of parse tree nodes (see Section 2), many important phenomena have not been modeled. The key properties needed to model this joint structure are: (1) no ﬁnite Markov horizon assumption for dependencies among node labels, (2) features looking at the labels of multiple argument nodes and internal features of these nodes, and (3) a statistical model capable of incorporating these long-distance dependencies and generalizing well. We show how to build a joint model of argument frames, incorporating novel features into a discriminative log-linear model. This system achieves an error reduction of 24.1% on ALL arguments and 36.8% on CORE arguments over a state-of-the-art independent classiﬁer for gold-standard parse trees on Propbank. If we consider the linguistic basis for joint modeling of a verb’s arguments (including modiﬁers), there are at least three types of information to be captured. The most basic is to limit occurrences of each kind of argument. For instance, there is usually at most one argument of a verb that is an ARG0 (agent), and although some modiﬁer roles such as ARGM-TMP can fairly easily be repeated, others such as ARGM-MNR also generally occur at most once.1 The remaining two types of information apply mainly to core arguments (the strongly selected arguments of a verb: ARG0–ARG5 in Propbank), which in most linguistic theories are modeled as belonging together in an argument frame (set of arguments). The information is only marginally useful for adjuncts (the ARGM arguments of Propbank), which are usually treated as independent realizational choices not included in the argument frame of a verb. Firstly, many verbs take a number of different argument frames. Previous work has shown that these are strongly correlated with the word sense of the verb (Roland and Jurafsky 2002). If verbs were disambiguated for sense, the semantic roles of phrases would be closer to independent given the sense of the verb. However, because in almost all semantic role labeling work (including ours), the word sense is unknown and the model conditions only on the lemma, there is much joint information between arguments when conditioning only on the verb lemma. For example, compare: (1) Britain’s House of Commons passed a law on steroid use. (2) The man passed the church on his way to the factory. In the ﬁrst case the noun phrase after passed is an ARG1, whereas in the second case it is a ARGM-LOC, with the choice governed by the sense of the verb pass. Secondly, even with same sense of a verb, different patterns of argument realization lead to joint information between arguments. Consider: (3) The day that the ogre cooked the children is still remembered.  
 Volume 34, Number 2  to a predicate and assigning semantic role labels to them based on the context in which they occur. Since the seminal work of Gildea and Jurafsky (2002), statistical and machine learning approaches have been the predominant research paradigm in semantic role labeling, like most of the subﬁelds in natural language processing and computational linguistics. A prerequisite for statistical and machine learning approaches to semantic role labeling is the availability of a signiﬁcant amount of semantically interpreted corpora from which automatic systems can learn. The recent activities in semantic role labeling (Carreras and Ma`rquez 2004b, 2005; Litkowski 2004) have in large part been driven by the availability of semantically annotated corpora such as the FrameNet (Baker, Fillmore, and Lowe 1998), Proposition Bank (Palmer, Gildea, and Kingsbury 2005), and Nombank (Meyers et al. 2004) projects for English; the tectogrammatical layer annotation of the Prague Dependency Treebank (Sgall, Panevova´, and Hajicˇova´ 2004) for Czech; and the Salsa Project for German (Burchardt et al. 2006). These semantically annotated corpora not only provide the training and test material for the development of machine learning systems, but also effectively deﬁne semantic role labeling as a task. PropBank and FrameNet have been the two most widely used corpora in developing automatic semantic role labeling systems. Although both corpora provide predicate–argument structure annotation, they use very different semantic role labels, especially for the core arguments of each predicate. In FrameNet, the semantic roles of a predicate (called a Lexical Unit (LU)) are organized by semantic frames, which are conceptual structures that describe a particular situation or event along with their participants, which are called frame elements (FEs). All LUs in the same semantic frame share one set of semantic roles. For example, the verbs buy and sell both belong to the semantic frame Commercial_transaction, which involves a Buyer and Seller exchanging Money and Goods. In addition to these four core FEs, there are also three Non-Core FEs: Means, the manner in which the transaction takes place; Rate, the price of payment per unit of Goods; and Unit, the unit of measure for the Goods. Semantic role labeling based on FrameNet annotation attempts to identify the syntactic constituents in a sentence and assign FEs to them (1a). Notice that for any given sentence, not all FEs have to be realized and they do not have to be realized in the same syntactic position. (1) a. FrameNet [Buyer We] always [LU bought] [Goods a few dark-red carnations] [Seller from her] During the later part of the nineteenth century, [Seller the landowners] [LU sold] [Goods the land] [Buyer to developers] in very small lots. b. PropBank [Arg0 We] always [Rel bought] [Arg1 a few dark-red carnations] [Arg2 from her] During the later part of the nineteenth century, [Arg0 the landowners] [Rel sold] [Arg1 the land] [Arg2 to developers] in very small lots. Like FrameNet, PropBank also assigns semantic role labels to syntactic constituents (rather than to the heads in a dependency structure) in a sentence. Unlike FrameNet, there is no reference ontology like the semantic frame that provides a general set of semantic roles. Instead, for the core arguments, the PropBank uses a set of predicatespeciﬁc semantic role labels represented by an integer preﬁxed by Arg: Arg0 through Arg5. Predicates vary on the number of core arguments they take, but generally the total number of core arguments does not exceed six. These core arguments are deﬁned in frame ﬁles, with one frame ﬁle for each predicate. Within a frame ﬁle, the core 226  Xue  Semantic Role Labeling of Chinese Predicates  arguments are organized by framesets, which are the major senses of a predicate. A new frameset is postulated only when it takes a different set of core arguments from existing framesets. In addition to the core arguments, there is also a ﬁnite set of roles reserved for adjunct-like arguments. Each adjunct-like argument is represented as ArgM, indicating that it is a modiﬁer argument, followed by a secondary tag indicating the type of modiﬁer. Secondary tags are for semantic information such as location, manner, and time that are not speciﬁc to a particular verb or even a particular class of verbs and they are deﬁned based on a general set of guidelines. There is thus a dichotomy in the representation of the semantic roles for the core and peripheral arguments in the PropBank annotation. The predicate-speciﬁc nature of the PropBank semantic roles is clear when compared with the FrameNet FE. In (1b), for example, the seller is always labeled Seller and the buyer is always labeled Buyer in the FrameNet annotation whether the predicate is buy or sell. In contrast, in the PropBank annotation, the buyer is Arg0 when the predicate is buy and Arg2 when the predicate is sell. Conversely, the seller is Arg0 when the predicate is sell and Arg2 when the predicate is buy. While FrameNet annotates the semantic roles of both verbal and nominal predicates, the annotation of PropBank is limited to verbs, with the nominal predicates annotated in a separate but related project, the Nombank Project (Meyers et al. 2004). The NomBank Project adopted the same predicate-speciﬁc approach in representing the core arguments of a predicate as PropBank, with special treatment for noun-speciﬁc phenomena such as support verbs. There is considerable work on English semantic role labeling with both annotation conventions. Gildea and Jurafsky (2002) did their seminal work using data from FrameNet. The Senseval-3 international competition on semantic role labeling (Litkowski 2004) also used the FrameNet annotation. There is an even larger body of work using PropBank because it has a larger amount of annotated data on a wellestablished data set, the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Using the standard training and test sets in the Penn Treebank, there has been a rapid improvement in performance due to the use of more advanced machine-learning techniques and more informative linguistic features. The performance using automatic parses on Section 23 of the Penn Treebank has approached 0.81 F-score (Pradhan, Ward et al. 2005). There have also been two consecutive CoNLL competitions (Carreras and Ma`rquez 2004b, 2005) on semantic role labeling using the PropBank data. Research on Chinese semantic role labeling is still in its infancy. Work on Chinese semantic role labeling has been scant and sporadic, mostly due to the lack of a publicly available semantically annotated corpus of signiﬁcant size. Although most of the machine-learning techniques used in English semantic role labeling are readily transferable to Chinese, such technological transfer is only possible with similarly annotated data. To our knowledge, there are only two such data sets, which all used a small corpus that the authors created on their own. Sun and Jurafsky (2004) did preliminary work on Chinese semantic role labeling on 10 selected verbs using Support Vector Machines and reported promising early results.1 Noting that Chinese syntactic parsing is an especially challenging task, Kwong and T’sou (2005) reformulated semantic role labeling as a task of detecting and classifying the heads of arguments to avoid the hard problem of getting the correct text spans for the arguments. In this article, we  
© 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 1  2004; Karamanis et al. 2004). It is also supported by much psycholinguistic evidence. For instance, McKoon and Ratcliff (1992) argue that local coherence is the primary source of inference-making during reading. The key premise of our work is that the distribution of entities in locally coherent texts exhibits certain regularities. This assumption is not arbitrary—some of these regularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein 1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981). The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences, a representation that reﬂects distributional, syntactic, and referential information about discourse entities. We argue that the proposed entity-based representation of discourse allows us to learn the properties of coherent texts from a corpus, without recourse to manual annotation or a predeﬁned knowledge base. We demonstrate the usefulness of this representation by testing its predictive power in three applications: text ordering, automatic evaluation of summary coherence, and readability assessment. We formulate the ﬁrst two problems—text ordering and summary evaluation—as ranking problems, and present an efﬁciently learnable model that ranks alternative renderings of the same information based on their degree of local coherence. Such a mechanism is particularly appropriate for generation and summarization systems as they can produce multiple text realizations of the same underlying content, either by varying parameter values, or by relaxing constraints that control the generation process. A system equipped with a ranking mechanism could compare the quality of the candidate outputs, in much the same way speech recognizers employ language models at the sentence level. In the text-ordering task our algorithm has to select a maximally coherent sentence order from a set of candidate permutations. In the summary evaluation task, we compare the rankings produced by the model against human coherence judgments elicited for automatically generated summaries. In both experiments, our method yields improvements over state-of-the-art models. We also show the beneﬁts of the entitybased representation in a readability assessment task, where the goal is to predict the comprehension difﬁculty of a given text. In contrast to existing systems which focus on intra-sentential features, we explore the contribution of discourse-level features to this task. By incorporating coherence features stemming from the proposed entity-based representation, we improve the performance of a state-of-the-art readability assessment system (Schwarm and Ostendorf 2005). In the following section, we provide an overview of entity-based theories of local coherence and outline previous work on its computational treatment. Then, we introduce our entity-based representation, and deﬁne its linguistic properties. In the subsequent sections, we present our three evaluation tasks, and report the results of our experiments. Discussion of the results concludes the article.  2. Related Work Our approach is inspired by entity-based theories of local coherence, and is well-suited for developing a coherence metric in the context of a ranking-based text generation system. We ﬁrst summarize entity-based theories of discourse, and overview previous attempts for translating their underlying principles into computational coherence models. Next, we describe ranking approaches to natural language generation and focus on coherence metrics used in current text planners. 2  Barzilay and Lapata  Modeling Local Coherence  2.1 Entity-Based Approaches to Local Coherence Linguistic Modeling. Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and Hahn 1999; Poesio et al. 2004), salience concerns how entities are realized in an utterance (e.g., whether they are they pronominalized or not). In other theories, salience is deﬁned in terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday and Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993). More reﬁned accounts expand the notion of salience from a binary distinction to a scalar one; examples include Prince’s (1981) familiarity scale, and Givon’s (1987) and Ariel’s (1988) givenness-continuum. The salience status of an entity is often reﬂected in its grammatical function and the linguistic form of its subsequent mentions. Salient entities are more likely to appear in prominent syntactic positions (such as subject or object), and to be introduced in a main clause. The linguistic realization of subsequent mentions—in particular, pronominalization—is so tightly linked to salience that in some theories (e.g., Givon 1987) it provides the sole basis for deﬁning a salience hierarchy. The hypothesis is that the degree of underspeciﬁcation in a referring expression indicates the topical status of its antecedent (e.g., pronouns refer to very salient entities, whereas full NPs refer to less salient ones). In Centering Theory, this phenomenon is captured in the Pronoun Rule, and Givon’s Scale of Topicality and Ariel’s Accessibility Marking Scale propose a graded hierarchy of underspeciﬁcation that ranges from zero anaphora to full noun phrases, and includes stressed and unstressed pronouns, demonstratives with modiﬁers, and deﬁnite descriptions. Entity-based theories capture coherence by characterizing the distribution of entities across discourse utterances, distinguishing between salient entities and the rest. The intuition here is that texts about the same discourse entity are perceived to be more coherent than texts fraught with abrupt switches from one topic to the next. The patterned distribution of discourse entities is a natural consequence of topic continuity observed in a coherent text. Centering Theory formalizes ﬂuctuations in topic continuity in terms of transitions between adjacent utterances. The transitions are ranked, that is, texts demonstrating certain types of transitions are deemed more coherent than texts where such transitions are absent or infrequent. For example, CONTINUE transitions require that two utterances have at least one entity in common and are preferred over transitions that repeatedly SHIFT from one entity to the other. Givon’s (1987) and Hoey’s (1991) accounts of discourse continuity complement local measurements by considering global characteristics of entity distribution, such as the lifetime of an entity in discourse and the referential distance between subsequent mentions. Computational Modeling. An important practical question is how to translate principles of these linguistic theories into a robust coherence metric. A great deal of research has been devoted to this issue, primarily in Centering Theory (Miltsakaki and Kukich 3  Computational Linguistics  Volume 34, Number 1  2000; Hasler 2004; Karamanis et al. 2004). Such translation is challenging in several respects: one has to determine ways of combining the effects of various constraints and to instantiate parameters of the theory that are often left underspeciﬁed. Poesio et al. (2004) note that even for fundamental concepts of Centering Theory such as “utterance,” “realization,” and “ranking,” multiple—and often contradictory—interpretations have been developed over the years, because in the original theory these concepts are not explicitly ﬂeshed out. For instance, in some Centering papers, entities are ranked with respect to their grammatical function (Brennan, Friedman, and Pollard 1987; Walker, Iida, and Cote 1994; Grosz, Joshi, and Weinstein 1995), and in others with respect to their position in Prince’s (1981) givenness hierarchy (Strube and Hahn 1999) or their thematic role (Sidner 1979). As a result, two “instantiations” of the same theory make different predictions for the same input. Poesio et al. (2004) explore alternative speciﬁcations proposed in the literature, and demonstrate that the predictive power of the theory is highly sensitive to its parameter deﬁnitions. A common methodology for translating entity-based theories into computational models is to evaluate alternative speciﬁcations on manually annotated corpora. Some studies aim to ﬁnd an instantiation of parameters that is most consistent with observable data (Strube and Hahn 1999; Karamanis et al. 2004; Poesio et al. 2004). Other studies adopt a speciﬁc instantiation with the goal of improving the performance of a metric on a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with entity transition information, and show that the distribution of transitions correlates with human grades. Analogously, Hasler (2004) investigates whether Centering Theory can be used in evaluating the readability of automatic summaries by annotating human and machine generated extracts with entity transition information. The present work differs from these approaches in goal and methodology. Although our work builds upon existing linguistic theories, we do not aim to directly implement or reﬁne any of them in particular. We provide our model with sources of knowledge identiﬁed as essential by these theories, and leave it to the inference procedure to determine the parameter values and an optimal way to combine them. From a design viewpoint, we emphasize automatic computation for both the underlying discourse representation and the inference procedure. Thus, our work is complementary to computational models developed on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in a ranking function. The top-ranked candidate is selected for presentation. A two-stage generate-and-rank architecture circumvents the complexity 4  Barzilay and Lapata  Modeling Local Coherence  of traditional generation systems, where numerous, often conﬂicting constraints, have to be encoded during development in order to produce a single high-quality output. Because the focus of our work is on text coherence, we discuss here ranking approaches applied to text planning (see Walker et al. [2001] and Knight and Hatzivassiloglou [1995] for ranking approaches to sentence planning and surface realization, respectively). The goal of text planning is to determine the content of a text by selecting a set of information-bearing units and arranging them into a structure that yields well-formed output. Depending on the system, text plans are represented as discourse trees (Mellish et al. 1998) or linear sequences of propositions (Karamanis 2003). Candidate text structures may differ in terms of the selected propositions, the sequence in which facts are presented, the topology of the tree, or the order in which entities are introduced. A set of plausible candidates can be created via stochastic search (Mellish et al. 1998) or by a symbolic text planner following different text-formation rules (Kibble and Power 2004). The best candidate is chosen using an evaluation or ranking function often encoding coherence constraints. Although the type and complexity of constraints vary greatly across systems, they are commonly inspired by Rhetorical Structure Theory or entity-based constraints similar to the ones captured by our method. For instance, the ranking function used by Mellish et al. gives preference to plans where consecutive facts mention the same entities and is sensitive to the syntactic environment in which the entity is ﬁrst introduced (e.g., in a subject or object position). Karamanis ﬁnds that a ranking function based solely on the principle of continuity achieves competitive performance against more sophisticated alternatives when applied to ordering short descriptions of museum artifacts.1 In other applications, the ranking function is more complex, integrating rules from Centering Theory along with stylistic constraints (Kibble and Power 2004). A common feature of current implementations is that the speciﬁcation of the ranking function—feature selection and weighting—is performed manually based on the intuition of the system developer. However, even in a limited domain this task has proven difﬁcult. Mellish et al. (1998; page 100) note: “The problem is far too complex and our knowledge of the issues involved so meager that only a token gesture can be made at this point.” Moreover, these ranking functions operate over semantically rich input representations that cannot be created automatically without extensive knowledge engineering. The need for manual coding impairs the portability of existing methods for coherence ranking to new applications, most notably to text-to-text generation applications, such as summarization. In the next section, we present a method for coherence assessment that overcomes these limitations: We introduce an entity-based representation of discourse that is automatically computed from raw text; we argue that the proposed representation reveals entity transition patterns characteristic of coherent texts. The latter can be easily translated into a large feature space which lends itself naturally to the effective learning of a ranking function, without explicit manual involvement. 3. The Coherence Model In this section we describe our entity-based representation of discourse. We explain how it is computed and how entity transition patterns are extracted. We also discuss how  
Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 1  Tsujii 2005), statistical modeling of these grammars is attracting considerable attention. This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation. The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing. Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable. This article ﬁrst proposes feature forest models, which are a general solution to the problem of maximum entropy modeling of tree structures (Miyao and Tsujii 2002). Our algorithm avoids exponential explosion by representing probabilistic events with feature forests, which are packed representations of tree structures. When complete structures are represented with feature forests of a tractable size, the parameters of maximum entropy models are efﬁciently estimated without unpacking the feature forests. This is due to dynamic programming similar to the algorithm for computing inside/outside probabilities in PCFG parsing. The latter half of this article (Section 4) is on the application of feature forest models to disambiguation in wide-coverage HPSG parsing. We describe methods for representing HPSG parse trees and predicate–argument structures using feature forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicalized grammars. Section 3 proposes feature forest models for solving this problem. Section 4 describes the application of feature forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36  Miyao and Tsujii  Feature Forest Models for Probabilistic HPSG Parsing  feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incorporated, and we can expect higher accuracy in disambiguation. A maximum entropy model gives a probabilistic distribution that maximizes the likelihood of training data under given feature functions. Given training data E = { x, y }, a maximum entropy model gives conditional probability p(y|x) as follows.  Deﬁnition 1 (Maximum entropy model) A maximum entropy model is deﬁned as the solution of the following optimization problem.          pM ( y|x )  =  argmax p  −  x,y  p˜(x, y) log p(y|x) ∈E  where:  p(y|x) = 1 exp Z(x)  λi fi(x, y)  i  Z(x) =  exp  λi fi(x, y)  y∈Y(x)  i  In this deﬁnition, p˜(x, y) is the relative frequency of x, y in the training data. fi is a feature function, which represents a characteristic of probabilistic events by mapping an event into a real value. λi is the model parameter of a corresponding feature function fi, and is determined so as to maximize the likelihood of the training data (i.e., the optimization in this deﬁnition). Y(x) is a set of y for given x; for example, in parsing, x is a given sentence and Y(x) is a parse forest for x. An advantage of maximum entropy models is that feature functions can represent any characteristics of events. That is, independence assumptions are unnecessary for the design of feature functions. Hence, this method provides a principled solution for the estimation of consistent probabilistic distributions over feature structure grammars. The remaining issue is how to estimate parameters. Several numerical algorithms, such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limitedmemory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright 1999), have been proposed for parameter estimation. Although the algorithm proposed in the present article is applicable to all of the above algorithms, we used L-BFGS for experiments. However, a computational problem arises in these parameter estimation algorithms. The size of Y(x) (i.e., the number of parse trees for a sentence) is generally  37  Computational Linguistics  Volume 34, Number 1  very large. This is because local ambiguities in parse trees potentially cause exponential growth in the number of structures assigned to sub-sequences of words, resulting in billions of structures for whole sentences. For example, when we apply rewriting rule S → NP VP, and the left NP and the right VP, respectively, have n and m ambiguous subtrees, the result of the rule application generates n × m trees. This is problematic because the complexity of parameter estimation is proportional to the size of Y(x). The cost of the parameter estimation algorithms is bound by the computation of model expectation, µi, given as (Malouf 2002):  µi = p˜(x)  fi(x, y)p(y|x)  x∈X  y∈Y(x)      = p˜(x)  fi  (x,  y)  
Dublin City University IBM Center for Advanced Studies A number of researchers have recently conducted experiments comparing “deep” hand-crafted wide-coverage with “shallow” treebank- and machine-learning-based parsers at the level of dependencies, using simple and automatic methods to convert tree output generated by the shallow parsers into dependencies. In this article, we revisit such experiments, this time using sophisticated automatic LFG f-structure annotation methodologies with surprising results. We compare various PCFG and history-based parsers to ﬁnd a baseline parsing system that ﬁts best into our automatic dependency structure annotation technique. This combined system of syntactic parser and dependency structure annotation is compared to two hand-crafted, deep constraint-based parsers, RASP and XLE. We evaluate using dependency-based gold standards ∗ Now at the Institut fu¨ r Maschinelle Sprachverarbeitung, Universita¨t Stuttgart, Germany. E-mail: aoife. cahill@ims.uni-stuttgart.de. ∗∗ National Centre for Language Technology, Dublin City University, Dublin 9, Ireland. † IBM Dublin Center for Advanced Studies (CAS), Dublin 15, Ireland. ‡ Now at Google Inc., Mountain View, CA. Submission received: 24 August 2005; revised submission received: 20 March 2007; accepted for publication: 2 June 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 1  and use the Approximate Randomization Test to test the statistical signiﬁcance of the results. Our experiments show that machine-learning-based shallow grammars augmented with sophisticated automatic dependency annotation technology outperform hand-crafted, deep, widecoverage constraint grammars. Currently our best system achieves an f-score of 82.73% against the PARC 700 Dependency Bank, a statistically signiﬁcant improvement of 2.18% over the most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically signiﬁcant 3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsing system. 1. Introduction Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g., Penn-II WSJ Section 23 trees) reporting traditional PARSEVAL metrics (Black et al. 1991) of labeled and unlabeled bracketing precision, recall and f-score measures, number of crossing brackets, complete matches, and so forth. Although tree-based parser evaluation provides valuable insights into the performance of grammars and parsing systems, it is subject to a number of (related) drawbacks: 1. Bracketed trees do not always provide NLP applications with enough information to carry out the required tasks: Many applications involve a deeper analysis of the input in the form of semantically motivated information such as deep dependency relations, predicate–argument structures, or simple logical forms. 2. A number of alternative, but equally valid tree representations can potentially be given for the same input. To give just a few examples: In English, VPs containing modals and auxiliaries can be analyzed using (predominantly) binary branching rules (Penn-II [Marcus et al. 1994]), or employ ﬂatter analyses where modals and auxiliaries are sisters of the main verb (AP treebank [Leech and Garside 1991]), or indeed do without a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank bracketing guidelines can use “traditional” CFG categories such as S, NP, and so on (Penn-II) or a maximal projection-inspired analysis with IPs and DPs (Chinese Penn Treebank [Xue et al. 2004]). 3. Because a tree-based gold standard for parser evaluation must adopt a particular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82  Cahill et al.  Statistical Parsing Using Automatic Dependency Structures  structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate strings to information/meaning, often in the form of predicate–argument structure, dependency relations,2 or logical forms. By contrast, a shallow grammar simply deﬁnes a language and may associate syntactic (e.g., CFG tree) representations with strings. Natural languages do not always interpret linguistic material locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia 1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii 2003], LFG [Cahill et al. 2002b, 2004]). To a ﬁrst approximation, these approaches can be classiﬁed as “conversion”- or “annotation”-based. TAG-based approaches convert  
 Volume 34, Number 1  4. generation of preverbal messages (mapping the conceptual representations that have been handled so far to semantic content that can interface with the linguistic formulator) This chapter also introduces referential nets, the formalism that is used to represent conceptual content. Part B (“Incrementality”) traces the roots of the notion of incrementality in computer science, and provides an extensive overview of various notions of incrementality. Guhe settles on a deﬁnition of incrementality whose crux is the piecemeal processing of information and production of output before all input has been seen. He distinguishes between incremental processes, algorithms, and models; roughly speaking, incremental models contain a strictly uni-directional cascade of incremental processes that recursively call incremental algorithms. For Guhe, an essential characteristic of incremental algorithms is that they use only a local context, as opposed to all available knowledge, for their computations. He also adopts the common distinction between working memory and long-term memory. The former mediates the ﬂow of information between incremental processes. “Increments,” the small pieces of information that incremental processes operate with, can be read from it and written to it. It contains “situation and discourse knowledge,” whereas long-term memory stores static “encyclopedic knowledge.” This “blueprint for incrementality” is accompanied by a useful discussion of various dimensions of incrementality, such as monotonicity, lookahead, feedback, and discreteness. Part C focuses on INC, the incremental conceptualizer, which is an implemented “working model” of the blueprint for incrementality. INC is offered as a framework, that is, a model which has been ﬂeshed out in detail in some respects and left underspeciﬁed in others. A central role is played by four parameters of INC which inﬂuence its behavior. For example, two of these concern the storage of event representations in a buffer in working memory which mediates the ﬂow of information between incremental processes. One parameter, length of traverse buffer (LOTB), concerns the size of this buffer, whereas the other, latency (LT), determines for how long an element is kept in the buffer until it is picked up by preverbal message generation. Small values for LOTB in combination with a large value for LT can lead to the “forgetting” of information: If the buffer has ﬁlled up and new information is added, the ﬁrst element on the buffer is discarded and never reaches preverbal message generation. The book presents some evidence that variation of the parameter settings can account for some of the variation found among human speakers. This part of the book concludes with a discussion of the output of INC for two domains and output of human speakers for the same domains. It concerns a visual scene, from a bird’s eye perspective, of two moving planes on a runway, and the replay of the drawing of a simple line drawing consisting of eight lines that represents a crossing. The “Results” summarizes the main contributions of the book, makes some comparisons with Levelt’s (1989) model, and proposes a number of future extensions, such as the addition of Levelt’s monitor. The monitor takes as input the output of the speechcomprehension system and uses this to inﬂuence the processing of the conceptualizer. Finally, there are a good number of suggestions for further ways to parameterize INC. The book is a rich source of information on language production, both from a computational and a cognitive point of view. It includes a good introduction to conceptualizing, and provides an insightful discussion of many varieties of incrementality. INC is an excellent starting point for others interested in on-line data-driven generation to both build on and respond to. The breadth of the work means that one gets a truly 130  Book Reviews holistic view of the problem and is given a good impression of the many debates that cross the boundaries of different disciplines. In this respect, the book goes against a recent trend in computational linguistics to show less interest in other language-related research communities (see Reiter 2007). Although the wide scope of this book is in many ways what makes it attractive, it also leads to some of its weaknesses. In particular, the way INC is presented in this broad context did not feel optimal to me. Although the proper description of INC is delayed until Part C, there are numerous forward references to INC in the preceding parts. The reader will ﬁnd several instances where a certain aspect of conceptualization or incrementality is discussed with reference to INC, only to ﬁnd out later that this particular feature “is not implemented yet (apart from a dummy function).” It would have been fairer to the reader to separate a clear description of the current state of INC from the wider discussion surrounding it. Another presentational issue concerns the tight integration of locality and incrementality in the book’s deﬁnitions. In particular, the virtual identiﬁcation of incremental algorithms with computation on a local context makes one question why the book speaks of incremental rather than local algorithms. A more substantive point relates to Part C on INC. This part includes the description of two simulations that were run with INC. Somewhat frustratingly, both descriptions are incomplete. For instance, whereas for the ﬁrst simulation the appendix contains the texts produced by human participants for the same task, there is no systematic analysis of the structural (dis)similarities between the output of INC and that of the human speakers. For the second simulation, there are some analyses of the similarities between the structure of INC’s and the human speakers’ output, but no transcripts of complete human outputs are provided. In both cases, there is also no detail about how longterm memory, referred to as the concept storage (CS), was populated for the relevant domains, even though the CS must have had a signiﬁcant inﬂuence on the output that INC produces. This book will be useful to research students and researchers in natural language generation who are interested in the study of generation systems as a computational model of human language production. Part B of the book, on incrementality, might also prove useful to those approaching NLG as an engineering problem. The main reason to consult this book is that it brings together in a single place information on conceptualization, incrementality, and various debates in philosophy, cognitive science, and computer science affecting these topics. INC, the incremental conceptualizer which is described in part C of the book, presents an ambitious attempt to implement a computational model of incremental conceptualization. The verdict on its adequacy is still out, given the limited empirical evaluation to which it has been subjected thus far. Online generation, a central theme of this book, was adopted in 2007 at the International Conference on Intelligent Virtual Agents as a task (automated real-time reporting on simulated horse races) in the GALA competition for Embodied Lifelike Agents.1 Work on embodiment and conceptualization, new insights into societal grounding of conceptual representations (e.g., DeVault, Oved, and Stone 2006), empirical and computational studies on generation (both incremental and non-incremental, from numerical data; e.g., van Deemter 2006), and recent experimental techniques for studying language production (see Roelofs 2004 for an overview) give a sense that this book could be part of an exciting revival of cognitively motivated NLG. 
Volume 34, Number 1  same as errors made by native speakers. The section on error analysis (19 pages) is the heart of this chapter. It covers both the relevant developments in second language acquisition theory and a detailed description of the identiﬁcation of errors in a system by one of the authors. A ﬁnal section is devoted to corpus linguistics and the use of learner corpora. Chapter 4, “Feedback” (56 pages), takes the reader on a rather long general tour before arriving at CALL. After sections on feedback in general, in human–computer interaction, in general learning, and in language learning, as well as the general problem of language generation (together 20 pages), it is only Section 4.6 that treats “Feedback in CALL.” This section and the next (together 36 pages) give an overview of the issues including the problem of giving feedback that is used and understood by the learner and the requirements on system design. The discussion is again based on a detailed description of a system by one of the authors. Chapter 5, “Student Modelling” (41 pages), devotes again a long section to general issues (21 pages) before the topic in the title is explored from the perspective of parserbased CALL. In this case, this is understandable because most of the problems encountered in the latter are speciﬁc versions of problems of Intelligent Tutoring Systems for areas other than a second language. Chapter 6, “The Past and the Future” (15 pages), starts in the by-now-familiar way with some general thoughts about the problems of predicting future developments before sketching some of the current lines of development. On the whole, the book is a very useful overview of a ﬁeld of application that, as I have argued elsewhere (ten Hacken 2003), is very much in line with modern trends in computational linguistics because it aims for the solution of real-life, practical problems rather than abstract, general problems such as understanding natural language. Research students and academics will ﬁnd a wealth of well-organized information and references for further study. The fact that the authors often take their own work as a basis for illustrating certain general ideas should not be held against them because of their prominent position in the ﬁeld. In some cases computational linguists can skip the ﬁrst section(s) of a chapter because they will already know the topic to a higher standard than achieved here, but this is the price for having an accessible introduction for all of the surrounding ﬁelds of research. One problem with this book is that it shows signs of a rushed production process. We can live with inconsistencies in section numbering, but inaccuracies in the references are more disturbing. For a book that is unique in its kind and could have been a highly useful reference work it is particularly disappointing that the index is poor and no detailed table of contents is provided. This makes it difﬁcult to access the wealth of information contained in the book efﬁciently. Despite these problems, the book is a must-read for any computational linguist intending to start work in the ﬁeld of CALL.  References ten Hacken, Pius. 2003. Computer-assisted language learning and the revolution in computational linguistics. Linguistik On-Line, 17:23–39.  Nerbonne, John. 2003. Natural language processing in computer-assisted language learning.InMitkov,Ruslan,editor,The Oxford Handbook of ComputationalLinguistics.Oxford University Press, New York, pages 670–698.  Pius ten Hacken is a senior lecturer in translation and linguistics at Swansea University (UK). His research interests include morphology, the lexicon, and the epistemological basis of generative grammar and computational linguistics. Ten Hacken’s address is Department of Modern Languages, Swansea University, Singleton Park, Swansea, SA2 8PP, United Kingdom; e-mail: p.ten-hacken@swansea.ac.uk.  
© 2008 Association for Computational Linguistics  Computational Linguistics  Volume 34, Number 1  How different this public image is from that of computational linguistics, or of artiﬁcial intelligence, and even core computer science. To the extent that the public thinks at all about what we do, they think of us as producing gadgets, such as amusing new sorts of telephone. Only the other day, a colleague was called up by someone in a neighboring department asking if we could mend his PC for him. (“Don’t you have a Little Man? You used to have a Little Man.”) As a result, computer science is continually subjected to governmental reviews seeking assurance that we know what we are doing, and are doing enough for the economy. Many of these reviews draw very negative conclusions—the 1966 report of the Automatic Language Processing Advisory Committee (ALPAC) of the US National Academy of Sciences effectively shut down research in machine translation for over a decade, and was the main reason for ACL changing its name in 1968 from the Association for Machine Translation and Computational Linguistics (AMTCL). The 1973 report of Sir James Lighthill (a ﬂuid dynamicist known for his foundational work in the ﬁeld of aeroacoustics) to the UK Science Research Council (SRC) closed down artiﬁcial intelligence and NLP research for a decade, until the Alvey report decided that British industry had fallen behind in this area, and opened it up again. There have been many similar examples since then, though few as catastrophic. Nobody goes around telling physicists what not to work on, or setting up commissions chaired by complete outsiders (roboticists or computational linguists, perhaps?) to decide whether physicists are earning their keep. The physicists tell the goverment what they think it is right to do, and the government either funds it or it doesn’t. Even when it doesn’t, as in the case of the superconducting supercollider, it’s because they can’t afford it, or lack the political power, not because of low esteem. How do the physicists do it? Of course, as Duke Ellington said when asked how he kept his band together through the Beatles’ era, you have to have a gimmick. The physicists gave us atomic energy and the bomb, so no one can ever suggest again that they do not deliver Bang for the Buck, even when they actually don’t, as may well turn out to be the case for the past twenty years or so of research in string theory and supersymmetry.1 However, it is far more important that physics consists of a body of great empirically proven laws that all scientists recognize, from the laws of thermodynamics to the special and general theories of relativity and quantum theory. This body of knowledge lends both authority, and a breadth of vision that transcends any individual physicist’s work and any individual theory, even if parts of it can be temporarily ignored when convenient. But we too have awesome devices. Search engines have arguably changed people’s lives at least as profoundly as atomic energy. The statistical machine translation tools that Google launched around May 2006 with Arabic for all the world to freely betatest, and which have since been extended to Chinese, Russian, Japanese, and Korean, imperfect as they are, may well have an even bigger impact. Our colleagues in AI rejoice in beating international chess Grand Masters with Deep Blue, and boast of robots on Mars and autonomous vehicles charging around the Mojave desert. Computer science has the Internet itself to show off. We too have discovered great truths—Zipf’s Law, Information Theory, the power of statistically approximate language models, the only-just-trans-context-free automatatheoretic level of natural languages, the surface-compositionality of natural language  
We propose a transformation based sentence splitting method for statistical machine translation. Transformations are expanded to improve machine translation quality after automatically obtained from manually split corpus. Through a series of experiments we show that the transformation based sentence splitting is effective pre-processing to long sentence translation. 
A speech-to-speech translation project (S2S) has been conducted since 2006 by the Human Language Technology laboratory at the National Electronics and Computer Technology Center (NECTEC) in Thailand. During the past one year, there happened a lot of activities regarding technologies constituted for S2S, including automatic speech recognition (ASR), machine translation (MT), text-to-speech synthesis (TTS), as well as technology for language resource and fundamental tool development. A developed prototype of English-to-Thai S2S has opened several research issues, which has been taken into consideration. This article intensively reports all major research and development activities and points out remaining issues for the rest two years of the project.  basic technologies are ready to seed for S2S research. The S2S project has then been conducted in NECTEC since the end of 2006. The aim of the 3-year S2S project initiated by NECTEC is to build an English-Thai S2S service over the Internet for a travel domain, i.e. to be used by foreigners who journey in Thailand. In the first year, the baseline system combining the existing basic modules applied for the travel domain was developed. The prototype has opened several research issues needed to be solved in the rest two years of the project. This article summarizes all significant activities regarding each basic technology and reports remaining problems as well as the future plan to enhance the baseline system. The rest of article is organized as follows. The four next sections describe in details activities conducted for ASR, MT, TTS, and language resources and fundamental tools. Section 6 summarizes the integration of S2S system and discusses on remaining research issues as well as on-going works. Section 7 concludes this article.  
This paper presents a technique for transliteration based directly on techniques developed for phrase-based statistical machine translation. The focus of our work is in providing a transliteration system that could be used to translate unknown words in a speech-to-speech machine translation system. Therefore the system must be able to generate arbitrary sequence of characters in the target language, rather than words chosen from a pre-determined vocabulary. We evalauted our method automatically relative to a set of human-annotated reference transliterations as well as by assessing it for correctness using human evaluators. Our experimental results demonstrate that for both transliteration and back-transliteration the system is able to produce correct, or phonetically equivalent to correct output in approximately 80% of cases. 
 The paper outlines the development of a large vocabulary continuous speech recognition (LVCSR) system for the Indonesian language within the Asian speech translation (A-STAR) project. An overview of the A-STAR project and Indonesian language characteristics will be brieﬂy described. We then focus on a discussion of the development of Indonesian LVCSR, including data resources issues, acoustic modeling, language modeling, the lexicon, and accuracy of recognition. There are three types of Indonesian data resources: daily news, telephone application, and BTEC tasks, which are used in this project. They are available in both text and speech forms. The Indonesian speech recognition engine was trained using the clean speech of both daily news and telephone application tasks. The optimum performance achieved on the BTEC task was 92.47% word accuracy. 
This paper presents a new method of using confidence vector as an intermediate input feature for the multi-stage based speech recognition. The multi-stage based speech recognition is a method to reduce the computational complexity of the decoding procedure and thus accomplish faster speech recognition. In the multi-stage speech recognition, however, the error of previous stage is transferred to the next stage. This tends to cause the deterioration of the overall performance. We focus on improving the accuracy by introducing confidence vector instead of phoneme which typically used as an intermediate feature between the acoustic decoder and the lexical decoder, the first two stages in the multi-stage speech recognition. The experimental results show up to 16.4% error reduction rate(ERR) of word accuracy for 220k Korean Point-of-Interest (POI) domain and 29.6% ERR of word accuracy for hotel reservation dialog domain. 
In this paper, we present a report on the research and development of speech to speech translation system for Asian languages, primarily on the design and implementation of speech recognition and machine translation systems for Indonesia language. As part of the A-STAR project, each participating country will need to develop each component of the full system for the corresponding language. We will specifically discuss our method on building speech recognition and stochastic language model for statistically translating Indonesian into other Asian languages. The system is equipped with a capability to handle variation of speech input, a more natural mode of communication between the system and the users. 
This paper describes our approaches for the preparation of gazetteers for named entity recognition (NER) in Indian languages. We have described two methodologies for the preparation of gazetteers1. Since the relevant gazetteer lists are more easily available in English we have used a transliteration based approach to convert available English name lists to Indian languages. The second approach is a context pattern induction based domain speciﬁc gazetteer preparation. This approach uses a domain speciﬁc raw corpus and a few seed entities to learn context patterns and then the corresponding name lists are generated by using bootstrapping. 
An ontology can be seen as a representation of concepts in a specific domain. Accordingly, ontology construction can be regarded as the process of organizing these concepts. If the terms which are used to label the concepts are classified before building an ontology, the work of ontology construction can proceed much more easily. Part-of-speech (PoS) tags usually carry some linguistic information of terms, so PoS tagging can be seen as a kind of preliminary classification to help constructing concept nodes in ontology because features or attributes related to concepts of different PoS types may be different. This paper presents a simple approach to tag domain terms for the convenience of ontology construction, referred to as Term PoS (TPoS) Tagging. The proposed approach makes use of segmentation and tagging results from a general PoS tagging software to predict tags for extracted domain specific terms. This approach needs no training and no context information. The experimental results show that the proposed approach achieves a precision of 95.41% for extracted terms and can be easily applied to different domains. Comparing with some existing approaches, our approach shows that for some specific tasks, simple method can obtain very good performance and is thus a better choice. Keywords: ontology construction, part-ofspeech (PoS) tagging, Term PoS (TPoS) tagging.  
Terminology development in education, science and technology is a key to formulating a knowledge society. The authors are developing a multilingual engineering terminology dictionary consisting of more than ten thousand engineering terms each for ten Asian languages and English. The dictionary is primarily designed to support foreign students who are studying engineering subjects at Japanese higher educational institutions in the Japanese language. Analysis of the lexical terms could provide useful knowledge for language resource creators. There are two adoption approaches, “phonetic adoption” (transliteration of borrowed terms) and “semantic adoption” (where the meaning is expressed using native words). The proportion of the two options found in the terminology set of each country (or language) shows a substantial difference and seems to reflecti language policies of the country and the influence of foreign languages on the host language. This paper presents preliminary results of our investigation on this question based on a comparative study of three languages: Japanese, Vietnamese and Thai.  
In this paper, the authors mainly describe about the selections of XML tag set for Myanmar National Corpus (MNC). MNC will be a sentence level annotated corpus. The validity of XML tag set has been tested by manually tagging the sample data. Keywords: Corpus, XML, Myanmar, Myanmar Languages 
In Myanmar language, sentences are clearly delimited by a unique sentence boundary marker but are written without necessarily pausing between words with spaces. It is therefore non-trivial to segment sentences into words. Word tokenizing plays a vital role in most Natural Language Processing applications. We observe that word boundaries generally align with syllable boundaries. Working directly with characters does not help. It is therefore useful to syllabify texts ﬁrst. Syllabiﬁcation is also a non-trivial task in Myanmar. We have collected 4550 syllables from available sources . We have evaluated our syllable inventory on 2,728 sentences spread over 258 pages and observed a coverage of 99.96%. In the second part, we build word lists from available sources such as dictionaries, through the application of morphological rules, and by generating syllable n-grams as possible words and manually checking. We have thus built list of 800,000 words including inﬂected forms. We have tested our algorithm on a 5000 sentence test data set containing a total of (35049 words) and manually checked for evaluating the performance. The program recognized 34943 words of which 34633 words were correct, thus giving us a Recall of 98.81%, a Precision of 99.11% and a FMeasure is 98.95%.  Key Words:- Myanmar, Syllable, Words, Segmentation, Syllabiﬁcation, Dictionary 
Since its inception, the World Wide Web (WWW) has exponentially grown to shelter billions of monolingual and multilingual web pages that can be navigated through hyperlinks. Its structural properties provide useful information in presenting the sociolinguistic properties of the web. In this study, about 26 million web pages under the South East Asian country code toplevel-domains (ccTLDs) are analyzed, several language communities are identified, and the graph structure of these communities are analyzed. The distance between language communities are calculated by a distance metrics based on the number of outgoing links between web pages. Intermediary languages are identified by graph analysis. By creating a language subgraph, the size and diameter of its stronglyconnected components are derived, as these values are useful parameters for languagespecific crawling. Performing a link structure analysis of the web pages can be a useful tool for socio-linguistic and technical research purposes. 
This paper describes first steps towards extending the METU Turkish Corpus from a sentence-level language resource to a discourse-level resource by annotating its discourse connectives and their arguments. The project is based on the same principles as the Penn Discourse TreeBank (http://www.seas.upenn.edu/~pdtb) and is supported by TUBITAK, The Scientific and Technological Research Council of Turkey. We first present the goals of the project and the METU Turkish corpus. We then describe how we decided what to take as explicit discourse connectives and the range of syntactic classes they come from. With representative examples of each class, we examine explicit connectives, their linear ordering, and types of syntactic units that can serve as their arguments. We then touch upon connectives with respect to free word order in Turkish and punctuation, as well as the important issue of how much material is needed to specify an argument. We close with a brief discussion of current plans. 
We describe our initial efforts towards developing a large-scale corpus of Hindi texts annotated with discourse relations. Adopting the lexically grounded approach of the Penn Discourse Treebank (PDTB), we present a preliminary analysis of discourse connectives in a small corpus. We describe how discourse connectives are represented in the sentence-level dependency annotation in Hindi, and discuss how the discourse annotation can enrich this level for research and applications. The ultimate goal of our work is to build a Hindi Discourse Relation Bank along the lines of the PDTB. Our work will also contribute to the cross-linguistic understanding of discourse connectives. 
The purpose of this study was to provide an example of how to build a Yami ontology from traditional songs by employing Protégé, an open-source tool for editing and managing ontologies developed by Stanford University. Following Conceptual Blending Theory (Fauconnier and Turner, 1998), we found that Yami people use the conceptual metaphor of “fishing” in traditional songs when praising the host’s diligence in a ceremony celebrating the completion of a workhouse. The process of building ontologies is explored and illustrated. The proposed construction of an ontology for Yami traditional songs can serve as a fundamental template, using the corpus available online from the Yami documentation website (http://yamiproject.cs.pu.edu.tw/yami) to build ontologies for other domains. 
In this paper, we first had a overall study of existing POS tag sets for European and Indian languages. Till now, most of the research done on POS tagging is for English. We observed that even though the research on POS tagging for English is done exhaustively, part-of-speech annotation in various research applications is incomparable which is variously due to the variations in tag set definitions. We understand that the morphosyntactic features of the language and the degree of desire to represent the granularity of these morpho-syntactic features, domain etc., decide the tags in the tag set. We then examined how POS tagset design has to be handled for Indian languages, taking Telugu language into consideration. 1. Introduction Annotation is the process of adding some additional information (grammatical features like word category, case indicator, other morph features) about the word to each word of the text. This additional information is called a tag. The set of all these tags is called a tag set. When words are considered in isolation, they can have one or more number of tags for each word. But when these words are used in a certain context, the tags representing morphological and syntactic feature reduce to one tag. The information to be captured as a tag is an application specific issue (Anne,1997, David, 1994 and David,  1995). A number of tag sets have been evolved for a number of languages. These tag sets not only differ with each other from language to language, but vary within the language itself. The reasons for the variation of tags in the tag sets are as follows. As taggers give additional information like grammatical features such as number, gender, person, case markers for noun inflections; tense markers for verbal inflections, the number of tags used by different systems varies depending on the information encoded in the tag. However the tag set design plays a vital role when data is tagged according to it and hence it affects the development of NLP application tools within and across that language. Language independent representation of a tag set help to find out the hidden information like context, structure, syntactic and semantic aspect of the word. It also gives an overview of language modeling features. 2. Desirable Features of a Tag Set 
Research in Parts-of-Speech (POS) tagset design for European and East Asian languages started with a mere listing of important morphosyntactic features in one language and has matured in later years towards hierarchical tagsets, decomposable tags, common framework for multiple languages (EAGLES) etc. Several tagsets have been developed in these languages along with large amount of annotated data for furthering research. Indian Languages (ILs) present a contrasting picture with very little research in tagset design issues. We present our work in designing a common POS-tagset framework for ILs, which is the result of in-depth analysis of eight languages from two major families, viz. Indo-Aryan and Dravidian. Our framework follows hierarchical tagset layout similar to the EAGLES guidelines, but with significant changes as needed for the ILs. 
In this paper, we report a survey of language resources in Indonesia, primarily of indigenous languages. We look at the official Indonesian language (Bahasa Indonesia) and 726 regional languages of Indonesia (Bahasa Nusantara) and list all the available LRs that we can gathered. This paper suggests that the smaller regional languages may remain relatively unstudied, and unknown, but they are still worthy of our attention. Various LRs of these endangered languages are being built and collected by regional language centers for study and its preservation. We will also briefly report its presence on the Internet. 
 In this paper, we report a Japanese language resource for answering how-type questions. It was developed it by using mails posted to a mailing list. We show a QA system based on this language resource.  
 This paper presents an ongoing research aimed to build the first corpus, 5 million words, for Mongolian language by focusing on annotating and tagging corpus texts according to TEI XML (McQueen, 2004) format. Also, a tool, MCBuilder, which provides support for flexibly and manually annotating and manipulating the corpus texts with XML structure, is presented.  Figure 1. Current and future states of building a Mongolian corpus. And, we manually build the corpus until collecting and annotating 1 million words and tagging 100 thousand words of them for semiautomatically building the corpus in the future.  
Urdu is spoken by more than 100 million speakers. This paper summarizes the corpus and lexical resources being developed for Urdu by the CRULP, in Pakistan. 
Construction of 100 million words balanced corpus of contemporary written Japanese is underway at the National Institute for Japanese Language. The unique property of the corpus consists in that the majority of its sample texts are selected randomly from well-defined statistical populations covering wide range of written texts. 
The aim of this paper is to present a basic framework to build a test collection for a Vietnamese text categorization. The presented content includes our evaluations of some popular text categorization test collections, our researches on the requirements, the proposed model and the techniques to build the BKTexts - test collection for a Vietnamese text categorization. The XML specification of both text and metadata of Vietnamese documents in the BKTexts also is presented. Our BKTexts test collection is built with the XML specification and currently has more than 17100 Vietnamese text documents collected from e-newspapers. 
This paper reports our recent work of tool development for language resource construction. To make a revision of Asian WordNet which is automatically generated by using the existing English translation dictionary, we propose an online collaborative tool which can organize multiple translations. To support the work of syntactic dependency tree annotation, we develop an editing suite which integrates the utilities for word segmentation, POS tagging and dependency tree into a sequence of editing. 
This report introduces the activities of the two organizations related to collection and distribution of text and speech corpora in Japan. One is the Language Resource Association (GSK) and the other is NII-Speech Resources Consortium (NII-SRC). 
We explore the effects of language relatedness within a multilingual information retrieval (IR) framework which can be deployed to virtually any language, focusing specifically on Indo-European versus Semitic languages. The Semitic languages present unique challenges to IR for a number of reasons, so we set out to answer the question of whether cross-language IR for Semitic languages can be boosted by manipulation of the training data (which, in our framework, includes multilingual parallel text, some of which is morphologically analyzed). We attempted three measures to achieve this: first, the inclusion of genetically related (i.e., other Semitic) languages in the training data; second, the inclusion of non-related languages sharing the same script, and third, the inclusion of morphological analysis for Semitic languages. We find that language relatedness is a definite factor in boosting IR precision; script similarity can probably be ruled out as a factor; and morphological analysis can be helpful, but – perhaps paradoxically – not necessarily to the languages which are subjected to morphological analysis.  interesting for exploration. Taking one example, Semitic languages are distinguished by their complex morphology, a characteristic which presents challenges to an information retrieval model in which terms (usually, separated by white space or punctuation) are implicitly treated as individual units of meaning. We consider three possible methods for investigating the phenomena. In all cases, we keep the overall framework the same but simply make changes to the training data. One method we consider is to augment the training data with text from related languages; we compare results obtained from using Semitic languages with those obtained when non-Semitic languages are used. The other two relate to morphological analysis: the second is to replace inflected forms (in just one language, Arabic) with just the root in the training data; and the third is to remove vowels (again in just one language, Hebrew). The paper is organized as follows. Section 2 describes our general framework, which is a standard one used for CLIR. At a high level, section 3 outlines some of the challenges Semitic languages present within the context of our approach. In section 4, we compare results from using a number of different combinations of training data with the same test data. Finally, we conclude on our findings in section 5. 2 The Framework  
This paper presents a methodology for finding similarity and co-reference of documents across languages. The similarity between the documents is identified according to the content of the whole document and co-referencing of documents is found by taking the named entities present in the document. Here we use Vector Space Model (VSM) for identifying both similarity and co-reference. This can be applied in cross-lingual search engines where users get documents of very similar content from different language documents. 
Discovering parallel corpora on the web is a challenging task. In this paper, we use cross-language information retrieval techniques in combination with structural features to retrieve candidate page pairs from a commercial search engine. The candidate page pairs are then filtered using techniques described by Resnik and Smith (2003) to determine if they are translations. The results allow the comparison of efficiency of different parameter settings and provide an estimate for the percentage of pages that are parallel for a certain language pair. 
Parallel Named Entity pairs are important resources in several NLP tasks, such as, CLIR and MT systems. Further, such pairs may also be used for training transliteration systems, if they are transliterations of each other. In this paper, we profile the performance of a mining methodology in mining parallel named entity transliteration pairs in English and an Indian language, Tamil, leveraging linguistic tools in English, and article-aligned comparable corpora in the two languages. We adopt a methodology parallel to that of [Klementiev and Roth, 2006], but we focus instead on mining parallel named entity transliteration pairs, using a well-trained linear classifier to identify transliteration pairs. We profile the performance at several operating parameters of our algorithm and present the results that show the potential of the approach in mining transliterations pairs; in addition, we uncover a host of issues that need to be resolved, for effective mining of parallel named entity transliteration pairs. 
Accurate high-coverage translation is a vital component of reliable cross language information access (CLIA) systems. While machine translation (MT) has been shown to be effective for CLIA tasks in previous evaluation workshops, it is not well suited to specialized tasks where domain speciﬁc translations are required. We demonstrate that effective query translation for CLIA can be achieved in the domain of cultural heritage (CH). This is performed by augmenting a standard MT system with domainspeciﬁc phrase dictionaries automatically mined from the online Wikipedia. Experiments using our hybrid translation system with sample query logs from users of CH websites demonstrate a large improvement in the accuracy of domain speciﬁc phrase detection and translation. 
In this paper we present a statistical transliteration technique that is language independent. This technique uses Hidden Markov Model (HMM) alignment and Conditional Random Fields (CRF), a discriminative model. HMM alignment maximizes the probability of the observed (source, target) word pairs using the expectation maximization algorithm and then the character level alignments (n-gram) are set to maximum posterior predictions of the model. CRF has efﬁcient training and decoding processes which is conditioned on both source and target languages and produces globally optimal solutions. We apply this technique for Hindi-English transliteration task. The results show that our technique perfoms better than the existing transliteration system which uses HMM alignment and conditional probabilities derived from counting the alignments. 
 This paper describes a method for script independent word spotting in multilingual handwritten and machine printed documents. The system accepts a query in the form of text from the user and returns a ranked list of word images from document image corpus based on similarity with the query word. The system is divided into two main components. The ﬁrst component known as Indexer, performs indexing of all word images present in the document image corpus. This is achieved by extracting Moment Based features from word images and storing them as index. A template is generated for keyword spotting which stores the mapping of a keyword string to its corresponding word image which is used for generating query feature vector. The second component, Similarity Matcher, returns a ranked list of word images which are most similar to the query based on a cosine similarity metric. A manual Relevance feedback is applied based on Rocchio’s formula, which re-formulates the query vector to return an improved ranked listing of word images. The performance of the system is seen to be superior on printed text than on handwritten text. Experiments are reported on documents of three different languages: English, Hindi and Sanskrit. For handwritten English, an average precision of 67% was obtained for 30 query words. For machine printed Hindi, an average precision of 71% was obtained for 75 query words and for Sanskrit, an average precision of 87% with 100 queries was obtained.  Figure 1: A Sample English Document - Spotted Query word shown in the bounding box. 
This paper explores the research issue and methodology of a query focused multidocument summarizer. Considering its possible application area is Web, the computation is clearly divided into offline and online tasks. At initial preprocessing stage an offline document graph is constructed, where the nodes are basically paragraphs of the documents and edge scores are defined as the correlation measure between the nodes. At query time, given a set of keywords, each node is assigned a query dependent score, the initial graph is expanded and keyword search is performed over the graph to find a spanning tree identifying relevant nodes satisfying the keywords. Paragraph ordering of the output summary is taken care of so that the output looks coherent. Although all the examples, shown in this paper are based on English language, we show that our system is useful in generating query dependent summarization for non- English languages also. We also present the evaluation of the system. 
The talk deals with different approaches used for Named Entity recognition and how they are used in developing a robust Named Entity Recognizer. The talk includes the development of tagset for NER and manual annotation of text. 
 organizations, monetary expressions, dates, numer-  This paper is about Named Entity Recognition (NER) for Telugu. Not much work has been done in NER for Indian languages in general and Telugu in particular. Adequate annotated corpora are not yet available in Telugu. We recognize that named entities are usually nouns. In this paper we therefore start with our experiments in building a CRF (Conditional Random Fields) based Noun Tagger. Trained on a manually tagged data of 13,425 words and tested on a test data set of 6,223 words, this Noun Tagger  ical expressions etc. In the taxonomy of Computational Linguistics, NER falls within the category of Information Extraction which deals with the extraction of speciﬁc information from given documents. NER emerged as one of the sub-tasks of the DARPA-sponsored Message Understanding Conference (MUCs). The task has important signiﬁcance in the Internet search engines and is an important task in many of the Language Engineering applications such as Machine Translation, Question-Answering systems, Indexing for Information Retrieval and Automatic Summarization.  has given an F-Measure of about 92%. We then develop a rule based NER system for  2 Approaches to NER  Telugu. Our focus is mainly on identifying person, place and organization names. A manually checked Named Entity tagged corpus of 72,157 words has been developed using this rule based tagger through bootstrapping. We have then developed a CRF based NER system for Telugu and tested it on several data sets from the Eenaadu and Andhra Prabha newspaper corpora developed by us here. Good performance has been obtained using the majority tag concept. We have obtained overall F-measures between 80% and 97% in various experiments. Keywords: Noun Tagger, NER for Telugu, CRF, Majority Tag.  There has been a considerable amount of work on NER in English (Isozaki and Kazawa, 2002; Zhang and Johnson, 2003; Petasis et al., 2001; Mikheev et al., 1999). Much of the previous work on name ﬁnding is based on one of the following approaches: (1) hand-crafted or automatically acquired rules or ﬁnite state patterns (2) look up from large name lists or other specialized resources (3) data driven approaches exploiting the statistical properties of the language (statistical models). The earliest work in named-entity recognition involved hand-crafted rules based on pattern matching (Appelt et al., 1993). For instance, a sequence of capitalized words ending in ”Inc.” is typically the name of an organization in the US, so one could implement a rule to that effect. Another example of  
Named Entity Recognition (NER) aims to classify each word of a document into predefined target named entity classes and is nowadays considered to be fundamental for many Natural Language Processing (NLP) tasks such as information retrieval, machine translation, information extraction, question answering systems and others. This paper reports about the development of a NER system for Bengali using Support Vector Machine (SVM). Though this state of the art machine learning method has been widely applied to NER in several well-studied languages, this is our first attempt to use this method to Indian languages (ILs) and particularly for Bengali. The system makes use of the different contextual information of the words along with the variety of features that are helpful in predicting the various named entity (NE) classes. A portion of a partially NE tagged Bengali news corpus, developed from the archive of a leading Bengali newspaper available in the web, has been used to develop the SVM-based NER system. The training set consists of approximately 150K words and has been manually annotated with the sixteen NE tags. Experimental results of the 10-fold cross validation test show the effectiveness of the proposed SVM based NER system with the overall average Recall, Precision and F-Score of 94.3%, 89.4% and 91.8%, respectively. It has been shown that this system outperforms other existing Bengali NER systems. 
In this paper, we present a domain focused Tamil Named Entity Recognizer for tourism domain. This method takes care of morphological inflections of named entities (NE). It handles nested tagging of named entities with a hierarchical tagset containing 106 tags. The tagset is designed with focus to tourism domain. We have experimented building Conditional Random Field (CRF) models by training the noun phrases of the training data and it gives encouraging results. 
Named Entity Recognition (NER) is the task of identifying and classifying all proper nouns in a document as person names, organization names, location names, date & time expressions and miscellaneous. Previous work (Cucerzan and Yarowsky, 1999) was done using the complete words as features which suffers from a low recall problem. Character n-gram based approach (Klein et al., 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. Applying the same technique on Indian Languages, we experimented with Conditional Random Fields (CRFs), a discriminative model, and evaluated our system on two Indian Languages Telugu and Hindi. The character n-gram based models showed considerable improvement over the word based models. This paper describes the features used and experiments to increase the recall of Named Entity Recognition Systems which is also language independent. 
Several preprocessing steps are necessary in various problems of automatic Natural Language Processing. One major step is named-entity detection, which is relatively simple in English, because such entities start with an uppercase character. For Indian scripts like Bangla, no such indicator exists and the problem of identification is more complex, especially for human names, which may be common nouns and adjectives as well. In this paper we have proposed a three-stage approach of namedentity detection. The stages are based on the use of Named-Entity (NE) dictionary, rules for named-entity and left-right cooccurrence statistics. Experimental results obtained on Anandabazar Patrika (Most popular Bangla newspaper) corpus are quite encouraging. 
This paper is submitted for the contest NERSSEAL-2008. Building a statistical based Named entity Recognition (NER) system requires huge data set. A rule based system needs linguistic analysis to formulate rules. Enriching the language specific rules can give better results than the statistical methods of named entity recognition. A Hybrid model proved to be better in identifying Named Entities (NE) in Indian Language where the task of identifying named entities is far more complicated compared to English because of variation in the lexical and grammatical features of Indian languages. 
Abstract Stub This paper talks about a new approach to recognize named entities for Indian languages. Phonetic matching technique is used to match the strings of different languages on the basis of their similar sounding property. We have tested our system with a comparable corpus of English and Hindi language data. This approach is language independent and requires only a set of rules appropriate for a language. 
 trieving biomedical information (Tsai, 2006).  The process of NER consists of two steps  Named Entity Recognition(NER) is the task of identifying and classifying tokens in a  • identiﬁcation of boundaries of proper nouns.  text document into predeﬁned set of classes. In this paper we show our experiments with various feature combinations for Telugu NER. We also observed that the preﬁx and sufﬁx information helps a lot in ﬁnding the class of the token. We also show the effect of the training data on the performance of the system. The best performing model gave an Fβ=1 measure of 44.91. The language independent features gave an Fβ=1 measure of 44.89 which is close to Fβ=1 measure obtained even by including the language dependent features.  • classiﬁcation of these identiﬁed proper nouns. The Named Entities(NEs) should be correctly identiﬁed for their boundaries and later correctly classiﬁed into their class. Recognizing NEs in an English document can be done easily with a good amount of accuracy(using the capitalization feature). Indian Languages are very much different from the English like languages. Some challenges in named entity recognition that are found across various languages are: Many named entities(NEs) occur rarely in the corpus i.e they belong to the open class of nouns. Ambiguity of NEs. Ex Washington can be a person’s name or a  
In this paper, we propose an example-based decoder for a statistical machine translation (SMT) system, which is used for spoken language machine translation. In this way, it will help to solve the re-ordering problem and other problems for spoken language MT, such as lots of omissions, idioms etc. Through experiments, we show that this approach obtains improvements over the baseline on a Chinese-English spoken language translation task. 
Recently, many studies have been focused on extracting transliteration pairs from bilingual texts. Most of these studies are based on the statistical transliteration model. The paper discusses the limitations of previous approaches and proposes novel approaches called dynamic window and tokenizer to overcome these limitations. Experimental results show that the average rates of word and character precision are 99.0% and 99.78%, respectively. 
We study an adaptive learning framework for phonetic similarity modeling (PSM) that supports the automatic acquisition of transliterations by exploiting minimum prior knowledge about machine transliteration to mine transliterations incrementally from the live Web. We formulate an incremental learning strategy for the framework based on Bayesian theory for PSM adaptation. The idea of incremental learning is to benefit from the continuously developing history to update a static model towards the intended reality. In this way, the learning process refines the PSM incrementally while constructing a transliteration lexicon at the same time on a development corpus. We further demonstrate that the proposed learning framework is reliably effective in mining live transliterations from Web query results. 
We present a hybrid machine learning approach for coreference resolution. In our method, we use CRFs as basic training model, use active learning method to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge. We built a coreference resolution system based on the proposed method and evaluate its performance from three aspects: the contributions of active learning; the effects of different clustering algorithms; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method; clustering algorithm has a great effect on coreference resolution’s performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun’s resolution, especially the pronoun’s resolution. 
This paper investigates a machine learning approach for identification of temporal relation between events in Chinese text. We proposed a temporal relation annotation guideline (Cheng, 2007) and constructed temporal information annotated corpora. However, our previous criteria did not deal with various uses of Chinese verbs. For supplementing the previous version of our criteria, we introduce attributes of verbs that describe event types. We illustrate the attributes by the different examples of verb usages. We perform an experiment to evaluate the effect of our event type attributes in the temporal relation identification. As far as we know, this is the first work of temporal relation identification between verbs in Chinese texts. The result shows that the use of the attributes of verbs can improve the annotation accuracy. 
Word sense disambiguation is a basic problem in natural language processing. This paper proposed an unsupervised word sense disambiguation method based PageRank and HowNet. In the method, a free text is firstly represented as a sememe graph with sememes as vertices and relatedness of sememes as weighted edges based on HowNet. Then UW-PageRank is applied on the sememe graph to score the importance of sememes. Score of each definition of one word can be computed from the score of sememes it contains. Finally, the highest scored definition is assigned to the word. This approach is tested on SENSEVAL-3 and the experimental results prove practical and effective. 
Dependency parsing has gained attention in natural language understanding because the representation of dependency tree is simple, compact and direct such that robust partial understanding and task portability can be achieved more easily. However, many dependency parsers make hard decisions with local information while selecting among the next parse states. As a consequence, though the obtained dependency trees are good in some sense, the N-best output is not guaranteed to be globally optimal in general. In this paper, a stochastic dependency parsing scheme based on A* admissible search is formally presented. By well representing the parse state and appropriately designing the cost and heuristic functions, dependency parsing can be modeled as an A* search problem, and solved with a generic algorithm of state space search. When evaluated on the Chinese Tree Bank, this parser can obtain 85.99% dependency accuracy at 68.39% sentence accuracy, and 14.62% node ratio for dynamic heuristic. This parser can output N-best dependency trees, and integrate the semantic processing into the search process easily. 
The lack of internal information of Chinese synthetic words has become a crucial problem for Chinese morphological analysis systems which will face various needs of segmentation standards for upper NLP applications in the future. In this paper, we first categorize Chinese synthetic words into several types according to their inside semantic and syntactic structure, and then propose a method to represent these inside information of word by applying a tree-based structure. Then we try to automatically identify the inner morphological structure of 3-character synthetic words by using a large corpus and try to add syntactic tags to their internal structure. We believe that this tree-based word internal information could be useful in specifying a Chinese synthetic word segmentation standard. 
Since the first Chinese Word Segmentation (CWS) Bakeoff on 2003, CWS has experienced a prominent flourish because Bakeoff provides a platform for the participants, which helps them recognize the merits and drawbacks of their segmenters. However, the evaluation metric of bakeoff is not sufficient enough to measure the performance thoroughly, sometimes even misleading. One typical example caused by this insufficiency is that there is a popular belief existing in the research field that segmentation based on word can yield a better result than character-based tagging (CT) on in-vocabulary (IV) word segmentation even within closed tests of Bakeoff. Many efforts were paid to balance the performance on IV and out-ofvocabulary (OOV) words by combining these two methods according to this belief. In this paper, we provide a more detailed evaluation metric of IV and OOV words than Bakeoff to analyze CT method and combination method, which is a typical way to seek such balance. Our evaluation metric shows that CT outperforms dictionary-based (or so called word-based in general) segmentation on both IV and OOV words within Bakeoff * The work is done when the first author is working in MSRA as an intern.  closed tests. Furthermore, our analysis shows that using confidence measure to combine the two segmentation results should be under certain limitation. 
The Fourth International Chinese Language Processing Bakeoff was held in 2007 to assess the state of the art in three important tasks: Chinese word segmentation, named entity recognition and Chinese POS tagging. Twenty-eight groups submitted result sets in the three tasks across two tracks and a total of seven corpora. Strong results have been found in all the tasks as well as continuing challenges. 
This paper describes a Chinese part-ofspeech tagging system based on the maximum entropy model. It presents a novel two-stage approach to using the part-ofspeech tags of the words on both sides of the current word in Chinese part-of-speech tagging. The system is evaluated on four corpora at the Fourth SIGHAN Bakeoff in the close track of the Chinese part-ofspeech tagging task. 
This paper presents the Chinese word segmentation system developed by NOKIA Research Center (NRC), which was evaluated in the Fourth International Chinese Language Processing Bakeoff and the First CIPS Chinese Language Processing Evaluation organized by SIGHAN. In our system, a preprocessing module was used to discover the out-ofvocabulary words which occur repeatedly in the text, then an improved n-gram model was used for segmentation and some post processing strategies are adopted in system to recognize the organization names and new words. We took part in three tracks, which are called the open and closed track on corpora State Language Commission of P.R.C. (NCC), and closed track on corpora Shanxi University (SXU). Our system achieved good performance, especially in the open track on NCC, our system ranks 1st among 11 systems. 
Chinese word segmentation (CWS), named entity recognition (NER) and part-ofspeech tagging is the lexical processing in Chinese language. This paper describes the work on these tasks done by France Telecom Team (Beijing) at the fourth International Chinese Language Processing Bakeoff. In particular, we employ Conditional Random Fields with different features for these tasks. In order to improve NER relatively low recall; we exploit non-local features and alleviate class imbalanced distribution on NER dataset to enhance the recall and keep its relatively high precision. Some other post-processing measures such as consistency checking and transformation-based error-driven learning are used to improve word segmentation performance. Our systems participated in most CWS and POS tagging evaluations and all the NER tracks. As a result, our NER system achieves the first ranks on MSRA open track and MSRA/CityU closed track. Our CWS system achieves the first rank on CityU open track, which means that our systems achieve state-of-the-art performance on Chinese lexical processing.  
Chinese Word Segmentation(WS), Name Entity Recognition(NER) and Part-OfSpeech(POS) are three important Chinese Corpus annotation tasks. With the great improvement in these annotations on some corpus, now, the robustness, a capability of keeping good performances for a system by automatically fitting the different corpus and standards, become a focal problem. This paper introduces the work on robustness of WS and POS annotation systems from Beijing University of Posts and Telecommunications(BUPT), and two NER systems. The WS system combines a basic WS tagger with an adaptor used to fit a specific standard given. POS taggers are built for different standards under a two step frame, both steps use ME but with incremental features. A multiple knowledge source system and a less knowledge Conditional Random Field (CRF) based systems are used for NER. Experiments show that our WS and POS systems are robust. 
This paper describes the Chinese Word Segmenter for the fourth International Chinese Language Processing Bakeoff. Base on Conditional Random Field (CRF) model, a basic segmenter is designed as a problem of character-based tagging. To further improve the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora. 
We report a high-performance Chinese NER system that incorporates Conditional Random Fields (CRFs) and ﬁrst-order logic for the fourth SIGHAN Chinese language processing bakeoff (SIGHAN-6). Using current state-of-theart CRFs along with a set of well-engineered features for Chinese NER as the base model, we consider distinct linguistic characteristics in Chinese named entities by introducing various types of domain knowledge into Markov Logic Networks (MLNs), an effective combination of ﬁrst-order logic and probabilistic graphical models for validation and error correction of entities. Our submitted results achieved consistently high performance, including the ﬁrst place on the CityU open track and fourth place on the MSRA open track respectively, which show both the attractiveness and effectiveness of our proposed model. 
This paper describes a novel character tagging approach to Chinese word segmentation and named entity recognition (NER) for our participation in Bakeoff-4.1 It integrates unsupervised segmentation and conditional random ﬁelds (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 
This paper presents the results of our system that has participated in the word segmentation task in the Fourth SIGHAN Bakeoff. Our system consists of several basic components which include the preprocessing, token identification and the post-processing. An agent-based approach is introduced to identify the weak segmentation points. Our system has participated in two open and five closed tracks in five major corpora. Our results have attained top five in most of the tracks in the bakeoff. In particular, it is ranked first in the open track of the corpus from Academia Sinica, second in the closed track of the corpus from City University of Hong Kong, third in two closed tracks of the corpora from State Language Commission of P.R.C. and Academia Sinica.  2 System Description 2.1 Preprocessing In the preprocessing, the traditional Chinese characters, punctuation marks and other symbols are first identified. Instead of training all these symbols with the traditional Chinese characters in an agent-based system, an initial, but rough, segmentation points (SPr) are first inserted to distinguish the symbols and Chinese characters. For example, for the input sentence shown in Figure 1, segmentation points are first assumed in the sentence as shown in the Figure 2, where ‘/’ indicates the presence of a segmentation point. This roughly segmented sentence is then subject to an agent-based learning algorithm to have the token identification. 昨日 6 時 05 分終於成功發射了第一顆自行研 製的探月衛星「嫦娥一號」。 Figure 1: Original sentence for the process  
This paper expounds a Chinese word segmentation system built for the Fourth SIGHAN Bakeoff. The system participates in six tracks, namely the CityU Closed, CKIP Closed, CTB Closed, CTB Open, SXU Closed and SXU Open tracks. The model of Conditional Random Field is used as a basic approach in the system, with attention focused on the construction of feature templates and Chinese character categorization. The system is also augmented with some post-processing approaches such as the Extended Word String, model integration and others. The system performs fairly well on the 5 tracks of the Bakeoff. 
This paper mainly describes a Chinese named entity recognition (NER) system NER@ISCAS, which integrates text, partof-speech and a small-vocabularycharacter-lists feature and heristic postprocess rules for MSRA NER open track under the framework of Conditional Random Fields (CRFs) model. 
This paper presents a morpheme-based part-of-speech tagger for Chinese. It consists of two main components, namely a morpheme segmenter to segment each word in a sentence into a sequence of morphemes, based on forward maximum matching, and a lexical tagger to label each morpheme with a proper tag indicating its position pattern in forming a word of a specific class, based on lexicalized hidden Markov models. This system have participated four closed tracks for POS tagging at the Fourth International Chinese Language Processing Bakeoff sponsored by the ACLSIGHAN. 
Chinese word segmentation and named entity recognition (NER) are both important tasks in Chinese information processing. This paper presents a character-based Conditional Random Fields (CRFs) model for such two tasks. In The SIGHAN Bakeoff 2007, this model participated in all closed tracks for both Chinese NER and word segmentation tasks, and turns out to perform well. Our system ranks 2nd in the closed track on NER of MSRA, and 4th in the closed track on word segmentation of SXU. 
This paper presents the Chinese lexical analysis systems developed by Natural Language Processing Laboratory at Dalian University of Technology, which were evaluated in the 4th International Chinese Language Processing Bakeoff. The HMM and CRF hybrid model, which combines character-based model with word-based model in a directed graph, is adopted in system developing. Both the closed and open tracks regarding to Chinese word segmentation, POS tagging and Chinese Named Entity Recognition are involved in our systems’ evaluation, and good performance are achieved. Especially, in the open track of Chinese word segmentation on SXU, our system ranks 1st. 
In the Fourth SIGHAN Bakeoff, we took part in the closed tracks of the word segmentation, part of speech (POS) tagging and named entity recognition (NER) tasks. Particularly, we evaluated our word segmentation model on all the corpora, namely Academia Sinica (CKIP), City University of Hong Kong (CITYU), University of Colorado (CTB), State Language Commission of P.R.C. (NCC) and Shanxi University (SXU). For POS tagging and NER tasks, our models were evaluated on CITYU corpus only. Our models for the evaulation are based on the maximum entropy approach, we concentrated on the word segmentation task for the bakeoff and our best official results on all the corpora for this task are 0.9083 F-score on CITYU, 0.8985 on CKIP, 0.9077 on CTB, 0.8995 on NCC and 0.9146 on SXU. 
This paper proposes the use of global features for Chinese word segmentation. These global features are combined with local features using the averaged perceptron algorithm over N-best candidate word segmentations. The N-best candidates are produced using a conditional random ﬁeld (CRF) character-based tagger for word segmentation. Our experiments show that by adding global features, performance is signiﬁcantly improved compared to the character-based CRF tagger. Performance is also improved compared to using only local features. Our system obtains an F-score of 0.9355 on the CityU corpus, 0.9263 on the CKIP corpus, 0.9512 on the SXU corpus, 0.9296 on the NCC corpus and 0.9501 on the CTB corpus. All results are for the closed track in the fourth SIGHAN Chinese Word Segmentation Bakeoff. 
This paper briefly describes our system in The Fourth SIGHAN Bakeoff. Discriminative models including maximum entropy model and conditional random fields are utilized in Chinese word segmentation and named entity recognition with different tag sets and features. Transformation-based learning model is used in part-of-speech tagging. Evaluation shows that our system achieves the F-scores: 92.64% and 92.73% in NCC Word Segmentation close and open tests, 89.11% in MSRA name entity recognition open test, 91.13% and 91.97% in PKU part-of-speech tagging close and open tests. All the results get medium performances on the bakeoff tracks. 
This paper describes a Chinese word segmentation system based on word boundary token model and triple template matching model for extracting unknown words; and word support model for resolving segmentation ambiguity. 
This paper describes three systems: the Chinese word segmentation (WS) system, the named entity recognition (NER) system and the Part-of-Speech tagging (POS) system, which are submitted to the Fourth International Chinese Language Processing Bakeoff. Here, Conditional Random Fields (CRFs) are employed as the primary models. For the WS and NER tracks, the ngram language model is incorporated in our CRFs based systems in order to take into account the higher level language information. Furthermore, to improve the performances of our submitted systems, a transformationbased learning (TBL) technique is adopted for post-processing. 
In Chinese, most of the language processing starts from word segmentation and part-of-speech (POS) tagging. These two steps tokenize the word from a sequence of characters and predict the syntactic labels for each segmented word. In this paper, we present two distinct sequential tagging models for the above two tasks. The first word segmentation model was basically similar to previous work which made use of conditional random fields (CRF) and set of predefined dictionaries to recognize word boundaries. Second, we revise and modify support vector machine-based chunking model to label the POS tag in the tagging task. Our method in the WS task achieves moderately rank among all participants, while in the POS tagging task, it reaches very competitive results. 
This paper presents systems submitted to the close track of Fourth SIGHAN Bakeoff. We built up three systems based on Conditional Random Field for Chinese Word Segmentation, Named Entity Recognition and Part-Of-Speech Tagging respectively. Our systems employed basic features as well as a large number of linguistic features. For segmentation task, we adjusted the BIO tags according to confidence of each character. Our final system achieve a F-score of 94.18 at CTB, 92.86 at NCC, 94.59 at SXU on Segmentation, 85.26 at MSRA on Named Entity Recognition, and 90.65 at PKU on Part-Of-Speech Tagging. 
Chinese Named entity recognition is one of the most important tasks in NLP. Two kinds of Challenges we confront are how to improve the performance in one corpus and keep its performance in another different corpus. We use a combination of statistical models, i.e. a language model to recognize person names and two CRFs models to recognize Location names and Organization names respectively. We also incorporate an efficient heuristic named entity list searching process into the framework of statistical model in order to improve both the performance and the adaptability of the statistical NER system. We participate in the NER tests on open tracks of MSRA. The testing results show that our system can performs well. 
This paper introduces the system of Word Segmentation and analyzes its evaluation results in the Fourth SIGHAN Bakeoff1. A novel method has been used in the system, which main idea is: firstly, the main problems of WS have been classified, and then a cascaded model has been used to gradually optimize the system. The core of this WS system is the segmentation of ambiguous words and the internal information extraction of unknown words. The experiments show that the performance is satisfying, with the RIV-measure 96.8% in NCC open test in the SIGHAN bakeoff 2007.  two difficult problems. So, we aim at the solution of the both problem in our WS system. We participated the SIGHAN bakeoff 2007 evaluation, and a cascade model has been used in the process of word segmentation. In the WS system, the core modules are the segmentation of ambiguous words and the extraction of internal information of unknown words. 2 System Description Introduction Figure1 shows the workflow of our WS system. The system is made up of the following modules: small sentences segmentation, disambiguation, and unknown words recognition.  
We created a new Chinese morphological analyzer, Achilles, by integrating rule-based, dictionary-based, and statistical machine learning method, conditional random ﬁelds (CRF). The rulebased method is used to recognize regular expressions: numbers, time and alphabets. The dictionary-based method is used to ﬁnd in-vocabulary (IV) words while outof-vocabulary (OOV) words are detected by the CRFs. At last, conﬁdence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus. In spite of an unexpected ﬁle encoding errors, the system exhibited a top level performance. A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the ﬁfth and eighth position out of all 19 and 26 submissions respectively for the two corpus. Achilles uses a feature combined approach for partof-speech tagging. Our post-evaluation results prove the effectiveness of this approach for POS tagging. 
There are perhaps seven thousand languages in the world, ranging from the largest with hundreds of millions of speakers, to the smallest, with one speaker. On a different axis, languages can be ranked according to the quantity and quality of computational resources. Not surprisingly, there are correlations between these two axes: languages like English and Mandarin have substantial resources, while many of the smallest languages are completely undocumented. Nevertheless, the correlation is not perfect; there are languages with a million speakers which are more or less unwritten, and there are very large languages – some of the languages of India, for example – which are relatively resource-poor. Unfortunately, what counts as resource-rich (or even resource-adequate) in computational linguistics is a moving target. For languages to move in the direction of resource richness, considerable effort (people and money) have to be provided over a prolonged period of time. One can sit back and wait for this to happen, or give up; alternatively, one can map out a realistic way forward, building on the strengths of each language’s situation. Among the strengths which may prove useful to building computational resources for languages are the following: • Long traditions of grammatical and lexical description • Traditions of literacy and literature • Local expertise in linguistics and computing • The world-wide community of linguists and computer experts • Resource availability in related languages At the same time, there are weaknesses and other problems – some language specific, some more general – which need to be considered: • Lack of consensus on ways of representing the language (scripts, character encoding) • Complexities inherent in particular languages (complex scripts, complex morphologies, variant orthographies, diglossia, dialectal variation) • Economic and educational realities in the countries where the language is spoken • Political attitudes towards some languages, particularly minority languages • The 'not invented here' syndrome • Software obsolescence, and the potential obsolescence of language data This talk will look at ways in which the strengths enumerated above might be leveraged, while avoiding the potential weaknesses. 
Language resource development is crucial for language study in current approaches. Many efforts have been made to model a language on very large scaled corpora. Statistical and probabilistic approaches are playing a major role in taking the advantage of incorporating the context to improve their performance to a promising result in many areas of natural language processing such as machine translation, parsing, POS tagging, morphological analysis, etc. It is believed that if there are sufficient corpora for a language, we can develop many efficient language processing applications within an expectable period. However, corpus development is a labor intensive task and requires a continuous effort in maintaining the result to such a qualified level. The problem is magnified when we need to deal with the less computerized languages. The availability of the computerized language data can be varied by the availability of the standard of language encoding, number of speakers, economic scale of the speakers, and the language supporting tools. As a result, the technology gap between languages are widened as we can see in the evidence of online language populations and web contents which are mainly occupied by English, and others major languages distributed in Chinese, Spanish, Japanese, German and French. The major concern in the less computerized languages is how to leverage the technology for those languages which will result in scaling up the number of online language populations. Cross language resource sharing is one of the efforts to increase the opportunity for the access to those languages. We are expecting that a language may utilize the resource from other similar languages in terms of computational approaches and corpora. To relate the language resources among the less computerized languages has brought us to the following open questions: 1. Is there any intermediate representation that can efficiently relate among the languages? Will it be an approach of meaning representation such as conceptual unit, WordNet, or etymological word form representation such as Pali, Sanskrit, Chinese character? 2. Can a shallow language processing approach be used to increase the resources, namely orthographic conversion, transliteration? 3. Will English be a good intermediate language? This is because of the availability of the language pairing resources with the English language. As a platform for cross language resource development, we have developed KUI (Knowledge Unifying Initiator: http://www.tcllab.org/kui) equipped with a voting function to measure for the most reliable translation. The English language is not only a possible intermediate representation for languages. Other appropriate approaches could be considered to maximize the resource sharing among the less computerized languages if we can determine a better common feature among those languages. 
We know that the distribution of most of the linguistic entities (e.g. phones, words, grammar rules) follow a power law or the Zipf's law. This makes NLP hard. Interestingly, the distribution of speakers over the world, content over the web and linguistic resources available across languages also follow power law. However, the correlation between the distribution of number of speakers to that of web content and linguistic resources is rather poor, and the latter distributions are much more skewed than the former. In other words, there is a large volume of resources only for a very few languages and a large number of widely spoken languages, including all the Indian languages, have little or no linguistic resource at all. This is a serious challenge for NLP in these languages, primarily because state-of-the-art techniques and tools in NLP are all data-driven. I refer to this situation as the "Zipfian Barrier of NLP" and offer a mathematical analysis of the growth dynamics of the linguistic resources and NLP research worldwide, which, afterall, is very much a socio-economic process. Based on the analysis and otherwise, I propose certain technical ( e.g. unsupervised learning, wiki based approaches to gather data) and community-wide (e.g. acceptance of language specific works and resource building projects in top NLP conferences/journals, Special Interest Groups) initiatives that could possibly break this Zipfian Barrier. 5 Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 5–6, Hyderabad, India, January 2008. c 2008 Asian Federation of Natural Language Processing  6 
 equitable availability of opportunities made possi-  In the context of the IJCNLP workshop on Natural Language Processing (NLP) for Less Privileged Languages, we discuss the obstacles to research on such languages. We also brieﬂy discuss the ways to make progress in removing these obstacles. We mention some previous work and comment on the papers selected for the workshop.  ble by the language technology. There have already been attempts in this direction and this workshop will hopefully take them further. Figure-1 shows one possible view of the computational infrastructure needed for language processing for a particular language, or more preferably, for a set of related languages. In this paper, we will ﬁrst discuss various aspects of the problem. We will then look back at the work  
Collective intelligence is the capability for a group of people to collaborate in order to achieve goals in a complex context than its individual member. This common concept increases topic of interest in many sciences including computer science where computers are bring about as group support elements. This paper presents a new platform, called Knowledge Unifying Initiator (KUI) for knowledge development which enables connection and collaboration among individual intelligence in order to accomplish a complex mission. KUI is a platform to unify the various thoughts following the process of thinking, i.e., initiating the topic of interest, collecting the opinions to the selected topics, localizing the opinions through the translation or customization and posting for public hearing to conceptualize the knowledge. The process of thinking is done under the selectional preference simulated by voting mechanism in case that many alternatives occur. By measuring the history of participation of each member, KUI adaptively manages the reliability of each member’s opinion and vote according to the estimated ExpertScore. 
This paper presents a prototype Text-ToIndian Sign Language (ISL) translation system. The system will help dissemination of information to the deaf people in India. The current system takes English sentence as input, performs syntactic analysis, and generates the corresponding ISL structure. Since ISL does not have any written form, the output is represented in terms of prerecorded video streams. The system uses Lexical Functional Grammar (LFG) formalism for representing ISL syntax. 
For languages with inflectional morphology, development of a morphological parser can be a bottleneck to further development. We focus on two difficulties: first, finding people with expertise in both computer programming and the linguistics of a particular language, and second, the short lifetime of software such as parsers. We describe a methodology to split parser building into two tasks: descriptive grammar development, and formal grammar development. The two grammars are combined into a single document using Literate Programming. The formal grammar is designed to be independent of a particular parsing engine’s programming language, so that it can be readily ported to a new parsing engine, thus helping solve the software lifetime problem. 
The present paper describes an approach to adapting a parser to a new language. Presumably the target language is much poorer in linguistic resources than the source language. The technique has been tested on two European languages due to test data availability; however, it is easily applicable to any pair of sufficiently related languages, including some of the Indic language group. Our adaptation technique using existing annotations in the source language achieves performance equivalent to that obtained by training on 1546 trees in the target language. 
Sinhala, spoken in Sri Lanka as an ofﬁcial language, is one of the less privileged languages; still there are no established text input methods. As with many of the Asian languages, Sinhala also has a large set of characters, forcing us to develop an input method that involves a conversion process from a key sequence to a character/word. This paper proposes a novel word-based predictive text input system named SriShell Primo. This system allows the user to input a Sinhala word with a key sequence that highly matches his/her intuition from its pronunciation. A key to this scenario is a pre-compiled table that lists conceivable roman character sequences utilized by a wide range of users for representing a consonant, a consonant sign, and a vowel. By referring to this table, as the user enters a key, the system generates possible character strings as candidate Sinhala words. Thanks to a TRIE structured word dictionary and a fast search algorithm, the system successively and efﬁciently narrows down the candidates to possible Sinhala words. The experimental results show that the system greatly improves the userfriendliness compared to former characterbased input systems while maintaining high efﬁciency. 
Myanmar script uses no space between words and syllable segmentation represents a significant process in many NLP tasks such as word segmentation, sorting, line breaking and so on. In this study, a rulebased approach of syllable segmentation algorithm for Myanmar text is proposed. Segmentation rules were created based on the syllable structure of Myanmar script and a syllable segmentation algorithm was designed based on the created rules. A segmentation program was developed to evaluate the algorithm. A training corpus containing 32,283 Myanmar syllables was tested in the program and the experimental results show an accuracy rate of 99.96% for segmentation. 
We present some Language Technology applications that have proven to be effective tools to promote the use of Basque, a European less privileged language. We also present the strategy we have followed for almost twenty years to develop those applications as the top of an integrated environment of language resources, language foundations, language tools and other applications. When we have faced a difficult task such as Machine Translation to Basque, our strategy has worked well. We have had good results in a short time just reusing previous works for Basque, reusing other open-source tools, and developing just a few new modules in collaboration with other groups. In addition, new reusable tools and formats have been produced. 
This paper presents a rule-based approach for finding out the stems from text in Bengali, a resource-poor language. It starts by introducing the concept of orthographic syllable, the basic orthographic unit of Bengali. Then it discusses the morphological structure of the tokens for different parts of speech, formalizes the inflection rule constructs and formulates a quantitative ranking measure for potential candidate stems of a token. These concepts are applied in the design and implementation of an extensible architecture of a stemmer system for Bengali text. The accuracy of the system is calculated to be ~89% and above. 
 have been reported include Kinande, Latin, Bam-  Reduplication, the remaining problem in computational morphology is a morphological process that involves copying the base form wholly or partially. Reduplication can also be classiﬁed as either bounded or unbounded reduplication. Some solutions have been proposed for bounded reduplication. Some of the proposed solutions use ordered replace rules while others use simultaneous two-level rules. In our attempt to solve both bounded and unbounded reduplication we used a combination of two-level rules and replace rules. All our experiments were are carried out on Kinyarwanda an underresourced language with complex agglutinative morphology.  bara (Roark and Sproat, 2007); Tagalog and Malay (Beesley and Karttunen, 2003; Antworth, 1990). In these cases, one language may be exhibiting full stem reduplication while another may be exhibiting partial stem reduplication (Syllable). Reduplication may generally be divided into two: bounded and unbounded. Bounded reduplication is the kind that involves just repeating a given part of the word. Unbounded reduplication differs from bounded reduplication in that bounded reduplication involves copying of a ﬁxed number of morphemes. Unbounded reduplication is considerably more challenging to deal with compared with bounded reduplication. Unbounded reduplication has received little attention from researchers no wonder it is yet to be fully solved (Roark and Sproat, 2007). In principle, ﬁnite state methods are capable of han-  
Handling of human language by computer is a very intricate and complex task. In natural languages, sentences are usually part of discourse units just as words are part of sentences. Anaphora resolution plays a significant role in discourse analysis for chopping larger discourse units into smaller ones. This process is done for the purpose of better understanding and making easier the further processing of text by computer. This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora resolution in Urdu. Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate. 
A good POS tagger is a critical component of a machine translation system and other related NLP applications where an appropriate POS tag will be assigned to individual words in a collection of texts. There is not enough POS tagged corpus available in Manipuri language ruling out machine learning approaches for a POS tagger in the language. A morphology driven Manipuri POS tagger that uses three dictionaries containing root words, prefixes and suffixes has been designed and implemented using the affix information irrespective of the context of the words. We have tested the current POS tagger on 3784 sentences containing 10917 unique words. The POS tagger demonstrated an accuracy of 69%. Among the incorrectly tagged 31% words, 23% were unknown words (includes 9% named entities) and 8% known words were wrongly tagged. 
 started. The immediate aim of the project was to  This paper discusses an open source project1 which provides a framework for working with Indian language scripts using a uniform  rewrite the editor, remove its limitations and redesign it for use in a News Production environment using modern design and development tools.  syllable based text encoding scheme. It also  1.2 Acharya Text Editor  discusses the design and implementation of a multi-platform text editor for 9 Indian languages which was built based on this encoding scheme.  Acharya is a multi-platform text editor that supports Asamiya, Bangla, Devanagari, Gujarati, Kannada, Malayalam, Oriya, Punjabi, Tamil and Telugu. In addition to these scripts, it can also display text in  Keywords: Syllabic Encoding, Text Editor  Braille and RomanTrans using transliteration. It  implementation, Transliteration  achieves this functionality by storing Indic text in  
Human computer interaction through Natural Language Conversational Interfaces plays a very important role in improving the usage of computers for the common man. It is the need of time to bring human computer interaction as close to human-human interaction as possible. There are two main challenges that are to be faced in implementing such an interface that enables interaction in a way similar to human-human interaction. These are Speech to Text conversion i.e. Speech Recognition & Text To Speech (TTS) conversion. In this paper the implementation of one issue Speech Recognition for Indian Languages is presented. 
In this paper, we report a survey of language resources in Indonesia, primarily of indigenous languages. We look at the official Indonesian language (Bahasa Indonesia) and 726 regional languages of Indonesia (Bahasa Nusantara) and list all the available lexical resources (LRs) that we can gathered. This paper suggests that the smaller regional languages may remain relatively unstudied, and unknown, but they are still worthy of our attention. Various LRs of these endangered languages are being built and collected by regional language centers for study and its preservation. We will also briefly report its presence on the Internet.  that needs worried about, that it is part of a natural process that should be left to take its course. This paper suggests otherwise. The smaller regional languages may remain relatively unstudied, and unknown, but they are still worthy of our attention (Lauder, 2007). This paper puts forward a number of claims that have been made in favour of linguistic diversity and how we can preserve this diversity.  
This paper describes a machine learning algorithm for Gujarati Part of Speech Tagging. The machine learning part is performed using a CRF model. The features given to CRF are properly chosen keeping the linguistic aspect of Gujarati in mind. As Gujarati is currently a less privileged language in the sense of being resource poor, manually tagged data is only around 600 sentences. The tagset contains 26 different tags which is the standard Indian Language (IL) tagset. Both tagged (600 sentences) and untagged (5000 sentences) are used for learning. The algorithm has achieved an accuracy of 92% for Gujarati texts where the training corpus is of 10,000 words and the test corpus is of 5,000 words. 
 plicable in a wide variety of circumstances. There  Speech-to-speech machine translation is in some ways the peak of natural language processing, in that it deals directly with our original, oral mode of communication (as opposed to derived written language). As such, it presents challenges that are not to be taken lightly. Although existing technology covers each of the steps in the process, from speech recognition to synthesis, deriving a model of translation that is effective in the domain of spoken language is an interesting and challenging task. If we could teach our algorithms to learn as children acquire language, the result would be useful both for language technology and cognitive science. We propose several potential approaches, an implementation of a multi-path model that translates recognized morphemes alongside words, and a web-interface to test our speech translation tool as trained for Finnish to English. We also discuss current approaches to machine translation and the problems they face in adapting simultaneously to morphologically rich languages and to the spoken modality.  are still untapped resources, however, that might beneﬁt machine translation systems. Most statistical approaches do not take into account any similarities in word forms, so words that share a common root, (like “blanche” and “bianca”, meaning “white” in French and Italian respectively) are no more likely to be aligned than others (like “vache” and “guardare”, meaning “cow” and “to watch” respectively). Such a root is sometimes subject to vowel shift and consonant gradation, and may not be reﬂected in orthography, since it is often purely phonetic. This means we are not taking advantage of everything that normally beneﬁts human speakers, hearers and translators. It may be that a more natural approach to translation would ﬁrst involve understanding of the input, stored in some mental representation (an interlingua), and then generation of an equivalent phrase in the target language, directly from the knowledge sources. In order to allow for more dramatic differences in grammar like agglutinativity, it seems that the statistical machine translation (SMT) system must be more aware of sub-word units (morphemes) and features (phonetic similarity). This general sort of morphological approach could potentially beneﬁt any language pair, but might be crucial for a sys-  
Machine Translation has evolved tremendously in the recent time and stood as center of research interest for many computer scientists. Developing a Machine Translation system for ancient languages is much more fascinating and challenging task. A detailed study of Sanskrit language reveals that its well-structured and ﬁnely organized grammar has afﬁnity for automated translation systems. This paper provides necessary analysis of Sanskrit Grammar in the perspective of Machine Translation and also provides one of the possible solution for Samaas Vigraha(Compound Dissolution). Keywords: Machine Translation, Sanskrit, Natural Language Parser, Samaas Vigraha, Tokenization 
Stemmers have many applications in natural language processing and some fields such as information retrieval. Many algorithms have been proposed for stemming. In this paper, we propose a new algorithm for Persian language. Our algorithm is a bottom up algorithm that is capable to reorganize without changing the implementation. Our experiments show that the proposed algorithm has a suitable result in stemming and flexibility. 
This paper reports about the development of a Named Entity Recognition (NER) system for Bengali using the statistical Conditional Random Fields (CRFs). The system makes use of the different contextual information of the words along with the variety of features that are helpful in predicting the various named entity (NE) classes. A portion of the partially NE tagged Bengali news corpus, developed from the archive of a leading Bengali newspaper available in the web, has been used to develop the system. The training set consists of 150K words and has been manually annotated with a NE tagset of seventeen tags. Experimental results of the 10-fold cross validation test show the effectiveness of the proposed CRF based NER system with an overall average Recall, Precision and F-Score values of 93.8%, 87.8% and 90.7%, respectively. 
Multiword chunking is defined as a task to automatically analyze the external function and internal structure of the multiword chunk(MWC) in a sentence. To deal with this problem, we proposed a rule acquisition algorithm to automatically learn a chunk rule base, under the support of a large scale annotated corpus and a lexical knowledge base. We also proposed an expectation precision index to objectively evaluate the descriptive capabilities of the refined rule base. Some experimental results indicate that the algorithm can acquire about 9% useful expanded rules to cover 86% annotated positive examples, and improve the expectation precision from 51% to 83%. These rules can be used to build an efficient rule-based Chinese MWC parser. 
This paper presents an approach that uses structural information for Japanese named entity recognition (NER). Our NER system is based on Support Vector Machine (SVM), and utilizes four types of structural information: cache features, coreference relations, syntactic features and caseframe features, which are obtained from structural analyses. We evaluated our approach on CRL NE data and obtained a higher F-measure than existing approaches that do not use structural information. We also conducted experiments on IREX NE data and an NE-annotated web corpus and conﬁrmed that structural information improves the performance of NER. 
Query and document representation is a key problem for information retrieval and filtering. The vector space model (VSM) has been widely used in this domain. But the VSM suffers from high dimensionality. The vectors built from documents always have high dimensionality and contain too much noise. In this paper, we present a novel method that reduces the dimensionality using multilingual resource. We introduce a new metric called TC to measure the term consistency constraints. We deduce a TC matrix from the multilingual corpus and then use this matrix together with the termby-document matrix to do the Latent Semantic Indexing (LSI). By adopting different TC threshold, we can truncate the TC matrix into small size and thus lower the computational cost of LSI. The experimental results show that this dimensionality reduction method improves the retrieval performance significantly. 
We describe an algorithm that relies on web frequency counts to identify and correct writing errors made by non-native writers of English. Evaluation of the system on a realworld ESL corpus showed very promising performance on the very difﬁcult problem of critiquing English determiner use: 62% precision and 41% recall, with a false ﬂag rate of only 2% (compared to a random-guessing baseline of 5% precision, 7% recall, and more than 80% false ﬂag rate). Performance on collocation errors was less good, suggesting that a web-based approach should be combined with local linguistic resources to achieve both effectiveness and efﬁciency. 
We propose an intention analysis system for instant messaging applications. The system adopts Yahoo! directory as category trees, and classifies each dialogue into one of the categories of the directory. Two weighting schemes in information retrieval, i.e., tf and tf-idf, are considered in our experiments. In addition, we also expand Yahoo! directory with the accompanying HTML files and explore different features such as nouns, verbs, hypernym, hyponym, etc. Experiments show that category trees expanded with snippets together with noun features under tf scheme achieves a best Fscore, 0.86, when only 37.46% of utterances are processed on the average. This methodology is employed to recommend advertisements relevant to the dialogue. 
Term Extraction (TE) is an important component of many NLP applications. In general, terms are extracted for a given text collection based on global context and frequency analysis on words/phrases association. These extracted terms represent effectively the text content of the collection for knowledge elicitation tasks. However, they fail to dictate the local contextual information for each document effectively. In this paper, we refine the state-of-the-art C/NCValue term weighting method by considering both termhood and unithood measures, and use the former extracted terms to direct the local term extraction for each document. We performed the experiments on Straits Times year 2006 corpus and evaluated our performance using Wikipedia termbank. The experiments showed that our model outperforms C/NC-Value method for global term extraction by 24.4% based on term ranking. The precision for local term extraction improves by 12% when compared to pure linguistic based extraction method. 
Search results clustering helps users to browse the search results and locate what they are looking for. In the search result clustering, the label selection which annotates a meaningful phrase for each cluster becomes the most fundamental issue. In this paper, we present a new method of using the language modeling approach over Dmoz for label selection, namely label language model. Experimental results show that our method is helpful to obtain meaningful clustering labels of search results. 
To transliterate foreign words, in Japanese and Korean, phonograms, such as Katakana and Hangul, are used. In Chinese, the pronunciation of a source word is spelled out using Kanji characters. Because Kanji is ideogrammatic representation, different Kanji characters are associated with the same pronunciation, but can potentially convey different meanings and impressions. To select appropriate Kanji characters, an existing method requests the user to provide one or more related terms for a source word, which is time-consuming and expensive. In this paper, to reduce this human effort, we use the World Wide Web to extract related terms for source words. We show the effectiveness of our method experimentally. 
We propose a new formally syntax-based method for statistical machine translation. Transductions between parsing trees are transformed into a problem of sequence tagging, which is then tackled by a searchbased structured prediction method. This allows us to automatically acquire translation knowledge from a parallel corpus without the need of complex linguistic parsing. This method can achieve comparable results with phrase-based method (like Pharaoh), however, only about ten percent number of translation table is used. Experiments show that the structured prediction approach for SMT is promising for its strong ability at combining words. 
Target task matched parallel corpora are required for statistical translation model training. However, training corpora sometimes include both target task matched and unmatched sentences. In such a case, training set selection can reduce the size of the translation model. In this paper, we propose a training set selection method for translation model training using linear translation model interpolation and a language model technique. According to the experimental results, the proposed method reduces the translation model size by 50% and improves BLEU score by 1.76% in comparison with a baseline training corpus usage. 
 2 Related Work  This paper presents methods to combine large language models trained from diverse text sources and applies them to a state-ofart French–English and Arabic–English machine translation system. We show gains of over 2 BLEU points over a strong baseline by using continuous space language models in re-ranking. 
We present an approach to text navigation conceived as a cognitive process exploiting linguistic information present in texts. We claim that the navigational knowledge involved in this process can be modeled in a declarative way with the Sextant language. Since Sextant refers exhaustively to specific linguistic phenomena, we have defined a customized text representation. These different components are implemented in the text navigation system NaviTexte. Two applications of NaviTexte are described. 
This paper explores an automatic WordNet synset assignment to the bi-lingual dictionaries of languages having limited lexicon information. Generally, a term in a bilingual dictionary is provided with very limited information such as part-of-speech, a set of synonyms, and a set of English equivalents. This type of dictionary is comparatively reliable and can be found in an electronic form from various publishers. In this paper, we propose an algorithm for applying a set of criteria to assign a synset with an appropriate degree of confidence to the existing bi-lingual dictionary. We show the efficiency in nominating the synset candidate by using the most common lexical information. The algorithm is evaluated against the implementation of ThaiEnglish, Indonesian-English, and Mongolian-English bi-lingual dictionaries. The experiment also shows the effectiveness of using the same type of dictionary from different sources.  
Deﬁning all words in a Japanese dictionary by using a limited number of words (deﬁning vocabulary) is helpful for Japanese children and second-language learners of Japanese. Although some English dictionaries have their own deﬁning vocabulary, no Japanese dictionary has such vocabulary as of yet. As the ﬁrst step toward building a Japanese deﬁning vocabulary, we ranked Japanese words based on a graphbased method. In this paper, we introduce the method, and show some evaluation results of applying the method to an existing Japanese dictionary. 
In this paper we explore the potential for identifying computationally relevant typological features from a multilingual corpus of language data built from readily available language data collected off the Web. Our work builds on previous structural projection work, where we extend the work of projection to building individual CFGs for approximately 100 languages. We then use the CFGs to discover the values of typological parameters such as word order, the presence or absence of deﬁnite and indeﬁnite determiners, etc. Our methods have the potential of being extended to many more languages and parameters, and can have signiﬁcant effects on current research focused on tool and resource development for low-density languages and grammar induction from raw corpora. 
Automatic paraphrasing is a transformation of expressions into semantically equivalent expressions within one language. For generating a wider variety of phrasal paraphrases in Japanese, it is necessary to paraphrase functional expressions as well as content expressions. We propose a method of paraphrasing of Japanese functional expressions using a dictionary with two hierarchies: a morphological hierarchy and a semantic hierarchy. Our system generates appropriate alternative expressions for 79% of source phrases in Japanese in an open test. It also accepts style and readability speciﬁcations. 
This paper presents a Prefix Tree (Trie) based model for Generation of Referring Expression (GRE). The existing algorithms in GRE lie in two extremities. Incremental algorithm is simple and speedy but less expressive in nature whereas others are complex and exhaustive but more expressive in nature. Our prefix tree based model not only incorporates all relevant features of GRE (like describing set, generating Boolean and context sensitive description etc.) but also try to attain simplicity and speed properties of Incremental algorithm. Thus this model provides a simple and linguistically rich approach to GRE. 
We have carried out a series of coverage evaluations of diverse types of parsers using texts from several genres such as newspaper, religious, legal and biomedical texts. We compared the overall coverage of the evaluated parsers and analyzed the differences by text genre. The results indicate that the coverage typically drops several percentage points when parsers are faced with texts on genres other than newspapers. 
This paper presents a method to enhance a Chinese parser in parsing conjunctive structures. Long conjunctive structures cause long-distance dependencies and tremendous syntactic ambiguities. Pure syntactic approaches hardly can determine boundaries of conjunctive phrases properly. In this paper, we propose a divide-andconquer approach which overcomes the difficulty of data-sparseness of the training data and uses both syntactic symmetry and semantic reasonableness to evaluate ambiguous conjunctive structures. In comparing with the performances of the PCFG parser without using the divide-andconquer approach, the precision of the conjunctive boundary detection is improved from 53.47% to 83.17%, and the bracketing f-score of sentences with conjunctive structures is raised up about 11 %. 
The paper introduces a dependency annotation effort which aims to fully annotate a million word Hindi corpus. It is the first attempt of its kind to develop a large scale tree-bank for an Indian language. In this paper we provide the motivation for following the Paninian framework as the annotation scheme and argue that the Paninian framework is better suited to model the various linguistic phenomena manifest in Indian languages. We present the basic annotation scheme. We also show how the scheme handles some phenomenon such as complex verbs, ellipses, etc. Empirical results of some experiments done on the currently annotated sentences are also reported. 
We constructed a system for answering nonfactoid Japanese questions. We used various methods of passage retrieval for the system. We extracted paragraphs based on terms from an input question and output them as the preferred answers. We classiﬁed the non-factoid questions into six categories. We used a particular method for each category. For example, we increased the scores of paragraphs including the word “reason” for questions including the word “why.” We participated at NTCIR-6 QAC-4, where our system obtained the most correct answers out of all the eight participating teams. The rate of accuracy was 0.77, which indicates that our methods were effective. 
Abstract. In this paper, a new multidocument multi-lingual text summarization technique, based on singular value decomposition and hierarchical clustering, is proposed. The proposed approach relies on only two resources for any language: a word segmentation system and a dictionary of words along with their document frequencies. The summarizer initially takes a collection of related documents, and transforms them into a matrix; it then applies singular value decomposition to the resulted matrix. After using a binary hierarchical clustering algorithm, the most important sentences of the most important clusters form the summary. The appropriate place of each chosen sentence is determined by a novel technique. The system has been successfully tested on summarizing several Persian document collections.  to this solution (McKeown et. al., 2002; Radev et. al., 2001). Generally in the process of multi-document text summarization, a collection of input documents about a particular subject is received from the user and a coherent summary without redundant information is generated. However, several challenges exist in this process the most important of which are removing redundant information from the input sentences and ordering them properly in the output summary. In a new approach to multi-document summarization proposed in this paper, Singular Value Decomposition (SVD) is used to find the most important dimensions and also to remove noisy ones. This process makes clustering of similar sentences easer. In order to determine the level of importance of different clusters, the generated singular values and singular vector of the SVD have been used in a fashion similar to that (Steinberger and., Ježek, 2004). To evaluate generated summaries the SVD-based method proposed in the same paper is used.  
Automatic summarization is an important task as a form of human support technology. We propose in this paper a new summarization method that is based on example-based approach. Using example-based approach for the summarization task has the following three advantages: high modularity, absence of the necessity to score importance for each word, and high applicability to local context. Experimental results have proven that the summarization system attains approximately 60% accuracy by human judgment. 
In this paper, we propose a cluster-adjacency based method to order sentences for multi-document summarization tasks. Given a group of sentences to be organized into a summary, each sentence was mapped to a theme in source documents by a semi-supervised classification method, and adjacency of pairs of sentences is learned from source documents based on adjacency of clusters they belong to. Then the ordering of the summary sentences can be derived with the first sentence determined. Experiments and evaluations on DUC04 data show that this method gets better performance than other existing sentence ordering methods. 1. Introduction The issue of how to extract information from source documents is one main topic of summarization area. Being the last step of multi-document summarization tasks, sentence ordering attracts less attention up to now. But since a good summary should be fluent and readable to human being, sentence ordering which organizes texts into the final summary could not be ignored. Sentence ordering is much harder for multi-document summarization than for single-document summarization (McKeown et al., 2001; Barzilay and Lapata, 2005). The main reason is that unlike single document, multi-documents don’t provide a natural order of texts to be the basis of sentence ordering judgment. This is more obvious for sentence extraction based summarization systems. Majority ordering is one way of sentence ordering (McKeown et al., 2001; Barzilay et al., 2002). This method groups sentences in source documents into different themes or topics based on summary sentences to be ordered, and the order of summary sentences is determined based on the order of themes. The idea of this method is reasonable since the summary of multi-documents usually covers several topics in source documents to achieve representative, and the theme ordering can suggest sentence ordering somehow. However, there are two challenges for this method. One is how to cluster sentences into topics, and the other is how to order sentences belonging to the same topic. Barzilay et al. (2002) combined topic relatedness and chronological ordering together to order sentences. Besides chronological ordering,  sentences were also grouped into different themes and ordered by the order of themes learned from source documents. The results show that topic relatedness can help chronological ordering to improve the performance. Probabilistic model was also used to order sentences. Lapata (2003) ordered sentences based on conditional probabilities of sentence pairs. The conditional probabilities of sentence pairs were learned from a training corpus. With conditional probability of each sentence pairs, the approximate optimal global ordering was achieved with a simple greedy algorithm. The conditional probability of a pair of sentences was calculated by conditional probability of feature pairs occurring in the two sentences. The experiment results show that it gets significant improvement compared with randomly sentence ranking. Bollegala et al. (2005) combined chronological ordering, probabilistic ordering and topic relatedness ordering together. They used a machine learning approach to learn the way of combination of the three ordering methods. The combined system got better results than any of the three individual methods. Nie et al. (2006) used adjacency of sentence pairs to order sentences. Instead of the probability of a sentence sequence used in probabilistic model, the adjacency model used adjacency value of sentence pairs to order sentences. Sentence adjacency is calculated based on adjacency of feature pairs within the sentence pairs. Adjacency between two sentences means how closely they should be put together in a set of summary sentences. Although there is no ordering information provided by sentence adjacency, an optimal ordering of summary sentences can be derived by use of adjacency information of all sentence pairs if the first sentence is properly selected. In this paper, we propose a new sentence ordering method named cluster-adjacency based ordering. Like the featureadjacency based ordering mentioned above, the ordering process still depends on sentence adjacency. But we cluster sentences first and use cluster adjacency instead of feature adjacency to calculate sentence adjacency. The advantage of this change is to avoid the sensitivity of the adjacency  745  information to limited number of individual features, which usually needs manual intervention. The remainder of this paper is organized as follows. In section 2, we specify the motivation of this method. In section 3, we talk about the sentence classification using a semi-supervised method. In section 4, we discuss the procedure for sentence ordering. In section 5, we present experiments and evaluation. In section 6, we give the conclusion and future work. 2. Motivation Majority ordering assumes that sentences in the summary belong to different themes or topics, and the ordering of sentences in the summary can be determined by the occurring sequence of themes in source documents. In order to derive the order of themes, Barzilay et al. (2002) presented themes and their relations as a directed graph. In the graph, nodes denote themes; an edge from one node to another denotes the occurring of one theme before another in a source document, and the weight of an edge is set to be the frequency of the theme pair co-occurring in the texts. Each theme is given a weight that equals to the difference between its outgoing edges and incoming edges. By finding and removing a theme with the biggest weight in the graph recursively, an ordering of themes is determined. Probabilistic ordering method treats the ordering as a task of finding the sentence sequence with the biggest probability (Lapata, 2003). For a sentence sequence T= S1, S2,…,Sn , suppose that the probability of any given sentence is determined only by its previous sentence, the probability of a sentence sequence can be generated based on the condition probabilities P(Si|Si-1) of all adjacent sentence pairs in the sequence. The condition probability P(Si|Si-1) can be further resolved as the product of condition probabilities of feature pairs P(fl|fm), where fl is the feature in Si, fm is the feature in Si-1. By finding the sentence with the biggest condition probability with the previous one recursively, an ordering of sentences is determined. A null sentence is normally introduced at the beginning of each source document to find the first sentence (Lapata, 2003). Both majority ordering and probabilistic ordering determine text sequences in the summary based on those in the source documents. The intuition behind the idea is that the ordering of summary sentences tends to be consistent with those of document sentences. However, we notice that some important information might be lost in the process. Consider examples below: Example 1: Source Document = ……ABA…… Example 2: Source Document 1 = ……AB…… Source Document 2 = ……BA…… Here A and B denote two themes. Let’s assume that A and B are both denoted by the summary sentences. In both examples, the frequency of A preceding B equals to that of B preceding A, thus no sequence preference could be learned  from the two examples, and we can only estimate a probability of 0.5 following one by another. With such estimation, the intuition that A and B shall be put adjacently although their ordering is not clear would be difficult to capture.  An adjacency based ordering (Nie et al., 2006) was proposed to capture such adjacency information between texts during sentence ordering. It uses adjacency of sentence pairs to order summary sentences. Adjacency between two sentences can be seen as how closely they should be put together in an output summary. In general, sentence adjacency is derived from that of feature pairs within sentences. Note that there is no clue to decide the sequence of two sentences purely based on their adjacency value. However, if the first sentence has been decided, the total sentence sequence can be derived according to the adjacency values by recursively selecting one having the biggest adjacency value with the most recently selected.  For adjacency based ordering, a problem is how to calculate the adjacency value between two sentences. For feature-adjacency based ordering, the sentence adjacency is calculated based on that of feature pairs within the two sentences. But a sentence may contain many single word features, and there may exist many noisy features, especially for longer sentences. To eliminate the impact of noisy features, one simple method is to select top n most adjacent feature pairs among the two sentences (Nie et al., 2006). However, the parameter heavily influences the performance, as shown in Table 1, where each row gives the result of a run with the same window range and different top n adjacent feature pairs.  Win_ range 2 3 4  τ(top- τ(top- τ(top τ(top- τ(top- τ(top- n=1) n=2) -n=3) n=4) n=5) n=10) 0.184 0.213 0.253 0.262 0.261 0.224 0.251 0.252 0.273 0.274 0.257 0.213 0.201 0.253 0.268 0.316 0.272 0.248 Table 1. Feature-Adjacency Based Ordering  The heavy reliance on the manually pre-defined parameter is an obstacle for implementation of the feature-adjacency based ordering, since it’s hard to determine the most suitable value for the parameter across different tasks. More generally, the feature-adjacency method depends on limited number of individual features, which normally needs very strong feature selection techniques to be effective. To avoid the sensitivity to individual features, we propose a cluster-adjacency based sentence ordering. Although the clustering will also use individual features, the noisy ones would be lower weighted via appropriate weighting schemes. Assuming there are n summary sentences to be ordered, we cluster sentences in source documents into n clusters based on the n summary sentences. Each cluster represents a summary sentence. Then we use the cluster adjacency instead of feature adjacency to produce sentence adjacency. Since features are not directly used in calculating sentence  746  adjacency, the setting of the parameter to remove noisy features is no more needed. In addition, we expect the clustering to determine the themes properly and reduce the affect of noisy features. 3. Sentence Clustering Assume there are K summary sentences to be ordered, and there are N sentences in source documents, we cluster the N sentences into K clusters using a semi-supervised classification method, Label Propagation (Zhu and Ghahramani, 2003). The advantage of this method is that it can exploit the closeness between unlabeled data during classification, thus ensuring a better classification result even with very fewer labeled data. This is exactly the situation here, where each summary sentence can be seen as the only one labeled data for the class. Following are some notations for the label propagation algorithm in sentence classification: {rj} (1≤j≤K): the K summary sentences {mj} (1≤j≤N): the N document sentences to be classified X = {xi} (1≤i≤K+N) refers to the union set of the above two categories of sentences, i.e. xi (1≤i≤K) represents the K summary sentences, xi (K+1≤i≤K+N+1) represents the N sentences to be classified. That is, the first K sentences are labeled sentences while the remaining N sentences are to be re-ranked. C = {cj} (1≤j≤K) denotes the class set of sentences, each one in which is labeled by a summary sentence. Y0 ∈ Hs×K (s=K+N) represents initial soft labels attached to each sentence, where Yij0= 1 if xi is cj and 0 otherwise. Let YL0 be top l=K rows of Y0, which corresponds to the labeled data, and YU0 be the remaining N rows, which corresponds to the unlabeled data. Here, each row in YU0 is initialized according to the similarity of a sentence with the summary sentences. In the label propagation algorithm, the manifold structure in X is represented as a connected graph and the label information of any vertex in the graph is propagated to nearby vertices through weighted edges until the propagation process converges. Here, each vertex corresponds to a sentence, and the edge between any two sentences xi and xj is weighted by wij to measure their similarity. Here wij is defined as follows: wij = exp(-dij2/ σ 2) if i ≠ j and wii = 0 (1≤i,j≤l+u), where dij is the distance between xi and xj, and σ is a scale to control the transformation. In this paper, we set σ as the average distance between summary sentences. Moreover, the weight wij between two sentences xi and xj is transformed to a probability tij = P(j→i) =wij/(∑sk=1wkj), where tij is the probability to propagate a label from sentence xj to sentence xi. In principle, larger weights between two sentences mean easy travel and similar labels between them according to the global consistency assumption applied in this algorithm. Finally, tij is normalized row by row as in (1), which is to maintain the class probability interpretation of Y. The s × s matrix is denoted asT as in (1). During the label propagation process, the label distribution of the labeled data is clamped in each loop and acts like forces to push out labels through unlabeled data. With this push originates from labeled data, the label boundaries will  be pushed much faster along edges with larger weights and settle in gaps along those with lower weights. Ideally, we can expect that wij across different classes should be as small as possible and wij within a same class as big as possible. In this way, label propagation happens within a same class most likely.  (1 )  ∑ t ij = t ij  t s k = 1 ik  (2)  YˆU  =  lim t→∞  Y  t U  =  (I  − T uu ) −1T ul  Y  0 L  .  (3)  T  =  ⎡T ll ⎢  ⎢⎣T ul  T lu ⎤ ⎥ T uu ⎥⎦  This algorithm has been shown to converge to a unique  solution (Zhu and Ghahramani, 2003) with u=M and l=K as  in (2), where I is u × u identity matrix. T uu and T ul are acquired by splitting matrix T after the l-th row and the l-th  column into 4 sub-matrices as in (3).  In theory, this solution can be obtained without iteration  and the initialization does not affect the  of YU0 is not estimation of  important, since YˆU . However,  YU0 the  initialization of YU0 helps the algorithm converge quickly  in practice. In this paper, each row in YU0 is initialized  according the similarity of a sentence with the summary  sentences. Fig. 1 gives the classification procedure.  INPUT  {xi} (1≤i≤K): set of summary sentences as labeled data; {xi} (K+1≤i≤K+N+1): set of document sentences; Algorithm: Label_Propagation({rj}, {mj}) BEGIN  Set the iteration index t=0  BEGIN DO Loop  Propagate the label by Yt+1 = T Yt; Clamp labeled data by replacing top l row of Yt+1 with YL0 END DO Loop when Yt converges; END  Fig. 1 Label propagation for sentence classification  The output of the classification is a set of sentence clusters, and the number of the clusters equals to the number of summary sentences. In each cluster, the members can be ordered by their membership probabilities. In fact, the semisupervised classification is a kind of soft labeling (Tishby and Slonim, 2000; Zhou et al., 2003), in which each sentence belongs to different clusters, but with different probabilities. For sentence ordering task here, we need to get hard clusters, in which each sentence belongs to only one cluster. Thus, we need to cut the soft clusters to hard ones. To do that, for each cluster, we consider every sentence inside according to their decreasing order of their membership probabilities. If a sentence belongs to the current cluster with the highest probability, then it is selected and kept. The selection repeats until a sentence belongs to another cluster with higher probability. 4. Sentence Ordering Given a set of summary sentences {S1,…,SK}, sentences of the source documents are clustered into K groups G1,…,GK,  747  where Si is corresponding with Gi. For each pair of sentences Si and Sj, the adjacency of Si and Sj can be defined as the adjacency of Gi and Gj, defined in (4).  C i, j =  f (G i, G j )2 f (G i ) f (G j )  (4)  Here f(Gi) and f(Gj) respectively denote the frequency of cluster Gi and Gj in source documents, f(Gi, Gj) denotes the frequency of Gi and Gj co-occurring in the source documents within a limited window range.  The first sentence S1 can be determined according to (5) based on the adjacency between null clusters (containing only the null sentence) and any sentence clusters.  S 1 = arg max( C o , j )  (5)  S j∈T  Here C0,j denotes how close the sentence Sj and a null sentence are. By adding a null sentence at the beginning of each source document as S0 , and assuming it contains one null sentence, C0,j can be calculated with equation (4). Given an already ordered sentence sequence, S1, S2,…,Si, whose sentence set R is subset of the whole sentence set T, the task of finding the (i+1)th sentence can be described as:  S i + 1 = arg max( C i , j )  (6)  S j∈T − R  Now the sentence sequence become S1, S2,…,Si, Si+1. By repeating the step the whole sequence can be derived.  5. Experiments and Evaluation In this section, we describe the experiments with clusteradjacency based ordering, and compared it with majority ordering, probability-based ordering and feature-adjacency based ordering respectively. Some methods [e.g., 8] tested ordering models using external training corpus and extracted sentence features such as nouns, verbs and dependencies from parsed tress. In this paper, we only used the raw input data, i.e., source input documents, and didn’t use any grammatical knowledge. For feature-adjacency based model, we used single words except stop words as features to represent sentences. For cluster-adjacency based model, we used the same features to produce vector representations for sentences.  5.1 Test Set and Evaluation Metrics Regarding test data, we used DUC04 data. DUC 04 provided 50 document sets and four manual summaries for each document set in its Task2. Each document set consists of 10 documents. Sentences of each summary were taken as inputs to ordering models, with original sequential information being neglected. The output ordering of various models were to be compared with that specified in manual summaries. A number of metrics can be used to evaluate the difference between two orderings. In this paper, we used Kendall’s τ [9], which is defined as:  τ = 1 − 2 ( number _ of _ inversions  )  (7)  N ( N − 1) / 2  Here N is the number of objects to be ordered (i.e., sentences). Number_of_inversions is the minimal number of interchanges of adjacent objects to transfer an ordering into another. Intuitively, τ can be considered as how easily an ordering can be transferred to another. The value of τ ranges from -1 to 1, where 1 denotes the best situation ---- two orderings are the same, and -1 denotes the worst situation--completely converse orderings. Given a standard ordering, randomly produced orderings of the same objects would get an average τ of 0. For examples, Table 2 gives three number sequences, their natural sequences and the corresponding τ values.  Examples  Natural sequences τ values  
In this paper, we propose a novel approach for Cross-Lingual Question Answering (CLQA). In the proposed method, the statistical machine translation (SMT) is deeply incorporated into the question answering process, instead of using it as the pre-processing of the mono-lingual QA process as in the previous work. The proposed method can be considered as exploiting the SMT-based passage retrieval for CLQA task. We applied our method to the English-toJapanese CLQA system and evaluated the performance by using NTCIR CLQA 1 and 2 test collections. The result showed that the proposed method outperformed the previous pre-translation approach. 
We present experiments that analyze the necessity of using a highly interconnected word/sense graph for unsupervised allwords word sense disambiguation. We show that allowing only grammatically related words to inﬂuence each other’s senses leads to disambiguation results on a par with the best graph-based systems, while greatly reducing the computation load. We also compare two methods for computing selectional preferences between the senses of every two grammatically related words: one using a Lesk-based measure on WordNet, the other using dependency relations from the British National Corpus. The best conﬁguration uses the syntactically-constrained graph, selectional preferences computed from the corpus and a PageRank tie-breaking algorithm. We especially note good performance when disambiguating verbs with grammatically constrained links. 
In this paper, we present an in depth revision of the preliminary version of PrepNet, an lexical semantics database of preposition behaviors. This revision includes a much more detailed structure for the language realization level.  in an abstract way, where frames represent some generic semantic aspects of these notions, • the language realization levels that deal with realizations for various languages, using a variety of marks (postpositions, afﬁxes, compounds, etc.). However, we will keep the term ’preposition’ hereafter for all these marks.  
Previous research has shown that syntactic features are the most informative features in automatic verb classiﬁcation. We experiment with a new, rich feature set, extracted from a large automatically acquired subcategorisation lexicon for English, which incorporates information about arguments as well as adjuncts. We evaluate this feature set using a set of supervised classiﬁers, most of which are new to the task. The best classiﬁer (based on Maximum Entropy) yields the promising accuracy of 60.1% in classifying 204 verbs to 17 Levin (1993) classes. We discuss the impact of this result on the stateof-art, and propose avenues for future work. 
 This paper reconsiders the task of MRDbased word sense disambiguation, in extending the basic Lesk algorithm to investigate the impact on WSD performance of different tokenisation schemes, scoring mechanisms, methods of gloss extension and ﬁltering methods. In experimentation over the Lexeed Sensebank and the Japanese Senseval2 dictionary task, we demonstrate that character bigrams with sense-sensitive gloss extension over hyponyms and hypernyms enhances WSD performance.  
Grammar-driven convolution tree kernel (GTK) has shown promising results for semantic role labeling (SRL). However, the time complexity of computing the GTK is exponential in theory. In order to speed up the computing process, we design two fast grammar-driven convolution tree kernel (FGTK) algorithms, which can compute the GTK in polynomial time. Experimental results on the CoNLL-2005 SRL data show that our two FGTK algorithms are much faster than the GTK. 
This paper proposes a ﬂexible matching method that can assimilate the expressive divergence. First, broad-coverage synonymous expressions are automatically extracted from an ordinary dictionary, and among them, those whose distributional similarity in a Web corpus is high are used for the ﬂexible matching. Then, to overcome the combinatorial explosion problem in the combination of expressive divergence, an ID is assigned to each synonymous group, and SYNGRAPH data structure is introduced to pack the expressive divergence. We conﬁrmed the effectiveness of our method on experiments of machine translation and information retrieval. 
In this article we want to demonstrate that annotation of multiword expressions in the Prague Dependency Treebank is a well deﬁned task, that it is useful as well as feasible, and that we can achieve good consistency of such annotations in terms of inter-annotator agreement. We show a way to measure agreement for this type of annotation. We also argue that some automatic pre-annotation is possible and it does not damage the results. 
Lexical mismatch is a problem that confounds automatic question answering systems. While existing lexical ontologies such as WordNet have been successfully used to match verbal synonyms (e.g., beat and defeat) and common nouns (tennis is-a sport), their coverage of proper nouns is less extensive. Question answering depends substantially on processing named entities, and thus it would be of signiﬁcant beneﬁt if lexical ontologies could be enhanced with additional hypernymic (i.e., is-a) relations that include proper nouns, such as Edward Teach is-a pirate. We demonstrate how a recently developed statistical approach to mining such relations can be tailored to identify named entity hyponyms, and how as a result, superior question answering performance can be obtained. We ranked candidate hyponyms on 75 categories of named entities and attained 53% mean average precision. On TREC QA data our method produces a 9% improvement in performance. 
In this paper, we present results of our experiments with ASR for a highly inﬂected Dravidian language, Telugu. First, we propose a new metric for evaluating ASR performance for inﬂectional languages (Inﬂectional Word Error Rate IWER) which takes into account whether the incorrectly recognized word corresponds to the same lexicon lemma or not. We also present results achieved by applying a novel method – errgrams – to ASR lattice. With respect to conﬁdence scores, the method tries to learn typical error patterns, which are then used for lattice correction, and applied just before standard lattice rescoring. Our conﬁdence measures are based on word posteriors and were improved by applying antimodels trained on anti-examples generated by the standard N-gram language model. For Telugu language, we decreased the WER from 45.2% to 40.4% (by 4.8% absolute), and the IWER from 41.6% to 39.5% (2.1 % absolute), with respect to the baseline performance. All improvements are statistically signiﬁcant using all three standard NIST signiﬁcance tests for ASR. 
Segmental SNR (Signal to Noise Ratio) is considered to be a reasonable measure of perceptual quality of speech. However it only reflects the distortion in time dependent contour of the signal due to noise. Objective Measures such as Log Area Ratio (LAR), Itakura-Saitio Distortion (IS), Log-Likelihood Ratio (LLR) and Weighted Spectral Slope (WSS) are better measures of perceptual speech quality as they represent deviation in the spectrum. Noise affects the speech time contour and the corresponding frequency content. Different languages have some peculiar characteristics due to variation in the phonetic content and their distribution. Distortion introduced by noise and application of enhancement algorithm varies for different phonemes. In this paper a novel idea of using noise and speech enhancement as means of identifying a language is presented, using objective measures of speech quality. Study is done on three spoken Indian regional languages namely Kashmiri, Bangla and Manipuri, when corrupted by white noise. It is found that the objective measures of noisy speech, when determined using corresponding clear and enhanced speech are different for different languages over a range of SNR, giving clue to the type of the language in use. 1. Introduction Speech is a signal which easily gets corrupted as it comes in contact with the environment.  Except in sound-proof rooms used in studios, it is not possible to find such ideal noise free conditions in practice. Although a large number of noises exist in environment, broadly they can be classified into Factory, Babble, Engine, White and Channel noises etc. However most common kind of noise encountered is white noise, may it be in communication systems due to channel or generated in the equipment due to thermal or other electronic sources or combination of noises due to Central Limit Theorem (Aleksandr Lyapunov, 1901). Noise thus corrupts the speech, causing listener’s fatigue and deteriorating performance of speech systems. Application of Speech enhancement or noise cancellation algorithms alleviates such problems to some extent. In literature several speech enhancement techniques exist. Though most traditional algorithms are based on optimizing mathematical criteria, they are not well correlated with speech perception and have not been as successful in preserving or improving quality in all regions of speech, especially transitional and unvoiced. Performance is also influenced by the specific type of noise, specific SNR, noise estimate updates and algorithm parameter settings. Spectral Subtraction technique of speech enhancement is popular and is still widely used as front end to speech systems for its simplistic nature and high quality performance except at very low SNRs (J. Lim, 1983). Variety of languages exists in Indian region, with Dravidian, Tibeto-Burman, Indo-European, Indo-Aryan and Indo Iranian background. Mostly Indian languages are phonetic in nature that is there is one to one correspondence between sounds and the representative alphabet, and combining them creates similar kind of sounds. However different languages vary in its perceptibility due to  811  differences in its phonetic contents and variations in distribution of different phonemes, stress level distribution among phonemes and of course intonation pattern, nasality usage, allophonic variants, contextual, phonotactic, or coarticulatory constraints etc. Introduction of noise in speech distorts the speech spectrum and affects its phonetic perceptibility differently, due to the factors mentioned above. Enhancement of noisy speech though reduces the noise and subsequent irritation, but generally results in distortion of the speech spectrum. The kind and amount of distortion in the spectrum of enhanced speech will depend on the particular enhancement technique applied, and the SNR of the noisy speech. Therefore different types of speech units will get affected differently by the noise and subsequent enhancement. In this paper, a novel work on identification of spoken languages, based on effect of distortion introduced by white noise in the phonetic contents of different Indian Regional languages namely Kashmiri, Bangla and Manipuri is reported. This kind of approach is not found in the literature for any other language as well. Effect of Speech enhancement technique namely spectral subtraction on noisy speech of these languages is also studied at different levels of segmental SNR. White noise has been considered for noisy spoken language, as it affects all frequency components of speech uniformly. The distortion introduced in the resulting speech is measured by estimating objective measures of perceptual speech quality such as LLR, LAR, IS and WSS (Hansen and Pellom, 1998). The variation of these estimated objective measures of the spectral distortion, with regard to a particular language, is studied and analyzed, to see language specific effects of the noise and enhancement algorithm, in order to provide clue to the identity of language in use. The paper has been organized in the following form: Section 2 gives details of Spectral Subtraction technique of enhancement used. Section 3 gives a comparative study of phonotactics of the three languages i.e. Kashmiri, Bangla and Manipuri in brief. Section 4 introduces the objective measures used, namely LAR, IS,  LLR and WSS. Section 5 describes the Results and discussion. Section 6 gives conclusions.  2. Spectral Subtraction  This technique of speech enhancement is  computationally very efficient, particularly for  stationary noise or slowly varying non-stationary  noise. Spectral subtraction is a noise suppression  technique used to reduce the effects of added noise  in speech. It estimates the power of clean speech  by explicitly subtracting the estimated noise power  from the noisy speech power. This of course  assumes that the noise and speech are uncorrelated  and additive in the time domain. Also, as spectral  subtraction based  techniques necessitate  estimation of noise during regions of non-speech  activity, it is supposed that noise characteristics  change slowly. However, because noise is  estimated during speech pauses, this makes the  method computationally efficient. Unfortunately,  for these reasons, spectral subtraction is beset by a  number of problems. First, because noise is  estimated during pauses the performance of a  spectral subtraction system relies upon a robust  noise/speech classification system. If a  misclassification occurs this may result in a  misestimating of the noise model and thus a  degradation of the speech estimate. Spectral  subtraction may also result in negative power  spectrum values, which are then reset to non-  negative values. This results in residual noise  known as musical noise. In a speech enhancement  application it has been shown that, at 5 dB SNR,  the quality of the speech signal is improved  without decreasing intelligibility. However, at  lower SNR speech this performance reduces  rapidly. When used in Automatic Speech  Recognition (ASR), the trade-off between SNR  improvement and spectral distortion is important.  To provide a mathematical description of the  spectral subtraction technique, we write the  spectrum of the noisy speech y (t) in terms of that  of the clean speech x (t) and additive noise n (t)  (the simplest acoustic distortion model):  y (t) = x (t) + n (t)  - (1)  The enhancement is explained in the following formula (Berouti et al., 1979).  812  ( ) Xˆ  w  =  ⎡ ⎢  Y  (w  )  λ  −α  ∧ N  (w )  γ  ⎤ ⎥  1γ  e  jθ y ( w )  ⎢⎣  ⎥⎦  - (2)  Xˆ (w ) and Y (w ) are DFT (discrete fourier transform) of the enhanced and noisy signal. N (w) is estimate of noise and θy phase of original signal. λ is 2 for working in power spectrum domain and α is the over subtraction factor.  3. Characteristics of Manipuri, Bangla and Kashmiri spoken languages  Different Indian regional languages have certain linguistic background of their own and later have added certain foreign loan words. Their phonotactics and grammar is also quite distinct Following are features of above spoken languages: Manipuri: It is a Tibeto-Burman language. Tone is used to convey phonemic distinction. Aspirates are present. High frequency of the velar nasal is particularly striking. Grammatical gender is missing. The normal order of words in a sentence is SOV-subject, object, verb, though this is not always and everywhere rigorously observed. Tibeto-Burman words are monosyllables. Phonological system of Manipuri can be categorized into two groups – segmental phonemes and supra-segmental phonemes. Segmental phoneme includes vowels and consonants and supra-segmental phoneme includes tone and juncture. All the six Manipuri vowels can occur in initial, medial and final position. There are six diphthong like sounds in Manipuri. They are - ( /əy/,/ay/, /əw/ ,/oy/, /uy/, /aw/) There are 24 consonant phonemes in Manipuri p,t,k, ph,th,kh,m, n,ŋ,c,s,l, h,w,y,b d,g,bh, dh,gh,j, jh,r . Among these the last 9 voiced sounds are borrowed from other languages and they cannot occur in the initial and final position. Only four phonemes can occur in the second element of the cluster. They are w, y, r and l. It can occur only in the initial and medial position of a word. There are two types of tone in the language level and falling tone. Juncture, other than phonetic features, has a phonemic status.  Bangla: An Indo-Aryan language. Standard colloquial Bengali contains 35 essential phonemes. 5 non-essential phonemes which occur only as variants of other sounds or in borrowed foreign words & not used by all speakers. The ten aspirated stops and affricates are characteristics and essential sounds of the language. They are not simple but compounds. Seven vowel phonemes occur with their opposite nasal phoneme. All may be long or short. Length is not considered to be phonemic. There is one 1st person pronoun, three 2nd person pronouns and three pairs of 3rd person pronouns with polite, informal, singular, plural discrimination. Pronoun and verb have no gender discriminatory word. Most of the sentences don’t explicitly use verbs. Verbs are inflected in person (1st, 2nd, 3rd), in degrees of politeness (intimate, familiar, respectful), and in tense (past, present, future). Plural can be inflected by adding suffix – ra, -der, -era, -diger, -guli, -gulo, -gana. The dominant word order in Modern Bengali sentences is: Subject + Indirect object + Direct object + Oblique object + Verb. Kashmiri: All the vowels have a nasal counterpart. Nasalization is phonemic in Kashmiri. Palatalization is phonemic in Kashmiri. All the non-palatal consonants in Kashmiri can be palatalized. There are eight pairs of short and long vowels. Kashmiri is a syllable-timed language, sometimes; individual words are stressed for emphasis. There are four major types of intonational patterns: (1) High - fall, (2) High rise, (3) Rise &fall, (4) Mid - level. Intonations have syntactic rather than emotional content. Vowels /ə/, /o/, /ɔ:/ do not occur in the word final position. The short vowels /ɨ/, /e/, /u/, and / ɔ/ do not occur in the word-initial position. Usually the semi-vowel /y/ is added in the initial position of the words beginning with /i/, /i:/, /e/ and /e:/. Similarly, the semi-vowel /v/ is added to the words beginning with /u/, and /u:/. Vowel sequences usually do not occur in Kashmiri. Word initial consonant clusters are not as frequent as the word medial consonant clusters. Kashmiri has (C)(C)V(C)(C) syllable structure.  813  4. Objective methods of speech quality measure  In general speech enhancement or noise reduction is measured in terms of improvement in SNR, but  in reality, this may not be the most appropriate performance criteria for improvement of perceptual speech quality. Humans do have an intuitive understanding of spoken language quality, however this may not be easy to quantify. In a number of studies, it has been shown that impact of noise on degradation of speech quality is non uniform. An objective speech quality measure shows, the level of distortion for each frame, across time. Since speech frequency content varies, across time, due to sequence of phonemes, needed to produce the sentence, impact of background distortion will also vary, causing some phone classes to get more effected than others, when produced in a noisy environment. Objective methods rely on mathematically based measure between reference signal and the signal under consideration. The objective measures are based on different parametric representation of the speech, and differ due to inclusion or noninclusion of various parameters and the different weightage given to them, in order to imitate auditory model and perception as closely as possible. The details of each one is given below.  Itakura-Saitio Distortion Measure (IS): If for  an original clean frame of speech, linear prediction (LP) coefficient vector is  ar Φ  , correlation matrix is RΦ. And for processed speech LP coefficient vector is  ard  ,  correlation matrix is Rd , then Itakura-Satio  distortion measure is given by,  ( ) d IS ard  , arφ  =  ⎡σ ⎢⎢⎣σ  2 φ 2 d  ⎤ ⎥ ⎥⎦  ⎡ ⎢ ⎢⎣  aarrφd  Rφ Rφ  aarrφdTT  ⎤ ⎥+ ⎥⎦  log  ⎡σ ⎢⎢⎣σ  2 d 2 φ  ⎤ ⎥ ⎥⎦  −1  - (3)  Where  σ  2 d  and  σ  2 Φ  represents  the all-pole gains for  the processed and clean speech frame respectively.  Log-Likelihood Ratio Measure (LLR): The LLR measure is also referred to as the Itakura distance. The LLR measure is found as follows,  ( ) dLLR ard , arφ  =  ⎡ log⎢ ⎢⎣  aarrφd  Rφ Rφ  aarrφdTT  ⎤ ⎥ ⎥⎦  - (4)  Log-Area-Ratio Measure (LAR): The LAR measure is also based on dissimilarity of LP  coefficients between original and processed speech  signals. The log-area-ratio parameters are obtained from the pth order LP reflection coefficients for the original rΦ ( j ) and processed rd ( j ) signals for frame j. The objective measure is formed as  follows,  
 The problem of identifying good features for improving conventional language models like trigrams is presented as a classiﬁcation task in this paper. The idea is to use various syntactic and semantic features extracted from a language for classifying between real-world articles and articles generated by sampling a trigram language model. In doing so, a good accuracy obtained on the classiﬁcation task implies that the extracted features capture those aspects of the language that a trigram model may not. Such features can be used to improve the existing trigram language models. We describe the results of our experiments on the classiﬁcation task performed on a Broadcast News Corpus and discuss their effects on language modeling in general.  
Text categorization is a fundamental task in natural language processing, and is generally deﬁned as a multi-label categorization problem, where each text document is assigned to one or more categories. We focus on providing good statistical classiﬁers with a generalization ability for multi-label categorization and present a classiﬁer design method based on model combination and F1-score maximization. In our formulation, we ﬁrst design multiple models for binary classiﬁcation per category. Then, we combine these models to maximize the F1-score of a training dataset. Our experimental results conﬁrmed that our proposed method was useful especially for datasets where there were many combinations of category labels. 
Many real-world systems for handling unstructured text data are rule-based. Examples of such systems are named entity annotators, information extraction systems, and text classiﬁers. In each of these applications, ordering rules into a decision list is an important issue. In this paper, we assume that a set of rules is given and study the problem (MaxDL) of ordering them into an optimal decision list with respect to a given training set. We formalize this problem and show that it is NP-Hard and cannot be approximated within any reasonable factors. We then propose some heuristic algorithms and conduct exhaustive experiments to evaluate their performance. In our experiments we also observe performance improvement over an existing decision list learning algorithm, by merely re-ordering the rules output by it. 
Extracting semantic relations between entities from natural language text is an important step towards automatic knowledge extraction from large text collections and the Web. The state-of-the-art approach to relation extraction employs Support Vector Machines (SVM) and kernel methods for classiﬁcation. Despite the diversity of kernels and the near exhaustive trial-and-error on kernel combination, there lacks a clear understanding of how these kernels relate to each other and why some are superior than others. In this paper, we provide an analysis of the relative strength and weakness of several kernels through systematic experimentation. We show that relation extraction can beneﬁt from increasing the feature space through convolution kernel and introducing bias towards more syntactically meaningful feature space. Based on our analysis, we propose a new convolution dependency path kernel that combines the above two beneﬁts. Our experimental results on the standard ACE 2003 datasets demonstrate that our new kernel gives consistent and signiﬁcantly better performance than baseline methods, obtaining very competitive results to the state-ofthe-art performance. 
Duplicate document detection is the problem of ﬁnding all document-pairs rapidly whose similarities are equal to or greater than a given threshold. There is a method proposed recently called preﬁx-ﬁlter that ﬁnds document-pairs whose similarities never reach the threshold based on the number of uncommon terms (words/characters) in a document-pair and removes them before similarity calculation. However, preﬁx-ﬁlter cannot decrease the number of similarity calculations sufﬁciently because it leaves many document-pairs whose similarities are less than the threshold. In this paper, we propose multi-level preﬁx-ﬁlter, which reduces the number of similarity calculations more efﬁciently and maintains the advantage of preﬁx-ﬁlter (no detection loss, no extra parameter) by applying multiple different preﬁx-ﬁlters. 
Recently, NLP researches have advanced using F-scores, precisions, and recalls with gold standard data as evaluation measures. However, such evaluations cannot capture the different behaviors of varying NLP tools or the different behaviors of a NLP tool that depends on the data and domain in which it works. Because an increasing number of tools are available nowadays, it has become increasingly important to grasp these behavioral differences, in order to select a suitable set of tools, which forms a complex workflow for a specific purpose. In order to observe such differences, we need to integrate available combinations of tools into a workflow and to compare the combinatorial results. Although generic frameworks like UIMA (Unstructured Information Management Architecture) provide interoperability to solve this problem, the solution they provide is only partial. In order for truly interoperable toolkits to become a reality, we also need  sharable and comparable type systems with an automatic combinatorial comparison generator, which would allow systematic comparisons of available tools. In this paper, we describe such an environment, which we developed based on UIMA, and we show its feasibility through an example of a protein-protein interaction (PPI) extraction system. 
A person may have multiple name aliases on the Web. Identifying aliases of a name is important for various tasks such as information retrieval, sentiment analysis and name disambiguation. We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts. Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url. For a given personal name, its neighboring nodes in the graph are considered as candidates of its aliases. We formalize alias identiﬁcation as a problem of ranking nodes in this graph with respect to a given name. We integrate various ranking scores through support vector machines to leverage a robust ranking function and use it to extract aliases for a given name. Experimental results on a dataset of Japanese celebrities show that the proposed method outperforms all baselines, displaying a MRR score of 0.562. 
In this paper, we present an empirical study on adapting Conditional Random Fields (CRF) models to conduct semantic analysis on biomedical articles using active learning. We explore uncertaintybased active learning with the CRF model to dynamically select the most informative training examples. This abridges the power of the supervised methods and expensive human annotation cost. 
A critical, yet not very well studied problem in medical applications is the issue of accurately labeling patient records according to diagnoses and procedures that patients have undergone. This labeling problem, known as coding, consists of assigning standard medical codes (ICD9 and CPT) to patient records. Each patient record can have several corresponding labels/codes, many of which are correlated to speciﬁc diseases. The current, most frequent coding approach involves manual labeling, which requires considerable human effort and is cumbersome for large patient databases. In this paper we view medical coding as a multi-label classiﬁcation problem, where we treat each code as a label for patient records. Due to government regulations concerning patient medical data, previous studies in automatic coding have been quite limited. In this paper, we compare two efﬁcient algorithms for diagnosis coding on a large patient dataset. 
This paper describes a method for extracting a large set of hyponymy relations from Wikipedia. The Wikipedia is much more consistently structured than generic HTML documents, and we can extract a large number of hyponymy relations with simple methods. In this work, we managed to extract more than 1.4 × 106 hyponymy relations with 75.3% precision from the Japanese version of the Wikipedia. To the best of our knowledge, this is the largest machine-readable thesaurus for Japanese. The main contribution of this paper is a method for hyponymy acquisition from hierarchical layouts in Wikipedia. By using a machine learning technique and pattern matching, we were able to extract more than 6.3 × 105 relations from hierarchical layouts in the Japanese Wikipedia, and their precision was 76.4%. The remaining hyponymy relations were acquired by existing methods for extracting relations from deﬁnition sentences and category pages. This means that extraction from the hierarchical layouts almost doubled the number of relations extracted. 
This paper addresses the difﬁculties in recognizing Japanese abbreviations through the use of previous approaches, examining actual usages of parenthetical expressions in newspaper articles. In order to bridge the gap between Japanese abbreviations and their full forms, we present a discriminative approach to abbreviation recognition. More speciﬁcally, we formalize the abbreviation recognition task as a binary classiﬁcation problem in which a classiﬁer determines a positive (abbreviation) or negative (nonabbreviation) class, given a candidate of abbreviation deﬁnition. The proposed method achieved 95.7% accuracy, 90.0% precision, and 87.6% recall on the evaluation corpus containing 7,887 (1,430 abbreviations and 6,457 non-abbreviation) instances of parenthetical expressions. 
Several approaches have already been employed to “sense” affective information from text, but none of those ever considered the cognitive and appraisal structure of individual emotions. Hence this paper aims at interpreting the cognitive theory of emotions known as the OCC emotion model, from a linguistic standpoint. The paper provides rules for the OCC emotion types for the task of sensing affective information from text. Since the OCC emotions are associated with several cognitive variables, we explain how the values could be assigned to those by analyzing and processing natural language components. Empirical results indicate that our system outperforms another state-of-the-art system. 
In this paper, we propose a novel approach to optimally employing the MCL (Markov Cluster Algorithm) by “neutralizing” the trivial disadvantages acknowledged by its original proposer. Our BMCL (Branching Markov Clustering) algorithm makes it possible to subdivide a large core cluster into appropriately resized sub-graphs. Utilizing three corpora, we examine the effects of the BMCL which varies according to the curvature (clustering coefficient) of a hub in a network. 
Part-Of-Speech(POS) tagging is the essential basis of Natural language processing(NLP). In this paper, we present an algorithm that combines a variety of context features, e.g. the POS tags of the words next to the word a that needs to be tagged and the context lexical information of a by Canonical Belief Network to together determine the POS tag of a. Experiments on a Chinese corpus are conducted to compare our algorithm with the standard HMM-based POS tagging and the POS tagging software ICTCLAS3.0. The experimental results show that our algorithm is more effective. 
Many natural language applications, like machine translation and information extraction, are required to operate on text with spelling errors. Those spelling mistakes have to be corrected automatically to avoid deteriorating the performance of such applications. In this work, we introduce a novel approach for automatic correction of spelling mistakes by deploying ﬁnite state automata to propose candidates corrections within a speciﬁed edit distance from the misspelled word. After choosing candidate corrections, a language model is used to assign scores the candidate corrections and choose best correction in the given context. The proposed approach is language independent and requires only a dictionary and text data for building a language model. The approach have been tested on both Arabic and English text and achieved accuracy of 89%. 
Semantic Role Labeling (SRL) as a Shallow Semantic Parsing causes more and more attention recently. The shortage of manually tagged data is one of main obstacles to supervised learning, which is even serious in SRL. Transductive SVM (TSVM) is a novel semi-supervised learning method special to small mount of tagged data. In this paper, we introduce an application of TSVM in Chinese SRL. To improve the performance of TSVM, some heuristics have been designed from the semantic perspective. The experiment results on Chinese Propbank showed that TSVM outperforms SVM in small tagged data, and after using heuristics, it performs further better. 
In this article we address the task of automatic text structuring into linear and nonoverlapping thematic episodes at a coarse level of granularity. In particular, we deal with topic segmentation on multi-party meeting recording transcripts, which pose speciﬁc challenges for topic segmentation models. We present a comparative study of two probabilistic mixture models. Based on lexical features, we use these models in parallel in order to generate a low dimensional input representation for topic segmentation. Our experiments demonstrate that in this manner important information is captured from the data through less features. 
This paper exploits unlabeled text data to improve new word identiﬁcation and Chinese word segmentation performance. Our contributions are twofold. First, for new words that lack semantic transparency, such as person, location, or transliteration names, we calculate association metrics of adjacent character segments on unlabeled data and encode this information as features. Second, we construct an internal dictionary by using an initial model to extract words from both the unlabeled training and test set to maintain balanced coverage on the training and test set. In comparison to the baseline model which only uses n-gram features, our approach increases new word recall up to 6.0%. Additionally, our approaches reduce segmentation errors up to 32.3%. Our system achieves state-of-the-art performance for both the closed and open tasks of the 2006 SIGHAN bakeoﬀ. 
Human language with all its intricacies is perhaps one of the ﬁnest examples of a complex system. Therefore, it becomes absolutely necessary to study the faculty of language from the perspective of a complex system. Of late, there has been an upsurge in the use of networks in modeling the complex dynamics of various natural and artiﬁcial systems. While some of these works aim at using social network techniques to build certain enduser applications, others are more fundamental in the sense that they employ these techniques to explain the emergent properties of a complex system as a whole. A substantial amount of research have also been done in the ﬁeld of linguistics to employ social networks in the design of efﬁcient solutions for numerous problems in NLP and language evolution. The objective of this tutorial is to show how language and its dynamics can be successfully studied in the framework of social networks. The tutorial will particularly demonstrate the relevance of social network-based methods in the development of a large variety of NLP applications and in understanding the dynamics of language evolution and change. The tutorial is divided into two parts. Part I begins with a brief introduction to this ﬁeld showing how linguistic entities and the interactions between them can be respectively represented through the nodes and edges of a network. This will be followed by a comprehensive survey of the general theory of social networks with a special emphasis on the methods of analysis and models of synthesis for such networks. Part II presents three case studies. The ﬁrst case study is on unsupervised POS tagging, the second one involves modeling of the mental lexicon and ap-  
Those of us whose mother tongue is not English or are curious about applications involving other languages, often ﬁnd ourselves in the situation where the tools we require are not available. According to recent studies there are about 7200 different languages spoken worldwide – without including variations or dialects – out of which very few have automatic language processing tools and machine readable resources. 
In recent years we have witnessed an explosion of on-line unstructured information in multiple languages, making natural language processing technologies such as automatic text summarization increasingly important for the information society. Text Summarization provides users with condensed descriptions of documents, allowing them to make informed decisions based on text summaries. Text summarization can be combined with Information Retrieval (IR) and Question Answering (QA) to provide users with focus-based or query-based summaries which are targeted towards the users’ speciﬁc needs. When the information a user looks for is spread across multiple sources, text summarization can be used to condense facts and present a nonredundant account of the most relevant facts found across a set of documents. The objective of this IJCNLP 2008 tutorial is to give an overview of a number of technologies in natural language processing for information access including: single and multidocument summarization, cross-lingual summarization; and summarization in the context of question answering. The tutorial will discuss summarization concepts and techniques as well as its relation and relevance to other technologies such as information retrieval and question answering. It will also include description of available resources for development, training and evaluation of summarization components. A summarization (multidocument and multilingual) toolkit will be used for demonstration purposes. A number of ques- 
This article provides description about the grammar checking software developed for detecting the grammatical errors in Punjabi texts and providing suggestions wherever appropriate to rectify those errors. This system utilizes a full-form lexicon for morphology analysis and rule-based systems for part of speech tagging and phrase chunking. The system supported by a set of carefully devised error detection rules can detect and suggest rectifications for a number of grammatical errors, resulting from lack of agreement, order of words in various phrases etc., in literary style Punjabi texts. 
We present the Global Health Monitor, an online Web-based system for detecting and mapping infectious disease outbreaks that appear in news stories. The system analyzes English news stories from news feed providers, classifies them for topical relevance and plots them onto a Google map using geo-coding information, helping public health workers to monitor the spread of diseases in a geo-temporal context. The background knowledge for the system is contained in the BioCaster ontology (BCO) (Collier et al., 2007a) which includes both information on infectious diseases as well as geographical locations with their latitudes/longitudes. The system consists of four main stages: topic classification, named entity recognition (NER), disease/location detection and visualization. Evaluation of the system shows that it achieved high accuracy on a gold standard corpus. The system is now in practical use. Running on a clustercomputer, it monitors more than 1500 news feeds 24/7, updating the map every hour. 
Many languages of the world (some with very large numbers of native speakers) are not yet supported on computers. In this paper we ﬁrst present a simple method to provide an extra layer of easily customizable language-encoding support for less computerized languages. We then describe an editor called Sanchay Editor, which uses this method and also has many other facilities useful for those using less computerized languages for simple text editing or for Natural Language Processing purposes, especially for annotation. 
This paper brings together the practical applications and the evaluation of the first Text-to-Speech (TTS) system for Sinhala using the Festival framework and an Optical Character Recognition system for Sinhala. 
We present a demo of our conversational system POLLy (POliteness in Language Learning) which uses a common planning representation to generate actions to be performed by embodied agents in a virtual environment and to generate spoken utterances for dialogues about the steps involved in completing the task. In order to generate socially appropriate dialogue, Brown and Levinson’s theory of politeness is used to constrain the dialogue generation process.  collaborative VE which accepts spoken input from the user and enables him or her to navigate within the VE. They use a ‘reference resolver’ which maps the entities mentioned in utterances to geometric objects in the VE and to actions.  
In Modern Mongolian, a content word can be inflected when concatenated with suffixes. Identifying the original forms of content words is crucial for natural language processing and information retrieval. We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval. We use technical abstracts to show the effectiveness of our method experimentally. 
This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a uniﬁed framework. Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to ﬁnd the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores. Experiments show that description length gain outperforms other measures because of its strength for identifying short words. Further performance improvement is also reported, achieved by proper candidate pruning and by assemble segmentation to integrate the strengths of individual measures. 
We present a technique for reﬁning a baseline segmentation and generating a plausible underlying morpheme segmentation by integrating hand-written rewrite rules into an existing state-of-the-art unsupervised morphological induction procedure. Performance on measures which consider surface-boundary accuracy and underlying morpheme consistency indicates this technique leads to improvements over baseline segmentations for English and Turkish word lists. 
This paper proposes a context-sensitive convolution tree kernel for pronoun resolution. It resolves two critical problems in previous researches in two ways. First, given a parse tree and a pair of an anaphor and an antecedent candidate, it implements a dynamic-expansion scheme to automatically determine a proper tree span for pronoun resolution by taking predicate- and antecedent competitor-related information into consideration. Second, it applies a context-sensitive convolution tree kernel, which enumerates both context-free and context-sensitive sub-trees by considering their ancestor node paths as their contexts. Evaluation on the ACE 2003 corpus shows that our dynamic-expansion tree span scheme can well cover necessary structured information in the parse tree for pronoun resolution and the context-sensitive tree kernel much outperforms previous tree kernels. 
This paper proposes a semi-supervised learning method for relation extraction. Given a small amount of labeled data and a large amount of unlabeled data, it first bootstraps a moderate number of weighted support vectors via SVM through a co-training procedure with random feature projection and then applies a label propagation (LP) algorithm via the bootstrapped support vectors. Evaluation on the ACE RDC 2003 corpus shows that our method outperforms the normal LP algorithm via all the available labeled data without SVM bootstrapping. Moreover, our method can largely reduce the computational burden. This suggests that our proposed method can integrate the advantages of both SVM bootstrapping and label propagation. 
Topic Detection and Tracking refers to automatic techniques for locating topically related materials in streams of data. As the core technology of it, story link detection is to determine whether two stories are about the same topic. To overcome the limitation of the story length and the topic dynamic evolution problem in data streams, this paper presents a method of applying dynamic information extending to improve the performance of link detection. The proposed method uses previous latest related story to extend current processing story, generates new dynamic models for computing the similarity between the current two stories. The work is evaluated on the TDT4 Chinese corpus, and the experimental results indicate that story link detection using this method can make much better performance on all evaluation metrics. 
Orthographic variance is a fundamental problem for many natural language processing applications. The Japanese language, in particular, contains many orthographic variants for two main reasons: (1) transliterated words allow many possible spelling variations, and (2) many characters in Japanese nouns can be omitted or substituted. Previous studies have mainly focused on the former problem; in contrast, this study has addressed both problems using the same framework. First, we automatically collected both positive examples (sets of equivalent term pairs) and negative examples (sets of inequivalent term pairs). Then, by using both sets of examples, a support vector machine based classiﬁer determined whether two terms (t1 and t2) were equivalent. To boost accuracy, we added a transliterated probability P (t1|s)P (t2|s), which is the probability that both terms (t1 and t2) were transliterated from the same source term (s), to the machine learning features. Experimental results yielded high levels of accuracy, demonstrating the feasibility of the proposed approach. 
Name origin recognition is to identify the source language of a personal or location name. Some early work used either rulebased or statistical methods with single knowledge source. In this paper, we cast the name origin recognition as a multi-class classification problem and approach the problem using Maximum Entropy method. In doing so, we investigate the use of different features, including phonetic rules, ngram statistics and character position information for name origin recognition. Experiments on a publicly available personal name database show that the proposed approach achieves an overall accuracy of 98.44% for names written in English and 98.10% for names written in Chinese, which are significantly and consistently better than those in reported work. 
Transliteration is the process of transcribing words from a source script to a target script. These words can be content words or proper nouns. They may be of local or foreign origin. In this paper we present a more discerning method which applies different techniques based on the word origin. The techniques used also take into account the properties of the scripts. Our approach does not require training data on the target side, while it uses more sophisticated techniques on the source side. Fuzzy string matching is used to compensate for lack of training on the target side. We have evaluated on two Indian languages and have achieved substantially better results (increase of up to 0.44 in MRR) than the baseline and comparable to the state of the art. Our experiments clearly show that word origin is an important factor in achieving higher accuracy in transliteration. 
In this paper, we propose an architecture, called UCSG Shallow Parsing Architecture, for building wide coverage shallow parsers by using a judicious combination of linguistic and statistical techniques without need for large amount of parsed training corpus to start with. We only need a large POS tagged corpus. A parsed corpus can be developed using the architecture with minimal manual eﬀort, and such a corpus can be used for evaluation as also for performance improvement. The UCSG architecture is designed to be extended into a full parsing system but the current work is limited to chunking and obtaining appropriate chunk sequences for a given sentence. In the UCSG architecture, a Finite State Grammar is designed to accept all possible chunks, referred to as word groups here. A separate statistical component, encoded in HMMs (Hidden Markov Model), has been used to rate and rank the word groups so produced. Note that we are not pruning, we are only rating and ranking the word groups already obtained. Then we use a Best First Search strategy to produce parse outputs in best ﬁrst order, without compromising on the ability to produce all possible parses in principle. We propose a bootstrapping strategy for improving HMM parameters and hence the performance of the parser as a whole. A wide coverage shallow parser has been implemented for English starting from the British National Corpus, a nearly 100 Million word POS tagged corpus. Note that the corpus is not a parsed corpus. Also, there are tagging errors, multiple tags assigned in many cases, and some words have not been  tagged. A dictionary of 138,000 words with frequency counts for each word in each tag has been built. Extensive experiments have been carried out to evaluate the performance of the various modules. We work with large data sets and performance obtained is encouraging. A manually checked parsed corpus of 4000 sentences has also been developed and used to improve the parsing performance further. The entire system has been implemented in Perl under Linux. Key Words:- Chunking, Shallow Parsing, Finite State Grammar, HMM, Best First Search 
This paper presents a generalized framework of syntax-based gap resolution in analytic language translation using an extended version of categorial grammar. Translating analytic languages into Indo-European languages suffers the issues of gapping, because “deletion under coordination” and “verb serialization” are necessary to be resolved beforehand. Rudimentary operations, i.e. antecedent memorization, gap induction, and gap resolution, were introduced to the categorial grammar to resolve gapping issues syntactically. Hereby, pronominal references can be generated for deletion under coordination, while sentence structures can be properly selected for verb serialization. 
This paper presents an effective dependency parsing approach of incorporating short dependency information from unlabeled data. The unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words. We then train another parser which uses the information on short dependency relations extracted from the output of the ﬁrst parser. Our proposed approach achieves an unlabeled attachment score of 86.52, an absolute 1.24% improvement over the baseline system on the data set of Chinese Treebank. 
The automatic compilation of bilingual dictionaries from comparable corpora has been successful for single-word terms (SWTs), but remains disappointing for multi-word terms (MWTs). One of the main problems is the insufﬁcient coverage of the bilingual dictionary. Using the compositional translation method improved the results, but still shows some limits for MWTs of different syntactic structures. In this paper, we propose to bridge the gap between syntactic structures through morphological links. The results show a signiﬁcant improvement in the compositional translation of MWTs that demonstrate the efﬁciency of the morphologically based-method for lexical alignment. 
Most research related to unithood were conducted as part of a larger effort for the determination of termhood. Consequently, novelties are rare in this small sub-ﬁeld of term extraction. In addition, existing work were mostly empirically motivated and derived. We propose a new probabilistically-derived measure, independent of any inﬂuences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from Google search engine for the measurement of unithood. Our comparative study using 1, 825 test cases against an existing empiricallyderived function revealed an improvement in terms of precision, recall and accuracy. 
Document clustering and classiﬁcation is usually done by representing the documents using a bag of words scheme. This scheme ignores many of the linguistic and semantic features contained in text documents. We propose here an alternative representation for documents using Lexical Chains. We compare the performance of the new representation against the old one on a clustering task. We show that Lexical Chain based features give better results than the Bag of Words based features, while achieving almost 30% reduction in the dimensionality of the feature vectors resulting in faster execution of the algorithms. 
In this paper we explore the beneﬁts from and shortcomings of entity-driven noun phrase rewriting for multi-document summarization of news. The approach leads to 20% to 50% different content in the summary in comparison to an extractive summary produced using the same underlying approach, showing the promise the technique has to offer. In addition, summaries produced using entity-driven rewrite have higher linguistic quality than a comparison non-extractive system. Some improvement is also seen in content selection over extractive summarization as measured by pyramid method evaluation. 
In this paper we propose a new approach based on Sequence Segmentation Models (SSM) to the extractive document summarization, in which summarizing is regarded as a segment labeling problem. Comparing with the previous work, the difference of our approach is that the employed features are obtained not only from the sentence level, but also from the segment level. In our approach, the semi-Markov CRF model is employed for segment labeling. The preliminary experiments have shown that the approach does outperform all other traditional supervised and unsupervised approaches to document summarization. 
This paper presents a strategy to generate generic summary of documents using Probabilistic Latent Semantic Indexing. Generally a document contains several topics rather than a single one. Summaries created by human beings tend to cover several topics to give the readers an overall idea about the original document. Hence we can expect that a summary containing sentences from better part of the topic spectrum should make a better summary. PLSI has proven to be an effective method in topic detection. In this paper we present a method for creating extractive summary of the document by using PLSI to analyze the features of document such as term frequency and graph structure. We also show our results, which was evaluated using ROUGE, and compare the results with other techniques, proposed in the past. 
A pair of sentences in different newspaper articles on an event can have one of several relations. Of these, we have focused on two, i.e., equivalence and transition. Equivalence is the relation between two sentences that have the same information on an event. Transition is the relation between two sentences that have the same information except for values of numeric attributes. We propose methods of identifying these relations. We ﬁrst split a dataset consisting of pairs of sentences into clusters according to their similarities, and then construct a classiﬁer for each cluster to identify equivalence relations. We also adopt a “coarse-to-ﬁne” approach. We further propose using the identiﬁed equivalence relations to address the task of identifying transition relations. 
We describe a graph-based approach to Scenario Template Creation, which is the task of creating a representation of multiple related events, such as reports of different hurricane incidents. We argue that context is valuable to identify important, semantically similar text spans from which template slots could be generalized. To leverage context, we represent the input as a set of graphs where predicate-argument tuples are vertices and their contextual relations are edges. A context-sensitive clustering framework is then applied to obtain meaningful tuple clusters by examining their intrinsic and extrinsic similarities. The clustering framework uses Expectation Maximization to guide the clustering process. Experiments show that: 1) our approach generates high quality clusters, and 2) information extracted from the clusters is adequate to build high coverage templates. 
With the popularity of the Internet at a phenomenal rate, an ever-increasing number of documents in languages other than English are available in the Internet. Cross language text categorization has attracted more and more attention for the organization of these heterogeneous document collections. In this paper, we focus on how to conduct effective cross language text categorization. To this end, we propose a cross language naive Bayes algorithm. The preliminary experiments on collected document collections show the effectiveness of the proposed method and verify the feasibility of achieving performance close to monolingual text categorization, using a bilingual lexicon alone. Also, our algorithm is more efﬁcient than our baselines. 
The web is growing at a rapid speed and it is almost impossible for a web crawler to download all new pages. Pages reporting breaking news should be stored into search engine index as soon as they are published, while others whose content is not time-related can be left for later crawls. We collected and analyzed into users’ page-view data of 75,112,357 pages for 60 days. Using this data, we found that a large proportion of temporal pages are published by a small number of web sites providing news services, which should be crawled repeatedly with small intervals. Such temporal web sites of high freshness requirements can be identified by our algorithm based on user behavior analysis in page view data. 51.6% of all temporal pages can be picked up with a small overhead of untemporal pages. With this method, web crawlers can focus on these web sites and download pages from them with high priority. 
In Cross Language Information Retrieval (CLIR), query terms can be translated to the document language using Bilingual Dictionaries (BDs) or Statistical Translation Models (STMs). Combining different translation resources can also be used to improve the performance. Unfortunately, the most studies on combining multiple resources use simple methods such as linear combination. In this paper, we drew up a comparative study between linear combination and confidence measures to combine multiple translation resources for the purpose of CLIR. We show that the linear combination method is unable to combine correctly different types of resources such as BDs and STMs. While the confidence measure method is able to re-weight the translation candidate more radically than in linear combination. It reconsiders each translation candidate proposed by different resources with respect to additional features. We tested the two methods on different test CLIR collections and the results show that the confidence measure outperforms the linear combination method. 
As the amount of information created by human beings is explosively grown in the last decade, it is getting extremely harder to obtain necessary information by conventional information access methods. Hence, creation of drastically new technology is needed. For developing such new technology, search engine infrastructures are required. Although the existing search engine APIs can be regarded as such infrastructures, these APIs have several restrictions such as a limit on the number of API calls. To help the development of new technology, we are running an open search engine infrastructure, TSUBAKI, on a high-performance computing environment. In this paper, we describe TSUBAKI infrastructure. 
To relax the Term Independence Assumption, Term Dependency is introduced and it has improved retrieval precision dramatically. There are two kinds of term dependencies, one is defined by term proximity, and the other is defined by linguistic dependencies. In this paper, we take a comparative study to re-examine these two kinds of term dependencies in dependence language model framework. Syntactic relationships, derived from a dependency parser, Minipar, are used as linguistic term dependencies. Our study shows: 1) Linguistic dependencies get a better result than term proximity. 2) Dependence retrieval model achieves more improvement in sentence-based verbose queries than keywordbased short queries. 
Automatic estimation of word signiﬁcance oriented for speech-based Information Retrieval (IR) is addressed. Since the signiﬁcance of words differs in IR, automatic speech recognition (ASR) performance has been evaluated based on weighted word error rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly. A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown, and the reduction of errors on both ASR and IR has been reported. In this paper, we propose an automatic estimation method for word signiﬁcance (weights) based on its inﬂuence on IR. Speciﬁcally, weights are estimated so that evaluation measures of ASR and IR are equivalent. We apply the proposed method to a speech-based information retrieval system, which is a typical IR system, and show that the method works well. 
 Language understanding (LU) modules for spoken dialogue systems in the early phases of their development need to be (i) easy to construct and (ii) robust against various expressions. Conventional methods of LU are not suitable for new domains, because they take a great deal of effort to make rules or transcribe and annotate a sufﬁcient corpus for training. In our method, the weightings of the Weighted Finite State Transducer (WFST) are designed on two levels and simpler than those for conventional WFST-based methods. Therefore, our method needs much fewer training data, which enables rapid prototyping of LU modules. We evaluated our method in two different domains. The results revealed that our method outperformed baseline methods with less than one hundred utterances as training data, which can be reasonably prepared for new domains. This shows that our method is appropriate for rapid prototyping of LU modules. 
Many acoustic approaches to prosodic labeling in English have employed only local classiﬁers, although text-based classiﬁcation has employed some sequential models. In this paper we employ linear chain and factorial conditional random ﬁelds (CRFs) in conjunction with rich, contextually-based prosodic features, to exploit sequential dependencies and to facilitate integration with lexical features. Integration of lexical and prosodic features improves pitch accent prediction over either feature set alone, and for lower accuracy feature sets, factorial CRF models can improve over linear chain based prediction of pitch accent. 
We propose a general approach for translating Chinese unknown words (UNK) for SMT. This approach takes advantage of the properties of Chinese word composition rules, i.e., all Chinese words are formed by sequential characters. According to the proposed approach, the unknown word is re-split into a subword sequence followed by subword translation with a subwordbased translation model. “Subword” is a unit between character and long word. We found the proposed approach signiﬁcantly improved translation quality on the test data of NIST MT04 and MT05. We also found that the translation quality was further improved if we applied named entity translation to translate parts of unknown words before using the subword-based translation. 
We propose a new method of selecting hypotheses for machine transliteration. We generate a set of Chinese, Japanese, and Korean transliteration hypotheses for a given English word. We then use the set of transliteration hypotheses as a guide to ﬁnding relevant Web pages and mining contextual information for the transliteration hypotheses from the Web page. Finally, we use the mined information for machine-learning algorithms including support vector machines and maximum entropy model designed to select the correct transliteration hypothesis. In our experiments, our proposed method based on Web mining consistently outperformed systems based on simple Web counts used in previous work, regardless of the language. 
In human translation, translators ﬁrst make draft translations and then modify them. This paper analyses these modiﬁcations, in order to identify the features that trigger modiﬁcation. Our goal is to construct a system that notiﬁes (English-to-Japanese) volunteer translators of awkward translations. After manually classifying the basic modiﬁcation patterns, we analysed the factors that trigger a change in verb voice from passive to active using SVM. An experimental result shows good prospects for the automatic identiﬁcation of candidates for modiﬁcation. 
We analyze the linguistic behaviour of participants in bilateral electronic negotiations, and discover that particular language characteristics are in contrast with face-to-face negotiations. Language patterns in the later part of electronic negotiation are highly indicative of the successful or unsuccessful outcome of the process, whereas in face-toface negotiations, the ﬁrst part of the negotiation is more useful for predicting the outcome. We formulate our problem in terms of text classiﬁcation on negotiation segments of different sizes. The data are represented by a variety of linguistic features that capture the gist of the discussion: negotiationor strategy-related words. We show that, as we consider ever smaller ﬁnal segments of a negotiation transcript, the negotiationrelated words become more indicative of the negotiation outcome, and give predictions with higher Accuracy than larger segments from the beginning of the process. 
An approach to solving the problem of automatic brieﬁng generation from non-textual events can be segmenting the task into two major steps, namely, extraction of brieﬁng templates and learning aggregators that collate information from events and automatically ﬁll up the templates. In this paper, we describe two novel unsupervised approaches for extracting brieﬁng templates from human written reports. Since the problem is non-standard, we deﬁne our own criteria for evaluating the approaches and demonstrate that both approaches are effective in extracting domain relevant templates with promising accuracies. 
Searching and reading the Web is one of the principal methods used to seek out information to resolve problems about technology in general and digital devices in particular. This paper addresses the problem of text mining in the digital devices domain. In particular, we address the task of detecting semantic relations between digital devices in the text of Web pages. We use a Na¨ıve Bayes model trained to maximize the margin and compare its performance with several other comparable methods. We construct a novel dataset which consists of segments of text extracted from the Web, where each segment contains pairs of devices. We also propose a novel, inexpensive and very effective way of getting people to label text data using a Web service, the Mechanical Turk. Our results show that the maximum margin model consistently outperforms the other methods. 
Named entity (NE) translation plays an important role in many applications. In this paper, we focus on translating NEs from Korean to Chinese to improve Korean-Chinese cross-language information retrieval (KCIR). The ideographic nature of Chinese makes NE translation diﬃcult because one syllable may map to several Chinese characters. We propose a hybrid NE translation system. First, we integrate two online databases to extend the coverage of our bilingual dictionaries. We use Wikipedia as a translation tool based on the inter-language links between the Korean edition and the Chinese or English editions. We also use Naver.com’s people search engine to ﬁnd a query name’s Chinese or English translation. The second component is able to learn Korean-Chinese (KC), Korean-English (K-E), and EnglishChinese (E-C) translation patterns from the web. These patterns can be used to extract K-C, K-E and E-C pairs from Google snippets. We found KCIR performance using this hybrid conﬁguration over ﬁve times better than that a dictionary-based conﬁguration using only Naver people search. Mean average precision was as high as 0.3385 and recall  reached 0.7578. Our method can handle Chinese, Japanese, Korean, and nonCJK NE translation and improve performance of KCIR substantially. 
We consider the problem of 1 identifying product features and opinion words in a unified process from Chinese customer reviews when only a much small seed set of opinion words is available. In particular, we consider a problem setting motivated by the task of identifying product features with opinion words and learning opinion words through features alternately and iteratively. In customer reviews, opinion words usually have a close relationship with product features, and the association between them is measured by a revised formula of mutual information in this paper. A bootstrapping iterative learning strategy is proposed to alternately both of them. A linguistic rule is adopted to identify lowfrequent features and opinion words. Furthermore, a mapping function from opinion words to features is proposed to identify implicit features in sentence. Empirical results on three kinds of product reviews indicate the effectiveness of our method. 
We propose a machine learning based method of sentiment classiﬁcation of sentences using word-level polarity. The polarities of words in a sentence are not always the same as that of the sentence, because there can be polarity-shifters such as negation expressions. The proposed method models the polarity-shifters. Our model can be trained in two different ways: word-wise and sentence-wise learning. In sentence-wise learning, the model can be trained so that the prediction of sentence polarities should be accurate. The model can also be combined with features used in previous work such as bag-of-words and n-grams. We empirically show that our method almost always improves the performance of sentiment classiﬁcation of sentences especially when we have only small amount of training data. 
We address the problem of sentiment and objectivity classification of product reviews in Chinese. Our approach is distinctive in that it treats both positive / negative sentiment and subjectivity / objectivity not as distinct classes but rather as a continuum; we argue that this is desirable from the perspective of would-be customers who read the reviews. We use novel unsupervised techniques, including a one-word 'seed' vocabulary and iterative retraining for sentiment processing, and a criterion of 'sentiment density' for determining the extent to which a document is opinionated. The classifier achieves up to 87% F-measure for sentiment polarity detection. 
Recognizing the emotive meaning of text can add another dimension to the understanding of text. We study the task of automatically categorizing sentences in a text into Ekman’s six basic emotion categories. We experiment with corpus-based features as well as features derived from two emotion lexicons. One lexicon is automatically built using the classification system of Roget’s Thesaurus, while the other consists of words extracted from WordNet-Affect. Experiments on the data obtained from blogs show that a combination of corpus-based unigram features with emotion-related features provides superior classification performance. We achieve Fmeasure values that outperform the rulebased baseline method for all emotion classes. 
Combining different metrics into a single measure of quality seems the most direct and natural way to improve over the quality of individual metrics. Recently, several approaches have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a). Although based on different assumptions, these approaches share the common characteristic of being parametric. Their models involve a number of parameters whose weight must be adjusted. As an alternative, in this work, we study the behaviour of non-parametric schemes, in which metrics are combined without having to adjust their relative importance. Besides, rather than limiting to the lexical dimension, we work on a wide set of metrics operating at different linguistic levels (e.g., lexical, syntactic and semantic). Experimental results show that non-parametric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation. 
This study presents a method to automatically acquire paraphrases using bilingual corpora, which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques. Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation, it would be possible to obtain interchangeable paraphrases under a given context. Also, we provide an advanced method to acquire generalized translation knowledge using the extracted paraphrases. We applied the method to acquire the generalized translation knowledge for KoreanEnglish translation. Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio. 
Chinese named entity recognition (NER) has recently been viewed as a classiﬁcation or sequence labeling problem, and many approaches have been proposed. However, they tend to address this problem without considering linguistic information in Chinese NEs. We propose a new framework based on probabilistic graphical models with ﬁrstorder logic for Chinese NER. First, we use Conditional Random Fields (CRFs), a standard and theoretically well-founded machine learning method based on undirected graphical models as a base system. Second, we introduce various types of domain knowledge into Markov Logic Networks (MLNs), an effective combination of ﬁrst-order logic and probabilistic graphical models for validation and error correction of entities. Experimental results show that our framework of probabilistic graphical models with ﬁrst-order logic signiﬁcantly outperforms the state-of-the-art models for solving this task. 
We describe our effort in developing a Named Entity Recognition (NER) system for Hindi using Maximum Entropy (MaxEnt) approach. We developed a NER annotated corpora for the purpose. We have tried to identify the most relevant features for Hindi NER task to enable us to develop an efﬁcient NER from the limited corpora developed. Apart from the orthographic and collocation features, we have experimented on the efﬁciency of using gazetteer lists as features. We also worked on semi-automatic induction of context patterns and experimented with using these as features of the MaxEnt method. We have evaluated the performance of the system against a blind test set having 4 classes - Person, Organization, Location and Date. Our system achieved a f-value of 81.52%. 
We propose a method that incorporates paraphrase information from the Web to boost the performance of a supervised relation extraction system. Contextual information is extracted from the Web using a semi-supervised process, and summarized by skip-bigram overlap measures over the entire extract. This allows the capture of local contextual information as well as more distant associations. We observe a statistically signiﬁcant boost in relation extraction performance. We investigate two extensions, thematic clustering and hypernym expansion. In tandem with thematic clustering to reduce noise in our paraphrase extraction, we attempt to increase the coverage of our search for paraphrases using hypernym expansion. Evaluation of our method on the ACE 2004 corpus shows that it out-performs the baseline SVM-based supervised learning algorithm across almost all major ACE relation types, by a margin of up to 31%. 
We propose a method for learning semantic categories of words with minimal supervision from web search query logs. Our method is based on the Espresso algorithm (Pantel and Pennacchiotti, 2006) for extracting binary lexical relations, but makes important modifications to handle query log data for the task of acquiring semantic categories. We present experimental results comparing our method with two state-ofthe-art minimally supervised lexical knowledge extraction systems using Japanese query log data, and show that our method achieves higher precision than the previously proposed methods. We also show that the proposed method offers an additional advantage for knowledge acquisition in an Asian language for which word segmentation is an issue, as the method utilizes no prior knowledge of word segmentation, and is able to harvest new terms with correct word segmentation. 
In this paper, we address the problem of knowing when to stop the process of active learning. We propose a new statistical learning approach, called minimum expected error strategy, to defining a stopping criterion through estimation of the classifier’s expected error on future unlabeled examples in the active learning process. In experiments on active learning for word sense disambiguation and text classification tasks, experimental results show that the new proposed stopping criterion can reduce approximately 50% human labeling costs in word sense disambiguation with degradation of 0.5% average accuracy, and approximately 90% costs in text classification with degradation of 2% average accuracy. 
This paper discusses a new approach to training of transliteration model from unlabeled data for transliteration extraction. We start with an inquiry into the formulation of transliteration model by considering different transliteration strategies as a multi-view problem, where each view exploits a natural division of transliteration features, such as phonemebased, grapheme-based or hybrid features. Then we introduce a multi-view Cotraining algorithm, which leverages compatible and partially uncorrelated information across different views to effectively boost the model from unlabeled data. Applying this algorithm to transliteration extraction, the results show that it not only circumvents the need of data labeling, but also achieves performance close to that of supervised learning, where manual labeling is required for all training samples. 
OBJECTIVE: The prior knowledge about the rhetorical structure of scientiﬁc abstracts is useful for various text-mining tasks such as information extraction, information retrieval, and automatic summarization. This paper presents a novel approach to categorize sentences in scientiﬁc abstracts into four sections, objective, methods, results, and conclusions. METHOD: Formalizing the categorization task as a sequential labeling problem, we employ Conditional Random Fields (CRFs) to annotate section labels into abstract sentences. The training corpus is acquired automatically from Medline abstracts. RESULTS: The proposed method outperformed the previous approaches, achieving 95.5% per-sentence accuracy and 68.8% per-abstract accuracy. CONCLUSION: The experimental results showed that CRFs could model the rhetorical structure of abstracts more suitably. 
We present a general approach to formally modelling corpora with multi-layered annotation, thereby inducing a lexicon model in a typed logical representation language, OWL DL. This model can be interpreted as a graph structure that offers ﬂexible querying functionality beyond current XML-based query languages and powerful methods for consistency control. We illustrate our approach by applying it to the syntactically and semantically annotated SALSA/TIGER corpus. 
Numerative classiﬁers are ubiquitous in many Asian languages. This paper proposes a method to construct a taxonomy of numerative classiﬁers based on a nounclassiﬁer agreement database. The taxonomy deﬁnes superordinate-subordinate relation among numerative classiﬁers and represents the relations in tree structures. The experiments to construct taxonomies were conducted for evaluation by using data from three different languages: Chinese, Japanese and Thai. We found that our method was promising for Chinese and Japanese, but inappropriate for Thai. It conﬁrms that there really is no hierarchy among Thai classiﬁers. 
This paper presents an approach to the translation of compound words without the need for bilingual training text, by modeling the mapping of literal component word glosses (e.g. “iron-path”) into ﬂuent English (e.g. “railway”) across multiple languages. Performance is improved by adding component-sequence and learnedmorphology models along with context similarity from monolingual text and optional combination with traditional bilingual-textbased translation discovery. 
A lightweight extraction method derives text snippets associated to dates from the Web. The snippets are organized dynamically into answers to deﬁnition questions. Experiments on standard test question sets show that temporally-anchored text snippets allow for efﬁciently answering deﬁnition questions at accuracy levels comparable to the best systems, without any need for complex lexical resources, or specialized processing modules dedicated to ﬁnding deﬁnitions. 
This paper proposes a corpus-based approach for answering why-questions. Conventional systems use hand-crafted patterns to extract and evaluate answer candidates. However, such hand-crafted patterns are likely to have low coverage of causal expressions, and it is also difﬁcult to assign suitable weights to the patterns by hand. In our approach, causal expressions are automatically collected from corpora tagged with semantic relations. From the collected expressions, features are created to train an answer candidate ranker that maximizes the QA performance with regards to the corpus of why-questions and answers. NAZEQA, a Japanese why-QA system based on our approach, clearly outperforms a baseline that uses hand-crafted patterns with a Mean Reciprocal Rank (top-5) of 0.305, making it presumably the best-performing fully implemented why-QA system. 
Document retrieval is a critical component of question answering (QA), yet little work has been done towards statistical modeling of queries and towards automatic generation of high quality query content for QA. This paper introduces a new, cluster-based query expansion method that learns queries known to be successful when applied to similar questions. We show that cluster-based expansion improves the retrieval performance of a statistical question answering system when used in addition to existing query expansion methods. This paper presents experiments with several feature selection methods used individually and in combination. We show that documents retrieved using the cluster-based approach are inherently different than documents retrieved using existing methods and provide a higher data diversity to answers extractors. 
Selecting appropriate features to represent an entity pair plays a key role in the task of relation recognition. However, existing syntactic features or lexical features cannot capture the interaction between two entities because of the dearth of annotated relational corpus specialized for relation recognition. In this paper, we propose a semantic feature, called the latent topic f eature, which is topic-based and represents an entity pair at the semantic level instead of the word level. Moreover, to address the problem of insufﬁciently annotated corpora, we propose an algorithm for compiling a training corpus from the Web. Experiment results demonstrate that latent topic f eatures are as effective as syntactic or lexical features. Moreover, the Web-based corpus can resolve the problems caused by insufﬁciently annotated relational corpora. 
Computer users increasingly need to produce text written in multiple languages. However, typical computer interfaces require the user to change the text entry software each time a different language is used. This is cumbersome, especially when language changes are frequent. To solve this problem, we propose TypeAny, a novel front-end interface that detects the language of the user’s key entry and automatically dispatches the input to the appropriate text entry system. Unlike previously reported methods, TypeAny can handle more than two languages, and can easily support any new language even if the available corpus is small. When evaluating this method, we obtained language detection accuracy of 96.7% when an appropriate language had to be chosen from among three languages. The number of control actions needed to switch languages was decreased over 93% when using TypeAny rather than a conventional method. 
We present a modular system for detection and correction of errors made by nonnative (English as a Second Language = ESL) writers. We focus on two error types: the incorrect use of determiners and the choice of prepositions. We use a decisiontree approach inspired by contextual spelling systems for detection and correction suggestions, and a large language model trained on the Gigaword corpus to provide additional information to filter out spurious suggestions. We show how this system performs on a corpus of non-native English text and discuss strategies for future enhancements. 
This paper proposes a method for identifying synonymous relations in a bilingual lexicon, which is a set of translation-equivalent term pairs. We train a classiﬁer for identifying those synonymous relations by using spelling variations as main clues. We compared two approaches: the direct identiﬁcation of bilingual synonym pairs, and the merger of two monolingual synonyms. We showed that our approach achieves a high pair-wise precision and recall, and outperforms the baseline method. 
We present a novel algorithm for the acquisition of multilingual lexical taxonomies (including hyponymy/hypernymy, meronymy and taxonomic cousinhood), from monolingual corpora with minimal supervision in the form of seed exemplars using discriminative learning across the major WordNet semantic relationships. This capability is also extended robustly and effectively to a second language (Hindi) via cross-language projection of the various seed exemplars. We also present a novel model of translation dictionary induction via multilingual transitive models of hypernymy and hyponymy, using these induced taxonomies. Candidate lexical translation probabilities are based on the probability that their induced hyponyms and/or hypernyms are translations of one another. We evaluate all of the above models on English and Hindi. 
We present the results of research with the goal of automatically creating a multilingual thesaurus based on the freely available resources of Wikipedia and WordNet. Our goal is to increase resources for natural language processing tasks such as machine translation targeting the Japanese-Spanish language pair. Given the scarcity of resources, we use existing English resources as a pivot for creating a trilingual JapaneseSpanish-English thesaurus. Our approach consists of extracting the translation tuples from Wikipedia, disambiguating them by mapping them to WordNet word senses. We present results comparing two methods of disambiguation, the first using VSM on Wikipedia article texts and WordNet definitions, and the second using categorical information extracted from Wikipedia, We find that mixing the two methods produces favorable results. Using the proposed method, we have constructed a multilingual Spanish-Japanese-English thesaurus consisting of 25,375 entries. The same method can be applied to any pair of languages that are linked to English in Wikipedia. 
In this paper, we propose a machine learning approach to rhetorical role identification from legal documents. In our approach, we annotate roles in sample documents with the help of legal experts and take them as training data. Conditional random field model has been trained with the data to perform rhetorical role identification with reinforcement of rich feature sets. The understanding of structure of a legal document and the application of mathematical model can brings out an effective summary in the final stage. Other important new findings in this work include that the training of a model for one sub-domain can be extended to another sub-domains with very limited augmentation of feature sets. Moreover, we can significantly improve extraction-based summarization results by modifying the ranking of sentences with the importance of specific roles. 
 John [met]event Mary [last night]timex.  We present a cross-lingual projection framework for temporal annotations. Automatically obtained TimeML annotations in the English portion of a parallel corpus are transferred to the German translation along a word alignment. Direct projection augmented with shallow heuristic knowledge outperforms the uninformed baseline by 6.64% F1-measure for events, and by 17.93% for time expressions. Subsequent training of statistical classiﬁers on the (imperfect) projected annotations signiﬁcantly boosts precision by up to 31% to 83.95% and 89.52%, respectively. 
Aiming at acquiring semantic relations between events from a large corpus, this paper proposes several extensions to a state-of-theart method originally designed for entity relation extraction, reporting on the present results of our experiments on a Japanese Web corpus. The results show that (a) there are indeed speciﬁc cooccurrence patterns useful for event relation acquisition, (b) the use of cooccurrence samples involving verbal nouns has positive impacts on both recall and precision, and (c) over ﬁve thousand relation instances are acquired from a 500M-sentence Web corpus with a precision of about 66% for action-effect relations. 
Bracketing Transduction Grammar (BTG) has been well studied and used in statistical machine translation (SMT) with promising results. However, there are two major issues for BTG-based SMT. First, there is no effective mechanism available for predicting orders between neighboring blocks in the original BTG. Second, the computational cost is high. In this paper, we introduce two reﬁnements for BTG-based SMT to achieve better reordering and higher-speed decoding, which include (1) reordering heuristics to prevent incorrect swapping and reduce search space, and (2) special phrases with tags to indicate sentence beginning and ending. The two reﬁnements are integrated into a well-established BTG-based Chinese-toEnglish SMT system that is trained on largescale parallel data. Experimental results on the NIST MT-05 task show that the proposed reﬁnements contribute signiﬁcant improvement of 2% in BLEU score over the baseline system. 
In this paper, we report our work on incorporating syntactic and morphological information for English to Hindi statistical machine translation. Two simple and computationally inexpensive ideas have proven to be surprisingly effective: (i) reordering the English source sentence as per Hindi syntax, and (ii) using the sufﬁxes of Hindi words. The former is done by applying simple transformation rules on the English parse tree. The latter, by using a simple sufﬁx separation program. With only a small amount of bilingual training data and limited tools for Hindi, we achieve reasonable performance and substantial improvements over the baseline phrase-based system. Our approach eschews the use of parsing or other sophisticated linguistic tools for the target language (Hindi) making it a useful framework for statistical machine translation from English to Indian languages in general, since such tools are not widely available for Indian languages currently. 
Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user proﬁle as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user proﬁle thus learnt is applied in a re-ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user proﬁles using snippets surrounding the results for a query gives better performance than learning from entire document collection. 
For the majority of the world’s languages, the number of linguistic resources (e.g., annotated corpora and parallel data) is very limited. Consequently, supervised methods, as well as many unsupervised methods, cannot be applied directly, leaving these languages largely untouched and unnoticed. In this paper, we describe the construction of a resource that taps the large body of linguistically analyzed language data that has made its way to the Web, and propose using this resource to bootstrap NLP tool development. 
In a broad range of natural language processing tasks, large-scale knowledge-base of paraphrases is anticipated to improve their performance. The key issue in creating such a resource is to establish a practical method of computing semantic equivalence and syntactic substitutability, i.e., paraphrasability, between given pair of expressions. This paper addresses the issues of computing paraphrasability, focusing on syntactic variants of predicate phrases. Our model estimates paraphrasability based on traditional distributional similarity measures, where the Web snippets are used to overcome the data sparseness problem in handling predicate phrases. Several feature sets are evaluated through empirical experiments. 
Wikipedia is the largest organized knowledge repository on the Web, increasingly employed by natural language processing and search tools. In this paper, we investigate the task of labeling Wikipedia pages with standard named entity tags, which can be used further by a range of information extraction and language processing tools. To train the classifiers, we manually annotated a small set of Wikipedia pages and then extrapolated the annotations using the Wikipedia category information to a much larger training set. We employed several distinct features for each page: bag-of-words, page structure, abstract, titles, and entity mentions. We report high accuracies for several of the classifiers built. As a result of this work, a Web service that classifies any Wikipedia page has been made available to the academic community. 
Distributional similarity is a widely used concept to capture the semantic relatedness of words in various NLP tasks. However, accurate similarity calculation requires a large number of contexts, which leads to impractically high computational complexity. To alleviate the problem, we have investigated the effectiveness of automatic context selection by applying feature selection methods explored mainly for text categorization. Our experiments on synonym acquisition have shown that while keeping or sometimes increasing the performance, we can drastically reduce the unique contexts up to 10% of the original size. We have also extended the measures so that they cover context categories. The result shows a considerable correlation between the measures and the performance, enabling the automatic selection of effective context categories for distributional similarity. 
In recent years there have been various approaches aimed at automatic acquisition of predominant senses of words. This information can be exploited as a powerful backoff strategy for word sense disambiguation given the zipﬁan distribution of word senses. Approaches which do not require manually sense-tagged data have been proposed for English exploiting lexical resources available, notably WordNet. In these approaches distributional similarity is coupled with a semantic similarity measure which ties the distributionally related words to the sense inventory. The semantic similarity measures that have been used have all taken advantage of the hierarchical information in WordNet. We investigate the applicability to Japanese and demonstrate the feasibility of a measure which uses only information in the dictionary deﬁnitions, in contrast with previous work on English which uses hierarchical information in addition to dictionary definitions. We extend the deﬁnition based semantic similarity measure with distributional similarity applied to the words in different deﬁnitions. This increases the recall of our method and in some cases, precision as well. 
Automatic processing of medical dictations poses a signiﬁcant challenge. We approach the problem by introducing a statistical framework capable of identifying types and boundaries of sections, lists and other structures occurring in a dictation, thereby gaining explicit knowledge about the function of such elements. Training data is created semiautomatically by aligning a parallel corpus of corrected medical reports and corresponding transcripts generated via automatic speech recognition. We highlight the properties of our statistical framework, which is based on conditional random ﬁelds (CRFs) and implemented as an efﬁcient, publicly available toolkit. Finally, we show that our approach is effective both under ideal conditions and for real-life dictation involving speech recognition errors and speech-related phenomena such as hesitation and repetitions. 
Contradiction Detection (CD) in text is a difﬁcult NLP task. We investigate CD over functions (e.g., BornIn(Person)=Place), and present a domain-independent algorithm that automatically discovers phrases denoting functions with high precision. Previous work on CD has investigated hand-chosen sentence pairs. In contrast, we automatically harvested from the Web pairs of sentences that appear contradictory, but were surprised to ﬁnd that most pairs are in fact consistent. For example, “Mozart was born in Salzburg” does not contradict “Mozart was born in Austria” despite the functional nature of the phrase “was born in”. We show that background knowledge about meronyms (e.g., Salzburg is in Austria), synonyms, functions, and more is essential for success in the CD task. 
Regular expressions have served as the dominant workhorse of practical information extraction for several years. However, there has been little work on reducing the manual effort involved in building high-quality, complex regular expressions for information extraction tasks. In this paper, we propose ReLIE, a novel transformation-based algorithm for learning such complex regular expressions. We evaluate the performance of our algorithm on multiple datasets and compare it against the CRF algorithm. We show that ReLIE, in addition to being an order of magnitude faster, outperforms CRF under conditions of limited training data and cross-domain data. Finally, we show how the accuracy of CRF can be improved by using features extracted by ReLIE. 
A human annotator can provide hints to a machine learner by highlighting contextual “rationales” for each of his or her annotations (Zaidan et al., 2007). How can one exploit this side information to better learn the desired parameters θ? We present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales. Thus, observing the rationales helps us infer the true θ. We collect substring rationales for a sentiment classiﬁcation task (Pang and Lee, 2004) and use them to obtain signiﬁcant accuracy improvements for each annotator. Our new generative approach exploits the rationales more effectively than our previous “masking SVM” approach. It is also more principled, and could be adapted to help learn other kinds of probabilistic classiﬁers for quite different tasks. 
Having seen a news title “Alba denies wedding reports”, how do we infer that it is primarily about Jessica Alba, rather than about weddings or reports? We probably realize that, in a randomly driven sentence, the word “Alba” is less anticipated than “wedding” or “reports”, which adds value to the word “Alba” if used. Such anticipation can be modeled as a ratio between an empirical probability of the word (in a given corpus) and its estimated probability in general English. Aggregated over all words in a document, this ratio may be used as a measure of the document’s topicality. Assuming that the corpus consists of on-topic and off-topic documents (we call them the core and the noise), our goal is to determine which documents belong to the core. We propose two unsupervised methods for doing this. First, we assume that words are sampled i.i.d., and propose an information-theoretic framework for determining the core. Second, we relax the independence assumption and use a simple graphical model to rank documents according to their likelihood of belonging to the core. We discuss theoretical guarantees of the proposed methods and show their usefulness for Web Mining and Topic Detection and Tracking (TDT). 
We propose a new approach to language modeling which utilizes discriminative learning methods. Our approach is an iterative one: starting with an initial language model, in each iteration we generate 'false' sentences from the current model, and then train a classifier to discriminate between them and sentences from the training corpus. To the extent that this succeeds, the classifier is incorporated into the model by lowering the probability of sentences classified as false, and the process is repeated. We demonstrate the effectiveness of this approach on a natural language corpus and show it provides an 11.4% improvement in perplexity over a modified kneser-ney smoothed trigram. 
We present a discriminative method for learning selectional preferences from unlabeled text. Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations. We train a Support Vector Machine classiﬁer to distinguish the positive from the negative instances. We show how to partition the examples for efﬁcient training with 57 thousand features and 6.5 million training instances. The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments. Compared to Mutual Information, it identiﬁes 66% more verb-object pairs in unseen text, and resolves 37% more pronouns correctly in a pronoun resolution experiment. 
We present a PropBank semantic role labeling system for English that is integrated with a dependency parser. To tackle the problem of joint syntactic–semantic analysis, the system relies on a syntactic and a semantic subcomponent. The syntactic model is a projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classiﬁers. The complete syntactic–semantic output is selected from a candidate pool generated by the subsystems. We evaluate the system on the CoNLL2005 test sets using segment-based and dependency-based metrics. Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 ﬁgure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently. Using a dependency-based metric, the F1 ﬁgure of our system is 84.29 on the test set from CoNLL-2008. Our system is the ﬁrst dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance. 
 Most Web-based Q/A systems work by ﬁnding pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, we introduce the HOLMES system, which utilizes textual inference (TI) over tuples extracted from text. Whereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, HOLMES utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, HOLMES doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, HOLMES’s runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus—they are “approximately” functional in a well-deﬁned sense. 
This paper proposes a novel maximum entropy based rule selection (MERS) model for syntax-based statistical machine translation (SMT). The MERS model combines local contextual information around rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 
This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. 
The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding. We propose a multipass, coarse-to-ﬁne approach in which the language model complexity is incrementally introduced. In contrast to previous orderbased bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language. Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score. Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder. 
In this paper, we present a novel method based on CRFs in response to the two special characteristics of “contextual dependency” and “label redundancy” in sentence sentiment classification. We try to capture the contextual constraints on sentence sentiment using CRFs. Through introducing redundant labels into the original sentimental label set and organizing all labels into a hierarchy, our method can add redundant features into training for capturing the label redundancy. The experimental results prove that our method outperforms the traditional methods like NB, SVM, MaxEnt and standard chain CRFs. In comparison with the cascaded model, our method can effectively alleviate the error propagation among different layers and obtain better performance in each layer. 
Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language. In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages. Speciﬁcally, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages. Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language. 
This paper presents two approaches to ranking reader emotions of documents. Past studies assign a document to a single emotion category, so their methods cannot be applied directly to the emotion ranking problem. Furthermore, whereas previous research analyzes emotions from the writer’s perspective, this work examines readers’ emotional states. The first approach proposed in this paper minimizes pairwise ranking errors. In the second approach, regression is used to model emotional distributions. Experiment results show that the regression method is more effective at identifying the most popular emotion, but the pairwise loss minimization method produces ranked lists of emotions that have better correlations with the correct lists. 
We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efﬁcient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features signiﬁcantly improve parse accuracy over exact ﬁrst-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively. 
We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the ﬁrst. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacriﬁcing efﬁcient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 
We present a study on how grammar binarization empirically affects the efﬁciency of the CKY parsing. We argue that binarizations affect parsing efﬁciency primarily by affecting the number of incomplete constituents generated, and the effectiveness of binarization also depends on the nature of the input. We propose a novel binarization method utilizing rich information learnt from training corpus. Experimental results not only show that different binarizations have great impacts on parsing efﬁciency, but also conﬁrm that our learnt binarization outperforms other existing methods. Furthermore we show that it is feasible to combine existing parsing speed-up techniques with our binarization to achieve even better performance. 
We present a novel unsupervised sentence fusion method which we apply to a corpus of biographies in German. Given a group of related sentences, we align their dependency trees and build a dependency graph. Using integer linear programming we compress this graph to a new tree, which we then linearize. We use GermaNet and Wikipedia for checking semantic compatibility of co-arguments. In an evaluation with human judges our method outperforms the fusion approach of Barzilay & McKeown (2005) with respect to readability. 
We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readability. This is the ﬁrst study to take into account such a variety of linguistic factors and the ﬁrst to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text. We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus. We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks. 
We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. 
Translation rule extraction is a fundamental problem in machine translation, especially for linguistically syntax-based systems that need parse trees from either or both sides of the bitext. The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors. So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses. Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses. When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points. 
We advance the state-of-the-art for discriminatively trained machine translation systems by presenting novel probabilistic inference and search methods for synchronous grammars. By approximating the intractable space of all candidate translations produced by intersecting an ngram language model with a synchronous grammar, we are able to train and decode models incorporating millions of sparse, heterogeneous features. Further, we demonstrate the power of the discriminative training paradigm by extracting structured syntactic features, and achieving increases in translation performance. 
Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We ﬁrst show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deﬁciencies in the Hiero hierarchical phrasebased model: ﬁrst, we simultaneously train a large number of Marton and Resnik’s soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain signiﬁcant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 
One major bottleneck in conversational systems is their incapability in interpreting unexpected user language inputs such as out-ofvocabulary words. To overcome this problem, conversational systems must be able to learn new words automatically during human machine conversation. Motivated by psycholinguistic ﬁndings on eye gaze and human language processing, we are developing techniques to incorporate human eye gaze for automatic word acquisition in multimodal conversational systems. This paper investigates the use of temporal alignment between speech and eye gaze and the use of domain knowledge in word acquisition. Our experiment results indicate that eye gaze provides a potential channel for automatically acquiring new words. The use of extra temporal and domain knowledge can signiﬁcantly improve acquisition performance. 
 Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon’s Mechanical Turk system, a signiﬁcantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate ﬁve tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all ﬁve, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that signiﬁcantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 
Compared to the telephone, email based customer care is increasingly becoming the preferred channel of communication for corporations and customers. Most email-based customer care management systems provide a method to include template texts in order to reduce the handling time for a customer’s email. The text in a template is suitably modiﬁed into a response by a customer care agent. In this paper, we present two techniques to improve the effectiveness of a template by providing tools for the template authors. First, we present a tool to track and visualize the edits made by agents to a template which serves as a vital feedback to the template authors. Second, we present a novel method that automatically extracts potential templates from responses authored by agents. These methods are investigated in the context of an email customer care analysis tool that handles over a million emails a year. 
This paper describes a language-independent, scalable system for both challenges of crossdocument co-reference: name variation and entity disambiguation. We provide system results from the ACE 2008 evaluation in both English and Arabic. Our English system’s accuracy is 8.4% relative better than an exact match baseline (and 14.2% relative better over entities mentioned in more than one document). Unlike previous evaluations, ACE 2008 evaluated both name variation and entity disambiguation over naturally occurring named mentions. An information extraction engine finds document entities in text. We describe how our architecture designed for the 10K document ACE task is scalable to an even larger corpus. Our cross-document approach uses the names of entities to find an initial set of document entities that could refer to the same real world entity and then uses an agglomerative clustering algorithm to disambiguate the potentially co-referent document entities. We analyze how different aspects of our system affect performance using ablation studies over the English evaluation set. In addition to evaluating cross-document coreference performance, we used the results of the cross-document system to improve the accuracy of within-document extraction, and measured the impact in the ACE 2008 withindocument evaluation. 
The Named Entity Recognition (NER) task has been garnering signiﬁcant attention in NLP as it helps improve the performance of many natural language processing applications. In this paper, we investigate the impact of using different sets of features in two discriminative machine learning frameworks, namely, Support Vector Machines and Conditional Random Fields using Arabic data. We explore lexical, contextual and morphological features on eight standardized data-sets of different genres. We measure the impact of the different features in isolation, rank them according to their impact for each named entity class and incrementally combine them in order to infer the optimal machine learning approach and feature set. Our system yields a performance of Fβ=1-measure=83.5 on ACE 2003 Broadcast News data. 
In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model. This paper describes a rather simple pairwise classiﬁcation model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features. 
Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topicoriented, informative multi-document summarization. In this paper, we have experimented with one empirical and two unsupervised statistical machine learning techniques: kmeans and Expectation Maximization (EM), for computing relative importance of the sentences. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. We extracted different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query. We used a local search technique to learn the weights of the features. For all our methods of generating summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) features. 
We describe the ﬁrst tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrasebased translation systems. 
In recent years, with the development of Chinese semantically annotated corpus, such as Chinese Proposition Bank and Normalization Bank, the Chinese semantic role labeling (SRL) task has been boosted. Similar to English, the Chinese SRL can be divided into two tasks: semantic role identification (SRI) and classification (SRC). Many features were introduced into these tasks and promising results were achieved. In this paper, we mainly focus on the second task: SRC. After exploiting the linguistic discrepancy between numbered arguments and ARGMs, we built a semantic role classifier based on a hierarchical feature selection strategy. Different from the previous SRC systems, we divided SRC into three sub tasks in sequence and trained models for each sub task. Under the hierarchical architecture, each argument should first be determined whether it is a numbered argument or an ARGM, and then be classified into finegained categories. Finally, we integrated the idea of exploiting argument interdependence into our system and further improved the performance. With the novel method, the classification precision of our system is 94.68%, which outperforms the strong baseline significantly. It is also the state-of-the-art on Chinese SRC. 
This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework.1 
There is growing interest in applying Bayesian techniques to NLP problems. There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on. This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes. Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM. We invesigate a variety of samplers for HMMs, including some that these earlier papers did not study. We ﬁnd that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers. In terms of times of convergence, we ﬁnd that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets. 
 This paper introduces a new method for identifying named-entity (NE) transliterations in bilingual corpora. Recent works have shown the advantage of discriminative approaches to transliteration: given two strings (ws, wt) in the source and target language, a classiﬁer is trained to determine if wt is the transliteration of ws. This paper shows that the transliteration problem can be formulated as a constrained optimization problem and thus take into account contextual dependencies and constraints among character bi-grams in the two strings. We further explore several methods for learning the objective function of the optimization problem and show the advantage of learning it discriminately. Our experiments show that the new framework results in over 50% improvement in translating English NEs to Hebrew. 
How can the development of ideas in a scientiﬁc ﬁeld be studied over time? We apply unsupervised topic modeling to the ACL Anthology to analyze historical trends in the ﬁeld of Computational Linguistics from 1978 to 2006. We induce topic clusters using Latent Dirichlet Allocation, and examine the strength of each topic over time. Our methods ﬁnd trends in the ﬁeld including the rise of probabilistic methods starting in 1988, a steady increase in applications, and a sharp decline of research in semantics and understanding between 1978 and 2001, possibly rising again after 2001. We also introduce a model of the diversity of ideas, topic entropy, using it to show that COLING is a more diverse conference than ACL, but that both conferences as well as EMNLP are becoming broader over time. Finally, we apply Jensen-Shannon divergence of topic distributions to show that all three conferences are converging in the topics they cover. 
This paper describes a lexical trigger model for statistical machine translation. We present various methods using triplets incorporating long-distance dependencies that can go beyond the local context of phrases or n-gram based language models. We evaluate the presented methods on two translation tasks in a reranking framework and compare it to the related IBM model 1. We show slightly improved translation quality in terms of BLEU and TER and address various constraints to speed up the training based on ExpectationMaximization and to lower the overall number of triplets without loss in translation performance. 
In this paper we present a textual dialogue system that uses word associations retrieved from the Web to create propositions. We also show experiment results for the role of modality generation. The proposed system automatically extracts sets of words related to a conversation topic set freely by a user. After the extraction process, it generates an utterance, adds a modality and veriﬁes the semantic reliability of the proposed sentence. We evaluate word associations extracted form the Web, and the results of adding modality. Over 80% of the extracted word associations were evaluated as correct. Adding modality improved the system signiﬁcantly for all evaluation criteria. We also show how our system can be used as a simple and expandable platform for almost any kind of experiment with human-computer textual conversation in Japanese. Two examples with affect analysis and humor generation are given. 
 Foreign name translations typically include multiple spelling variants. These variants cause data sparseness problems, increase Out-of-Vocabulary (OOV) rate, and present challenges for machine translation, information extraction and other NLP tasks. This paper aims to identify name spelling variants in the target language using the source name as an anchor. Based on wordto-word translation and transliteration probabilities, as well as the string edit distance metric, target name translations with similar spellings are clustered. With this approach tens of thousands of high precision name translation spelling variants are extracted from sentence-aligned bilingual corpora. When these name spelling variants are applied to Machine Translation and Information Extraction tasks, improvements over strong baseline systems are observed in both cases. 
Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models. 
This paper explores the challenge of scaling up language processing algorithms to increasingly large datasets. While cluster computing has been available in commercial environments for several years, academic researchers have fallen behind in their ability to work on large datasets. I discuss two barriers contributing to this problem: lack of a suitable programming model for managing concurrency and difﬁculty in obtaining access to hardware. Hadoop, an open-source implementation of Google’s MapReduce framework, provides a compelling solution to both issues. Its simple programming model hides system-level details from the developer, and its ability to run on commodity hardware puts cluster computing within the reach of many academic research groups. This paper illustrates these points with a case study in building word cooccurrence matrices from large corpora. I conclude with an analysis of an alternative computing model based on renting instead of buying computer clusters. 
We propose a novel lexicon acquirer that works in concert with the morphological analyzer and has the ability to run in online mode. Every time a sentence is analyzed, it detects unknown morphemes, enumerates candidates and selects the best candidates by comparing multiple examples kept in the storage. When a morpheme is unambiguously selected, the lexicon acquirer updates the dictionary of the analyzer, and it will be used in subsequent analysis. We use the constraints of Japanese morphology and effectively reduce the number of examples required to acquire a morpheme. Experiments show that unknown morphemes were acquired with high accuracy and improved the quality of morphological analysis. 
We investigate the problem of binary text classiﬁcation in the domain of legal docket entries. This work presents an illustrative instance of a domain-speciﬁc problem where the stateof-the-art Machine Learning (ML) classiﬁers such as SVMs are inadequate. Our investigation into the reasons for the failure of these classiﬁers revealed two types of prominent errors which we call conjunctive and disjunctive errors. We developed simple heuristics to address one of these error types and improve the performance of the SVMs. Based on the intuition gained from our experiments, we also developed a simple propositional logic based classiﬁer using hand-labeled features, that addresses both types of errors simultaneously. We show that this new, but simple, approach outperforms all existing state-of-the-art ML models, with statistically signiﬁcant gains. We hope this work serves as a motivating example of the need to build more expressive classiﬁers beyond the standard model classes, and to address text classiﬁcation problems in such nontraditional domains. 
String transformation, which maps a source string s into its desirable form t∗, is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1-regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision boundary of the model. The advantage of this approach is that candidate strings can be enumerated by an efﬁcient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inﬂected words and spelling variations.  matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007),  t∗ = argmax P (t|s).  (1)  t∈gen(s)  Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly deﬁned gen(s),  gen(s) = {t | dist(s, t) < δ}.  (2)  
Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNetbased models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined. 
We investigate the combination of several sources of information for the purpose of subjectivity recognition and polarity classiﬁcation in meetings. We focus on features from two modalities, transcribed words and acoustics, and we compare the performance of three different textual representations: words, characters, and phonemes. Our experiments show that character-level features outperform wordlevel features for these tasks, and that a careful fusion of all features yields the best performance. 1 
Most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains, making parser adaptation a pressing issue. In this paper we demonstrate that a CCG parser can be adapted to two new domains, biomedical text and questions for a QA system, by using manually-annotated training data at the POS and lexical category levels only. This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain. We ﬁnd that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation. 
Although Machine Translation (MT) is a very active research ﬁeld which is receiving an increasing amount of attention from the research community, the results that current MT systems are capable of producing are still quite far away from perfection. Because of this, and in order to build systems that yield correct translations, human knowledge must be integrated into the translation process, which will be carried out in our case in an InteractivePredictive (IP) framework. In this paper, we show that considering Mouse Actions as a signiﬁcant information source for the underlying system improves the productivity of the human translator involved. In addition, we also show that the initial translations that the MT system provides can be quickly improved by an expert by only performing additional Mouse Actions. In this work, we will be using word graphs as an efﬁcient interface between a phrase-based MT system and the IP engine. 
In this paper, we ﬁrst introduce a new architecture for parsing, bidirectional incremental parsing. We propose a novel algorithm for incremental construction, which can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve signiﬁcant improvement on accuracy over the previous best result on the same data set. 
A bst r act Parallel web pages are important source of training data for statistical machine translation. In this paper, we present a new approach to sentence alignment on parallel web pages. Parallel web pages tend to have parallel structures，and the structural correspondence can be indicative information for identifying parallel sentences. In our approach, the web page is represented as a tree, and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment. Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as “Hansard”. With improved sentence alignment performance, web mining systems are able to acquire parallel sentences of higher quality from the web. 
Previously topic models such as PLSI (Probabilistic Latent Semantic Indexing) and LDA (Latent Dirichlet Allocation) were developed for modeling the contents of plain texts. Recently, topic models for processing hypertexts such as web pages were also proposed. The proposed hypertext models are generative models giving rise to both words and hyperlinks. This paper points out that to better represent the contents of hypertexts it is more essential to assume that the hyperlinks are ﬁxed and to deﬁne the topic model as that of generating words only. The paper then proposes a new topic model for hypertext processing, referred to as Hypertext Topic Model (HTM). HTM deﬁnes the distribution of words in a document (i.e., the content of the document) as a mixture over latent topics in the document itself and latent topics in the documents which the document cites. The topics are further characterized as distributions of words, as in the conventional topic models. This paper further proposes a method for learning the HTM model. Experimental results show that HTM outperforms the baselines on topic discovery and document classiﬁcation in three datasets. 
This paper describes a new automatic method for Japanese predicate argument structure analysis. The method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types, words, semantic categories, parts of speech, functional words and predicate voices. We constructed decision lists in which these features were sorted by their learned weights. Using our method, we integrated the tasks of semantic role labeling and zero-pronoun identiﬁcation, and achieved a 17% improvement compared with a baseline method in a sentence level performance analysis. 
Obtaining labeled data is a signiﬁcant obstacle for many NLP tasks. Recently, online games have been proposed as a new way of obtaining labeled data; games attract users by being fun to play. In this paper, we consider the application of this idea to collecting semantic relations between words, such as hypernym/hyponym relationships. We built three online games, inspired by the real-life games of ScattergoriesTM and TabooTM. As of June 2008, players have entered nearly 800,000 data instances, in two categories. The ﬁrst type of data consists of category/answer pairs (“Types of vehicle”,“car”), while the second is essentially free association data (“submarine”,”underwater”). We analyze both types of data in detail and discuss potential uses of the data. We show that we can extract from our data set a signiﬁcant number of new hypernym/hyponym pairs not already found in WordNet. 
We examine the problem of content selection in statistical novel sentence generation. Our approach models the processes performed by professional editors when incorporating material from additional sentences to support some initially chosen key summary sentence, a process we refer to as Sentence Augmentation. We propose and evaluate a method called “Seed and Grow” for selecting such auxiliary information. Additionally, we argue that this can be performed using schemata, as represented by word-pair co-occurrences, and demonstrate its use in statistical summary sentence generation. Evaluation results are supportive, indicating that a schemata model signiﬁcantly improves over the baseline. 
It is a challenging task to identify sentiment polarity of Chinese reviews because the resources for Chinese sentiment analysis are limited. Instead of leveraging only monolingual Chinese knowledge, this study proposes a novel approach to leverage reliable English resources to improve Chinese sentiment analysis. Rather than simply projecting English resources onto Chinese resources, our approach first translates Chinese reviews into English reviews by machine translation services, and then identifies the sentiment polarity of English reviews by directly leveraging English resources. Furthermore, our approach performs sentiment analysis for both Chinese reviews and English reviews, and then uses ensemble methods to combine the individual analysis results. Experimental results on a dataset of 886 Chinese product reviews demonstrate the effectiveness of the proposed approach. The individual analysis of the translated English reviews outperforms the individual analysis of the original Chinese reviews, and the combination of the individual analysis results further improves the performance. 
Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beamsearch. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively. 
Syntactic word reordering is essential for translations across different grammar structures between syntactically distant languagepairs. In this paper, we propose to embed local and non-local word reordering decisions in a synchronous context free grammar, and leverages the grammar in a chartbased decoder. Local word-reordering is effectively encoded in Hiero-like rules; whereas non-local word-reordering, which allows for long-range movements of syntactic chunks, is represented in tree-based reordering rules, which contain variables correspond to sourceside syntactic constituents. We demonstrate how these rules are learned from parallel corpora. Our proposed shallow Tree-to-String rules show signiﬁcant improvements in translation quality across different test sets. 
We present a graph-based semi-supervised label propagation algorithm for acquiring opendomain labeled classes and their instances from a combination of unstructured and structured text sources. This acquisition method signiﬁcantly improves coverage compared to a previous set of labeled classes and instances derived from free text, while achieving comparable precision. 
Relationship discovery is the task of identifying salient relationships between named entities in text. We propose novel approaches for two sub-tasks of the problem: identifying the entities of interest, and partitioning and describing the relations based on their semantics. In particular, we show that term frequency patterns can be used effectively instead of supervised NER, and that the pmedian clustering objective function naturally uncovers relation exemplars appropriate for describing the partitioning. Furthermore, we introduce a novel application of relationship discovery: the unsupervised identiﬁcation of protein-protein interaction phrases. 
While signiﬁcant effort has been put into annotating linguistic resources for several languages, there are still many left that have only small amounts of such resources. This paper investigates a method of propagating information (speciﬁcally mention detection information) into such low resource languages from richer ones. Experiments run on three language pairs (Arabic-English, Chinese-English, and Spanish-English) show that one can achieve relatively decent performance by propagating information from a language with richer resources such as English into a foreign language alone (no resources or models in the foreign language). Furthermore, while examining the performance using various degrees of linguistic information in a statistical framework, results show that propagated features from English help improve the source-language system performance even when used in conjunction with all feature types built from the source language. The experiments also show that using propagated features in conjunction with lexicallyderived features only (as can be obtained directly from a mention annotated corpus) yields similar performance to using feature types derived from many linguistic resources. 
B is the de facto standard for evaluation and development of statistical machine translation systems. We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in B scores that are questionable or even absurd. These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modiﬁcation to B and a cross between B and word error rate that address these issues while improving correlation with human judgments. 
We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efﬁcient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisﬁes these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance. 
The conditional phrase translation probabilities constitute the principal components of phrase-based machine translation systems. These probabilities are estimated using a heuristic method that does not seem to optimize any reasonable objective function of the word-aligned, parallel training corpus. Earlier efforts on devising a better understood estimator either do not scale to reasonably sized training data, or lead to deteriorating performance. In this paper we explore a new approach based on three ingredients (1) A generative model with a prior over latent segmentations derived from Inversion Transduction Grammar (ITG), (2) A phrase table containing all phrase pairs without length limit, and (3) Smoothing as learning objective using a novel Maximum-A-Posteriori version of Deleted Estimation working with Expectation-Maximization. Where others conclude that latent segmentations lead to overﬁtting and deteriorating performance, we show here that these three ingredients give performance equivalent to the heuristic method on reasonably sized training data. 
We present a generative model for unsupervised coreference resolution that views coreference as an EM clustering process. For comparison purposes, we revisit Haghighi and Klein’s (2007) fully-generative Bayesian model for unsupervised coreference resolution, discuss its potential weaknesses and consequently propose three modiﬁcations to their model. Experimental results on the ACE data sets show that our model outperforms their original model by a large margin and compares favorably to the modiﬁed model. 
Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the ﬁrst unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classiﬁcation typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models. 
This paper investigates two strategies for improving coreference resolution: (1) training separate models that specialize in particular types of mentions (e.g., pronouns versus proper nouns) and (2) using a ranking loss function rather than a classiﬁcation function. In addition to being conceptually simple, these modiﬁcations of the standard single-model, classiﬁcation-based approach also deliver signiﬁcant performance improvements. Speciﬁcally, we show that on the ACE corpus both strategies produce f -score gains of more than 3% across the three coreference evaluation metrics (MUC, B3, and CEAF). 
We present a novel learning framework for pipeline models aimed at improving the communication between consecutive stages in a pipeline. Our method exploits the conﬁdence scores associated with outputs at any given stage in a pipeline in order to compute probabilistic features used at other stages downstream. We describe a simple method of integrating probabilistic features into the linear scoring functions used by state of the art machine learning algorithms. Experimental evaluation on dependency parsing and named entity recognition demonstrate the superiority of our approach over the baseline pipeline models, especially when upstream stages in the pipeline exhibit low accuracy. 
We present an algorithmic framework for learning multiple related tasks. Our framework exploits a form of prior knowledge that relates the output spaces of these tasks. We present PAC learning results that analyze the conditions under which such learning is possible. We present results on learning a shallow parser and named-entity recognition system that exploits our framework, showing consistent improvements over baseline methods.  
NLP tasks are often domain speciﬁc, yet systems can learn behaviors across multiple domains. We develop a new multi-domain online learning framework based on parameter combination from multiple classiﬁers. Our algorithms draw from multi-task learning and domain adaptation to adapt multiple source domain classiﬁers to a new target domain, learn across multiple similar domains, and learn across a large number of disparate domains. We evaluate our algorithms on two popular NLP domain adaptation tasks: sentiment classiﬁcation and spam ﬁltering. 
Previous work on ordering events in text has typically focused on local pairwise decisions, ignoring globally inconsistent labels. However, temporal ordering is the type of domain in which global constraints should be relatively easy to represent and reason over. This paper presents a framework that informs local decisions with two types of implicit global constraints: transitivity (A before B and B before C implies A before C) and time expression normalization (e.g. last month is before yesterday). We show how these constraints can be used to create a more densely-connected network of events, and how global consistency can be enforced by incorporating these constraints into an integer linear programming framework. We present results on two event ordering tasks, showing a 3.6% absolute increase in the accuracy of before/after classiﬁcation over a pairwise model. 
Chinese is a language that does not have morphological tense markers that provide explicit grammaticalization of the temporal location of situations (events or states). However, in many NLP applications such as Machine Translation, Information Extraction and Question Answering, it is desirable to make the temporal location of the situations explicit. We describe a machine learning framework where different sources of information can be combined to predict the temporal location of situations in Chinese text. Our experiments show that this approach signiﬁcantly outperforms the most frequent tense baseline. More importantly, the high training accuracy shows promise that this challenging problem is solvable to a level where it can be used in practical NLP applications with more training data, better modeling techniques and more informative and generalizable features. 
In this paper we present a machine learning system that ﬁnds the scope of negation in biomedical texts. The system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another that ﬁnds the full scope of these negation signals. Our approach to negation detection differs in two main aspects from existing research on negation. First, we focus on ﬁnding the scope of negation signals, instead of determining whether a term is negated or not. Second, we apply supervised machine learning techniques, whereas most existing systems apply rule-based algorithms. As far as we know, this way of approaching the negation scope ﬁnding task is novel. 
Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efﬁciently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice. Compared to N -best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show signiﬁcant runtime improvements and moderate BLEU score gains over N -best MERT. 
An important problem in translation neglected by most recent statistical machine translation systems is insertion and deletion of words, such as function words, motivated by linguistic structure rather than adjacent lexical context. Phrasal and hierarchical systems can only insert or delete words in the context of a larger phrase or rule. While this may suffice when translating in-domain, it performs poorly when trying to translate broad domains such as web text. Various syntactic approaches have been proposed that begin to address this problem by learning lexicalized and unlexicalized rules. Among these, the treelet approach uses unlexicalized order templates to model ordering separately from lexical choice. We introduce an extension to the latter that allows for structural word insertion and deletion, without requiring a lexical anchor, and show that it produces gains of more than 1.0% BLEU over both phrasal and baseline treelet systems on broad domain text. 
The performance of machine translation systems varies greatly depending on the source and target languages involved. Determining the contribution of different characteristics of language pairs on system performance is key to knowing what aspects of machine translation to improve and which are irrelevant. This paper investigates the effect of different explanatory variables on the performance of a phrase-based system for 110 European language pairs. We show that three factors are strong predictors of performance in isolation: the amount of reordering, the morphological complexity of the target language and the historical relatedness of the two languages. Together, these factors contribute 75% to the variability of the performance of the system. 
The graph-based ranking algorithm has been recently exploited for multi-document summarization by making only use of the sentence-to-sentence relationships in the documents, under the assumption that all the sentences are indistinguishable. However, given a document set to be summarized, different documents are usually not equally important, and moreover, different sentences in a specific document are usually differently important. This paper aims to explore document impact on summarization performance. We propose a document-based graph model to incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Various methods are employed to evaluate the two factors. Experimental results on the DUC2001 and DUC2002 datasets demonstrate that the good effectiveness of the proposed model. Moreover, the results show the robustness of the proposed model. 
Information of interest to users is often distributed over a set of documents. Users can specify their request for information as a query/topic – a set of one or more sentences or questions. Producing a good summary of the relevant information relies on understanding the query and linking it with the associated set of documents. To “understand” the query we expand it using encyclopedic knowledge in Wikipedia. The expanded query is linked with its associated documents through spreading activation in a graph that represents words and their grammatical connections in these documents. The topic expanded words and activated nodes in the graph are used to produce an extractive summary. The method proposed is tested on the DUC summarization data. The system implemented ranks high compared to the participating systems in the DUC competitions, conﬁrming our hypothesis that encyclopedic knowledge is a useful addition to a summarization system. 
In this paper we describe research on summarizing conversations in the meetings and emails domains. We introduce a conversation summarization system that works in multiple domains utilizing general conversational features, and compare our results with domain-dependent systems for meeting and email data. We ﬁnd that by treating meetings and emails as conversations with general conversational features in common, we can achieve competitive results with state-of-theart systems that rely on more domain-speciﬁc features. 
In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efﬁcient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 
Determining the polarity of a sentimentbearing expression requires more than a simple bag-of-words approach. In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity. In this paper, we view such subsentential interactions in light of compositional semantics, and present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%). We also ﬁnd that “contentword negators”, not widely employed in previous work, play an important role in determining expression-level polarity. Finally, in contrast to conventional wisdom, we ﬁnd that expression-level classiﬁcation accuracy uniformly decreases as additional, potentially disambiguating, context is considered. 
The alignment problem—establishing links between corresponding phrases in two related sentences—is as important in natural language inference (NLI) as it is in machine translation (MT). But the tools and techniques of MT alignment do not readily transfer to NLI, where one cannot assume semantic equivalence, and for which large volumes of bitext are lacking. We present a new NLI aligner, the MANLI system, designed to address these challenges. It uses a phrase-based alignment representation, exploits external lexical resources, and capitalizes on a new set of supervised training data. We compare the performance of MANLI to existing NLI and MT aligners on an NLI alignment task over the well-known Recognizing Textual Entailment data. We show that MANLI signiﬁcantly outperforms existing aligners, achieving gains of 6.2% in F1 over a representative NLI aligner and 10.5% over GIZA++. 
We introduce a method for solving substitution ciphers using low-order letter n-gram models. This method enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked. We carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment.  We ﬁnd the ideas in Shannon’s (1949) paper relevant to problems of statistical machine translation and transliteration. When ﬁrst exposed to the idea of statistical machine translation, many people naturally ask: (1) how much data is needed to get a good result, and (2) can translation systems be trained without parallel data? These are tough questions by any stretch, and it is remarkable that Shannon was already in the 1940s tackling such questions in the realm of code-breaking, creating analytic formulas to estimate answers. Our novel contributions are as follows:  
To improve the Mandarin large vocabulary continuous speech recognition (LVCSR), a uniﬁed framework based approach is introduced to exploit multi-level linguistic knowledge. In this framework, each knowledge source is represented by a Weighted Finite State Transducer (WFST), and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources. Due to the uniform transducer representation, any knowledge source can be easily integrated into the analyzer, as long as it can be encoded into WFSTs. Moreover, as the knowledge in each level is modeled independently and the combination is processed in the model level, the information inherently in each knowledge source has a chance to be thoroughly exploited. By simulations, the effectiveness of the analyzer is investigated, and then a LVCSR system embedding the presented analyzer is evaluated. Experimental results reveal that this uniﬁed framework is an effective approach which signiﬁcantly improves the performance of speech recognition with a 9.9% relative reduction of character error rate on the HUB-4 test set, a widely used Mandarin speech recognition task. 
In domains with insufﬁcient matched training data, language models are often constructed by interpolating component models trained from partially matched corpora. Since the ngrams from such corpora may not be of equal relevance to the target domain, we propose an n-gram weighting technique to adjust the component n-gram probabilities based on features derived from readily available segmentation and metadata information for each corpus. Using a log-linear combination of such features, the resulting model achieves up to a 1.2% absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task. 
Confusion networks are a simple representation of multiple speech recognition or translation hypotheses in a machine translation system. A typical operation on a confusion network is to ﬁnd the path which minimizes or maximizes a certain evaluation metric. In this article, we show that this problem is generally NP-hard for the popular BLEU metric, as well as for smaller variants of BLEU. This also holds for more complex representations like generic word graphs. In addition, we give an efﬁcient polynomial-time algorithm to calculate unigram BLEU on confusion networks, but show that even small generalizations of this data structure render the problem to be NP-hard again. Since ﬁnding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which ﬁnds a (not necessarily optimal) solution for n-gram BLEU in polynomial time. 
While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efﬁciency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically signiﬁcant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05). 
Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model. While bi-lingual parallel data are expensive to generate, monolingual data are relatively common. Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language. This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. The method exploits the existence of comparable text—multiple texts in the target language that discuss the same or similar stories as found in the source language document. For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to the source documents. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system. 
We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of reﬁnement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are reﬁned to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree conﬁgurations. We present a multiscale training method along with an efﬁcient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars. 
We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we deﬁne a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 
Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications. However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 
We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufﬁciently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words’ argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases. 
We consider a parsed text corpus as an instance of a labelled directed graph, where nodes represent words and weighted directed edges represent the syntactic relations between them. We show that graph walks, combined with existing techniques of supervised learning, can be used to derive a task-speciﬁc word similarity measure in this graph. We also propose a new path-constrained graph walk method, in which the graph walk process is guided by high-level knowledge about meaningful edge sequences (paths). Empirical evaluation on the task of named entity coordinate term extraction shows that this framework is preferable to vector-based models for smallsized corpora. It is also shown that the pathconstrained graph walk algorithm yields both performance and scalability gains. 
This paper presents a graph-theoretic model of the acquisition of lexical syntactic representations. The representations the model learns are non-categorical or graded. We propose a new evaluation methodology of syntactic acquisition in the framework of exemplar theory. When applied to the CHILDES corpus, the evaluation shows that the model’s graded syntactic representations perform better than previously proposed categorical representations. 
Question classiﬁcation plays an important role in question answering. Features are the key to obtain an accurate question classiﬁer. In contrast to Li and Roth (2002)’s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk’s word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%. 
An increasingly popular method for finding information online is via the Community Question Answering (CQA) portals such as Yahoo! Answers, Naver, and Baidu Knows. Searching the CQA archives, and ranking, filtering, and evaluating the submitted answers requires intelligent processing of the questions and answers posed by the users. One important task is automatically detecting the question’s subjectivity orientation: namely, whether a user is searching for subjective or objective information. Unfortunately, real user questions are often vague, ill-posed, poorly stated. Furthermore, there has been little labeled training data available for real user questions. To address these problems, we present CoCQA, a co-training system that exploits the association between the questions and contributed answers for question analysis tasks. The co-training approach allows CoCQA to use the effectively unlimited amounts of unlabeled data readily available in CQA archives. In this paper we study the effectiveness of CoCQA for the question subjectivity classification task by experimenting over thousands of real users’ questions. 
This paper explores the use of set expansion (SE) to improve question answering (QA) when the expected answer is a list of entities belonging to a certain class. Given a small set of seeds, SE algorithms mine textual resources to produce an extended list including additional members of the class represented by the seeds. We explore the hypothesis that a noise-resistant SE algorithm can be used to extend candidate answers produced by a QA system and generate a new list of answers that is better than the original list produced by the QA system. We further introduce a hybrid approach which combines the original answers from the QA system with the output from the SE algorithm. Experimental results for several state-of-the-art QA systems show that the hybrid system performs better than the QA systems alone when tested on list question data from past TREC evaluations. 
We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data. The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs. To represent a dialog for a learning purpose, we based our representation, the form-based dialog structure representation, on an observable structure. We show that this representation is sufficient for modeling phenomena that occur regularly in several dissimilar taskoriented domains, including informationaccess and problem-solving. With the goal of ultimately reducing human annotation effort, we examine the use of unsupervised learning techniques in acquiring the components of the form-based representation (i.e. task, subtask, and concept). These techniques include statistical word clustering based on mutual information and Kullback-Liebler distance, TextTiling, HMM-based segmentation, and bisecting K-mean document clustering. With some modifications to make these algorithms more suitable for inferring the structure of a spoken dialog, the unsupervised learning algorithms show promise. 
We introduce the relative rank differential statistic which is a non-parametric approach to document and dialog analysis based on word frequency rank-statistics. We also present a simple method to establish semantic saliency in dialog, documents, and dialog segments using these word frequency rank statistics. Applications of our technique include the dynamic tracking of topic and semantic evolution in a dialog, topic detection, automatic generation of document tags, and new story or event detection in conversational speech and text. Our approach benefits from the robustness, simplicity and efficiency of non-parametric and rank based approaches and consistently outperformed term-frequency and TF-IDF cosine distance approaches in several experiments conducted. 
Predicting possible code-switching points can help develop more accurate methods for automatically processing mixed-language text, such as multilingual language models for speech recognition systems and syntactic analyzers. We present in this paper exploratory results on learning to predict potential codeswitching points in Spanish-English. We trained different learning algorithms using a transcription of code-switched discourse. To evaluate the performance of the classiﬁers, we used two different criteria: 1) measuring precision, recall, and F-measure of the predictions against the reference in the transcription, and 2) rating the naturalness of artiﬁcially generated code-switched sentences. Average scores for the code-switched sentences generated by our machine learning approach were close to the scores of those generated by humans. 
Knowing the degree of antonymy between words has widespread applications in natural language processing. Manually-created lexicons have limited coverage and do not include most semantically contrasting word pairs. We present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus. The approach is evaluated on a set of closest-opposite questions, obtaining a precision of over 80%. Along the way, we discuss what humans consider antonymous and how antonymy manifests itself in utterances. 
Some phrases can be interpreted either idiomatically (ﬁguratively) or literally in context, and the precise identiﬁcation of idioms is indispensable for full-ﬂedged natural language processing (NLP). To this end, we have constructed an idiom corpus for Japanese. This paper reports on the corpus and the results of an idiom identiﬁcation experiment using the corpus. The corpus targets 146 ambiguous idioms, and consists of 102,846 sentences, each of which is annotated with a literal/idiom label. For idiom identiﬁcation, we targeted 90 out of the 146 idioms and adopted a word sense disambiguation (WSD) method using both common WSD features and idiomspeciﬁc features. The corpus and the experiment are the largest of their kind, as far as we know. As a result, we found that a standard supervised WSD method works well for the idiom identiﬁcation and achieved an accuracy of 89.25% and 88.86% with/without idiomspeciﬁc features and that the most effective idiom-speciﬁc feature is the one involving the adjacency of idiom constituents. 
The accuracy of current word sense disambiguation (WSD) systems is affected by the ﬁne-grained sense inventory of WordNet as well as a lack of training examples. Using the WSD examples provided through OntoNotes, we conduct the ﬁrst large-scale WSD evaluation involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory. We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop in accuracy when applied to a different domain. To address this issue, we propose combining a domain adaptation technique using feature augmentation with active learning. Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual beneﬁts of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types. 
Bootstrapping has a tendency, called semantic drift, to select instances unrelated to the seed instances as the iteration proceeds. We demonstrate the semantic drift of bootstrapping has the same root as the topic drift of Kleinberg’s HITS, using a simpliﬁed graphbased reformulation of bootstrapping. We conﬁrm that two graph-based algorithms, the von Neumann kernels and the regularized Laplacian, can reduce semantic drift in the task of word sense disambiguation (WSD) on Senseval-3 English Lexical Sample Task. Proposed algorithms achieve superior performance to Espresso and previous graph-based WSD methods, even though the proposed algorithms have less parameters and are easy to calibrate. 
Web-search queries are known to be short, but little else is known about their structure. In this paper we investigate the applicability of part-of-speech tagging to typical Englishlanguage web search-engine queries and the potential value of these tags for improving search results. We begin by identifying a set of part-of-speech tags suitable for search queries and quantifying their occurrence. We ﬁnd that proper-nouns constitute 40% of query terms, and proper nouns and nouns together constitute over 70% of query terms. We also show that the majority of queries are nounphrases, not unstructured collections of terms. We then use a set of queries manually labeled with these tags to train a Brill tagger and evaluate its performance. In addition, we investigate classiﬁcation of search queries into grammatical classes based on the syntax of part-of-speech tag sequences. We also conduct preliminary investigative experiments into the practical applicability of leveraging query-trained part-of-speech taggers for information-retrieval tasks. In particular, we show that part-of-speech information can be a signiﬁcant feature in machine-learned searchresult relevance. These experiments also include the potential use of the tagger in selecting words for omission or substitution in query reformulation, actions which can improve recall. We conclude that training a partof-speech tagger on labeled corpora of queries signiﬁcantly outperforms taggers based on traditional corpora, and leveraging the unique linguistic structure of web-search queries can improve search experience.  
 Formal  Informal  We present a novel method for discovering and modeling the relationship between informal Chinese expressions (including colloquialisms and instant-messaging slang) and their formal equivalents. Speciﬁcally, we proposed a bootstrapping procedure to identify a list of candidate informal phrases in web corpora. Given an informal phrase, we retrieve contextual instances from the web using a search engine, generate hypotheses of formal equivalents via this data, and rank the hypotheses using a conditional log-linear model. In the log-linear model, we incorporate as feature functions both rule-based intuitions and data co-occurrence phenomena (either as an explicit or indirect deﬁnition, or through formal/informal usages occurring in free variation in a discourse). We test our system on manually collected test examples, and ﬁnd that the (formal-informal) relationship discovery and extraction process using our method achieves an average 1-best precision of 62%. Given the ubiquity of informal conversational style on the internet, this work has clear applications for text normalization in text-processing systems including machine translation aspiring to broad coverage. 
We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent. We formulate a hierarchical Bayesian model for jointly predicting bilingual streams of part-of-speech tags. The model learns language-speciﬁc features while capturing cross-lingual patterns in tag distribution for aligned words. Once the parameters of our model have been learned on bilingual parallel data, we evaluate its performance on a held-out monolingual test set. Our evaluation on six pairs of languages shows consistent and signiﬁcant performance gains over a state-of-the-art monolingual baseline. For one language pair, we observe a relative reduction in error of 53%. 
Code-switching is an interesting linguistic phenomenon commonly observed in highly bilingual communities. It consists of mixing languages in the same conversational event. This paper presents results on Part-of-Speech tagging Spanish-English code-switched discourse. We explore different approaches to exploit existing resources for both languages that range from simple heuristics, to language identiﬁcation, to machine learning. The best results are achieved by training a machine learning algorithm with features that combine the output of an English and a Spanish Partof-Speech tagger. 
This paper presents a novel, ranking-style word segmentation approach, called RSVMSeg, which is well tailored to Chinese information retrieval(CIR). This strategy makes segmentation decision based on the ranking of the internal associative strength between each pair of adjacent characters of the sentence. On the training corpus composed of query items, a ranking model is learned by a widely-used tool Ranking SVM, with some useful statistical features, such as mutual information, difference of t-test, frequency and dictionary information. Experimental results show that, this method is able to eliminate overlapping ambiguity much more effectively, compared to the current word segmentation methods. Furthermore, as this strategy naturally generates segmentation results with different granularity, the performance of CIR systems is improved and achieves the state of the art. 
Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art. 
String-to-string transduction is a central problem in computational linguistics and natural language processing. It occurs in tasks as diverse as name transliteration, spelling correction, pronunciation modeling and inﬂectional morphology. We present a conditional loglinear model for string-to-string transduction, which employs overlapping features over latent alignment sequences, and which learns latent classes and latent string pair regions from incomplete training data. We evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results, even when trained on small data sets. On the task of generating morphological forms, we outperform a baseline method reducing the error rate by up to 48%. On a lemmatization task, we reduce the error rates in Wicentowski (2002) by 38–92%. 
We propose a new graph-based semisupervised learning (SSL) algorithm and demonstrate its application to document categorization. Each document is represented by a vertex within a weighted undirected graph and our proposed framework minimizes the weighted Kullback-Leibler divergence between distributions that encode the class membership probabilities of each vertex. The proposed objective is convex with guaranteed convergence using an alternating minimization procedure. Further, it generalizes in a straightforward manner to multi-class problems. We present results on two standard tasks, namely Reuters-21578 and WebKB, showing that the proposed algorithm signiﬁcantly outperforms the state-of-the-art. 
This paper presents a fully operational real-time event extraction system which is capable of accurately and efﬁciently extracting violent and natural disaster events from vast amount of online news articles per day in different languages. Due to the requirement that the system must be multilingual and easily extendable, it is based on a shallow linguistic analysis. The event extraction results can be viewed on a publicly accessible website. 
This article provides description about the grammar checking system developed for detecting various grammatical errors in Punjabi texts. This system utilizes a fullform lexicon for morphological analysis, and applies rule-based approaches for part-of-speech tagging and phrase chunking. The system follows a novel approach of performing agreement checks at phrase and clause levels using the grammatical information exhibited by POS tags in the form of feature value pairs. The system can detect and suggest rectifications for a number of grammatical errors, resulting from the lack of agreement, order of words in various phrases etc., in literary style Punjabi texts. To the best of our knowledge, this grammar checking system is the first such system reported for Indian languages. 
We present a chain of tools used by grammarians and computer scientists to develop grammatical and lexical resources from linguistic knowledge, for various natural languages. The developed resources are intended to be used in Natural Language Processing (NLP) systems. 
Punjabi and Hindi are two closely related languages as both originated from the same origin and having lot of syntactic and semantic similarities. These similarities make direct translation methodology an obvious choice for Punjabi-Hindi language pair. The purposed system for Punjabi to Hindi translation has been implemented with various research techniques based on Direct MT architecture and language corpus. The output is evaluated by already prescribed methods in order to get the suitability of the system for the Punjabi Hindi language pair.  concerned, Punjabi and Hindi are mutually intelligible to certain degree. This relation is further asymmetric with the speakers of Punjabi more able to understand Hindi but reverse is not true. 1.1 Punjabi Language Punjabi is the official language of the Indian state of Punjab and also one of the official languages of Delhi. It is used in government, education, commerce, art, mass media and in every day communication. A good deal of Sikh religious literature is written in Punjabi language. According to SIL Ethnologue, Punjabi is the language of about 57 million people and ranked 20th among the total languages of the world. It is written in Gurmukhy, Shahmukhy and roman scripts.  1. Introduction  1.2 Hindi Language  The Direct MT system is based upon exploitation of syntactic similarities between more or less related natural languages. Although its deficiencies soon became apparent, it remains popular in certain situations due to its usefulness, robustness and relative simplicity. One of such situation is machine translation of closely related languages. The general opinion is that it is easier to create an MT system for a pair of related languages (Hajic et.al. 2000). In the last decade, some of the systems utilizing this approach for translating between similar languages have confirmed this concept. In this paper, our attempt to use the same concept for language pair of Punjabi-Hindi is described. Punjabi and Hindi both are classified as Indo-Iranian languages. Although they are in the same family, but still they have lot of differences in order to make them not mutually intelligible. Punjabi and Hindi are not mutually intelligible in written form. As far as spoken form is © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved.  Hindi on the other hand has been one of the two official languages of all of India. Hindi is a language of about 577 million peoples all over the world and is ranked as 5th most widely spoken language by SIL Ethnologue. 2. The Need India being a large and multilingual society, and in the interest of the regional languages, the government of India has allowed to use regional languages as the official language of respective region and adopt bilingual form (Hindi/English) as the official language of Union Government. Most of the state governments work in their respective regional languages whereas the union government’s official documents and reports are in bilingual form (Hindi/English). In order to have a proper communication there is a need to translate these reports and documents in the respective regional languages and vice versa. Some other applications of Punjabi to Hindi MT system are Text Translation, Website Translation, Message Translation (Email), Cross Language Information Retrieval and Web Service.  157 
Building effective spoken dialogue systems (SDS) is currently a complex task requiring expert knowledge. Our tools give control of SDS application development to non-experts, who need only use a Graphical User Interface or GUI to develop state-of-the-art “Information State Update” (ISU) dialogue systems. Behind the GUI is a set of Advanced Dialogue Tools (ADT) that generate complete SDS based on Business User Resources. These resources include a database and a Process Model that captures the structure of an application, for example, banking or restaurant information. Also generated are speech recognition Language Models and grammars for robust interpretation of spontaneous speech. We will demonstrate how our simple GUI allows developers to easily and quickly create and modify SDS without the need for expensive speech application service providers. This demonstration shows the interface, the ADT components, and discusses some of the research issues involved. We also show an example application built with the tools: a tourist information system running on an ultra-mobile PC. 
 This demonstration introduces two new multilingual translation services for mobile phones. The ﬁrst translation service provides state-of-the-art text-to-text translations of Japanese as well as English conversational spoken language in the travel domain into 17 languages using statistical machine translation technologies trained automatically from a large-scale multilingual corpus. The second demonstration is a speech translation service between Japanese and English for real environments. It is based on distributed speech recognition with noise suppression. Flexible interfaces between internal and external speech translation resources ease the portability of the system to other languages and enable real-time location-free communication world-wide. 
This paper presents a real-world application for assisting medical diagnosis and drug prescription, which relies on the exclusive use of machine learning techniques. We have automatically processed an extensive biomedical literature to train a categorization algorithm in order to provide it with the capability of matching symptoms to MeSH descriptors. To interact with the classiﬁer, we have developed a multilingual web interface so that professionals in medicine can easily get some help in their decisions about diagnoses (lookfordiagnosis.com) and prescriptions (lookfortherapy.com). We also demonstrate the effectiveness of this approach with a test set containing several hundreds of real clinical histories. 
This paper describes a Question Answering system which retrieves answers from structured data regarding cinemas and movies. The system represents the first prototype of a multilingual and multimodal QA system for the domain of tourism. Based on specially designed domain ontology and using Textual Entailment as a means for semantic inference, the system can be used in both monolingual and cross-language settings with slight adjustments for new input languages. 
The existence of two scripts for Punjabi language has created a script barrier between the Punjabi literature written in India and Pakistan. This research has developed a new system for the first time of its kind for Shahmukhi text without diacritical marks. The purposed system for Shahmukhi to Gurmukhi transliteration has been implemented with various research techniques based on language corpus. The corpus analysis of both scripts is performed for generating statistical data of different types like character and word frequencies and bi-gram frequencies. This statistical analysis is used in different phases of transliteration. Potentially, all members of the substantial Punjabi community will benefit vastly from this transliteration system. 
In this paper, we will describe a search tool for a huge set of ngrams. The tool supports queries with an arbitrary number of wildcards. It takes a fraction of a second for a search, and can provide the fillers of the wildcards. The system runs on a single Linux PC with reasonable size memory (less than 4GB) and disk space (less than 400GB). This system can be a very useful tool for linguistic knowledge discovery and other NLP tasks. 
We present here VISUSYN, a prototype we developed in order to study meaning construction. This software implements the model of dynamic construction of meaning proposed by Victorri and Fuchs (1996). It allows a semantic visualization that can be used to compute the meaning of a lexical unit in a given context  allows us to determinate the region of the semantic space corresponding to the meaning of the unit studied within the utterance.  
We present the TARSQI Toolkit (TTK), a modular system for automatic temporal and event annotation of natural language texts. TTK identiﬁes temporal expressions and events in natural language texts, and parses the document to order events and to anchor them to temporal expressions. 
Metaphor understanding in Computational Linguistics has largely been focused on the development of stand-alone prototypes for which only small-scale evaluations are carried out. This has made difﬁcult the inclusion of metaphor in the development of natural language processing applications. However, dealing with metaphor properly is ultimately crucial for any automated language technology that is to be truly human-friendly or able to properly appreciate utterances by humans. This paper proposes to bring metaphor into the Recognizing Textual Entailment task. By doing so, the coverage of textual entailment systems would be broadened and metaphor research would beneﬁt from the textual entailment evaluation framework. 
In this paper, we describe a preliminary study for a discourse based opinion categorization and propose a new annotation schema for a deep contextual opinion analysis using discourse relations. 
We deﬁne the task of incremental or 0lag utterance segmentation, that is, the task of segmenting an ongoing speech recognition stream into utterance units, and present ﬁrst results. We use a combination of hidden event language model, features from an incremental parser, and acoustic / prosodic features to train classiﬁers on real-world conversational data (from the Switchboard corpus). The best classiﬁers reach an F-score of around 56%, improving over baseline and related work. 
Treating classiﬁcation as seeking minimum cuts in the appropriate graph has proven effective in a number of applications. The power of this approach lies in its ability to incorporate label-agreement preferences among pairs of instances in a provably tractable way. Label disagreement preferences are another potentially rich source of information, but prior NLP work within the minimum-cut paradigm has not explicitly incorporated it. Here, we report on work in progress that examines several novel heuristics for incorporating such information. Our results, produced within the context of a politically-oriented sentiment-classiﬁcation task, demonstrate that these heuristics allow for the addition of label-disagreement information in a way that improves classiﬁcation accuracy while preserving the efﬁciency guarantees of the minimum-cut framework. 
Phrasal segmentation models deﬁne a mapping from the words of a sentence to sequences of translatable phrases. We discuss the estimation of these models from large quantities of monolingual training text and describe their realization as weighted ﬁnite state transducers for incorporation into phrase-based statistical machine translation systems. Results are reported on the NIST Arabic-English translation tasks showing signiﬁcant complementary gains in BLEU score with large 5-gram and 6-gram language models. 
We present SMMR, a scalable sentence scoring method for query-oriented update summarization. Sentences are scored thanks to a criterion combining query relevance and dissimilarity with already read documents (history). As the amount of data in history increases, non-redundancy is prioritized over query-relevance. We show that SMMR achieves promising results on the DUC 2007 update corpus. 
We analyse Hindi complex predicates and propose linguistic tests for their detection. This analysis enables us to identify a category of V+V complex predicates called lexical compound verbs (LCpdVs) which need to be stored in the dictionary. Based on the linguistic analysis, a simple automatic method has been devised for extracting LCpdVs from corpora. We achieve an accuracy of around 98% in this task. The LCpdVs thus extracted may be used to automatically augment lexical resources like wordnets, an otherwise time consuming and labourintensive process 
This work presents the development of a system that detects incorrect uses of complex postpositions in Basque, an agglutinative language. Error detection in complex postpositions is interesting because: 1) the context of detection is limited to a few words; 2) it implies the interaction of multiple levels of linguistic processing (morphology, syntax and semantics). So, the system must deal with problems ranging from tokenization and ambiguity to syntactic agreement and examination of local contexts. The evaluation was performed in order to test both incorrect uses of postpositions and also false alarms.1 
We introduce a new type of discourse constraints for the interaction of discourse relations with the conﬁguration of discourse segments. We examine corpus-extracted examples as soft constraints. We show how to use Regular Tree Gramamrs to process such constraints, and how the representation of some constraints depends on the expressive power of this formalism. 
Language resource quality is crucial in NLP. Many of the resources used are derived from data created by human beings out of an NLP context, especially regarding MT and reference translations. Indeed, automatic evaluations need highquality data that allow the comparison of both automatic and human translations. The validation of these resources is widely recommended before being used. This paper describes the impact of using different-quality references on evaluation. Surprisingly enough, similar scores are obtained in many cases regardless of the quality. Thus, the limitations of the automatic metrics used within MT are also discussed in this regard. 
We propose a supervised word sense disambiguation (WSD) method using tree-structured conditional random ﬁelds (TCRFs). By applying TCRFs to a sentence described as a dependency tree structure, we conduct WSD as a labeling problem on tree structures. To incorporate dependencies between word senses, we introduce a set of features on tree edges, in combination with coarse-grained tagsets, and show that these contribute to an improvement in WSD accuracy. We also show that the tree-structured model outperforms the linear-chain model. Experiments on the SENSEVAL-3 data set show that our TCRF model performs comparably with state-of-the-art WSD systems. 
In this paper, we explore a conceptual resource for Chinese nominal phrases, which allows multi-dependency and distinction between dependency and the corresponding exact relation. We also provide an ILP-based method to learn mapping rules from training data, and use the rules to analyze new nominal phrases. 
Recent years have witnessed a growing interest in analogical learning for NLP applications. If the principle of analogical learning is quite simple, it does involve complex steps that seriously limit its applicability, the most computationally demanding one being the identiﬁcation of analogies in the input space. In this study, we investigate different strategies for efﬁciently solving this problem and study their scalability. 
We propose a method to obtain subsentential alignments from several languages simultaneously. The method handles several languages at once, and avoids the complexity explosion due to the usual pair-bypair processing. It can be used for different units (characters, morphemes, words, chunks). An evaluation of word alignments with a trilingual machine translation corpus has been conducted. A comparison of the results with those obtained by state of the art alignment software is reported. 
We propose an efﬁcient dialogue management for an information navigation system based on a document knowledge base with a spoken dialogue interface. In order to perform robustly for fragmental speech input and erroneous output of an automatic speech recognition (ASR), the system should selectively use N-best hypotheses of ASR and contextual information. The system also has several choices in generating responses or conﬁrmations. In this work, we formulate the optimization of the choices based on a uniﬁed criterion: Bayes risk, which is deﬁned based on reward for correct information presentation and penalty for redundant turns. We have evaluated this strategy with a spoken dialogue system which also has questionanswering capability. Effectiveness of the proposed framework was conﬁrmed in the success rate of retrieval and the average number of turns. 
This paper describes a parameter estimation method for multi-label classiﬁcation that does not rely on approximate inference. It is known that multi-label classiﬁcation involving label correlation features is intractable, because the graphical model for this problem is a complete graph. Our solution is to exploit the sparsity of features, and express a model structure for each object by using a sparse graph. We can thereby apply the junction tree algorithm, allowing for efﬁcient exact inference on sparse graphs. Experiments on three data sets for text categorization demonstrated that our method increases the accuracy for text categorization with a reasonable cost. 
Multilinguality in ontologies has become an impending need for institutions worldwide with valuable linguistic resources in different natural languages. Since most ontologies are developed in one language, obtaining multilingual ontologies implies to localize or adapt them to a concrete language and culture community. As the adaptation of the ontology conceptualization demands considerable efforts, we propose to modify the ontology terminological layer, and provide a model called Linguistic Information Repository (LIR) that associated to the ontology meta-model allows terminological layer localization. 
This paper elaborates a model for representing semantic calendar expressions (SCEs), which correspond to the intensional meanings of natural-language calendar phrases. The model uses ﬁnite-state transducers (FSTs) to mark denoted periods of time on a set of timelines represented as a ﬁnite-state automaton (FSA). We present a treatment of SCEs corresponding to quantiﬁed phrases (any Monday; every May) and an implication operation for requiring the denotation of one SCE to contain completely that of another. 
We report on work in progress on using very simple statistics in an unsupervised fashion to re-rank search engine results when review-oriented queries are issued; the goal is to bring opinionated or subjective results to the top of the results list. We ﬁnd that our proposed technique performs comparably to methods that rely on sophisticated pre-encoded linguistic knowledge, and that both substantially improve the initial results produced by the Yahoo! search engine.  search results by placing those that have the least idiosyncratic term distributions, with respect to the statistics of the top k results, at the head of the list. The fact that it is the least, not the most, rare terms with respect to the search results that are most indicative of subjectivity may at ﬁrst seem rather counterintuitive; indeed, previous work has found rare terms to be important subjectivity cues (Wiebe et al., 2004). However, reviews within a given set of search results may tend to resemble each other because they tend to all discuss salient attributes of the topic in question. 2 Algorithm  
Reasoning about how much to generate when space is limited is a challenge for generation systems. This paper presents two algorithms that exploit the discourse structure to decide which content to drop when there are space restrictions, in the context of producing documents from pre-authored text fragments. We analyse the effectiveness of both algorithms and show that the second is near optimal. 
We present Likey, a language-independent keyphrase extraction method based on statistical analysis and the use of a reference corpus. Likey has a very light-weight preprocessing phase and no parameters to be tuned. Thus, it is not restricted to any single language or language family. We test Likey having exactly the same conﬁguration with 11 European languages. Furthermore, we present an automatic evaluation method based on Wikipedia intra-linking. 
We present a corpus study of local discourse relations based on the Penn Discourse Tree Bank, a large manually annotated corpus of explicitly or implicitly realized relations. We show that while there is a large degree of ambiguity in temporal explicit discourse connectives, overall connectives are mostly unambiguous and allow high-accuracy prediction of discourse relation type. We achieve 93.09% accuracy in classifying the explicit relations and 74.74% accuracy overall. In addition, we show that some pairs of relations occur together in text more often than expected by chance. This ﬁnding suggests that global sequence classiﬁcation of the relations in text can lead to better results, especially for implicit relations. 
In this paper we propose a new distance function (rank distance) designed to reﬂect stylistic similarity between texts. To assess the ability of this distance measure to capture stylistic similarity between texts, we tested it in two different machine learning settings: clustering and binary classiﬁcation. 
We propose a spatio-temporal markup for the annotation of motion predicates in text, informed by a lexical semantic classiﬁcation of these verbs. We incorporate this classiﬁcation within a spatial event structure, based on Generative Lexicon Theory. We discuss how the spatial event structure suggests changes to annotation systems designed solely for temporal or spatial phenomena, resulting in spatio-temporal annotation. 
It is shown how weighted context-free grammars can be used to recognize languages beyond their weak generative capacity by a one-step constant time extension of standard recognition algorithms. 
Positive and bottom-up non-erasing binary range concatenation grammars (Boullier, 1998) with at most binary predicates ((2,2)-BRCGs) is a O(|G|n6) time strict extension of inversion transduction grammars (Wu, 1997) (ITGs). It is shown that (2,2)-BRCGs induce inside-out alignments (Wu, 1997) and cross-serial discontinuous translation units (CDTUs); both phenomena can be shown to occur frequently in many hand-aligned parallel corpora. A CYK-style parsing algorithm is introduced, and induction from aligment structures is brieﬂy discussed. Range concatenation grammars (RCG) (Boullier, 1998) mainly attracted attention in the formal language community, since they recognize exactly the polynomial time recognizable languages, but recently they have been argued to be useful for data-driven parsing too (Maier and Søgaard, 2008). Bertsch and Nederhof (2001) present the only work to our knowledge on using RCGs for translation. Both Bertsch and Nederhof (2001) and Maier and Søgaard (2008), however, only make use of so-called simple RCGs, known to be equivalent to linear context-free rewrite systems (LCFRSs) (Weir, 1988; Boullier, 1998). Our strict extension of ITGs, on the other hand, makes use of the ability to copy substrings in RCG derivations; one of the things that makes RCGs strictly more expressive than LCFRSs. Copying enables us to recognize the intersection of any two translations that we can recognize and induce the union c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.  of any two alignment structures that we can induce. Our extension of ITGs in fact introduces two things: (i) A clause may introduce any number of terminals. This enables us to induce multiword translation units. (ii) A clause may copy a substring, i.e. a clause can associate two or more nonterminals A1, . . . An with the same substring and thereby check if the substring is in the intersection of the languages of the subgrammars with start predicate names A1, . . . An. The ﬁrst point is motivated by studies such as Zens and Ney (2003) and simply reﬂects that in order to induce multiword translation units in this kind of synchronous grammars, it is useful to be able to introduce multiple terminals simultaneously. The second point gives us a handle on context-sensitivity. It means that (2,2)-BRCGs can deﬁne translations such as { anbmcndm, anbmdmcn | m, n ≥ 0}, i.e. a translation of cross-serial dependencies into nested ones; but it also means that (2,2)-BRCGs induce a larger class of alignment structures. In fact the set of alignment structures that can be induced is closed under union, i.e. any alignment structure can be induced. The last point is of practical interest. It is shown below that phenomena such as inside-out alignments and CDTUs, which cannot be induced by ITGs, but by (2,2)-BRCGs, occur frequently in many hand-aligned parallel corpora. 
Arabic morphological analysers and stemming algorithms have become a popular area of research. Many computational linguists have designed and developed algorithms to solve the problem of morphology and stemming. Each researcher proposed his own gold standard, testing methodology and accuracy measurements to test and compute the accuracy of his algorithm. Therefore, we cannot make comparisons between these algorithms. In this paper we have accomplished two tasks. First, we proposed four different fair and precise accuracy measurements and two 1000-word gold standards taken from the Holy Qur’an and from the Corpus of Contemporary Arabic. Second, we combined the results from the morphological analysers and stemming algorithms by voting after running them on the sample documents. The evaluation of the algorithms shows that Arabic morphology is still a challenge. 
We present a complete system that generates Japanese stand-up comedy. Different modules generating different types of jokes are tied together into a performance where all jokes are connected in some way to the other jokes. The script is converted to speech and two robots perform the comedy routine. Evaluations show that the performances are perceived as funny by many, almost half the evaluation scores for the total impression were 4 or 5 (top score). 
We seek to develop an efﬁcient algorithm selecting attributes that approximates human selection. In contrast to previous work we sought to combine the strengths of cognitive theories and simple learning algorithms. We then developed a new algorithm for attribute selection based on observations from a corpus, which outperformed a simple base algorithm by a signiﬁcant margin. We then carried out a detailed comparison between our algorithm and Reiter & Dale’s “Incremental Algorithm”. In terms of achieving a human-like attribute selection, the overall performance of both algorithms is fundamentally equivalent, while differing in the handling of redundancy in selected attributes. We further investigated this phenomenon and draw some conclusions for further improvement of attribute-selection algorithms. 
Our research organization has been constructing a large scale database named SHACHI by collecting detailed meta information on language resources (LRs) in Asia and Western countries. The metadata database contains more than 2,000 compiled LRs such as corpora, dictionaries, thesauruses and lexicons, forming a large scale metadata of LRs archive. Its metadata, an extended version of OLAC metadata set conforming to Dublin Core, have been collected semi-automatically. This paper explains the design and the structure of the metadata database, as well as the realization of the catalogue search tool. 
This paper studies the role of base-NP information in dependency parsing for Thai. The baseline performance reveals that the base-NP chunking task for Thai is much more difﬁcult than those of some languages (like English). The results show that the parsing performance can be improved (from 60.30% to 63.74%) with the use of base-NP chunk information, although the best chunker is still far from perfect (Fβ=1 = 83.06%). 
This paper proposes a novel method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT). Given two bilingual lexicons between language pairs Lf –Lp and Lp–Le, we assume these lexicons as parallel corpora. Then, we merge the extracted two phrase tables into one phrase table between Lf and Le. Finally, we construct a phrase-based SMT system for translating the terms in the lexicon Lf –Lp into terms of Le and, obtain a new lexicon Lf –Le. In our experiments with Chinese-English and JapaneseEnglish lexicons, our system could cover 72.8% of Chinese terms and drastically improve the utilization ratio. 
Computing the similarity between entities is a core component of many NLP tasks such as measuring the semantic similarity of terms for generating a distributional thesaurus. In this paper, we study the problem of explaining post-hoc why a set of terms are similar. Given a set of terms, our task is to generate a small set of explanations that best characterizes the similarity of those terms. Our contributions include: 1) an information-theoretic objective function for quantifying the utility of an explanation set; 2) a survey of psycholinguistics and philosophy for evidence of different sources of explanations such as descriptive properties and prototypes; 3) computational baseline models for automatically generating various types of explanations; and 4) a qualitative evaluation of our explanation generation engine. 
Data-driven learning based on shift reduce parsing algorithms has emerged dependency parsing and shown excellent performance to many Treebanks. In this paper, we investigate the extension of those methods while considerably improved the runtime and training time efficiency via L2SVMs. We also present several properties and constraints to enhance the parser completeness in runtime. We further integrate root-level and bottom-level syntactic information by using sequential taggers. The experimental results show the positive effect of the root-level and bottom-level features that improve our parser from 81.17% to 81.41% and 81.16% to 81.57% labeled attachment scores with modified Yamada’s and Nivre’s method, respectively on the Chinese Treebank. In comparison to well-known parsers, such as MaltParser (80.74%) and MSTParser (78.08%), our methods produce not only better accuracy, but also drastically reduced testing time in 0.07 and 0.11, respectively. 
The originality of this work leads in tackling text compression using an unsupervised method, based on a deep linguistic analysis, and without resorting on a learning corpus. This work presents a system for dependent tree pruning, while preserving the syntactic coherence and the main informational contents, and led to an operational software, named COLIN. Experiment results show that our compressions get honorable satisfaction levels, with a mean compression ratio of 38 %. 
Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We then propose a twophased approach, which ﬁrst uses lexicosyntactic patterns to acquire predicate pairs and then uses two types of anchors to identify shared arguments. The present results of our empirical evaluation on a large-scale Japanese Web corpus have shown that (a) the anchor-based ﬁltering extensively improves the accuracy of predicate pair acquisition, (b) the two types of anchors are almost equally contributive and combining them improves recall without losing accuracy, and (c) the anchor-based method also achieves high accuracy in shared argument identiﬁcation. 
VerbNet (VN) is a major large-scale English verb lexicon. Mapping verb instances to their VN classes has been proven useful for several NLP tasks. However, verbs are polysemous with respect to their VN classes. We introduce a novel supervised learning model for mapping verb instances to VN classes, using rich syntactic features and class membership constraints. We evaluate the algorithm in both in-domain and corpus adaptation scenarios. In both cases, we use the manually tagged Semlink WSJ corpus as training data. For indomain (testing on Semlink WSJ data), we achieve 95.9% accuracy, 35.1% error reduction (ER) over a strong baseline. For adaptation, we test on the GENIA corpus and achieve 72.4% accuracy with 10.7% ER. This is the ﬁrst large-scale experimentation with automatic algorithms for this task. 
In this paper we explore robustness and domain adaptation issues for Word Sense Disambiguation (WSD) using Singular Value Decomposition (SVD) and unlabeled data. We focus on the semi-supervised domain adaptation scenario, where we train on the source corpus and test on the target corpus, and try to improve results using unlabeled data. Our method yields up to 16.3% error reduction compared to state-of-the-art systems, being the ﬁrst to report successful semi-supervised domain adaptation. Surprisingly the improvement comes from the use of unlabeled data from the source corpus, and not from the target corpora, meaning that we get robustness rather than domain adaptation. In addition, we study the behavior of our system on the target domain. 
This paper addresses the fundamental problem of document classiﬁcation, and we focus attention on classiﬁcation problems where the classes are mutually exclusive. In the course of the paper we advocate an approximate sampling distribution for word counts in documents, and demonstrate the model’s capacity to outperform both the simple multinomial and more recently proposed extensions on the classiﬁcation task. We also compare the classiﬁers to a linear SVM, and show that provided certain conditions are met, the new model allows performance which exceeds that of the SVM and attains amongst the very best published results on the Newsgroups classiﬁcation task. 
The state-of-the-art system combination method for machine translation (MT) is the word-based combination using confusion networks. One of the crucial steps in confusion network decoding is the alignment of different hypotheses to each other when building a network. In this paper, we present new methods to improve alignment of hypotheses using word synonyms and a two-pass alignment strategy. We demonstrate that combination with the new alignment technique yields up to 2.9 BLEU point improvement over the best input system and up to 1.3 BLEU point improvement over a state-of-the-art combination method on two different language pairs. 
 sion when compared to the performance of human taggers on the same subset.  Our goal is to use natural language processing to identify deceptive and nondeceptive passages in transcribed narratives. We begin by motivating an analysis of language-based deception that relies on specific linguistic indicators to discover deceptive statements. The indicator tags are assigned to a document using a mix of automated and manual methods. Once the tags are assigned, an interpreter automatically discriminates between deceptive and truthful statements based on tag densities. The texts used in our study come entirely from “real world” sources—criminal statements, police interrogations and legal testimony. The corpus was hand-tagged for the truth value of all propositions that could be externally verified as true or false. Classification and Regression Tree techniques suggest that the approach is feasible, with the model able to identify 74.9% of the T/F propositions correctly. Implementation of an automatic tagger with a large subset of tags performed well on test data, producing an average score of 68.6% recall and 85.3% preci- © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved.  1. Introduction The ability to detect deceptive statements in text and speech has broad applications in law enforcement and intelligence gathering. The scientific study of deception in language dates at least from Undeutsch (1954, 1989), who hypothesized that it is “not the veracity of the reporting person but the truthfulness of the statement that matters and there are certain relatively exact, definable, descriptive criteria that form a key tool for the determination of the truthfulness of statements”. Reviews by Shuy (1998), Vrij (2000), and DePaulo et al. (2003) indicate that many types of deception can be identified because the liar’s verbal and non-verbal behavior varies considerably from that of the truth teller’s. Even so, the literature reports that human lie detectors rarely perform at a level above chance. Vrij (2000) gives a summary of 39 studies of human ability to detect lies. The majority of the studies report accuracy rates between 45-60%, with the mean accuracy rate at 56.6%. The goal of our research is to develop and implement a system for automatically identifying deceptive and truthful statements in narratives and transcribed interviews. We focus exclusively on verbal cues to deception for this initial experiment, ignoring at present potential prosodic cues (but see Hirschberg et al.).  41 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 41–48 Manchester, August 2008  In this paper, we describe a language-based analysis of deception that we have constructed and tested using “real world” sources—criminal narratives, police interrogations and legal testimony. Our analysis comprises two components: a set of deception indicators that are used for tagging a document and an interpreter that associates tag clusters with a deception likelihood. We tested the analysis by identifying propositions in the corpus that could be verified as true or false and then comparing the predictions of our model against this corpus of ground truth. Our analysis acheived an accuracy rate of 74.9%. In the remainder of this paper, we will present the analysis and a detailed description of our test results. Implementation of the analysis will also be discussed. 2. Studying Deception The literature on deception comes primarily from experimental psychology where much of the concentration is on lies in social life and much of the experimentation is done in laboratory settings where subjects are prompted to lie1. These studies lack the element of deception under stress. Because of the difficulties of collecting and corroborating testimony in legal settings, analysis of so-called ‘high stakes’ data is harder to come by. To our knowledge, only two studies (Smith, 2001; Adams, 2002) correlate linguistic cues with deception using high stakes data. For our data we have relied exclusively on police department transcripts and high profile cases where the ground truth facts of the case can be established. Previous studies correlating linguistic features with deceptive behavior (Smith, 2001; Adams, 2002; Newman et al. 2003, and studies cited in DePaulo et al. 2003) have classified narrators as truth-tellers or liars according to the presence, number and distribution of deception indicators in their narratives. Newman, et al. (2003), for example, proposes an analysis based on word likelihoods for semantically defined items such as action verbs, negative emotion words and pronouns. Narratives for their study were generated in the laboratory by student subjects. The goals of the project were to determine how well their word likelihood analysis classified the presumed author of each narrative as a liar or truth-teller and to compare their system's performance to that of human subjects. The analysis correctly 
Latent Semantic Analysis (LSA) is based on the Singular Value Decomposition (SVD) of a term-by-document matrix for identifying relationships among terms and documents from cooccurrence patterns. Among the multiple ways of computing the SVD of a rectangular matrix X, one approach is to compute the eigenvalue decomposition (EVD) of a square 2 × 2 composite matrix consisting of four blocks with X and XT in the off-diagonal blocks and zero matrices in the diagonal blocks. We point out that significant value can be added to LSA by filling in some of the values in the diagonal blocks (corresponding to explicit term-to-term or document-to-document associations) and computing a term-by-concept matrix from the EVD. For the case of multilingual LSA, we incorporate information on cross-language term alignments of the same sort used in Statistical Machine Translation (SMT). Since all elements of the proposed EVD-based approach can rely entirely on lexical statistics, hardly any price is paid for the improved empirical results. In particular, the approach, like LSA or SMT, can still be generalized to virtually any language(s); computation of the EVD takes similar resources to that of the SVD since all the blocks are sparse; © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.  and the results of EVD are just as economical as those of SVD. 
Much previous work has investigated weak supervision with HMMs and tag dictionaries for part-of-speech tagging, but there have been no similar investigations for the harder problem of supertagging. Here, I show that weak supervision for supertagging does work, but that it is subject to severe performance degradation when the tag dictionary is highly ambiguous. I show that lexical category complexity and information about how supertags may combine syntactically can be used to initialize the transition distributions of a ﬁrst-order Hidden Markov Model for weakly supervised learning. This initialization proves more effective than starting with uniform transitions, especially when the tag dictionary is highly ambiguous. 
We present an automatic method for senselabeling of text in an unsupervised manner. The method makes use of distributionally similar words to derive an automatically labeled training set, which is then used to train a standard supervised classiﬁer for distinguishing word senses. Experimental results on the Senseval-2 and Senseval-3 datasets show that our approach yields signiﬁcant improvements over state-of-the-art unsupervised methods, and is competitive with supervised ones, while eliminating the annotation cost. 
In this paper we present a taxonomy of dialogue moves which describe the actions that students and tutors perform in tutorial dialogue. We are motivated by the need for a categorisation of such actions in order to develop computational models for tutorial dialogue. As such, we build both on existing work on dialogue move categorisation for tutorial dialogue as well as dialogue taxonomies for general dialogue. Our taxonomy has been prepared by analysing a corpus of tutorial dialogues on mathematical theorem proving. We also detail an annotation experiment in which we apply the taxonomy and discuss idiosyncrasies in the data which inﬂuence the decisions in the dialogue move classiﬁcation. 
A noun-compound is a compressed proposition that requires an audience to recover the implicit relationship between two concepts that are expressed as nouns. Listeners recover this relationship by considering the most typical relations afforded by each concept. These relational possibilities are evident at a linguistic level in the syntagmatic patterns that connect nouns to the verbal actions that act upon, or are facilitated by, these nouns. We present a model of noun-compound interpretation that first learns the relational possibilities for individual nouns from corpora, and which then uses these to hypothesize about the most likely relationship that underpins a noun compound. 
Coordinations in noun phrases often pose the problem that elliptiﬁed parts have to be reconstructed for proper semantic interpretation. Unfortunately, the detection of coordinated heads and identiﬁcation of elliptiﬁed elements notoriously lead to ambiguous reconstruction alternatives. While linguistic intuition suggests that semantic criteria might play an important, if not superior, role in disambiguating resolution alternatives, our experiments on the reannotated WSJ part of the Penn Treebank indicate that solely morpho-syntactic criteria are more predictive than solely lexicosemantic ones. We also found that the combination of both criteria does not yield any substantial improvement. 
We present ParaMetric, an automatic evaluation metric for data-driven approaches to paraphrasing. ParaMetric provides an objective measure of quality using a collection of multiple translations whose paraphrases have been manually annotated. ParaMetric calculates precision and recall scores by comparing the paraphrases discovered by automatic paraphrasing techniques against gold standard alignments of words and phrases within equivalent sentences. We report scores for several established paraphrasing techniques. 
This paper studies three techniques that improve the quality of N-best hypotheses through additional regeneration process. Unlike the multi-system consensus approach where multiple translation systems are used, our improvement is achieved through the expansion of the Nbest hypotheses from a single system. We explore three different methods to implement the regeneration process: redecoding, n-gram expansion, and confusion network-based regeneration. Experiments on Chinese-to-English NIST and IWSLT tasks show that all three methods obtain consistent improvements. Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.81 BLEU-score on IWSLT’06, 0.57 on NIST’03, 0.61 on NIST’05 test set respectively. 
In this paper, we focus on the adaptation problem that has a large labeled data in the source domain and a large but unlabeled data in the target domain. Our aim is to learn reliable information from unlabeled target domain data for dependency parsing adaptation. Current state-of-the-art statistical parsers perform much better for shorter dependencies than for longer ones. Thus we propose an adaptation approach by learning reliable information on shorter dependencies in an unlabeled target data to help parse longer distance words. The unlabeled data is parsed by a dependency parser trained on labeled source domain data. The experimental results indicate that our proposed approach outperforms the baseline system, and is better than current state-of-the-art adaptation techniques. 
 We describe an entirely statistics-based,  unsupervised,  and  language-  independent approach to multilingual  information retrieval, which we call La-  tent Morpho-Semantic Analysis  (LMSA). LMSA overcomes some of the  shortcomings of related previous ap-  proaches such as Latent Semantic  Analysis (LSA). LMSA has an impor-  tant theoretical advantage over LSA: it  combines well-known techniques in a  novel way to break the terms of LSA  down into units which correspond more  closely to morphemes. Thus, it has a  particular appeal for use with morpho-  logically complex languages such as  Arabic. We show through empirical re-  sults that the theoretical advantages of  LMSA can translate into significant  gains in precision in multilingual infor-  mation retrieval tests. These gains are  not matched either when a standard  stemmer is used with LSA, or when  terms are indiscriminately broken down  into n-grams.  
In this paper we generalise the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suited to our task and a discriminative tree-totree transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression speciﬁc loss functions. 
In this paper, we analyze the state of current human and automatic evaluation of topic-focused summarization in the Document Understanding Conference main task for 2005-2007. The analyses show that while ROUGE has very strong correlation with responsiveness for both human and automatic summaries, there is a signiﬁcant gap in responsiveness between humans and systems which is not accounted for by the ROUGE metrics. In addition to teasing out gaps in the current automatic evaluation, we propose a method to maximize the strength of current automatic evaluations by using the method of canonical correlation. We apply this new evaluation method, which we call ROSE (ROUGE Optimal Summarization Evaluation), to ﬁnd the optimal linear combination of ROUGE scores to maximize correlation with human responsiveness. 
This paper presents an implemented hybrid approach to grammar and style checking, combining an industrial patternbased grammar and style checker with bidirectional, large-scale HPSG grammars for German and English. Under this approach, deep processing is applied selectively based on the error hypotheses of a shallow system. We have conducted a comparative evaluation of the two components, supporting an integration scenario where the shallow system is best used for error detection, whereas the HPSG grammars add error correction for both grammar and controlled language style errors. 
This paper presents a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources. Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semanticallyrelated concepts is a major step towards the autonomous acquisition of knowledge from raw corpora. In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework. 
In this paper, we present an approach to the automatic identiﬁcation and correction of preposition and determiner errors in nonnative (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present ﬁrst results in an error detection task for L2 writing. 
This paper describes the design and evaluation of an extractive summarizer for educational science content called COGENT. COGENT extends MEAD based on strategies elicited from an empirical study with science domain and instructional design experts. COGENT identifies sentences containing pedagogically relevant concepts for a specific science domain. The algorithms pursue a hybrid approach integrating both domain independent bottom-up sentence scoring features and domain-aware top-down features. Evaluation results indicate that COGENT outperforms existing summarizers and generates summaries that closely resemble those generated by human experts. COGENT concept inventories appear to also support the computational identification of student misconceptions about earthquakes and plate tectonics. 
This paper presents a method for mining potential troubles or obstacles related to the use of a given object. Some example instances of this relation are medicine, side effect and amusement park, height restriction . Our acquisition method consists of three steps. First, we use an unsupervised method to collect training samples from Web documents. Second, a set of expressions generally referring to troubles is acquired by a supervised learning method. Finally, the acquired troubles are associated with objects so that each of the resulting pairs consists of an object and a trouble or obstacle in using that object. To show the effectiveness of our method we conducted experiments using a large collection of Japanese Web documents for acquisition. Experimental results show an 85.5% precision for the top 10,000 acquired troubles, and a 74% precision for the top 10% of over 60,000 acquired object-trouble pairs. 
We present procedures which pool lexical information estimated from unlabeled data via the Inside-Outside algorithm, with lexical information from a treebank PCFG. The procedures produce substantial improvements (up to 31.6% error reduction) on the task of determining subcategorization frames of novel verbs, relative to a smoothed Penn Treebank-trained PCFG. Even with relatively small quantities of unlabeled training data, the re-estimated models show promising improvements in labeled bracketing f-scores on Wall Street Journal parsing, and substantial beneﬁt in acquiring the subcategorization preferences of low-frequency verbs. 
As it serves as a basis for POS tagging, category induction, and human category acquisition, we investigate the information needed to disambiguate a word in a local context, when using corpus categories. Speciﬁcally, we increase the recall of an error detection method by abstracting the word to be disambiguated to a representation containing information about some of its inherent properties, namely the set of categories it can potentially have. This work thus provides insights into the relation of corpus categories to categories derived from local contexts. 
We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT. This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules. In decoding, the alternatives are scored based on the output word order, not the order of the input. Unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based SMT. On an English-Danish task, we achieve an absolute improvement in translation quality of 1.1 % BLEU. Manual evaluation supports the claim that the present approach is signiﬁcantly superior to previous approaches. 
This paper provides a parsing algorithm for the Lambek calculus which is polynomial time for a more general fragment of the Lambek calculus than any previously known algorithm. The algorithm runs in worst-case time O(n5) when restricted to a certain fragment of the Lambek calculus which is motivated by empirical analysis. In addition, a set of parameterized inputs are given, showing why the algorithm has exponential worst-case running time for the Lambek calculus in general. 
The most critical issue in generating and recognizing paraphrases is development of wide-coverage paraphrase knowledge. Previous work on paraphrase acquisition has collected lexicalized pairs of expressions; however, the results do not ensure full coverage of the various paraphrase phenomena. This paper focuses on productive paraphrases realized by general transformation patterns, and addresses the issues in generating instances of phrasal paraphrases with those patterns. Our probabilistic model computes how two phrases are likely to be correct paraphrases. The model consists of two components: (i) a structured N -gram language model that ensures grammaticality and (ii) a distributional similarity measure for estimating semantic equivalence and substitutability. 
This paper presents a method of retrieving bilingual collocations of a verb and its objective noun from cross-lingual documents with similar contents. Relevant documents are obtained by integrating crosslanguage hierarchies. The results showed a 15.1% improvement over the baseline nonhierarchy model, and a 6.0% improvement over use of relevant documents retrieved from a single hierarchy. Moreover, we found that some of the retrieved collocations were domain-speciﬁc. 
This paper studies sentiment analysis from the user-generated content on the Web. In particular, it focuses on mining opinions from comparative sentences, i.e., to determine which entities in a comparison are preferred by its author. A typical comparative sentence compares two or more entities. For example, the sentence, “the picture quality of Camera X is better than that of Camera Y”, compares two entities “Camera X” and “Camera Y” with regard to their picture quality. Clearly, “Camera X” is the preferred entity. Existing research has studied the problem of extracting some key elements in a comparative sentence. However, there is still no study of mining opinions from comparative sentences, i.e., identifying preferred entities of the author. This paper studies this problem, and proposes a technique to solve the problem. Our experiments using comparative sentences from product reviews and forum posts show that the approach is effective. 
In contrast to LFG and HPSG, there is to date no large scale Tree Adjoining Grammar (TAG) equiped with a compositional semantics. In this paper, we report on the integration of a uniﬁcation-based semantics into a Feature-Based Lexicalised TAG for French consisting of around 6 000 trees. We focus on verb semantics and show how factorisation can be used to support a compact and principled encoding of the semantic information that needs to be associated with each of the verbal elementary trees. The factorisation is made possible by the use of XMG, a high-level linguistic formalism designed to specify and compile computational grammars and in particular, grammars based on non-local trees or tree descriptions. 
This paper presents a probabilistic model for resolution of non-pronominal anaphora in biomedical texts. The model seeks to ﬁnd the antecedents of anaphoric expressions, both coreferent and associative ones, and also to identify discourse-new expressions. We consider only the noun phrases referring to biomedical entities. The model reaches state-of-the art performance: 5669% precision and 54-67% recall on coreferent cases, and reasonable performance on different classes of associative cases. 
We present an approach to ontology population based on a lexical substitution technique. It consists in estimating the plausibility of sentences where the named entity to be classiﬁed is substituted with the ones contained in the training data, in our case, a partially populated ontology. Plausibility is estimated by using Web data, while the classiﬁcation algorithm is instance-based. We evaluated our method on two different ontology population tasks. Experiments show that our solution is effective, outperforming existing methods, and it can be applied to practical ontology population problems. 
The use of topical features is abundant in Natural Language Processing (NLP), a major example being in dictionary-based Word Sense Disambiguation (WSD). Yet previous research does not attempt to measure the level of topic cohesion in documents, despite assertions of its effects. This paper introduces a quantitative measure of Topic Homogeneity using a range of NLP resources and not requiring prior knowledge of correct senses. Evaluation is performed firstly by using the WordNet::Domains package to create word-sets with varying levels of homogeneity and comparing our results with those expected. Additionally, to evaluate each measure’s potential value, the homogeneity results are correlated against those of 3 co-occurrence/dictionarybased WSD techniques, tested on 1040 Semcor and SENSEVAL sub-documents. Many low-moderate correlations are found to exist with several in the moderate range (above .40). These correlations surpass polysemy and senseentropy, the 2 most cited factors affecting WSD. Finally, a combined homogeneity measure achieves correlations of up to .52. 
 Semantic relatedness between words is important to many NLP tasks, and numerous measures exist which use a variety of resources. Thus far, such work is confined to measuring similarity between two words (or two texts), and only a handful utilize the web as a corpus. This paper introduces a distributional similarity measure which uses internet search counts and also extends to calculating the similarity within word-groups. The evaluation results are encouraging: for word-pairs, the correlations with human judgments are comparable with state-ofthe-art web-search page-count heuristics. When used to measure similarities within sets of 10 words, the results correlate highly (up to 0.8) with those expected. Relatively little comparison has been made between the results of different search-engines. Here, we compare experimental results from Google, Windows Live Search and Yahoo and find noticeable differences.  
The paper offers a new type of approach to the semantic phenomenon of adverbial aspect shift within the framework of ﬁnitestate temporal semantics. The heart of the proposal is a supervaluational concept of underspeciﬁcation, and the idea of treating the meanings of temporal prepositions as dynamic presuppositions. The simple shifting algorithm used in the present approach derives the correct set of possible readings on the basis of lexical semantic input only, and, furthermore, may claim cognitive plausibility. 
We present dependency-based n-gram models for general-purpose, widecoverage, probabilistic sentence realisation. Our method linearises unordered dependencies in input representations directly rather than via the application of grammar rules, as in traditional chartbased generators. The method is simple, efﬁcient, and achieves competitive accuracy and complete coverage on standard English (Penn-II, 0.7440 BLEU, 0.05 sec/sent) and Chinese (CTB6, 0.7123 BLEU, 0.14 sec/sent) test data. 
 This paper explores the use of the homotopy method for training a semi-supervised Hidden Markov Model (HMM) used for sequence labeling. We provide a novel polynomial-time algorithm to trace the local maximum of the likelihood function for HMMs from full weight on the labeled data to full weight on the unlabeled data. We present an experimental analysis of different techniques for choosing the best balance between labeled and unlabeled data based on the characteristics observed along this path. Furthermore, experimental results on the ﬁeld segmentation task in information extraction show that the Homotopy-based method signiﬁcantly outperforms EM-based semisupervised learning, and provides a more accurate alternative to the use of held-out data to pick the best balance for combining labeled and unlabeled data.  
We introduce a technique for analyzing the temporal evolution of the salience of participants in a discussion. Our method can dynamically track how the relative importance of speakers evolve over time using graph based techniques. Speaker salience is computed based on the eigenvector centrality in a graph representation of participants in a discussion. Two participants in a discussion are linked with an edge if they use similar rhetoric. The method is dynamic in the sense that the graph evolves over time to capture the evolution inherent to the participants salience. We used our method to track the salience of members of the US Senate using data from the US Congressional Record. Our analysis investigated how the salience of speakers changes over time. Our results show that the scores can capture speaker centrality in topics as well as events that result in change of salience or inﬂuence among different participants. 
This paper proposes a novel lexicalized approach for rule selection for syntax-based statistical machine translation (SMT). We build maximum entropy (MaxEnt) models which combine rich context information for selecting translation rules during decoding. We successfully integrate the MaxEnt-based rule selection models into the state-of-the-art syntax-based SMT model. Experiments show that our lexicalized approach for rule selection achieves statistically signiﬁcant improvements over the state-of-the-art SMT system. 
This paper explores the relationship between various measures of unsupervised part-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags. We ﬁnd that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance. 
In this paper, we introduce a new framework for recognizing textual entailment (RTE) which depends on extraction of the set of publicly-held beliefs – known as discourse commitments – that can be ascribed to the author of a text (t) or a hypothesis (h). We show that once a set of commitments have been extracted from a t-h pair, the task of recognizing textual entailment is reduced to the identiﬁcation of the commitments from a t which support the inference of the h. Our system correctly identiﬁed entailment relationships in more than 80% of t-h pairs taken from all three of the previous PASCAL RTE Challenges, without the need for additional sources of training data. 
As Chinese text is written without word boundaries, effectively recognizing Chinese words is like recognizing collocations in English, substituting characters for words and words for collocations. However, existing topical models that involve collocations have a common limitation. Instead of directly assigning a topic to a collocation, they take the topic of a word within the collocation as the topic of the whole collocation. This is unsatisfactory for topical modeling of Chinese documents. Thus, we propose a topical word-character model (TWC), which allows two distinct types of topics: word topic and character topic. We evaluated TWC both qualitatively and quantitatively to show that it is a powerful and a promising topic model. 
To realize high quality machine translation, we proposed a Non-Compositional Language Model, and developed a sentence pattern dictionary of 226,800 pattern pairs for Japanese compound and complex sentences consisting of 2 or 3 clauses. In pattern generation from a parallel corpus, Compositional Constituents that could be generalized were 74% of independent words, 24% of phrases and only 15% of clauses. This means that in Japanese-to-English MT, most of the translation results as shown in the parallel corpus could not be obtained by methods based on Compositional Semantics. This dictionary achieved a syntactic coverage of 98% and a semantic coverage of 78%. It will substantially improve translation quality. 
In Japanese dependency parsing, Kudo’s relative preference-based method (Kudo and Matsumoto, 2005) outperforms both deterministic and probabilistic CKY-based parsing methods. In Kudo’s method, for each dependent word (or chunk) a loglinear model estimates relative preference of all other candidate words (or chunks) for being as its head. This cannot be considered in the deterministic parsing methods. We propose an algorithm based on a tournament model, in which the relative preferences are directly modeled by one-onone games in a step-ladder tournament. In an evaluation experiment with Kyoto Text Corpus Version 4.0, the proposed method outperforms previous approaches, including the relative preference-based method. 
This paper describes a system for processing economic documents written in the ancient Sumerian language. The system is application-oriented and takes advantage of the simplicity of ancient economy. We have developed an ontology for a selected branch of economic activities. We translate the documents into a meaning representation language by means of a semantic grammar. The meaning representation language is constructed in a way that allows us to handle massive ambiguity caused by: the speciﬁcs of the Sumerian writing system (signs’ polyvalence, lack of mid-word signs), our incomplete knowledge of the Sumerian language and frequent damages of documents. The system is augmented with the capability of processing documents whose parts describe concepts not included in the ontology and grammar. As an effect we obtain a structural description of the documents contents in the meaning representation language, ready to use in historical research. 
Part of the unique cultural heritage of China is the game of Chinese couplets (duìlián). One person challenges the other person with a sentence (first sentence). The other person then replies with a sentence (second sentence) equal in length and word segmentation, in a way that corresponding words in the two sentences match each other by obeying certain constraints on semantic, syntactic, and lexical relatedness. This task is viewed as a difficult problem in AI and has not been explored in the research community. In this paper, we regard this task as a kind of machine translation process. We present a phrase-based SMT approach to generate the second sentence. First, the system takes as input the first sentence, and generates as output an N-best list of proposed second sentences, using a phrase-based SMT decoder. Then, a set of filters is used to remove candidates violating linguistic constraints. Finally, a Ranking SVM is applied to rerank the candidates. A comprehensive evaluation, using both human judgments and BLEU scores, has been conducted, and the results demonstrate that this approach is very successful. 
In this paper, we describe a new reranking strategy named word lattice reranking, for the task of joint Chinese word segmentation and part-of-speech (POS) tagging. As a derivation of the forest reranking for parsing (Huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking. With a perceptron classiﬁer trained with local features as the baseline, word lattice reranking performs reranking with non-local features that can’t be easily incorporated into the perceptron baseline. Experimental results show that, this strategy achieves improvement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking. 
Almost all automatic semantic role labeling (SRL) systems rely on a preliminary parsing step that derives a syntactic structure from the sentence being analyzed. This makes the choice of syntactic representation an essential design decision. In this paper, we study the inﬂuence of syntactic representation on the performance of SRL systems. Speciﬁcally, we compare constituent-based and dependencybased representations for SRL of English in the FrameNet paradigm. Contrary to previous claims, our results demonstrate that the systems based on dependencies perform roughly as well as those based on constituents: For the argument classiﬁcation task, dependencybased systems perform slightly higher on average, while the opposite holds for the argument identiﬁcation task. This is remarkable because dependency parsers are still in their infancy while constituent parsing is more mature. Furthermore, the results show that dependency-based semantic role classiﬁers rely less on lexicalized features, which makes them more robust to domain changes and makes them learn more efﬁciently with respect to the amount of training data. 
Word clustering is a conventional and important NLP task, and the literature has suggested two kinds of approaches to this problem. One is based on the distributional similarity and the other relies on the co-occurrence of two words in lexicosyntactic patterns. Although the two methods have been discussed separately, it is promising to combine them since they are complementary with each other. This paper proposes to integrate them using hidden Markov random ﬁelds and demonstrates its effectiveness through experiments. 
This paper tackles textual demand analysis, the task of capturing what people want or need, rather than identifying what they like or dislike, on which much conventional work has focused. It exploits syntactic patterns as clues to detect previously unknown demands, and requires domaindependent knowledge to get high recall. To build such patterns we created an unsupervised pattern induction method relying on the hypothesis that there are commonly desired aspects throughout a domain corpus. Experimental results show that the proposed method detects twice to four times as many demand expressions in Japanese discussion forums compared to a baseline method. 
This paper discusses local alignment kernels in the context of the relation extraction task. We deﬁne a local alignment kernel based on the Smith-Waterman measure as a sequence similarity metric and proceed with a range of possibilities for computing a similarity between elements of sequences. We propose to use distributional similarity measures on elements and by doing so we are able to incorporate extra information from the unlabeled data into a learning task. Our experiments suggest that a LA kernel provides promising results on some biomedical corpora largely outperforming a baseline. 
 For instance, consider the following example:  The use of similarities has been one of the main approaches to resolve the ambiguities of coordinate structures. In this paper, we present an alternative method for coordination disambiguation, which does not use similarities. Our hypothesis is that coordinate structures are supported by surrounding dependency relations, and that such dependency relations rather yield similarity between conjuncts, which humans feel. Based on this hypothesis, we built a Japanese fully-lexicalized generative parser that includes coordination disambiguation. Experimental results on web sentences indicated the effectiveness of our approach, and endorsed our hypothesis. 
Existing algorithms for the Generation of Referring Expressions tend to generate distinguishing descriptions at the semantic level, disregarding the ways in which surface issues can aﬀect their quality. This paper considers how these algorithms should deal with surface ambiguity, focussing on structural ambiguity. We propose that not all ambiguity is worth avoiding, and suggest some ways forward that attempt to avoid unwanted interpretations. We sketch the design of an algorithm motivated by our experimental ﬁndings. 
Electronic written texts used in computermediated interactions (e-mails, blogs, chats, etc) present major deviations from the norm of the language. This paper presents an comparative study of systems aiming at normalizing the orthography of French SMS messages: after discussing the linguistic peculiarities of these messages, and possible approaches to their automatic normalization, we present, evaluate and contrast two systems, one drawing inspiration from the Machine Translation task; the other using techniques that are commonly used in automatic speech recognition devices. Combining both approaches, our best normalization system achieves about 11% Word Error Rate on a test set of about 3000 unseen messages. 
We conduct large-scale experiments to investigate optimal features for classiﬁcation of verbs in biomedical texts. We introduce a range of feature sets and associated extraction techniques, and evaluate them thoroughly using a robust method new to the task: cost-based framework for pairwise clustering. Our best results compare favourably with earlier ones. Interestingly, they are obtained with sophisticated feature sets which include lexical and semantic information about selectional preferences of verbs. The latter are acquired automatically from corpus data using a fully unsupervised method. 
In this paper, we work on extending a Chinese thesaurus with words distinctly used in various Chinese communities. The acquisition and classification of such region-specific lexical items is an important step toward the larger goal of constructing a Pan-Chinese lexical resource. In particular, we extend a previous study in three respects: (1) to improve automatic classification by removing duplicated words from the thesaurus, (2) to experiment with classifying words at the subclass level and semantic head level, and (3) to further investigate the possible effects of data heterogeneity between the region-specific words and words in the thesaurus on classification performance. Automatic classification was based on the similarity between a target word and individual categories of words in the thesaurus, measured by the cosine function. Experiments were done on 120 target words from four regions. The automatic classification results were evaluated against a gold standard obtained from human judgements. In general accuracy reached 80% or more with the top 10 (out of 80+) and top 100 (out of 1,300+) candidates considered at the subclass level and semantic head level respectively, provided that the appropriate data sources were used. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved.  
Active learning is a proven method for reducing the cost of creating the training sets that are necessary for statistical NLP. However, there has been little work on stopping criteria for active learning. An operational stopping criterion is necessary to be able to use active learning in NLP applications. We investigate three different stopping criteria for active learning of named entity recognition (NER) and show that one of them, gradient-based stopping, (i) reliably stops active learning, (ii) achieves nearoptimal NER performance, (iii) and needs only about 20% as much training data as exhaustive labeling. 
Media reporting shapes public opinion which can in turn inﬂuence events, particularly in political elections, in which candidates both respond to and shape public perception of their campaigns. We use computational linguistics to automatically predict the impact of news on public perception of political candidates. Our system uses daily newspaper articles to predict shifts in public opinion as reﬂected in prediction markets. We discuss various types of features designed for this problem. The news system improves market prediction over baseline market systems. 
Classifying what-type questions into proper semantic categories is found more challenging than classifying other types in question answering systems. In this paper, we propose to classify what-type questions by head noun tagging. The approach highlights the role of head nouns as the category discriminator of whattype questions. To reduce the semantic ambiguities of head noun, we integrate local syntactic feature, semantic feature and category dependency among adjacent nouns with Conditional Random Fields (CRFs). Experiments on standard question classification data set show that the approach achieves state-of-the-art performances. 
Query-oriented update summarization is an emerging summarization task very recently. It brings new challenges to the sentence ranking algorithms that require not only to locate the important and query-relevant information, but also to capture the new information when document collections evolve. In this paper, we propose a novel graph based sentence ranking algorithm, namely PNR2, for update summarization. Inspired by the intuition that “a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas a sentence receives a negative influence from the sentences that correlates to it in the different (perhaps previously read) collection”, PNR2 models both the positive and the negative mutual reinforcement in the ranking process. Automatic evaluation on the DUC 2007 data set pilot task demonstrates the effectiveness of the algorithm. 
Community-based question answering (cQA) services have accumulated millions of questions and their answers over time. In the process of accumulation, cQA services assume that questions always have unique best answers. However, with an indepth analysis of questions and answers on cQA services, we find that the assumption cannot be true. According to the analysis, at least 78% of the cQA best answers are reusable when similar questions are asked again, but no more than 48% of them are indeed the unique best answers. We conduct the analysis by proposing taxonomies for cQA questions and answers. To better reuse the cQA content, we also propose applying automatic summarization techniques to summarize answers. Our results show that question-type oriented summarization techniques can improve cQA answer quality significantly. 
Translation model size is growing at a pace that outstrips improvements in computing power, and this hinders research on many interesting models. We show how an algorithmic scaling technique can be used to easily handle very large models. Using this technique, we explore several large model variants and show an improvement 1.4 BLEU on the NIST 2006 ChineseEnglish task. This opens the door for work on a variety of models that are much less constrained by computational limitations. 
Most studies in statistical or machine learning based authorship attribution focus on two or a few authors. This leads to an overestimation of the importance of the features extracted from the training data and found to be discriminating for these small sets of authors. Most studies also use sizes of training data that are unrealistic for situations in which stylometry is applied (e.g., forensics), and thereby overestimate the accuracy of their approach in these situations. A more realistic interpretation of the task is as an authorship veriﬁcation problem that we approximate by pooling data from many different authors as negative examples. In this paper, we show, on the basis of a new corpus with 145 authors, what the effect is of many authors on feature selection and learning, and show robustness of a memory-based learning approach in doing authorship attribution and veriﬁcation with many authors and limited training data when compared to eager learning methods such as SVMs and maximum entropy learning. 
We propose an approach to natural language inference based on a model of natural logic, which identiﬁes valid inferences by their lexical and syntactic features, without full semantic interpretation. We greatly extend past work in natural logic, which has focused solely on semantic containment and monotonicity, to incorporate both semantic exclusion and implicativity. Our system decomposes an inference problem into a sequence of atomic edits linking premise to hypothesis; predicts a lexical entailment relation for each edit using a statistical classiﬁer; propagates these relations upward through a syntax tree according to semantic properties of intermediate nodes; and composes the resulting entailment relations across the edit sequence. We evaluate our system on the FraCaS test suite, and achieve a 27% reduction in error from previous work. We also show that hybridizing an existing RTE system with our natural logic system yields signiﬁcant gains on the RTE3 test suite. 
We present a sub-sentential alignment system that links linguistically motivated phrases in parallel texts based on lexical correspondences and syntactic similarity. We compare the performance of our subsentential alignment system with different symmetrization heuristics that combine the GIZA++ alignments of both translation directions. We demonstrate that the aligned linguistically motivated phrases are a useful means to extract bilingual terminology and more speciﬁcally complex multiword terms. 
Finite-state Transducers (FST) can be very efficient to implement inter-dialectal transliteration. We illustrate this on the Hindi and Urdu language pair. FSTs can also be used for translation between surface-close languages. We introduce UIT (universal intermediate transcription) for the same pair on the basis of their common phonetic repository in such a way that it can be extended to other languages like Arabic, Chinese, English, French, etc. We describe a transliteration model based on FST and UIT, and evaluate it on Hindi and Urdu corpora. 
This paper presents a methodology for the comparative performance analysis of the parsers developed for different grammar frameworks. For such a comparison, we need a common representation format of the parsing results since the representation of the parsing results depends on the grammar frameworks; hence they are not directly comparable to each other. We ﬁrst convert the parsing result to a shallow CFG analysis by using an automatic tree converter based on synchronous grammars. The use of such a shallow representation as a common format has the advantage of reduced noise introduced by the conversion in comparison with the noise produced by the conversion to deeper representations. We compared an HPSG parser with several CFG parsers in our experiment and found that meaningful differences among the parsers’ performance can still be observed by such a shallow representation. 
 will call the direction of offset (here, respectively,  In this paper we present a study on the interpretation of weekday names in texts. Our algorithm for assigning a date to a weekday name achieves 95.91% accuracy on a test data set based on the ACE 2005 Training Corpus, outperforming previously reported techniques run against this same data. We also provide the ﬁrst detailed comparison of various approaches to the problem using this test data set, em-  past, past and future). However, in other cases there is no explicit indication of the direction of offset from the temporal focus. This is most obviously the case when bare expressions based on calendar cycles—i.e., weekday names and month names—are used, as in the following example: (4) Jones met with Defense Minister Paulo Portas on Tuesday and will meet Foreign Minister Antonio Martins da Cruz before leaving Portugal Wednesday.  ploying re-implementations of key techniques from the literature and a range of additional heuristic-based approaches.  Here, the proper interpretation of the references to Tuesday and Wednesday requires at the least a correct syntactic analysis of the sentence, in order  
Self-training has been shown capable of improving on state-of-the-art parser performance (McClosky et al., 2006) despite the conventional wisdom on the matter and several studies to the contrary (Charniak, 1997; Steedman et al., 2003). However, it has remained unclear when and why selftraining is helpful. In this paper, we test four hypotheses (namely, presence of a phase transition, impact of search errors, value of non-generative reranker features, and effects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the beneﬁt of self-training appears most inﬂuenced by seeing known words in new combinations. 
This paper describes an incremental approach to parsing transcribed spontaneous speech containing disﬂuencies with a Hierarchical Hidden Markov Model (HHMM). This model makes use of the right-corner transform, which has been shown to increase non-incremental parsing accuracy on transcribed spontaneous speech (Miller and Schuler, 2008), using trees transformed in this manner to train the HHMM parser. Not only do the representations used in this model align with structure in speech repairs, but as an HMM-like timeseries model, it can be directly integrated into conventional speech recognition systems run on continuous streams of audio. A system implementing this model is evaluated on the standard task of parsing the Switchboard corpus, and achieves an improvement over the standard baseline probabilistic CYK parser. 
This paper looks at the transcribed data of patient-doctor consultations in an examination setting. The doctors are internationally qualiﬁed and enrolled in a bridging course as preparation for their Australian Medical Council examination. In this study, we attempt to ascertain if there are measurable linguistic features of the consultations, and to investigate whether there is any relevant information about the communicative styles of the qualifying doctors that may predict satisfactory or non-satisfactory examination outcomes. We have taken a discourse analysis approach in this study, where the core unit of analysis is a ‘turn’. We approach this problem as a binary classiﬁcation task and employ data mining methods to see whether the application of which to richly annotated dialogues can produce a system with an adequate predictive capacity. 
Och’s (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its beneﬁts. We compare several ways of performing random restarts with MERT. We ﬁnd that all of our random restart methods outperform MERT without random restarts, and we develop some reﬁnements of random restarts that are superior to the most common approach with regard to resulting model quality and training time. 
Matching coreferent named entities without prior knowledge requires good similarity measures. Soft-TFIDF is a ﬁne-grained measure which performs well in this task. We propose to enhance this kind of metrics, through a generic model in which measures may be mixed, and show experimentally the relevance of this approach. 
We study the self-organization of the consonant inventories through a complex network approach. We observe that the distribution of occurrence as well as cooccurrence of the consonants across languages follow a power-law behavior. The co-occurrence network of consonants exhibits a high clustering coefﬁcient. We propose four novel synthesis models for these networks (each of which is a reﬁnement of the earlier) so as to successively match with higher accuracy (a) the above mentioned topological properties as well as (b) the linguistic property of feature economy exhibited by the consonant inventories. We conclude by arguing that a possible interpretation of this mechanism of network growth is the process of child language acquisition. Such models essentially increase our understanding of the structure of languages that is inﬂuenced by their evolutionary dynamics and this, in turn, can be extremely useful for building future NLP applications. 
 We propose a new unsupervised method for topic detection that automatically identiﬁes the different facets of an event. We use pointwise Kullback-Leibler divergence along with the Jaccard coefﬁcient to build a topic graph which represents the community structure of the different facets. The problem is formulated as a weighted set cover problem with dynamically varying weights. The algorithm is domainindependent and generates a representative set of informative and discriminative phrases that cover the entire event. We evaluate this algorithm on a large collection of blog postings about different news events and report promising results.  
The ability to correctly classify sentences that describe events is an important task for many natural language applications such as Question Answering (QA) and Summarisation. In this paper, we treat event detection as a sentence level text classiﬁcation problem. We compare the performance of two approaches to this task: a Support Vector Machine (SVM) classiﬁer and a Language Modeling (LM) approach. We also investigate a rule based method that uses hand crafted lists of terms derived from WordNet. These terms are strongly associated with a given event type, and can be used to identify sentences describing instances of that type. We use two datasets in our experiments, and evaluate each technique on six distinct event types. Our results indicate that the SVM consistently outperform the LM technique for this task. More interestingly, we discover that the manual rule based classiﬁcation system is a very powerful baseline that outperforms the SVM on three of the six event types. 
Much effort in the research community has been spent on solving the anaphora resolution or pronoun resolution problem, and in particular for news texts. In order to selectively inherit the previous works and solve the same problem for a new domain, we carried out a comparative study with three different corpora: MUC, ACE for the news texts, and GENIA for bio-medical papers. Our corpus analysis and experimental results show the signiﬁcant differences in the use of pronouns in the two domains, thus by properly considering the characteristics of a domain, we can improve the performance of pronoun resolution for that domain. 
The effectiveness of parsers based on manually created resources, namely a grammar and a lexicon, rely mostly on the quality of these resources. Thus, increasing the parser coverage and precision usually implies improving these two resources. Their manual improvement is a time consuming and complex task : identifying which resource is the true culprit for a given mistake is not always obvious, as well as ﬁnding the mistake and correcting it. Some techniques, like van Noord (2004) or Sagot and Villemonte de La Clergerie (2006), bring a convenient way to automatically identify forms having potentially erroneous entries in a lexicon. We have integrated and extended such techniques in a wider process which, thanks to the grammar ability to tell how these forms could be used as part of correct parses, is able to propose lexical corrections for the identiﬁed entries. We present in this paper an implementation of this process and discuss the main results we have obtained on a syntactic widecoverage French lexicon. 
We present the ﬁrst results on parsing the SYNTAGRUS treebank of Russian with a data-driven dependency parser, achieving a labeled attachment score of over 82% and an unlabeled attachment score of 89%. A feature analysis shows that high parsing accuracy is crucially dependent on the use of both lexical and morphological features. We conjecture that the latter result can be generalized to richly inﬂected languages in general, provided that sufﬁcient amounts of training data are available. 
Distributional measures of lexical similarity and kernel methods for classiﬁcation are well-known tools in Natural Language Processing. We bring these two methods together by introducing distributional kernels that compare co-occurrence probability distributions. We demonstrate the effectiveness of these kernels by presenting state-of-the-art results on datasets for three semantic classiﬁcation: compound noun interpretation, identiﬁcation of semantic relations between nominals and semantic classiﬁcation of verbs. Finally, we consider explanations for the impressive performance of distributional kernels and sketch some promising generalisations. 
This paper presents a discriminative alignment model for extracting abbreviations and their full forms appearing in actual text. The task of abbreviation recognition is formalized as a sequential alignment problem, which ﬁnds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form). We design a large amount of ﬁnegrained features that directly express the events where letters produce or do not produce abbreviations. We obtain the optimal combination of features on an aligned abbreviation corpus by using the maximum entropy framework. The experimental results show the usefulness of the alignment model and corpus for improving abbreviation recognition. 
This paper presents a novel approach to the task of semantic role labelling for event nominalisations, which make up a considerable fraction of predicates in running text, but are underrepresented in terms of training data and difﬁcult to model. We propose to address this situation by data expansion. We construct a model for nominal role labelling solely from verbal training data. The best quality results from salvaging grammatical features where applicable, and generalising over lexical heads otherwise. 
This paper presents recent advances in an established treebank annotation framework comprising of an abstract XMLbased data format, fully customizable editor of tree-based annotations, a toolkit for all kinds of automated data processing with support for cluster computing, and a work-in-progress database-driven search engine with a graphical user interface built into the tree editor. 
In this paper, we present a method for modeling joint information when generating n-best lists. We apply the method to a novel task of characterizing the similarity of a group of terms where only a small set of many possible semantic properties may be displayed to a user. We demonstrate that considering the results jointly, by accounting for the information overlap between results, generates better n-best lists than considering them independently. We propose an information theoretic objective function for modeling the joint information in an n-best list and show empirical evidence that humans prefer the result sets produced by our joint model. Our results show with 95% confidence that the n-best lists generated by our joint ranking model are significantly different from a baseline independent model 50.0% ± 3.1% of the time, out of which they are preferred 76.6% ± 5.2% of the time. 
Quickly moving to a new area of research is painful for researchers due to the vast amount of scientiﬁc literature in each ﬁeld of study. One possible way to overcome this problem is to summarize a scientiﬁc topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others’ viewpoint of the target article’s contributions and the study of its citation summary network using a clustering approach. 
This paper proposes a new approach to dynamically determine the tree span for tree kernel-based semantic relation extraction. It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities, while removing the noisy information from the syntactic parse tree, eventually leading to a dynamic syntactic parse tree. This paper also explores entity features and their combined features in a unified parse and semantic tree, which integrates both structured syntactic parse information and entity-related semantic information. Evaluation on the ACE RDC 2004 corpus shows that our dynamic syntactic parse tree outperforms all previous tree spans, and the composite kernel combining this tree kernel with a linear state-of-the-art feature-based kernel, achieves the so far best performance. 
This paper proposes a method for automatic POS (part-of-speech) guessing of Chinese unknown words. It contains two models. The first model uses a machinelearning method to predict the POS of unknown words based on their internal component features. The credibility of the results of the first model is then measured. For low-credibility words, the second model is used to revise the first model’s results based on the global context information of those words. The experiments show that the first model achieves 93.40% precision for all words and 86.60% for disyllabic words, which is a significant improvement over the best results reported in previous studies, which were 89% precision for all words and 74% for disyllabic words. Further, the second model improves the results by 0.80% precision for all words and 1.30% for disyllabic words. © 
We present an algorithm for unsupervised induction of labeled parse trees. The algorithm has three stages: bracketing, initial labeling, and label clustering. Bracketing is done from raw text using an unsupervised incremental parser. Initial labeling is done using a merging model that aims at minimizing the grammar description length. Finally, labels are clustered to a desired number of labels using syntactic features extracted from the initially labeled trees. The algorithm obtains 59% labeled f-score on the WSJ10 corpus, as compared to 35% in previous work, and substantial error reduction over a random baseline. We report results for English, German and Chinese corpora, using two label mapping methods and two label set sizes. 
The WordNet verb hierarchy is tested, with a view to improving the performance of its applications, revealing topological anomalies and casting doubt on its semantic categories. Encoded troponyms frequently misrepresent other kinds of entailment. Approaches are proposed for correcting these anomalies including a new top ontology. 
User logs of search engines have recently been applied successfully to improve various aspects of web search quality. In this paper, we will apply pairs of user queries and snippets of clicked results to train a machine translation model to bridge the “lexical gap” between query and document space. We show that the combination of a query-to-snippet translation model with a large n-gram language model trained on queries achieves improved contextual query expansion compared to a system based on term correlations. 
In this paper, we consider classifying word positions by whether or not they can either start or end multi-word constituents. This provides a mechanism for “closing” chart cells during context-free inference, which is demonstrated to improve efﬁciency and accuracy when used to constrain the wellknown Charniak parser. Additionally, we present a method for “closing” a sufﬁcient number of chart cells to ensure quadratic worst-case complexity of context-free inference. Empirical results show that this O(n2) bound can be achieved without impacting parsing accuracy. 
sagae@ict.usc.edu  Jun’ichi Tsujii Department of Computer Science University of Tokyo School of Computer Science University of Manchester National Center for Text Mining tsujii@is.s.u-tokyo.ac.jp  Abstract Most data-driven dependency parsing approaches assume that sentence structure is represented as trees. Although trees have several desirable properties from both computational and linguistic perspectives, the structure of linguistic phenomena that goes beyond shallow syntax often cannot be fully captured by tree representations. We present a parsing approach that is nearly as simple as current data-driven transition-based dependency parsing frameworks, but outputs directed acyclic graphs (DAGs). We demonstrate the benefits of DAG parsing in two experiments where its advantages over dependency tree parsing can be clearly observed: predicate-argument analysis of English and syntactic analysis of Danish with a representation that includes long-distance dependencies and anaphoric reference links. 
This paper presents a probabilistic model for Japanese zero anaphora resolution. First, this model recognizes discourse entities and links all mentions to them. Zero pronouns are then detected by case structure analysis based on automatically constructed case frames. Their appropriate antecedents are selected from the entities with high salience scores, based on the case frames and several preferences on the relation between a zero pronoun and an antecedent. Case structure and zero anaphora relation are simultaneously determined based on probabilistic evaluation metrics. 
 We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of ﬁne-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed stateof-the-art POS taggers.  
Psycholinguistic studies suggest a model of human language processing that 1) performs incremental interpretation of spoken utterances or written text, 2) preserves ambiguity by maintaining competing analyses in parallel, and 3) operates within a severely constrained short-term memory store — possibly constrained to as few as four distinct elements. This paper describes a relatively simple model of language as a factored statistical time-series process that meets all three of the above desiderata; and presents corpus evidence that this model is sufﬁcient to parse naturally occurring sentences using human-like bounds on memory. 
The distance or similarity metric plays an important role in many natural language processing (NLP) tasks. Previous studies have demonstrated the effectiveness of a number of metrics such as the Jaccard coefﬁcient, especially in synonym acquisition. While the existing metrics perform quite well, to further improve performance, we propose the use of a supervised machine learning algorithm that ﬁne-tunes them. Given the known instances of similar or dissimilar words, we estimated the parameters of the Mahalanobis distance. We compared a number of metrics in our experiments, and the results show that the proposed metric has a higher mean average precision than other metrics. 
This work proposes opinion frames as a representation of discourse-level associations which arise from related opinion topics. We illustrate how opinion frames help gather more information and also assist disambiguation. Finally we present the results of our experiments to detect these associations. 
Supervised approaches to Word Sense Disambiguation (WSD) have been shown to outperform other approaches but are hampered by reliance on labeled training examples (the data acquisition bottleneck). This paper presents a novel approach to the automatic acquisition of labeled examples for WSD which makes use of the Information Retrieval technique of relevance feedback. This semi-supervised method generates additional labeled examples based on existing annotated data. Our approach is applied to a set of ambiguous terms from biomedical journal articles and found to signiﬁcantly improve the performance of a state-of-the-art WSD system. 
Within the area of general-purpose ﬁnegrained subjectivity analysis, opinion topic identiﬁcation has, to date, received little attention due to both the difﬁculty of the task and the lack of appropriately annotated resources. In this paper, we provide an operational deﬁnition of opinion topic and present an algorithm for opinion topic identiﬁcation that, following our new deﬁnition, treats the task as a problem in topic coreference resolution. We develop a methodology for the manual annotation of opinion topics and use it to annotate topic information for a portion of an existing general-purpose opinion corpus. In experiments using the corpus, our topic identiﬁcation approach statistically signiﬁcantly outperforms several non-trivial baselines according to three evaluation measures. 
We determine the subjectivity of word senses. To avoid costly annotation, we evaluate how useful existing resources established in opinion mining are for this task. We show that results achieved with existing resources that are not tailored towards word sense subjectivity classiﬁcation can rival results achieved with supervision on a manually annotated training set. However, results with different resources vary substantially and are dependent on the different deﬁnitions of subjectivity used in the establishment of the resources. 
In Semantic Role Labeling (SRL), arguments are usually limited in a syntax subtree. It is reasonable to label arguments locally in such a sub-tree rather than a whole tree. To identify active region of arguments, this paper models Maximal Projection (MP), which is a concept in Dstructure from the projection principle of the Principle and Parameters theory. This paper makes a new deﬁnition of MP in Sstructure and proposes two methods to predict it: the anchor group approach and the single anchor approach. The anchor group approach achieves an accuracy of 87.75% and the single anchor approach achieves 83.63%. Experimental results also indicate that the prediction of MP improves semantic role labeling. 
Shallow parsing is one of many NLP tasks that can be reduced to a sequence labeling problem. In this paper we show that the latent-dynamics (i.e., hidden substructure of shallow phrases) constitutes a problem in shallow parsing, and we show that modeling this intermediate structure is useful. By analyzing the automatically learned hidden states, we show how the latent conditional model explicitly learn latent-dynamics. We propose in this paper the Best Label Path (BLP) inference algorithm, which is able to produce the most probable label sequence on latent conditional models. It outperforms two existing inference algorithms. With the BLP inference, the LDCRF model signiﬁcantly outperforms CRF models on word features, and achieves comparable performance of the most successful shallow parsers on the CoNLL data when further using part-ofspeech features. 
Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules between templates with a single variable. In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method. The results show that the learned unary rule-sets outperform the binary rule-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure. 
Few attempts have been made to investigate the utility of temporal reasoning within machine learning frameworks for temporal relation classiﬁcation between events in news articles. This paper presents three settings where temporal reasoning aids machine learned classiﬁers of temporal relations: (1) expansion of the dataset used for learning; (2) detection of inconsistencies among the automatically identiﬁed relations; and (3) selection among multiple temporal relations. Feature engineering is another effort in our work to improve classiﬁcation accuracy. 
In this paper we describe a methodology for detecting preposition errors in the writing of non-native English speakers. Our system performs at 84% precision and close to 19% recall on a large set of student essays. In addition, we address the problem of annotation and evaluation in this domain by showing how current approaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issues that complicate evaluation of error detection systems. 
The task of identifying redundant information in documents that are generated from multiple sources provides a signiﬁcant challenge for summarization and QA systems. Traditional clustering techniques detect redundancy at the sentential level and do not guarantee the preservation of all information within the document. We discuss an algorithm that generates a novel graph-based representation for a document and then utilizes a set cover approximation algorithm to remove redundant text from it. Our experiments show that this approach offers a signiﬁcant performance advantage over clustering when evaluated over an annotated dataset. 
In this paper, we propose a data-oriented method for inferring the emotion of a speaker conversing with a dialog system from the semantic content of an utterance. We ﬁrst fully automatically obtain a huge collection of emotion-provoking event instances from the Web. With Japanese chosen as a target language, about 1.3 million emotion provoking event instances are extracted using an emotion lexicon and lexical patterns. We then decompose the emotion classiﬁcation task into two sub-steps: sentiment polarity classiﬁcation (coarsegrained emotion classiﬁcation), and emotion classiﬁcation (ﬁne-grained emotion classiﬁcation). For each subtask, the collection of emotion-proviking event instances is used as labelled examples to train a classiﬁer. The results of our experiments indicate that our method significantly outperforms the baseline method. We also ﬁnd that compared with the singlestep model, which applies the emotion classiﬁer directly to inputs, our two-step model signiﬁcantly reduces sentiment polarity errors, which are considered fatal errors in real dialog applications. 
State-of-the-art statistical parsing models applied to free word-order languages tend to underperform compared to, e.g., parsing English. Constituency-based models often fail to capture generalizations that cannot be stated in structural terms, and dependency-based models employ a ‘single-head’ assumption that often breaks in the face of multiple exponence. In this paper we suggest that the position of a constituent is a form manifestation of its grammatical function, one among various possible means of realization. We develop the Relational-Realizational approach to parsing in which we untangle the projection of grammatical functions and their means of realization to allow for phrase-structure variability and morphological-syntactic interaction. We empirically demonstrate the application of our approach to parsing Modern Hebrew, obtaining 7% error reduction from previously reported results. 
We address corpus building situations, where complete annotations to the whole corpus is time consuming and unrealistic. Thus, annotation is done only on crucial part of sentences, or contains unresolved label ambiguities. We propose a parameter estimation method for Conditional Random Fields (CRFs), which enables us to use such incomplete annotations. We show promising results of our method as applied to two types of NLP tasks: a domain adaptation task of a Japanese word segmentation using partial annotations, and a partof-speech tagging task using ambiguous tags in the Penn treebank corpus. 
Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a uniﬁed approach. We propose to subsume a broad range of phenomena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology. 
We report on the large-scale acquisition of class attributes with and without the use of lists of representative instances, as well as the discovery of unary attributes, such as typically expressed in English through prenominal adjectival modiﬁcation. Our method employs a system based on compositional language processing, as applied to the British National Corpus. Experimental results suggest that documentbased, open class attribute extraction can produce results of comparable quality as those obtained using web query logs, indicating the utility of exploiting explicit occurrences of class labels in text. 
In this paper, an extension of a dimensionality reduction algorithm called NONNEGATIVE MATRIX FACTORIZATION is presented that combines both ‘bag of words’ data and syntactic data, in order to ﬁnd semantic dimensions according to which both words and syntactic relations can be classiﬁed. The use of three way data allows one to determine which dimension(s) are responsible for a certain sense of a word, and adapt the corresponding feature vector accordingly, ‘subtracting’ one sense to discover another one. The intuition in this is that the syntactic features of the syntax-based approach can be disambiguated by the semantic dimensions found by the bag of words approach. The novel approach is embedded into clustering algorithms, to make it fully automatic. The approach is carried out for Dutch, and evaluated against EuroWordNet. 
This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%96.7% accuracy depending on classification method). The paper also examines in detail which positive markers are most powerful and identifies a number of linguistic aspects as well as culture- and domain-related ones.1 
Creative metaphor is a phenomenon that stretches and bends the conventions of semantic description, often to humorous and poetic extremes. The computational modeling of metaphor thus requires a knowledge representation that is just as stretchable and semantically accommodating. We present here a ﬂexible knowledge representation for metaphor interpretation and generation, called Talking Points, and describe how talking points can be acquired on a large scale from WordNet (Fellbaum, 1998) and from the web. We show how talking points can be ﬂuidly connected to form a slipnet, and demonstrate that talking points provide an especially concise representation for concepts in general. 
In this paper, we extend an existing paragraph retrieval approach to why-question answering. The starting-point is a system that retrieves a relevant answer for 73% of the test questions. However, in 41% of these cases, the highest ranked relevant answer is not ranked in the top-10. We aim to improve the ranking by adding a reranking module. For re-ranking we consider 31 features pertaining to the syntactic structure of the question and the candidate answer. We ﬁnd a signiﬁcant improvement over the baseline for both success@10 and MRR@150. The most important features for re-ranking are the baseline score, the presence of cue words, the question’s main verb, and the relation between question focus and document title. 
Various types of structural information e.g., about the type of constructions in which binding constraints apply, or about the structure of names - play a central role in coreference resolution, often in combination with lexical information (as in expletive detection). Kernel functions appear to be a promising candidate to capture structure-sensitive similarities and complex feature combinations, but care is required to ensure they are exploited in the best possible fashion. In this paper we propose kernel functions for three subtasks of coreference resolution - binding constraint detection, expletive identiﬁcation, and aliasing - together with an architecture to integrate them within the standard framework for coreference resolution. 
Previous methods usually conduct the keyphrase extraction task for single documents separately without interactions for each document, under the assumption that the documents are considered independent of each other. This paper proposes a novel approach named CollabRank to collaborative single-document keyphrase extraction by making use of mutual influences of multiple documents within a cluster context. CollabRank is implemented by first employing the clustering algorithm to obtain appropriate document clusters, and then using the graph-based ranking algorithm for collaborative single-document keyphrase extraction within each cluster. Experimental results demonstrate the encouraging performance of the proposed approach. Different clustering algorithms have been investigated and we find that the system performance relies positively on the quality of document clusters. 
We present recent work in the area of Cross-Domain Dialogue Act tagging. Our experiments investigate the use of a simple dialogue act classiﬁer based on purely intra-utterance features - principally involving word n-gram cue phrases. We apply automatically extracted cues from one corpus to a new annotated data set, to determine the portability and generality of the cues we learn. We show that our automatically acquired cues are general enough to serve as a cross-domain classiﬁcation mechanism. 
It is difficult to identify sentence importance from a single point of view. In this paper, we propose a learning-based approach to combine various sentence features. They are categorized as surface, content, relevance and event features. Surface features are related to extrinsic aspects of a sentence. Content features measure a sentence based on contentconveying words. Event features represent sentences by events they contained. Relevance features evaluate a sentence from its relatedness with other sentences. Experiments show that the combined features improved summarization performance significantly. Although the evaluation results are encouraging, supervised learning approach requires much labeled data. Therefore we investigate co-training by combining labeled and unlabeled data. Experiments show that this semisupervised learning approach achieves comparable performance to its supervised counterpart and saves about half of the labeling time cost. 
 Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text. In this paper, we propose a method to perform domain adaptation for statistical machine translation, where in-domain bilingual corpora do not exist. This method first uses out-of-domain corpora to train a baseline system and then uses in-domain translation dictionaries and in-domain monolingual corpora to improve the indomain performance. We propose an algorithm to combine these different resources in a unified framework. Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-ofdomain corpora.  
This paper presents an approach for substantially reducing the time needed to calculate the shortest paths between all concepts in a wordnet. The algorithm exploits the unique “star-like” topology of wordnets to cut down on time-expensive calculations performed by algorithms to solve the all-pairs shortest path problem in general graphs. The algorithm was applied to two wordnets of two different languages: Princeton WordNet (Fellbaum, 1998) for English, and GermaNet (Kunze and Lemnitzer, 2002), the German language wordnet. For both wordnets, the time needed for ﬁnding all shortest paths was brought down from several days to a matter of minutes. 
Bracketing Transduction Grammar (BTG) is a natural choice for effective integration of desired linguistic knowledge into statistical machine translation (SMT). In this paper, we propose a Linguistically Annotated BTG (LABTG) for SMT. It conveys linguistic knowledge of source-side syntax structures to BTG hierarchical structures through linguistic annotation. From the linguistically annotated data, we learn annotated BTG rules and train linguistically motivated phrase translation model and reordering model. We also present an annotation algorithm that captures syntactic information for BTG nodes. The experiments show that the LABTG approach signiﬁcantly outperforms a baseline BTGbased system and a state-of-the-art phrasebased system on the NIST MT-05 Chineseto-English translation task. Moreover, we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering. 
Words in Chinese text are not naturally separated by delimiters, which poses a challenge to standard machine translation (MT) systems. In MT, the widely used approach is to apply a Chinese word segmenter trained from manually annotated data, using a ﬁxed lexicon. Such word segmentation is not necessarily optimal for translation. We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT. Experiments show that our method improves a state-ofthe-art MT system in a small and a large data environment. 
In this paper we describe an empirical study of human-human multi-tasking dialogues (MTD), where people perform multiple verbal tasks overlapped in time. We examined how conversants switch from the ongoing task to a real-time task. We found that 1) conversants use discourse markers and prosodic cues to signal task switching, similar to how they signal topic shifts in single-tasking speech; 2) conversants strive to switch tasks at a less disruptive place; and 3) where they cannot, they exert additional effort (even higher pitch) to signal the task switching. Our machine learning experiment also shows that task switching can be reliably recognized using discourse context and normalized pitch. These ﬁndings will provide guidelines for building future speech interfaces to support multi-tasking dialogue. 
This paper presents a new approach for term extraction using minimal resources. A term candidate extraction algorithm is proposed to identify features of the relatively stable and domain independent term delimiters rather than that of the terms. For term verification, a link analysis based method is proposed to calculate the relevance between term candidates and the sentences in the domain specific corpus from which the candidates are extracted. The proposed approach requires no prior domain knowledge, no general corpora, no full segmentation and minimal adaptation for new domains. Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show quite significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach. Experiments on new term extraction also indicate that the approach is quite effective for identifying new terms in a domain making it useful for domain knowledge update. 
As human beings, our mental processes for recognising linguistic symbols generate perceptual neighbourhoods around such symbols where confusion errors occur. Such neighbourhoods also provide us with conscious mental associations between symbols. This paper formalises orthographic models for similarity of Japanese kanji, and provides a proofof-concept dictionary extension leveraging the mental associations provided by orthographic proximity. 
This paper proposes an approach using large scale case structures, which are automatically constructed from both a small tagged corpus and a large raw corpus, to improve Chinese dependency parsing. The case structure proposed in this paper has two characteristics: (1) it relaxes the predicate of a case structure to be all types of words which behaves as a head; (2) it is not categorized by semantic roles but marked by the neighboring modifiers attached to a head. Experimental results based on Penn Chinese Treebank show the proposed approach achieved 87.26% on unlabeled attachment score, which significantly outperformed the baseline parser without using case structures. 
Annotated corpora are only useful if their annotations are consistent. Most large-scale annotation efforts take special measures to reconcile inter-annotator disagreement. To date, however, no-one has investigated how to automatically determine exemplars in which the annotators agree but are wrong. In this paper, we use OntoNotes, a large-scale corpus of semantic annotations, including word senses, predicate-argument structure, ontology linking, and coreference. To determine the mistaken agreements in word sense annotation, we employ word sense disambiguation (WSD) to select a set of suspicious candidates for human evaluation. Experiments are conducted from three aspects (precision, cost-effectiveness ratio, and entropy) to examine the performance of WSD. The experimental results show that WSD is most effective on identifying erroneous annotations for highly-ambiguous words, while a baseline is better for other cases. The two methods can be combined to improve the cleanup process. This procedure allows us to find approximately 2% remaining erroneous agreements in the OntoNotes corpus. A similar procedure can be easily defined to check other annotated corpora. 
We propose a new integrated approach based on Markov logic networks (MLNs), an effective combination of probabilistic graphical models and ﬁrstorder logic for statistical relational learning, to extracting relations between entities in encyclopedic articles from Wikipedia. The MLNs model entity relations in a uniﬁed undirected graph collectively using multiple features, including contextual, morphological, syntactic, semantic as well as Wikipedia characteristic features which can capture the essential characteristics of relation extraction task. This model makes simultaneous statistical judgments about the relations for a set of related entities. More importantly, implicit relations can also be identiﬁed easily. Our experimental results showed that, this integrated probabilistic and logic model signiﬁcantly outperforms the current stateof-the-art probabilistic model, Conditional Random Fields (CRFs), for relation extraction from encyclopedic articles. 
We describe and evaluate a new method of automatic seed word selection for unsupervised sentiment classification of product reviews in Chinese. The whole method is unsupervised and does not require any annotated training data; it only requires information about commonly occurring negations and adverbials. Unsupervised techniques are promising for this task since they avoid problems of domain-dependency typically associated with supervised methods. The results obtained are close to those of supervised classifiers and sometimes better, up to an F1 of 92%. 
We generalize Uno and Yagiura’s algorithm for ﬁnding all common intervals of two permutations to the setting of two sequences with many-to-many alignment links across the two sides. We show how to maximally decompose a word-aligned sentence pair in linear time, which can be used to generate all possible phrase pairs or a Synchronous Context-Free Grammar (SCFG) with the simplest rules possible. We also use the algorithm to precisely analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 
Many reordering approaches have been proposed for the statistical machine translation (SMT) system. However, the information about the type of source sentence is ignored in the previous works. In this paper, we propose a group of novel reordering models based on the source sentence type for Chinese-toEnglish translation. In our approach, an SVM-based classifier is employed to classify the given Chinese sentences into three types: special interrogative sentences, other interrogative sentences, and non-question sentences. The different reordering models are developed oriented to the different sentence types. Our experiments show that the novel reordering models have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system. 
This paper presents a general platform, namely synchronous tree sequence substitution grammar (STSSG), for the grammar comparison study in Translational Equivalence Modeling (TEM) and Statistical Machine Translation (SMT). Under the STSSG platform, we compare the expressive abilities of various grammars through synchronous parsing and a real translation platform on a variety of Chinese-English bilingual corpora. Experimental results show that the STSSG is able to better explain the data in parallel corpora than other grammars. Our study further finds that the complexity of structure divergence is much higher than suggested in literature, which imposes a big challenge to syntactic transformationbased SMT. 
The need for syntactically annotated data for use in natural language processing has increased dramatically in recent years. This is true especially for parallel treebanks, of which very few exist. The ones that exist are mainly hand-crafted and too small for reliable use in data-oriented applications. In this paper we introduce a novel platform for fast and robust automatic generation of parallel treebanks. The software we have developed based on this platform has been shown to handle large data sets. We also present evaluation results demonstrating the quality of the derived treebanks and discuss some possible modiﬁcations and improvements that can lead to even better results. We expect the presented platform to help boost research in the ﬁeld of dataoriented machine translation and lead to advancements in other ﬁelds where parallel treebanks can be employed. 
 2Harbin Institute of Technology Harbin, China  {mingzhou,muli,dozhang} @microsoft.com  {bowang,Shujieliu,tjzhao} @mtlab.hit.edu.cn  Abstract We present a diagnostic evaluation platform which provides multi-factored evaluation based on automatically constructed check-points. A check-point is a linguistically motivated unit (e.g. an ambiguous word, a noun phrase, a verb~obj collocation, a prepositional phrase etc.), which are pre-defined in a linguistic taxonomy. We present a method that automatically extracts check-points from parallel sentences. By means of checkpoints, our method can monitor a MT system in translating important linguistic phenomena to provide diagnostic evaluation. The effectiveness of our approach for diagnostic evaluation is verified through experiments on various types of MT systems. 
In this paper, we address the issue of deciding when to stop active learning for building a labeled training corpus. Firstly, this paper presents a new stopping criterion, classification-change, which considers the potential ability of each unlabeled example on changing decision boundaries. Secondly, a multi-criteriabased combination strategy is proposed to solve the problem of predefining an appropriate threshold for each confidence-based stopping criterion, such as max-confidence, min-error, and overalluncertainty. Finally, we examine the effectiveness of these stopping criteria on uncertainty sampling and heterogeneous uncertainty sampling for active learning. Experimental results show that these stopping criteria work well on evaluation data sets, and the combination strategies outperform individual criteria. 
This paper addresses two issues of active learning. Firstly, to solve a problem of uncertainty sampling that it often fails by selecting outliers, this paper presents a new selective sampling technique, sampling by uncertainty and density (SUD), in which a k-Nearest-Neighbor-based density measure is adopted to determine whether an unlabeled example is an outlier. Secondly, a technique of sampling by clustering (SBC) is applied to build a representative initial training data set for active learning. Finally, we implement a new algorithm of active learning with SUD and SBC techniques. The experimental results from three real-world data sets show that our method outperforms competing methods, particularly at the early stages of active learning. 
Probabilistic synchronous context-free grammar (PSCFG) translation models deﬁne weighted transduction rules that represent translation and reordering operations via nonterminal symbols. In this work, we investigate the source of the improvements in translation quality reported when using two PSCFG translation models (hierarchical and syntax-augmented), when extending a state-of-the-art phrasebased baseline that serves as the lexical support for both PSCFG models. We isolate the impact on translation quality for several important design decisions in each model. We perform this comparison on three NIST language translation tasks; Chinese-to-English, Arabic-to-English and Urdu-to-English, each representing unique challenges. 
This paper investigates the extent to which a useful general purpose Translation Memory (TM) can be built based on very large amounts of heterogeneous parallel texts mined from the Web. In particular, we evaluate whether such a TM could add value over TMs built from other large, publicly available parallel corpora, such as the Canadian Hansard. In the case of Canadian translators working with English and French, we show that the answer to both questions is a resounding yes. Using field data collected through contextualized observation and interviews with translators at their workplace, we show how this concept is well grounded in existing workpractices of translators, especially Canadian ones. We also show that a TM based on 10 million pairs of pages from Government of Canada Web sites is able to cover 90% of the translation problems observed in our interview subjects. This turns out to be significantly better than coverage of a general purpose TM built from a smaller corpus, namely, the Canadian Hansard. The difference is most notable for the harder problems, such as specialized terminology. We also evaluate the approach on Web parallel corpora for other languages (European Commission Web sites, and 5000 Inuktitut-English pages harvested from the Nunavut domain), and find the approach to not be as advantageous there. We conclude that, while the concept of building TMs from Web corpora holds great promise, more research may be needed to make it work for language pairs other than English-French. 1. INTRODUCTION This paper investigates the extent to which a useful general purpose Translation Memory (TM) can be built based on very large amounts of heterogeneous parallel texts mined from the Web. In particular, we evaluate whether such a TM could add value over TMs built from other large, publicly available parallel corpora, such as the Canadian Hansard. We call this concept WeBiText, and one can think of it as a sort of “Google of parallel text”. We believe that it may be an attractive concept, especially for translators working as freelancers, or within small to medium organizations. Indeed, although conventional Translation Memories (TM) and Bitexts are  routinely used in most large translation organizations, these tools have not been as readily adopted by smaller outfits. An important obstacle to deployment in this sort of environment is the lack of availability of parallel corpora sufficiently large and rich to cover most translation problems encountered by translators in their work. A WeBiText might address this issue by providing translators with TMs that are pre-seeded with large and varied amounts of parallel content harvested from the Web. The paper is organized as follows. Section 2 describes prior work which is relevant to the concept of a WeBiText. Section 3 justifies the concept using qualitative and quantitative data gathered through contextualized observation and interview of professional translators at work in their normal environment. Section 4 describes the WeBiText framework, which we designed in order to deal with the unique technical challenges involved in building a Translation Memory from such heterogeneous and often unpredictable sources. Section 5 describes three instances of a WeBiText which we built based on different Internet domains, to cover different language pairs. Section 6 evaluates the degree to which these three WeBiText instances are able to cover the different types of translation difficulties encountered by translators in their work. Finally, Section 7 offers conclusions and directions for future research. 2. RELATED WORK The idea of building a general purpose TM based on publicly available parallel texts has already been investigated in the context of the TransSearch project (Macklovitch et al, 2000). This is a bilingual concordancer (English-French) based on the Canadian Hansard (transcripts of the debates at the Canadian House of Commons), one of the most widely studied aligned corpora in translation technology research. The system has been deployed as a commercial product, and has been in daily use by translators since 2003. It is mostly used to find translations of short units (2-3 words), which are either generic phraseology expressions (ex: “as a result of”, “at this point”) or general language, highly polysemic words (ex: “meaningful”, “hopefully”) (Simard et al, 2005, Macklovitch et al, 2008). Although the system has a Web interface, the actual corpus was not harvested from the Web, and its size and diversity are much  smaller than what we aim to build in the context of the WeBiText project. This is important because, as pointed out by Macklovitch et al (2008): “[...] the collections currently offered in TransSearch only cover a few technical domains, most notably court rulings and labour relations; otherwise, the majority of the text found in TransSearch are parliamentary debates. Hence, for translation problems involving technical terminology, Canadian translators would be well-advised to consult one of these large term banks [i.e. TERMIUM, Gdt], rather than TransSearch.” Also, in a survey asking users about ways to improve TransSearch (Macklovitch, 2000), the same authors found that 58% of the respondents suggested including more data from non-Hansard domains. Also, the domains suggested by users tended to cover a wide range of fields. Consequently, a particularly interesting research question in the context of WeBiText, is whether increasing the size and diversity of the corpus through Web mining might address this particular craving. Other projects have also aimed at building large, general purpose TMs based on donated corpora. This includes initiatives such as TAUS Data Association (TAD)1, Very Large Translation Memory (VLTM)2, and My Memory3. These systems have never been the object of scientific evaluation and publications. One possible drawback of this kind of approach is that, since it it relies on corpus donations, it may prove difficult for the TM to rapidly achieve sufficient critical mass to be useful. In contrast, by harvesting the large amounts of parallel texts that already exist on the Web, a WeBiText may be able to reach critical mass rapidly without relying on the goodwill of hundreds, if not thousands of translation organizations. The downside however, is that Web parallel corpora may be much noisier than donated corpora, since the data has not been specifically prepared for inclusion into a TM. The idea of harvesting parallel corpora from the Web has also been investigated (Nie et al, 1999, McEwan et al, 2002, Resnick and Noah, 2003, Fattah et al, 2004). However, this work focused on how such corpora can help with tasks like Cross Lingual Information Retrieval, automatic building of bilingual dictionaries, and training of Statistical Machine Translation systems. In contrast, the present paper evaluates the extent to which TMs based on Web corpora can help translators resolve typical translation problems that they encounter in their work. 3. JUSTIFICATION OF CONCEPT The concept of a WeBiText was inspired and motivated by a Contextual Inquiry study carried out in order to better understand translators' workpractices, their use of technology, and how new technologies might assist them better in their work (Désilets et al, 
We will describe ongoing work in developing a collaborative environment to construct a CLIRbased multilingual terminological dictionary dedicated to the Digital Silk Road project and web site launched and managed by NII (National institute of Informatics, Japan-Tokyo). A considerable amount of cultural resources has been digitized, including 95 rare books written in 10 different languages. In order to make them searchable and accessible easily by the visitors of the site, themselves multilingual as well, a cross lingual information retrieval system is being built. As these books are very rich in specialized terms, an important part of that endeavour is to gather these terms in many languages in a terminologicial dictionary (a database of terms contianing some information potentially usable to later build a real terminological database). For that purpose, we use a participative approach, where visitors of the online archive are the main source of the terms used in the languages they know, while multilingual online resources are used to initialize the term base through a process that depends on the archived textual data. 
Dr. Hendrik J. KOCKAERT Lessius/KULeuven Department of Applied Language Studies Sint-Andriesstraat 2 2000 Antwerpen, BELGIUM  Not everyone understands what a completely rational process this is, this maintenance of a motorcycle. <…> A motorcycle functions entirely in accordance with the laws of reason, and a study of the art of motorcycle maintenance is really a miniature study of the art of rationality itself. Robert M. Pirsig, “Zen and the Art of Motorcycle Maintenance” 1. Introduction Life is constantly getting faster, and we are forced to follow the trend, whether we like it or not. Every year, the translation market pursues improved ROI and is therefore shifting from single-piece production to serial production and further to mass production. To weather successfully this development, the translation business needs appropriate automation. Automated translation management has become unavoidable, hence is booming in today's translation business. Like in manufacturing, automated processes in translation result in high volume increase in less time with less effort. But while automating in manufacturing helps exclude “human errors” and therefore avoid deviations and variance in products being manufactured, which contributes to higher quality, the impact of automation on quality in translation has always been questionable. The most commonly accepted belief is that automation minimizes time and cost but gives also rise to reduced quality. Or, in which manner, if at all, does the maxim “Quality Doesn’t Matter” fit in today's translation industry? However, we need to understand exactly what quality in translation, hence in translation assessment, refers to. It has been debated for ages, and is not defined yet. Turning e.g. to the EN 15038 [2006] standard on Translation services and Service requirements, we notice that the standard considers quality of paramount importance, but does not define quality in this context1. "[EN 15038] encompasses the core translation process and all other related aspects involved in providing the service, including quality assurance and traceability". That translation quality is a must-have across the entire translation workflow management, including a positive vendor-buyer relationship, has been put forward by Nataly, Beninatto & DePalma [2008]. While translation service providers [TSPs2] mainly focus on delivering linguistic quality, buyers of translation services often consider the attitude, the quality of service, the relationship between them and the service provider as essential value-added components in translation quality processes. Probably the most comprehensive [but again uncertain] definition is that translation quality refers to conformance to requirements. Numerous quality standards exist in the industry and many providers try to conform to them, but quality is still very subjective and difficult to measure. There is no existing recipe on how to achieve overall quality, because it is unknown what exactly needs to be achieved. 
 2. Infrastructure 2.1. How we chose our wiki The starting point in selecting our wiki software was the comparison of wiki software on Wikipedia, which provides a good review of available software.3 This article currently (21/10/2008) lists over 50 wiki software packages, both open source and proprietary. Wikipedia's article on MediaWiki—the software Wikipedia uses, which is an exceptional reference in itself—convinced me that we should start with MediaWiki. It offers a range of features that we considered potentially useful, in particular templates, and is one of the most popular and best-known open source wiki packages. As the software behind Wikipedia, it is definitely stable and powerful enough for a small or medium-sized LSP, while being easy to install in our intranet environment. Our servers and in-house planning database system run on an intranet, so it was easy to add a TransForm Wiki link to the company intranet homepage. The intranet is accessible from all computers in the local address space (192.168.0.1255) and from computers connected to our network via VPN. 2.2. Infrastructure and expertise necessary to get it up and running The infrastructure necessary to run a wiki in-house is basically a network with a suitable server platform. The commonest type of platform is probably the AMP stack, which incorporates Apache as a Web server, MySQL as a database engine and PHP as the scripting language. This particular combination has the advantage that it is available as open source software for both Windows and Linux. When based on Linux, AMP becomes LAMP—the basis for our intranet. MediaWiki is relatively easy to set up on such a software stack. Our Linux expert, then an IT student, had no difficulty getting MediaWiki up and running. Incidentally, even disregarding licensing issues with Microsoft-based intranets, we believe that Linux offers a better platform for productive-use intranets as opposed to sandbox-level development. Open source packages such as XAMPP for Windows4 do, however, make it possible to play around with ideas on a home network. The introduction of new Linux distributions such as Ubuntu and the wide availability of support for such systems has significantly lowered the barriers to entry here. Getting hold of open source software is easy. Google and Wikipedia make it possible to identify a large number of software packages and determine whether or not they are open source and how they can be obtained. Wikipedia often contains detailed comparative information on types of software, and even on specific packages. Many open source projects are hosted on SourceForge.5 2.3. Hosted wikis as an alternative Many companies offer hosted wikis for businesses. These hosted service providers or "wiki farms" provide a wide range of wikis6 using open-source and proprietary wiki engines. Costs start at free, and licensing can be anything from copyleft or other open-source type licenses to proprietary. Some such providers offer additional facilities such as Wysiwyg editing or business-specific templates and secure private wikis. Such solutions exhibit the same advantages and disadvantages as any remotely hosted "software as a service" or "cloud computing" applications. Issues such as reliability and security must be considered, as well as what happens when the competition buys your main software provider. 2.3.1. TikiWiki TikiWiki is an open source CMS incorporating a structured wiki engine with multilingual capabilities, calendar facilities and a wider range of applications than a pure wild engine like MediaWiki. Like MediaWiki, it can run on a LAMP stack. For these reasons, we have installed and are testing TikiWiki. Dependencies One of the downsides in using open source software is the need to keep compatible versions of the different packages in the stack. We have already experienced incompatibilities between our older chat system based on the Jabber protocol (server: Wildfire, formerly known as Jive Messenger and now called Openfire) and a previous version of TikiWiki. The TikiWiki documentation also contains a dire warning for anyone wanting to install Tiki with multilingual capability: if you install this without PHP 6 (or later) and MySQL 4.2 (or later) "you will regret it later if you have to move Tiki to a new server." Our internal planning database application was written for PHP 4 and will have to be ported to PHP 6 before we can run Tiki on the same server stack. 
 Dr. Fola Yahaya Managing Partner Strategic Agenda LLP fola.yahaya@strategicagenda.com  Globalisation has meant that companies and organisations are increasingly looking to get their point, product or service across to an international audience. This has led to a concomitant rise in the demand for translation and localisation services. The range of languages required and the often ad hoc nature of translation coupled with organisational drives to keep costs low, has meant that this demand is typically met through outsourcing translation projects to translation agencies, language service providers or freelancers. However, the complexity involved in translating and maintaining global content in multiple languages is immense. The issues surrounding translation are exemplified further when content needs to be delivered to customers in multiple formats, through multiple distribution channels in geographically dispersed locations. As a result global translation projects often suffer from issues such as:  poor communication inter and intra staff within translation project teams  lack of automated technology processes to manage, grow and protect translation assets, which put a company’s intellectual property at risk  delays in disseminating centralised terminology militate against content quality and consistency drives  quality compromise as a result of pressure to deliver content in more languages in reduced time while operating under tighter budgets. Thus, whilst outsourcing translation projects may reduce short-terms costs, the increased quality and information insecurity risks may mean a higher total cost in the long run.  Managing complex translation projects through virtual spaces: A case study Innovations in computer-aided translation software and server-based terminology management have improved the ability of disparate teams of translators to produce a more consistent output. However, there is much room for improvement in global collaboration processes and technology. This paper explores the practical application of technologies and methods that translation companies and their clients can deploy to mitigate the risks of outsourced translations and increase the probability of a successful project outcome. The paper focuses on the practical use of online collaboration tools such as ‘virtual spaces’ which can effectively support global project collaboration between geographically dispersed teams of translators. The methodological tool employed is the descriptive case study method. The case study is based on the execution of a complex global translation project for a division within the United Nations. A key aspect of this was the use of a virtual online space to mediate all project communication and interaction. The outcome was faster project turnaround and greater terminology consistency and quality. The main contribution of this paper is an analysis of the practical value of managing global translation projects through the use of virtual spaces. Keywords Terminology management, translation memory, global collaboration, virtual spaces, intranet, online collaboration, web 3.0, the semantic web, computer-mediated communication. 2|P a g e  Managing complex translation projects through virtual spaces: A case study Table of Contents Abstract ............................................................................................................................................ 1 1. Introduction .............................................................................................................................. 4 2. Case Study: a complex translation project for the United Nations .............................................. 5 3. Analysis of case study .............................................................................................................. 10 4. Key findings and lessons learned.............................................................................................. 12 5. Greater automation................................................................................................................. 14 6. Conclusion............................................................................................................................... 17 Figure 1: Sample Translation Project Virtual Space ............................................................................ 6 Figure 2: Strategic Agenda Process for managing complex translation projects via virtual spaces ...... 8 3|P a g e  Managing complex translation projects through virtual spaces: A case study 1. Introduction Globalisation has meant that companies and organisations are increasingly looking to get their point, product or service across to an international audience. This has led to a concomitant rise in the demand for translation and localisation services. The range of languages required and the often ad hoc nature of translation coupled with organisational drives to keep costs low, has meant that this demand is typically met through outsourcing translation projects to a Language Service Provider (LSP) or freelance translators. However, the complexity involved in translating and maintaining global content in multiple languages is immense. The issues surrounding translation are exemplified further when content needs to be delivered to customers in multiple formats, through multiple distribution channels in geographically dispersed locations. As a result global translation projects often suffer from issues such as:  poor communication inter and intra staff within translation project teams  lack of automated technology processes to manage, grow and protect translation assets, which put a company’s intellectual property at risk  delays in disseminating centralised terminology militate against content quality and consistency drives  quality compromise as a result of pressure to deliver content in more languages in reduced time while operating under tighter budgets. Thus, whilst outsourcing translation projects may reduce short-term costs, the increased quality and information insecurity risks may mean a higher total cost in the long run. The translation industry has responded with innovations in computer aided translation software and server-based terminology management, which have improved the ability of disparate teams of translators to produce a consistent output. However, there is much room for improvement in global collaboration processes and technology. This paper discusses an alternative approach to translation workflow for complex translation projects that improves the probability of a quality product via a mediated virtual space. Complex translation projects are defined as high volume (over 50,000 words), translation of non-standard texts into multiple languages requiring multiple translators in each language. 4|P a g e  Managing complex translation projects through virtual spaces: A case study 2. Case Study: a complex translation project for the United Nations Strategic Agenda was approached by a division of the United Nations to translate a large report and associated project collateral into several languages within three weeks. The traditional use of inhouse staff was not feasible given the lack of staff availability and the short deadline, but there were fears that without significant experience of, and expertise in, the specialised fields covered by the report the outcome would fall far below the high quality standards expected of a publication of this kind. The client organisation tendered the translation project in the hope that they would be able to find a Language Service Provider (LSP) that fitted the profile. After a competitive tender process Strategic Agenda was selected and the project setup process embarked upon. From an analysis of the project requirements it was clear that deploying the traditional translation project management approach would not be sufficient. An analysis of client requirements during the bidding process had highlighted that the high word count and short deadline meant that a team of translators would be needed. Complex, specialised texts require a far greater degree of research and term validation than simpler text hence the UN itself recommends that translators translate no more than 1500 words per day. This meant that at least four translators were needed in order to meet the project deadline. However, there is evidently an inverse relationship between the number of translators deployed on a project and the consistency of the final translation. To combat this risk, and the problems associated with the traditional translation lifecycle mentioned above, Strategic Agenda devised a translation process that involved greater client/translator interaction supported by a virtual space or collaboration platform (see Figure 1). The first phase of this process was the set up of a virtual project space. This space was an online meeting place where documents, messages and real time chat could take place in order to allow all team members to interact. 5|P a g e  Managing complex translation projects through virtual spaces: A case study Figure 1: Example Translation Project Virtual Space Strategic Agenda chose a hosted platform that excelled in usability and which had features including:  Messaging via Outlook – users can reply to project questions via MS outlook  Integrated chat room for real time discussion  Group calendars  Group to do lists  Project whiteboard that enables interactive editing of a document  Secure file management and sharing  Granular permissioning structure that would allow for multiple levels of messaging. Messages can sent via the virtual space to individuals, language groups, just Strategic Agenda employees or clients or to everyone on the project 6|P a g e  Managing complex translation projects through virtual spaces: A case study Once the space had been set up, team members were sent information on how to use it and were invited to participate in a real-time project kick off session in the project chat room to allow everyone to get to know their fellow translators and client counterparts understand roles and responsibilities and agree milestones. This virtual meeting was an important step in building corps d’esprit and breaking down the traditional divide between client reviewer and translator. As part of the project set up phase, an extraction of the key terms within the text to be translated was conducted. Without an existing translation memory and a translation team of four geographically disparate translators, it was critical that there was firm agreement on the key terms in the document. Extraction was carried out using a tool called Multiterm Extract. Multiterm identifies and extracts corporate terminology using a powerful statistical analysis to determine the frequency of appearance of term candidates. Key terms were then extracted and translated by a single senior translator. These terms were then posted to the project space whiteboard for debate within a defined period before being sent to the client for final sign off. 7|P a g e  Figure 2: Strategic Agenda Process for managing complex translation projects via virtual spaces  These terms were agreed by the client and then exported into a Multiterm termbase which was made available through the project space for use by the translators. Stage two of the process was the iterative translation of documents which had been divided up and assigned to the translators. During this stage, translators were able to ask the client reviewers and their fellow translations questions relating to terminology and style. These questions were typically posted publicly and copied to all on the translation team, which meant instant adjustments could be made to the various segments. Once the draft translation was complete a number of internal proofreading iterations were carried out during stage three between Strategic Agenda’s translators and reviewers before the final document was presented to the client reviewer for final sign off in stage four. The translation of all of the documents was achieved on target and the quality was adjudged to be excellent. Moreover the client acknowledged the key role played by an iterative translation process supported by virtual spaces.  Managing complex translation projects through virtual spaces: A case study 3. Analysis of case study The United Nations is one of the largest multilateral development agencies in the world, and is one of the largest global buyers of translation services. Traditionally most of its translation expenditure has been for the employment of in-house language specialists. However, in an attempt to reduce costs translation is being increasingly outsourced to third parties – typically freelancers or language service providers (LSPs). The speed of outsourcing has been slowed down by a certain degree of reluctance over using third parties who often inexperienced the often arcane terminology of various branches of the institution. In-house translators have had to pass rigorous and exceptionally competitive exams to achieve their coveted positions and there is still a perception that an LSP can never achieve the same level of quality as an experienced translator – many of whom are ex-staff members. However, this reluctance to outsource holds little sway in an era of aggressive cost cutting and efficiency drives, so organisations are increasingly seeking out LSPs who can offer them the best of both worlds: direct access to experienced translators without the overhead of in-house staff. Innovations in translation technologies and processes in the last 10 years have seen the rapid uptake of translation management software and increasingly globalised translation projects designed to improve the quality of outsourced translation. Coupled with low cost, efficient communication channels translation has truly become a de-coupled industry, with work increasingly being carried out by decentralised and often geographically dispersed translation resources. Yet, despite the increased availability of tools and technologies to support translation projects, traditional approaches still dominate the industry. Companies often have unrealistic expectations that translation is simple and that they can outsource their requirements to a third party only to have their expectations dashed when they receive sub-standard translations carried out using a standard translation lifecycle approach. The traditional translation lifecycle involves the following steps:  document divided into parts  parts assigned to individual translators  translators work in isolation  reviewers consolidate individual sections into a single document ready for client review 10 | P a g e  Managing complex translation projects through virtual spaces: A case study  client reviewer assesses and corrects text as necessary  LSP returns finalised text to client for final sign off The problems with this approach include:  opaqueness - lack of process transparency leaves clients in the dark on translation quality until it is (sometimes) too late  inconsistency- consolidation is difficult, even when working from an existing termbase due to different styles and interpretations of translators  lack of interaction - there is little if any direct interaction between client and translator  too little too late – too little time and resource is allocated to document revision which means that finalising a file for say publication is squeezed into the end of translation project  rapid incorporation of changes can be difficult to achieve without a centralised messaging platform By using a virtual space the project overcame the traditional problems associated with dispersed teams working in isolation. Client can replicate their direct relationship with a translator through a mediated platform. Their anxiety over the quality of the final ‘product’ is lessened by their interaction with the actual translator working on their project and their ability to shape the translation as it progresses. Thus virtual spaces offer clear benefits to all parties involved: client, LSP and freelance translator. 11 | P a g e  Managing complex translation projects through virtual spaces: A case study 4. Key findings and lessons learned The project exemplifies an innovative approach to rapid translation of high volumes of specialised text supported by a virtual space. The key findings of the project case study were that virtual spaces can be a valuable tool for supporting global translation project management. Benefits included:  increased buy-in: translators feel part of the team rather than hired hands  interacting with the client review team from the project outset improves the translation/reviewer dynamic and reduces the likelihood of “not translated by us” syndrome  a shared sense of responsibility for project success means that both the client and reviewer feel they play an important part in the translation process and achieving a successful project outcome  reduces client anxiety over outsourcing their text to third parties - clients enjoy having direct access to translators  translation can be faster – a centralised messaging system means communicating changes to terminology etc is faster and documented  clear audit trail – sophisticated reporting mechanisms mean that all versions are stored and accessible online and who said what when is captured for later analysis. Lessons learned Using virtual spaces effectively is not as simple as setting up a hosted service and hoping for the best. Rather the level of transparency that typifies the majority of collaboration platforms means that client/LSP resource interaction must be carefully managed. Key lessons learned include:  Ensure freelancers are fully briefed on how to interact with the client if that option is enabled. They represent the public face of your company even though they often ‘guns for hire’. Ensure each freelance project resource undergoes a thorough induction in how to use the virtual space.  Carefully manage what is posted online. Early drafts may be better managed offline rather than being posted for the client to see. Alternatively have a private ‘sandbox’ area on your 12 | P a g e  Managing complex translation projects through virtual spaces: A case study virtual space that will allow your internal resources to post and discuss early drafts. Collaboration spaces are like goldfish bowls where potentially everything is visible so manage permissioning (who can see/do what where and when) carefully.  Ensure clients are full briefed on their roles and responsibilities on the translation project. There needs to be an informal service level agreement on how quickly client reviewers can respond to translators questions (typically 24hrs). 13 | P a g e  Managing complex translation projects through virtual spaces: A case study 5. The future of global translation management and collaboration Greater automation It is clear that the trend towards increased automation of translation processes will continue unabated. While the holy grail of inputting complex source text, pressing a button and instantly receiving perfect target text will remain a dream for somewhat longer, it will not be long before companies with considerable translation volumes will insist on a level of automated translation in order to improve the three key criteria in the translation equation: cost, time and quality. While cost and time can be measured exactly, quality remains a subjective factor. This is likely to have an impact on how the key actors in the translation equation, clients, LSPs and translators, collaborate in the future in the following ways: Greater openness and willingness to share translation assets. As translation workflow and linguistic asset management become standard features, industries will collaborate on an agreed terminology and will be increasingly likely to share translation memories in order to reduce costs. It is likely that translation assets - translation memories and terminology – will be organised by industry domains rather than being limited by company boundaries. This will enable the quick assembly of unified linguistic databases for industrial domains. Clients may create industry specific virtual spaces for storing translation assets and making them available to translators et al. Intelligent customers - Companies get smarter about content management Companies will implement controlled language authoring, machine translation, XML based publishing and global workflow systems to improve the quality and speed of translation whilst lowering the costs. As companies collaborate on production so will they collaborate to reduce industry-wide translation costs. Expect the emergence of vertical translation organisations1. 
Bible translation sets a number of particular challenges for machine translation and analysis. The nature of the work is such that translators are often working with local vernacular languages for which there are little in the way of lexica and other linguistic databases. Commercial text processing and translation systems are rarely able to contribute. Translation Consultants (TCs) charged with advising translation teams often have few computer based aids to assist them in their work. This paper describes the development of the Statistical Glossing Tool (SGT) which ships with the Paratext translation editor. SGT is a language independent method for the analysis of bible translations. It provides an objective assessment which TCs can use to help them identify key issues in a new translation where further work may be needed. SGT requires no information about target languages other than the text itself. 
As part of its Masters in Scientific, Technical and Medical Translation with Translation Technology (MScTrans), for the last six years Imperial College has been offering a six-week module component on practical software and website localisation. This module, however, has only been offered to one language group (German > English and English > German). In view of the rising level of interest amongst students and professional translators in localisation as well as the continued importance of localisation in the translation industry, a decision was made in 2007 to transform the current course unit on software and website localisation into a 12-week stand-alone e-learning course. The new e-course is to be made available to all of the MScTrans students (irrespective of their language combination) as an adjunct to the existing teaching. In addition, the course will also be made available to external delegates. The aim of this paper is to describe our journey and to provide an analysis of the feedback received from the testers of this new e-course. During the creation and testing phase of the new e-course, it became clear that a simple transformation of the existing face-to-face course into an e-learning course was not possible and that a lot of work had to be put into adapting the content for an online course. An unexpected finding was that learners got through the online course content in half the time that it took to go through the same material in a classroom setting, even when the group in the class was quite small (2-5 students). 
However, there are also important differences between terms and multiword units. One relates to their morpho-syntactic behaviour. Most terms are nouns or complex nominals. Multi-word units include a much larger proportion of verb phrases. Therefore, multi-word units tend to display a larger degree of variability, making it more difficult for a CAT tool to recognize them in a text. Another difference relates to the ontological status of terms. A term is a link between a form, a concept, and a field. It is therefore possible to collect terms for a particular field and activate them when a text from this field is translated. Multi-word units belong to the general expressive means of a language. Although some of them are marked for register or -1 -  text type, many are entirely unmarked. It is therefore not possible to collect a relatively small subset of multi-word units that are most likely to occur in a particular ST. No criteria comparable to the subject field for terminology can be used. The latter difference has consequences for the identification phase. In terminology it is possible to build a termbase for a field on the basis of a corpus of texts from that field and the study of the subject matter. The translator can be reasonably confident that a new ST from the same field will contain few if any new terms. In the case of multi-word units, individual STs will generally have to be the basis for the identification. There will be too many new multi-word units to rely on an existing database to contain most of them. For this reason, the identification of multi-word units in the ST is much more important than the corresponding process for terms. It is on the basis of these considerations that we decided to explore how term extraction software could be used for the identification of multiword units. Here we will concentrate on MultiTerm Extract, the term extraction component of SDL Trados 2007. The questions we investigated were to what extent MultiTerm Extract supports the professional translator in the identification of multi-word units, what the optimal settings are in this context of use, and how the software might be further developed to improve its support. 2. The experiment 2.1. General considerations In all our experiments we took chapter 10 of UNAIDS (2006) and its official Spanish translation as the material from which multi-word units were extracted. This chapter is entitled "Financing the response to AIDS" and is approximately 10,000 words long. In order to set the target for measuring the success of automatic identification, we started by searching multi-word units manually. We found 72, including at risk, take into account, and close scrutiny. In the course of our experiments, we found a further 18 multi-word units which had been missed in the manual search. The total of 90 was used as a standard for evaluating the success of automatic identification. Term candidates identified by extraction software are always uninterrupted strings. As a consequence, it is not possible to get only the string put to use as a term candidate if it occurs in a context such as (la). -2-  (1) a. That means streamlining the flow of financial resources to the front lines of the epidemic, putting it to optimal use and providing HIV-related prevention, treatment, care and support as quickly as possible to everyone in need. b. putting it to optimal use c. putting it to optimal use and providing HIV-related prevention In evaluating the performance of extraction software, we counted any proposal in which the three components of put, to, and use occur and belong together as successful. The minimal string to be extracted from (la) would be (1b). However, we also accept (1c) as a case where put to use was recognized correctly. MultiTerm Extract was designed to be used both for the identification of term candidates and for the verification of termbases. It supports the five types of project listed in (2). (2) a. Monolingual Extraction b. Bilingual Extraction c. QA Project d. Translation Project e. Dictionary Compilation Project For the identification of multi-word units, only (2a-b) are relevant. Whereas these two are meant to be performed for the collection of term candidates on the basis of texts, the projects in (2c-e) are used when a termbase has already been compiled. 2.2. Setting up a Monolingual Extraction project In a Monolingual Extraction project, a number of settings can be used to influence the selection of term candidates. They are listed in (3). The dialog box in Fig. 1 illustrates the interface for specifying them. (3) a. Minimum term length b. Maximum term length c. Maximum number of extracted terms d. Silence/noise ratio e. Stopword lists f. Learning settings -3-  Fig. 1: Default term extraction settings. The settings (3a-b) together determine the length of the term candidates returned. As we are interested in multi-word units, (3a) should be at least 2. The longest multi-word unit in the target set has a length of 4, so that (3b) should be at least 4. In view of the problem illustrated in the discussion of (1), we also considered a maximum term length of 10. In our experiments we used twelve different settings for (3a-b), three with maximal length 4 and minimal length ranging from 2 to 4 and nine with maximal length 10 and minimal length ranging from 2 to 10. Setting (3c) can be used to restrict the number of term candidates. Assuming that this would simply remove the tail of the list without affecting the order of confidence for the individual items on the list, we did not use this feature in our experiments. Its optimal value might be an outcome of our experiments. Setting (3d) is represented in Fig. 1 as a scale from maximal noise to maximal silence with a default setting in the middle. We will refer to them as noise levels, ranging from 0 (minimal noise) to 1 (minimal silence). The scale in Fig. 1 suggests nine intermediate points. In practice, however, we verified that there was no difference in output for noise -4-  levels from 0.8 to 1 and from 0.6 to 0.7. As a consequence, there are eight different noise levels that can be chosen. The option (3e) makes it possible to specify a stopword list. Normally, a stopword list contains high-frequency words that are not part of the target set. Considering that multi-word units such as as of, all but, and by and large contain typical stopwords, we first tried extraction without specifying a stopword list. We discovered, however, that using the default stopword list reduces noise considerably without excluding such multi-word units. Therefore, we used the default stopword list. The final option, (3f), is not visible in the screen in Fig. 1, but can be specified elsewhere in the project setup. The underlying idea is that the system can learn from the evaluation by the terminologist of the items returned in extraction. In this way, future extraction projects can be more targeted. The way our experiments were set up makes it impractical to use this option. We compared a large number of settings for (3a-d) independently of each other for the same text. In order to evaluate the contribution to the efficiency by Learning Settings, we would have to consider a single setting over a number of texts. Therefore we did not explore this option. 2.3. Setting up a Bilingual Extraction project Official texts on the website of the UNO (of which UNAIDS is a part) are published in five languages. We used the parallel English and Spanish version as a basis for Bilingual Extraction projects. The evaluation of the results can be based on the two questions in (4). (4) a. How does the quality of the output for English compare to the quality of corresponding Monolingual Extraction projects? b. What is the quality of the Spanish translations? The settings for Bilingual Extraction projects include all settings for Monolingual Extraction, so that a point-by-point comparison for the evaluation of (4a) is possible. In addition, the three translation settings in (5) can be specified. The default settings are illustrated in Fig. 2. (5) a. Search for new translations. b. Maximum number of translations. c. Minimum translation frequency. -5-  Fig. 2: Default translation settings for Bilingual Extraction project. Option (5a) is a tickbox. Unticking it restricts the search to pairs of SLTL expressions already recorded in an existing database. In our experiments, we do not have an existing database to start with, so that there is no sense in unticking it. Option (5b) limits the number of different translations for a particular SL term candidate. In the case of genuine terms, this makes sense, because a small number of translations is typical. Rogers (2007), for instance, shows how the ideal of one expression for one concept is approximated very closely in the case of a concept in medical technology. If the target is the identification of multi-word units, no such assumption can be made. Therefore, we unticked this option. Option (5c), finally, limits the number of SL-TL pairs given as output by imposing the condition that each pair has a minimum frequency. Again, this option is more appropriate for terms than for multi-word units. Terms in a particular domain are relatively frequent in a text from that domain and should normally be translated in the same way in each occurrence. Therefore, the default is set at 3. Multi-word units, however, are not necessarily specialized, so that no similar frequency effect can be expected. Therefore we changed the value to 1. -6-  2.4. Processing the list of candidates returned For each project, a number of term candidates is given as output in a format illustrated in Fig. 3. Fig. 3: Output of a term extraction project. Fig. 3 shows the output of a monolingual project. For a bilingual project, an additional TL column is displayed. Each column can be used as a basis for sorting candidates. The column with the form of the candidates, headed by the language name, has tickboxes for each item. They can be used to verify an item (i.e. normally to confirm that it is a term) and to perform operations such as exporting selected items to a termbase. The column labelled Domain gives project-specific information about the candidates. When the output is produced, all candidates have a value of <None>. In each project, the user can specify a list of possible values and then assign one of them to each accepted candidate. In our experiments, we used this column to specify syntactic classes of multi-word units. We can then use the column to sort the multi-word units. The leftmost column in Fig. 3 shows the Score. This is a value between 1 and 99 indicating the system's confidence in proposing the term candidate. It is generated by the system and constitutes an important piece of information to be used in processing the list of results. -7-  3. Overview of results We carried out and evaluated a total of 192 experiments, systematically varying the term length settings (12 combinations as indicated in section 2.2 above) and the noise levels (8 genuinely different values as explained above) for monolingual and bilingual extraction projects. In presenting the results, we will start by considering these three factors in isolation. Then we turn to the evaluation of scores as a possible way to process the results more efficiently. Finally, we discuss how the type of multi-word unit influences retrievability. 3.1. The influence of noise level The large number of experiments with multiple variables makes it difficult to present the results in a clear and systematic way. When we consider an individual parameter, such as noise level, there are at least two interesting questions we can ask, formulated in (6). (6) a. What is the best setting for this parameter? b. How good are the results? The most straightforward question is (6a). Here we compare the different settings of a single parameter. Each setting corresponds to a class of experiments with different settings for other parameters. In our case, the 192 experiments are divided into 8 classes of 24 experiments each. In principle, we are only interested in the best result in each class. The result of each experiment is a list of candidate terms. In (6b) we are interested in criteria to evaluate this list. Classical measures to evaluate such retrieval sets are precision and recall. There is an obvious tendency for precision to go down when recall goes up (and the reverse). Manning & Schütze (1999:269) propose an F-measure which combines the two in order to produce a single evaluation measure for the performance of a retrieval system. Fig. 4 gives a table with these three measures for the different noise levels. -8-  Fig. 4: Precision, recall, and F-measure for different noise levels. As illustrated in Fig. 4 the noise levels of 0.6-0.7 and of 0.8-1.0 produce identical results. A first observation is that precision is overall very low. The highest precision is 16.7%, achieved with noise level 0.1 in a bilingual project with minimal term length 3 and maximal 4. This results in 2 multi-word units found in a list of 12 term candidates. Recall is also very low, except with high noise levels. The highest recall is achieved with noise levels 0.8-1.0 in monolingual projects with minimal term length 3 and maximal 10. This results in 77 multi-word units found in a list of 1498 term candidates. The F-measure generally follows recall, because there are much bigger differences in recall than in precision, but for high noise levels the value does not nearly go up to the same degree. It is interesting to consider the practical implications of these results. Precision is particularly important if the entire set of term candidates returned has to be evaluated manually. In such a scenario finding even a small number of multi-word units by a relatively modest effort is not such a bad result. The alternative is to translate these items without special help. Optimal recall only has a practical relevance if additional methods can be found to manipulate the set of candidate terms in such a way that it is possible to find the multi-word units without going through the entire -9-  set manually. In both cases it has to be kept in mind that items such as put to use have to be isolated from within a larger term candidate. 3.2. The influence of term length settings The questions in (6) can also be asked for the term length settings, but this parameter is different from noise level in two respects. First, the minimal and maximal term length can be varied independently. Second, the set of possible values is not finite. The 12 combined settings we investigated are a sample. Each of these corresponds to a class of 16 experiments. Fig. 5 gives the best values for each of these classes. Fig. 5: Precision, recall, and F-measure for different term lengths It is not surprising that precision and F-measure are low everywhere, because any higher value would have appeared in Fig. 4 as well. It is interesting, however, that with both maximal term lengths considered, the best precision is achieved with a minimal term length of 3. In absolute figures, this corresponds to 2 multi-word units found in 12 for 3-4 and in 13 for 3-10. Recall measures were on the whole better with a maximum term length of 10. Of course, it is to be expected that increasing the minimum term length results in a lower recall. What is remarkable in Fig. 5 is that this drop in recall is very slow. For term lengths 2-10 to 7-10, recall drops from 85.6% to 81.1%. In absolute terms, this corresponds to 77 multiword units found in 1,619 for 2-10 and 73 found in 1,125 for 7-10. By -10-  raising the minimal term length from 2 to 7, we can eliminate over 30% of candidates while sacrificing only 5% of genuine multi-word units.  3.3. Monolingual vs. bilingual projects  Monolingual and bilingual projects can be compared as to the results they yield for English. In addition, bilingual projects can be evaluated for the quality of the translation into Spanish. These two modes correspond to the two questions formulated in (4) above.  We did not expect substantial differences in performance for English between monolingual and bilingual projects. The Spanish translation might in some cases help the system, but it might equally confuse it. In fact, when noise level and term length settings are kept constant, we found that results of monolingual and bilingual projects are identical or very similar. This is illustrated for a number of settings in Table 1.  Project Term length  type Min Max  Mono 10 10  Bi 10 10  Mono 3  4  Bi  3  4  Mono 2  10  Bi 2  10  Mono 2  4  Bi 2  4  Mono 8  10  Bi 8  10  Mono 2  4  Bi 2  4  Noise level 1 1 0.3 0.3 0.1 0.1 0.8 0.8 0.6 0.6 0.7 0.7  Cand. terms 613 646 25 24 21 21 2,482 2,544 26 72 351 444  Expr. Recall Preci-  identified  sion  51  0.5667 0.0832  52  0.5778 0.0805  3  0.0333 0.1200  3  0.0333 0.1250  3  0.0333 0.1429  3  0.0333 0.1429  63  0.7000 0.0254  62  0.6889 0.0244  
While a multitude of machine translation (MT) evaluation metrics exist, most require one or more gold standard references. For the DARPA GALE program, source data is translated according to detailed guidelines2 and high quality standards, but these raw translations then undergo a rigorous and carefully constructed quality control (QC) process to create the final references. GALE evaluation translation references undergo several distinct phases of translation and quality control, utilizing a specially-designed toolkit, QCTool. This paper describes this framework and suggests that it may be successfully applied to improve and streamline other large-scale translation projects. 1. Introduction While a multitude of machine translation (MT) evaluation metrics exist, most require one or more gold standard references. For the DARPA GALE program, source data in four genres (newswire, web, broadcast conversation, and broadcast news) is translated according to detailed guidelines2 and high quality standards, but these raw translations then undergo a rigorous and carefully constructed quality control (QC) process to create the final references. Creating human translations (HT) for the express purpose of MT evaluation requires different standards and priorities than HT created for general use. GALE evaluation translation references undergo multiple distinct phases of translation and quality control. Raw translations are created by translators under contract to the Linguistic Data Consortium 
This paper reports on the use of the Mitkov ́s algorithm for pronoun resolution in texts written in Brazilian Portuguese. Third person pronouns are the only ones focused upon here, with noun phrases as antecedents. A system for anaphora resolution in Brazilian Portuguese texts was built that embeds most of the Mitkov{'}s features. Some of his resolution factors were directly incorporated into the system; others had to be slightly modified for language adequacy. The resulting approach was intrinsically evaluated on hand-annotated corpora. It was also compared to Lappin {\&} Leass{'}s algorithm for pronoun resolution, also customized to Portuguese. Success rate was the evaluation measure used in both experiments. The results of both evaluations are discussed here.
We consider the value of replacing and/or combining string-basedmethods with syntax-based methods for phrase-based statistical machine translation (PBSMT), and we also consider the relative merits of using constituency-annotated vs. dependency-annotated training data. We automatically derive two subtree-aligned treebanks, dependency-based and constituency-based, from a parallel English{--}French corpus and extract syntactically motivated word- and phrase-pairs. We automatically measure PB-SMT quality. The results show that combining string-based and syntax-based word- and phrase-pairs can improve translation quality irrespective of the type of syntactic annotation. Furthermore, using dependency annotation yields greater translation quality than constituency annotation for PB-SMT.
Translation with pivot languages has recently gained attention as a means to circumvent the data bottleneck of statistical machine translation (SMT). This paper tries to give a mathematically sound formulation of the various approaches presented in the literature and introduces new methods for training alignment models through pivot languages. We present experimental results on Chinese-Spanish translation via English, on a popular traveling domain task. In contrast to previous literature, we report experimental results by using parallel corpora that are either disjoint or overlapped on the pivot language side. Finally, our original method for generating training data through random sampling shows to perform as well as the best methods based on the coupling of translation systems.
Large amounts of training data are essential for training statistical machine translations systems. In this paper we show how training data can be expanded by paraphrasing one side. The new data is made by parsing then generating using a precise HPSG based grammar, which gives sentences with the same meaning, but minor variations in lexical choice and word order. In experiments with Japanese and English, we showed consistent gains on the Tanaka Corpus with less consistent improvement on the IWSLT 2005 evaluation data.
This paper is about Translation Dictation with ASR, that is, the use of Automatic Speech Recognition (ASR) by human translators, in order to dictate translations. We are particularly interested in the productivity gains that this could provide over conventional keyboard input, and ways in which such gains might be increased through a combination of ASR and Statistical Machine Translation (SMT). In this hybrid technology, the source language text is presented to both the human translator and a SMT system. The latter produces N-best translations hypotheses, which are then used to fine tune the ASR language model and vocabulary towards utterances which are probable translations of source text sentences. We conducted an ergonomic experiment with eight professional translators dictating into French, using a top of the line off-the-shelf ASR system (Dragon NatuallySpeaking 8). We found that the ASR system had an average Word Error Rate (WER) of 11.7 percent, and that translation using this system did not provide statistically significant productivity increases over keyboard input, when following the manufacturer recommended procedure for error correction. However, we found indications that, even in its current imperfect state, French ASR might be beneficial to translators who are already used to dictation (either with ASR or a dictaphone), but more focused experiments are needed to confirm this. We also found that dictation using an ASR with WER of 4 percent or less would have resulted in statistically significant (p less than 0.6) productivity gains in the order of 25.1 percent to 44.9 percent Translated Words Per Minute. We also evaluated the extent to which the limited manufacturer provided Domain Adaptation features could be used to positively bias the ASR using SMT hypotheses. We found that the relative gains in WER were much lower than has been reported in the literature for tighter integration of SMT with ASR, pointing the advantages of tight integration approaches and the need for more research in that area.
Significant advances have been achieved in Speech-to-Speech (S2S) translation systems in recent years. However, rapid configuration of S2S systems for low-resource language pairs and domains remains a challenging problem due to lack of human translated bilingual training data. In this paper, we report on an effort to port our existing English/Iraqi S2S system to the English/Farsi language pair in just 90 days, using only a small amount of training data. This effort included developing acoustic models for Farsi, domain-relevant language models for English and Farsi, and translation models for English-to-Farsi and Farsi-to-English. As part of this work, we developed two novel techniques for expanding the training data, including the reuse of data from different language pairs, and directed collection of new data. In an independent evaluation, the resulting system achieved the highest performance of all systems.
In an increasingly globalized world, situations in which people of different native tongues have to communicate with each other become more and more frequent. In many such situations, human interpreters are prohibitively expensive or simply not available. Automatic spoken language translation (SLT), as a cost-effective solution to this dilemma, has received increased attention in recent years. For a broad number of applications, including live SLT of lectures and oral presentations, these automatic systems should ideally operate in real time and with low latency. Large and highly specialized vocabularies as well as strong variations in speaking style {--} ranging from read speech to free presentations suffering from spontaneous events {--} make simultaneous SLT of lectures a challenging task. This paper presents our progress in building a simultaneous German-English lecture translation system. We emphasize some of the challenges which are particular to this language pair and propose solutions to tackle some of the problems encountered.
Sentence-aligned bilingual texts are a crucial resource to build statistical machine translation (SMT) systems. In this paper we propose to apply lightly-supervised training to produce additional parallel data. The idea is to translate large amounts of monolingual data (up to 275M words) with an SMT system, and to use those as additional training data. Results are reported for the translation from French into English. We consider two setups: first the intial SMT system is only trained with a very limited amount of human-produced translations, and then the case where we have more than 100 million words. In both conditions, lightly-supervised training achieves significant improvements of the BLEU score.
Similar to phrase-based machine translation, hierarchical systems produce a large proportion of phrases, most of which are supposedly junk and useless for the actual translation. For the hierarchical case, however, the amount of extracted rules is an order of magnitude bigger. In this paper, we investigate several soft constraints in the extraction of hierarchical phrases and whether these help as additional scores in the decoding to prune unneeded phrases. We show the methods that help best.
Search is a central component of any statistical machine translation system. We describe the search for phrase-based SMT in detail and show its importance for achieving good translation quality. We introduce an explicit distinction between reordering and lexical hypotheses and organize the pruning accordingly. We show that for the large Chinese-English NIST task already a small number of lexical alternatives is sufficient, whereas a large number of reordering hypotheses is required to achieve good translation quality. The resulting system compares favorably with the current stateof-the-art, in particular we perform a comparison with cube pruning as well as with Moses.
This paper gives an overview of the evaluation campaign results of the International1Workshop on Spoken Language Translation (IWSLT) 2008 . In this workshop, we focused on the translation of spontaneous speech recorded in a real situation and the feasability of pivot-language-based translation approaches. The translation directions were English into Chinese and vice versa for the Challenge Task, Chinese into English and English into Spanish for the Pivot Task, and Arabic, Chinese, Spanish into English for the standard BTEC Task. In total, 19 research groups building 58 MT engines participated in this year{'}s event. Automatic and subjective evaluations were carried out in order to investigate the impact of spontaneity aspects of field data experiments on automatic speech recognition (ASR) and machine translation (MT) system performance as well as the robustness of state-of-the-art MT systems towards speech-to-speech translation in real environments.
We present the CMU Syntax Augmented Machine Translation System that was used in the IWSLT-08 evaluation campaign. We participated in the Full-BTEC data track for Chinese-English translation, focusing on transcript translation. For this year{'}s evaluation, we ported the Syntax Augmented MT toolkit [1] to the Hadoop MapReduce [2] parallel processing architecture, allowing us to efficiently run experiments evaluating a novel {``}wider pipelines{''} approach to integrate evidence from N -best alignments into our translation models. We describe each step of the MapReduce pipeline as it is implemented in the open-source SAMT toolkit, and show improvements in translation quality by using N-best alignments in both hierarchical and syntax augmented translation systems.
In this paper, we give a description of the machine translation (MT) system developed at DCU that was used for our third participation in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT 2008). In this participation, we focus on various techniques for word and phrase alignment to improve system quality. Specifically, we try out our word packing and syntax-enhanced word alignment techniques for the Chinese{--}English task and for the English{--}Chinese task for the first time. For all translation tasks except Arabic{--}English, we exploit linguistically motivated bilingual phrase pairs extracted from parallel treebanks. We smooth our translation tables with out-of-domain word translations for the Arabic{--}English and Chinese{--}English tasks in order to solve the problem of the high number of out of vocabulary items. We also carried out experiments combining both in-domain and out-of-domain data to improve system performance and, finally, we deploy a majority voting procedure combining a language model-based method and a translation-based method for case and punctuation restoration. We participated in all the translation tasks and translated both the single-best ASR hypotheses and the correct recognition results. The translation results confirm that our new word and phrase alignment techniques are often helpful in improving translation quality, and the data combination method we proposed can significantly improve system performance.
This paper reports on the participation of FBK at the IWSLT 2008 Evaluation. Main effort has been spent on the Chinese-Spanish Pivot task. We implemented four methods to perform pivot translation. The results on the IWSLT 2008 test data show that our original method for generating training data through random sampling outperforms the best methods based on coupling translation systems. FBK also participated in the Chinese-English Challenge task and the Chinese-English and Chinese-Spanish BTEC tasks, employing the standard state-of-the-art MT system Moses Toolkit.
This year's GREYC machine translation (MT) system presents three major changes relative to the system presented during the previous campaign, while, of course, remaining a pure example-based MT system that exploits proportional analogies. Firstly, the analogy solver has been replaced with a truly non-deterministic one. Secondly, the engine has been re-engineered and a better control has been introduced. Thirdly, the data used for translation were the data provided by the organizers plus alignments obtained using a new alignment method. This year we chose to have the engine run with the word as the processing unit on the contrary to previous years where the processing unit used to be the character. The tracks the system participated in are all classic BTEC tracks (Arabic-English, Chinese-English and Chinese-Spanish) plus the so-called PIVOT task, where the test set had to be translated from Chinese into Spanish by way of English.
In this paper, we describe the system and approach used by the Institute for Infocomm Research (I2R) for the IWSLT 2008 spoken language translation evaluation campaign. In the system, we integrate various decoding algorithms into a multi-pass translation framework. The multi-pass approach enables us to utilize various decoding algorithm and to explore much more hypotheses. This paper reports our design philosophy, overall architecture, each individual system and various system combination methods that we have explored. The performance on development and test sets are reported in detail in the paper. The system has shown competitive performance with respect to the BLEU and METEOR measures in Chinese-English Challenge and BTEC tasks.
This paper presents a description for the ICT systems involved in the IWSLT 2008 evaluation campaign. This year, we participated in Chinese-English and English-Chinese translation directions. Four statistical machine translation systems were used: one linguistically syntax-based, two formally syntax-based, and one phrase-based. The outputs of the four SMT systems were fed to a sentence-level system combiner, which was expected to produce better translations than single systems. We will report the results of the four single systems and the combiner on both the development and test sets.
This paper is a description of the system presented by the LIG laboratory to the IWSLT08 speech translation evaluation. The LIG participated, for the second time this year, in the Arabic to English speech translation task. For translation, we used a conventional statistical phrase-based system developed using the moses open source decoder. We describe chronologically the improvements made since last year, starting from the IWSLT 2007 system, following with the improvements made for our 2008 submission. Then, we discuss in section 5 some post-evaluation experiments made very recently, as well as some on-going work on Arabic / English speech to text translation. This year, the systems were ranked according to the (BLEU+METEOR)/2 score of the primary ASR output run submissions. The LIG was ranked 5th/10 based on this rule.
This paper describes the system developed by the LIUM laboratory for the 2008 IWSLT evaluation. We only participated in the Arabic/English BTEC task. We developed a statistical phrase-based system using the Moses toolkit and SYSTRAN{'}s rule-based translation system to perform a morphological decomposition of the Arabic words. A continuous space language model was deployed to improve the modeling of the target language. Both approaches achieved significant improvements in the BLEU score. The system achieves a score of 49.4 on the test set of the 2008 IWSLT evaluation.
This paper describes the MIT-LL/AFRL statistical MT system and the improvements that were developed during the IWSLT 2008 evaluation campaign. As part of these efforts, we experimented with a number of extensions to the standard phrase-based model that improve performance for both text and speech-based translation on Chinese and Arabic translation tasks. We discuss the architecture of the MIT-LL/AFRL MT system, improvements over our 2007 system, and experiments we ran during the IWSLT-2008 evaluation. Specifically, we focus on 1) novel segmentation models for phrase-based MT, 2) improved lattice and confusion network decoding of speech input, 3) improved Arabic morphology for MT preprocessing, and 4) system combination methods for machine translation.
This paper describes the National Institute of Information and Communications Technology/Advanced Telecommunications Research Institute International (NICT/ATR) statistical machine translation (SMT) system used for the IWSLT 2008 evaluation campaign. We participated in the Chinese{--}English (Challenge Task), English{--}Chinese (Challenge Task), Chinese{--}English (BTEC Task), Chinese{--}Spanish (BTEC Task), and Chinese{--}English{--}Spanish (PIVOT Task) translation tasks. In the English{--}Chinese translation Challenge Task, we focused on exploring various factors for the English{--}Chinese translation because the research on the translation of English{--}Chinese is scarce compared to the opposite direction. In the Chinese{--}English translation Challenge Task, we employed a novel clustering method, where training sentences similar to the development data in terms of the word error rate formed a cluster. In the pivot translation task, we integrated two strategies for pivot translation by linear interpolation.
This paper describes our statistical machine translation system (CASIA) used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. In this year's evaluation, we participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Here, we mainly introduce the overview of our system, the primary modules, the key techniques, and the evaluation results.
The NTT Statistical Machine Translation System consists of two primary components: a statistical machine translation decoder and a reranker. The decoder generates k-best translation canditates using a hierarchical phrase-based translation based on synchronous context-free grammar. The decoder employs a linear feature combination among several real-valued scores on translation and language models. The reranker reorders the k-best translation candidates using Ranking SVMs with a large number of sparse features. This paper describes the two components and presents the results for the evaluation campaign of IWSLT 2008.
In this paper, we describe POSTECH system for IWSLT 2008 evaluation campaign. The system is based on phrase based statistical machine translation. We set up a baseline system using well known freely available software. A preprocessing method and a language modeling method have been applied to the baseline system in order to improve machine translation quality. The preprocessing method is to identify and remove useless tokens in source texts. And the language modeling method models phrase level n-gram. We have participated in the BTEC tasks to see the effects of our methods.
The QMUL system to the IWSLT 2008 evaluation campaign is a phrase-based statistical MT system implemented in C++. The decoder employs a multi-stack architecture, and uses a beam to manage the search space. We participated in both BTEC Arabic → English and Chinese → English tracks, as well as the PIVOT task. In our first submission to IWSLT, we are particularly interested in seeing how our SMT system performs with speech input, having so far only worked with and translated newswire data sets.
RWTH{'}s system for the 2008 IWSLT evaluation consists of a combination of different phrase-based and hierarchical statistical machine translation systems. We participated in the translation tasks for the Chinese-to-English and Arabic-to-English language pairs. We investigated different preprocessing techniques, reordering methods for the phrase-based system, including reordering of speech lattices, and syntax-based enhancements for the hierarchical systems. We also tried the combination of the Arabic-to-English and Chinese-to-English outputs as an additional submission.
This paper gives a description of the statistical machine translation (SMT) systems developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya) for our participation in the IWSLT{'}08 evaluation campaign. We present Ngram-based (TALPtuples) and phrase-based (TALPphrases) SMT systems. The paper explains the 2008 systems{'} architecture and outlines translation schemes we have used, mainly focusing on the new techniques that are challenged to improve speech-to-speech translation quality. The novelties we have introduced are: improved reordering method, linear combination of translation and reordering models and new technique dealing with punctuation marks insertion for a phrase-based SMT system. This year we focus on the Arabic-English, Chinese-Spanish and pivot Chinese-(English)-Spanish translation tasks.
This paper reports on the first participation of TCH (Toshiba (China) Research and Development Center) at the IWSLT evaluation campaign. We participated in all the 5 translation tasks with Chinese as source language or target language. For Chinese-English and English-Chinese translation, we used hybrid systems that combine rule-based machine translation (RBMT) method and statistical machine translation (SMT) method. For Chinese-Spanish translation, phrase-based SMT models were used. For the pivot task, we combined the translations generated by a pivot based statistical translation model and a statistical transfer translation model (firstly, translating from Chinese to English, and then from English to Spanish). Moreover, for better performance of MT, we improved each module in the MT systems as follows: adapting Chinese word segmentation to spoken language translation, selecting out-of-domain corpus to build language models, using bilingual dictionaries to correct word alignment results, handling NE translation and selecting translations from the outputs of multiple systems. According to the automatic evaluation results on the full test sets, we top in all the 5 tasks.
In this study, we paid attention to the reliability of phrase table. We have been used the phrase table using Och{'}s method[2]. And this method sometimes generate completely wrong phrase tables. We found that such phrase table caused by long parallel sentences. Therefore, we removed these long parallel sentences from training data. Also, we utilized general tools for statistical machine translation, such as {''}Giza++{''}[3], {''}moses{''}[4], and {''}training-phrase-model.perl{''}[5]. We obtained a BLEU score of 0.4047 (TEXT) and 0.3553(1-BEST) of the Challenge-EC task for our proposed method. On the other hand, we obtained a BLEU score of 0.3975(TEXT) and 0.3482(1-BEST) of the Challenge-EC task for a standard method. This means that our proposed method was effective for the Challenge-EC task. However, it was not effective for the BTECT-CE and Challenge-CE tasks. And our system was not good performance. For example, our system was the 7th place among 8 system for Challenge-EC task.
We present the TÜBİTAK-UEKAE statistical machine translation system that participated in the IWSLT 2008 evaluation campaign. Our system is based on the open-source phrase-based statistical machine translation software Moses. Additionally, phrase-table augmentation is applied to maximize source language coverage; lexical approximation is applied to replace out-of-vocabulary words with known words prior to decoding; and automatic punctuation insertion is improved. We describe the preprocessing and postprocessing steps and our training and decoding procedures. Results are presented on our participation in the classical Arabic-English and Chinese-English tasks as well as the new Chinese-Spanish direct and Chinese-English-Spanish pivot translation tasks.
nicola.cancedda@xerox.com Despite its relative maturity, Machine Translation is an extremely active field, enjoying a highly inter-disciplinary community. This presentation will try to convey two very distinct perspectives: that of the engineer proposing MT tools to Language Service Providers (LSPs), and that of the Machine Learning researcher. While sheer translation quality is crucial, several other requirements must be met in order to successfully deploy an MT system. The first part of this presentation will focus on some desiderata gathered from LSPs considering the option of deploying MT to support professional translators. While statistical MT is now mainstream, the interaction between the Machine Learning (ML) community and the MT community remains limited. The second part of the talk will present some approaches proposed by pure-ML researchers when brought to applying their tools of the trade to Machine Translation. 4  
uwe.reinke@fh-koeln.de Round about three decades ago Martin Kay wrote his well-known paper “The Proper Place of Men and Machines in Translation”. Some ten years ago the paper, which until then had only been published as an internal report, had not lost its topicality, and so it was reprinted in a 1997 issue of the “Machine Translation” journal. Kay’s major argument was that automating translation requires a modest and granular approach, starting with tools for machine-aided human translation and then, little by little, moving towards translation proper. Yet, since then several research projects have followed the opposite, much more ambitious line which, unfortunately, was much more promising with respect to the acquisition of research funds, so that until recently most of the tools relevant to translation service providers have been developed by companies having hands-on know-how in the translation business, but little interest and experience in methods related to computational linguistics in general and to MT in particular. Since the late 1970’s machine translation has seen many ups and downs, a “paradigm shift” from rule-based to statistical approaches, a transformation from a rather exclusive and expensive technology that only few can afford to a piece of software that is available in computer shops, bookshops, and department stores or even accessible at your fingertips via the Internet. Like a city map the “MT roadmap” seems to contain dead-end streets and road works but also highways and sign posts. The presentation will take a look at both promising and less promising pathways from a translation-oriented viewpoint that is based on an action-theoretical approach in the tradition of Justa Holz-Mänttäri and other scholars. 5   
Otto-von-Guericke-University of Magdeburg Building 29, Universitätsplatz 2 39106 Magdeburg {farag.ahmed, andreas.nuernberger}@ovgu.de Abstract. The limited coverage of available Arabic language lexicons causes a serious challenge in Arabic cross language information retrieval. Translation in cross language information retrieval consists of assigning one of the semantic representation terms in the target language to the intended query. Despite the problem of the completeness of the dictionary, we also face the problem of which one of the translations proposed by the dictionary for each query term should be included in the query translations. In this paper, we describe the implementation and evaluation of an Arabic/English word translation disambiguation approach that is based on exploiting a large bilingual corpus and statistical co-occurrence to find the correct sense for the query translations terms. The correct word translations of the given query term are determined based on their cohesion with words in the training corpus and a special similarity score measure. The specific properties of the Arabic language that frequently hinder the correct match are taken into account. Keywords: Word Translation Disambiguation (WTD), Arabic language, Parallel Corpus, Naïve Bayesian Classifiers 
We compare three commercial Machine Translation (MT) systems, Power Translator Pro1, SYSTRAN2, and T1 Langenscheidt3, with the research hybrid, statistical and rule-based system, METIS-II, with respect to identification of idioms. Firstly, we make a distinction between continuous (adjacent constituents) and discontinuous idioms (non-adjacent constituents). Secondly, we describe our idiom resources within METIS-II, the system’s identification process, and we evaluate the results with simple techniques. From the translation outputs of the commercial systems we deduce that they cannot identify discontinuous idioms. We prove that, within METIS-II, the identification of discontinuous idioms is feasible, even with low resources.  
Abstract. We present an extension to IBM Model 1 for training word-to-word lexicon probabilities. This model takes into account a given ﬁxed segmentation of the source and target sentences in the estimation of the statistical dictionary. Our experimentation on the Europarl corpus shows that a statistical consistent improvement in the translation quality can be achieved by including our proposed model as a new information source in a log-linear combination of models.  
We present a method of encoding transfer rules in a highly efﬁcient packed structure using contextualized constraints (Maxwell and Kaplan, 1991), an existing method of encoding adopted from LFG parsing (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001). The packed representation allows us to encode O(2n) transfer rules in a single packed representation only requiring O(n) storage space. Besides reducing space requirements, the representation also has a high impact on the amount of time taken to load large numbers of transfer rules to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 
As part of a general strategy to strengthen cooperation between the research community and small and medium-sized enterprises, the Danish Council for Strategic Research has decided to co-finance two projects involving machine translation. The aim of both projects has been to explore the possibilities of using statistical machine translation (SMT) approaches in small and medium sized companies (SMEs). The project goals were to find out not only whether it would possible for translation companies to integrate SMT systems in their daily translation flow, but also to assess whether it would be financially beneficial. The primary tasks of the involved translation companies have been to provide bilingual corpora consisting of sentence aligned documents and then subsequently to test and evaluate the translation results as a first step to uncover the commercial potential of using SMT in their translation process. This paper builds on the results of the second project, mainly carried out in 2007, involving at the research side Copenhagen Business School and the University of Copenhagen and on the business side Inter-Set, a medium-sized language service provider (LSP) with subsidiaries in other countries. The Open Source Moses MT system [1] was used both for training the SMT system and for translation. MOSES is currently mainly supported under the EuroMatrix project, funded by the European Commission. The language model was 150  12th EAMT conference, 22-23 September 2008, Hamburg, Germany trained using the language modelling toolkit IRSTLM [2]. The language models were trained with order 5. The maximum length of phrases in the phrase tables was set to 5. Domain Issues in SMT The assumption that there would be a productivity gain using SMT was the prime motivation factor for the LSP to investigate the potential of SMT systems. One of the issues to be considered in this context was the handling of subject domains. In an ideal world, all users involved with translation of technical documents would apply the same large-scale general subject classification system such as Lenoch [3]. From an SMT point of view the advantages of a consistent use of a classification system would be obvious. Not only would it ease the identification of consistent and representative bilingual training data, it would also, via the fine-grained subject classification, increase the probability that the lexical coverage of a given SMTsystem would be tuned for the texts to be translated. But unfortunately experiences show that use of a large universal classification system involves too much administrative work [4]. In addition, subject classification systems do not take into account possible divergences in the data within the same subject domain, e.g. different companies may have chosen to use different specific company terminologies. Besides, texts from the same subject domain will make use of very different writing styles in terms of sentence types and varieties in language usage according to the genre of the text. Marketing texts, for instance, may praise the features of the product while manuals focus on strict instructions on how to use the product. Consequently, in principle it would be preferable to train an SMT system on texts with almost identical writing styles and within the same subject domain. On the other hand, for practical and financial reasons, it would desirable that the SMT system had a broad coverage being usable for different text types without a negative impact on the translation quality. So, the solution is a compromise. In general the LSPs are very aware that different products and clients use different writing styles in terms of sentence types and variation in language usage and terminology. With a focus on delivering high quality translation, it is obvious that the clients’ expectations regarding correct handling of terminology and writing style had to be met, also for the SMT system. Therefore, the researchers and the LSP in collaboration tested the suitability of 5 different sub-domains of manuals to see how well these different sub-domains could be translated with the SMT system. Selection of Sub-domains The LSP chose 5 candidate sub-domains within the domain of technical manuals and collected training material for these topics. The training data were aligned sentences extracted from the LSP’s translation memories (TMs) and consisted of approximately 135,000 parallel sentences. From each sub-domain a development test corpus of 250 test sentences was extracted. 151  12th EAMT conference, 22-23 September 2008, Hamburg, Germany  Table 1. Training and test material for English->Danish SMT system.  Sub-  Training  Training Training Development Development  domains  words sentences sentence test set  test set  (Danish)  av. length Words  Sentences  A:Camcorders  262,138 24,897  10.5  3,263  250  B:Software  1,609,943 73,517  21.9  7,282  250  C:DVD  136,683 12,991  10.5  2,416  250  D:Printers  144,379 14,657  9.9  1,989  250  E:Mobile phones  127,701  8,740  14.6  3,216  250  Total  2,280,844 134,802  16.9  18,166  1,250  To do a quick evaluation of the system it was decided to score the translation output with the two automatic metrics BLEU [5] and TER [6]. As can be seen the subdomain with the best score is B, the second best score is found for sub-domain E, where a high figure for BLEU and a low figure for TER state that the translation output is closer to the reference, cf. table 2 below.  Table 2. Translation quality in terms of BLEU and TER scores for 5 sub-domains.  Development test data: A:Camcorders B:Software C:DVD D:Printers E:Mobile phones Total  BLEU 0.5517 0.7564 0.4766 0.6539 0.6713 0.6818  TER 33.17 16.81 37.69 23.71 24.82 24.72  Based on this evaluation, the sub-domains B, E and D seemed to the best candidates to focus on. For sub-domain B, however, the client had already started to use MT and an additional SMT system for this sub-domain therefore turned out to be without interest. The LSP chose to focus on sub-domain E, since sub-domain D has shorter sentences and better match in TM.  Evaluation Test Results The focus point of MT evaluation differs dependent on your perspective. From the developers’ point of view, evaluation as part of testing the MT system has to be quick and cheap. While from the users’ point of view, the evaluation has to focus on easier use, better translation quality, quicker post-editing etc. We have carried out both types of evaluation and compared results. Automatic evaluation measures For the system developer and for system tuning the goal is to have automatic metrics that give reproducible results and save users from doing expensive post-editing tasks in each iteration of system improvements. Two automatic evaluation metrics were used: BLEU metric [5] and Translation Edit Rate, TER [6]  152  12th EAMT conference, 22-23 September 2008, Hamburg, Germany There has been much focus on evaluation of SMT and MT-systems in the last decades [7], [8], [9]. For a brief overview of other currently used evaluation metrics used for SMT and MT and recent experiences within the field, see [10] and [11]. The two selected metrics were chosen because they are easy for the developer to apply, given a translation reference. It has been argued that an increase/decrease in the value of the BLEU score does not guarantee a better/worse translation quality [13]. But nevertheless, the metric is still widely used to measure development improvements in systems. TER is calculated as the ratio of edits (insertions, deletions and substitutions of single words as well as shifts of word sequences) compared to the average number of words in the references. TER is stated to correlate reasonably well with human judgements [6]. TER values will be in the range from 0 (translated sentence is exactly like the reference) to in principle more that 100, e.g. if the reference sentence consists of only a few words whereas the translation output contains too many words and therefore needs more edits that the length of the reference sentence. User evaluation measures From a user’s point of view, automatic evaluation figures are somewhat abstract and difficult to comprehend and do not necessarily provide feedback to the questions raised above. Alternative evaluation metrics focussing much more on the human translation aspect have conceived in order to meet this problem. The following metrics represent this alternative evaluation approach:  Fluency and adequacy scoring  Usability scoring  Post-editing time Fluency and adequacy have been defined using a five point scale [8]. Recent studies show that scores for fluency and adequacy apparently do not correlate very well between users1, and therefore these score results would be more difficult to use for system testing and tuning. The LSP post-editors involved in evaluating SMT output stated that a five point scale would be much too difficult to use. We therefore reduced the scale to a four point scale which gave the users an easier job and thereby probably more reliable results. The measure that the users suggested themselves is here called Usability. In an LSP environment, the conventional translation platform is a TM platform. 'Usability' is a measure that allows post-editors to score a machine-translated translation unit in terms of usability compared to a fuzzy match in a TM tool. A machine-translated translation unit may not be adequate or fluent, but it may be usable. When it is usable, the time needed to edit the machine-translated translation unit will be shorter than the time needed to translate the segment from scratch. It is in this context defined as a three point scale. The scale is focused on the post-editing process, and the user can use the following scores: 3: Good translation – few key strokes needed to edit translation. Corrections of casing or layout can be needed. Use of terminology is correct. 2: Translation can be post-edited using less time than a translation of the sentence from scratch – number of key strokes needed to edit translation is less than the key strokes needed to translate from scratch. 
Abstract. Context-sensitive alignments are shown to be frequent in hand-aligned parallel corpora, e.g. in 24%–85% of the sentence pairs in the corpora documented in [GPCC08]. A O(|G|n6) time strict extension of inversion transduction grammars (ITGs) [Wu97] called (2,2)-BRCGs is proposed in [Søg08] that induces such alignments. The increase in generative capacity comes from the ability to copy strings in derivations, which means that the (i) intersection of two translations and (ii) the union of two alignment structures are easily deﬁned. The problem for real-life applications is how to induce the grammars from available resources; in particular, how to learn when copying is needed. This paper presents a quadratic time algorithm that reduces the problem of how to induce (2,2)-BRCGs from m : n-alignments to the same problem for ITGs by unravelling alignment structures. The algorithm was run on a parallel corpus in the Copenhagen Dependency Treebank [BK07] (Danish–English); the ratio of new alignment structures over the number of sentence pairs was 38.08%. For the ones in [GPCC08], the size of the corpora increased by a factor of 1.74–2.0.  
Abstract. This paper reports preliminary results of an evaluation during which two different bidirectional versions of the limited-domain medical spoken language translator MedSLT were compared in a hospital setting. The more restricted version (V.1) only allows Yes-No answers and short elliptical sentences, while the less restricted version (V.2) allows Yes-No answers, short elliptical sentences and full sentences. Although WER is marginally better for V.1, task performance is marginally worse. There appear to be two main reasons for this disparity; short sentences are often badly recognised and patients tend to find it difficult to limit themselves to ellipsis, even if they receive clear instructions about not using full sentences. 1. Introduction 
Syntax-based approaches to statistical MT require syntax-aware methods for acquiring their underlying translation models from parallel data. This acquisition process can be driven by syntactic trees for either the source or target language, or by trees on both sides. Work to date has demonstrated that using trees for both sides suffers from severe coverage problems. This is primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce, which adversely affects the recall of the resulting translation models. Approaches that project from trees on one side, on the other hand, have higher levels of recall, but suffer from lower precision, due to the lack of syntactically-aware word alignments. In this paper we explore the issue of lexical coverage of the translation models learned in both of these scenarios. We specifically look at how the non-isomorphic nature of the parse trees for the two languages affects recall and coverage. We then propose a novel technique for restructuring target parse trees, that generates highly isomorphic target trees that preserve the syntactic boundaries of constituents that were aligned in the original parse trees. We evaluate the translation models learned from these restructured trees and show that they are significantly better than those learned using trees on both sides and trees on one side.
Errors in English parse trees impact the quality of syntax-based MT systems trained using those parses. Frequent sources of error for English parsers include PP-attachment ambiguity, NP-bracketing ambiguity, and coordination ambiguity. Not all ambiguities are preserved across languages. We examine a common type of ambiguity in English that is not preserved in Chinese: given a sequence {``}VP NP PP{''}, should the PP be attached to the main verb, or to the object noun phrase? We present a discriminative method for exploiting bilingual Chinese-English word alignments to resolve this ambiguity in English. On a held-out test set of Chinese-English parallel sentences, our method achieves 86.3{\%} accuracy on this PP-attachment disambiguation task, an improvement of 4{\%} over the accuracy of the baseline Collins parser (82.3{\%}).
Different approaches in machine translation achieve similar translation quality with a variety of translations in the output. Recently it has been shown, that it is possible to leverage the individual strengths of various systems and improve the overall translation quality by combining translation outputs. In this paper we present a method of hypothesis selection which is relatively simple compared to system combination methods which construct a synthesis of the input hypotheses. Our method uses information from n-best lists from several MT systems and features on the sentence level which are independent from the MT systems involved to improve the translation quality.
More and more Translation Memory (TM) systems nowadays are fortified with machine translation (MT) techniques to enable them to propose a translation to the translator when no match is found in his TM resources. The system attempts this by assembling a combination of terms from its terminology database, translations from its memory, and even portions of them. This paper reviews the most popular commercial TM systems with integrated MT techniques and explores their usefulness based on the perceived practical benefits brought to their users. Feedback from translators reveals a variety of attitudes towards machine translation, with some supporting and others contradicting several points of conventional wisdom regarding the relationship between machine translation and human translators.
In this paper we describe and compare two techniques for the automatic diacritization of Arabic text: First, we treat diacritization as a monotone machine translation problem, proposing and evaluating several translation and language models, including word and character-based models separately and combined as well as a model which uses statistical machine translation (SMT) to post-edit a rule-based diacritization system. Then we explore a more traditional view of diacritization as a sequence labeling problem, and propose a solution using conditional random fields (Lafferty et al., 2001). All these techniques are compared through word error rate and diacritization error rate both in terms of full diacritization and ignoring vowel endings. The empirical experiments showed that the machine translation approaches perform better than the sequence labeling approaches concerning the error rates.
Multi-parallel corpora provide a potentially rich resource for machine translation. This paper surveys existing methods for utilizing such resources, including hypothesis ranking and system combination techniques. We find that despite significant research into system combination, relatively little is know about how best to translate when multiple parallel source languages are available. We provide results to show that the MAX multilingual multi-source hypothesis ranking method presented by Och and Ney (2001) does not reliably improve translation quality when a broad range of language pairs are considered. We also show that the PROD multilingual multi-source hypothesis ranking method of Och and Ney (2001) cannot be used with standard phrase-based translation engines, due to a high number of unreachable hypotheses. Finally, we present an oracle experiment which shows that current hypothesis ranking methods fall far short of the best results reachable via sentence-level ranking.
We present our initial strategy for Spanish-to-Basque MultiEngine Machine Translation, a language pair with very different structure and word order and with no huge parallel corpus available. This hybrid proposal is based on the combination of three different MT paradigms: Example-Based MT, Statistical MT and Rule- Based MT. We have evaluated the system, reporting automatic evaluation metrics for a corpus in a test domain. The first results obtained are encouraging.
This paper presents a method for exploiting document-level similarity between the documents in the training corpus for a corpus-driven (statistical or example-based) machine translation system and the input documents it must translate. The method is simple to implement, efficient (increases the translation time of an example-based system by only a few percent), and robust (still works even when the actual document boundaries in the input text are not known). Experiments on French-English and Arabic-English showed relative gains over the same system without using document-level similarity of up to 7.4{\%} and 5.4{\%}, respectively, on the BLEU metric.
This work extends phrase-based statistical MT (SMT) with shallow syntax dependencies. Two string-to-chunks translation models are proposed: a factored model, which augments phrase-based SMT with layered dependencies, and a joint model, that extends the phrase translation table with microtags, i.e. per-word projections of chunk labels. Both rely on n-gram models of target sequences with different granularity: single words, micro-tags, chunks. In particular, n-grams defined over syntactic chunks should model syntactic constraints coping with word-group movements. Experimental analysis and evaluation conducted on two popular Chinese-English tasks suggest that the shallow-syntax joint-translation model has potential to outperform state-of-the-art phrase-based translation, with a reasonable computational overhead.
We construct a discriminative, syntactic language model (LM) by using a latent support vector machine (SVM) to train an unlexicalized parser to judge sentences. That is, the parser is optimized so that correct sentences receive high-scoring trees, while incorrect sentences do not. Because of this alternative objective, the parser can be trained with only a part-of-speech dictionary and binary-labeled sentences. We follow the paradigm of discriminative language modeling with pseudo-negative examples (Okanohara and Tsujii, 2007), and demonstrate significant improvements in distinguishing real sentences from pseudo-negatives. We also investigate the related task of separating machine-translation (MT) outputs from reference translations, again showing large improvements. Finally, we test our LM in MT reranking, and investigate the language-modeling parser in the context of unsupervised parsing.
Convergence and simplification are two of the so-called universals in translation studies. The first one postulates that translated texts tend to be more similar than non-translated texts. The second one postulates that translated texts are simpler, easier-to-understand than non-translated ones. This paper discusses the results of a project which applies NLP techniques over comparable corpora of translated and non-translated texts in Spanish seeking to establish whether these two universals hold Corpas Pastor (2008).
Reordering is one source of error in statistical machine translation (SMT). This paper extends the study of the statistical machine reordering (SMR) approach, which uses the powerful techniques of the SMT systems to solve reordering problems. Here, the novelties yield in: (1) using the SMR approach in a SMT phrase-based system, (2) adding a feature function in the SMR step, and (3) analyzing the reordering hypotheses at several stages. Coherent improvements are reported in the TC-STAR task (Es/En) at a relatively low computational cost.
Source languages with complex word-formation rules present a challenge for statistical machine translation (SMT). In this paper, we take on three facets of this challenge: (1) common stems are fragmented into many different forms in training data, (2) rare and unknown words are frequent in test data, and (3) spelling variation creates additional sparseness problems. We present a novel, lightweight technique for dealing with this fragmentation, based on bilingual data, and we also present a combination of linguistic and statistical techniques for dealing with rare and unknown words. Taking these techniques together, we demonstrate +1.3 and +1.6 BLEU increases on top of strong baselines for Arabic-English machine translation.
To aid research and development in machine translation, we have produced a test collection for Japanese/English machine translation. To obtain a parallel corpus, we extracted patent documents for the same or related inventions published in Japan and the United States. Our test collection includes approximately 2000000 sentence pairs in Japanese and English, which were extracted automatically from our parallel corpus. These sentence pairs can be used to train and evaluate machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval, which can be used to evaluate the contribution of machine translation to retrieving patent documents across languages. This paper describes our test collection, methods for evaluating machine translation, and preliminary experiments.
We present an approach for online handling of Out-of-Vocabulary (OOV) terms in Urdu-English MT. Since Urdu is morphologically richer than English, we expect a large portion of the OOV terms to be Urdu morphological variations that are irrelevant to English. We describe an approach to automatically learn English-irrelevant (target-irrelevant) Urdu (source) morphological variation rules from standard phrase tables. These rules are learned in an unsupervised (or lightly supervised) manner by exploiting redundancy in Urdu and collocation with English translations. We use these rules to hypothesize in-vocabulary alternatives to the OOV terms. Our results show that we reduce the OOV rate from a standard baseline average of 2.6{\%} to an average of 0.3{\%} (or 89{\%} relative decrease). We also increase the BLEU score by 0.45 (absolute) and 2.8{\%} (relative) on a standard test set. A manual error analysis shows that 28{\%} of handled OOV cases produce acceptable translations in context.
Phrase-based translation models are widely studied in statistical machine translation (SMT). However, the existing phrase-based translation models either can not deal with non-contiguous phrases or reorder phrases only by the rules without an effective reordering model. In this paper, we propose a generalized reordering model (GREM) for phrase-based statistical machine translation, which is not only able to capture the knowledge on the local and global reordering of phrases, but also is able to obtain some capabilities of phrasal generalization by using non-contiguous phrases. The experimental results have indicated that our model out- performs MEBTG (enhanced BTG with a maximum entropy-based reordering model) and HPTM (hierarchical phrase-based translation model) by improvement of 1.54{\%} and 0.66{\%} in BLEU.
This paper describes a new alignment method that extracts high quality multi-word alignments from sentence-aligned multilingual parallel corpora. The method can handle several languages at once. The phrase tables obtained by the method have a comparable accuracy and a higher coverage than those obtained by current methods. They are also obtained much faster.
We extend discriminative n-gram language modeling techniques originally proposed for automatic speech recognition to a statistical machine translation task. In this context, we propose a novel data selection method that leads to good models using a fraction of the training data. We carry out systematic experiments on several benchmark tests for Chinese to English translation using a hierarchical phrase-based machine translation system, and show that a discriminative language model significantly improves upon a state-of-the-art baseline. The experiments also highlight the benefits of our data selection method.
Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases.
This paper presents an attempt at developing a technique of acquiring translation pairs of technical terms with sufficiently high precision from parallel patent documents. The approach taken in the proposed technique is based on integrating the phrase translation table of a state-of-the-art statistical phrase-based machine translation model, and compositional translation generation based on an existing bilingual lexicon for human use. Our evaluation results clearly show that the agreement between the two individual techniques definitely contribute to improving precision of translation candidates. We then apply the Support Vector Machines (SVMs) to the task of automatically validating translation candidates in the phrase translation table. Experimental evaluation results again show that the SVMs based approach to translation candidates validation can contribute to improving the precision of translation candidates in the phrase translation table.
In this paper, we propose a probabilistic phrase alignment model based on dependency trees. This model is linguistically-motivated, using syntactic information during alignment process. The main advantage of this model is that the linguistic difference between source and target languages is successfully absorbed. It is composed of two models: Model1 is using content word translation probability and function word translation probability; Model2 uses dependency relation probability which is defined for a pair of positional relations on dependency trees. Relation probability acts as tree-based phrase reordering model. Since this model is directed, we combine two alignment results from bi-directional training by symmetrization heuristics to get definitive alignment. We conduct experiments on a Japanese-English corpus, and achieve reasonably high quality of alignment compared with word-based alignment model.
Most work in syntax-based machine translation has been in translation modeling, but there are many reasons why we may instead want to focus on the language model. We experiment with parsers as language models for machine translation in a simple translation model. This approach demands much more of the language models, allowing us to isolate their strengths and weaknesses. We find that unmodified parsers do not improve BLEU scores over ngram language models, and provide an analysis of their strengths and weaknesses.
This paper applies nonparametric statistical techniques to Machine Translation (MT) Evaluation using data from a large scale task-based study. In particular, the relationship between human task performance on an information extraction task with translated documents and well-known automated translation evaluation metric scores for those documents is studied. Findings from a correlation analysis of this connection are presented and contrasted with current strategies for evaluating translations. An extended analysis that involves a novel idea for assessing partial rank correlation within the presence of grouping factors is also discussed. This work exposes the limitations of descriptive statistics generally used in this area, mainly correlation analysis, when using automated metrics for assessments in task handling purposes.
State-of-the-art statistical machine translation systems use hypotheses from several maximum a posteriori inference steps, including word alignments and parse trees, to identify translational structure and estimate the parameters of translation models. While this approach leads to a modular pipeline of independently developed components, errors made in these {``}single-best{''} hypotheses can propagate to downstream estimation steps that treat these inputs as clean, trustworthy training data. In this work we integrate N-best alignments and parses by using a probability distribution over these alternatives to generate posterior fractional counts for use in downstream estimation. Using these fractional counts in a DOP-inspired syntax-based translation system, we show significant improvements in translation quality over a single-best trained baseline.
The continuous emergence of new technical terms and the difficulty of keeping up with neologism in parallel corpora deteriorate the performance of statistical machine translation (SMT) systems. This paper explores the use of morphological information to improve English-to-Chinese translation for technical terms. To reduce the morpheme-level translation ambiguity, we group the morphemes into morpheme phrases and propose the use of domain information for translation candidate selection. In order to find correspondences of morpheme phrases between the source and target languages, we propose an algorithm to mine morpheme phrase translation pairs from a bilingual lexicon. We also build a cascaded translation model that dynamically shifts translation units from phrase level to word and morpheme phrase levels. The experimental results show the significant improvements over the current phrase-based SMT systems.
We introduce a method for learning to find domain-specific translations for a given term on the Web. In our approach, the source term is transformed into an expanded query aimed at maximizing the probability of retrieving translations from a very large collection of mixed-code documents. The method involves automatically generating sets of target-language words from training data in specific domains, automatically selecting target words for effectiveness in retrieving documents containing the sought-after translations. At run time, the given term is transformed into an expanded query and submitted to a search engine, and ranked translations are extracted from the document snippets returned by the search engine. We present a prototype, TermMine, which applies the method to a Web search engine. Evaluations over a set of domains and terms show that TermMine outperforms state-of-the-art machine translation systems.
We propose a two-stage system for spoken language machine translation. In the first stage, the source sentence is parsed and paraphrased into an intermediate language which retains the words in the source language but follows the word order of the target language as much as feasible. This stage is mostly linguistic. In the second stage, a statistical MT is performed to translate the intermediate language into the target language. For the task of English-to-Mandarin translation, we achieved a 2.5 increase in BLEU score and a 45{\%} decrease in GIZA-Alignment Crossover, on IWSLT-06 data. In a human evaluation of the sentences that differed, the two-stage system was preferred three times as often as the baseline.
Trasla´n makes full use of MT during our translation workﬂow, where the raw output from our Machine Translation (MT) system is passed onto human translators who perform post-editing (if necessary) to arrive at the ﬁnal translation. Within Trasla´n we have found that using MT has enabled us to increase the speed, accuracy and consistency of translation - elements which allow us to process larger amounts of translation; with quicker turnaround times, which in turn has resulted in overall savings of approx. 20% so far. One of the main challenges in using MT within a commercial setting is getting human translators to adopt and make full use of the technology. Within Trasla´n we overcome this obstacle by working closely and intensively with our translators, getting them involved directly in the development process. Doing so enables translators in turn to train new users of the system and to communicate effectively to other translators the beneﬁts of integrating MT into the translation pipeline. 
◦ What are the business and in technical requirements?  First Question? 
The National Ground Intelligence Center (NGIC) collects massive quantities of textual data in foreign languages. To support exploitation in light of intelligence requirements, a triage process must be applied to this data as those requirements emerge, to identify the most useful data for further exploitation. Machine translation provides critical support for this triage. This paper outlines the types of collected data and the different challenges they present for machine translation, as well as the types of triage to support for collections of this nature, and the issues raised for machine translation by those uses.
GPHIN is a secure Internet-based {``}early warning{''} system that gathers preliminary reports of public health significance on a near {``}real-time{''} basis, 24 hours a day, 7 days a week. This unique multilingual system gathers and disseminates relevant information on disease outbreaks and other public health events by monitoring global media sources such as news wires and web sites. This monitoring is done in nine languages with machine translation being used to translate non-English articles into English and English articles into the other languages. The information is filtered for relevancy by an automated process which is then complemented by human analysis. The output is categorized and made accessible to users. Notifications about public health events that may have serious public health consequences are immediately forwarded to users. GPHIN is managed by the Public Health Agency of Canada{'}s Centre for Emergency Preparedness and Response (CEPR), which was created in July 2000 to serve as Canada{'}s central coordinating point for public health security. It is considered a centre of expertise in the area of civic emergencies including natural disasters and malicious acts with health repercussions.
Careful tuning of user-created dictionaries is indispensable when using a machine translation system for computer aided translation. However, there is no widely used standard for user dictionaries in the Japanese/English machine translation market. To address this issue, AAMT (the Asia-Pacific Association for Machine Translation) has established a specification of sharable dictionaries (UTX-S: Universal Terminology eXchange -- Simple), which can be used across different machine translation systems, thus increasing the interoperability of language resources. UTX-S is simpler than existing specifications such as UPF and OLIF. It was explicitly designed to make it easy to (a) add new user dictionaries and (b) share existing user dictionaries. This facilitates rapid user dictionary production and avoids vendor tie in. In this study we describe the UTX-Simple (UTX-S) format, and show that it can be converted to the user dictionary formats for five commercial English-Japanese MT systems. We then present a case study where we (a) convert an on-line glossary to UTX-S, and (b) produce user dictionaries for five different systems, and then exchange them. The results show that the simplified format of UTX-S can be used to rapidly build dictionaries. Further, we confirm that customized user dictionaries are effective across systems, although with a slight loss in quality: on average, user dictionaries improved the translations for 44.8{\%} of translations with the systems they were built for and 37.3{\%} of translations for different systems. In ongoing work, AAMT is using UTX-S as the format in building up a user community for producing, sharing, and accumulating user dictionaries in a sustainable way.
Particularly considering the requirement of high reliability, we argue that the most appropriate architecture for a medical speech translator that can be realised using today{'}s technology combines unidirectional (doctor to patient) translation, medium-vocabulary controlled language coverage, interlingua-based translation, an embedded help component, and deployability on a hand-held hardware platform. We present an overview of the Open Source MedSLT prototype, which has been developed in accordance with these design principles. The system is implemented on top of the Regulus and Nuance 8.5 platforms, translates patient examination questions for all language pairs in the set {English, French, Japanese, Arabic, Catalan}, using vocabularies of about 400 to 1 100 words, and can be run in a distributed client/server environment, where the client application is hosted on a Nokia Internet Tablet device.
Overview  • Background • Methodology • Results and analysis • Conclusion • Recommendations  2 
This document provides information on how companies and researchers in machine translation can work with the U.S. Government. Specifically, it addresses information on (1) groups in the U.S. Government working with translation and potentially having a need for machine translation; (2) means for companies and researchers to provide information to the United States Government about their work; and (3) U.S. Government organizations providing grants of possible interest to this community.
Machine Translation (MT) is rapidly progressing towards quality levels that might make it appropriate for broad user populations in a range of scenarios, including gisting and post-editing in unconstrained domains. For this to happen, the field may however need to switch gear and move away from its current technology driven paradigm to a more user-centered approach. In this paper, we discuss how ethnographic techniques like Contextual Inquiry could help in that respect, by providing researchers and developers with rich information about the world and needs of potential end-users. We discuss how data from Contextual Inquiries with professional translators was used to concretely and positively influence several research and development projects in the area of Computer Assisted Translation technology. These inquiries had many benefits, including: (i) grounding developers and researchers in the world of their end-users, (ii) generating new technology ideas, (iii) selecting between competing development project ideas, (iv) finding how to alleviate friction for important ideas that go against the grain of current user practices, (v) evaluating existing or experimental technologies, (vi) helping with micro level design decision, (vii) building credibility with translators, and (viii) fostering multidisciplinary discussion between researchers.
From the Automatic Language Processing Advisory Committee (ALP AC) (Pierce et al., 1966) machine translation (MT) evaluations of the {`}60s to the Defense Advanced Research Projects Agency (DARPA) Global Autonomous Language Exploitation (GALE) (Olive, 2008) and National Institute of Standards and Technology (NIST) (NIST, 2008) MT evaluations of today, the U.S. Government has been instrumental in establishing measurements and baselines for the state-of-the-art in MT engines. In the same vein, the Automated Machine Translation Improvement Through Post-Editing Techniques (PEMT) project sought to establish a baseline of MT engines based on the perceptions of potential users. In contrast to these previous evaluations, the PEMT project{'}s experiments also determined the minimal quality level output needed to achieve before users found the output acceptable. Based on these findings, the PEMT team investigated using post-editing techniques to achieve this level. This paper will present experiments in which analysts and translators were asked to evaluate MT output processed with varying post-editing techniques. The results show at what level the analysts and translators find MT useful and are willing to work with it. We also establish a ranking of the types of post-edits necessary to elevate MT output to the minimal acceptance level.
Human Translators  Some Facts about Machine Translation • Research has made significant progress in the quality of MT in the past five years (DARPA/GALE) • However, current machine translation technology does not match the performance of expert human translation 
The dramatic improvements shown by statistical machine translation systems in recent years clearly demonstrate the benefits of having large quantities of manually translated parallel text for system training and development. And while many competing evaluation metrics exist to evaluate MT technology, most of those methods also crucially rely on the existence of one or more high quality human translations to benchmark system performance. Given the importance of human translations in this framework, understanding the particular challenges of human translation-for-MT is key, as is comprehending the relative strengths and weaknesses of human versus machine translators in the context of an MT evaluation. Vanni (2000) argued that the metric used for evaluation of competence in human language learners may be applicable to MT evaluation; we apply similar thinking to improve the prediction of MT performance, which is currently unreliable. In the current paper we explore an alternate model based upon a set of genre-defining features that prove to be consistently challenging for both humans and MT systems.
This document presents an experiment in the automatic translation of Canadian Court judgments from English to French and from French to English. We show that although the language used in this type of legal text is complex and specialized, an SMT system can produce intelligible and useful translations, provided that the system can be trained on a vast amount of legal text. We also describe the results of a human evaluation of the output of the system.
• What we do – Basic and Applied Research in HLT – Research in MT for low density languages customized for military applications. – Engineering Lead for Sequoyah Program • What resources do we have available – Configurable, distributed MT testbed with COTS and GOTS systems 
Syndicated feeds in RSS, Atom, and related formats have emerged as ubiquitous information sources in World Wide Web language communities including Arabic, Farsi, Chinese, and others, providing subscribers with timely updates on topics of particular interest. We have modified an existing Open Source RSS reader, Sage, for cross-language use, permitting English-speakers to discover, subscribe to, update, and browse RSS feeds in ten languages. This early prototype, called Clip- perRSS, has been integrated with the Clipper cross-language information retrieval tool. The integrated system provides English-speakers with an effective means of exploring the potential of foreign-language syndicated feeds in their domains of interest.
SDL, in association with the International Association for Machine Translation (IAMT) and Association for Machine Translation Americas (AMTA), ran a survey which was completed by over 385 individuals in global businesses. The results were fascinating and definitely show an increased interest in the use of automated translation over the last two years.
Kataku is a hybrid MT system for Indonesian to English and English to Indonesian translation, available on Windows, Linux and web-based platforms. This paper briefly presents the technical background to Kataku, some of its use cases and extensions. Kataku is the flagship product of ToggleText, a language technology company based in Melbourne, Australia.
Notwithstanding machine translation{'}s impressive progress over the last decade, many translators remain convinced that the output of even the best MT systems is not sufficient to facilitate the production of publication-quality texts. To increase their productivity they turn instead to translator support tools. We examine the use of one such tool: TransSearch, an online bilingual concordancer. From the millions of requests stored in the system{'}s logs over a 6-year period, we extracted and analyzed the most frequently submitted queries, in an effort to characterize the kinds of problems for which translators turn to this system for help. What we discover, somewhat surprisingly, is that our system seems particularly well-suited to help translate highly polysemous adverbials and prepositional phrases.
At the request of the USG National Virtual Translation Center, the University of Maryland Center for Advanced Study of Language conducted a study that assessed the role of several factors mediating transcript usefulness during translation tasks. These factors included source language (Mandarin or Modern Standard Arabic), native speaker status of the translators, transcript quality (low or moderate word error rate), and transcript functionality (static or dynamic). Using 54 Mandarin and 54 Arabic translators (half native speakers in each language) and broadcast news clips for input, the study demonstrated that translation environments that provide dynamic transcripts with low or moderate word error rates are likely to improve performance (measured as integrated speed and accuracy scores) among non-native speakers without decreasing performance among native speakers.
The United States Army has a wide range of language requirements, varying greatly in both the number of requisite languages, and the complexity of the tasks for which language translation is crucial. Machine language translation will be an important part of the support needed to translate documents, monitor news media, and engage non-English speakers in conversation. The machine language translation community has made significant advances in the technology over the past several years, and the Army is looking to both support research and development, and to capitalize on the technology to improve communication and save lives. The Army Language Requirements Branch and the Sequoyah Program Office have received several requests from language technology developers for information on the direction and end-state goals of the Sequoyah program. In this paper, we will attempt to describe the Army{'}s language needs and to document requirements and goals for a machine language translation program.
In this paper, a system is presented that recognizes spoken utterances in Arabic Dialects which are translated into text in English. The input is recorded from a broadcast channel and recognized using automatic speech recognition that recognize Modern Standard Arabic and Iraqi Colloquial Arabic. The recognized utterances are normalized into Modern Standard Arabic and the output of this Modern Standard Arabic interlingua is then translated by a hybrid machine translation system, combining statistical and rule-based features.
Post-editing (PE) is a necessary process in every MT deployment environment. The compe{\-}tences needed for PE are traditionally seen as a subset of a human translator's competence. Meanwhile, some companies are accepting that the PE process involves self-standing linguistic tasks, which need their own training efforts and appropriate software tool support. To date, we still lack recorded qualitatively and quantitatively PE user-activity data that adequately describe the tasks and in particular the human cognitive processes accomplished. This data is needed to effectively model, de{\-}sign and implement supportive software sys{\-}tems which, on the one hand, efficiently guide the human post-editor and enhance her cogni{\-}tive capabilities, and on the other hand, have a certain influence on the translation perfor{\-}mance and competence of the employed MT system. In this paper we argue for a frame{\-}work of practices to describe the PE process by correlating data obtained in laboratory ex{\-}periments and augmented by additional data from different resources such as interviews and mathematical prediction models with the tasks fulfilled, and to model the identified pro{\-}cess in a multi-facetted fashion as a basis for the implementation of a human PE-aware in{\-}teractive software system.
This paper describes an operational case and document management and exploitation system, GlobalView, that includes Machine Translation (MT) for use as an aid to human effort in analysis and investigation. It also presents the REFLEX platform for experimenting with language processing tools.
This paper describes the strategic vision for a new translation management workflow for the US Government{'}s National Virtual Translation Center (NVTC). The paper also describes past, current, and planned experiments validating the vision, along with experiment results to-date. The most salient features of the new workflow include the embedding of translation technology at the front end of the workflow (e.g., translation memory technology, specialized lexicons, and machine translation), technology-generated {``}seed translation{''}, a new human work role called {``}paralinguist{''} to assess the {``}seed translation{''} and assign an appropriate translator/post-editor, and new human translation strategies including federated search of online dictionaries and collaborative translation.
Machine translation (MT) has been studied and developed since the advent of computers, and yet is rarely used in actual business. For business use, rule-based MT has been developed, but it requires rules and a domain-specific dictionary that have been created manually. On the other hand, as huge amounts of text data have become available, corpus-based MT has been actively studied, particularly corpus-based statistical machine translation (SMT). In this study, we tested and verified the usefulness of SMT for aviation manuals. Manuals tend to be similar and repetitive, so SMT is powerful even with a small amount of training data. Although our experiments with SMT are at the preliminary stage, the BLEU score is high. SMT appears to be a powerful and promising technique in this domain.
The DoD already makes extensive use of machine translation and language support tools in a many environments to address a variety of communications, training, and intelligence challenges, and has done so for over 30 years. Mr. Bemish draws on his personal experience deploying MT, as well as his broad exposure to how translation technology is used in the branches of service and in military intelligence, to describe current uses of translation technology across a range of organizations within the DoD. He also addresses the technical issues that slow deployment and the cultural challenges involved in setting expectations and introducing technology that changes the way people work.
This paper describes the Language Technology Resource Center (LTRC), a U.S. Government website for providing information and tools for users of languages (e.g., translators, analysts, etc.) The LTRC provides information on a broad range of products and tools, and provides a means for product developers and researchers to provide the U.S. Government and the public with information about their work. The LTRC is part of the Foreign Language Resource Center (FLRC) program, which provides support to Government and professional organizations regarding language tools.
