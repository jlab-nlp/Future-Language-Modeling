∗ Thanks to Kiyong Lee for comments on an earlier version. Copyright 2007 by Harry Bunt 13  the ambiguity problem that arises in context-independent interpretation of natural language is to some extent artificial: it is in part caused by the aim to derive ‘disambiguated interpretations’ within the framework of a formal logical system, which brings a level of granularity which forces one to deal with issues that are often irrelevant in practical situations. 2. The robustness problem: The methods of formal semantics tend not to be robust enough when applied to practical uses of natural language, such as spoken dialogue, on-line chatting, sms messages, or dynamic web pages. This is because linguistic semantic theories have been developed as components of grammatical theories, and have been informed primarily by the analysis of carefully constructed, grammatically perfect sentences rather than by the informal, elastic way in which language is used in spoken and multimodal interaction, which commonly involves nonsentential and grammatically irregular utterances that work semantically and pragmatically quite well. In this paper I explore a novel approach to the computation of semantic information, namely through semantic annotation. I will argue and illustrate that this approach may make it possible to address both the amibguity problem and the robustness problem successfully. I will argue that the approach offers the exciting perspective of a practial, incremental approach to the automatic use of semantic information from natural language; a use which may become more and more powerful as more sophisticated semantic annotation tools and methods are developed. 2. Semantic Interpretation and Semantic Annotation Attempts to address the ambiguity problem include relaxing the aim of deriving fully disambiguated interpretations to the more modest aim of constructing underspecified semantic representations. These are partial representations of the meaning of an utterance, which leave open certain aspects of the meaning for which no or incomplete information is available. Underspecified semantic representations can be viewed as quasi-formal representations of partially disambiguated sentences; ‘quasi-formal’ in the sense of being cast in a formal syntax but not having a formal semantics. In computational work, underspecified semantic representations are treated as an intermediate stage of semantic interpretation, and can be used in computations (such as inferencing) only after being disambiguated – which takes the form of replacing the underspecified bits by fully specified parts (see e.g. van Deemter, 1996; Bunt, 2007). Underspecified semantic representations are a computationally attractive idea, but if they always have to be disambiguated before anything can be done with them, then they may help to postpone having to deal with ambiguity explosions in natural language processing systems, but they don’t really present a solution to the ambiguity problem. Robustness problems in natural language processing, especially in syntactic processing, are often addressed by replacing the aim of producing a full syntactic disambiguation (i.e., producing a set of full parses) by that of identifying important chunks, such as noun phrases and preposition phrases; this is often pursued in combination with statistical or machine learning techniques. Only identifying syntactic and semantic chunks, without information about their semantic role in the sentence, can be useful for certain applications (such as spoken dialogue systems based on semantic slot filling), but is semantically primitive. Corpus-based, statistical and machine learning approaches to language processing have proved to be more robust than traditional approaches based on predefined grammars; however, while successful for syntactic processing, these approaches have so far not been applied with much success in the area of computational semantics. The approach that is outlined in this paper is based on the observation that semantic annotations are intended to capture some of the meaning of the annotated text. Annotations have 14  traditionally been viewed as a kind of labels, potentially useful for identifying linguistic patterns in corpora. But since semantic annotations capture semantic characteristics of linguistic material, it ought to be possible to interpret them as partial descriptions of the meaning of that material. In other words, it should be possible to view semantic annotations as expressions in a language which has a well-defined semantics. In fact, the use of a semantic annotation language without a semantics would make little sense, since there is no reason to think that semantically undefined annotations would describe the meanings of natural language expressions any better than the expressions themselves (cf. Bunt and Romary, 2002; 2004). Still, existing work in this area, for instance on semantic role annotation (as in the FrameNet and PropBank initiatives) or on the annotation of temporal information (as in the TimeML effort) make use of uninterpreted annotation languages. (It is only recently, as part of an ISO initiative to develop annotation standards, that an effort has begun to define an annotation language for temporal information which has a formal semantics.) While the semantic annotation schemes that have been applied so far do not have a formal semantics, I believe that it is possible to define annotation languages for such schemes which do have a well-defined semantics. In fact, defining a formal semantics for an existing annotation scheme may be helpful for improving the scheme’s design. 3. Semantic Annotation Schemas The inspiration of this paper comes mostly from participating in two recent and ongoing efforts in the area of semantic annotation, namely in the International Organisation for Standards ISO, in particular in its expert group on semantic content (http://iso-tdg3.uvt.nl), and in the European eContent project LIRICS (Linguistic Infrastructure for Interoperable Resources and Systems, http://lirics.loria.fr). One of the most important activities in the ISO expert group concerns the development of an international standard for the annotation of temporal information in documents, provisionally known as ISO-TimeML. Other activities, performed in concert with the LIRICS project, concern the design of sets of well-defined and well-documented (following ISO standard 12620) concepts for semantic annotation in an on-line registry. The focus of the latter activities is in three areas of semantic annotation: semantic roles, referential relations, and communicative functions of utterances in interactive discourse (‘dialogue acts’). In this paper we focus on the interpretation of annotations concerned with temporal information, referential relations, and semantic roles. The combination of semantic annotations for different areas requires a common, integrated format. The ISO and LIRICS activities, while aiming at the use of standardized annotation concepts, do not provide standardized formats. XML is a de facto standard in many NLP applications, however, and we believe that an XML-based in-line format as used in ISOTimeML documents is slightly more readable than a stand-off format. (Stand-off representations are more expressive than in-line representations, since the latter have a problem in marking up discontinuous markables; moreover, stand-off annotations keep the annotated material unaffected, and also have the benefit of allowing multiple annotations to be linked with the same source material. Stand-off representations are therefore recommended by ISO.) We will call the XML-based semantic annotation language that we will develop in the course of this paper “SemML”, and define its semantics following the familiar ‘interpretation-by-translation’ approach, translating SemML expressions into a well-known formal logical language. 3.1 Temporal Information For temporal information, our point of departure is the ISO-TimeML standard under development. ISO-TimeML is a further development of the TimeML annotation language 15  (Pustejovsky et al. 2003; 2007) within ISO, taking other studies of temporal information into account and a wide range of natural languages (see ISO, 2007). The following types of temporal information can be expressed in ISO-TimeML annotations: • Times (12:25), days (Tuesday) dates (29 February), years (2007), and so on; • Periods, such as last week, next year, yesterday, the 20th century,… • Durations (5 minutes, 2.5 hours, seven days,…) • The temporal anchoring of events and states: John drove to Boston last Monday; Harry will meet Sally tomorrow at noon, Mary is pregnant since August,,… • Temporal relations between events: After his talk with Mary, John drove to Boston As an example, consider the (slightly simplified) ISO-TimeML annotation of sentence (1a), illustrating both the annotation of temporal event anchoring and temporal ordering of events: (1a) After his talk with Mary, John drove to Boston (1b) <TIME_STRUC> <SIGNAL sid=”s1”> After </SIGNAL> his <EVENT eid=”e01” eiid=”e1” eventclass=”OCCURRENCE” pos=”NOUN” tense=”NONE” aspect=”NONE”> talk </EVENT> <SIGNAL sid=”s2”> with </SIGNAL> Mary, John <EVENT eid=”e02” eiid=”e2” eventclass=”OCCURRENCE” pos=”VERB” tense=”PAST” aspect=”NONE”> drove </EVENT> <SIGNAL sid=”s3”> to </SIGNAL> Boston <TLINK eventInstance=”e2” signalID=”s1” relatedToEventInstance=”e1” tempRelType=”AFTER”/> </TIME_STRUC > The ISO-TimeML draft proposal (ISO, 2007) specifies a formal semantic interpretation of the temporal markup using Interval Temporal Logic (ITL), a first-order approach to reasoning about time (see Pratt-Hartman, 2007 and http://www.cse.dmu.ac.uk/STRL/ITL/) . On this approach, the annotation structure (1b) is interpreted as a statement about the time intervals associated with the two events mentioned in the sentence. This interpretation is represented as in (1c), where P1 and P2 stand for unary predicates that characterize those sets of intervals during which John talked with Mary and John drove to Boston, respectively. The interval variables should be understood to be existentially quantified. (1c) P1(I1) ∧ P2(I2) ∧ AFTER (I2, I1) Note that only the temporal markup is interpreted here. Temporal relations between events are interpreted as relations between temporal intervals. Other information about the events, such as who did what, is not represented (but is hidden in the predicate constants P1 and P2). A sentence such as (2a), stating a temporal relation between an event and a temporal interval is treated as shown in (2b) and (2c), where the predicate P2004-01-31 should be interpreted as characterizing the (singleton) set of time-intervals coinciding with the 31st of January, 2004. 16  (2a) John drove to Boston on Saturday, January 31, 2004. <2b)<TIME_STRUC> John <EVENT eid=”e01” eiid=”e1” eventclass=”OCCURRENCE” pos=”VERB” tense=”PAST” aspect=”NONE”> drove </EVENT> <SIGNAL sid=”s1”> to </SIGNAL> Boston <SIGNAL sid=”s2”> on </SIGNAL> <TEMP_ENTITY tid=”t1” value=”2004-01-31”> Saturday, January 31 </TEMP_ENTITY> <TLINK eventInstance=”e2” signalID=”s1” relatedToTime=”t1” tempRelType=”DURING”/> </TEMP_STRUC > (2c) P2004-01-31(I1) ∧ P2(I2) ∧ DURING(I2, I1) Of interest is also the treatment of negation in this approach. A sentence like John did not drive to Boston is interpreted as “within some contextually determined interval no event of John’s driving to Boston took place. If we represent our contextually determined interval using the variable Iei1 (corresponding to the eiid value of the relevant EVENT tag), then we can express these truth conditions using the formula ¬∃I1 DURING(I1, Iei1) ∧ Pe1(I1)” (ISO 2007, p. 35). Note that on this approach events do not have to be represented explicitly. A predicate such as Pe1, expressing that John drove to Boston during a certain interval of time, can for instance be expanded as Pe1 = λI. DRIVE(john, boston, I). The inclusion of a temporal argument in event predicates suffices for representing temporal information. 3.2 Coreference Information Representing only the temporal information in a sentence is not very useful. Knowing that an event of a certain type occurred at a certain time is only useful when we have information about that event, such as who was involved, and in what way. As a first step towards representing the latter type of information we now turn to annotation structures for referential entities and coreference relations. We will subsequently turn to the annotation and representation of the involvement of referential entities in events, in subsection 3.3. We will take the LIRICS annotation scheme for reference annotation as our point of departure, which is based on the reference annotation framework proposed by Salmon-Alt and Romary (2005). This scheme (see Bunt and Schiffrin, 2007), allows annotations to capture information about referential entities, coreference and anaphoric relations. Using elements from the LIRICS 17  scheme, we can represent the referential information in sentence (1a) in SemML as shown in (4a).1 (4a) <REF_STRUC> After <REFENT rid=”x1” animacy=”ANIMATE” naturalGender=”MALE” pos=”PRON” case=”GEN” cardinality=”1”> his </REFENT> talk with <REFENT rid=”x2” animacy=”ANIMATE” naturalGender=”FEMALE” pos=”PN” cardinality=”1”> Mary </REFENT> <REFENT rid=”x3” animacy=”ANIMATE” naturalGender=”MALE” pos=”PN” cardinality=”1”> John </REFENT> drove to <REFENT rid=”x4” animacy=”INANIMATE” naturalGender=”NONE” pos=”PN” cardinality=”1”> Boston </REFENT> <REFLINK referent=”x1” antecedent=”x3” refRelType=”OBJECTAL_IDENTITY” /> </REF_STRUC> A formal semantic representation of the information in this annotation structure calls for the introduction of variables ranging over individual objects other than temporal intervals, to allow the expression of the anaphoric relation through the equality of the variables corresponding to “John” and “his”. This could be done in first-order logic as follows: (4b) ∃x1, x2, x3, x4: MALE(x1) ∧ FEMALE(x2) ∧ MARY(x2) ∧ MALE(x3) ∧ JOHN(x3) ∧ BOSTON(x4) ∧ INANIMATE(x4) ∧ x1 = x3 (Stipulating that John is male, that Mary is female, and that Boston is inanimate may seem redundant but really isn’t, as can be seen by replacing “John” or “Mary” in the sentence by “Chris”, and similarly by replacing “Boston” by “Nancy”, which may refer to a female person or to a town in France.) While the interpretation of temporal information in ISO-TimeML is accomplished in first-order logic, for information about reference this is in general not possible, since coreference relations may exist not just between individuals but also between sets of individuals. The following example illustrates this. (5a) After they washed their hands, the men lifted the piano. Assuming that the men washed their hands individually but lifted the piano collectively, there is no coreference relation between the entities that perform the wash-events and the lift-event, but we do have a coreference relation between the two sets of men involved in the events. As sentence (5a) is structurally identical to (1a), it could be represented by the ITL-formula (1c) P1(I1) ∧ P2(I2) ∧ AFTER (I2, I1), with appropriate re-interpretation of the predicate terms, but to represent the identity of the two sets of men we need to expand these predicates. A naïve way to that could be as follows: (5b) ∀x: MAN(x) → [WASHANDS(x, I1)] ∧ [∀y: MAN(y) → LIFT(y, thepiano, I2)] ∧ PAST(I2) ∧ AFTER (I2, I1) 
These will not be just any core theories, but theories that ﬁnd their place in a schema I call the “Ontological Ascent”. They include a theory of systems or composite entities and the ﬁgureground relation, which subsumes a theory of scales; a theory of change of state; a theory of causality, which provides support for a theory of goal-directed behavior; and very importantly a theory of shifts in granularity. The Ontological Ascent is described in Section 2. Then in the subsequent sections I sketch the outlines of each of the above theories. The enterprise is therefore to axiomatize these core theories in as clean a fashion as possible, and then to deﬁne, or at least characterize, various words in terms of predicates supplied by these core theories. For example, a core theory of scales will provide axioms involving predicates such ∗Copyright 2007 by Jerry R. Hobbs. 29  as scale, <, subscale, top, bottom, and at. Then, at the “lexical periphery” we will be able to deﬁne the rather complex word “range” by an axiom such as the following: (∀ x, y, z)range(x, y, z) ≡ (∃ s, s1, u1, u2)scale(s) ∧ subscale(s1, s) ∧ bottom(y, s1) ∧ top(z, s1) ∧ u1 ∈ x ∧ at(u1, y) ∧ u2 ∈ x ∧ at(u2, z) ∧ (∀ u ∈ x)(∃ v ∈ s1)at(u, v) That is, x ranges from y to z if and only if there is a scale s with a subscale s1 whose bottom is y and whose top is z, such that some member u1 of x is at y, some member u2 of x is at z, and every member u of x is at some point v in s1. Many things can be conceptualized as scales, and when this is done, a large vocabulary, including the word “range”, becomes available. Two methodological principles should be mentioned ﬁrst. Above, I said “deﬁne, or at least characterize, various words”. In general, we cannot hope to ﬁnd deﬁnitions for words. That is, for very few words p will we ﬁnd necessary and sufﬁcient conditions, giving us axioms of the sort (∀ x)p(x) ≡ . . . Rather, we will ﬁnd many necessary conditions and many sufﬁcient conditions. (∀ x)p(x) ⊃ . . . (∀ x) . . . ⊃ p(x) However, the accumulation of enough such axioms will tightly constrain the possible interpretations of the predicate, and hence the meaning of the word. The second methodological point is that we need to be careful how we use an argument from the “naturalness” of an expression. Not all expressions that will be allowed by our core theories will sound natural. Our knowledge of language consists of thousands of very speciﬁc conventions, each of which has a rationale in terms of core theories. But not everything that has a rationale has been conventionalized. Conventional expressions sound natural. Other expressions with a rationale are interpretable, but may not sound natural. For example, it is conventional to say “at work” and “in progress”, and recently in corporate America, the expression “on travel” has become conventional. There is no particular reason that these expressions are better than “on work”, “on progress”, and “at travel”. It just happens that the latter did not become conventional. The account of lexical meaning given here is intended to provide a rationale for expressions, but not to explain why one version rather than another has been conventionalized. 2. The Ontological Ascent A general pattern in natural language, illustrated in Figure 1, consists of the following ﬁve steps: 1. A collection of entities and relations among them can be viewed as a single system of entities. 2. An element external to a system can be located at an element in a system. 3. There can be a change of state from that element’s being at one point to its being at another point in the system. 4. An agent can cause such a change. 5. At any point in this ascent, we can coarsen the granularity and view the complex entity that has been constructed as an indecomposable entity, itself ready to become the basis for another ascent. 30  Coarsening of Granularity  cause(change) 6 change(at,at)  6  at(ent,ent,sys)   3   6    system of  entities  P i PPP entity  6  Figure 1: The Ontological Ascent This ascent is reﬂected in the common pattern or decomposition for verbs, which we can represent, not quite formally, as cause(a, change(at(b, c), at(b, d))) That is, a causes there to be a change of state from b being at c to b being at d. This schema, or something similar to it, has been central in much work on lexical semantics, including Gruber (1965), Jackendoff (1976), Hobbs (1974), Talmy (1983), and Croft (1991). The transitive verb “move” illustrates this pattern. If A moves B from C to D, then A causes a change from B being at C to B being at D. To illustrate just how pervasive this pattern is in natural language discourse, let us consider a random sentence taken from the business news section of a newspaper. In a stunning reversal for one of Silicon Valley’s fastest-growing companies, Media Vision Technology Inc. said Thursday it will report a sharp decline in sales and ”a substantial loss” in the quarter ending March 31 – a jolt that cut its stock price in half. (San Jose Mercury News, March 25, 1994, p. 12E) We will work from the inside out. Consider the word “sales”. Possession can be conceived of as an “at” relation, an entity being at a person within the social network of people, the ﬁgure-ground relation. A sale is a change of state from an entity being at one person to the entity being at another person, and a corresponding change in the location of money. When we form the plural “sales”, we are ﬁrst coarsening the granularity to view a sale as an indecomposable entity, and then we are forming the simplest sort of system, a set, out of a collection of them. “Decline in sales”: We ﬁrst coarsen the granularity on the set of sales, to view it as an indecomposable entity. This entity can be measured in amounts of money. To measure something is to locate it at a point on a particular kind of scale. Here the scale is amount of money. A decline then is a change of state from the set of sales (in a particular time period) being at one point on this scale to the sales (in a more current time period) being at a lower point on the scale. 31  “Sharp decline in sales”: Now the decline itself is viewed as an entity and located at the high end of a scale that encodes the rate of change on a quantitative scale with respect to time. “A substantial loss”: A loss is also a change of state in a downward direction on a scale for amounts of money. Like “sharp”, “substantial” is a measure of that change. “A sharp decline in sales and a substantial loss”: The conjunction takes two events, viewed as indecomposable entities, and forms the aggregate consisting of the two of them. “In the quarter ending March 31”: This adverbial applied to the conjoined NP locates the aggregate on the time scale within a particular interval. “Report”: Cognition can be viewed in terms of a ﬁgure-ground, or location “at”, metaphor. We have ideas in mind, events in memory, and so on. Communication, on this metaphor, is a change of state from a proposition being in one mind to the proposition being in another mind. Of course, this sort of change of location differs from change of phyiscal location, in that the proposition is still in mind of the originator of the message. A report involves just such a communication event. The eventuality described as “a sharp decline in sales and ‘a substantial loss’ in the quarter ending March 31” moves from being located in the minds of the company ofﬁcials to the minds of the public. Moreover, this change is caused by the entity issuing the report. 
epistemic/deontic readings, they are bafﬂed: what the two meanings have in common?  must  may  epistemic have ﬁrm knowledge have vague knowledge  deontic force obligation  permit tolerantly  The possible world semantics may (or must?) convince some of them, that is,  must for all possible worlds, denoted by may some possible worlds exist, denoted by ♦  Remember in First-Order Logic, ∀ (for all) and ∃ (some · · · exist) are interchangeable each other; i. e., ∀x p(x) is equal to ¬∃¬p(x) (there exists no x such that ¬p(x)). In the similar way, ‘ ’ can be replaced by ‘¬♦¬’ in modal logic. In Japanese, the deontic reading of ‘must’ is translated as  –nakere–ba–naranai.  Here, the ﬁrst ‘–nakere–’ is the negation, and the second ‘–ba–’ suggests a case, and the third ‘–naranai’ is again the negation. Thus, Japanese represent deontic ‘ ’ by ‘¬♦¬’. Secondly, we can introduce a hypothetical belief situation either in the past tense or in the conjunctive mood in Indo-European languages, and also we may mention an imaginary current situation in the conditionalmood,1 as  ,,Wenn ich mehr Zeit hu¨tte, so wu¨rde ich Ihnen einen la¨ngeren Brief schreiben.“ S’il venait demain, tu le verrais. “Se facesse bel tempo domani, che faresti?”  In English both the conjunctive and the conditional moods have been reduced to the subjunctive mood.  “If I were a bird, I would ﬂy to you.” ∗Copyright 2007 by Satoshi Tojo 1Konjunctive in German.  42  Why do we employ the past tense when we introduce a counterfactual situation? One explanation is that we consider retreating once to a past (by the conjunctive mood) to annihilate the current situation, and restarting from the past to the different ‘now’ (by the conditional mood) as in the following ﬁgure.  −→ ◦ −→ ◦  ◦ new now  ◦ −→ ◦ −→ now Actually, in English and German which do not possess the conditional mood, ‘would’ and ‘wu¨rden’ are called past-future (future seen from the past). Later, we will explain such a branching time in terms of modal logic.  2. Looking back · · ·  2.1. Intensional Logic  One of the outstanding achievements of Montague semantics (Dowty, 1979; Dowty, Wall, and Peters, 1981; Gamut, 1991) is the analysis of oblique sentences (or opaqueness), besides proper treatment of quantiﬁcation, as in  Electra does not know that the man in front of her is her brother.  (1)  Electra knows that Orestes is her brother. The man in front of her is Orestes. Electra knows that the man in front of her is her brother.  language expression ↓ intension possible world −−−−−−−−−−−−−−−→ extension  de re = extensional = reference (Bedeutung) de dicto = intensional = sense (Sinn)  To include intensional logic, we need to revise the syntax of formal language. If α is a language expression, then so is ˆα. For any w1 and w2, [[ˆα]]w1 = [[ˆα]]w2 which implies that [[ˆα]]w does not depend upon w. s is an index of a possible world, and the meaning of α depends upon w. α: a ↔ ˆα: s, a ↔ ˇˆα: a γ{x} ≡ ˇγ(x) (Brace convention) 
a Department of English, Konkuk University, Seoul 143-701, Korea, hdahn@konkuk.ac.kr b Department of English, Sungkyunkwan University, Seoul 110-745, Korea, scho1007@skku.edu  Abstract. Case markers in Korean are omissible in colloquial speech. Previous discourse studies of Caseless bare NPs in Korean show that the information structure of zero Nominative not only differs from that of overt Nominative but it also differs from that of zero Accusative in many respects. This paper aims to provide a basis for these semantic/pragmatic properties of Caseless NPs through the syntactic difference between bare subjects and bare objects: namely, the former are left-dislocated NPs, whereas the latter form complex predicates with the subcategorizing verbs. Our analysis will account for the facts that (i) the distribution of bare subject NPs are more restricted than that of bare object NPs; (ii) bare subject NPs must be specific or topical; (iii) Acc-marked NPs in canonical position tend to be focalized. Keywords: Case markers, bare nominals, left-dislocated NPs, focalization  1. Introduction Case markers in Korean are omissible in colloquial speech. Many previous studies of Caseless bare NPs in Korean show that subject-object asymmetries are observed in various respects. For example, as observed in the wide range of conversational data (H. Lee 2006b-c), occurrence rate of bare NPs in complement position is higher than that of bare NPs in subject positions. The grammatical contrast in (1) further shows that the distribution of bare NP subject in (1b) is not only less common but also severely restricted in canonical subject position, namely, Spec-T, in contrast to the bare NP object in (1a) in canonical object position. (1) a. Mary-ka Chelswu-(lul) manna-ss-e. Mary-Nom Chelswu-Acc meet-Past-Dec ‘Mary met Chelswu.’ b. Chelswu-lul Mary-*(ka) manna-ss-e. Chelswu-Acc Mary-Nom meet-Past-Dec ‘Chelswu, Mary met.’ It is plausible to assume that the subject Mary-ka in (1b) is “frozen” in the subject position, Spec-T, due to the scrambled object John-ul. Thus, (1b) sharply contrasts with (1a) in that Nominative Case must be marked unlike Accusative. * An earlier extended version of this study appeared in Ahn & Cho (2007a). This work was supported by the Konkuk University (the first author), and by the Brain Korea 21 Project (the corresponding author). Copyright 2007 by Hee-Don Ahn, and Sungeun Cho 57  Another subject-object asymmetry is D-linking restriction some non-Case-marked wh-phrases show. As initially noted by Ahn & Cho (2006), non-Case-marked subject wh-phrase nwukwu 'who' has only D(iscourse)-linked interpretation in the sense of Pesetsky (1987), as shown in (2).  (2) a. Nwukwu-∅ Yenghi-lul manna-ss-ni?  who  Yenghi-Acc meet-Past-Q  ‘Who is such that he/she met Yenghi?’  b. Nwu(kwu)-ka Yenghi-lul manna-ss-ni?  who-Nom Yenghi-Acc meet-Past-Q  'Who met Yenghi?'  (only D-linked reading is possible) (non-D-linked reading is also possible)  However, such restriction isn’t observed in the case of bare object wh-phrases in (3).  (3) a. Yenghi-ka nwukwu-∅ manna-ss-ni?  Yenghi-Nom who  meet-Past-Q  b. Yenghi-ka nwukwu-lul manna-ss-ni?  Yenghi-Nom who-Acc meet-Past-Q  'Who did Yenghi meet?'  (non-D-linked reading is also possible) (non-D-linked reading is also possible)  Interestingly, if the Case marker is absent in the scrambled object wh-phrase, only D-linked interpretation is possible, as shown in (4).  (4) Nwukwu-∅ Yenghi-ka manna-ss-ni?  Who  Yenghi-Nom meet-Past-Q  ‘Who is such that Yenghi meet (him/her)?’  
The second difference of the current system is that it is not intended to parse into phrases per se, but to identify the functional roles of the phrases. To recognize phrases, it resorts to a general characteristic of phrases in sentences found in formal documents. The general feature is that the component representing the content of a phrase is transcribed in Kanji (Chinese characters) or/and Katakana (phonetic characters used for transcribing words of foreign origin), and that the component expressing the function in Hiragana (phonetic characters used for transcribing words or morphemes of Japanese origin). There have been developed such morphological analyzers that exploit the orthographic difference between the content and ∗ Copyright 2007 by Yukiko Sasaki Alam 67  functional components of phrases: to name a few, Asahara (2003), Kazama (2001), Kashioka et al (1998), and Kameda (1996). All of them, unlike the current system, focus on the parsing into morphemes or phrases, but not on the identification of the functional roles of the phrases. The third feature different from other systems is that it is purely rule-based, unlike Kudo (2002), Sekine (2001), Uchimoto (2002), Kanayama et al (2000), and Haruno et al. (1999), all of which are statistically modeled. To give an idea of an output parsed by the current system, an example is given in Table 1.  Table 1: A successful output.  ０５年度は中途採用を当初計画の２０人から最大１５０人まで拡大。 zero-go-nen-do-wa-chuuto-saiyo-o-tousho-keikaku-no (0-5-fiscal-year-Topic-midway-employment-Object-initial-plan-of) -nijuu-nin-kara-saidai-hyakugojuu-nin-made-kakudai (-20-person-from-at-most-150-person-to-expansion)  内容 機能  文法的役割  ０５年度 は 話題 (TOPIC)  中途採用 を 目的語 (OBJECT)  当初計画 の 名詞修飾句 (NOMINAL MODIFIER)  ２０人  から 始点 (POINT OF DEPARTURE)  最大１５０人 まで 継続終点 (UP-TO)  拡大  (省 名詞止め文 (NOUN-ENDING 略) SENTENCE)  The first column in the table contains the content components, the second the functional components, the third the descriptions of the functional roles. The meaning of the parsed sentence is that in the fiscal year of 2005, (the company) expanded the number of midway employment from the initially planned 20 to 150. The present paper first describes the components of the system and the relations among them, then discusses the pros and cons before ending with a short conclusion.  2. Architecture The current system is structured in an object-oriented design, comprising four classes (or programs). The main is the MorphAlgorithm class, and it is supported by the remaining three classes: the Phrase, Grammar and CharIdentifier classes. The object-oriented design facilitates the creation of classes that represent concepts we are already familiar with, and thus helps us understand the functions of the classes and the interaction between them. Since the current system attempts to parse sentences into phrases, it has classes representing the parsing algorithm, the syntactic unit of phrase, and the grammar containing grammatical information. In addition, the system is provided with a class named CharIdentifier so that it is able to recognize the types of characters, because it parses texts transcribed in Joyo Kanji (frequently used Chinese characters) wherever applicable, and thus makes use of the orthographic difference in the transcription of the content and the functional parts of phrases. Figure 1 below shows a simplified state chart in UML (Unified Modeling Language) for illustrating the relations among the classes in the current system.  68  Figure 1: A simplified state chart diagram for the current system. Below comes a more detailed description of each class in the current system. 2.1. The MorphAlgorithm class The MorphAlgorithm class1 is the main class of the present system that controls the process of recognizing phrases and identifying the functional roles played in the sentences of the text. The target texts are articles in newspapers, magazines, professional journals and public documents that are transcribed wherever applicable by using so called Joyo Kanji (frequently used Chinese characters). In such texts, the content parts of phrases are likely to be transcribed in Kanji (Chinese characters) or/and Katakana (phonetic characters used for transcribing words of foreign origin). At the same time most parts that indicate the functions of phrases are written in Hiragana (phonetic characters for transcribing words of Japanese origin). The system utilizes these orthographic features to recognize phrases, and identify the content and the functional parts. In addition, it resorts to another feature characteristic of Japanese (a head-final language) that the functional part is located at the end of a phrase. The MorphAlgorithm class first cleans up the sentences to parse, for instance, by removing a tab stop and a new line characters. All the sentences in the text are appended into a StringBuffer instance, and are dealt with one sentence at a time. The algorithm that makes up the backbone of the MorphAlgorithm class (illustrated on Appendix A in this paper) is discussed in detail in Alam (2007). The following is a synopsis of the algorithm. The algorithm first looks for a character or characters that are unacceptable at the beginning of a phrase. Such a character or characters are a period, a comma, a parenthesis and characters indicating a complementizer.2 After examining these elements, the algorithm yet has to look for another irregular situation before going on to find a typical phrase with the content section consisting of Kanji or/and Katakana, followed by the functional Hiragana part. The irregular case is that a phrase, in particular, the content part begins with a Hiragana character. When it finds only one Hiragana character followed by a Kanji or a Katakana, it checks if the Hiragana is an honorific prefix. If not, and followed by another Hiragana, it keeps reading Hiragana characters until it hits a character that is not a Hiragana. Then the Hiragana sequence is assumed to be a phrase, and sent to the procedure in order to find the functional role of the phrase by its ending part. When the algorithm does not find a Hiragana at the beginning of the phrase, but a Kanji or a Katakana, it keeps reading it until it meets a character that is not a Kanji or a Katakana. It 
Researchers have been exploring techniques for classifying documents according to sentiment orientation, or positive/negative (p/n) in particular. Turney (2002) extracts phrases containing adjectives or adverbs, and determines their semantic orientation. Further, p/n of a document is judged by computing the average of the semantic orientations. The NEAR operator in the Alta Vista search engine is used in the method. If you search a query like A NEAR B in Alta Vista, the search engine shows pages containing within near words of each other. However, this operator is not provided in Japanese version. Wang and Araki (2007) extend Turney's method into Japanese by collecting and using a set of words that contribute significantly to p/n orientation. Fujimura et al. (2004) use corpora divided into p/n words, and statistically classify a document by extracting opinions. These methods provide the semantic orientation only with bag-of-words. A word, however, contains little and partial information. Therefore, we assert that the scope of the process needs to be expanded. Moreover, these methods do not identify reasons for p/n judgment, that is, expressions that cause semantic orientation. For a commercial application, this function is essential in marketing research. Besides these, other document classification approaches have also been proposed, which prepare a semantic orientation dictionary in advance. Tateishi et al. (2004) construct this * Copyright 2007 by Natural Language Processing Laboratory, Department of Electrical Engineering, Nagaoka University of Technology. 76  dictionary in advance by extracting opinion triplets that consist of `object name, attribute expression, evaluative expression’, and classify documents by using the triplets. The method extracts only expressions that appear in a definite pattern, and it is thus difficult to obtain satisfactory coverage and accuracy. Kobayashi et al. (2005) first extract a few opinion pairs of attribute and value with an anaphora resolution technique, and construct a semantic orientation dictionary of the pairs for a target domain. They then gradually expand the entries of the dictionary from the pair seed. However, if the accuracy of the primary pairs is not adequate, the quality of the dictionary becomes gradually poorer due to gradually involving noises. In addition, making the dictionary domain by domain is very expensive. In this paper we do not use a word as an unit of tagging information to extract opinions. As other work does, we also think that a document can be classified when we only extract partial sentiment expressions. However, we do not think that bag-of-words approach is not suitable for this task, and we need something else instead. One can see this fact that, for example, the semantic orientation of a word can vary according to a given domain. Let us consider this example; a word `big’ is positive when used in sentence such as `this LCD monitor is big.’, while the word should be judged negative in a sentence such as `that portable audio player is too big to me.’ Consequently, we assert that it is necessary to use a longer unit as a sentiment expression instead of using an unit of word in which conventional works do so. In this paper, we propose an opinion-mining method that utilize a new notion of syntactic piece. Syntactic piece is a unit of sentiment expression that is suitable for keeping semantic orientation. More explanation of syntactic piece is described later. 2. Syntactic Piece Syntactic piece and other units are illustrated in Figure 1. Figure 1: Idea of syntactic piece compared with other units Syntactic piece is a minimum unit of syntactic structure of an expression. It is defined as a pair consisting of a modifier and a modifiee (modified entity) from dependency analysis result. This pair is expressed as follows. 77  syntactic piece : modifier ⇒ modifiee Syntactic piece has several characteristics: ・it is very simple; it is easy, just like n-gram statistics, to extract pieces from any given expression since it requires only (partial) parsing result. In contrast, using the whole syntactic structure for opinion extraction is computationally very expensive. Consequently, we think the notion of syntactic piece is considered to have advantages of both n-gram and (whole) tree structure. ・it contains far more information than n-gram. N-gram is a consecutive sequence that keeps information of local context. It is observed that agglutinative languages such as Japanese and Korean has enormous combination of word sequence, since the word order is relatively free. In such languages n-gram-based model is expected not to work well, and some kinds of syntax should be dealt with. ・it can deal with a chunk of meaning, such as a phrasal idiom; e.g. a Japanese phrase `気-に ⇒ なる’ that means `feel uneasy.’ Conventional methods have avoided this problem by (1)ignoring them, or (2) importing an idiom dictionary from outside. In contrast, our method gives them the same treatment, hence we do not need such idiom dictionary or we do not need to recognize idioms as they are. Here we present Japanese patterns of the syntactic piece below: continuous modification ・case frame : noun(-particle) ⇒ predicate e.g. 画面-が ⇒ きれい (clear screen) ・adverbial modification : adverb ⇒ predicate e.g. とても ⇒ おいしい (delicious) adnominal modification ・noun modification : noun-no ⇒ noun e.g. キャノン-の ⇒ カメラ (Canon’s camera) ・verbal modification : verb ⇒ noun e.g. くつろげる ⇒ 店 (comfortable shop) ・adjectival modification : adjective ⇒ noun e.g. おいしい ⇒ ケーキ (delicious cake) ・compound noun : noun-noun e.g. 携帯-電話 (cellular phone) ・prefix : adverb ⇒ prefix-noun e.g. 高-画質 (high picture quality) 3. Method Our opinion extraction model is illustrated in Figure 2. To begin with, our system extracts syntactic pieces1 from a training corpus. We then compute a semantic orientation score for each piece, and construct a seed dictionary of the pieces. We then generalize the dictionary by increasing the entries of the pieces that are labeled according to information of the existing pieces. We extend the dictionary employing large texts 
* Acknowledgements: Special thanks go to the DEMGOL organizers at Trieste (Italy), in particular Mr. Giovanni Zorzetti, who helped us to transfer hundreds of multilingual documents by developping special scripts, and Ms. Francesca Marzari, who has devoted much time and effort to evaluating and testing BEYTrans in a real-world environment. * This research is partly supported by grant-in-aid (A) 17200018 “Construction of online multilingual reference tools for aiding translators” by the Japan Society for the Promotion of Sciences (JSPS). © Copyright 2007 by Alfred Hofmann, Ursula Barth, Ingrid Beyer, Christine Günther, Frank Holzwarth, Anna Kramer, and Erika Siebert-Cole 87  standalone translation memory (TM) systems such as Omega-T are now available (OMEGAT, 2007). While appreciating the importance of these online environments and systems for directly or indirectly promoting the multilingual exchange of information, we recognize the lack of an integrated environment to help online translator communities in a systematic way. Against this backdrop, a free online computer-aided translation (CAT) environment, BEYTrans (Better Environment for Your TRANSlation) has been developed, and the system is now in its experimental stage. This first experimental version has two levels of functionality. The first level corresponds to the translators, who act as separate entities and need specific functionalities (translation editor, linguistic help, etc.). The second level corresponds to the community, in which translators work as an integrated entity. Both functionalities have been integrated into our system using a collaborative Wiki-based technology which provides to volunteer translators with a user-friendly environment and helps them improve translation consistency (Schwartz et al., 2004) (Augar et al., 2004). This paper explains the basic background, concepts and functionalities of BEYTrans. In section 2, we describe the flow of linguistic data and the interactions among translators, on the basis of which the system requirements have been identified. Section 3 describes the different functionalities that should ideally be made available to 1) individual translators and 2) translation communities. In section 4, a detailed explanation of BEYTrans and its position among existing CAT environments is given. The system is currently being used experimentally by the DEMGOL project and other communities, which is briefly examined in the final section. 2. Virtual translation network Online volunteer translators are organized in communities in which they perform translation together or separately, and disseminate multilingual content on the Web. In so doing, they use their private translation environments and communicate frequently with their counterparts (other translators). To build a support system that deals adequately with this situation, for this situation, we need to understand the requirements and the needs of communities, considered as integrated entities, and those of translators, considered as separate entities (Figure 1). In other words, it is important to help translators at two levels: the translator level and the community level. Distinguishing between these two levels allows us to more effectively identify and fulfill existing needs in the translation process. For example, the translation progress in the context of the ArabicMozilla project (ARABICMOZILLA, 2007) needs to be checked constantly and translators should be kept aware of changes in the content. At the same time, each translator, as individual entity, performs translation separately and sends it to the repository (web location dedicated to the relevant community) where the new content is updated. The translation process led us to differentiate two categories of practices: (i) Individual translators working as separated entities; (ii) A translators working as an integrated entities. The former is related to translator behavior where she/he uses private environment (editors, dictionaries, etc.). The later needs separate functionalities for increasing collaboration and consistency. The communication and data exchange in ARABICMOZILLA can be schematized as shown in Figure 1. The community itself is divided into three sub-communities and modeled as a network where nodes represent translators and edges represent change and data manipulation – it reflects also the action that data is subjected to – Furthermore, nodes reflect the translator’s position and requirements inside her/his community and edges represent the requirements and needs of the community. For example, translator A adds an entry in the community dictionary (requirement category (i): dictionary addition function). The same translation could be used by B for the translation of a 88  source word (requirement category: (i) search function and (ii) control change function). After that, it could be updated by C (requirement category (i): dictionary update function). Translators D and E may exchange comments about the possible validation of the new entry (requirement category (i): communication). In fact, in the process of translation, these tasks are done manually and still need to be controlled and supported by an integrated environment. To further clarify our method and show how it applies to the real situation of translation communities in a concrete way, according to these categories of functionalities, we take as an example the ARABICMOZILLA community, in which the translation process is as follows:  Creation A  Search  B Update  E Validation  C  Search  KDE sub-community 
This paper is organized as follows: section 2 introduces the pregroup formalism; section 3 describes the properties and characteristics of Japanese causatives; section 4 presents the analysis of Japanese causatives based on pregroup grammar; and finally, section 5 discusses about the implementation of a practical parser. 2. Pregroup grammar The main idea of a pregroup grammar, which is shared with other categorial grammars, is to assign one (or more) compound types to each word of the language and to check the grammaticality of sentences by doing a calculation on strings of types. We construct the compound types in the following way: we begin with a partially ordered set (A, →) of basic types, the partial order being denoted by the arrow. From the basic types we build simple types by taking adjoints or repeated adjoints. Thus from the basic type a, we obtain the simple types …, all, al, a, ar, arr, … Compound types are strings of simple types. We then form the free pregroup generated by A, whose elements are compound types. The only computations required are contractions ala → 1, aar → 1, and expansions 1 → aal, 1 → ara, where a is a simple type. * Copyright 2007 by Kumi Cardinal 96  One can extend the operation ()l and ()r to compound types, by defining 1l = 1 = 1r, (a・b)l = bl・al, (a・b)r = br・ar. The symbol 1 stands for the empty string of types and will be usually omitted, as will be the dot that stands for multiplication. What makes free pregroups particularly suitable for computation is the following observation: SWITCHING LEMMA (Lambek, 1999) When showing that x1… xm → y1 … yn for simple types xi and yi, one may assume without loss of generality that all contractions ala → 1 and aar → 1 precede all expansions 1 → aal and 1 → ara. The importance of the Switching Lemma is that it offers a deciding procedure, by a sequence of contractions, when a string of simple types can be rewritten as a single simple type. If expansions were needed in addition to contractions, we could for instance compute a number of expansions, followed by a number of contractions, followed by another series of expansions, and continue and so on in an ever-ending process. So, for the purpose of sentence verification, expansions are not needed, but only contractions, combined with some rewriting by the partial order of A. Expansions are useful for theoretical purposes, for example to prove the following: arl = a = alr, a → b ⇒ bl → al, a → b ⇒ br → ar.  3. The structure of causative verbs In Japanese, the causative is formed by attaching the bound causative morpheme –(s)ase to the stem of a verb. If the verb to which it attaches ends in a consonant, the initial consonant s of the morpheme is deleted. In (1b), the causative morpheme is attached to a transitive verb stem, kak ‘write’. Hanako, the original agent of the transitive verb is designated with the dative particle ni.  (1) a. b.  Hanako ga tegami o kaita. Hanako NOM letter ACC wrote ‘Hanako wrote a letter.’ Taroo ga Hanako ni tegami o kak-ase-ta. Taro NOM Hanako DAT letter ACC write-CAUS-PAST ‘Taro made Hanako write a letter.’  The Japanese causative is well known for having two types, the so-called o-causative and the ni-causative. When the causative morpheme is attached to an intransitive verb, as in (2), the original agent of the verb stem can take either the accusative o or the dative ni.  (2) a. Hanako ga aruita. Hanako NOM walked. ‘Hanako walked.’ b. Taroo ga Hanako o aruk-ase-ta. Taro NOM Hanako ACC walk-CAUS-PAST ‘Taro made Hanako walk.’  97  c. Taroo ga Hanako ni aruk-ase-ta. Taro NOM Hanako DAT walk-CAUS-PAST ‘Taro let Hanako walk.’ Kuno (1973), Shibatani (1973), and Kitagawa (1974), among others, have commented on the semantics difference between the o-causative and the ni-causative. O-causatives have been characterized as coercive causatives whereas ni-causatives have been referred to as noncoercive causatives. Note that this choice of o or ni exists only if the verb to which –(s)ase is attached is intransitive. If the verb is transitive, the original agent of the verb can appear only with the dative ni. This constraint against having two or more accusative cases in the same clause is called the Double-o Constraint (Harada, 1973).  4. Computational approach to causative verbs We shall assign one ore more (compound) types to Japanese words and verify whether a given string of words is a grammatical sentence by performing a calculation in the pregroup. First, we introduce the following basic types: π = pronoun; ñ = proper name; n = noun; si = sentence, i = 1 for the present tense; i = 2 for the past tense. c1 = nominative complement; c3 = dative complement; c4 = accusative complement; c6 = ablative complement.  Concepts describing the grammatical constructions of the language fragment help to choose the basic types and the ordering. However, in selecting basic types, we are primarily interested in their algebraic effectiveness, whether or not they correspond to traditional grammatical categories. We also postulate n → ñ → π. We then assign the following types to some representative verbs:  aruku ‘walk’: kaku ‘write’: otiru ‘drop’: aruk-ase-ru ‘walk-CAUS-PRES’: kak-ase-ru ‘write-CAUS-PRES’: oti-sase-ru ‘drop-CAUS-PRES’:  c1rs1 c4rc1rs1 c6rc1rs1 c4rc1rs1, c3rc1rs1 c4rc3rc1rs1 c6rc4rc1rs1  As Harada (1973) notes, ni-causatives are possible only when the causee holds control over the action that he/she performs. Since ‘to drop’ is usually not considered as a self-controllable action, the causative verb otisaseru has type c6rc4rc1rs1 but not c6rc3rc1rs11. The typing in the pregroup formalism assumes that the verb is the central point of the sentence. The type of the verb may change depending on the order of the complements.  
* This work was supported by the IT R&D program of MIC/IITA, Domain Customization Machine Translation Technology Development for Korean, Chinese, and English. Copyright 2007 by Sung-Kwon Choi and Young-Gil Kim 105  Though intensive research has been made on patent MT for the domain-specific advantages, there still remain many issues to be tackled. We focus on the several issues that have continusely been problems in existing English-to-Korean MT systems: (1) new terminology construction, (2) patent-specific probabilities of POS tagger, (3) long and complex sentence analysis, and (4) target word selection. This paper addresses the customization of an English-Korean MT system for patent translation. The English-Korean patent MT system described in this paper is based on an English-Korean MT system developed for the web translation in a general domain. English-Korean patent MT system belongs to basically the pattern-based methodology for machine translation. It has the formalism that does English sentence analysis in which English patent-specific patterns are used, matches the English patent pattern with its Korean patent pattern, and then generates a Korean sentence from it. English-Korean patent MT system consists of an English morphological analysis module based on lexicalized HMM, an English syntactic analysis module by patternbased full parsing, a pattern-based transfer, and a Korean morphological generation. According to experience of patent attorneys, it is said that they read about 7 English patent documents to examine one Korean patent document in average. It means that they examine about 1,000,000 English patent documents for new 150,000 Korean patent documents every year. Korean patent attorneys have required any machine translation system to solve language barrier because they prefer reading Korean translated patent documents to reading English patent documents in spite of such linguistic competency as English native speaker. In this point, th development of the English-Koran patent translation system is closely related to offering of English-to-Korean patent machine translation service through Internet. KIPO (Korean Intellectual Property Office) pushes on with on-line translation service of patent documents by using MT system. The English-to-Korean patent machine translation system described in this paper was developed by ETRI (Electronics and Telecommunications Research Institute) under the auspices of the MIC (Ministry of Information and Communication, Korea) during 2005-2006. In 2006, the patent MT system started an on-line patent MT service in IPAC (International Patent Assistance Center) under MOCIE (Ministry of Commerce, Industry and Energy) in Korea. In 2007, KIPO (Korean Intellectual Property Office) tries to launch an English-Korean patent MT service. Section 2 describes the customization processes that relate to new terminology construction, patent-specific probabilities of POS tagger, long and complex sentence analysis, and target word selection, respectively. The experimental work is presented in section 3. Lastly, in section 4, we present some conclusions. 2. Customization Process Some methods of customization to change general MT system to domain-specific MT system have been introduced. For example, the customization process in SYSTRAN as multilingual MT system consists of the following steps: term extraction, dictionary customization, linguistic customization, and testing/evaluation (Zajac, 2003). Hong (2005) applied such an existing customization process to a Korean-English MT system. In comparison with the existing customization methods above mentioned, the customization process described in this paper is the first worth-mentioning large-scale customization effort of an MT system for English and Korean. The customization process for an English-Korean patent MT system includes the following steps: 1) linguistically studying about characteristics of patent documents, 2) extracting unknown words from large patent documents and constructing large bilingual terminology, 3) extracting and constructing patent translation patterns 4) customizing the translation engine modules of the existing general MT system according to linguistic study about characteristics of 106  patent documents, and 5) evaluating the accuracy of translation modules and the translation quality  2.1.Construction of Patent Terminology The first step of customization process for patent MT system is to gather the existing terms, extract the unknown words from patent documents, and build the bilingual terms. The customization process described in this paper is similar to the method of Kaji(2005), Shimohata(2005), and Kim(2005) in respect of using the monolingual dictionary and the monolingual patent corpus, but our method is different in that it contains a step inverting the existing bilingual terminology with opposite direction. Extraction and construction of terminology might be represented as a following customization process:  Existing English-to-Korean Dictionary  adding  Korean-to-English Terminology English Patent Documents  Transforming into English-to-Korean Terms Extracting Unknown Words  Removing Overlapped English Entries Removing Overlapped English Entries  Semiautomatically Building Target Words adding Manually Building Target Words  English-to-Korean Patent Dictionary  Figure 1: Customization process for building English-Korean patent terminology  As shown in Figure 1, the patent terminology can be built in two ways. One is to extract the unknown words, remove the overlapped entries, and build manually new bilingual terminology. The other is to build semi-automatically new bilingual terminology, assumed we have the existing bilingual terminology with reverse direction (for example, Korean-to-English terminology). By use of the above customization process, we built semi-automatically 801,046 of new bilingual terminoloy and manually 1,039,189 of new bilingual terminology, that is, 1,840,235 English-Korean terms were totally built for 7 months. 23 people as lexicographers have worked to build the new bilingual terms every day.  Table 1: New English-Korean patent terms semi-automatically built by inverting the existing bilingual terminology  Items Number of existing Korean-English terminolgy Number of entries of general-purpose English-Korean dictionary New English-Korean terms with the exception of the English terms of existing Korean-English terminolgy overlapped with entries of general-purpose English-Korean dictionary  Number of entries 3,052,655 836,000 801,046  Table 1 shows that new 801,046 English-Korean terms were semi-automatically constructed from the existing Korean-English terminology. They are consisted of 207,329 single words and 593,717 compounds.  107  In addition to new patent terms constructed semi-automatically, we had to extract a number of unknown words from English patent documents and manually build new English-Korean terms, because we had no English-Korean patent corpora.  Table 2: New English-Korean patent terms built manually by use of large English patent documents  Items English patent documents used Extracted unknown words New English-Korean terms  Number of entries 1,001,419 9,662,266 1,039,189  In Table 2, we can know that new 1,039,189 English-Korean terms were built from the very large English patent documents. They consisted of 492,295 single terms and 546,894 compound terms. In the result, we have now 2,676,235 English-Korean terms including existing 836,000 terms of general-purpose English-Korean dictionary.  2.2.Customization of POS Tagger We define three customization phases for customizing a general POS tagger based on HMM (Hidden Markov Model) to patent domain, according to the characteristics of English patent document mentioned in the section 2.1: - Customization of surface form analysis : a tokenization module and/or a morphological analyzer are modified for tokenizing and/or analyzing the peculiar surface forms found in the specific domain. - Customization of the lexical information : lexical probabilities (output probabilities) are adjusted for holding domain-specific lexical information. - Customization of the context information : contextual probabilities (transition probabilities) are adjusted for holding the domainspecific contextual information. In the first phase for customization of surface form analysis, the tokenization module is modified to tokenize and/or chunk very complex symbol words, a chemical formula, a mathematical formula, programming codes, and so on. And our morphological analyzer is improved to assign the estimated part-of-speeches into a compound word connected with hyphen or slash. The estimated part-of-speeches are estimated using the part-of-speeches of its components. The POS tagging module of English-to-Korean patent machine translation system is based on lexicalized HMM (Pla & Molina, 2005). Therefore, the best simple strategy for the second and third customization phase is retrained from a very large tagged patent corpus. However, there is not a tagged patent corpus and it is also very difficult to construct it. Accordingly, for customizing the lexical and contextual probabilities, we used a raw patent corpus consisting of about one million US patent documents applied for from 2001 to 2005. First, the words of the raw corpus are automatically tagged by our general domain POS tagging system, and then the lexical and contextual probabilities are extracted from the machine-tagged patent corpus. Next, we extracted high-frequent lexical having very different probability with that of the general domain. And we extracted the high-frequent contextual n-grams that didn’t appear in the general domain. The extracted lexical and contextual n-grams are tuned by the human experts. For customization of our POS tagger, we tuned about 6,000 lexical and about 1,500 tri-grams.  108  2.3.Customization of Syntactic Analyzer for Long Sentences The important syntactic characteristics of the patent document are the frequent use of the patent intrinsic translation pattern and abnormally long sentences. With these as central figures, the main contents of customization of syntax analysis are as follows: - A build-up and application of the patent translation pattern : the patent-specific patterns are manually built up and the processing for the recognition of patterns is performed. The general forms of the patent-specific patterns are composed of lexical words and syntactic nodes. Therefore, for the recognition of the patterns, the lexical words are firstly matched, and then the ranges between the lexical words are parsed. If all ranges are parsed into corresponding syntactic nodes in the translation pattern, the pattern is recognized. - A large amount of lexical pattern collections and application : in the patent documents, the high frequency lexical patterns corresponding to the specific part-of-speech patterns are automatically extracted and are applied to syntactic analysis. - Performing the coordinate construction recognition for long sentences : for the coordinate construction recognition, first, the possible site which can become the initial point, the intermediate point, and an endpoint of the parallel construct. Then, the similarity table between each node is constructed. For the all possible coordinate structures, the coordinate weight is calculated using the similarity table. Finally, the coordinate structure having maximum coordinate weight is selected as a final result. The recognized coordinate construction is chunked to one unit, and accordingly the sentence is simplified. - Performing the sentence segmentation for the long sentence : in case of being too long to analyze the sentence at a time in syntactic analyzer, even after the parallel construction is recognized, the sentence segmentation is performed. The sentence is segmented by recognizing participles or simple sentences. - Reflecting attachment preferences : priority for the attachment of ’for’ prepositional phrase and participle is given to the NP attachment than VP attachment. 2.4.Customization of Transfer Module for Target Word Selection We customized the transfer module for patent document translation. Following customization items for transfer modules were considered: - The registration of the default target word according to patent technical field : In the case that the same source word can be translated into different target word depending on the patent field, the specific value of the 'field' feature is assigned to dictionary. - The gathering of collocation information for noun/verb with high frequency : We use collocation information to select the proper target word depending on the context. The collocation information is used as main knowledge to cope with the problem of the target word selection. - The implementation of the module to achieve target word selection using collocation information : This module carries out the task to select proper target word using collocation information. The approach consists of two levels. In the first step, sense ambiguity of English word is resolved. In the second step, the most suitable Korean target word is selected. To select the most suitable target word, our approach uses multiple knowledge sources such as verb frame patterns, sense vectors based on collocations, statistical Korean local context information and co-occurring POS information. Sense vectors are made using EnglishKorean parallel corpus. (Lee, 2006) 109  - The implementation of the interpreter for patent-specific patterns : The patent-specific patterns were introduced to translate highly frequent expression. Our parser uses these patent-specific patterns in parsing time and then transfer module interprets the patent-specific patterns applied by the parser.  3. Evaluation 3.1.Evaluation of Morphological Analyzer We evaluated the performance the POS tagger specialized to the patent domain (PatTagger), compared with the performance of our general-purpose POS tagger (GPTagger). For the evaluation, we used 100 sentences of the electrical and electronics field (EEF) among the whole translation evaluation test set. The EEF test set consists of 2,942 words and the number of words per a sentence is 29.42. Table 3 shows the word accuracy and sentence accuracy of two taggers. From these results we can draw the following conclusions. First, the PatTagger reduced significantly the error tagging about 91% with respect to the GPTagger. Second, PatTagger improved the sentence accuracy with 41% compared with GPTagger. This improvement seems to contribute to the performance improvement of the proposed English-Korean patent translation system.  Table 3: Comparison of the tagging accuracy between GPTagger and PatTager  GPTagger PatTagger  Word tagging accuracy  95.85%  99.62%  Up 3.77%  Sentence tagging accuracy  50.00%  91.00%  Up 41.00%  Table 4 shows the performance improvement factors of PatTagger and the improved word accuracy according to the factors. The improvement factors of PatTagger are three customization phases mentioned in the section 2.2 and construction of terminology mentioned in the section 2.1. The construction of terminology is to add unknown words and their part-ofspeeches into morphological analysis dictionary. The performance improvement of word supplement is very low because our POS tagger handles unknown words using suffix analysis as proposed in Brants(2000). From the results of table 4, the customization of lexical and context information is surely needed in order to specialize a general-purpose POS tagger based on HMM to a specific domain.  Table 4: The performance improvement of PatTagger and the improvement of its word tagging  accuracy.  The performance improvement factor The # of The  The improvement  tagging error correction of word tagging  correction  rate  accuracy  Customization of surface form  6  5.41 %  0.20%  analysis  Customization of the lexical  81  72.97 %  2.75%  information  Customization of the context  22  19.82 %  0.75%  information  Construction of Terminology  2  1.80 %  0.07%  Total  111  100.00 %  3.77%  3.2.Evaluation of Syntactic Analyzer for Long Sentences The evaluation result by the customization of syntactic analyzer is as follows:  110  Table 5: Evaluation of customization of syntactic analyzer  Syntactic Translation  analysis  accuracy  accuracy  General-purpose Syntactic 69%  73%  Analyzer  Customized Syntactic Analyzer  85%  81.6%  ERR  Up 16%  Up 8.6%  Number of translation patterns 47,413 75,931 6 months, 3 people per day  In the above table, the syntactic analysis accuracy is calculated by the ratio of the number of correctly analyzed sentences to the number of total sentences1. We use the accuracy by the sentence unit instead of the common parsing evaluation metrics by the bracketing match, because the accuracy by the sentence unit shows the direct correlation with the translation accuracy. And the translation accuracy is the comparison result between before and after the customization of syntactic analyzer in the translation system customized for patent documents.  3.3.Evaluation of Transfer Module for Target Word Selection We compared general-purpose transfer module and patent-specific transfer module for evaluating the performance of target word selection for noun. The test set for the experiment consists of 100 sentences from patent documents. Table 6 shows the experimental results of target word selection of the customized MT system and the non-customized MT system. The performance of customized MT system which has taken customization process for patent document into account overcomes the one of counterpart.  Table 6: Result of target word selection for noun  Accuracy of target word Percentage  of  selection for noun  unknown word  General-purpose  71.7%  16.3%  Transfer Module  Customized Transfer  92.4%  1.5%  Module  3.4.Translation Accuracy In this chapter, we describe the evaluation about translation quality of English-to-Korean patent MT system. It relates to 5 major patent fields selected from different patent fields. We used the following test sentences, evaluation method and evaluation criterion for translation quality: - Test sentences : translation accuracy was assessed with 100 test sentences randomly extracted from each one of 5 major patent fields (machinery, electronics, chemistry, medicine and computer). The test set was so open that it might reflect a real patent document. Among 100 sentences for each patent field, about 54 sentences were selected from the “detailed description” section of patents, 24 were extracted from the “claim” section, the rest from the “description of the drawing” and the “background of the invention” section. The average length of a sentence was 28.09 words. - Evaluation criterion:  
Among the studies of NPIs, downward entailment (DE, Ladusaw 1979, cited in 1996) is the first to show the property of NPI licensing context. Later on more accurate properties (Zwarts 1993 cited in Ladusaw 1996 and Nam 1998) were proposed, creating a hierarchy of negative ∗ Copyright 2007 by Yoon-Hee Choi 115  expressions since DE simply distinguishes negation from affirmation. The followings are three negative functions (examples are from Lee 1999). (1) Three negative functions (ⅰ) Downword entailment If A and B are two Boolean algebras, the function f from A into B is polarity reversing iff for any a1, a2 ∈A, if a1 ≤ a2, then f(a2) ≤f(a1). e.g. at most (weak) (ⅱ) A functor f is anti-additive iff f(X∨Y)=f(X)∧f(Y). e.g. no, before, every (strong) (ⅲ) A functor f is antimorphic iff f is anti-additive and additionally f(X∧Y)=f(X) ∨f(Y). e.g. not (the strongest) Zwarts (1995) added here a weaker function, that is ‘nonveridicality’ and Giannakodou(2002, 2007) developed it with Greek subjunctive mood. Zwarts' (1995) definition about nonveridicality is as below: (2) Nonverdicality Let O be a monadic sentential operator. O is said to be veridical just in case Op⇒p is logically valid. If O is not veridical, then O is nonveridical. A nonveridical operator called averidical iff Op ⇒ ~p is logically valid. Despite the fact that three negative functions and nonveridicality have been instrumental in our understanding of NPI licensing context, some problems remain unsolved: emotive factive predicates that are veridical but license NPIs (e.g. I am happy to get any ticket), and some nonveridical predicates that partially (e.g. % I hope there is any food left) license NPIs. Moreover there are many examples which do not exactly correspond to the typology based on negative functions. French subjunctive and ne explétif can guide us to reconsider the previous unsolved problems about NPI licensing context. 3. French subjunctive and nonveridicality 3.1. French subjunctive, nonveridicality and the weak NPIs Traditionally indicatives represent the act or state as an objective fact while subjunctives1 express subjective actions such as will/wanting, emotion, doubt, possibility, necessity, judgement, comparatives. French subjunctive also appears after the conjunctions like 'before', 
Abstract. We propose and test several computational methods to automatically determine possible saliency cut-off points in Sketch Engine (Kilgarriff and Tugwell, 2001). Sketch Engine currently displays collocations in descending importance, as well as according to grammatical relations. However, Sketch Engine does not provide suggestions for a cut-off point such that any items above this cut-off point may be considered significantly salient. This proposal suggests improvement to the present Sketch Engine interface by calculating three different cut-off point methods, so that the presentation of results can be made more meaningful to users. In addition, our findings also contribute to linguistic analyses based on empirical data. Keywords: saliency, cut-off point, threshold, collocations 1. Introduction All lexical resources, at the point of their design, will take into consideration whether the resources are useful to a target group. For example, WordNet (Fellbaum, 1998) was originally designed for psychologists, but later was used extensively by computational linguists. Similarly, corpora such as British National Corpus (BNC), the Academia Sinica Corpus of Mandarin Chinese (Chen et al., 1996) and the Gigaword corpus were also designed for the use of target groups such as lexicographers, linguists, language teachers, language learners, etc. These corpora usually provide some forms of statistical analyses so that users will be able to summarize their research results quickly. For example, many corpora provide collocational measures such as Mutual Information values (Church and Hanks, 1989) so that collocated words can be sorted according to their frequency of co-occurrence. Sketch Engine (Kilgarriff and Tugwell, 2001) is a powerful resource which displays search summary in collocated patterns, as well as according to grammatical relations. However, like many other resources, Sketch Engine is unable to determine which of the results in the list are meaningful linguistically. Therefore, when provided with collocation lists, most linguists report the top “few,” based on their preferences. Some linguists report the top one or two and keep the rest in appendixes. In fact, the current search summary from corpora or lexical resources does not give enough * We would like to thank Professor Shu-Chuan Tseng for her comments on this paper, and for suggesting the idea underlying third method for calculating cut-off points. Copyright 2007 by Siaw-Fong Chung, Kathleen Ahrens, Chung-Ping Cheng, Chu-Ren Huang and Petr Šimon. 126  information regarding which of the collocational patterns are significantly different from the bottom words. In this paper, a research question is asked, i.e., whether or not one can select top rankings from linguistic results using principled measures. This selection of top rankings is useful because it will provide an automatic identification of significant linguistic results from the data. This also involves deciding which significant results are likely to be prototypically used in certain linguistic environments (Rosch and Mervis, 1975). In this paper, we propose three methods in which the threshold of linguistic listings can be extracted. In the following section, data presentation in Sketch Engine is first discussed.  2. Data Presentation in the Sketch Engine Sketch Engine is a system that provides the collocations of words according to grammatical relations. It has been used to analyze large scale corpora data such as the British National Corpus (BNC) and the Chinese Gigaword corpus. The Chinese Sketch Engine was created by Kilgarriff, Huang, Rychly et al. (2005). It has the same function as the English Sketch Engine, which also arranges collocates for query words in grammatical relations. For example, when a query word is searched in Sketch Engine, the system will return with the collocates for this query word. Sketch Engine then arranges them in grammatical relations such as ‘objects of the query word,’ ‘subjects of the query word,’ ‘modifiers of the query word,’ etc. The following Figure 1 shows an example of the search result for 經濟 jing1ji4 ‘economy’ in the Chinese Sketch Engine.  Query word  Frequency of query word in both Taiwan and Chinese corpora  Collocates of query word  Frequency of each collocate and query word  Saliency of each collocate and query word  Figure 1: Collocates for the Query Word 經濟 jing1ji4 ‘Economy’ in the Chinese Sketch Engine  127  In Figure 1, the query word and its frequency in the entire Gigaword corpus are shown (i.e. 1,295,965 instances). The frequency for pair of collocates such as 經濟 jing1ji4 ‘economy’ and 振興 zheng4xing4 ‘to give life to’ under the ‘object-of’ relation (arrow in Figure 1) is given. In this case, it is 4,046 (in the second column for each relation), indicating that 經濟 jing1ji4 ‘economy’ appears as the ‘object of’ the verb 振興 zheng4xing4 ‘to give life to’ 4046 times in the whole Gigaword corpus. In addition to frequency, Sketch Engine provides an additional score for the ranking of saliency of collocates. This is because Kilgarriff and Tugwell (2001) suggest that frequency alone may not be a reliable score because frequency of the collocates are relative to the number of both words in the whole corpus. Therefore, they suggest using a more reliable account to standardize all frequencies for the collocations based on the overall performance of the collocates in a particular condition. However, while the presentation of saliency in Sketch Engine is robust and useful, it does not indicate which of the collocates in each relation are meaningfully salient. WordNet (http://wordnet.princeton.edu/) can also display search results based on a “high frequency count” (see Figure 2). Figure 2: Displayed by Frequency Counts in WordNet 3.0 This frequency count is ordered from the most frequent sense to the least frequent sense (Tengi, 1999) that is computed using a semantic concordance created by Landes, Leacock and Tengi (1999) based on two corpora – the Brown corpus and Stephen Crane’s novella entitled The Red Badge of Courage.1 From Figure 2, one can see that the sense frequencies for ‘depart’ are 11, 5, 3 and 1. We can see that there is a bigger gap between the frequency of the first sense (11) and the frequency of the second sense (5). Based on this gap, we may say that the first sense is more often used than the second one. It is also possible to say that the first sense is more prototypical than the other 1 Only senses that were found in the two corpora can be shown their frequency counts in brackets. 128  senses. Therefore, there is possibly a threshold after the first sense to make the first sense more distinctive in use than the others. Therefore, this paper suggests that there should be some objective methods which can help determine the threshold of linguistic listings as such. This paper suggests three methods to find out how many of the top few results should be considered significant in Sketch Engine. These methods are elaborated below. 3. Computing Thresholds of Linguistic Listings This paper will discuss three methods. Methods One and Two are based on the characteristics of the distributional listings, which usually follow Zipf’s law (Zipf, 1932). Therefore, these two methods will be discussed together in section 3.1 below. Section 3.2 will discuss Method Three, which is different from both methods one and two. Section 4 will present results from all three methods. 3.1.Methods One and Two Zipf’s law states that the most frequent value is most likely to be twice as much as the second most frequent value. For example, when a sample size is large enough, the result of a frequency listing is likely to be in a distributional pattern. For instance, the expression 起飛 qi3fei1 ‘takeoff’ in (1) below, has the following collocates from the Sketch Engine (Figure 3). (1) 但 在 台灣 經濟 起飛 後 (Central News Agency of Taiwan) dan4 zai4 tai2wan1 jing1ji4 qi3fei2 hou4 but at Taiwan economy takeoff after “But after the economy of Taiwan takeoffs…” The collocates for 起飛 qi3fei1 ‘takeoff’ which have similar grammatical relations with 經濟 jing1ji4 ‘economy’ (the ‘subject’ relation) can be seen in Figure 3 (such as 飛機 fei1ji1 ‘airplane,’ 班機 ban1ji1 ‘flight,’ 跑道 pao3dao4 ‘path’ as well as 經濟 jing1ji4 ‘economy’). We can see that in Figure 3, the saliency values of the collocates are arranged in descending order (from 55.67, 48.31, 38.64, and continue on until the lowest value, which is zero). 129  Figure 3: Collocates of ‘Subjects’ of 起飛 qi3fei1 ‘takeoff’ in the CNA in the Sketch Engine Most frequency list follows the pattern of the Zipf’s law, where the top few are usually very high and the values will decrease until a state where changes become minimum. For example, for the saliency list in Figure 3, when plotted in graph, the representation can be seen in Figure 4 below. In Figure 4, the x-axis is the ‘Chinese subject’ and the y-axis is the ‘saliency’ (Figure 4 uses the rank of the Chinese word to represent the Chinese character – rank 1, 2, 3…). All these Chinese words are the collocates of 起飛 qi3fei1 ‘takeoff.’ 130  Figure 4: Pattern of Distributional Data for 起飛 qi3fei1 ‘takeoff’ following Zipf’s Law The function for the type of graph in Figure 4 is such that in (2), where any point in the graph will be (x, f(x)). x is the rank of Chinese subjects on the x-axis and f(x) is the function to calculate the value on the y-axis. f ( x ) = b ( x a ) (2) Using this formula, Methods One and Two will find a point that separates any distributional listing into two lists, i.e., significant and insignificant lists. The purpose of doing this is to find out which among the list should be considered significant and which to be insignificant. Figure 5: Three Ways to find Threshold Values Methods One and Two are based on the assumption that there is a point where the curve changes the most when it goes down the y-axis to the x-axis. Methods One calculates the position of (w, z) where it is of shortest distance from (0, 0). This is because when every line departs from the starting point of (0,0), there will be a line that is the shortest distance from the curve. The point where this line touches the curve is the point where the curve changes the most from the y-axis to the x-axis. Method Two calculates the most slanted slope between the x-axis and the y-axis. When the slope is most slanted, the possibility is high that the curve changes the most at a certain point (w, z). This is because the higher the curve on the y-axis, the more vertical the slope will be. Moreover, the further the curve moves away from (0, 0) on the x-axis, the more horizontal the slope will be. Therefore, the most slanted slope between the vertical and horizontal will be the possible threshold representing where the curve has changed the most. 131  The formulas for the two methods are shown in (3a) and (3b) below. In these two formula, a and b are the variables in the function of the nonlinear regression y = b(xa ) while i is the threshold value and n is the total number of collocates in the relation.  Method One: Method Two: Method Three is elaborated below.  i  =  ⎡ ⎢ (( − ab  2  )  (  2  
* This research was supported in part by the National Science Council under a Center Excellent Grant NSC 95-2752-E-001-001-PAE and Grant NSC95-2221-E-001-039. Copyright 2007 by You-Shan Chung, Shu-Ling Huang, and Keh-Jiann Chen 136  subcategory of function words that also has properties of content words (Chen 2005 et al.). The definition of modality thus provides insights into how words that fall somewhere in the middle on the content-function word continuum are defined. Third, modals’ co-occurrences with negation markers show discrepancies between surface structure and meaning, and serve as a on the content-function word continuum are defined. Third, modals’ co-occurrences with negation markers show discrepancies between surface structure and meaning, and serve as a testing ground for the defining capability of the framework. In E-HowNet, the word to be defined is assigned a head, which is semantically and syntactically similar to it. Then, words that describe the head are linked to the head through features. For example, the word 小子 xiaozi ‘lad’ refers to someone who is young. Therefore, we represent the word with the head 人 ren ‘person’ and the modifying word 年幼 nianyou ‘young’. Since 年幼 nianyou refers to the age of the person, the two concepts are linked by the semantic role ‘age.’ Its representations are as the following, with the first defined by simple concepts and the second by sememes:  (1) 小子 xiaozi ‘lad’  def: {人:age={年幼}} def:{human|人:age={child|少兒}}  Eventually, the meaning of words and phrases in E-HowNet will be integrated for the semantic representation of sentences. The organization of the paper is as follows: In Section 2, we state the definition of modality in E-HowNet. In Section 3, we explain how modals are represented as single words and as components of larger linguistic constituents. In Section 4, we deal with the co-occurrence of negation markers and modals. We show with that E-HowNet is able to cope with meanings that are determined by its relative position with other elements in a sentence. Following that, in Section 5, we conclude that E-HowNet can represent a semantic category like modality that (a) involves pragmatics and (b) belongs to function words but is like content words in some aspects.  2. The scope of modality Following Hsieh (2005), we do not assume that modals have to be auxiliaries but identify them on semantic grounds. They all refer to speakers’ judgment. There are two meanings unanimously recognized as central to modality: epistemic and deontic. The former refers to a speaker’s judgment of whether a situation will happen and the latter to a speaker’s attitude toward whether something is required to be done. Another two categories admitted by many researchers are words that denote abilities and volition (Hwang 1999, Li 2003, Hsieh 2003, Hsieh 2005). Another modal category that we recognize is expectation, which includes words  137  that describe whether a situation’s taking place is expected or not. Below we summarize the five kinds of modal categories adopted by the current study, each followed by some examples: Epistemic: judgment that something will (not) happen: e.g. 絕對 ‘juedui ‘absolutely,’ 會 hui ‘will,’ 也許 yiexu ‘maybe,’ 不一定 buyiding ‘not necessarily,’ 不可能 bukeneng ‘impossible,’ 未必 weibi ‘not necessarily’ Deontic: judgment that something is (not) allowed to happen due to the speaker’s will or social or ethical reasons: e.g. 可以 keyi ‘may,’ 應該 yinggai ‘be supposed to,’ 理當 lidang ‘be supposed to,’ 不該 bugai ‘be not supposed to,’ 不應 buying ‘be not supposed to,’ 不可 buke ‘may not’ Ability: judgment that someone/something is (in)capable of something: e.g. 能 neng ‘be able to,’ 會 hui ‘can,’ 不能 buneng ‘cannot,’ 不會 buhui ‘cannot’ Volition: judgment that someone is (un)willing to do something: e.g. 想 xiang ‘hope to,’ 不想 buxiang ‘does not want to’ Expectation: judgment that something was (not) expected to happen or someone was (not) expected to do something: e.g. 果然 guoran ‘as expected,’ 果真 guozhen ‘as expected,’ 不出所料 buchusuoliao ‘as expected,’ 竟 然 jingran ‘unexpectedly,’ 不 料 buliao ‘unexpectedly,’ 沒 想 到 meixiangdao ‘unexpectedly’ The above examples tell three things about our identification of modals. First, besides auxiliaries, some adverbs are also considered modals, e.g. 果 然 guoran and 沒 想 到 meixiangdao. Second, like Hsieh (2003), we think that some modals express a positive meaning whereas the others express a negative meaning. The former half of the examples of each modal category is on the positive side whereas the latter is on the negative side. Third, like most researchers, we believe that modals within the same category differ in modal strength (Hwang 1999, Li 2003, Hsieh 2003, Hsieh 2005). Lyons (1977, reviewed in Hsieh 1999) thinks the basic definition of modality is a semantic scope that refers to possibility and necessity, two meanings that differ in strength of assertion. Such a definition suggests that, within the same modal category, modals that express that a judgment is possible is weaker in modal strength than those that express that a situation is necessary. For example, in epistemic modality, the modal 也許 yiexu ‘maybe’ indicates the speaker’s speculation that something might happen, whereas the modal 一定 yiding ‘certainly’ conveys the speaker’s certainty for something to take place. 一 定 yiding thus has stronger modal strength than 也許 yiexu. Therefore, for each modal category, we adopt two sememes to scale modal strengths: ish|稍: sememe signaling weak to moderate modal strength. 138  extreme|極: sememe signaling strong modal strength 3. The representation of modals and words/sentences that contain modal meanings In Section 2 we have described in brief the representation of meaning in E-HowNet. We have proposed five modal categories. Besides, we believe that each category consists of modals that express positive and negative meanings. Finally, we give grades for modal strengths. The complete inventory of modal meaning representations is as follows: Epistemic: possibility={extreme|極}; possibility={ish|稍}; impossibility={extreme|極}; impossibility={ish|稍}; Deontic: allowance ={extreme|極}; allowance ={ish|稍}; disallowance ={extreme|極}; disallowance ={ish|稍}; Ability: capacity={extreme|極}; capacity={ish|稍}; incapacity={extreme|極}; incapacity ={ish|稍}; Volition: willingness ={extreme|極}; willingness ={ish|稍}; unwillingness={extreme|極}; unwillingness={ish|稍}; Expectation: expectedness={extreme|極}; expectedness ={ish|稍}; unexpectedness ={extreme|極}; unexpectedness ={ish|稍}; In the following table we give an example for each modal meaning: 139  Table 1: Examples of each modal meaning  epistemic  strength ish|稍  negative/positive possibility  extreme|極  impossibility possibility  deontic  ish|稍  impossibility allowance  extreme|極  disallowance allowance  ability  ish|稍  disallowance capacity  extreme|極  incapacity capacity  volition  ish|稍  incapacity willingness 
a Information Technology Department, School of Computer Studies Mindanao State University-Iligan Institute of Technology, Tibanga, Iligan City d_dimalen@yahoo.com b College of Computer Studies, De La Salle University-Manila, roxasr@dlsu.edu.ph  Abstract. AutoCor is a method for the automatic acquisition and classification of corpora of documents in closely-related languages. It is an extension and enhancement of CorpusBuilder, a system that automatically builds specific minority language corpora from a closed corpus, since some Tagalog documents retrieved by CorpusBuilder are actually documents in other closely-related Philippine languages. AutoCor used the query generation method odds ratio, and introduced the concept of common word pruning to differentiate between documents of closely-related Philippine languages and Tagalog. The performance of the system using with and without pruning are compared, and common word pruning was found to improve the precision of the system. Keywords: document acquisition, document classification. 1. Introduction A corpus is a term used to designate a body of authentic language data that can be used as a basis for linguistic research. 1 It is also applied to a body of language texts that exist in electronic format. It is estimated that there are currently over 4 billion pages on the world wide web (WWW) covering most areas of human endeavor. And as more information are becoming electronically available on the web, we need more effective methods and techniques to access these information. To date, there has been limited effort in taking advantage of this available information on the web for building natural language resources especially for sparse languages (or minority languages) like Tagalog and other Philippine languages. Unfortunately, to manually collect and organize a language specific corpus over the Web is difficult. The process is tedious and time consuming. To add, an expert in linguistics is needed to manually determine the language where the document collected is written.  * This project is funded by the Philippine Council for Advanced Science and Technology for Research and Development, Department of Science and Technology, Philippine Government. Copyright 2007 by Davis Muhajereen D. Dimalen, Rachel Edita O. Roxas 1 Orasan, C. and R. Krishnamurthy 2000. An Open Architecture for the Construction and Administration of Corpora. Proceedings of the Second International Conference on Language Resources and Evaluation. pp. 793-800. 146  A system that automatically acquires language specific documents from the Web is one good solution in corpora building. Creating such a system requires knowledge in information retrieval and natural language processing. 2. Automatic Corpora Builder on a Closed and Open Corpus Several components are required for an automatic corpora builder: a set of seed documents, a language modeler, a query generator, a web search engine, and a language filter2. The CorpusBuilder takes advantage of existing search engine database to collect documents from the web3. It iteratively creates new queries to build a corpus in a single minority language. Sets of relevant and non-relevant documents are taken as initial inputs. Relevant documents are those that belong to the target language, while non-relevant documents are other documents that belong to other languages. These documents are used as inclusion and exclusion terms for the query. The query is sent to the search engine and the document that has the highest rank will be retrieved. The document retrieved is processed through the language filter and classified as either relevant or non-relevant document. The newly classified set of documents is the product of the system and is the basis for the next term selection as the system iterates. CorpusBuilder is a system that automatically builds a minority language corpus. An examination of this corpus showed that the corpus also contained documents in languages that are closely-related to the identified minority language. Specifically, there were documents retrieved that are closely-related Philippine languages to the identified minority language Tagalog. Thus, in this study, we considered the three most closely-related languages in the Philippines, Bicolano, Cebuano and Tagalog, as identified by Fortunato4, that belong to the Austronesian family of languages. This can be explained by the fact that closely-related languages within the same family of languages exhibit common linguistic phenomena. For instance, there are several Bicolano, Cebuano and Tagalog words which are common to these languages as illustrated in Tables 1 to 3.  Table 1: Words Common to Tagalog and Cebuano.  Tagalog/ Cebuano apo anak bayaw langgam (Tagalog) langgam (Cebuano) bangka  English grandchild son/daughter in-law ant bird sailboat  Table 2: Words Common to Tagalog and Bicolano.  Tagalog/Bicolano English  hayop  animal  tao langit  human heaven  pakpak  wings  2 Ghani, R., R. Jones and D. Mladenic. 2001. Using the Web to Create Minority Language Corpora. Proceedings of the 10th International Conference on Information and Knowledge Management. pp. 279 – 286. 3 Jones, R. and R. Ghani. 2000. Automatically Building a Corpus for a Minority Language on the Web. In the Proceedings of the Annual Meeting of the Association of Computational Linguistics 2000. pp. 29-36. 4 Fortunato, F. T. 1993. Mga Pangunahing Etnoling-guistikong Grupo sa Pilipinas. Malate, Manila, Philippines: De La Salle University Press.  147  pinsan  cousin  Table 3: Words Common to Bicolano, Cebuano, and Tagalog.  Bicolano/Cebuano/Tagalog English  agaw  snatch  bawi  snatch  kadena belen  chain manger  Thus, AutoCor considered closely-related languages rather than a single minority language, and used document classification using common word pruning which has shown to improve the precision of the system. The corpus that was used in this research contains documents from the web. The corpus contains 4,000 documents, wherein the target or relevant documents were tagged correspondingly, having 250 documents each in Bicolano, Cebuano and Tagalog, and the rest of the documents functioned as the non-relevant documents were in English, Hungarian and Polish. The selection of the set of non-relevant documents was based on similar character sets and the availability of documents. Figure 1 illustrates the overall architecture of AutoCor on a closed corpus. There are 5 main routines namely, the Language Modeler, Common Word Pruning, the Query Generator, Sampling, and finally the Document Classifier. Each routine is done in sequence. Initially the first routine (Language Modeler) requires initial seed documents for each of the selected closely-related languages (L) and for the other languages (OL). Each language in (L) and (OL) is denoted by the sets {L1…Ln} and {OL1…OLn}, respectively. The “Initial Documents” is defined by the sets (iDL) and (iDOL) wherein (iDL) is the set of initial documents in closelyrelated languages (L) and (iDOL) is the set of initial documents in other languages (OL). The language models are composed of (LML) and (LMOL) wherein (LML) is the set of language models for the closely-related languages (L) and (LMOL) is the set of language models for the other languages (OL). The Pruned Language Models are the sets (PLML) for the closely-related languages (L) and (PLMOL) for the other languages (OL). The output corpus is composed of a set of documents classified as closely related languages (DL) and another set of documents classified as other languages (DOL) wherein (DL) is also equal to the set {DL1,DL2,…,DLn}. Documents are retrieved via Sampling from a Closed Corpus. The system works as follows:  a. Select one seed document each from the set of initial documents in iDL and the set of initial document in iDOL. b. Using the seed or initial documents in the target language and other languages, build language models LML and LMOL for each of the languages in L and OL. 
Hong, Jia-Fei , Chu-Ren Huang , Kathleen Ahrens ,  a National Taiwan University, Graduate Institute of Linguistics No. 1, Sec. 4, Roosevelt Road 106, Taipei, Taiwan R.O.C b Institute of Linguistics Academia Sinica No. 128, Section 2, Academia Road 115, Taipei, Taiwan R.O.C {jiafei, churen}@gate.sinica.edu.tw, kathleenahrens@yahoo.com  Abstract. In this study, we explore the polysemy of da3 through the ontological conceptual structure found in SUMO. First, we divide several different senses for da3, clustering physical event senses and metaphorical event senses. In here, we only focus on physical event senses of da3. From the physical event senses of da3, we divide them into two main categories: 1) hit and 2) pump. We then use SUMO ontological concepts to identify these physical senses. Finally, we can observe the common patterns of the “hit” sense group and the “pump” sense group for da3. Keywords: da3, Polysemy, ontology, lexical semantics 1. Introduction In this study, we explore all possible concepts for physical event senses of da3 through the SUMO ontological concept system (Huang et al. 2004). According to previous work (Gao 2001), da3 is a basic verb in the large domain of physical action verbs in Chinese, as 1) it refers to the most basic action of the hand; and 2) at the same time it can refer to a wide-range of actions or events that involve physical contact of one kind or another. We will compare her analysis with the analysis we provide based on SUMO. First, we collect our data from Sinica Corpus and check their senses from Chinese Wordnet. Next, we take these physical event senses of da3 into SUMO concept system (Huang et al. 2004) to find all possible concepts and distinguish them into different categories. Finally, we analyzed these concepts for semantic features which can help us to compare our analysis with the analysis in Gao’s study (2001). 2. Previous research Regarding verb studies, previous research has focused on VV compound verbs in Modern Chinese (Hong and Huang, 2004), or on near synonyms in Modern Chinese (Chief et al, 2000; Huang et al. 2000; Liu 2002; Tsai, 2002; Huang and Hong, 2005). Also, some scholars have worked on da3 polysemy analyses. Da3 is one if the most frequently used verbs, being ranked 16 in the list of most frequently used verbs in Chinese (Bei and Zhang, 1988). Specifically, Gao * Copyright 2007 by Hong, Jia-Fei, Chu-Ren Huang , Kathleen Ahrens 155  (2001) explored the semantic properties of da3 and its prototypical meaning and categorized its semantic representations to show the systematic patterning of its meaning extensions.  3. Motivation and Goals  Language knowledge representation is a manifestation of the systematic contrasts found in  human communication, which defies conventional description. Take modal verbs as examples.  Modal verbs have similar semantic functions and cannot be easily distinguished in terms of their  lexical senses. Therefore, they are considered to be interchangeable. Nevertheless, it is not  uncommon that this kind of polysemy always has contrasts in usage, as we can see the contrast  between hui4 (know) and hui4 (can) below:  (1a) 臺灣廠商到德國開商展時，非常需要會德語和中文的人，做為溝通橋樑。  Tai2 wan1 chang3 shang1 dao4 de2 guo2 kai1 shang1 zhan3 shi2, fei1 chang2  Taiwan factory to Germany exhibit  time, so  xu1 yao4 hui4/ * neng2 de2 yu3 han4 zhong1 wen2 de5 ren2, zuo4 wei2 need could German and Chinese MOD persons, to  gou1 tong1 qiao2 liang1. communicate bridge  “When Taiwan factories exhibit in Germany, they so need some persons who could speak German and Chinese to communicate with other persons.”  (1b) 在上課的時候，他不會講德語或中文和學生溝通，因為他怕自己沒辦法完整表達意 思。 Zai4 shang4 ke4 de5 shi2 hou4, ta1 bu2 hui4 jiang3 de2 yu3 huo4 In class MOD time, he will not speak German or  zhong1 wen2 han4 xue2 sheng1 gou1 tong1, yin1 wei4 ta1 pa4 zi4ji3 Chinese with student communicate, because he worry himself  mei2 ban4 fa3 wan2 zheng3 biao3 da2 yi4 si1. no way complete express meaning.  “In the class time, he can’t communicate with his students in German or Chinese, because he worries that he can’t completely express his meaning.”  Considering the lexical sense of modal verb polysemy and its natural language use, hui4 means both “know” or “can”. As defined, we notice that they differ from each other, even though they share similar concept. This paper will investigate the lexical semantic relations between each sense of da3 polysemy (excluding metaphorical senses) by studying their sense distinction, word formation collocation, and distribution pattern.  4. SUMO In this study, we use Suggested Upper Merged Ontology (SUMO) to analyze all concepts for da3. We find out all possible concepts and divide different them into different categories. WordNet is inspired by current psycholinguistic and computational theories of human lexical memory (Fellbaum (1998), Miller et al. (1993)). English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept.  156  Different semantic relations link the synonym sets (synsets). The version of WordNet that Sinica BOW implemented is version 1.6, with nearly 100,000 synsets. In Sinica BOW, ach English synset was given up to 3 most appropriate Chinese translation equivalents. In cases where the translation pairs are not synonyms, their semantic relations are marked (Huang et al. 2003). The bilingual WordNet is further linked to the SUMO ontology. We use the semantic relations in bilingual resource to expand and predict domain classification when it cannot be judged directly from a lexical lemma. 5. Data collection From Sinica Corpus and Gigaword Corpus, we find out several patterns for da3. According to the Chinese Wordnet Group analysis (Huang et al., 2003), there are 114 senses which include physical activity senses, metaphor, metonymy and extension senses. In this study, we want to focus on physical activity senses, but not metaphor, metonymy and extension senses. Da3 has 35 physical event senses listed, along with 79 additional senses. We will take focus on the physical activity senses. 6. Data analysis The analysis, based on the Sinica Corpus, Gigaword Corpus and the criteria proposed by Huang et al. (2003) to differentiate the lexical meaning, presents several different senses for da3. Table 1: The analysis of da3 from Chinese Wornet Group 157  Among these 35 physical activity senses of da3, we divide two main physical event senses for da3: 1) hit and 2) pump such as below: (2) a.母親忽然沉下臉<打>他一下手背，並告誡他不能指月亮娘娘，會爛耳朵的。 Mu3 qin1 hu1 ran2 chen2 xia4 lian3 da3 ta1 yi2 xia4 shou3 bei4, Mother suddenly sink down face hit he one time hand, bing4 gao4 jie4 ta1 bu4 neng2 zhi3 yue4 liang4 niang2 niang5, and warn he can’t point moon queen , 
(2) I thak bo chhe. he study not book ‘He can not study well.’ The main goal of this paper is to argue that the different interpretations of VbN construction in Taiwanese Southern Min may be due to different structural positions which the post-verbal negative marker bo occupies on the ground of Zanuttini’s (1997) proposal that argues for there to be different structural positions for two kinds of post-verbal negative markers, namely presuppositional versus non-presuppositional, as stated in (3). * This article began as a term paper for my first-year syntax seminar course. I am grateful to C.-S. Luther Liu for his comments of that paper. I am also appreciative of P.-Y. Katherine Hsiao’s discussions with me. The author is responsible for all the mistakes in the article and understands further modifications are required in the future. Copyright 2007 by Hui-yu Huang 163  (3) a. Presuppositional negative markers, which negate a proposition that is assumed in the discourse. b. Non-presuppositional negative markers, which negate a proposition that does not have a special discourse status. (Zanuttini 1997: 99) More precisely, bo in the episode reading context corresponds to the presuppositional negative marker whereas bo in the generic reading context corresponds to the non-presuppositional negative marker. The remaining sections of this article is organized as follows. Section 2 is a summary of Zanuttini’s (1997) analysis of post-verbal negative markers. Section 3 shows an overview of VbN construction and provides an analysis of the distinction between an episode and a generic reading. Section 4 briefly reviews previous study on the post-verbal negative bo. Section 5 concludes this article.  2. Framework: Zanuttini's (1997) Analysis of Post-verbal Negative Markers Zanuttini (1997) examines several Romance varieties and offers a systematic investigation of negative markers. She argues that there are two kinds of post-verbal negative markers. Based on their contributions to the meaning of the clause, post-verbal negative markers are distinguished as presuppositional negative markers when they negate a proposition that is assumed in the discourse, and as non-presuppositional negative markers when they negate a proposition that does not have a prior discourse grounding. Take (4) as an example.  (4) a. Maria a mangia pa/nen la carn. (Piedmontese)  Maria s.cl eats neg the meat  ‘Maria doesn’t eat meat.’  b. Gianni a capis  pa/nen tut.  Gianne s.cl understands net everything  ‘Gianni doesn’t understand everything.’  67)  (Zanuttini 1997:  In these examples, although there is no apparent difference between the use of pa and nen, they indeed contribute different interpretations to the sentences. Pa is taken as a presuppositional negative marker since it negates a proposition assumed in the discourse, whereas nen as a nonpropositional negative marker since it does not. Her syntactic analysis of post-verbal negative markers is essentially based on two findings of Cinque's work1. First, the relative ordering of adverbs in the clause is fixed in the structure. Second, for each adverb, there is one head position to its immediate right and one head position to its immediate left. Regarding the adverbs she mainly considers the ones which occur in lower positions (compared with those appear in a higher portion in the clausal structure) such as ‘already’, ‘no more’ and ‘always’ since they are the crucial ones which help determine the distribution of postverbal negative markers. She further proposes that the negative marker occurs in the specifier of a projection labeled NegP2. Assuming this, the NegP-1 is labelled for the projection headed by the pre-verbal negative marker and the NegPs such as NegP-2, NegP-3, NegP-4 are required for the projections headed by post-verbal negative markers. The relative order of post-verbal negative 
173  assimilating the two different domains, conceptual metaphor specifies the concrete idea into abstract entity. In general, conceptual metaphor is the surface structures which make metaphors understandable. (Lakoff and Johnnson, 1980; Goddard, 1998; McGlone, 2007; Charteris-Black & Ennis, 2001). As Lakoff (1993:228) claims, “We do not have detectors for time. Thus, it makes good biological sense that time should be understood in terms of things and motion.” That is, the comprehension of the abstract time understood via space is biologically determined. The two space → time metaphors under examination are time-moving and ego-moving metaphors. Based on the study of Gentner, Imai, and Boroditsky (2002) in which English native speakers conceptualize ego-moving metaphor faster, two research goals are proposed: Chinese native speakers and EFL learners process ego-moving metaphors better. In order to answer the two research questions, this paper is organized as follows, (1) Introduction, (2) the theoretical framework on temporal metaphors, (3) the methodology, (4) results, (5) discussion, and (6) conclusions. 2. Literature review This study examines how English-Chinese bilinguals in Taiwan structure temporal metaphors. The introduction of the two time metaphors, the different perspective that Chinese and English speakers hold, and the study conducted by Gentner, Imai, and Boroditsky (2002) are covered. 2.1. Sequencing time domain The two space → time metaphoric systems are ego-moving and time-moving metaphor. The primary difference is that they posit different assignments of front and back in a time line. Time-moving metaphor Time-moving metaphors identify the events temporally ordered with another in the time line. In time-moving metaphors, time can be conceived of as preceding and following one another in which time flows from the future via the ego, the point of reference, to the past (Li, 2005; Ahrens and Huang, 2002 ). In this metaphor, the future is in the back and the past is in the front (Gentner, Imai, and Boroditsky, 2002: 539). For example, ‘The final exam is before Thursday’ in which ‘before’, a space term, indicates ‘the final exam’ is proceeding ‘Thursday’. Therefore, the final exam is in the relative the past and Thursday in the relative future. (see Figure 1.) 174  Ex. The final exam is before Thursday. Past _______________________________________________ Future the final exam Thursday Figure 1. An example of time-moving metaphor Ego-moving metaphor Ego-moving metaphor recognizes the event in the time order with the ego/observer. It attributes motion over a landscape to an entity. Li (2005: 16-17) proposes that “the observer comes from the past and moves via the present to into the future, while time as the reference ground remains stationary.” Indicated by this metaphor, front is assigned to the future and back to the past (Gentner, Imai, and Boroditsky, 2002: 539). For instance, ‘The final exam is before us’ in which the space “before” specifies the linear time relationship of “us” as the present time and “the final exam” as the future event. (see Figure 2) Ex. The final exam is before us. Past _______________________________________________ Future Now the final exam Figure 2. Ego-moving metaphor 2.2.Cultural difference regarding the orientation of the ego Culture influences people’s understanding about the world, as Kövecses (2006: 135) indicates, ‘our understandings are mental representations structured by cultural models or frames.’ In English, the ego always takes a front-to-the-future orientation. However, in Chinese, ego has dual orientations: a front-to-the-future orientation and a front-to-the-past orientation, while the latter is predominant in Chinese. (Li, 2005: 40). For instance, ‘The best is before you.’ means that the best is waiting in the ‘future.’ However, for the Chinese speakers, qian suo wei jian 前 所未見 ‘it has never been seen before’ refers to the event that has never been seen in the past. 2.3.Gentner, Imai, and Boroditsky’s (2002) study on the temporal metaphors Their research on this topic provides the present study with a theoretical basis. The three experiments conducted show that the English native speakers, apart from relying on an ego-moving framework to interpret time, conceptualize ego-moving metaphor faster than time-moving metaphor. Inspired by their research, the present study aims to reexamine whether it is shared by Chinese native speakers and the EFL learners. 175  3. Methodology The present study is conducted to explore how people in Taiwan, who have Chinese as the L1 and English as their foreign language, process time-moving and ego-moving metaphors.  Participants & Materials The participants are twenty-five English and Chinese bilinguals who are female aged at 31.7. They are chosen, for they have no problem conceptualizing English and Chinese metaphors. Thirty-two test sentences are designed to examine the participants’ accuracy. Sixteen of them are in Chinese in which nine used the time-moving metaphors and the others used ego-moving metaphors. As for the other sixteen, they are mostly taken from the study of Gentner, Imai, and Boroditsky (2002) in which eight used the time-moving metaphors and the others used ego-moving metaphors. For example, Christmas is six days ahead of New Year’s Day.  Procedures After the participants read the sample in Chinese and English, they are tested by Chinese test sentences and followed by the English sentences. They see each sentence one at a time by indicating the event ‘I will see you’ happened in the past or future relative to the reference (4 o’clock). (see Figure 3.) Totally, there are thirty-two such blocks. The arrangement of all the testing sentences is randomized, so the subjects will not notice the two metaphorical types.  I will see you before 4 o’clock.  Past ____________________________________________________ 4 o’clock Figure 3. A sample of the English testing sentence.  Future  4. Results This section is divided into two parts to examine whether the faster reaction to ego-moving metaphors is a shared value for both native speakers and EFL learners. 4.1.Chinese version The results are summarized in the following figures to verify whether ego-moving metaphors in Chinese are better processed by its native speakers.  176  The distribution of the participants’ accuracy in the two metaphors in Chinese The following two figures show the distributions of the participants in Chinese time-moving metaphors and Chinese ego-moving metaphors respectively.  The number of the participants (total=25)  20 19  Chinese time-moving metaphor  15  10  5  2  3  
speaker’s intension to have an action fulfilled in imperative sentences. ma2 ma2, though still controversial, is mostly analyzed as a yes-no question particle. ou ou implies a friendly warning showing concern and caring on the part of the speaker. (1) a. hua hong le. flower red SFP ‘The flower becomes red.’ * Copyright 2007 by Xiao-You Kevin Huang 182  b. ta zhidao zhe jian shi  le.  he know this Cl. incident SFP  ‘He knows the incident (now).’  (2) a. ta you san bu che ne!  he have three Cl. car SFP  ‘He has three cars!’  b. wo xihuan zhe bu dianying, ni ne?  I like this Cl. movie you SFP  ‘I like this movie, how about you?’  (3) a. zhe fu hua bucuo ba./?  this Cl. panting good SFP  ‘This panting is good. / (right)?’  b. haohao nianshu ba!  hard study SFP  ‘Study hard, (okay)?’  c. ni hui kaiche ba?  You can drive SFP  ‘You can drive, can’t you?’  (4) a. guolai a!  come SFP  ‘Come.’  b. shei a?  who SFP  ‘Who?’  (5) a. wo shuo jintian shi zhouri ma1.  I say today is Sunday SFP  ‘I said today is Sunday!  b. zai he yi bei ma1!  more drink one glass SFP  ‘Have one more glass (of wine)!’  (6) ni shi xuesheng ma2?  
A typical natural language interface has to solve two main tasks: (i) translating a natural language query to a query language; and (ii) generating a natural language answer by using information from a query result table. In this paper, we focus on solving the second task. A frame-based approach, which bases on predefined elementary frames and text generation rules to generate new frames and to produce flexible text, is introduced. The implemented system is called Text Generator or TGEN for short. The remaining sections of this paper are organized as follows: Section 2 introduces the architecture of our proposed Natural Language Interface for Querying Database and Automatically Generating Reports (NLI4DB), in order to get an overview of the TGEN role in the interface. Section 3 describes the rule set used by the TGEN to generate text. Section 4 * The author gratefully acknowledges the receipt of a grant from the Flemish Interuniversity Council for University Development Cooperation (VLIR UOS) which enabled the research team to carry out this work. Copyright 2007 by Huong Thanh Le 192  presents the major components and the data flow in the TGEN. In Section 5, two examples are given to illustrate the working process of the TGEN. Our implementation discussion and some experimental results are given in Section 6. Finally, Section 7 concludes the paper and proposes possible future work on this approach. 2. The NLI4DB Architecture The NLI4DB is a system that is integrated with a traditional database management system to provide a natural language interface for querying database and automatically generating answers. An overview of the NLI4DB architecture is shown in Figure 1.  Natural Language Query  Outputs  Natural Language Interface  Query Translator SQL query  Knowledge Sources  Text Generator Result Table  A Database Management System Figure 1: Architecture of the NLI4DB The NLI4DB consists of two main modules: - A Query Translator (QTRAN) to translate natural language questions to SQL queries. - A Text Generator (TGEN) to generate query responses under the form of short answers or summary reports in Vietnamese language. A natural language question composed by a user is translated into an SQL query by the Query Translator. After that, the Database Management System processes the SQL query and returns the query result in the form of a table. The Text Generator then transforms this result table into a textual answer. To test the feasibility of the NLI4DB, a specific Database Management System - a student management database – is used. The entity relationship of the database is shown in Figure 2. Knowledge sources (e.g., syntactic rules and thesaurus) are needed in the working processes of the QTRAN and the TGEN. The remaining sections present our main focus of this paper - the implementation of the TGEN. The rule set used in generating answers is introduced first. 3. The Grammar used in the TGEN The TGEN does not generate free texts, but the texts that are based on predefined frames. These frames are typical structures of answers. For example, the frame for an answer of the List type is:  193  Figure 2 – The entity relationship of the student management database [Noun phrase] [Verb phrase]: 1. [Item_1] 2. . . . . . . 3. [Item_n] A List answer can also be represented by another frame: [Noun phrase] [Verb phrase] [Item_1], …, [Item_n]. Each label [. . .] in the frame is a slot that needs to be filled. The [Noun phrase] and the [Verb phrase] are generated by using the syntactic structure of the user’s question. This problem is analyzed in detail in Section 5. The slots [Item_1], …, [Item_n] are filled in by values from the result table. In order to create the frame set that is used in generating text, we first identify and categorize question types, then we define frames for each question type. The question types that have been considered by us are: 1. Questions that return a single value (e.g., Who is the leader of the class BK20 in the academic year 2004-2005?1). This question type is called a Single_value question. 2. List questions (e.g., Which subjects did the class BK20 study in Semester 1 last year?) 3. Statistical questions (e.g., Show us the quality of students in the academic year 2005 – 2006.) 4. Comparison questions (e.g., Compare the percentage of excellent students of the classes BK20 and BK21.) 5. Description questions (e.g., Give us information about the student Pham Thanh of the class BK20.) 6. Evaluation questions (e.g., Evaluate the study progress of the student Nguyen Van Minh.) The name of a frame is called by its corresponding question type. For example, the frame of a List question is a List frame. 
(1) Compound verbs in Korean: a. VV type: ttwi-nolta ‘to run and play’ b. V-e-V type: ttut-e-nayta ‘to rip off’ c. V-ko-V type: mil-ko-tangkita ‘to push and pull’ d. V-eta-V type: tol-ata-pota ‘to look back’ Among these four types, the V-e-V type shows certain degree of productivity depending upon kinds of preceding and following verbs, as shown in (2). * An earlier version of this paper was presented at Harvard International Symposium on Korean Linguistics 2007. The second stage of Brain Korea 21 project provided me with an ideal atmosphere to work on my research. This work was supported in part by Hankuk University of Foreign Studies Research Fund of 2007. Copyright 2007 by Jong Sup Jun 202  (2) Elements of V-e-V compounds (K-H Kim 1996: 3): a. Preceding verbs: kal- ‘to grind’, kennu- ‘to cross over’, ket- ‘to walk’, kkul- ‘to drag’, kwulu-‘to roll’, nal- ‘to fly’, nayli- ‘to go down’, noh- ‘to put down’, tul- ‘to enter’, olu- ‘to go up’, ttalu- ‘to follow’, etc. b. Following verbs: kata ‘to go’, nohta ‘to put down’, mekta ‘to eat’, pota ‘to try’, oluta ‘to go up’, ota ‘to come’, cwuta ‘to give’, etc. Among various verbs in (2), kata ‘to go’as the second verb draws our attention in terms of productivity. Some possible compounds with kata are listed in (3). (3) V-e-kata (K-H Kim 1996: 3): kacyekata ‘to take (something) and go’, kechyekata ‘to pass (through)’, kwulekata ‘to go by rolling’, kkwulyekata ‘to be dragged to’, nakata ‘to go out’, nalakata ‘to fly’, naylyekata ‘to go down’, nemekata ‘to go/jump over’, takakata ‘to approach’, ttalakata ‘to follow’, ttekata ‘to scoop up and go’, ttwiekata ‘to run and go’, molyekata ‘to go in a group’, molakata ‘to drive (something/somebody) into some space or situation’, ahpsekata ‘to go forward, to take precedence’, olakata ‘to go up’, capakata ‘to catch somebody and go’, caphyekata ‘to be taken to some place after being caught’, ccochakata ‘to follow’, chacakata ‘to go by searching’, etc. These verbs are basic in form, and are very productive compared with other V-e-V compounds. Furthermore, these verbs are particularly important from the perspective of lexical semantics, in that they all have the meaning component of MOVE or GO which is the foundation of all motion events in Talmy’s (2000) cognitive theory of lexical semantics. This paper aims to answer interesting questions about V-e-kata compounds under the framework of cognitive semantics. Is a V-e-kata compound a syntactic phrase or a lexical item? Does it show any properties of a phrase? Does it show any properties of a lexical item? How can we understand V-e-kata compounds from a general perspective of grammar? To answer these qeustions, I present syntactic and lexical properties of V-e-kata compounds in section 2, problems for C-H Lee’s (2006) categorial conversion analysis in section 3, and then my alternative proposal based on Talmy’s (2000) theory of lexicalization and Goldberg’s (1995) construction grammar in section 4. Section 5 is the conclusion of this paper, and suggests a way of applying the proposed analyses to ontology-building. 2. Syntactic and Lexical Properties of V-e-kata Compounds V-e-kata compounds have dual faces. They show properties of both syntactic phrases and lexical items. A well-known syntactic property of V-e-kata compounds is the intervention of other morpho-syntactic elements. That is, such elements as a topic marker, delimiters like –man ‘only’ and –to ‘also’, plural/manner markers, and even a case marker can intervene between the V-e and -kata (K-H Kim 1996, C-S Suh 1996, C-H Lee 2006). (4) a. salamtul-i kicha-eyse naylyeka-ss-ta people-NOM train-from take.off-Pst-Dec ‘People took off the train.’ b. salamtul-i kicha-eyse naylye-nun-ka-ss-ta TOP c. salamtul-i kicha-eyse naylye-man-ka-ss-ta only d. salamtul-i kicha-eyse naylye-to-ka-ss-ta also e. salamtul-i kicha-eyse naylye-tul-ka-ss-ta 203  Plural f. salamtul-i kicha-eyse naylye-se-ka-ss-ta Manner g. salamtul-i kicha-eyse naylye-lul-ka-ss-ta ACC  Interestingly,–se intervention as shown in (4f) is not always possible. Compare the data in (4) with (5).  (5) a. olakata ‘to go up’:  ola-nun-ka-ss-ta, ola-man-ka-ss-ta, ola-to-ka-ss-ta, ?ola-se-ka-ss-ta, ola-lul-ka-ss-ta  b. tulekata ‘to enter’:  tule-nun-ka-ss-ta, tule-man-ka-ss-ta, tule-to-ka-ss-ta, *tule-se-ka-ss-ta, tule-lul-ka-ss-ta  c. kacyekata ‘to take (something) and go’:  kacye-nun-ka-ss-ta, kacye-man-ka-ss-ta, kacye-to-ka-ss-ta, *kacye-se-ka-ss-ta, kacye-  lul-  ka-ss-ta  Noticing the difference between (4) and (5), K-H Kim (1996) claims that V-e-kata compounds that allow the intervention of –se are not (real) lexical compounds. To him, the possibility of – se intervention is a good test to show whether a V-e-kata compound is a syntactic phrase or a word. Kim’s analysis, however, cannot explain that the intervention of –nun, -man, -to, -lul is generally allowed for all V-e-kata compounds whether they allow –se intervention or not. A second syntactic property of V-e-kata compounds is the –ki repetition construction (cf. C-H Lee 2006: 134).  
Linguistics and Cognitive Science Department at HUFS, October 12. I thank the audiences for comments and questions. I also thank two anonymous reviewers of this paper. All errors and misinterpretations are of course mine. Copyright 2007 by Ilkyu Kim 210  meaning (e.g. Hong, K-S 1991, Kim, H-S 1989, Kim, K-H 2003, Kim, Y-J 1990, Kim, S-J 1994, Lee & Lee 2005, Nam, S-H 2007, Yeon, J-H 1996). With most semanticists agreeing with the idea that the meanings of the two constructions are certainly different from each other, what is at issue is how to capture the difference and similarity between the two. Regarding this issue, one of the most controversial and difficult problems is whether the psych-construction has Action (Agency) as part of its meaning (Kim S-J 1994). The purpose of this paper is to solve this problem by answering the question why psychconstructions are much more natural when they are used as negative imperative than when they are used as positive imperative. In answering this question, first, we show that –e ha- adds the meaning of non-volitional action to psych-adjectives, thus making the whole construction a kind of action. This characteristic of the psych-construction and its difference from psych-adjectives are captured by Jackendoff's (1990, 2002a, 2002b, 2007) Conceptual Semantics, particularly its mechanism of distinguishing thematic-tier and macrorole tier. Then, we show, with Talmy’s (1985, 2003) Force Dynamics theory, in the negative imperative of the psych-construction, what the speaker requires from the hearer is internal volitional action. The content of the rest of the paper is like the following. In section 2, we will first introduce the concept of Action and Actor on which our analysis is based. Then, in section 3, we will analyze the meaning of psych-constructions, particularly the values they take for the features [VOLITION] and [ACTION], in order to answer our first question: why is positive imperative for psych-constructions impossible or unnatural at least? In doing so, we will focus on the difference between the conceptual structure of psych-constructions and that of psych-adjectives, working under the framework of Jackendoff's (1990, 2002a, 2002b, 2007) Conceptual Semantics. After that, in section 4, we will answer the second question which is, we believe, much more interesting: why is negative imperative for psych-constructions so natural? We will analyze negative imperative of psych-constructions with Talmy's (1985, 2003) force dynamics theory, showing that it can naturally account for why negative but not positive imperative is possible, or at least much more natural, for psych-constructions. In section 5, we support our argument by expanding the scope of predicates that go naturally along with only negative imperative and figuring out their semantic similarity with respect to the features [VOLITION] and [ACTION]. Finally, our conclusion will be given in section 6. 2. Action, Actor and Macrorole Tier Following Culicover & Wilkins (1986) and Talmy (1985), Jackendoff (1990:128) argues that “conceptual roles fall into two tiers: a thematic tier dealing with motion and location, and an action tier dealing with Actor-Patient relations.”2 In addition, he adds two more conceptual 
1. Introduction  Bound nouns (BN) exhibit various peculiar properties, not found in common nouns in the language. For example, unlike canonical nouns, bound nouns cannot occur independently: they obligatory select a complement (determiner or sentence). This is rather unusual when considering the language allows most of the arguments to be freely omitted with proper context:  (1) a. *(i) kes this thing  b. *(wuli-ka motu nollass-ten) kes we-NOM all surprise-MOD BN ‘the thing that we all surprised’  
(1) a. That John will help us is hard for us to rely on ___. b. That Chomsky might be wrong is hard to think of _____. Prepositions like on or of would normally be subcategorized to take a nominal complement, but in (1) the filler site for the corresponding gap is occupied by a that clause, which would pose a puzzle to a constraint-based analysis as well as to a minimalist approach. This kind of mismatch is not peculiar to the missing object construction (MOC, hereafter). The same kind of mismatch can also be found in a topicalized construction as shown in (2). (2) a. That some passives lack active counterparts, no theory can capture ____. b. *No theory can capture that some passives lack active counterparts. c. That some passives lack active counterparts, few teachers were aware of ___. The verb capture does not take as its complement a that clause in an ordinary construction as shown in (2b), but in a topicalized construction like (2a), a that clause can initiate the construction with the same verb governing the gap site. This kind of weak connectivity is not restricted to the above two constructions. It can be found in passives as well. (3) a. That some passives lack active counterparts can be captured by no theory. b. That Chomsky might be wrong has never been thought of ___. ∗ Copyright 2007 by Yong-Beom Kim 234  cf. No one has ever thought (*of) that Chomsky might be wrong. c. That John will help us may not be relied on.  As shown in (3a) and (3b), a that clause can appear as the subject in these passives, but their active counterparts cannot take the clause-type constituent as their complement. There is also a less deviant and less pernicious kind of problem, as in the ordinary missing object construction, as shown in (4)  (4) a. He/*Him is easy to please ___. b. They/*Them will take only five minutes to boil ____.  There arises a case clash between the subject and the corresponding gap in (4) but this issue has been more or less elegantly handled in the framework of HPSG. It is taken care of by requiring not the total identity but a partial identity of feature specifications. However, as we will see in section 2.1, such an account is not still satisfactory. This paper will argue that the filler-gap relations in these constructions can in general be semantically accounted for, since the categorical mismatch is sometimes inevitable in syntax and should be allowed so far as other components of grammar require such mismatch.  2. Proposals In this section we will claim that the filler-gap dependency cannot be a relation of total identity between the two elements but one of partial overlap, considering that the features may differ where other grammatical requirements or other structural forces dictate them to be nonidentical. One of the proposals in this line of conception has already been presented in PSG framework (See Hukari and Levine (1991)). We will consider missing object constructions, first.  2.1 Missing Object Constructions (MOCs) Feature mismatches in MOCs can be plainly seen in the examples in (1) through (4) where the ‘displaced’ main clause subject should be somehow related to the object position of the embedded clause. This construction is relatively easy to deal with since there is a licensing lexical item and its related lexical rule can mediate the feature mismatch. For instance, Hukari and Levine (1991) formulated a rule that ignores [CASE] feature in this construction, and Levine and Hukari (2006, p 355) also suggested a rule as shown in (5)  (5) partial representation of SYNSEM value for tough lexical rule  LOCAL | CAT  HEAD adjective SUBJ <[1]i >, COMPS < …, VP INHER|SLASH {[1]NPi } > TO-BIND|SLASH {[1]i }  This rule may take care of case clashes between the filler and the gap, since its identity requirement is limited to INDEX value. However, this rule wrongly demands that the filler and the gap should be simultaneously an NP. So the syntactic category mismatch shown in (1a) and (1b) is not taken care of by Levine and Hukari. This is because the feature specification in the tough lexical rule explicitly mentions NP as the SLASH value. To correct this problem, we may loosen the lexical rule to such a degree that the filler and the gap may be minimally different from each other, and thus the SLASH value could be simply a phrase (i.e., XP) instead of an NP. The possible revised lexical rule would contain (6) as the VP specification as the value of COMPS.  235  (6) VP INHER|SLASH { [1]i } TO-BIND|SLASH {[1]i } With this revision, we could displace any type of category from the complement position in the MOC, and the sentences like (1a) and (1b) may be licensed by the grammar regardless of whether the preposition licensing the gap in the infinitival phrase can take S as its complement or not1. 2.2 Passives The types of passive sentences shown in (3) are not accounted for within the current constraint-based phrase structure grammar. The problems with these examples are twofold: one involves a reanalysis assumption; the other relates to category mismatch as in MOCs. These examples show that passives are not simply a process of demoting and promoting some arguments. If the passive sentences like (3b) and (3c) are to be generated, there should be a process that concatenates relied and on, in addition to the ordinary passive lexical rule since rely on or think of cannot be an input the Passive Lexical Rule (PLR) of Sag and Wasow (1999, p235) as it is. Even though the verb-preposition sequence can somehow be fed into the Passive Lexical Rule, the least oblique complement cannot be S-bar or S, since it would be governed by a preposition, not by the verb in question, in the active counterpart. Furthermore, the PRL demotes the first element of ARG-STR value and promotes the least oblique argument in the remainder so that the category next to the main verb becomes the subject in ordinary cases. But this general rule would license the following ill-formed cases shown in (7a), (7b) and (7c). Furthermore, the well-formed ones in (7a’), (7b’), and (7c’) are not generated by the PLR. (7) a. *That John is a liar is not thought. a’. It is not thought that John is a liar b. *That John is a liar is said. b’. It is said that John is a liar. c. ?*That John is a liar has never been thought c’. That John is a liar has never been thought of. I assume there is a slight difference in what are presupposed by (7a’) and (7c’): (7a’) does not carry as much factive presupposition as (7c’) does. In order to take care of this situation, some parochial passive rules need to be posited. What is clear is that the Sag and Wasow’s (1999) PLR cannot deal with the examples shown above and that it has to be flanked with subsidiary rules. One instance of such rules that can deal with (7c’) would look like the one proposed in (8) and this would take care of the two problems: need for reanalysis and categorical mismatch. In fact, the rule proposed by Sag and Wasow does not take care of the examples in (7a’) and (7b’). This cannot be dealt with by a kind of extraposition operation, either, since the (7a) and (7b) are not well-formed sentences. Therefore, the passive forms without prepositions, like thought and said are to be dealt with separately from the ones with prepositions. 
a Department of Linguistics and Cognitive Science Hankuk University of Foreign Studies San89 Wansanri Mohyunmeon Yonginsi, Kyunggido Korea {dsk202, rayr}@hufs.ac.kr  Abstract. In this paper, we propose a Transformation-Based Learning (TBL) method on generating the Korean standard pronunciation. Previous studies on the phonological processing have been focused on the phonological rule applications and the finite state automata (Johnson 1984; Kaplan and Kay 1994; Koskenniemi 1983; Bird 1995). In case of Korean computational phonology, some former researches have approached the phonological rule based pronunciation generation system (Lee et al. 2005; Lee 1998). This study suggests a corpus-based and data-oriented rule learning method on generating Korean standard pronunciation. In order to substituting rule-based generation with corpusbased one, an aligned corpus between an input and its pronunciation counterpart has been devised. We conducted an experiment on generating the standard pronunciation with the TBL algorithm, based on this aligned corpus. Keywords: Transformation-Based Learning, Computational Phonology, Data-oriented Processing, Corpus-based Learning, Pronunciation Generation 1. Introduction This paper presents a Transformation-Based Learning (TBL) method on generating Korean standard pronunciation. Previous studies on the phonological processing have been focused on the computation of the phonological rule application and the representation of the finite state automata (Johnson 1984; Kaplan and Kay 1994; Koskenniemi 1983; Bird 1995). In case of Korean computational phonology, some former researches have approached the pronunciation generation based on the phonological rules (Lee et al. 2005; Lee 1998)1. Unlike previous works, this study suggests a standard Korean pronunciation generation method on the basis of corpusbased and data-oriented TBL learning. The role of the computational phonology is to generate a legitimate output counterpart of the underlying phonological input. Phonological rules are involved in the process of phonological generation. The SPE style operations on the computational phonology have used the rewriting rule ordering or the finite state transducer (Bird 1995; Bird and Ellison 1994; Gildea and Jurafsky 1996; Kaplan and Kay 1994). Those approaches, however, should reduce complicated * This paper was supported by the Second Brain Korea 21. Copyright 2007 by Kim Dong-Sung and Chang-Hwa Roh 
Incremental processing of a sentence as it is inputted from left to right has been taken as most accurately simulating human sentence processing. When we attempt theoretically to account for this incrementality of sentence processing, however, we notice that there is a difference between head-initial languages and head-ﬁnal languages. What we need to take into account is that in headinitial languages, like English, parsers can look ahead to syntactic structures to be established to some extent as a word is consumed. By contrast, this is not the case in head-ﬁnal languages such as Japanese, because in these languages the syntactic positions of NPs remain unﬁxed until the matrix verb is inputted ﬁnally in the sentence. The Dynamic Syntax approach (Kempson, Meyer-Viol, and Gabbay, 2001; Cann, Kempson, and Marten, 2005), which allows the parser to process a sentence in an incremental fashion, has settled this difﬁculty by adopting structural underspeciﬁcation. However, Kempson, Meyer-Viol, and Gabbay (2001) and Cann, Kempson, and Marten (2005) do not provide an explicit algorithm for implementation of the framework as formally as Moot (1999) presents Grail, the toolkit for a parser of Categorial Grammar Logics. This paper will delineate the basic idea of a parser for Japanese based on the Dynamic Syntax (hereafter, DS) framework, providing the algorithm of the application of lexical rules and transition ∗We would like to thank two anonymous reviewers for valuable comments. 0 Copyright 2007 by Masahiro Kobayashi and Kei Yoshimoto  rules. This paper will also show that the parser is able to cope with null arguments or empty pronouns in embedded clauses. Currently the parser is implemented in Prolog. The outline of this paper is as follows: the subsequent section will provide a brief introduction to the DS formalism and address issues of parsing inefﬁciency and the algorithm of rule application, discussing the previous studies. Section 3 will illustrate the implementation of the DS formalism and our algorithm to process head-ﬁnal languages like Japanese. Section 4 shows how the parser deals with the complex sentences with empty pronouns. Section 5 will be devoted to the discussion. Section 6 will be a conclusion. 2. Dynamic Syntax and Issues of Implementation 2.1. Formalization and Unﬁxed Nodes This subsection presents a brief illustration of the DS formalism. DS is a grammar formalism which allows a parser to process a sentence from the onset word to the ﬁnal word in an incremental way. The (partial) tree structure grows larger and larger, step by step, by the application of transition rules as well as the word consumption; the initial tree structure T0 shifts to a subsequent structure T1, and ultimately to the ﬁnal tree structure Tn as seen in (1). (1) T0 =⇒ rule application =⇒ T1 =⇒ · · · · · · =⇒ Tn−1 =⇒ rule application =⇒ Tn The tree structure T is a set of nodes in the DS formalism, and the initial tree structure consists of a single node {T n(a), ?T y(t), 3} (Kempson, Meyer-Viol, and Gabbay, 2001, pp.57) called the root node, where T n is a predicate which expresses an address of a node which includes the variable a indicating that a position of this node has not been speciﬁed yet. Moreover, the predicate T n expresses the relationship between nodes; for instance, T n(01) is the functor node of T n(0), T n(00) is the argument node of T n(0). T y(t) is a predicate which expresses the type of the node. The question mark ‘?’ preﬁxed to T y(t) is called a requirement, which expresses that T y(t) needs to be satisﬁed by the end of processing: in this initial state, the goal of processing is to meet the condition that this node is of type t. In this sense, the parsing formalism of DS is called goal-directed. The pointer ‘3’ is used to highlight a node and the lexical rules and transition rules are applied to only pointed nodes. The lexical items themselves take the form of rules such as IF A holds THEN execute B ELSE execute C. These lexical rules as well as transition rules update the current tree structure to the subsequent structure. We assume the following transition rules in this paper for Japanese; LOCAL ∗ADJUNCTION, which is used to introduce an unﬁxed NP for local scrambling to the tree structure; GENERALISED ADJUNCTION for introducing the embedded clause to the tree; ∗ADJUNCTION for long-distance scrambling; ELIMINATION for the functional application; COMPLETION, used to bring the pointer back to the mother node; THINNING for deleting or satisfying the requirements; LINK ADJUNCTION for introducing the relative clauses to the tree; LINK EVALUATION for building up the semantic representation for relative clause construction; MERGE for the uniﬁcation of unﬁxed nodes with the vacuous, ﬁxed nodes. As we have accounted for, NPs in Japanese need to be kept unﬁxed until the verb has been consumed when the parser processes a sentence. In (2a) the sentence initial NP is the object of the simple main clause, whereas the ﬁrst NP of (2b) is the object of the embedded, subordinate clause. However, the parser cannot specify the ﬁxed positions of the initial NP of both (2a) and (2b) when the parser consumes them. (2) a. Bo¯ru o John ga nageta. ball ACC John NOM throw-PAST “John threw the ball.” 250  b. Bo¯ru o John ga nageta to Taro¯ ga itta. ball ACC John NOM throw-PAST COMP Taro NOM say-PAST “Taro said that John threw the ball.“ LOCAL ∗ADJUNCTION introduces an unﬁxed node which is immediately dominated by the root node. By contrast, in (2b), ﬁrst, the unﬁxed root node of type t for the embedded clause is introduced by GENERALISED ADJUNCTION, then LOCAL ∗ADJUNCTION introduces an unﬁxed node which is immediately dominated by the root node of the embedded clause. These unﬁxed nodes need to ﬁnd their ﬁxed position by the end of parsing by means of the application of MERGE. Thus, the DS formalism is non-deterministic because the processing of the same sentence initial NP may yield multiple different tree structures. For a more detailed description about the formalism, readers are referred to Kempson, Meyer-Viol, and Gabbay (2001) and Cann, Kempson, and Marten (2005). 2.2. Issues of Implementation This subsection addresses the issues of implementation of the DS framework. When we try to develop an account of linguistic phenomena with limited data, the paper-and-pencil approach is very inefﬁcient. In implementing the DS framework we have two problems to address. The ﬁrst issue is how we should handle the unﬁxed nodes efﬁciently: in the previous subsection we showed that in some case the sentence initial NP is treated as the locally unﬁxed node introduced by LOCAL ∗ADJUNCTION, whereas it is treated as the unﬁxed node dominated by the type t, an unﬁxed root node for the subordinate clause, introduced by GENERALISED ADJUNCTION. These unﬁxed nodes all need to ﬁnd their ﬁxed positions in the course of processing with the transition rule MERGE or other update processes. Originally Kempson, Meyer-Viol, and Gabbay (2001) deals with nodes in the tree structure as a set and thus it is very inefﬁcient to search the pointed node among a set each time a rule is applied, if we implement the grammar this way. Purver and Otsuka (2003) and Otsuka and Purver (2003) present a generation and parsing model for English, in which the tree structure is represented as a set of nodes in Prolog. Since in Japanese we need to keep some nodes unﬁxed, this is not so efﬁcient an approach to Japanese. Instead if we try to cope with these nodes as a normal binary tree structure, we will get into trouble. For example, in Prolog the normal binary tree structure is often implemented recursively: node(s, node(np, [], []), node(vp, [], [])) is the well-known Prolog notation for the tree diagram in which S goes to NP and VP. Unﬁxed nodes are not easy to be handled in this way. We propose a partitioned-parsing state to cope with these unﬁxed nodes and MERGE operation. The second issue this paper addresses is the algorithm of lexical rules and transition rules of DS. DS is a kind of an abstract grammar formalism and in the current framework lexical rules and transition rules are applied arbitrarily: DS does not provide any algorithm to specify which rule is applied when. When the parser processes a Japanese sentence such as simple sentences, relative clause constructions, and complex sentences, we assume that 8 rules are needed to establish the semantic representation as we explained in the previous subsection. If the rules are applied in an arbitrary way, the parser has to compute the repeated permutation of the rules, including vacuous application. This results in very inefﬁcient parsing. Although Purver and Otsuka (2003) and Otsuka and Purver (2003) propose a generation and parsing model based on the DS framework, which is written in Prolog, their parser mainly deals with only English. As we explained, in headﬁnal languages such as Japanese various unﬁxed nodes are introduced and merged in the course of parsing so that we need a different algorithm. In the subsequent section we propose the algorithm to implement a DS grammar for Japanese, a slightly modiﬁed version of Kobayashi (2007). 251  3. Implementation of DS for Japanese 3.1. Parsing State This subsection illustrates the parsing state. Our parser consists of a triple W , S, P , where W is a list of words to be consumed (words which have not been processed yet), S is a list of (partial) tree structures which the parser obtains until it processes the current word, P indicates the position at which the pointer exists right now. The initial and ﬁnal position of the pointer is the root node. The parsing successfully ends if and only if W contains no word to be scanned, and the pointer goes back to the root node of the resulting ﬁxed tree structure; P is pn(fixed, [root]), and the semantic formula is built up at the root node. The basic idea of the parsing process, from the initial to the ﬁnal stage, is shown in (3). (3) W0, S0, pn(fixed, [root]) ⇒ · · · · · · ⇒ φ, Sn, pn(fixed, [root]) In a triple W , S, P , S consists of a double R, T where R is a list of rules which has been applied to establish the current tree structure. T consists of a triple F , G, L where F is a ﬁxed tree structure, G is a tree structure which is introduced by GENERALISED ADJUNCTION rule, and L is a tree structure which is introduced by LINK ADJUNCTION. This deﬁnition of the parsing state allows the parser to have additional spaces for unﬁxed nodes and execute MERGE operation efﬁciently. 3.2. Algorithm of Rule Application This subsection describes the algorithm for processing Japanese sentence on the basis of the DS framework. We also take a brief look at the approach to English sentences presented by Purver and Otsuka (2003) and Otsuka and Purver (2003). The basic idea of Purver and Otsuka’s approach is to divide transition rules into two groups in order to improve efﬁciency. One group is always rules, which apply forcibly between the transitions, and the other is possible rules, which do not necessarily apply but can be applied. Since Japanese is a typical head-ﬁnal language, it is not easy to predict what syntactic structure comes next when the parser processes a sentence in an incremental fashion. To put it another way, the syntactic structure of Japanese is determined (or ﬁxed) by the word the parser has processed to some extent. Japanese is lexically driven in that sense. In order to process Japanese sentences efﬁciently we need to take into account when lexical rules are applied together with transition rules. Therefore, Purver and Otsuka’s approach is efﬁcient to parse English sentences, but it does not apply directly to Japanese. This subsection proposes an algorithm for Japanese, which is a slightly revised version of Kobayashi (2007). The key idea of this algorithm is that the transition rules are classiﬁed into two groups: one is tree expansion rules, which are used to expand the current tree structure, providing, for example, a new unﬁxed node; the other one is node update rules used to update the information of the pointed node and to move the pointer up to the mother node (COMPLETION). tree expansion rules includes LOCAL ∗ADJUNCTION, GENERALISED ADJUNCTION, and LINK ADJUNCTION. By contrast, MERGE, LINK EVALUATION, ELIMINATION, THINNING, and COMPLETION belong to node update rules. The algorithm is illustrated in Figure 1. In node update rules, rules are applied in the following order; MERGE, LINK EVALUATION, ELIMINATION, THINNING, and COMPLETION. This is because the parser has to establish the semantic representation with ELIMINATION and delete the requirement with THINNING at the current pointed node before the pointer goes up to the mother node (with COMPLETION). node update rules can vacuously apply, so the tree structure is passed to the next rule without any modiﬁcation if some of node update rules cannot apply. As Figure 252       start  ? Apply node update rules  as many times as possible.  NO ? Apply node update rules as many times as possible. ? Print results.  PPPPPPDaPowePosrPdthPteor?PebPreePmpaaPrisnPedP? PPPP YES    ?NO PPPPDthPoe  ePrsePtqhuePeni?rdpPemoPine?Pnte tPdoPfnotPyNdpeOPehPta?vPe PPPPPPPCtoPatnPhAaePplcpePulxyrPircaeaPnllte??PrxusYitPlcaEeagPSlbere?PualPep.pPliPedPPP   halt  YES  6  ?  Apply a tree expansion rule.  Figure 1: Flow Chart of Algorithm  253  
CR is not merely a repetition or duplicated form of a lexical element for the purpose of intensive use. Consider the examples of CR given in (1).1 (1) a. I'll make the tuna salad, and you make the SALAD-salad. b. Oh, we're not LIVING-TOGETHER-living-together. c. My car isn't MINE-mine; it's my parents'. d. I had a JOB-job once. [a 'real' 9-to-5 office job, as opposed to an academic job] ∗ Copyright 2007 by Binna Lee, Chungmin Lee 
* I would like to express my heartfelt thanks to the anonymous reviewers for comments and suggestions on the earlier abridged version of this paper. Needless to say, I am solely responsible for any infelicities. Copyright 2007 by Chiachun Lee 268  paper is organized by the following parts. Chapter 2 tackle with the function and features of the structure “(NP) + (intensifier) + gei3ta1 + adjective”, including its syntactic variations and features of the compatible adjectives, the function of gei3ta1, the reference of ta1. Chapter 3 is conclusion.  2. The structure“gei3 + ta1 + adjective” in TM  2.1 Construction meaning  Based on Goldberg (2006), grammatical constructions are conventionalized pairings of form and function. She claims that constructions bear specific function, especially idiosyncratic structure.1 While verbs are  important in semantics of a construction, construction also denotes certain kind of meaning. Moreover,  CG claims that different surface structure spells different meaning. Furthermore, CG also take  information structure and pragmatics into consideration. In the step of mapping from semantics to syntax,  not all roles in frame are necessarily profiled in surface structure. Some could be omitted if they are  recoverable from the context or they are of no importance in information.  In this study, I am going to investigate the newly developed construction in terms of CG on account  of the following reasons. First, this construction is signified by its characteristics. This structure amplifies  speaker’s emotive evaluation on undesired or unpleasant event or situation. The event or situation must  be realis. That is to say that, it must be fact. Second, the adjectives compatible with this structure must be  adjectives with higher degree. General adjectives are seldom found in this structure. There are two kinds of special structures evoking “gei3 + ta1”2 in TM. Type I, descriptive expressions, “gei3ta1 + adjective” and “gei3ta1 + verb”. The other one is resultative structure, “verb + gei3ta1 + complement”. In the present study, the focus is on “gei3ta1 + adjective”. The core part of the structure is “gei3ta1 + adjective”. By adding adverbs there are several variations3: 1. gei3ta1 + ge0 + N a. Wo3 zhan4zai4 yuan2 di4 bu4 zhi1dao4 yiao4 shuo1 sha2,  I stand at original place NOT know will speak what zhen1shi4 gei3ta1 ge0 bu4 yu3 zhi4 pien4  really GEI TA CL not give place comment  Standing at the original place without knowing what to say, I really do not intend to give any  comment. b. Wo3jüe2de2 jin1tian1zhen1de0hen3gei3ta1ge0 jin4bao4  I feel today really very GEI Ta CL surprise  Today, I really feel very much surprised.  2. gei3ta1 + ADJP a. ta1 suo3xue2de0 fan2wei2 zhen1de0 shi4 you3dian3 gei3ta1 za2  he learn field really be a little GEI TA complicated  What he learned was a little complicated.  3. fei1chang2 + gei3ta1 + ADJ a. jin3ji2 xun2zhao3…(fei1chang2 gei3ta1 jin3ji2)  urgent look for (very much GEI TA urgent  Look for something in emergency. (Extremely in emergency)  4. (zhen1de0 +) you3dian3+ gei3ta1 + ADJ  a. Tian1yu3chuan2shuo1…  zhen1de0you3dian3 gei3ta1 wu2liao2 shuo1  Tianyuchanshuo(a puppet play) really a little GEI TA boring speak  Tianyuchanshuo is really very boring.  
1. Introduction  In some languages, long-distance reflexives are allowed in addition to their sentence-bound counterparts. Among the languages that allow long-distance reflexives, some languages have blocking effects, but others don’t. Chinese is one language that has blocking effects and the sentence (1) demonstrates an example of blocking effects (Cole, et al., 2000:14).1  (1) Blocking Effect in Chinese  Zhangsani  renwei woj zhidao Wangwuk  Zhangsan  think I  know Wangwu  ‘Zhangsan thinks I know Wangwu likes self.’  xihauan like  ziji*i/*j/k self  Here, the reflexive ziji cannot refer to Zhangsan because wo blocks co-reference between ziji and Zhangsan. Let’s compare this sentence with (2). (2) is the Korean counterpart of sentence (1).2  ∗ Copyright 2007 by Yong-hun Lee  
The PWCC system uses a four-step process to fetch parallel corpora from the web. In the first step, a tool called web spider is employed to fetch all the web pages from specific hosts which * The work was finished while the first author visited Wuhan Office of Comet Electronics Hong Kong. Copyright 2007 by Bo Li, Juan Liu and Huili Zhu. 285  probably contain high-quality parallel web pages. The software WebZip1 is utilized to fetch web pages in the PWCC system. In the second step, candidate parallel web page pairs are prepared from the raw web page set based on the outer features of the web pages. The third step is the key of the whole system, in which the candidate parallel web page pairs produced by the second step are evaluated and the actually parallel pairs are saved. The evaluation module first extracts the sentences from each candidate page pair and aligns them, and then the similarity of the web pages in a pair is evaluated based on the similarities of the sentences which have been aligned. The sentences are aligned based on the length correlation criterion and the sentence similarity is measured by a novel strategy we design. We also design a novel strategy for evaluating the web page similarity based on the aligned sentence similarities. The last step is to save the parallel web pages. It can be concluded from the results of the experiments that the PWCC system is a high-performance and reliable tool for automatically constructing parallel corpora from the web. The structure of the paper is as follows. The PWCC architecture is introduced in Section 2. The strategy for candidate parallel pair preparation is described in Section 3. The evaluation process is discussed in Section 4 which is the key section of the paper. We practice the experiments and discuss the results in Section 5. The paper is concluded in Section 6.  2. System Architecture The PWCC system implemented as a four-step process is designed for automatically mining the web for parallel corpora (Figure 1). The first step of the system is the web page fetching process which downloads all the web pages from the hosts specified by the user. In the experiment section, the site of the ministry of foreign affairs of China is selected to be crawled because this site contains a great amount of parallel web pages with high quality. There have already been some free tools to do this work, and the software WebZip is chosen for the PWCC system. The candidate parallel page pairs are prepared in the second step of PWCC. The pairing process mainly relies on the similarity of the URLs, and also some other features such as web page size and anchor text are considered too. The candidate page pairs are then evaluated by the third step Specific hosts  www  Web  
It is not easy to explain “extent” since it is somewhat abstract; however, extent seems to exist in our daily lives. Everything bears a relation to extent: the redness of roses, the extent of saltiness, or even the maturity of a man. Mentioning extent, hen is always the term for use in Mandarin, such as Ta hen mei 她很美 (She is very beautiful). However, hen cannot indicate the highest extent. To show excessiveness, there is a construction which is popular with the young generation in Taiwan nowadays. The excessive construction is structured as “X + dao + si”, and it pragmatically emphasizes the speaker’s subjective emotion.  (1) 我 今天 累 到 死 wo jin-tian lei dao si I today tired DAO die  “I’m extremely tired today.”  The phrase is always used to indicate the very high extent of a certain state; therefore, we name the construction “the excessive construction” and dao the excessive structural particle, which appears in sentences mainly to link the word or phrase that precedes it to the elements that immediately follows it. In example (1), dao links the predicate lei and the complement si. As for the excessive construction, dao is worthy of discussion. Dao originally is a spatial term indicating “arrive at”. Since we failed to find dao’s use as an extent marker in ancient Mandarin, it might be until this modern time it is used to indicate excessive extent. How could a spatial term derive extent meaning? In addition, dao is not obligatory; that is, it could be omitted.  (2) 我 今天 累 死 了 wo jin-tian lei si le I today tired die ASP  “I’m extremely tired today.”  ∗ Copyright 2007 by Hsiu-Ying Liu 1 The symbol X refers to either a verb or an adjective. 293 
∗ Copyright 2007 by Xinnian Mao, Wei Xu, Yuan Dong, Saike He, and Haila Wang 303  only used to improve label consistency. To our best knowledge, up to now, non-local information has not been explored to improve NER recall in previous researches; on the other hand, NER is always impaired by its lower recall due to the imbalanced distribution where the NONE class dominates the entity classes. Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class (Kambhatla, 2006). In this paper, we employ non-local information to recall the missed entities. Similar to Krishnan and Manning (2006), we also encode non-local information with features and apply the simple two-stage architecture. Different from their work for improve label consistency, their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, our features are fired on the raw token sequence directly with forward maximum match. Compared to their non-local information extracted from training data with 10-fold cross-validation, our non-local information is extracted from the training date directly; our approach obtaining the non-local features is simpler. Moreover, we design different non-local features encoding different useful information for NER two subtasks: entity boundary detection and entity semantic classification. Our features are also inspired by Wong and Ng (2007). They extract entity majority type features from unlabelled data with an initial maximum entropy classifier. Our approach is validated on the third International Chinese language processing bakeoff (SIGHAN 2006) MSRA and CityU NER closed track, the experimental results show that non-local features can significantly improve the recall of the state-of-the-art NER system using local context alone. The remainder of the paper is structured as follows. In Section 2, we introduce the first stage CRF with local features alone; then we describe the second stage CRF using non-local features we design in Section 3. We demonstrate the experiments in Section 4 and we conclude the paper in Section 5.  2. Our Baseline NER System  To validate the effectiveness of our approach of exploiting non-local features, we need to establish a baseline with state-of-the-art performance using local context alone. Similar to (Krishnan and Manning, 2006), we employ two-stage architecture under conditional random fields (CRFs) framework. In the first stage, we build the baseline with local features only, and then we build the second NER system with non-local features. We will introduce them step by step.  2.1. Conditional random fields We regard the NER task as a sequence labeling problem and apply Conditional Random Fields (Lafferty et al., 2001; Sha and Pereira, 2003) since it represents the state of the art in sequence modeling and has also been very effective at NER task. It is undirected graph established on G = (V, E), where V is the set of random variables Y = {Yi|1≤i≤ n} for each the n tokens in an input sequence and E = {(Yi−1, Yi) |1≤i≤n} is the set of (n − 1) edges forming a linear chain. Following Lafferty et al. (2001), the conditional probability of the state sequence (s1, s2…sn) given the input sequence (o1, o2…on) is computed as follows:  
(1) A: 어서 그에게 가서 잘못을 사과해. (Promptly apologize to him for your fault.)1 * This research was performed for the Intelligent Robotics Development Program, one of the 21st Century Frontier R&D Programs, and Brain Science Research Center, funded by the Ministry of Commerce, Industry and Energy of Korea. Copyright 2007 by Hye-Jin Min and Jong C. Park 
c  Maki Miyake , Terry Joyce , Jaeyoung Jung , and Hiroyuki Akama  a Osaka University, 1-8 Machikaneyama-cho, Toyonaka-shi, Osaka, 560-0043, Japan b Tama University, 802 Engyo, Fujisawa-shi, Kanagawa-ken, 252-0805, Japan c Tokyo Institute of Technology, O-okayama, Meguro-ku, Tokyo, 152-8552, Japan  mmiyake@lang.osaka-u.ac.jp terry@tama.ac.jp {catherina, akama}@dp.hum.titech.ac.jp  Abstract. This paper reports on the application of network analysis approaches to investigate the characteristics of graph representations of Japanese word associations. Two semantic networks are constructed from two separate Japanese word association databases. The basic statistical features of the networks indicate that they have scale-free and smallworld properties and that they exhibit hierarchical organization. A graph clustering method is also applied to the networks with the objective of generating hierarchical structures within the semantic networks. The method is shown to be an efficient tool for analyzing large-scale structures within corpora. As a utilization of the network clustering results, we briefly introduce two web-based applications: the first is a search system that highlights various possible relations between words according to association type, while the second is to present the hierarchical architecture of a semantic network. The systems realize dynamic representations of network structures based on the relationships between words and concepts. Keywords: Network analysis, Graph clustering, Japanese word associations. 1. Introduction As an approach to deepening our understanding of lexical knowledge, many areas of cognitive science, including psychology and computational linguistics, are seeking to unravel the rich networks of associations that connect words together. Key methodologies for that enterprise are the techniques of graph representation and their analysis that allow us to discern the patterns of connectivity within large-scale resources of linguistic knowledge and to perceive the inherent relationships between words and word groups. * This research has been supported by the 21st Century Center of Excellence Program “Framework for Systematization and Application of Large-scale Knowledge Resources”. The authors would like to acknowledge here the generosity of the Center. The first and second authors have been supported by Grants-in-Aid for Scientific Research from the Japanese Society for the Promotion of Science (research project number 19700238 to first author and 18500200 to the second). In addition, the authors wish to express their thanks to Professor Shun Ishizaki for permission to use his Associative Concepts Dictionary in this study. Copyright 2007 by Maki Miyake, Terry Joyce, Jaeyoung Jung, and Hiroyuki Akama 321  Although studies applying versions of the multidimensional space model, such as Latent Semantic Analysis (LSA) and multidimensional scaling, to the analysis of texts have been fairly fruitful, the methodologies of graph theory and network analysis are particularly suitable for elucidating the important characteristics of semantic networks. Recently, a number of studies have applied graph theory approaches in investigating linguistic knowledge resources (Church and Hanks, 1990; Dorow, Widdows, Ling, Eckmann, Danilo and Moses, 2005; Steyvers and Tanenbaum 2005; van Dongen, 2000; Watts and Strogatz, 1998). For instance, Dorow, et al (2005) utilize two graph clustering techniques as methods of detecting lexical ambiguity and of acquiring semantic classes instead of word frequency based computations. This paper applies graph theory and network analysis methods to the analysis of semantic network representations of Japanese word associations. After briefly outlining the two separate Japanese word association databases used—the Associative Concept Dictionary (Okamoto and Ishizaki, 2001) and the Japanese Word Association Database (Joyce, 2005, 2006, 2007)—the paper calculates some basic statistical features, such as degree distributions, clustering coefficients and the average clustering coefficient distribution for nodes with degrees. We also apply the recently developed Recurrent Markov Clustering (RMCL) algorithm (Jung, Miyake and Akama, 2006) which enhances the bottom-up classification method of the basic MCL algorithm by making it possible to adjust the proportion in cluster sizes. Given this greater control over cluster sizes, the RMCL clearly provides a very appealing approach to the automatic construction of condensed network representations, which, in turn, can facilitate the creation of hierarchically-organized semantic spaces as a way of visualizing large-scale linguistic knowledge resources. 2. Building Semantic Network Graphs of Japanese Word Associations This section outlines the semantic network representations of the Japanese word association databases. Specifically, the section briefly describes two separate databases of Japanese word associations—the Associative Concept Dictionary (ACD) and the Japanese Word Association Database (JWAD)—and the semantic network representations created from them. 2.1.Existing word association norms As frames of reference concerning the scales of the two Japanese word association databases, it worth noting that large-scale, comprehensive word association normative data has existed for some time for English. For example, Moss and Older (1996) collected between 40-50 responses for some 2,400 words of British English, while Nelson, McEvoy and Schreiber (1998) compiled perhaps the largest database of American English covering some 5,000 words with approximately 150 responses per item. Notwithstanding the early survey by Umemoto (1969), which gathered free associations from 1,000 university students for a very small set of 210 words, clearly there has been a serious lack of comparative databases of Japanese word associations. Both the ACD and the JWAD seek to redress this situation, especially the ongoing JWAD project which is committed to constructing a large-scale database for its current survey corpus of 5,000 basic Japanese kanji and words. 2.2.Associative Concept Dictionary Okamoto and Ishizaki (2001) created the Associative Concept Dictionary (ACD), which is organized as a hierarchal structure of higher/lower level concepts. The data consists of 33,018 word association responses provided by 10 respondents according to specified response categories for 1,656 nouns. By excluding response words with a frequency of 1 and a clustering coefficient of 0, 9,373 words were selected for use in creating a semantic network representation. 322  2.3.Japanese Word Association Database The Japanese Word Association Database is being constructed as part of a project to investigate lexical knowledge in Japanese by mapping out Japanese word associations (Joyce, 2005; 2006; 2007). While the particular task—specifying in advance the associative relationship for responses—employed in creating the ACD can arguably be justified in terms of constructing a dictionary of associated concepts, the data provides little insight into the rich and diverse nature of word associations. Accordingly, the JWAD employs the free word association task in collecting association responses. Also in contrast to the ACD, which only examined nouns, the JWAD is surveying words of all word classes. Version 1 of the JWAD consists of a random sample of 2,099 items from the survey corpus of 5,000 basic Japanese kanji and words that were presented to up to 50 respondents. For the JWAD network, only words with a frequency of 2 or more were selected, which resulted in set of 7,966 words to be clustered. 3. Analyses of the Network Structures As already suggested, graph representations and the techniques of graph theory and network analysis are particularly promising techniques with which to examine the intricate patterns of connectivity within large-scale linguistic knowledge resources. For instance, Steyvers and Tenenbaum (2005) conducted a noteworthy study that examined the structural features of three semantic networks. By calculating a range of statistical features, including the average shortest paths, diameters, clustering coefficients, and degree distributions, they observed interesting similarities between the three networks in terms of their scale-free patterns of connectivity and small-world structures. Following their basic approach, we analyze the characteristics of the two semantic network representations of Japanese word associations by calculating the statistical features of degree distribution and clustering coefficient—an index of the interconnectivity strength between neighboring nodes in a graph. 3.1.Degree distribution From their computations of degree distributions, Balabasi and Albert (1999) suggest that the degree distribution, P(k), for scale-free network structures will correspond to a power law, which can be expressed as P(k) ≈ k −r . Figure 1 presents degree distributions for word occurrences in the two semantic networks, which indicate that P(k) conforms to a power-law in both cases (with exponent values, r, of 1.8 for the ACD (panel a) and 2.3 for the JWAD (panel b). In the case of the ACD, the average degree value is 19.96 (0.2%) for the complete semantic network of 9,373 nodes, while the average degree value is 3.67 (0.05% for 7,966 nodes) in the JWAD’s case. The results clearly indicate that the networks exhibit a pattern of sparse connectivity; in other words, that they possess the characteristics of a scale-free network. 323  
a  Masaki Murata , Toshiyuki Kanamaru , Koichirou Nakamoto , Katsunori Kotani ,  a and Hitoshi Isahara  a National Institure of Information and Communications Technology, 3-5, Hikaridai, Soraku-gun, Kyoto, 619-0289, Japan {murata, kanamaru, isahara}@nict.go.jp, kat@khn.nict.go.jp b Osaka University of Foreign Studies, 8-1-1 Aomatani-Higashi, Minoh, Osaka, 562-8558, Japan nobi@sings.jp  Abstract: We extracted English expressions that appear in Japanese sentences in newspaper articles and on the Internet. The results obtained from the newspaper articles showed that the preposition “in” has been regularly used for more than ten years, and it is still regularly used now. The results obtained from the Internet articles showed there were many kinds of English expressions from various parts of speech. We extracted some interesting expressions that included English prepositions and verb phrases. These were interesting because they had different word orders to the normal order in Japanese expressions. Comparing the extracted English and katakana expressions, we found that the expressions that are commonly used in Japanese are often written in the katakana syllabary and that the expressions that are not so often used in Japanese, such as prepositions, are hardly ever written in the katakana syllabary. Keyword: English expression, katakana expression, newspaper article, Internet  1. Introduction We often see English expressions inserted in Japanese sentences, such as “コンサート in 東 京” (“コンサート”=concert, “東京”=Tokyo). We extracted such expressions that were written in kanji, katakana, hiragana, and Roman alphabet characters or combinations of these characters from articles in the Mainichi Shinbun (a major daily Japanese language newspaper) and on the Internet. From the same sources, we also extracted expressions that used katakana characters instead of English characters, such as “博覧会イン京都” (“博覧会”= exposition, “イン”=in, “京都”=Kyoto). The study of such expressions is useful for linguists studying the development process of languages (Ito (2001), Daulton (2003), and Murata et al. (2004)). 2. Automatic extraction of English and Katakana expressions We used computers to extract candidate expressions in Japanese that included English expressions from Mainichi Shinbun articles published between 1991 and 2005 and from about 2 GB of Internet articles. We then checked the candidate expressions by hand to select the actual expressions we would use in our research. We initially extracted expressions from Mainichi Shinbun articles. We took only the expressions that contained brackets. This is because expressions that include English * Copyright 2007 by Masaki Murata, Toshiyuki Kanamaru, Koichirou Nakamoto, Katsunori Kotani, Hitoshi Isahara  330  expressions in Japanese sentences often appear in brackets. We automatically extracted expressions that satisfied the following conditions: i) included both Japanese characters in hiragana, katakana, and kanji and characters in the Roman alphabet, ii) the adjacent characters to English expressions are Japanese characters, or English expressions appear at the beginning of the expressions or the end of the expressions, iii) the second to last characters of the English expression were not capitalized, and iv) English expressions that consisted of more than one character. The condition ii) is used because there were many cases where the expressions were just replaced with Japanese expressions such as “過労死 (karoushi) 病” (“過労死”=“overwork death”, “karoushi” is the reading of “ 過 労 死 ”, “病”=disease), and we excluded such expressions. The condition iii) is used because there were many cases where expressions consisted of capitalized characters that were probably abbreviations, such as “OPEC” and “ASEAN”, and we excluded such expressions. We used only the cases were an English expression inserted into a Japanese sentence consisted of one word. This is because English expressions that consist of multiple words are often quotations of English phrases or sentences and are not expressions where the Japanese and English languages merge. We extracted expressions that did not contain appropriate spaces, such as “Welcometo”. We did not use expressions where the English expression was a noun, a proper noun, a symbol, or adjective that was modified by adding the Japanese suffixes “na” (for an adjective) and “suru” (for a verb). This is because the deleted expressions are popular in the conventional Japanese language. (In this study, we extracted “de”, although it is not English but French. This is because its frequency is high and it is also an interesting expression.) We arranged the final list of expressions in order of frequency (see Table 1). “V”, “AJ”, “AV”, “PP”, “CJ”, “AR”, “PO”, and “PH” (in the table below) indicate verbs, adjectives, adverbs, prepositions, conjunctions, articles, possessive pronouns, and phrases, respectively. Next, we extracted expressions from articles on the Internet. We used the same conditions as above and one additional condition: the first character of an English expression at the beginning of the sentence had to be capitalized. The results are shown in Table 2. Here, the extracted results are arranged by parts of speech. The table shows the English expression, its frequency, and some examples for each expression. In the example, the corresponding part of the English expression is marked with a *. Table 2 shows the results in descending order of frequency. The table only contains expressions that appeared more than 10 times. Finally, we extracted from Mainichi Shinbun and Internet articles Japanese expressions that contained katakana expressions. We next constructed a list of the English expressions written in katakana characters. Using only katakana expressions, we compiled a list of expressions in order of frequency (Tables 3 and 4). We plotted graphs showing the average (we call it average value) of the average, mode, and median years when the extracted expressions appeared (Figures 1 and 2). In the figures, each expression is displayed in ascending order of the average value. We added the total number of expression and the average value to each expression in the figures. Therefore, expressions that appeared in the earlier years are displayed higher on the graph, while expressions that appeared in the later years are displayed lower. 3. Discussion Table 1 shows that English prepositions have been used for more than 10 years. The use of the expression “in” rapidly increased in 1994. After that, it appeared more than 10 times each year. “Let’s” has also been used for more than 10 years; however, it stopped appearing in 2000. The prepositions “at”, “of”, “with”, “from”, “by”, “of”, “with”, “from”, “by”, and “to” have been also used for a relatively long time. We extracted various English expressions that form various parts of speech from articles on the Internet (Table 2). Prepositions appear the most frequently, followed by conjunctions. Adjectives are third. After that, in descending order of frequency, come phrases, adverbs, 331  Table 1: English expressions in Mainichi Shinbun articles  1991  1992  1993  1994  1995  ｉｎ  PP 2 ｉｎ  PP 4 Ｌｅｔ’ ｓ PH 1 ｉｎ PP 12 ｉｎ PP 21  Ｌｅｔ’ ｓ PH 1 Ｔｈｅ AR 1 ｉｎ PP 1 ｔｈｅ AR 1 ａｎｄ CJ 1  ａｔ  PP 1 Ｈａｎｄｙ AJ 1 ｖｓ PP 1 ｍｙ PO 1 Ｆｒｏｍ PP 1  ｄｅ  PP 1 Ｎｅｗ AJ 1 ｗｉｔｈ PP 1 ｂｙ PP 1 ｂｙ PP 1  ｖｓ  PP 1 ｏｆ  PP 1  ｄｅ PP 1 ｆｒｏｍ PP 1  ｔｏ PP 1 ｔｏ PP 1  ｗｉｔｈ PP 1  1996  1997  1998  1999  2000  ｉｎ  PP 15 ｉｎ  PP 28 ｉｎ PP 21 ｉｎ PP 22 ｉｎ PP 19  Ｄｅａｒ AJ 3 Ｗｉｔｈ PP 2 ｄｅ PP 6 ｄｅ PP 3 ｄｅ PP 9  Ｔｈａｔ’ ｓ PH 2 Ｄｅａｒ AJ 1 Ｔｈｅ AR 3 Ｍｙ PO 2 ａｔ PP 2  Ｎｅｗ AJ 2 Ｊｒ  AJ 1 Ｌｅｔ’ ｓ PH 1 ｖｓ PP 2 ｆｒｏｍ PP 2  Ｌｅｔ’ ｓ PH 1 ｆｏｒ PP 1 ｉｎｇ CJ 1 Ｉｔ’ ｓ PH 1 ｗｉｔｈ PP 2  ｄｅ  PP 1 ｖｓ  PP 1 Ｂｙ PP 1 Ｎｅｗ AJ 1 ａｎｄ CJ 1  ｔｏ  PP 1 ｗｉｔｈ PP 1 ｆｏｒ PP 1 ｉｎｇ CJ 1 ｂｙ PP 1  ｖｓ  PP 1 ＦｏｒｅｖｅｒAD 1 ｔｏ PP 1 ｏｒ CJ 1 ｔｏ PP 1  ｗｉｔｈ PP 1  ｗｉｔｈ PP 1 ｆｒｏｍ PP 1  Ｂｅ V 1  2001  2002  2003  2004  2005  ｉｎ  PP 32 ｉｎ  PP 13 ｉｎ PP 17 ｉｎ PP 12 ｉｎ PP 12  ｄｅ  PP 6 ｄｅ  PP 7 ｄｅ PP 5 ｄｅ PP 6 ｗｉｔｈ PP 3  Ｎｅｗ AJ 2 Ｎｅｗ AJ 2 ｂｙ PP 4 ｏｆ PP 2 Ｇｏｏｄ AJ 1  ｆｒｏｍ PP 2 ａｎｄ CJ 1 Ｊｒ． AJ 1 Ｆｒｏｍ PP 1 Ｎｅｗ AJ 1  ｔｏ  PP 2 ｆｒｏｍ PP 1 Ｔｈｅ AR 1 Ｊｒ． AJ 1 Ｔｈｅ AR 1  ｔｈｅ AR 1 ｗｉｔｈ PP 1  Ｎｅｗ AJ 1 ｍｙ PO 1  Ｔｈａｔ’ ｓ PH 1  ｔｏ PP 1 ｔｈｅ AR 1  Ｗｈａｔ’ ｓ PH 1  Ｄｉｇｉｔａｌ AJ 1  ｃａｎ AX 1  Ｄｅ PP 1  Ｗｉｔｈ PP 1  ｆｏｒ PP 1  ｖｓ  PP 1  ａｇａｉｎ AD 1  Table 2: English expressions on the Internet  Prepositions (34 types, 16352 tokens in total) with  in 4852 キャンペーン＊渋谷  (campaign * Shibuya)  for  by 2504 現地レポート＊[PERSON]  (on-site report * [PERSON])  to  vs 1894 [PERSON]＊[PERSON]  de 1290 メール＊ゲット！(mail * get!)  from  ｉｎ 1276 ジャズトリオ＊神戸  (jazz trio * Kobe)  ｖｓ 888 山形市民＊天童市民  (Yamagata citizens * Tendo citizens)  533 [PERSON]＊オーケストラ ([PERSON] * orchestra) 461 データ＊ピアノ・レッスン (data * piano lesson) 360 固定電話＊固定電話 (fixed-line phone * fixed-line phone) 354 文部科学省の働き＊[PERSON] (activities of Ministry of Education, Culture, Sports, Science and Technology * [PERSON])  332  Table 2: English expressions on the Internet (cont.)  Prepositions (34 types, 16352 tokens in total) (cont.) ｎｅｗ  ｂｙ  269 医療施設＊[ORGANIZATION]  Online  (medical facility * [ORGANIZATION]back  at  254 忘年会＊難波  Free  (year-end party * Namba)  vs.  254 湘南＊新潟 (Shonan * Niigata)  Ｎｅｗ  ｄｅ 240 ねっと＊しょっぷ (net * shop)  all  on  189 奥津＊メディア (Okutsu * media) more  From 145 ＊こうち (* Kochi)  of  77 編集部＊速報ニュー ス  free  (editorial office * breaking news) public  By  75 ＊[PERSON]  ｔｏ  60 ドア＊ドア (door * door)  For  44 ＊ビギナー ス (* beginners)  next Open  To  42 ＊軽井沢 (* Karuizawa)  Weekly  ｗｉｔｈ Ｂｙ  42 [PERSON]＊[PERSON] 27 ＊監督 (* director)  active close  About about ｆｏｒ  26 ＊文学部 (* literature department) 25 ＊海洋堂 (* Kaiyodo) 25 コンサー ト＊[ORGANIZATION]  ｇｏｏｄ Dear Special  (concert * [ORGANIZATION])  Digital  ｏｎ  24 アンケー ト集計＊ネット  cool  (summary of questionnaire * network) official  Vs.  21 ＊[ORGANIZATION]  Personal  since 20 ＊平成11年9月27日  Cool  (* September 27, 1999)  Jr.  ａｔ  20 コンテスト＊[LOCATION]  Back  (contest * [LOCATION])  Next  With  17 ＊[PERSON]  virtual  ｆｒｏｍ 17 独り言＊[PERSON]  weekly  (soliloquy * [PERSON])  Happy  by.  14 ＊[PERSON]  In  13 ＊[LOCATION]  digital  mobile  Conjunctions (4 types, 3458 tokens in total) special  or  2236 交換＊差し上げます  Best  (exchange * give)  happy  and  641 [PERSON]＊[PERSON]  Mobile  ｏｒ 541 おかゆ＊ご飯 (porridge * rice) ａｎｄ 40 パスタ＊ワイン (pasta * wine)  closed Ｊｒ．  Virtual  Adjectives (59 types, 3066 tokens in total)  new 582 ＊おまかせプラン  inline  (* prepared plan)  personal  New 399 ＊ぐるりん阿蘇  (* touring Aso)  Good  main open  158 ＊作品 (* work) 133 ＊会議 (* conference)  Hot Monthly  online 130 ＊雑誌 (* magazine) good 111 ＊性能バランス  short Big  (* performance balance)  Easy  No  72 ＊残業DAY  (* overtime working day)  Global offline  Super 72 ＊アモルファス (* amorphous)  Hyper  72 ＊アルバム (* album) 67 ＊作文教室 (* writing class) 65 ＊ナンバー (* number) 52 ＊座談室 (* round-table meeting room) 74 ＊ステッカー (* sticker) 50 ＊ほっかいどう(* Hokkaido) 47 ＊レッドハットニュー ス (* Red Hat news) 45 ＊素材 (* material) 42 ＊参加 (* entry) 41 個展＊開催 (personal exhibition * holding) 40 ＊フォー ラム (* forum) 39 ＊レポー ト(* report) 35 ＊手段 (* measures) 34 全て＊です (be all *) 34 ＊ウェイキ (* wake) 33 ＊[PERSON] 29 ＊キャンペー ン (* campaign) 27 ＊出力 (* output) 27 ＊ブランド(* brand) 26 ＊サイト(* site) 25 ＊モデル (* model) 24 ＊サイト(* site) 24 ＊選手権 (* championship) 23 ＊モデル (* model) 23 ＊シー ズン (* season) 23 ＊体験 (* experience) 22 ＊パッケー ジ (* package) 21 ＊懸賞らいふ！ (* days with prize) 20 ＊回路 (* circuit) 20 ＊端末 (* terminal) 20 ＊企画 (* enterprise) 19 ＊記録 (* record) 18 ＊ハー ト(* heart) 17 ＊端末 (* terminal) 16 ＊会議 (* meeting) 16 ＊フルー ト教室 (* flute class) 15 ＊ゼミ訪問 (* visit to seminar) 15 ＊画像 (* image) 15 ＊ホー ムペー ジサー バ (* homepage server) 14 ＊リフォー ム (* reform) 14 ＊リスト(* list) 14 ＊セミナー (* seminar) 14 ＊ブー ツ (* boots) 13 ＊プレゼント(* present) 13 ＊登録 (* registration) 13 ＊企業 (* company) 13 ＊状態 (* status) 12 ＊和紙 (* Japanese paper)  333  Table 2: English expressions on the Internet (cont.)  Adjectives (59 types, 3066 tokens in total) (cont.) only  super  12 ＊特急 (* limited express) all  Daily  11 ＊ランキング (* ranking)  ｏｆｆ  High  11 ＊レベル (* level)  ｏｎｌｙ  63 土日＊ (Saturdays and Sundays * ) 50 同業者＊ (* peer) 30 ５００円＊ (500 yen * ) 16 英語＊ (English * )  Phrases (23 types, 909 tokens in total)  Articles (3 types, 364 tokens in total)  Welcometo 294 ＊腕時計資料室  The  201 ＊沖縄 (* Okinawa)  (* wrist watch archives)  the  115 ＊古書 (* old book)  What's  114 ＊けいば (* horse race)  Ｔｈｅ  48 ＊日本列島 (* Japanese archipelago)  Let's  75 ＊スター ト！ (* start!)  Presentedby 34 ＊関東バス (* Kanto Bus)  Possessive pronouns (4 types, 357 tokens in total)  writtenby 32 ＊[PERSON]  My  181 ＊コレクション (* collection)  Ｌｅｔ’ｓ  31 ＊テニス！ (* play tennis!) Ｍｙ  77 ＊ペー ジ (* page)  Howto  29 ＊アクセス (* access)  my  69 ＊テー マ (* theme)  Producedby 29 ＊[ORGANIZATION]  ｍｙ  30 ＊ペー ジ (* page)  Writtenby 28 ＊[PERSON]  producedby 27 ＊[ORGANIZATION]  Verbs (9 types, 246 tokens in total)  photoby  25 ＊[ORGANIZATION]  start  56 ＊インター ネット(* Internet)  welcometo 24 ＊情報科学部 (* information Welcome 48 ＊えひめ (* Ehime)  science department)  Do  29 ＊中国茶 (* Chinese tea)  Poweredby 23 ＊[PERSON]  welcome 28 ＊[ORGANIZATION]病院  Backto  22 ＊広県大ＨＰ  (* [ORGANIZATION] hospital)  (* Hiroshima Prefecture  Get  25 ＊懸賞 (* prize)  University homepage)  love  22 ＊熊本電波 (* Kumamoto radio wave)  It's  18 ＊オススメ!! (* recommended!!)enjoy  16 ＊講座 (* course)  postedby  17 ＊[PERSON]  Ｗｅｌｃｏｍｅ 12 ＊[ORGANIZATION]ホー ムペー ジ  foryou  15 メッセー ジ＊ (* message)  (* [ORGANIZATION] homepage)  presentedby 14 ＊[ORGANIZATION]  Love  10 ＊みやざき (* Miyazaki)  Textby  12 ＊[PERSON]  what's  12 ＊ろうきん？ (* Labour Bank?)  Suffixes (2 types, 202 tokens in total)  Ｗｈａｔ’ｓ 12 ＊ろうきん (* Labour Bank) ing  148 俳句＊ (haiku * )  That's  11 ＊インタビュー (* interview) ｉｎｇ  54 職＊全国版求人情報  howto  11 ＊物 (* related things)  (work * job information for all  regions)  Adverbs (7 types, 593 tokens in total)  ｕｐ  208 9,000円＊ (9,000 yen * )  Interrogative pronouns (1 type, 10 tokens in total)  off  131 10パー セント＊ (10 percent * ) What  10 ＊共栄会 (* Kyoueikai)  not  95 ＊オススメ(* recommended)  Table 3: Katakana expressions in Mainichi Shinbun  1991  アップ (up)  AD  ニュー (new)  AJ  グロー バル (global) AJ  スー パー (super)  AJ  デジタル (digital)  AJ  ジュニア (junior)  AJ  パー ソナル (personal) AJ  ベスト(best)  AJ  ザ (the)  AR  ハウツー (how to) PH  オー プン (open)  AJ  オー ル (all)  AJ  ノー (no)  AJ  ウエルカム (welcome) V  1992 11 デジタル (digital) 7 スー パー (super) 5 ニュー (new) 3 アップ (up) 3 パー ソナル (personal) 2 オー ル (all) 2 ジュニア (junior) 2 ノー (no) 1 ベスト(best) 1 オー プン (open) 1 グロー バル (global) 1 スペシャル (special) 1 ハイパー (hyper) 1 ハウツー (how to) アンド(and) ウェルカム (welcome)  1993 AJ 11 アップ (up) AJ 7 スー パー (super) AJ 7 デジタル (digital) AD 7 ジュニア (junior) AJ 6 ニュー (new) AJ 3 ザ (the) AJ 3 オー ル (all) AJ 3 オンライン (online) AJ 3 グロー バル (global) AJ 2 ザッツ (that's) AJ 2 オー プン (open) AJ 2 ハンディー (handy) AJ 2 バー チャル (virtual) PH 1 フリー (free) CJ 1 ベスト(best) V 1 イン (in) オンリー (only)  AD 21 AJ 8 AJ 6 AJ 4 AJ 4 AR 3 AJ 3 AJ 2 AJ 2 PH 1 AJ 1 AJ 1 AJ 1 AJ 1 AJ 1 PP 1 AD 1  334  Table 3: Katakana expressions in Mainichi Shinbun (cont.)  1994  1995  1996  アップ (up)  AD 22 オー ル (all)  AJ 20 アップ (up)  AD 20  デジタル (digital)  AJ 14 デジタル (digital)  AJ 12 デジタル (digital)  AJ 19  スー パー (super)  AJ 12 アップ (up)  AD 12 グロー バル (global) AJ 10  オー ル (all)  AJ 4 スー パー (super)  AJ 9 スー パー (super)  AJ 8  オンライン (online) AJ 3 グロー バル (global)  AJ 5 オー ル (all)  AJ 6  グロー バル (global) AJ 3 オンライン (online)  AJ 4 バー チャル (virtual) AJ 5  ジュニア (junior/Jr.) AJ 3 オー プン (open)  AJ 3 ニュー (new)  AJ 4  オー プン (open)  AJ 1 ニュー (new)  AJ 3 オー プン (open)  AJ 2  オフライン (offline) AJ 1 イン (in)  PP 2 オンライン (online)  AJ 2  ニュー (new)  AJ 1 イッツ (it's)  PH 1 ジュニア (junior)  AJ 2  ノー (no)  AJ 1 ハウツー (how to)  PH 1 ビッグ (big)  AJ 2  バー チャル (virtual) AJ 1 レッツ (let's)  PH 1 ベスト(best)  AJ 2  パー ソナル (personal) AJ 1 キッズ (kids)  AJ 1 イン (in)  PP 2  ベスト(best)  AJ 1 ジュニア (junior)  AJ 1 ザ (the)  AR 1  ウェルカム (welcome) V  
Syntax and semantics of complex verbs have long been the focus of attention in Japanese linguistics. This paper proposes a new approach to the formation of passive and potential verbs, both of which include the auxiliary verb -rare, and explore the relationship between the complex verb formation and projections of syntactic structures. The auxiliary verb -rare has been assumed to be semantically ambiguous among passive, potential, spontaneous and honorific interpretations. We will take up the passive and potential interpretations of the auxiliary verb, and argue that there is a crucial difference between the two use of this verb. First, observe the sentence in (1): (1) Kurisumasu-ni-wa takusan-no keeki-ga taber-are-ru. Christmas-at-Top a lot of cake-Nom eat-Can/Pass-Pres 'At Christmas, a lot of cakes are/can be eaten.' (1) can be interpreted as passive on one reading, i.e., a lot of cakes are eaten at Christmas. On another reading, (1) is taken to be a statement of the possibility; we (or arbitrary people) can eat a lot of cakes at Christmas. * Copyright 2007 by Hiroaki Nakamura 340  The multiple usages of the auxiliary verb -rare has been assumed to be due to the common origin, sharing the meaning of spontaneity (get, become, etc.), though there are different hypotheses concerning which use is original. The interpretations of -rare are ambiguous in some cases and it is necessary to take contextual information into account. We will argue, however, that the two complex verbs must be derived in a completely different manner, exploring the interactions between the word formation and honorification or choice of subjects in sentences projected from them. Honorification is a device to indicate that the speaker feels respect for the person referred to by the grammatical subject or object, using the discontinuous honorific morpheme comprising the prefix o- and the verb -ni-nar (a kind of auxiliary verb). This morpheme combines with (actually wrap) verbal roots (infinitives) without changing the argument structures of the latter. (2) illustrates the subject honorification we address in this paper. Hereafter, HP stands for the honorific prefix (o-) and HS the honorific suffix (-ninar).  (2) Sensei-wa keeki-o o-tabe-nina(r)-tta. Sensei-Top cake-Acc HP-eat-HS-Past 'Sensei ate a cake.'  Interestingly, the passive and potential complex verbs, both of which are formed by the concatenation with the auxiliary verb -rare, shows different processes of honorification. The honorific morpheme wraps derived complex passive forms, while it wraps only base verbs, and -rare follows the derived honorific forms in the potential verb formation. Compare the following examples:  (3) a. Sensei-wa okusama-ni o-sikar-are-nina(r)-tta.  teacger-Top wife-BY  HP-reproach-Pass-HS-Past  'The teacher was reproached by his wife.'  b. Sensei-wa okusama-o/-ga o-sikar-ninar-(ar)e-nai. teacher-Top wife-Acc/-Nom HP-reproach-HS-Can-Neg-Pres 'The teacher cannot reproach his wife.'  This paper provide a new account for the relation between syntactic structures and the complex verb formation, in which passive and potential complex verbs are formed in a completely different manner. We discuss how subjects with a wide variety of semantic relations to base verbs are derived in potential constructions and consider the properties of (major) subjects in stative sentences, the license of which should be distinguished from that of ordinary subjects.  2. Framework: Combinatory Categorial Grammar In this paper we adopt a mildly context-sensitive grammar, Combinatory Categorial Grammar (henceforce, CCG; see Steedman 1996, 2000, Baldridge 2002, among others) as a descriptive framework. 'Mildly context-sensitive' means that this grammar formalism allows associativity (rebracketing) and permutativity (reordering) among the modes of combination, in addition to the standard context-free concatenation (application). The combinatory rules allowing such flexible combinations are often said to be too strong in generative capacity, and quite often derive illicit strings in some languages. For example, it has been pointed out that the nonassociative and permutative mode (for the mixed composition rules) is not necessary for English grammar, but necessary to derive some constructions in Dutch. To constrain the applicability of rules, Baldridge (2002) and Steedman & Baldridge (2007) propose to use modalized slashes in combinatory rules, so that the applicability of rules can be lexically controlled. The Multimodal CCG defines the hierarchy of modes as in (6):  341  (4)  *  * non-permutative/non-associative  ◇ associative/non-permutative  ◇  ×  × non-associative/permutative  ・associative/permutative  ・  The * modality is the most restricted and allows only functional application. The ◇ modality  permits order-preserving associativity in derivation. The mode・is placed at the bottom of  hierarchy, which indicates that it is the most permissive one, inheriting the properties of all the  others. Then we have the following list of combinatory rules:  (5) a. b. c. d. e.  (>) X/Y Y ⇒ X (<) Y X\Y ⇒ X (>B) X/◇Y Y/◇Z ⇒ X/◇Z (<B) Y\◇Z X\◇Y ⇒ X\◇Z (>B×) X/×Y Y\×Z ⇒ X\×Z B (<B×) Y\×Z X\×Y ⇒ X/×Z B (>T) X ⇒ Y/i(Y\iX) (<T) X ⇒ Y\i(Y/iX) LWRAP: Y X↓◇Y ⇒ X  We will simply omit the symbol '*' in the derivations hereafter. The harmonic composition rules in (5b) allows associativity, while the disharmonic, mixed composition rules in (5c) allow permutation of elements in input strings. The input to the type-raising in (5d) does not make reference to any slashes, but the slashes in the output category must have the same mode, and the subscript i stands for a variable over modes. CCG has no (left) wrap rules as in (5e) but we will use the wrapping rule as in (5e) only for expository purposes when the honorific morpheme combines with verbs, and assume that it may participate in harmonic composition.  3. Derivations of Passive and Potential Complex Verbs In this section, let us consider how to derive the two complex verbs, especially addressing the interaction with honorification. We also examine the subject selection in sentences projected from these complex predicates (especially in potential constructions), which is proved to be a direct consequence of the concatenation we propose shortly. First let us start with the fact that the discontinuous honorific form wraps the complex passive verbs, whereas it wraps only the base infinitive of potential verbs.  (6)a. o- sikar- are- nina(r)-tta. HP- scold- PASS- HS-PAST  (Passive)  b. o- sikari- ni_nar- (ar)e- ru. HP- scold- HS- CAN- PRES  (Potential)  342  The dependency relations between corresponding morphemes can be shown respectively as in (6). We should notice the change in argument structures through the concatenation. In (6a), the honorific expressions mark the derived external argument of the passive verb sikar-are 'reproached', which was originally the theme argument of the base verb, as the person the speaker has respect for. Since this change of argument structure is not derived syntactically, the derivation of passive verbs must be treated in the lexicon. In examples and derivations, the respected referents are indicated with underlines annotated with H if necessary. In (6b), the relation between discontinuous honorific expressions and that between the base and potential verbs may be said to show crossing dependencies. The base verb is wrapped by the honorifics first and the derived form o-sikari-ninar, is followed by the potential auxiliary verb. We assume that the original external argument of infinitive is suppressed/demoted and the original theme is promoted to the outermost argument in the argument structure of a passive, whereas such argument change does not occur in that of a potential verb. The derivation of a passive verb with honorifics can be shown as in (7) where the passive complex verb is formed by a lexical operation, and honorification may apply to it in syntax.  (7)  sikar-  -(r)are  o...nar  reproach(x)(y) Pass  HP+HS  <Lex.  sikar-are: be_reproached(y)(x)  <W  o-sikar-are-ninar: be_reproached(y)(xH)  The subject of a potential construction is often difficult to distinguish from that of passive, as illustrated in (1), where the theme of the original base verb is the subject in both readings, but we can see a significant difference in subject selection between two constructions. In potential sentences, any argument or adjunct can be subjectivized, as illustrated in (8), the derivation of which will be explored in the following section.  (8) a. Kono resutoran-ga/-de oisii keeki-o/-ga  taber-are-ru.  this restaurant-Nom/-Loc delicious cake-Acc/-Nom eat-Can-Pres  'This restaurant is such that people can eat delicious cakes there.'  b. Kono naifu-ga/-de katai niku-o/-ga  kantan-ni kir-(rar)e-ru.  this-knife-Nom/-By leathery meat-Nom/-Acc easily cut-Can-Pres  'This knife is such that people can cut leathery meat easily with it.'  It should be noticed that adjuncts cannot be subjectivized in passive sentences. The examples in (8) remind us of the missing object constructions in English. In fact, any argument/adjunct can become a subject in tough-type sentences in Japanese. The only candidate for the syntactic structure of these sentences can be derived by via null operator movement in the governmentbinding theory, as shown in (9). Here let us assume that adjuncts are optional arguments to the verb following Marten (2002), McConnell-Ginet (1982), Steedman (1996).  (9) [Kono naifu-ga [CP OPi [IP PROarb katai-niku-o ti kir] -(ar)e-ru].  this knife-Nom  leathery meat-Acc cut- Can-Pres  The direct movement of the adjunct to the subject position is not allowed in the theory, so the null operator must be posited in its base position and be moved up to associate the matrix subject with the proper semantic role (i.e., instrumental). Because the null operator movement is a kind of wh-movement, adjuncts as well as arguments can be moved to the [Spec, CP] position. It is not clear, however, how the structure like (9) can provide a proper interpretation  343  compositionally, which is considered in section 4. Sentences like (8) suggest that the argument structures are not changed as seen in the passive case (7), so we can infer that the potential complex verb is derived syntactically. Observe the formation of the potential verb with honorifics in (10).  (10)  sikar-  o...-ninar-  -rare  V: λxλy.reproach(y)(x) (V\◇V)  V\◇V: λP.can(P)  <B  o-tabe-ninar-  V: λxλy.eat(y)(xH) <B  o-tabe-ninar-(rar)e-ru  V: λxλy.can'(eat(y)(x+H))  In derivation (10), the base verb -tabe combines with/wrapped by the honorific morpheme first, postponing the concatenation with arguments/adjuncts lexically or optionally specified. The information of arguments/adjuncts can be passed along to the resulting complex form. Then the honorific base verb combines with the potential auxiliary verb by composition. The complex potential verb with the honorific morpheme actually conveys information of arguments or adjuncts which were not consumed throughout the formation. This will explain the occurrence of multiple nominative terms in potential constructions, which is not allowed in passive sentences. Notice that any absorption of arguments or externalization of internal arguments does not occur here. The information of semantic roles of a base verb are simply passed up via a device like the slash-feature passing mechanism. This operation must be carried out in syntax. Here let us consider the examples of case alternation like (11).  (11) a. Taroo-ga eigo-o  hanas-(ar)e-ru.  Taroo-Nom English-Acc speak-Can-Pres  'Taroo can speak English.'  b. Taroo-ga eigo-o  hanas-(ar)e-ru.  Taroo-Nom English-Acc speak-Can-Pres  ibid.  There is no semantic difference between (11a) and (11b) (but we will see a difference in scope interpretation in the next section). In (11a) with the accusative object, the arguments combine with the base verb in the order lexically specified. On the other hand, in (11b) with the nominative object, the base verb 'speak' combines with the potential verb, which assigns the nominative case to the object as a stative verb (because stative verbs has no ability to assign accusative case). This alternation is not allowed in passive sentences because passive verbs are lexically specified to consume arguments by the most restrictive mode of operation, which results in obligatory case absorption. The * modality in the category of passive verbs allows only the applicative rule. On the other hand, the potential verb are lexically specified as having less restrictive modality. Naturally, we can conclude that the potential suffix are allowed to concatenate in both the * and ◇ mode, exactly as in the tough-sentences in (12).  (12) a. It is easy to play the sonata with the violin. b. This violin is easy to play the sonata with.  (13) a. It seems to need repainting this wall. b. This wall seems to need repainting e.  344  In (a) sentences, the accusative case is assigned to the object in situ by the infinitive. In (b) sentences, the information of missing objects is passed up to the top of complement clauses. It should be noticed that the matrix subjects are licensed by the embedded clauses with missing objects (i.e., open propositions, ignoring agent arguments of infinitives). The point is the optionality of the application of function composition/slash-feature passing operation. This optionality is not available for the passive verb formation which is lexically specified to undergo the most restrictive combination. The hierarchy for slash modalities can account for the difference in availability of case alternation between passive and potential verbs.  4. Selection of Subjects and its Implications In this section let us extend the analysis proposed in the previous section and derive potential sentences including subjects with various semantic roles. We proposed that passives are formed only by application while potentials are formed by application or composition. Therefore, only the theme arguments of base verbs can be subjects in passive sentences, as expected. So the problem may happen only if the base verbs have more than one objects like ditransitive verbs. On the other hand, any argument or adjunct can be subjectivized in potential sentences, which we want to explain. Let us take a look at derivation (14) as a first approximation:  (14) kono naifu-ga this knife NNom  yasai-o kantan-ni  o-kiri-ninar-(ar)e-ru  vegetables easily  HP-cut-HS-Can-Pres  NACC  VP\VP  (((S\NPRO)\◇NInst)\NAcc  λx.λy.can'(cut'(y)(PROH)(with_x) <B  S\NPInst: λx.can'(cut'(vegetables)(PROH)(with_x)  As mentioned above, we take adjuncts to be of e-type as optional arguments to base verbs, not of <<e,t>,<e,t>> type, which is assigned to typical verb phrase modifiers. Notice that the accusative object can be marked with nominative if function composition is applied before it is consumed by application. Composition rule can be generalized as (15), so that we can easily transmit any number of arguments/adjuncts to the resulting category. Then the features of missing instrumental and theme arguments may be inherited to the predicate phrase.  (15) Backward Composition (<B):1 Y\Z$ X\Y ⇒ X\Z$ where the symbol Z$ stands for Z and all lefttward-looking functor categories into Z.  In derivation (14) we encounter another problem. The derived predicate phrase is the functor looking leftward for the adjunct which seems to convey oblique case, but the corresponding subject is marked with nominative case, so case conflict arises here. This is a very important point to be accounted for to deal with the notion of subject in stative sentences in languages like Japanese. In the generative grammar tradition, the notion of major subject has been appealed to, which is not directly licensed by the argument structure of a verb, but by a whole predicate phrase containing some gap. Typical example of a sentence containing the major subject can be shown in (16), which has been referred to as a so-called multiple subject construction.  
 Text documents often contain valuable relations among entities. For example, in the sentence taken from the WSJ corpus:  There's a generally more positive attitude toward the economy, said Bette Raptapoulos, analyst for  Prudential-Bache Securities Inc., …  (1)  there are relations: “Bette Raptapoulos” is-a analyst, and analyst for “Prudential-Bache Securities Inc.” Such relations may be beneficial in many NLP applications, such as for answering Who and List questions, e.g., “Who is Bette Raptapoulos?'' or “Give me the list of analyst for PrudentialBache Securities Inc.” Relations in text documents can be extracted by pattern extraction as in (Brin 1998). (Brin 1998) presented Dual Iterative Pattern Relation Extraction (DIPRE), and used DIPRE to extract 〈author, title〉 tuples describing the relation: the author of the book title is author. Based on Brin's model, (Agichtein and Gravano 2000) presented the Snowball system to extract 〈organization, location〉 tuples indicating that the headquarters of organization is in location. (Nguyen and Shimazu 2007) developed the PCE system for extracting 〈person, category〉 tuples describing that the person is-a category. This study proposes to automatically extract quadruples 〈ne, category, related-to, object〉 describing that the named entity ne ISA category, and the category IS-RELATED-TO object. We call such relations “named-entity is-a category” relation, and “category related-to object”  * This study was supported by Japan Advanced Institute of Science and Technology, the 21st Century COE Program: “Verifiable and Evolvable e-Society”. Copyright 2007 by Tri-Thanh Nguyen and Akira Shimazu.  349  relation. The related-to and object of a quadruple may be null. We extend PCE algorithm to extract quadruples, and build a semantic search system to utilize extracted quadruples for answering some types of questions. The remainder of this paper is organized as follows: Section 2 summarises some related work. Section 3 describes the original PCE algorithm and our extraction model. Section 4 gives a possible application of the extracted quadruples; Section 5 presents experiments and evaluation; Conclusions are given in the last section.  2. Related work (Brin 1998) presented the DIPRE algorithm for extracting relations, and used DIPRE to extract (author, title) tuples having the relation: the author of the book title is author. Starting with a small number of (author, title) seed tuples, DIPRE finds the occurrences of tuples in order to generate new patterns. New patterns are, again, used to extract further (author, title) tuples. The DIPRE algorithm is graphically depicted in Figure 1. Based on DIPRE, (Agichtein and Gravano 2000) introduced another method of generating new patterns, and developed the Snowball system for extracting (organization, location) tuples expressing the relation: the headquarters of organization is in location. Current Named Entity Recognition (NER) systems often operate based on a predefined set of named entity classes, and assign a unique class to a discovered named entity (Chieu and Tou 2003). This is not natural, since the potential classes of named entity is large, and a named entity may belong to more than one class. For example, a person named entity may be both “executive vice president” and “chief financial officer” as expressed in the sentence: “Daniel Akerson, executive vice president and chief financial officer, said MCI's growth is being fueled by …” With the purpose of extending the number of named entity classes, (Nguyen and Shimazu 2007) proposed the “Person Category Extraction” (PCE) algorithm to automatically extract fine-grained categories of person Named Entities from text corpora. Based on DPIRE, (Nguyen and Shimazu 2007) introduced new types of patterns based on part-of-speech (POS) and chunk tags. One more proposal of their study which improved the perfomance of PCE a lot was the use of a validation function in the extraction procedure. Details of the PCE algorithm are provided in Section 3.1.  3. Extraction system In this section, we describe the original PCE algorithm, and our extension for extracting 〈ne, category, related-to, object〉 quadruples.  4. PCE algorithm  The PCE algorithm is depicted in Figure 2, and the description is given in Figure 3. Starting  with two seed patterns, PCE extracted  〈person, category〉 tuples. The extracted  tuples were used to extract occurrences  Find occurrences  of 〈person, category〉 tuples in texts for  generating new patterns. Again, new  patterns were used to extract new tuples. The process terminated when no more  Tuples  Web documents  Generate patterns  patterns were produced. A pattern is  defined as a 4-tuple:  (order,  person_slot,  category_pattern),  middle,  Search tuples using patterns  where order (a Boolean value) indicates the occurrence order of person and  Figure 1: The DIPRE model.  350  category in a sentence; person_slot is a slot which will be replaced with a person named entity; middle is the string surrounded by person and category; category_pattern is defined as: category_pattern:=noun_phrase1 (and noun_phrase2)?1  where noun_phrasei is a regular expression that matches a noun phrase with added POS tags.  In order to extract new 〈person, category〉 tuples, for every sentence s, from a pattern (whose order is true), and for each person NE named_entity in s, we construct a regular expression:  * named_entity middle category_pattern *  If s matches the above regular expression, the 〈person, category〉 tuple is extracted according to the algorithm in Figure 4, where is_valid(category) is a function that returns true if category is a sort of ‘person’, and false otherwise. The purpose of this function is to ignore unexpected matches, i.e., matches that give incorrect tuples. The is_valid function operates based on the fact that if a person is-a category, then the category must be a sort (or subtype) of person. Since a category is a sort of person is equivalent to the category is a hyponym of person (or person is a hypernym of the category), this constraint is checked by using WordNet (Fellbaum 1998) which contains hyponymic and hypernymic relations among concepts. When order is false, named_entity and category_pattern are switched. PCE can extract two tuples from a match, if there are two.  Occurrences: An occurrence of a 〈person, category〉 tuple is defined as a 4-tuple:  (order, person, middle, category),  where middle is a string surrounded by person and category. An occurrence of a (person, category) tuple is extracted if a sentence s matches the regular expression:  * person middle category *  or  * category middle person *  After extracting occurrences from the text corpus, they are used to generate new patterns. However, a middle of an occurrence is not necessarily reliable, (Nguyen and Shimazu) proposed a method to retain reliable ones based on two criteria: repetition and diversity as follows:  Repetition of a middle (repetition(middle)) is the number of times the middle appears between the person and category of (person, category) tuples of same person.  Diversity of a middle (diversity(middle)) is the number of times the middle appears between the  person and category of (person,  category) tuples of different persons.  A  middle  that  has  Extract tuples  L Tuples  repetition(middle)>thresholdR seems reliable and is kept. A pattern seems specific if it is generated based on tuples of a person, so only middles that have  P Patterns  D Text corpus  O Extract occurrences  diversity(middle)>thresholdD are kept to make the generated patterns general (Condition 1).  Generate patterns from occurrences  Figure 2: The PCE model.  
Syntactic parsers often produce parses many of whose parts are correct but which do contain partially inaccurate structures. To attain a high level of precision and recall, serious research has to be conducted and just about every kind of research in this field may well benefit from a moderate amount of manually parsed, i.e, correctly parsed, sentences. When it comes to the Korean language, a widely available, yet dependable corpus of parsed sentences is something that has yet to be constructed. In this paper, we describe a move that will help build a parsed corpus of about one thousand sentences. Our idea here is to use an initial stage probabilitic CFG parser with a partially bracketed input. This work is inspired by Pereira and Schabes (1992), who use bracketed sentences to induce grammar rules automatically. We will describe our initial stage parser, Paak, in Section 2. How the input string with some bracketings are processed in this parser is described in the next section. Section 4 gives algorithms for keeping track of the bracketing information and for affecting the probabilities of constituents, which conform to, are consistent with, or contradict the information annotated in the input. The performance gains at each degree of bracketing information are estimated in Section 5, where it is shown that no more than two pairs of bracketings suffice to yield the correct parse of a sentence of length twenty. 2. The probabilistic CFG parser Paak Paak is a probabilistic parser of the Korean language with a set of context-free grammar rules. It relies on a tagger of Korean for its analysis of input string into the smallest units of syntactic * I am grateful to one of the anonymous referees of the Program Committee of PACLIC 21, who gave me useful comments. Copyright 2007 by Yongkyoon No 358  processing.1 The tagger uses fifty three tags, many of which correspond to parts-of-speech. Paak has 45 preterminal symbols and 20 nonterminals. Currently it has just over three hundred PCFG rules. The parser is a probabilistic parser implemented in Java. The implementation basically follows Charniak(1993: 1.4). It is a bottom up parser. Its grammar rules have been extracted from a very small hand-parsed corpus of twenty sentences.  3. Adding constituent structures to the input string The parser takes an input file and gives an output file. Its input file normally consists of sentences of the language. What we suggest is add information about the input sentence's constituent structures so as to guide the parser to its correct parse. The input string itself may have bracketings added to them. Adding bracketings to a string of words would be the easiest way of annotating their syntactic structure. (1) a. 일상생활에 지장이 있는 정도는 아니라지만, 오른쪽 목과 어깨의 통증으로 고생이 심하다. b. (일상생활에 지장이 있는 정도는 아니라)지만, (오른쪽 목과 어깨의 통증)으로 고생이 심하다. c. (일상생활에 지장이 있는 정도는 아니라)지만, (오른쪽 (목과 어깨)의) 통증으로 고생이 심하다. d. (((((((일상생활)에) (지장이) 있는) 정도)는 아니라)지만), ((((오른쪽 ((목과) 어깨))의) 통증)으로) ((고생이) 심하다).) The first string above, (1a), has no information added. It is the kind of input string ordinary parsers would expect. (1b) has two constituents marked with pairs of parentheses. The next string has three constituents marked and the last, (1d) has all its nonlexical constituents marked. None of these strings mark their constituents with labels, however. We are to allow our parser to process all of the input types given in (1). A special provision has to be made in order to distinguish the ordinary characters `(' and `)' from our metacharacters for marking the boundaries of constituents. The parser ought to identify the metacharacters and their positions and then has to remove them before it passes them to the tagger. The parser will make use of the partial constituency information in calculating the input's constituents. If the constituent being built by the parser at a given point is one of the spans indicated by the bracketings, its probability is to be heightened. If the constituent being built is in conflict with a bracketing marked in the input, its probability is to be lowered. As the probability of a parse is the product of the probabilities of all its parts, making a constituent's probability higher or lower directly affects the probability the parse of the whole sentence ends up getting.  4. Algorithms Getting the parser to make use of preannotated constituency involves two sorts of algorithms. The first is to identify the positions of the opening and closing brackets in such a way that they correspond to boundaries of the words to be fed to the parser. As the words to be fed to the  
a School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, Scotland, United Kingdom b Faculty of Informatics, Osaka Gakuin University, 2-36-1 Kishibe-minami, Suita, Osaka 564-8511, Japan {aotani, steedman}@inf.ed.ac.uk  Abstract. This paper investigates the nature of Japanese argument cluster (Steedman 2000b). Based on Combinatory Categorial Grammar, a type-raising analysis of case particles which captures some aspects of the information structure in Japanese is discussed, including contrastive interpretation of coordination, wh-constructions, and some theme and rheme-related grammatical phenomena. These observations offer further support for the study of syntax, semantics, and phonology interface and the earlier analysis of English information structure. Keywords: argument cluster, Japanese case particles, information structure, Combinatory Categorial Grammar (CCG), coordination, wh-constructions, multiple-ga, theme, rheme 1. Introduction Steedman (2000b: p.172) accounts for a number of facts about “non-constituent” coordination in Japanese by allowing type-raised subject and object NPs in Japanese to combine not only by forward application to the verb (>), as in (1b) below, but also by forward-composition (>B), as in (1c) below, under the framework of Combinatory Categorial Grammar: (1)  * We are indebted to Takeo Kurafuji and an anonymous PACLIC21 reviewer for their invaluable comments on an earlier version of this paper. Our thanks also go to Yoshiyasu Shiraii, the president of Osaka Gakuin University. All remaining inadequacies are our own. Copyright 2007 by Akira Ohtani and Mark Steedman 365  The derivation in (1b) is isomorphic to a standard phrase structure analysis, as in (2a) below. However, the derivation in (1c) allows the NPs to compose non-standard constituent cluster, as shown in (2b): (2) In this paper, we discuss the nature of non-standard constituent cluster in Japanese. Based on Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000b, Steedman and Baldridge 2007), we propose a type-raising analysis of case particles which captures some aspects of the information structure in Japanese/Korean. 1 This account of the information structure of argument clusters offers further support for the earlier analysis of English information structure. 2. A CCG Analysis of Gapping 2.1.Gapping Revisited One motivation for the non-standard structure in (2b) is the tendency of argument clusters to act like constituents under coordination, and the relation of this phenomenon to the base order of constituents across SOV, VSO and SVO language and/or construction (Ross 1970). Ross's generalization follows as a theorem from the axioms of CCG, as illustrated by Japanese gapping construction like (3a) and the possibility of the non-standard constituent illustrated in (1c) as its part, as in derivation (3b): (3) The possibility of semantically surface-compositional syntactic derivations like this is one of the main theoretical attractions of CCG. The availability of two different derivations in (1b) and (1c) for sentences like (1a) allows us to consider the possibility that they are semantically and pragmatically distinct in some way. There are some specific properties of gapping that are interesting in this respect. See below: (4) In English, the second occurrence of the verb visited in (4) can be gapped. The arguments left in the gapped conjunct are in a contrastive relation to the correspondents in the full conjunct.2 Note that the same intuition is hold for Japanese gapping sentence in (3a), repeated as (5): 
Our method provides zero pronoun identification as a preliminary process for long distance dependency (LDD) resolution, based on the morphology of verbs and on the probability of subcategorization frames, associated with particular verbs. This paper has the following structure: in Section 2 we summarize the background of this research, including LFG and related work. In Section 3, we describe in detail our method of automatic annotation of f-structure functional equations on KTC4, and show how we approach the problem of zero-pronoun identification and present results of our f-structure annotation and * We gratefully acknowledge support from Science Foundation Ireland grant 04/IN/I527 for the research reported in this paper. Copyright 2007 by Masanori Oya and Josef van Genabith 375  parsing experiments. We discuss the overall results and their implications in Section 4, and conclude in Section 5.  2. Background  2.1 Lexical-Functional Grammar Lexical-Functional Grammar (LFG) (Bresnan 2001; Dalrymple 2001) is a syntactic theory in which there are two levels of representation: c-structures are phrase-structure trees, and fstructures are attribute-value matrices encoding abstract grammatical relations such as subject, object, oblique or adjunct, mapped from the c-structure through functional equations annotated to c-structure nodes. Figure 1 is the c-structure for the sentence “Taro went to Seoul”, and Figure 2 is the f-structure for the same sentence:  S  NP  VP  (↑SUBJ)=↓  ↑=↓  Taro 
More recently, a large corpus annotated with linguistic information is used in natural language processing. By using this corpus, natural language processing systems have learn some linguistic phenomena automatically. Building such a corpus, however, is an expensive, laborintensive and time-consuming work. Furthermore, maintaining consistency of a constructed corpus is difficult. Therefore, we need an annotation tool for improving annotation efficiency and maintaining consistency. To help such work, some annotation tools (Atalay, 2003; Lim, 2002; Morton, 2003; Day, 1997; Brants T. and Plaehn, 2000) have already been used. In this paper, we describe an annotation tool for building a Korean dependency tree-tagged corpus with linguistic information about the segmentation of word phrases (called eojeols in Korean), partof-speech (POS) tags, boundaries of chunks, and dependency links and relations. We design an annotation tool so that an annotator can carefully investigate them and edit errors on them through a GUI. We also design it so that errors in low level processing like POS tagging might not be propagated to higher level processing step like parsing. Moreover, the tool is characterized by the following features; 1) It is independent of special applications like information extraction. 2) It focuses on localizing errors to modules as far as possible, such as morphological analyzers. It can make annotators find and pay attention to errors related to the modules easily. 3) It has an error checking function to make possible that errors can not be stored as it can be. 4) It promptly shares annotated information among annotators so that annotators can keep consistency with others’ annotation within a working group. 5) It has a user-friendly interface. This paper is organized as follows: In Section 2, we introduce other annotation tools for establishing corpora In Section 3, we describe the architecture of our annotation tool for ∗ Copyright 2007 by Eun-Jin Park, Jae-Hoon Kim, Chang-Hyun Kim, and Young-Kill Kim 385  building a Korean dependency tree-tagged corpus. In Section 4 and 5, we explain the implementation details of our tool and guide process of the annotation using our tool, respectively. Finally, we draw conclusions, and discuss future works in Section 6. 2. Related Works Several annotation tools (Atalay, 2003; Day, 1997; Lim, 2002; Morton, 2003; Day, 1997; Brants T. and Plaehn, 2000; Carletta, 2002) have been developed and used in several projects (KIBS, 2005; Marcus, 1994; SEJONG, 2005). In this section, we briefly introduce such annotation tools for building a tagged corpus: Alembic workbench (Day, 1997), WorkFreak (Morton, 2003) and a semi-automatic tree annotating workbench (Lim, 2002) developed in the Sejong Project (SEJONG, 2005). 2.1.Alembic Workbench Alembic Workbench (Day, 1997) developed at MITR1 is the system which annotates namedentity for an effective information extraction system. It supports multi-languages and SGML formats. Also it learns the user’s working pattern to construct the corpus semi-automatically. It helps annotators by graphic user-interface. This system had been upgraded and released as Callisto2 in 2004. In spite of such upgrade, this system is not yet for general purpose, only information extraction. Furthermore, adapting it to Korean requires preprocessing like morphological analysis and POS tagging. 2.2.WordFreak WordFreak 3 (Morton, 2003) is a java-based linguistic annotation tool designed to support automatically annotating linguistic data and it employs active-learning for the human correction of automatically annotated data. It annotates several linguistic information like syntactic structure, named-entity and anaphoric information, etc. It provides automatic taggers for tokenization, POS tagging, chunking, full parsing, and name finding through OpenNLP4 project and also automatically annotates linguistic information by learning the user pattern of work. And it can extend its component to other languages like Korean easily, but also requires preprocessing like morphological analysis and POS tagging for each language. 2.3.Korean semi-automatic tree annotation workbench In this section, we will describe the workbench (Lim, 2002) which is building a Korean dependency tree-tagged corpus. It extracts various syntactic patterns from the constructed corpus based on the selected features, and automatically applies the extracted syntactic patterns to the appropriate states. It provides an integrated environment for searching, converting and editing Korean parsing tree corpus in the Sejong project (SEJONG, 2005). However, it is for a stand-alone system, but not for a multi-user system, and then cannot share annotated information instantly. It is improper for building a large-scale corpus. 3. Annotation Tool for Building a Korean Dependency Tree-Tagged Corpus Our work annotates naturally-occurring text for linguistic structure. Most notably, we produce skeletal dependency trees with links and relations showing rough syntactic and semantic information called Korean dependency tree-tagged corpus. We also annotate text with 
1. Introduction Human languages enjoy the common rule of economy: when some word or words are repeated, we do not verbally pronounce what is repeated. This linguistic phenomenon is called ellipsis or, in more technical terms, deletion, which is to delete or suppress the phonological features of repeated word/words in the course of syntactic derivation. Deletion is known to be rather widely available to the relevant contexts in English. It has been noted that there are three types of ellipsis depending on what constituent undergoes deletion: (i) VP ellipsis (or maybe vP ellipsis); (ii) NP ellipsis (in the DP system); (iii) T/IP ellipsis. What draws particular attention recently among the three types of ellipsis is the last kind, where wh-movement is mandatory before TP is elided. T/IP ellipsis or what Ross (1969) calls sluicing is illustrated by the following example:  (1) John met someone, but I don't know [CP [who] [TP John met t]]  The fact that wh-movement feeds sluicing raises a question of whether in contrast to English with single wh-fronting, multiple sluicing (sluicing with multiple survivors) is possible in languages with multiple wh-fronting. Bulgarian and Serbo-Croatian, which allow for multiple wh-fronting, make a test case for this question, and indeed multiple sluicing is attested in these two languages as follows:  (2) Njakoj vidja njakogo, no ne znam [koj] [kogo] [vidja] Bulgarian  someone saw someone, but not I-know who whom saw  Richards (1997)  (3) Neko je vidio nekog, ali ne znam [ko] [koga] [je vidio] Serbo-Croatian  
Presently, widespread use of text-to-speech technology is limited by its inability to produce highquality speech. That is, intelligibility and naturalness of synthetic speech is still not quite at the level acceptable by human listeners. In particular, the naturalness issue can be attributed to the lack of sophisticated prosody-generating scheme. Prosody is often described as a suprasegmental feature of speech (a term for describing phonological features of those aspects of speech that involve more than single consonants or vowels). Acoustically speaking, prosody can be defined as change in the fundamental frequency (F0), timing, and amplitude of a speech signal. Speakers control the prosody of an utterance in order to signal linguistic and affective information. Linguistic prosody is used by speakers to signal * The author would like to thank the Citadel Foundation for its financial support in the form of a presentation grant. Copyright 2007 by Siripong Potisuk 405  grammatical information at the syllable, word, or sentence level (e.g., stress, intonation). Affective prosody, on the other hand, is used to convey information that indicates speaker’s intentions, attitudes, or emotional states. In addition to linguistic and affective information, prosody can also be used to convey non-linguistic information concerning speaker’s personal characteristics such as age, gender, idiosyncrasy, speaking style, and physical condition. Such characteristics may or may not be under the speaker’s volitional control. It is part of the intelligibility and naturalness of his/her speech. This paper will deal with linguistic prosody only. The role of linguistic prosody in spoken language is similar to that of punctuation in written language. Punctuation is used to divide a stream of text into smaller segments such as a phrase, clause, or sentence, and thus, helps readers interpret the message according to the intention of the writer. Likewise, prosodic information helps listeners interpret a spoken utterance in the way the speakers intends. The need for punctuation or prosody can be attributed in part to the inherent ambiguity of natural language. Intuition tells us that intelligibility and naturalness of speech can be attributed to prosody. Some words in an utterance are louder and longer than others. Because function words are acoustically less prominent than the semantically important content words, such as nouns and verbs, we can prosodically distinguish them. Pauses tend to be inserted at certain points in the utterance, and words at the end of the utterance are likely to be lengthened. This suggests the existence of prosodic constituents that are used in the overall prosodic structure or melody of an utterance. Linguists have posited units such as syllables, prosodic words, phonological phrases, and intonational phrases. The use of prosody by speakers, in attempting to sound intelligibly and naturally, can be best exemplified by considering its use in ambiguous sentences. When two sentences are segmentally identical, a problem of identifying the correct meaning arises for listener, especially when the contextual information is not adequate. In such cases, the listener can make use of another type of information, namely prosody. The question arises, from the speaker’s point of view, as to how this information is decoded or associated with different meanings. At an abstract level, a commonly accepted hypothesis is that there is a direct relationship between the syntactic structure of a sentence and its prosodic structure as suggested by Selkirk (1984), and Nespor (1986). This hypothesis implies that an ambiguous sentence will have a different prosodic structure for each syntactic structure, and as such it can be used to determine the correct meaning. At the phonetic level, the speaker tends to manipulate the acoustic correlates of prosody, such as F0, segmental and pause duration, amplitude, and spectrum of the speech signal in order to signal prosody. The listener, in turn, will try to translate changes in these physical correlates into abstract linguistic concepts in order to arrive at the intended meaning of the utterance. As in human speech, it is believed that prosodic information can help improve performance of a text-to-speech system. Prosodic information is particularly helpful in generating synthetic speech because of lexical and structural ambiguities of written forms. Prosodic information could be used by computers to generate phonetically similar, but syntactically different utterances. In the following sections, a novel method for annotating prosody in a text-to-speech system will be described. The process will be abstractly described and demonstrated by using structurally ambiguous sentences involving different types of compounds in Thai. Vongvipanond (1993) concluded that compounds are a major cause of structural ambiguity in Thai and often create problems because of their high frequency of occurrence. Compounding is the most widespread word formation process in Thai. Structural ambiguities often result from compounds because Thai words lack inflectional and derivational affixes to indicate, for example, subject-verb agreement. Nevertheless, compounds can be prosodically distinguished from syntactic phrases by differences in stress patterns. In addition, the process of generating durational patterns for the utterance based on the prosodic annotation process will also be described. 406  2. Text Processing Text processing is considered one of the many important aspects of a text-to-speech system involving language modeling. A language model often consists of a grammar written using some formalism which is applied to a sentence by utilizing some sort of parsing algorithm. One popular example is a set of context-free grammar (CFG) production rules, which is based on a phrasestructure representation of syntax by Chomsky (1963), can be used to parse sentences in the language defined by that grammar. A phrase-structure grammar uses a phrase-structure tree (PStree) to describe the groupings of words into the so-called constituents at different levels of sentence construction. A PS-tree shows which items go together with other items to form tight units of a higher-order, a distributional characteristic of a grouping within a larger grouping. Syntactic class membership is a way of labeling syntactic roles in a PS-tree because a PS-tree does not and cannot specify the types of syntactic links existing between two items in a natural and explicit way. Another approach to syntactic parsing is based on dependency grammar. A dependency grammar describes the syntactic structure of a sentence by using a dependency tree (D-tree) to establish dependencies among words in terms of head and dependents. A D-tree shows a relational characteristic of the syntactic representation in the form of hierarchical links between items, i.e., which items are related to which other items and in which way. In contrast to PS-tree, class membership is not specified in a D-tree. Instead, a D-tree puts a particular emphasis on specifying in detail the type of any syntactic relation between two related items. Such syntactic relations are, for example, predicative, determinative, coordinative relations, etc. 
This paper will focus on the ACE RDC task1 and employ kernel method to extract semantic relationships between named entity pairs. Many feature-based approaches transform relation instances into feature vectors of high dimension, and compute the inner dot product between these feature vectors. Current research (Kambhatla 2004, Zhao et al 2005, Zhou et al. 2005, Wang et al. 2006) shows that it is very difficult to extract new effective features from relation examples. Kernel methods are non-parametric estimation techniques that computer a kernel function between data instances. By avoiding transforming data examples into feature vectors, kernel methods can implicitly explore much larger feature space than could be searched by a * This research is supported by Project 60673041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the “863” National High-Tech Research and Development of China. We would also like to thank Dr. Alessando Moschitti for his great help in using his Tree Kernel Toolkits, including binary package and source codes. Copyright 2007 by Longhua Qian, Guodong Zhou, Qiaomin Zhu, Peide Qian 
So, researchers have created methods to automatically do summarization task and extract the most important concepts of the information without any need to involve humans. Now a second question emerges. How could we know about the correctness and the completeness of the results generated in an automatic manner? The answer is hidden in evaluation techniques on which we are going to focus in this paper. Therefore besides the growth of summarization methods, evaluation techniques must improve and mature so as to provide acceptable scores for summaries according to some sort of evaluation metrics. Evaluation approaches mostly evaluate created summaries based on (1) ideal (gold) summary, (2) use in an application and (3) original document. Techniques from the first category compare system generated summaries with a summary known as the best possible summary! This ideal summary currently is created by humans and it could be influenced by subjective effects from judges. Application driven techniques evaluate content of summary by analyzing the level of information that could be obtained from it for a specific task. In this approach the performance of the application is analyzed when using the original document and ∗ Copyright 2007 by Mehrnoush Shamsfard, Amir Saffarian, Samaneh Ghodratnama 422  its summary separately. On the other hand evaluation by original document is really ambiguous because specifying evaluation parameters and metrics is hard but it is more natural. In this paper we focus on the first category of evaluation approaches; comparing with an ideal (gold) summary. Section 2 of this paper provides a brief explanation about the summarization methods and their high level architecture. Then in the following sections some recent methods for summary evaluation are introduced. 2. Document Summarization Text summarization is the process of extracting the most important parts of information from source document(s) to produce a reduced version for a particular user or a particular task. According to Mani (2001) automatic summarization is an automated process in which a computer takes a piece of information, also called the source (i.e. an electronic document), selects the most important content in it, and presents that content to the user in a condensed form. Automatic text summarization can be used in various areas of applications such as telecommunications industry, intelligent tutoring systems, text mining, and filters for web-based information retrieval and word processing tools. Researchers are also investigating the application of this technology to a variety of new and challenging problems, including Multilingual Summarization, Multimedia News Broadcast, Summarization of Online Medical Literature of a Patient, Audio Scanning Services for the Blind and Providing Captions for TV Programs. 2.1. High Level Architecture As a general view for summarizer architecture, the input to the system could be one or more documents in different forms of text or multimedia. The process of summarization has three main phases: 1. Analyzing the input text or Topic Identification 2. Transforming it into a summary representation or Interpretation 3. Synthesizing an appropriate output form or Generation Any summary can be characterized by (at least) three major classes of characteristics: (1) Input: characteristics of the source text(s), (2) Output: characteristics of the summary as a text (3) Purpose: characteristics of the summary usage which is discussed in more details in Hovy (2000). The output may be an extract of the source, or an abstract. Extracts consist of portions of text extracted verbatim, but abstracts consist of novel phrasings which describe the content of the original document(s). In general, producing abstracts requires stages of topic fusion and text generation, but producing extracts requires only the stage of topic identification. Moreover, summarizers can usually produce either generic or user–focused summaries. A user-focused summary is the one in which specific information is selected in order to satisfy the requirements of a particular user group. As opposed to that, a generic summary is more suitable for the average reader. There are also two types of summaries depending on their function. There can be informative and indicative summaries. The purpose of the first type is to deliver as much information as possible to the user and can also serve as a substitute for the source. Indicative summaries on the other hand are only meant to help the user decide whether or not to read the source document. 2.2. Approaches As defined by Mani (1999) there can be surface-level, corpus based, and discourse structure based and knowledge based approaches for single document summarization. All these approaches could successfully be applied to multi-document summarization. 423  The surface-level approach requires shallow understanding of the text and this usually involves analysis of the syntactic structure of sentences. It is used to extract salient information by taking into account some key features of a sentence. Corpus based approaches involve statistical analysis of large bodies of text (corpora) to find specific features about the documents in them. Human abstractors create a mental discourse model of a document while reading it. Discourse-level approaches try to create similar model of the discourse structure of a document which can later be used for the generation of a summary. Knowledge based approaches are used for the creation of summarization systems which act in a specific domain (i.e. domain dependent approaches). Such systems usually produce high quality summaries but do not have the ability to adapt to different types of documents (domains). Almost all of the summarizer systems in the world use a combination of approaches mentioned above. The final report of SUMMAC project by Mani (1998) listed some the systems in its time. For example, BT’s ProSum used statistical techniques based on the co-occurrences of word stems, the length of sentences and their position in the original text to calculate the importance of a sentence in the context of the overall text in which it occurs. The most important sentences are then used to construct the summary. CIR created a thematic representation of a text that included nodes of thematically related terms simulating topics of the text. Related terms were identified using a thesaurus specially constructed for this task. CGI_CMU used a technique called “Maximal Marginal Relevance” (MMR) which produces summaries of very long documents by identifying key relevant, non-redundant information found within the document. This technique is also used for eliminating or clustering redundant information in multi-document summarization applications. Cornell SabIR used the document ranking and passage retrieval capabilities of the SMART IR engine to effectively identify relevant related passages in a document. GE identified the discourse macro structure for each document and selected the passages from each component that scored well using both content and contextual clues. Currently, we are working to create a more updated list of single- and multi-document summarizer systems and classifying them by the methods they use to create summaries. 3. Summary Evaluation Text summarization is still an emerging field and serious questions remain concerning the appropriate methods and types of evaluation. There are a variety of possible bases for comparison of summarization system performance e.g., summary to source, machine to human generated, system to system. The problem with matching a system summary against a best of breed summary is that it is really hard and maybe impossible to find an ideal summary. Indeed, the human summary may be supplied by the author of the article, by a judge asked to construct an abstract, or by a judge asked to extract sentences. There can be a large number of generic and user-focused abstracts that could summarize a given document, just as there can be many ways of describing something. 3.1. Categories of Methods Methods for evaluating text summarization approaches can be broadly classified into two categories. 1. Intrinsic methods which are based on the comparison of the automatically produced summary with an ideal summary usually produced by human abstractors. 2. Extrinsic methods which are mostly based on tasks that use the output results of summarization systems. From another point of view we can divide evaluation methods into manual and automatic ones. In manual techniques an individual or a group of individuals compare machine or human 424  generated summaries (peers) to an ideal summary (model) or source text to find out about the extent of coverage of the main concepts between peer and model summaries. At the end an average of the scores given by group of judgments are used for the diversity of ideal summaries mentioned in the previous paragraph. In the other hand, automatic methods, without any human interference, try to reduce subjective effects that may influence evaluation scores. Members of this family may have different levels of complexity according to the level of details they use for comparison. Those that use lexical similarities as their main approach are the simplest and those which use semantics to find the coverage of concepts are the most complex techniques here. 3.2. Evaluation Metrics The comparison between summaries is best carried out by humans, but it can also be computed automatically. A variety of different measures can be used. Evaluation metrics can be grouped in at least three categories: (1) Sentence Recall measures, (2) Utility-based measures and (3) Content-Based measures (Mani, 2001-2). There are many metrics to evaluate summaries but precision and recall measures are used extensively in ideal summary based evaluation of summarization systems. However, they are not appropriate for the summarization task due to their binary nature and the fact that there is no single correct summary. An example of how the binary precision and recall measures fail the task is as follows: suppose there are two sentences which are interchangeable in terms of producing a summary. If five human subjects extract sentences to build a summary, two of subjects chose sentence 1, two of them chose sentence 2 and the 5th chose sentence 1 by chance. By majority method, sentence 1 will be an ideal sentence. In evaluation, if a summarization system chooses sentence 1, it wins, if it chooses sentence 2, it loses, although sentence 1 and 2 are interchangeable. This is obviously not a good measure (Jing, 1998). A tested summary should agree with model not only in content but also in length, an aspect which is measured by Precision. Combining both informativeness and brevity is possible through F-Score, a measure often used in Information Retrieval: There could be situations that a combinational metrics is used based on the several parameters. For example in Rigouste (2003) a recall metric is introduced which is using the unigram and bigram matching techniques mentioned by Lin and Hovy: 4. Pyramid, a Manual Evaluation Technique The motivation behind pyramid method (Passonneau, 2003) is that most of the contents which appear in human summaries are conceptions from the source text, expressed by different sentences or words. The fact behind generating subjective summaries that avoids any two summaries to be unique is that information units in them could be prioritized based on their importance in people views. Pyramid method is introduced to abstract and prioritize content units in source text considering this human behavior. Actually, a pyramid is made up of summary content units (SCU). There is no accurate definition of SCU because the granularity of information comprising it, is not clear enough. In this technique functional or semantic specifications of SCUs are not very important; however focus is on the way in which summaries are compared to find similar and non-similar SCUs. All 425  SCUs are weighted according to the frequency they appear in different summaries and after that each summary is scored based on the SCUs it is comprised of. More information about SCU and how to identify them could be found in DUC2005 SCU Annotation Guide1. Suppose the pyramid has n tiers, with tier Tn on top and T1 on the bottom. The weight of SCUs in tier Ti will be i . Let | Ti | denote the number of SCUs in tier Ti. Let Di be the number of SCUs in the summary that appear in Ti . Other SCUs in a summary that do not appear in the pyramid are assigned weight zero. The total SCU weight D is computed as in equation 3 and the optimal content score for a summary with X SCUs is shown in equation 4. With the help of Pyramid, score of a candidate summary (D) is computed with dividing the frequency of SCUs appeared in D by the value acquired by the best distribution of SCUs in an ideal summary (Max). The strengths of pyramid scores are that they are reliable, predictive, and diagnostic. There are also two problems with Pyramid method. First, pyramid scores ignore interdependencies among content units, including ordering. Second, creating an initial pyramid is laborious so large-scale application of the method would require an automated or semi-automated approach. DUC conference data of year 2005 and 2006 are analyzed using Pyramid method and the results are available in Nenkova (2005) and Passonneau (2006). There are two tasks involved in Pyramid evaluation: creating a pyramid by annotating model summaries, and evaluating a new summary (peer) against a pyramid. Ideally, an automated evaluation component would address both tasks. However, the task of creating a pyramid is far more complex than the task of scoring a new summary against existing (hand created) pyramid, and the automated scoring component is useful when doing a large amount of evaluation (of multiple summarizers, or different versions of the same summarizer). Pyramid creators have proposed a four step algorithm to score summaries according to the manually created pyramid. Steps are as follows (Harnly, 2005):  Enumerate Enumerates all candidate contributors (contiguous phrases) in each sentence of the peer summary.  Match For each candidate contributor, find the most similar SCU in the pyramid. In the process, the similarity between the candidate contributor and all pyramid SCUs is computed.  Select From the set of candidate contributors, find a covering, and disjoint set of contributors that have maximum overall similarity with the pyramid.  Score Calculate the pyramid score for the summary, using the chosen contributors and their SCU weights. More details including the reliability and robustness of the Pyramid method could be found in Nenkova (2007). 5. ROUGE, an Automatic Summary Evaluation System ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans (Lin ,2004). 
NMF is a dimensional reduction method and an effective document clustering method, because a term-document matrix is high-dimensional and sparse, from Xu et al. (2003). Let X to be a m × n term-document matrix, consisting of m rows (terms) and n columns (documents). If the number of clusters is k, NMF decomposes X to the matrices U and V t as follows: X = UV t where U is m × k , V is n × k and V t is the transposed matrix of V. The matrix U and V are non- negative. In NMF, each k dimensional column vector in V corresponds to a document. An actual clustering procedure is usually performed using these reduced vectors. However, NMF does not need such a clustering procedure. The reduced vector expresses its cluster by itself, because each column axis of V represents a topic of the cluster. Furthermore, the matrices V and U are * This research was partially supported by the Ministry of Education, Science, Sports and Culture, Grantin-Aid for Scientific Research on Priority Areas, “Japanese Corpus”, 19011001, 2007. Copyright 2007 by Hiroyuki Shinnou and Minoru Sasaki. 430  obtained by a simple iteration, from Lee (2000), where the initial matrices U 0 and V0 are updated. Therefore, we can regard NMF as a refinement method for a given clustering result, because the matrix V represents a clustering result. In this paper, we use NMF to improve clustering results. Providing NMF with an accurate document clustering result, we can ensure a more accurate result, because NMF is effective for document clustering. However, NMF often fails to improve the initial clustering result. The main reason for this is that the object function of NMF does not properly represent the goodness of clustering. To overcome this problem, we use another object function. After each iteration of NMF, the current clustering result is evaluated by that object function. We first need the initial clustering result. To obtain this, we perform min-max cut (Mcut) proposed by Ding et al. (2001), which is a spectral clustering method. Mcut is a very powerful clustering method, and we can obtain an accurate clustering result by improving the clustering result generated through Mcut, In the experiment, we used 19 data set provided via the CLUTO website. Our method improved the clustering result generated by Mcut. In addition, the accuracy of the obtained clustering result was higher than those of NMF, CLUTO and Mcut.  2. Refinement using NMF  2.1. Features of NMF  NMF decomposes the m × n term-document matrix X to the m × k matrix U and the  transposed matrix V t of the n × k matrix V , from Xu et al. (2003), where k is the number of  clusters:  X = UV t .  NMF attempts to find the axes corresponding to the topic of the clusters, and represents the  document vector and the term vector as a linear combination of the found axes.  NMF has following three features:  i. V and U are non-negative.  The element of V and U refers to the degree of relevance to the topic corresponding to the  axis of its element. It is therefore natural to assign a non-negative value to the element.  SVD can also reduce dimensions, but negative values appear unlike with NMF.  ii. The matrix V represents the clustering result.  The dimensional reduction translates high-dimensional data to lower-dimensional data.  Therefore, we usually must perform actual clustering for the reduced data. However, NMF  does not require this, because the matrix V represents the clustering result. The i-th  document di corresponds to the i-th row vector of V, that is, di = (vi1, vi2 ,L , vij ) . The  cluster  number  is  obtained  from  arg  max j  vij  .  iii. V and U do not need to be an orthogonal matrix.  LSI constructs orthogonal space from document space. On the other hand, in NMF, the axis  in the reduced space corresponds to a topic, therefore, these axes do not need to be  orthogonal. As a result, NMF attempts to find the axis corresponding to the cluster that has  documents containing identical words.  2.2. NMF algorithm For the given term-document matrix X, we can obtain U and V by the following iteration, shown by Lee (2000).  431  uij  ← uij  ( XV )ij (UV tV )ij  (Eq.1)  vij  ← vij  ( X tU )ij (VU tU )ij  (Eq.2)  Here, uij , vij and ( X )ij are the i-th row and the j-th column element of U, V and a matrix X  respectively.  After each iteration, U must be normalized as follows:  uij ←  uij ∑ ui2j i  The iteration stops by the fixed maximum iteration number, or the distance J between X and  UV t :  J = X − UV t  (Eq.3)  Here, J is the decomposition error.  2.3. Clustering result and initial matrices  In general, the initial matrices U 0 and V0 are constructed using random values. In this paper,  we construct the U 0 and V0 through a clustering result.  In particular, if the cluster number of the i-th data is clustered into the c-th cluster, the i-th row  vector of V0 is constructed as follows:  vij  =  ⎧1.0 ⎩⎨0.1  ( j = c) ( j ≠ c)  Here, U 0 is constructed via XV0 .  2.4. Problem of the object function of NMF We can use NMF as a refinement method for a clustering result, because the initial matrix of NMF corresponds to a clustering result. However, NMF often fails to improve the given clustering result. This is because the object function of NMF, that is, Eq. 3, does not properly represent the goodness of clustering. To confirm this problem, we performed NMF using the document data set ``tr45'' which is a part of the data set used in Section 5. The initial matrix was constructed using the clustering result obtained by Mcut. Figure 1 shows the results of this experiment. LINE-1 and LINE-2 in Figure 1 show the change in J in each iteration and the change in the clustering accuracy, respectively. From Figure 1, we can confirm that a smaller J does not always mean a more accurate clustering. To overcome this problem, we evaluated the current clustering result using another object function after each iteration of NMF. Specifically, we used the object function of Mcut. We calculated the value of the object function after each iteration of NMF. If the best value was not improved for three consecutive iterations, we stopped NMF.  432  49  46  810  800  LINE-1  790  decompostion error  780  770  760  750  43  40  37  34  31  28  25  22  19  16  13  10  7  4  
(1) Serial predicate constructions consist of two or more predicate (flanked by a complementizer) which denote sequential actions or states that denote a single coextensive or extended event. From a cross-linguistic perspective, Serial Verb Constructions (hereafter SVCs) are well-known for productivity. Dixon (2006:338) claims that ‘a SVC is a clearly recognizable, robust grammatical constructions type which carries a considerable functional and semantic load.’ Since the same goes for Korean, SVCs frequently appear in Korean, too. For example, (2) are extracted from the Sejong POS-tagged Corpora,1 which take mek- ‘eat’ as V2 within the frame of ‘V1 + e + V2.’ (2) nanwu-e mek-ta ‘divide and eat’, kkulhi-e mek-ta ‘boil and eat’, mandul-e mek-ta ‘make and eat’, cap-a mek-ta ‘catch and eat’, cip-e mek-ta ‘pick up and eat’, ssip-e mek-ta ‘chew and eat’, kwu-e mek-ta ‘broil and eat’, ppal-a mek-ta ‘suck and eat’,… * I would like to return thanks to Prof. Jae-Woong Choe, who helped this study forward. I also want to appreciate the comments of anonymous readers. Due to the comments, I could elaborate this paper. Of course, all errors are my responsibility. Copyright 2007 by Sanghoun Song 1 I extracted all data from these resources (morpheme-tagged corpora which cover ten million eojeol). According to inquiry into the corpora, 27% of verbs marked with ‘vv’ can be used as members in SVCs. 440  The purpose of this study is to provide an overall picture of KSVCs within the framework of the unification-based grammar2, in particular, Head-driven Phrase Structure Grammar (HPSG)3. This paper makes a fine-grained analysis of constraints on KSVCs, and also proposes the type hierarchies for KSVCs within the HPSG framework.  2. Basic data The expression form that this paper deals with is like (3), and examples that Sohn (1999:380) provides are given in (4).  (3) V1 + COMP[e/a, ko, eta] + V2  (4) a. Cihwan.i-nun ttek-ul son-ulo cip-e  mek-ess-e.  Cihwan-TOPIC cake-ACC hand-with pick up-INF eat-PST-INT  ‘Cihwan (picked up and) ate the rice cake with his fingers.’  b. mulkoki-ka kom-eykey cap-hi-e  mek-hi-ess-ta.  fish-NOM bear-by catch-PAS-INF eat-PAS-PST-DC  ‘The fish was (caught and) eaten by the bear.’  c. Milan.i-nun kapnag-ul an tul-ko ka-ss-e.  Milan-TOPIC bag-ACC not hold-and go-PST-INT  ‘Milan didn’t take her bag with her.’  d. wuli-nun tongkwul sok-ul  tuli-eta po-ass-ta.  we-TOPIC cave  inside-ACC put in-TR see-PST-DC  ‘We looked into the cave.’  Though the KSVC is a productive operation as aforesaid, yet there are selectional restrictions between V1 and V2. The constraints on how to combine with are exemplified below.  (5) a. mek- ‘eat’, masi- ‘drink’, cip- ‘pick up’, chayngki- ‘take care of, collect’ b. *masi-e mek-ta, *mek-e masi-ta c. cip-e mek-ta, *mek-e cip-ta d. chayngki-e mek-ta ‘take meals’, mek-e chayngki-ta ‘profiteer’  Verbs in (5b) cannot combine with each other regardless of their ordering. (5c) shows that the verb which denotes a manner is followed by the other verb, and the reversed order cannot be accepted. Both orders in (5d) are possible, but they have different meanings respectively. This paper gives an account of these restrictions with the typed feature structure of HPSG. While other researches generally have not regarded -ko and -eta as complementizers 4 (hereafter COMP) which is used to form KSVCs, Sohn (1999) says that -ko and -eta as well as e/a are used for KSVCs. Accepting his idea, I suppose constructions such as (4c-d) to be a sort of KSVCs. To be sure, if -ko in (4c) is regarded as a COMP for SVCs, it should be differentiated from its homonym, -ko ‘and,’ which is used for coordination constructions.  2 Sag and Wasow (1999:52) defines unification as below. Unification, then, is just a general method for allowing two compatible descriptions to amalgamate the information they contain into a single (usually larger) description. The function of unification implies that HPSG can provide a more superior solution to SVCs, because SVCs basically stand for the process that two or more verbs are unified into one single unit. 3 I find few studies have been done on KSVCs in HPSG. An overview of Korean grammar within the HPSG framework is given in Chang (1995) and Kim (2004), but they do not deal with KSVCs. 4 This term may be rather controversial because KSVCs are not complex clauses in a general sense. I tentatively define the suffix that attach to V1 as COMP in this paper, which is similar to a COMP of Chang (1995:16) or a complementizer affix of Kim (2004:52).  441  (6) Mia-ka phathi-eyse mek-ko masi-ess-ta. Mia-NOM party-LOC eat-and drink-PST-DC ‘Mia ate and drank at the party.’ Despite a superficial resemblance, there is an obvious difference between -ko in (4c) and -ko in (6). A tense marker such as -ess can attach to V1 mek- in (6), while it can not attach to V1 tul- in (4c). I treat the construction like (4c) as KSVCs on the ground of this difference. In the case of eta, I think it is a variation of -e/a, which has been formed through a historical development. Aikhenvald (2006) suggests that SVC functions like a single predicate to represent ‘One Event.’ Building upon her claim, I assume that two verbs combine with each other before anything else to be a single predicate. I also consider V2 the head of SVCs, because tense or aspect makers should attach to V2, which is similar to Chung (1995)’s structure.  Figure 1: The structure of KSVCs (Chung 1995:70)  3. Constraints In this section, I will inspect the constraints on KSVCs in terms of four aspects; transitivity, argument structure, semantic properties, and COMPs. In order to make this study based on more synthetic approach, I tried to collect relevant data in a systematic way from large corpora. To tell in the concrete, I made practical application of the Sejong POS-tagged Corpora. Implementing some programs which aim to extract the form ‘X/vv + X/ec + X/vv’ from the corpora, I could obtain first data. After that, I excluded problematic forms from the list in conformity to criteria for distinguishing between KSVCs and others. From now on, all analyses to constraints are grounded upon these demonstrative data.  3.1.Transitivity After investigating the data, I discover the forms that the transitivity of V1 is smaller than that of V2 or equal to are more common. However, there are also the cases that the transitivity of V1 is bigger than that of V2. For instance, cip-e ka-ta ‘pick up and go’ is made up of transitive V1 cip- and intransitive V2 ka-. In this case, it is interesting that V2 is a deictic verb almost invariably.5 This analysis is also applicable to the constructions which take -ko as a COMP; for example, tul[transitive]-ko ka[intransitive, DEIXIS +]-ta ‘hold and go’ in (4c). Meanwhile, I also observe that ‘V1[ditransitive] + V2[transitive]’ constructions are not SVCs, because their arguments cannot be unified.  3.2.Argument structure From the data shown below, I conclude that grammatical cases are unified into the single predicate which is composed of V1 and V2. And also (7) shows that oblique cases will not be constraints on KSVCs. They are merely subsumed into the unified argument structure.  (7) a. Mia-ka hak.kyo-ey kel-e  ka-ass-ta.  Mia-NOM school-DIR walk-INF go-PST-DC  ‘Mia went to school on foot.’  ket(kel)- ‘walk’ (NOM/AGT)  ka- ‘go’ (NOM/AGT, OBL/DIR)  5 It is analogous to Hashimoto and Bond (2005:153)’s analysis of V-V Compounds in Japanese. Another peculiarity involves the fact that the V2 is restricted to a monotrans verb that expresses a spatial motion, while the V1 is transitive and must not be a spatial motion verb.  442  b. Mia-ka  ppang-ul hak.kyo-ey cip-e  ka-ass-ta.  Mia-NOM bread-ACC school-DIR pick up-INF go-PST-DC  ‘Mia picked up the bread and went to school.’  cip- ‘pick up’ (NOM/AGT, ACC/THM)  ka- ‘go’ (NOM/AGT, OBL/DIR)  c. Mia-ka sakwa-lul khal-lo kkakk-a mek-ess-ta.  Mia-NOM apple-ACC knife-INST pare-INF eat-PST-DC  ‘Mia pared an apple with a knife and ate it.’  kkakk- ‘pare’ (NOM/AGT, ACC/THM, OBL/INST)  mek- ‘eat’ (NOM/AGT, ACC/THM)  d. Mia-ka  Cihwan.i-ekey chayk-ul sa-a  cu-ess-ta.  Mia-NOM Cihwan-DAT book-ACC buy-INF give-PST-DC  ‘Mia bought a book and gave it to Cihwan.’  sa- ‘buy’ (NOM/AGT, ACC/THM)  cu- ‘give’ (NOM/AGT, ACC/THM, OBL/DAT)  3.3.Semantic properties Maunsuwan (2000) introduces FIRST and LAST features into the analysis of Thai SVCs, because a ‘manner-of-motion’ verb is followed by a ‘deictic’ verb in Thai. (8) FIRST and LAST (Maunsuwan 2000:241) a. manner-of-motion verbs or verbs that entail motion are lexically marked as [FIRST +], meaning that a VP headed by a verb from this class must occur first in the SVC. b. verbs that take a deictic verb as complement are lexically marked as [LAST +], meaning that a VP headed by a verb from this class must occur last in the sequence of verb complexes. c. other non-deictic serial verbs are lexically marked as [FIRST boolean, LAST boolean] meaning that they are not constrained in their order of occurrence. Although Maunsuwan adopts double-headed structure for SVCs, the above solution is similarly available for KSVCs. But, it is so difficult to classify all verbs into subclasses only by intuition. Instead, this paper defines the feature of a verb by inductive methods based on large corpora. Since I extracted verbs’ frequency according to their distribution from the corpora previously, it does not fall into hard work. (9) a. [FIRST +, LAST −]: cip- ‘pick up’, kkekki- ‘be broken’, khay- ‘dig’, cec- ‘stir’, … b. [FIRST −, LAST +]: masi- ‘drink’, kku- ‘extinguish’, sey- ‘count’, kkaywu- ‘wake up’, … c. [FIRST boolean, LAST boolean]: ka- ‘go’, ket(kel)- ‘walk’, tani- ‘wander’, wus- ‘laugh’, , mek- ‘eat’, chayngki- ‘take care of, collect’, ccic- ‘tear’, chac- ‘find’, tat- ‘close’, … The restriction on ordering between V1 and V2, mentioned in (5b-c), can be solved with these typed feature structures, which will be presented in (18). There is, however, a weak point in the collection (9). The underlined items in (9) belong to the so-called motion verbs, but (9) cannot give a solution to discriminate between acceptability and unacceptability in (10). Since motion verbs play a significant role in SVCs in any kind of languages, the grammar for SVCs should take motion verbs into consideration.  (10) a. kel-e ka-ta, *ka-a ket-ta b. tani-e ka-ta, *ka-a tani-ta c. kel-e tani-ta, *tani-e ket-ta d. *wus-e ka-ta, *ka-a wus-ta Lee (1977) classifies motion verbs into two subclasses; one denotes a ‘manner-of-motion’, the other denotes a ‘spatial movement.’ In (10), ket(kel)- ‘walk’ expresses a ‘manner-of-motion,’  443  ka- ‘go’ is a typical deictic verb, and tani- ‘wander’ belongs to both the former and the latter. In line with Lee’s classification, I build up types for motion verbs as below. Figure 2: The types of motion verbs (11) a. [DEIXIS +, MANNER –] : ka- ‘go’ b. [DEIXIS +, MANNER +] : tani- ‘wander’ c. [DEIXIS –, MANNER +] : ket(kel)- ‘walk’ (11) offers a solution to the puzzle raised in (10a-c). On the other side, verbs which do not express motion, such as wus- ‘laugh,’ have the typed feature structure like [DEIXIS –, MANNER –]. This structure plays a role to block the ungrammatical construction such as (10d). With these types, I can seek an appropriate treatment for KSVCs which include motion verbs. The related constraints will be shown in (21) and (22). 3.4.COMPs The constructions with -e/a are classified into six subclasses with reference to each composition. They are exemplified in (12)6. (12) a. intransitive + intransitive : kel-e ka-ta ‘go on foot’ b. transitive + intransitive : cip-e ka-ta ‘pick up and go’ c. ditransitive + intransitive : ponay-e o-ta ‘send to me/us’ d. transitive + transitive : cip-e mek-ta ‘pick up and eat’ e. intransitive + transitive : ttwi-e nem-ta ‘jump over’ f. transitive + ditransitive : sa-a cu-ta ‘buy and give’ The construction with -ko has only one type such as ‘transitive + intransitive’ (e.g. tul-ko ka-ta ‘hold and go’). In this case, it is clear that the intransitive V2 has a [DEIXIS +] feature. In the case of the construction with -eta, although Sohn (1999) presents only one case whose V2 is po- ‘look’, there are various cases in my data. It is noticeable that the constructions with eta select their verbs in restricted lexicon. In other words, -eta constructions have a tendency to become lexicalized. (13) a. V1 in constructions with -eta: kaci- ‘have’, nay- ‘put out’, nay-li- ‘be set down’, tuli‘put in’, pili- ‘borrow’, chi- ‘hit’, … b. V2 in constructions with -eta: peli- ‘discard’, po- ‘see’, po-i- ‘be seen’, ssu- ‘use’, phal ‘sell’, … There are two types in -eta constructions. One is ‘transitive + transitive’ (e.g. tuli-eta po-ta ‘look inside’), the other is ‘intransitive[PASSIVE +] + intransitive[PASSIVE +]’ (e.g. nay-li-eta po-i-ta7 ‘be looked down’). 3.5.Summary Generalizing facts discussed so far, I sum up constraints on KSVCs as follows. 6 (12) is partially adapted from Lee (1994). 7 In this example, I come to the conclusion that nay-li- is transformed into passives after once becoming causatives (i.e. na-[root] → nay-[causative] → nay-li-[passive]). 444  (14) Constraints on KSVCs a. If the transitivity of V1 is bigger than that of V2, the V2 is an intransitive verb which has a [DEIXIS +] feature. b. Grammatical cases are unified into the argument structure of their mother-category, whereas oblique cases are subsumed. c. V1 has a [FIRST +] feature and V2 has a [LAST +] feature. d. If a SVC includes motion verbs, V1 has a [MANNER +] and V2 has a [DEIXIS +]. e. In -ko constructions, V2 is an intransitive with a [DEIXIS +]. f. In -eta constructions, the set of lexicon is rather restricted.  4. Type Hierarchies Dixon (2006:342), from a typological standpoint, claims ‘two basic varieties of SVC can be distinguished, asymmetrical and symmetrical.’ Asymmetrical constructions consist of some limited lexicon (e.g. motion verbs) and tend to become grammaticalized, whereas symmetrical constructions where both members come from an open class tend to become lexicalized. It is said that grammaticalization is the development from lexical expression to functional expression. The other way around, lexicalization refers to ‘process whereby concepts are encoded in the words of a language (O'Grady et al. 2005:212).’ In this context, it seems that a deictic verbs are under grammaticalization because the original meaning of ka- ‘go’ or o‘come’ is diluted in KSVCs. In contrast, -e/a constructions, the ordinary form of KSVCs, are inclined to become lexicalized. The obvious evidence is the so-called compound verb.  (15) a. Mia-ka pam-ul  kka-a mek-ess-ta.  Mia-NOM chestnut-ACC peel-INF eat-PST-DC  ‘Mia peeled a chestnut and ate it.’  b. Mia-ka yaksok-ul  kka-a mek-ess-ta.  Mia-NOMappointment-ACC peel-INF eat-PST-DC  ‘Mia forgot an appointment.’  (Oh 1997:26)  (15) shows that the expression such as kka-a mek-ta ‘peel and eat, forget’ is being lexicalized. mek-e chayngki-ta ‘profiteer’ in (5d), likewise, is the result of lexicalization, because it cannot convey senses of mek- ‘eat’ and chayngki- ‘take care of, collect’ wholly. In addition, -eta construction, as stated before, is another evidence for lexicalization of symmetrical KSVCs. If we remember members in -eta construction are both transitive or both intransitive[PASSIVE +], we can suggest the -eta construction shows a typical symmetry. In sum, we can divide KSVCs into two groups in accordance with compositionality.  (16) Compositionality of KSVCs a. Asymmetrical constructions: TRANSITIVITY(V1) > TRANSITIVITY(V2) b. Symmetrical constructions: TRANSITIVITY(V1) ≤ TRANSITIVITY(V2)  In this paper, I adapt above compositionality as a prominent branching node for type hierarchies. The reason why I consider compositionality as a major point of hierarchies is that the unification of argument structure mainly depends on compositionality. In asymmetrical constructions, the first complement of the mother-category is co-indexed with the first complement of V1. On the other hand, in symmetrical constructions, the complements of V2 are mainly transmitted to the complements of the mother-category. Another important criterion to decide its types is what COMP is made use of. The major types are sketched out below.  445  Figure 3: The major types of KSVCs (17) a. serial-ea-sym-intr-intr: kel-e ka-ta ‘go on foot’ b. serial-ea-sym-intr-tr: ttwi-e nem-ta ‘jump over’ c. serial-ea-sym-tr-tr: cip-e mek-ta ‘pick up and eat’ d. serial-ea-sym-tr-ditr: sa-a cu-ta ‘buy and give’ e. serial-ea-asym-tr-intr: cip-e ka-ta ‘hold and go’ f. serial-ea-asym-ditr-intr: ponay-e o-ta ‘send to me/us’ g. serial-ko: tul-ko ka-ta ‘hold and go’ h. serial-eta-intr: nay-li-eta po-i-ta ‘be looked down ’ i. serial-eta-tr: tuly-eta po-ta ‘look inside’ Korean syntactic structure presented below is adapted from Kim(2004:76). I would like to locate hd-serial-ex as a subclass of lex-ex, because V1 and V2 combined with each other to build a new verbal expression at the stage of lexical category. Figure 4: The revised syntactic structure including hd-serial-ex hd-serial-ex shares some properties with hd-lex-ex from the points that two verbs combine with each other so as to be a single verb, and the V2 is the head. But, hd-serial-ex draws a clear difference with hd-lex-ex in respect of argument structure. In hd-lex-ex such as the auxiliary construction, V2 takes V1 as its complement (see Kim 2004:123), whereas both V1 and V2 in hd-serial-ex do not take each other as complement. The whole type hierarchies that I propose are shown below. Figure 5: The whole hierarchies for hd-serial-ex Constraints for each type are given as follows. In particular, (18), the top node of KSVCs, indicates that V1 and V2 share the subject, they have a [FIRST +] feature and V2 has a [LAST +] feature, and the head of SVCs is V2. 446  (18) hd-serial-ex ⇒  ⎡VAL | SUBJ 1  ⎤  ⎢ ⎢⎢⎣ARGS  ⎡VAL | SUBJ 1⎤ ⎡VAL | SUBJ 1⎤  ⎢⎣FIRST +  ⎥ ⎦  ,  H ⎢⎣LAST  +  ⎥ ⎦  ⎥ ⎥ ⎥⎦  (19) serial-ea-sym-ex ⇒ [ARGS [VFORM ea], [ ] ]  (20) serial-ea-sym-intr-ex ⇒  ⎡VAL | COMPS 1  ⎤  ⎢⎣ARGS  [  ], [VAL | COMPS  1]  ⎥ ⎦  (21) serial-motion-ex ⇒  ⎡ ⎢ARGS ⎢⎣  [MOTION  |  MANNER  +],  ⎡MOTION | DEIXIS + ⎢⎣VAL | COMPS [CASE  |  SCASE  scase]  ⎤ ⎥ ⎦  ⎤ ⎥ ⎥⎦  (22) serial-nonmotion-ex ⇒ [ARGS [MOTION | DEIXIS −], [MOTION | DEIXIS −] ]  (23) serial-ea-sym-intr-tr-ex ⇒ [ ] [ ARGS VAL | COMPS ], [VAL | COMPS [CASE | GCASE acc], ... ]  (24) serial-ea-sym-tr-ex ⇒  ⎡VAL.COMPS 1, ...  ⎤  [ ] [ ] ⎢ ⎢⎣ARGS  VAL | COMPS 1, ... , VAL | COMPS 1[CASE | GCASE acc], ...  ⎥ ⎥⎦  (25) serial-ea-sym-tr-tr-ex ⇒  ⎡VAL | COMPS [ ], 1  ⎤  ⎢ ⎢⎣ARGS  [VAL | COMPS  [  ], 1  ], [VAL | COMPS  [  ]  ]  ⎥ ⎥⎦  (26) serial-ea-sym-tr-ditr-ex ⇒  ⎡VAL | COMPS [ ], 1, 2  ⎤  ⎢ ⎢⎣ARGS  [VAL | COMPS  [  ], 2  ], [VAL | COMPS  [  ], 1  ]  ⎥ ⎥⎦  (27) serial-eta-ex ⇒  ⎡VAL | COMPS 1  ⎤  ⎢  ⎥  ⎢⎢⎣ARGS  ⎡VFORM eta ⎢⎣VAL | COMPS  ⎤ 1⎥⎦  ,  [VAL  |  COMPS  1]  ⎥ ⎥⎦  (28) serial-eta-intr-ex ⇒ [ARGS [PASS +], [PASS +] ]  (29) serial-eta-tr-ex ⇒ [ ] ARGS [ ], [VAL | COMPS [CASE | GCASE acc], ... ]  (30) serial-asym-ex ⇒  ⎡VAL | COMPS 1, ...  ⎤  [ ] ⎢ ⎢⎣ARGS  VAL | COMPS 1[CASE | GCASE acc], ...  ,  [MOTION  |  DEIXIS  +]  ⎥ ⎥⎦  (31) serial-ea-asym-ex ⇒ [ARGS [VFORM ea], [ ] ]  (32) serial-ea-asym-tr-intr-ex ⇒  ⎡VAL | COMPS [ ],1  ⎤  ⎢ ⎢⎣ARGS  [VAL | COMPS  [  ]  ], [VAL | COMPS  1]  ⎥ ⎥⎦  447  (33) serial-ea-asym-ditr-intr-ex ⇒  ⎡VAL | COMPS [ ],1, 2  ⎤  ⎢ ⎢⎣ARGS  [VAL | COMPS  [  ], 1  ], [VAL | COMPS  2]  ⎥ ⎥⎦  (34) serial-ko-ex ⇒  ⎡VAL | COMPS [ ],1  ⎤  ⎢  ⎥  ⎢ ⎢ARGS ⎢⎣  ⎡VFORM ko ⎢⎣VAL | COMPS  [  ]  ⎤ ⎥  ,  [VAL  |  COMPS  ⎦  1]  ⎥ ⎥ ⎥⎦  Finally, referring to Kim and Yang (2006), I suggest Head-Serial-Lex Rule as (35) for the semantic representation of KSVCs.  (35) ⎡LEX + ⎢ ⎢ ⎢⎢C - CONT | RELS ⎢⎣  ⎤  ⎥ ⎡v-word ⎤ ⎡v-word ⎤  ⎡serial-rel ⎤  ⎢⎢L  -  IND  
Research on tombstones thus backs the study of language and culture. Creating a corpus of tombstones, as opposed to other research designs used with to tombstones, requires most ∗ Copyright 2007 by Oliver Streiter, Leonhard Voltmer, Yoann Goudin 450  investments, but is also the most rewarding strategy. First, a corpus can reveal facts, such as local, temporal, ethnic, religious, social or gender-related differences, that cannot be learned from individual tombstones. In addition, a corpus, when properly balanced, paves the way to innumerable investigations beyond the initial motivation for the construction of the corpus. Third, a corpus with digital recordings such as photos as integral part can be continuously annotated, opening new perspectives with each new annotation. Finally, corpora from different resources can be used in comparative studies. Tombstone corpora cannot bridge the gap between the rapid extinction of cultures and languages (Wurm 1991) and the missing research activities in language documentation. Although one might hope that tombstones will still be recoverable after the death of a culture, factors like urbanization, industrialization, tourism and construction work or acid rain threaten the existence of these mute witnesses. Although after 100 years one might still find individual tombstones, any systematic comparison across regions, ethnicities or time periods would be difficult. Our study on Taiwanese tombstones confirms the precarious state, showing a massive loss of tombstones of the time before 1950. Figure 1: The density of tombstones compared to the density of the population in Taiwan through time. The lack of older tombstones cannot be explained through a smaller population in earlier times. Data are based on 3000 tombstones from 30 graveyards. 2. Annotating Tombstones Different grass-root activities have sprung up, e.g. in the US and Australia, to preserve the cultural heritage of tombstones by photographing or transcribing them (e.g. http://www.rootsweb.com/~cemetery/). However, the nature of the transcriptions determines the use one can make of them, cf. Debartolo Carmack 2002. Unstructured transcriptions, for example, leave too much ambiguity for automatic analysis. The word 'Brown' might be a name or a color, 'Miller' a name or profession. 'Brown' thus should be annotated as 'name' and 'Miller' as 'profession'. To achieve this we use textual segments as 'date', 'location', 'epitaph' which describe the arrangement of the inscription. The knowledge of the textual segments allows for the determination of reference systems, references and meanings. Reference systems and references, as opposed to textual or editorial annotations, constitute a conceptual framework for corpus annotation which are important for cross-cultural and cross-linguistic comparisons. XML (Bray et al. 2004) is, without question, the best supported annotation meta-language. To our knowledge, however, no XML language for tombstone corpora has been developed so far. EpiDoc for example, aims at the annotation of Epigraphs (Anderson et al 2007) and developed a rich scheme for the textual elements on the basis of TEI (Sperberg-McQueen & Burnard 2002). 451  However, EpiDOC yet does not provide an annotation framework beyond the text, such as the description of graves, graveyards or their ethnic and religious environments. In addition, EpiDoc stresses the individuality of the object, given the function of the epitaphs as revelation of the individual personality (cf. Edgette 1989/1992). A corpus, however, serves a different purpose. In a corpus the individual stone, as well as any other individual feature is meaningless. When annotating a corpus, we annotate only those features which, beyond the purposes of data management and data retrieval, enter a system of meaningful oppositions (cf. Fages 1968). In terms of statistics, a feature is not annotated as long as there is no conjecture of a correlation with another feature. TSML is thus basically designed , for the annotation of features of correlation, developed on the basis of our experience with the massive annotation of Taiwanese tombstones and some tombstones from other countries. 2.1.TSML, Basic Structure Simplicity, uniformity and flexibility of TSML is achieved by using the <div> element in combination with a type-attribute as shown in Figure 2. We do not specify any constraints on the hierarchy of div-types as there are tombs without graveyards, tombs within tombs, graveyards without tombs, tombs in a church, tombstones without tombs and a tombstone-side, for example as photo, without the stone. Symbols, images, photos, maps can be at all levels, as well as texts which can be within the images, on the grave or on the tombstone.  Figure 2: The basic XML-structure of TSML based on div-elements and type-attributes, describing here an imaginary tomb. Type-attributes cascade from div to div.  Table 1: Values of the type-attribute of the div-element in TSML.  Explanation  graveyard graveyard_section church, temple, ... tomb tomb_side tombstone tombstone_side tombstone_unit text, p, w, c, stroke  Site where tombs are located. Graveyard sections may relate to different ethnicities or religions. A building related to cults which contains a grave or is located in a graveyard. A site containing the remains of one or more deceased. An inner or outer wall of a tomb. The tombstone as 3-dimensional object. A 2-dimensional view on the tombstone. Relatively independent units within a tombstone or a tombstone-side. Containing mainly text.  452  image symbol photo  Containing mainly an image Containing mainly a non-figurative symbol Containing mainly a photo  For all div-elements, attributes are assumed to be inherited (to cascade) from the mother divelement to the daughter div-element in the absence of a more specific value. The information contained by some of these attributes cannot be seen on the tomb or tombstone directly and must be inferred or measured from other sources (GPS, compass, map, archives).  Table 2: Attributes of the div-element to be inherited from mother-div to daughter. Attributes marked '*' have been suggested in this or similar way in Debartolo Carmack 2002.  Examples  Explanation  name*  Taipei Fude  description  location*  Taipei  caretaker*  Taipeishi  caretaker address*  composition*  marble  status*  abandoned  north east elevation direction orientation  5,88789 52,87465 417 90 downhill  side vertical set-up floating  inside 90 2001-09-01 right  display  block  background- color red  foreground-color red, green, ...  religion  Buddhism, ...  ethnicity  Hakka, Ami  language  eng, deu  writing-direction t2bl2r, l2rt2b  t2br2l, ..  script  Latin, Arabic  nb_of_tombs  301  nb_of_tombstones 417  Graveyards, graves may have official or unofficial names. Free text input Name of town, city, township where the entity is located. Caretakers might be contacted for additional information. Basic material: marble, slate, granite, sandstone, limestone, metal, brick, concrete, ceramics. Useful to explain data loss, data endangerment. Values: abandoned, maintained, overgrown, eroded, broken, lost. Latitude as decimal WGS84 datum (cf. NIMA 97). Longitude as decimal WGS84 datum (cf. NIMA 97). The elevation above mean see level in meter. Cardinal direction: 0=360=North, 90=East, 180=South, ... Non-compass directional system: uphill, downhill, upcoast, downcoast, upstre am, downstream, landward, seaward, lakeward, mountainward, streetward, concentric. Inside, outside with respect to the outer border of an object. 90=vertical, 0=horizontal, wall and roof respectively. Time of construction/ building/ writing/ photographing. Relative position within the mother div-element, observer position opposite to the orientation, as in CSS absolute position (Bos et al. 2007). Alternative values: right, left, top, bottom. Display according to CSS (Bos et al. 2007). Alternative values: block, inline, list-item, superimposed, none. The color of the background, as in Çelik and Lilley 2003. The color of the foreground, as in Çelik and Lilley 2003. The main religious orientation according to XNLRDF1. The main ethnicity according to XNLRDF. ISO 639-3 language codes, cf. XNLRDF. Top-to-bottom left-to-right (Chinese), right-to-left top-to-bottom (English), top-to-bottom left-to-right (Mongolian), cf. XNLRDF. The set of characters or signs used according to XNLRDF. The number of tombs in this div. The number of tombstones in this div.  2.2.Balanced Data A corpus should be balanced (Biber et al. 1998). Although for a tombstone corpus, criteria of balancedness might be better to define than for a text corpus, e.g. collecting one photo per 1000 tombs, balancedness through sampling is impossible to achieve. Nobody knows all graveyards and how many tombs there are and those that we find may be inaccessible or decayed. In addition, naïve balancedness is not what we want. We want a tombstone corpus to have different  
In the field of stylistics, as computational approaches have been developed and many on-line corpora have been constructed, statistical textual characteristics have been systematically examined. These textual characteristics have traditionally been used for detecting the authors, registers, and chronological variations of texts. Recently, they have also been used for more practical applications such as spam filtering (Argamon, Whitelaw, Chase, Raj Hota, Garg, and Levitan, 2007). With this expansion in the scope of stylistics and the production of a wide variety of new types of texts especially with the growth of the Web (Aitchison and Lewis, 2003), the microscopic characterization of different types of texts has become more and more important. Turning our eyes to the field of political science, the content of political speeches is regarded as important for analyzing the policies, attitudes and thoughts of political actors (Axelrod, 1976). Among such speeches, prime ministers' Diet addresses are recognized as the most * This study was supported by a Suntory Foundation Research Grant, 2007-2008. We would like to express our gratitude for this support. An earlier version of this study was presented at the 35th annual meeting of the Behaviormetric Society of Japan. We would like to thank the participants for their useful comments. Copyright 2007 by Takafumi Suzuki and Kyo Kageura 459  important material for understanding Japanese politics as they reflect Japanese governmental policies, and prime ministerial attitudes and thoughts (Watanabe, 1974; Kusano, 2005). As the role of the media and the performance of politicians increase in importance in contemporary politics (Kusano, 2006), the style of prime ministers' speeches, as well as their content, have attracted more attention (Ahrens, 2005; Azuma, 2006). Against this background, this study explores the textual characteristics of Japanese prime ministers’ Diet addresses, focusing on (a) the difference between the two types of Diet addresses and (b) the perceived changes made to these addresses by two powerful prime ministers, comparing the characteristics of their addresses with those of all prime ministers from 1945 to 2006. In order to clarify these points, we focus on the quantity and diversity of nouns, textual characteristics strongly related to the content of texts, instead of conventional contentindependent stylistic characteristics, because the purpose of this study is to analyze these characteristics in order to better understand two political phenomena: the difference and changes in political content. From the point of view of computational linguistics, this study can be seen as a case study for exploring microscopic textual characteristics. The rest of this paper is organized as follows. In Section 2, we review previous work, and in Section 3, explain our two research questions in detail. In Section 4, we describe our data, and in Section 5, discuss the results. In Section 6, we make concluding remarks. 2. Previous work In the field of stylistics, many textual characteristics have been examined (Grieve, 2007). Those which have been frequently measured are the length of sentences, the length of words and the relative frequency of different parts of speech (Kenny, 1982). Lexical richness measures including those we use in this study have been systematically examined by Tweedie and Baayen (1998). Textual characteristics have been examined for formalizing the mathematical characteristics of texts (e.g., Simpson, 1949; Yule, 1944), and also for authorship attribution (Hoover, 2003) and genre-based text classification (Cortina-Borja and Chappas, 2006). They have proved to be reliable measures for classification because they take the information of function words as well as content words (Garcìa and Martìn, 2007). But for our purpose, the lexical diversity of specific parts of speech need to be examined, and there are few studies examining these characteristics. Some studies in political science examine prime ministers’ Diet speeches using quantitative methods, but most studies focus only on the frequencies of specific words (e.g., Watanabe, 1974). Though Azuma (2006) examined sentence lengths and other specific functional expressions in the speeches, and Reinem (2005) used comprehensive types of content words for her analysis, the lexical quantity and diversity of Diet speeches have not been examined fully. 3. Research questions The two issues we address in this study are as follows: 3.1 The difference between the two types of Diet addresses Japanese prime ministers deliver two types of Diet addresses: Shisei Hoshin Enzetsu (Speech to Express Policy: SEP) and Syoshin Hyomei Enzetsu (Speech to Express Belief: SEB).1 The SEP is delivered at the ordinary Diet session in which the budget is compiled. The SEB, on the other hand, is delivered at extraordinary Diet sessions in which the budget is not a major issue. In the SEP, prime ministers deal with many political issues related to the budgets, while in the SEB they highlight the particular political goals they wish to achieve. But some studies insist that the two types of addresses need not be distinguished when analyzing their content because they are 1Both of these two types of addresses are normally translated as 'general policy speech' in English. 460  similar (Reinem, 2007). By examining the textual characteristics of the two types of Diet addresses, we will clarify this point. It will help us to determine which (or both) types of addresses should be analyzed when we wish to investigate the political content related to the budget or content related to political goals prime ministers would especially like to achieve. 3.2 The perceived changes made to these addresses by two prime ministers Nakasone, one of the strongest and most powerful prime ministers in Japanese political history, is reported to have increased the length of his Diet addresses by developing his own philosophy and ideas in order to show the concentration of political power in the office of prime minister (Shinoda, 1994). Koizumi, another powerful and strong prime minister, is famous for having used more dramatic and entertaining style of speech (Kabashima and Steel, 2007). His speech style is thought to have been characterized by the repetition of specific topics (typically privatization of the postal system). He succeeded politically partly because he was able to attract people by his way of speaking (Otake, 2006). However, these perceptions about the distinctive styles of speech of these two powerful prime ministers have never been empirically examined. We do so by measuring the textual characteristics of their Diet addresses, thus providing empirical evidences for discussion of their political styles or media strategy as revealed through their speeches.  4. Data The corpora we used in this study consists of 150 Diet addresses covering the 28 tenures in office of 27 Japanese prime ministers2 from 1945 to 2006.3 We downloaded the addresses from the on-line database Sekai to Nihon (The World and Japan).4 We applied morphological analysis to the addresses using ChaSen, a Japanese morphological analysis system (Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda, Takaoka, and Asahara, 2003). We extracted nouns according to part-of-speech tags assigned by ChaSen. Table 1 shows the number of addresses, the number of tokens and the number of types of the SEP and the SEB after 1953.5 Table 2 sets out the names, the initials, the date of assumption to office, the number of addresses, and the total and mean number of tokens for each prime minister.  Table 1: Basic data of the two types of Diet addresses  Address Token Type  SEP  61 120005 6056  SEB  72 91635 5374  5. Lexical indices The three lexical indices we used in this study for analyzing the difference between and the changes in the addresses were number of tokens, type-token ratio and Simpson's D (Simpson, 1949) of nouns. These simple measures reflect the quantity and the diversity of political content in the Diet addresses, and can explain the two questions we proposed in Section 3. Number of tokens The first index is the number of tokens, which we denote as N. It shows the number of items 2 Yoshida was elected prime minister twice. We treat these as different tenures. 3 The texts of the speeches are written down by secretaries in shorthand. 4 http://www.ioc.u-tokyo.ac.jp/~worldjpn/index.html 5 We used only the addresses from after 1953 in our analysis, as before that date the two types of addresses were not distinguished.  461  used in the sample. N of nouns reflects the quantity of political content mentioned in the addresses. Type-token ratio The second index is a type-token ratio formulated as follows: TTR= V N N where V(N) represents the number of types. It shows the diversity of items used in the sample. TTR of nouns reflects the diversity of political content per unit noun-tokens. A high TTR means that a variety of political content is mentioned, but the average amount of discussion devoted to each topic is limited. A low TTR indicates the opposite.  Table 2: Basic data of Japanese prime ministers and their addresses  Name HIGASHIKUNI Naruhiko SHIDEHARA Kijuro YOSHIDA Shigeru KATAYAMA Tetsu ASHIDA Hitoshi YOSHIDA Shigeru HATOYAMA Ichiro ISHIBASHI Tanzan KISHI Nobusuke IKEDA Hayato SATO Eisaku TANAKA Kakuei MIKI Takeo FUKUDA Takeo OHIRA Masayoshi SUZUKI Zenko NAKASONE Yasuhiro TAKESHITA Noboru UNO Sosuke KAIFU Toshiki MIYAZAWA Kiichi HOSOKAWA Morihiro HATA Tsutomu MURAYAMA Tomiichi HASHIMOTO Ryutaro OBUCHI Keizo MORI Yoshiro KOIZUMI Junichiro  Initials HN SK YS1 KTe AH YS2 HI IT KN IH SE TK MiT FT OM SZ NY TN US KTo MK HM HT MuT HR OK MY KJ  Date Address Tokens(total) Tokens(mean)  8/1945  
2. Deontic-epistemic polysemy in cognitive linguistics and grammaticalization * Thanks are due to two anonymous reviewers for their constructive criticism. The usual disclaimer applies. This study was supported in part by a grant from the Japan Society for the Promotion of Science (#19520349). Copyright 2007 by Mizuho Tamaji and Kaoru Horie 471  The phenomenon of polysemy between deontic and epistemic modalities is observable among typologically different languages, therefore this phenomenon is considered as a cross-linguistically prevalent tendency. There are two main approaches which propose to account for the deontic-epistemic polysemy prevalent across languages, i.e. (I) the polysemic approach and (II) the monosemic approach. 2.1.The polysemic approach According to recent studies of cognitive linguistics (Sweetser 1990) and of grammaticalization (Bybee et al. 1994, Traugott and Dasher 2002), the deontic-epistemic modality results from the cognitive-diachronic process whereby epistemic modal meaning derives from deontic epistemic modal meaning rather than vice versa. For example, Sweetser (1990) explains this process as in (1). (1) John must be at home right now. The sentence (1) can be interpreted in two ways: (i) John is obliged to be at home (deontic usage of must) and (ii) the circumstance compels the speaker to judge that John is at home (epistemic usage of must). Deontic usage implies that the force-dynamics in a real world imposed by the speaker and/or other several actors compels the subject (or others) to perform a certain action. Epistemic usage suggests that cognitive force employed by a certain actor compels the speaker (or people in general) to reach the conclusion described in a sentence. There is parallelism in force dynamics between deontic and epistemic usages. Force dynamics in a socio-physical domain derives force dynamics in a cognitive domain. Therefore, it is considered that deontic usage derives epistemic usage and the former is more prototypical than the latter. 2.2. The monosemic approach There is another approach to explain the relationship between deontic and epistemic modalities. This approach, referred to as the ‘monosemic’ approach, was advocated by Kratzer (1981). It proposes that each modal marker generally contains a common core meaning which covers several different interpretations. Papafragou (2000) developed Kratser’s approach and proposed that the interpretation of modal meaning is context-dependent, by adopting the ‘relevance theory’ by Sperber and Wilson (1995). The monosemic approach was proposed because there are some cases which cannot be explained by the ‘polysemic’ approach. While the polysemic approach regards a semantic change from deontic to epistemic modality as metaphoric mapping in force-dynamics from socio-physical domains to epistemic domains, the monosemic approach regards modal meanings as clear-cut, i.e. either deontic or epistemic. Contrary to the polysemic approach, the monosemic approach proposes that modal meaning is not determined by a priori but is determined by specific contexts. 3. A cognitive account for the absence of deontic-epistemic polysemy in Japanese 472  As noted in Section 1, modal markers in Modern Japanese generally fail to exhibit the deontic-epistemic polysemy. For example, the deontic modal sense of should is encoded by a periphrastic modal marker bekida, while its epistemic modal sense is encoded by another periphrastic marker hazuda. This tendency is further exemplified by another set of modal markers nakerebanaranai and nichigainai. Both are translated into English must: the deontic modal sense of must corresponds to nakerebanaranai, while its epistemic modal sense corresponds to nichigainai. The virtual non-existence of deontic-epistemic polysemy in Japanese suggests that the relationship between deontic modal markers and epistemic modal markers cannot be explained by the cross-linguistically observable unidirectional grammaticalization process of modal markers. Yamada (1990) thus maintains that both categories of modality originated independently and that neither of them is more prototypical than the other. This view accords with the monosemic approach in that both views consider that deontic modality and epistemic modality are distinctive cognitive domains. The other approach maintains that epistemic modality derives deontic modality and, therefore, epistemic modality is more prototypical than deontic modality in Japanese, unlike the cross-linguistically prevalent reverse tendency (Kurotaki 2005). This approach is similar to the polysemic approach in that both approaches consider deontic and epistemic modal meanings to be continuous. A most common method to examine the prototypicality of modal meaning in a language is to examine the process of diachronic grammaticalization in that language. Grammaticalization in the area of modality refers to the process of semantic change of modal markers: the prototypical meaning emerged earlier than the peripheral one. The evidence of diachronic grammaticalization, however, is not available to explain the prototypicality of Japanese modal markers, because the development of the modal markers in Modern Japanese is apparently independent of the modal markers in Classical Japanese (Onoe 2001). Therefore, we examine the relationship between deontic modal markers and epistemic modal markers using the data of second language acquisition as an alternative method. 4. Studies of Chinese learners’ acquisition of Japanese modal markers 4.1. The parallelism between L2 acquisition and diachronic grammaticalization Certain parallelism between grammaticalization and the order of acquisition of polyfunctional words has been recognized in functional-cognitive linguistics. That is, the emergence and the acquisition of core meaning have been known to precede those of peripheral one. Recently, this parallel relationship was extended to a research program that describes the typological characteristics of grammaticalization of a language based on the language acquisition order of polysemous words (e.g.Giacalone-Ramat 2003, Giacalone-Ramat and Crocco-Gales 1995). Unlike L1 acquisition, the order of L2 learners’ acquisition of polysemous words does not involve the creation of new patterns of grammaticalization like diachronic grammaticalization. Instead, it demonstrates various ways of approximating to a subsystem (Giacalone-Ramat 2003: 473  28). Therefore, L2 acquisition, especially the strategy utilized by adult learners, recapitulates the internal factors of the diachronic grammaticalization in the target language more overtly than L1 acquisition. Hence, we consider that the data of second language acquisition will be an appropriate method in order to analogize the relationship between deontic and epistemic modal markers in Japanese. 4.2. The target and the hypothesis According to the parallelism between the order of acquisition of meaning and diachronic grammaticalization, learners acquire more prototypical modal markers earlier than non-prototypical ones. This suggests that the modal marker acquired earlier is more prototypical. The notion of the parallelism between the order and the degree of prototypicality is also applicable to the acquisition of Japanese modal markers, in which deontic modality and epistemic modality are encoded by two distinct modal markers. In this study, we will deal with the case of Chinese learners’ acquisition of two pairs of Japanese modal markers bekida/hazuda and nakerebanaranai/nichigainai. Bekida and hazuda correspond to a single modal marker ying1gai1 in Chinese, whereas nakerebanaranai and nichigainai are correspond to a single modal marker yao（4 the numbers indicate the intonation of Chinese: number 1 stands for flat tone, 2 for rising tone, 3 for falling-rising tone, and 4 for falling tone). Chinese is a language in which the deontic-epistemic polysemy is manifested, with the deontic meaning more prototypical than the epistemic meaning (Li 2003). The mapping patterns of deontic modal markers and epistemic modal markers and the degree of prototypicality are thus different between the two languages. Our null hypotheses are that (i) learners acquire more prototypical modal markers earlier than non-prototypical ones, and that (ii) neither of deontic or epistemic meaning is more prototypical than the other if learners acquire them simultaneously. We define the survey of prototypicality based on the order of acquisition as the survey 1. 5. Survey 1: the prototypicality of Japanese modal markers based on the second language acquisition 5.1. Methodology The tasks in Survey 1 were given in the form of multiple choice questions. Chinese learners of Japanese were instructed to choose one appropriate modal marker out of four alternatives for a question. Two out of four choices given were bekida and hazuda, which competed with each other. There were 20 questions in total. The learners had to choose bekida as the correct answers for 10 questions and hazuda for another 10 questions. (2) is an example of the task. Learners are instructed to read the sentence and to choose the appropriate modal marker among the four alternatives. As demonstrated by (c) and (d), two out of the four alternatives are bekida and hazuda, which compete with each other. 474  (2) sake-wa  tomokaku  tabako-wa (  ).  alcohol-TOP  if not  tobacco-TOP  ‘You (  ) tobacco, if not alcohol.’  (a) herasa nai monoda  (b) herasu wakeda  ‘do not naturally cut down on’  
Kimitaka Tsutsumi , Kazutaka Shimada and Tsutomu Endo  a Department of Artificial Intelligence, Kyushu Institute of Technology, Iizuka, Fukuoka 820-8502 Japan {k_tsutsumi, shimada, endo}@pluto.ai.kyutech.ac.jp  Abstract. In this paper, we propose a method to classify movie review documents into positive or negative opinions. There are several approaches to classify documents. The previous studies, however, used only a single classifier for the classification task. We describe a multiple classifier for the review document classification task. The method consists of three classifiers based on SVMs, ME and score calculation. We apply two voting methods and SVMs to the integration process of single classifiers. The integrated methods improved the accuracy as compared with the three single classifiers. The experimental results show the effectiveness of our method. Keywords: Sentiment analysis, p/n classification, Integration, Movie reviews, WWW. 1. Introduction The World Wide Web contains a huge number of on-line documents that are easily accessible. Finding information relevant to user needs has become increasingly important. The most important information on the Web is usually contained in the text. We obtain a huge number of review documents that include user’s opinions for products. For example, buying products, users usually survey the product reviews. Movie reviews are likewise one of the most important information for users who go to a movie. More precise and effective methods for evaluating the products are useful for users. Many researchers have recently studied extraction of evaluative expressions and classification of opinions (Kobayashi et al., 2005; Osajima et al., 2005; Pang et al., 2002; Turney, 2002). Studies of opinion classification are generally classified into three groups: (1) classifying documents into the positive (p) or negative (n) opinions, (2) classifying sentences into the positive or negative opinions, and (3) classifying words into the positive or negative expressions. In this paper, we focus on the classification of movie review documents. Pang et al. (2002) have reported the effectiveness of applying machine learning techniques to the p/n classification. They compared three machine learning methods: Naive Bayes, Maximum Entropy and Support Vector Machines. In their experiment, SVMs produced the best performance. Osajima et al. (2005) have proposed a method for polarity classification of sentences in review documents. The method is based on a score calculation process of word polarity and outperformed SVMs in the sentence classification task. The previous studies, however, used only a single classifier for the classification task. We (Tsutsumi 2006 et al.) have proposed a method consisting of two classifiers: SVMs and the scoring method by Osajima et al. (2005). The method identified the class (p/n) of a document on the basis of the distances that were measured from the hyperplane of each classifier. It obtained * Copyright 2007 by Kimitaka Tsutsumi, Kazutaka Shimada and Tsutomu Endo. 481  the better accuracy as compared with the single classifiers. This method, however, contained a problem for the determination of the final output, namely positive or negative. We needed to normalize the classifier’s outputs manually because the scale of the scoring method was different from that of SVMs. To solve this problem, we apply the 3rd machine learning method (Maximum Entropy) into the method based on the scoring and SVMs. Figure 1 shows the outline of the proposed method. In this paper, we compare three processes for the method: naive voting, weighted voting and determination with SVMs. Figure. 1.The outline of our method. 2. Classifiers In this section, we explain classifiers for a multiple classifier that will be proposed in this paper. The 1st and 2nd classifiers are SVMs and Maximum Entropy respectively. These classifiers have been used in related work by Pang et al. (2002). In their paper, they reported that SVMs produced the best performance. The 3rd classifier is based on polarity scores of words in documents. The method is an expansion of Osajima et al. (2005). 2.1.SVMs SVMs are a machine learning algorithm that was introduced by Vapnik (1999). They have been applied to tasks such as face recognition and text classification. An SVM is a binary classifier that finds a maximal margin separating hyperplane between two classes. The hyperplane can be written as: yi = w ⋅ x + b (1) 482  where x is an arbitrary data point, i.e., feature vectors, w and b are decided by optimization, and yi ∈ {+ 1,−1}. The instances that lie closest to the hyperplane are called support vectors. We use SVMlight package1 for training and testing, with all parameters set to their default values (Joachims, 1999).  2.2.Maximum Entropy Maximum entropy modeling (ME) is one of the best techniques for natural language processing (Berger et al., 1996). The principle of the ME is expressed as follows:  ∑ PΛ  (c  |  d)  =  Z  
Yu-Chun Wang , Yi-Hsun Lee , Chu-Cheng Lin ,  d*  a  Richard Tzong-Han Tsai , Wen-Lian Hsu  a Institute of Information Science, Academia Sinica, Taiwan b Department of Electrical Engineering, National Taiwan University, Taiwan c Department of Computer Science and Information Engineering, National Taiwan University, Taiwan d Department of Computer Science and Engineering, Yuan Ze University, Taiwan  {albyu, rog, as1986}@iis.sinica.edu.tw thtsai@saturn.yzu.edu.tw hsu@iis.sinica.edu.tw *corresponding author  Abstract. Named entity translation plays an important role in many applications, such as information retrieval and machine translation. In this paper, we focus on translating person names, the most common type of name entity in Korean-Chinese cross language information retrieval (KCIR). Unlike other languages, Chinese uses characters (ideographs), which makes person name translation difficult because one syllable may map to several Chinese characters. We propose an effective hybrid person name translation method to improve the performance of KCIR. First, we use Wikipedia as a translation tool based on the inter-language links between the Korean edition and the Chinese or English editions. Second, we adopt the Naver people search engine to find the query name’s Chinese or English translation. Third, we extract Korean-English transliteration pairs from Google snippets, and then search for the English-Chinese transliteration in the database of Taiwan’s Central News Agency or in Google. The performance of KCIR using our method is over five times better than that of a dictionary-based system. The mean average precision is 0.3490 and the average recall is 0.7534. The method can deal with Chinese, Japanese, Korean, as well as non-CJK person name translation from Korean to Chinese. Hence, it substantially improves the performance of KCIR. Keywords: Person Name Translation, Korean-Chinese Cross Language Information Retrieval 1. Introduction Named entity (NE) translation plays an important role in machine translation, information retrieval, and question answering. It is a particularly challenging task because, although there are many online bilingual dictionaries, they usually lack domain specific words or NEs. Furthermore, new NEs are generated everyday, but the content of bilingual dictionaries cannot be updated frequently. Therefore, it is necessary to construct a named entity translation (NET) system. Economic ties between China and Korea have become closer as China has opened its markets further, and demand for the latest news and information from China continues to grow rapidly in Korea. One key way to meet this demand is to retrieve information written in Chinese by ∗ Copyright 2007 by Yu-Chun Wang, Yi-Hsun Lee, Chu-Cheng Lin, Richard Tzong-Han Tsa, Wen-Lian Hsu  489  using Korean queries, referred to as Korean-Chinese cross-language information retrieval (KCIR). The main challenge involves translating NEs because they are usually the main concepts of queries. In Chen (1998), the authors romanized Chinese NEs and selected their English transliterations from English NEs extracted from the Web by comparing their phonetic similarities with Chinese NEs. Al-Onaizan and Knight (2002) transliterated an NE in Arabic into several candidates in English and ranked the candidates by comparing their occurrences in several English corpora. In the above works, the target languages are alphabetic; however, in KC translation, the target language is Chinese, which uses an ideographic writing system. KoreanChinese NET is much more difficult than NET considered in previous works because, in Chinese, one syllable may map to tens or hundreds of characters. For example, if an NE written in Korean comprises three syllables, there may be thousands of translation candidates in Chinese. In this paper, we focus on translating person names, and propose an effective hybrid method to improve the performance of our Korean-Chinese cross-language information retrieval system 2. Difficulties in Korean-Chinese Person Name Translation for IR In this section, we discuss the phenomena observed in the transliteration of person names in Korean and Chinese. We begin with a brief review of the relationship between the Korean and Chinese languages. Korean is an Altaic language, while Chinese is a Sino-Tibetan language; hence, their phonology and grammar are quite different. Due to a long history of contact with Chinese, Koreans adopted Chinese characters and incorporated a lot of Chinese vocabulary into their language. Chinese characters used in Korean are called “Hanja”, and Chinese loanwords used in Korean are called Sino-Korean words. The pronunciation of Hanja in Korean is very different from modern Chinese, Mandarin, because it follows the pronunciation of Middle Chinese; thus, it has not undergone many of the sound changes evident in modern Chinese, Interestingly, Song (2005) mentioned that over 52 percent of the words in the modern Korean vocabulary are Sino-Korean In 1443, Koreans invented their own alphabetic writing system called “Hangul”. Each Hanja character has a corresponding Hangul character based on its Korean pronunciation. However, Hanja is only used in some limited domains now. 2.1.Korean Name Translation Korean and Chinese name systems are very similar. Because of historical links, almost all Koreans have names that are exclusively Hanja (Han- "Chinese, "-ja "characters"). Therefore, the most straightforward way to translate a Korean name into Chinese is to adopt its Hanja equivalent. Take the Korean president’s name “노무현” (No Mu-Hyeon) as an example. In this case, we can adopt the Hanja equivalent “盧武鉉” (Lu Wu-Xuan) directly. However, if a Korean’s Hanja name is unknown, the name is translated character by character. Each Hangul character is basically translated into its corresponding Hanja character. For example, the name of the Korean actor “조인성” (Cho In-Seong) is usually translated as “趙仁成” because ‘조’ is mapped to ‘趙’, ‘인’ is mapped to ‘仁’, and ‘성’ is mapped to ‘成’. However, that translation may not be the same as the actor’s Hanja name. In addition, some Hangul characters do not have corresponding Chinese characters, so Chinese characters with similar pronunciations are used to translate the Hangul characters. Take the Korean actress “김하늘” (Kim Ha-Neul) for example. Her given name “하늘” (“ha-neul” meaning “sky”) is a native Korean word that has no corresponding Hanja characters. We use “荷娜” (He-Na) or “哈嫩” (Ha-Nen), which have similar pronunciations to translate “하늘” (ha-neul). These examples show that there may be many Chinese translations for a Korean name. This phenomenon makes Korean-Chinese information retrieval more difficult because reporters usually use one or two common 490  translations to write articles. However, we cannot guarantee that our translations are the most common ones. 2.2.Chinese Name Translation To translate a Chinese person name written in Korean, we consider two ways that are used to translate a Chinese person name into spoken Korean. The first method uses Sino-Korean pronunciation. For example, consider the name “馬英九” (Ma Ying-Jiu, the ex-chairman of the Kuomintang (KMT), a Taiwanese political party); its Sino-Korean pronunciation is “마영구” (Ma Yeong-Gu). However, in recent years, Koreans have started to transliterate a Chinese person name based on its Mandarin pronunciation. Therefore, the name “ 馬 英 九 ” is transliterated to “마잉주” (Ma Ing-Ju). Translating Chinese person names by either method is a major challenge because one Hangul character corresponds to several Chinese characters that have the same pronunciation in Korean. This results in thousands of possible combinations of Chinese characters, making it very difficult to choose the right one. Therefore, we must develop different techniques to find the correct Chinese translation that is used in articles. 2.3.Japanese Name Translation Chinese and Korean use different strategies to translate Japanese person names. Korean transliterates a Japanese person name into Hangul characters based on the name’s pronunciation in Japanese, whereas, Chinese speakers use the name in Kanji directly. Take the Japanese expremier “小泉純一郎” (Koizumi Junichiro) for example. In Korean, his name is transliterated into “고이즈미 준이치로” (Ko-i-jeu-mi Jun-i-chi-ro). In contrast, the Kanji name “小泉純一 郎” (Xiao quan chun yi lang) is used directly in Chinese. Therefore, it is very difficult to translate Japanese names written in Korean into Chinese based on phonetic information. 2.4.Non-CJK Name Translation In both Korean and Chinese, transliteration methods are used to translate non-CJK person names. Korean uses the Hangul alphabet for transliteration. Because of the phonology of Korean, some phonemes are changed during translation because the language lacks such phonemes as described in Oh (2003) In contrast, Chinese transliterates each syllable in a name into Chinese characters with similar pronunciation. Although there are some conventions for selecting transliteration characters, there are still many possible alternatives. For instance, Greenspan has several Chinese transliterations, such as “葛林斯班” (Ge-lin-si-ban) and “葛林 斯潘” (Ge-lin-si-pan). In summary, it is difficult to match a non-CJK person name transliterated from Korean with its Chinese transliteration due to the latter’s variations. However, this task is the key to retrieving Chinese articles by using Korean queries. 3. Our Method We now describe our Korean-Chinese person name/NE translation method for dealing with the problems described in Section 2. We either translate NE candidates from Korean into Chinese directly, or translate them into English first and then into Chinese. 3.1.Named Entity Selection The first step is to identify which words in a query are NEs. In general, Korean queries are composed of several eojeols, each of which is composed of a noun followed by the noun’s postposition, or a verb stem followed by the verb’s ending. We remove the postposition or the ending to extract the key terms, and then select person name candidates from the key terms. Next, the maximum matching algorithm is applied to further segment each term into words in 491  the Korean-Chinese bilingual dictionary1. If a segment’s length is equal to one, the term is regarded as an NE candidate to be translated. 3.2.Using Wikipedia for Translation Wikipedia is a multilingual online encyclopedia comprised of content written by volunteers all over the world. Unlike traditional encyclopedias, the number of articles in Wikipedia increases rapidly, and each article usually lists hyperlinks to other relevant content. Currently, Wikipedia is available in 252 languages. It is a highly consistent, human-made corpus. Each article in Wikipedia has an inter-language link to other language editions, which we exploit to translate NEs. An NE candidate is first input to the Korean Wikipedia, and the title of the matched article’s Chinese version is treated as the NE’s translation in Chinese. However, if the article lacks a Chinese version, we use the English edition’s version to acquire the NE’s translation in English. The English translation is then transliterated into Chinese by the method described in Section 3.5. 3.3.Using the Naver People Search Engine for Translation The Naver people search engine is a translation tool that maintains a database of famous people’s basic profiles. If the person is from China, Japan, or Korea, the search engine returns his/her name in Chinese. For example, if we input the Japanese actor’s name “사나다 히로유키” (Sanada Hiroyuki) to the Naver people search engine, it will return his Japanese name “真田広之” with Chinese characters. In such cases, we can adopt the retrieved name directly. However, for other nationalities, the Naver search engine returns person names in English, and we have to translate them into Chinese. The translation method is also described in Section 3.5. 3.4.Web-Based Korean-English Transliterations Obviously, the above methods cannot cover all possible translations used in newspaper articles. Therefore, we propose a web-based transliteration method. First, each NE candidate, NEC, is input to Google to retrieve snippets of relevant documents in the first ten pages. Second, we use the following template to extract the NEC’s English translation from the snippets. NECK(e1e2e3…en), where NECK represents the NEC in Hangul characters and ei∈ English alphabet. The string e1e2e3…en is regarded as the NEC’s English translation. In Section 3.5, we describe the method used to further transliterate an NEC’s English translation into Chinese. 3.5.Searching English-Chinese Transliteration in the CNA Database and Google In this section, we discuss two methods that we use to transliterate English names generated by the above Korean-English translation methods into Chinese. The first obtains the Chinese translations of English names from Taiwan’s Central News Agency (CNA) database2, which stores all the transliterations used by CNA since 1954. The second method exploits the Web to extract other possible Chinese transliterations not available in the CNA database. The latter have a significant influence on IR’s performance. The English name NECE is also input to Google and snippets are extracted from the first 10 returned pages. Then, we use the following template to extract the Chinese translation: wboundaryc1c2c3…cm(NECE), 
c  XiangFeng Wei , Ning Jia , Quan Zhang , HanFen Zang  a Institute of Acoustics, Chinese Academy of Sciences, China b Graduate University of Chinese Academy of Sciences, China c Captical University of Economics and Business, China {wxf, zhq}@mail.ioa.ac.cn, gnin_aij@sina.com, zanghf@163.com  Abstract. In order to extract some important information of a person from text, an extracting model was proposed. The person’s name is recognized based on the maximal entropy statistic model and the training corpus. The sentences surrounding the person’s name are analyzed according to the conceptual knowledge base. The three main elements of events, domain, situation and background, are also extracted from the sentences to construct the structure of events about the person. Keywords: Person’s Name Recognition; Hierarchical Network of Concepts; Main Elements of Events; Semantic Parsing; Information Extraction 1. Introduction In news reports, person is one of the most important elements. On the internet politicians, famous enterprisers, celebrities and hot persons are always the focus what people want to know. It would be very useful and valuable to compile and extract the information about a person with the assistance of using automatic information extraction technology. ACE (Automatic Content Extraction) program is organized by NIST (National Institute of Standards and Technology, U.S.A). It is aimed to develop automatic content extraction technology to support automatic processing of human language in text form, including the detection and recognition of entity, value, time, relation and event. In the description of events, ACE defined the allowable roles of an event, such as person, place, position, etc. Persou, which is studied by Liu and etc. (2006), is a web search engine for searching persons’ information. It can distinguish automatically or semi-automatically the persons who own the same name, and produce their resume and activities. A fame evaluation system studied by Zan and etc. (2003) can calculate the correlative degree between the basic information such as name, specialty, affiliation, character words of a person and the information of a web page. If the count of the strong correlative web pages is greater than a threshold, the relative person will be a famous person. In above studies, they all adopted statistic method. The advantage of using statistic models is high efficient performance * This work is supported by the National Basic Research Program of China (973 Program, the contract No. 2004CB318104) and the Fund for Excellence by the Director of the Institute of Acoustics, Chinese Academy of Sciences (the contract No. GS13SJJ04). Copyright 2007 by XiangFeng Wei, Ning Jia, Quan Zhang, and HanFen Zang. 508  to process large-scale data, but the precision is restricted by training set and it is not suitable to process sparse individual data, although the data is very important to the user. This paper proposed an model of information extraction based on statistic algorithm and conceptual semantic knowledge to extract persons’ information from the text. The primary information of a person includes name, gender, age, nationality, department in organization, career, title, and so on. The name of a person was recognized by using maximal entropy statistic model. The text around the name was analyzed based on conceptual model. According to some associated key words about the primary information, the information was extracted from the context. If there are some events in the text, the main elements of events will be extracted based on the conceptual structures of a sentences and the conceptual knowledge base, which was stored in computer in advance. 2. The model of extracting persons’ information Figure 1 shows the whole process of extracting a person’s name, his or her primary information and events about a person. In this process, recognizing persons’ names is one of the key subprocesses. In order to recognize a person’s name, there must be some tagged texts which indicate the right persons’ names. In the tagged texts, the contexts around the names present some statistic characters. Therefore, recognizing a person’s name was transformed into classifying a word belongs to a name or not. This paper focused on Chinese names. A Chinese name is composed of two parts. The first part is family name and the second is given name. Most family names are limited in about 100 Chinese characters. These characters are used to activate persons’ names. The Chinese character (one or two) behind the family will be considered as candidates of a person’s name. A classifier can divide all candidates into name or non-name according to the statistic model based on the tagged texts. Figure 1: The model of extracting persons’ information. Once a person’s name is recognized, the surrounding sentences will be analyzed to extract the primary information and the events about the person. To extract the primary information of a person, there are some activating words to point out the information. For example, ‘he’ pointed to a person’s name indicates that the person’s gender is male, and ‘president’ points out the person’s title. To extract the information of events about the person, we must analyze the related sentences at first. All sentences are mapped into the conceptual structures, and the main elements of events are extracted from text based on conceptual knowledge bases. There are 509  three kinds of conceptual knowledge bases for words, sentences, and text respectively, as showed in figure 1.  3. Recognition of Chinese names The maximal entropy model is a well adaptive, flexible, and high-efficient statistic model. It can synthetically process all kinds of related and irrelated characters from data. The key point of the model is fitting the known data and equably distributing the unknown data. To apply the maximal entropy model, the restricted condition is described as an character function f(a,b)　{0,1} with alternative value. For example, a function to judge whether a noun is name is described as formula (1).  { f (a, b) = 1 (a=Pr esident )∩(b==Noun), 0 otherwise.  (1)  For a character function, its expectation relative to the experiential probability is calculated as formula (2).  ∑ E ~p fi = ~p(a,b) fi (a,b).  (2)  a,b  Its expectation relative to the model is calculated as formula (3).  E p fi = ∑ ~p(b)p(a | b) fi (a,b).  (3)  a,b  In the training data set, the two expectation are assumed to be equal as formula (4).  E p f i = E ~p f i .  (4)  If there is more than one restricted condition, a classifying problem will be changed into working out the optimized answer as formula (5).  { } P = p | E p fi = E ~p f , i = 1,2,L , k ,  p* = arg max H ( p) .  (5)  p∈P  The optimized answer is showed as formula (6) by using Lagrangian arithmetic.  ∑ p*  (a  |  b)  =  π  
Quan Zhang , Yun-liang Zhang , and Yi Yuan  a The Institute of Acoustics, CAS, Beijing 100080, P. R. China b Institute of Scientific & Technical Information of China, Beijing 100038, P. R. China  {zhq, yuan}@mail.ioa.ac.cn  Abstract. The text categorization is an important field for the automatic text information processing. Moreover, the authorship identification of a text can be treated as a special text categorization. This paper adopts the conceptual primitives’ expression based on the Hierarchical Network of Concepts (HNC) theory, which can describe the words meaning in hierarchical symbols, in order to avoid the sparse data shortcoming that is aroused by the natural language surface features in text categorization. The KNN algorithm is used as computing classification element. Then, the experiment has been done on the Chinese text authorship identification. The experiment result gives out that the processing mode that is put forward in this paper achieves high correct rate, so it is feasible for the text authorship identification. Keywords: Lingual Conceptual Space, Hierarchical Network of Concepts (HNC) theory, Text Categorization, KNN Algorithm, Authorship of a text. 1. Introduction Along with the progress of social information industry, especially with the Internet development, more and more documents exist in the electronical form. It provides convenience for information automatic processing. The text categorization is the basic work for the automatic processing, and it is the foundation for the information retrieval, information mining, and question-answer system too. In some applications, it is need to identify the text authorship. The identification need use the documents written by the author as reference. Since the literary style of an author is relatively steady in some time, if we can mine the style character of the author then the identification is realized. Of course, the more documents which are gathered, the better result which is achieved. In many cases, the text authorship identification is treated as text categorization as the researchers can accomplish the work according the text categorization way. In this way, the frequency of glossary, punctuation, n-Gram string, syntax feature, average sentence length, the length of paragraph, and so on, are be used as the features to identify the author of a text * The paper is supported by the National Fundamental Research Project (973 Project) (Grant No. 2004CB318104) and The Knowledge Innovation Engineering Project of The Institute of Acoustics, CAS (Grant No. 0654091431). Copyright 2007 by Quan Zhang, Yun-liang Zhang and Yi Yuan 515  [Burrows J. F. 1987, Baayen R. H. 1996, David I Holmes 1997, Yuta Tsuboi 2002]. The texts are often literature, and sometimes they are internet text, such as E-mail [Olivier de Vel 2001]. Presently, there are a few research works in the Chinese text authorship identification, and many of them orient to the linguistic research for the concrete literature. Sun and Jin [Yi-jian Jin, Xiao-ming Sun, and Shao-ping Ma, 2003] use the vector space model (VSM) which takes the syntax structural words as feature to identify the text authorship, they achieved good result in novel author identification. The best precision of pattern matching, KNN algorithm and SVM algorithm are 89.51%, 91.54%, and 93.58% separately. Ma and Chang study the E-mail authorship identification successively [Jian-bin Ma 2004, Shu-hui Chang 2005]. In addition, Wu introduces the HowNet knowledge base; he performs the text authorship identification according to the evaluation method of glossary semantic similarity. His best marco-average Fmeasure is 86.23% that is achieved in 202 People Daily texts written by 5 reporters [Xiao-chun Wu 2006]. In text categorization, one important aspect is text expression. The text is mainly expressed as discrete glossary. As to Chinese, it is also expressed as Chinese character, phrase, term, n-gram string, etc. When the discrete unites are obtained, they can be used in feature vector constructing. These features that come from natural language directly can represent the topic of the text and the writing style of the author in some aspects. However, these features also arouse the spare data and too large feature space in the computing, because there are more than ten thousand basic words in natural language, and the total of words even come to million, the dimension of the corresponding vector space is huge. In order to perform the computing, it is necessary to reduce the dimension. Huang put forwards the linguistic conceptual space according his Hierarchical Network of Concepts (HNC, in short) theory [Zengyang Huang 1998, 2004]. The HNC conceptual primitives of the linguistic conceptual space and its word knowledge expression can be used to reduce the dimension. In fact, the structure of HNC conceptual primitives is a tree list, which the nodes are semantic primitives. The word meaning is expressed with the primitives, and it can accomplish the computing of correlation in the glossary, finding the thesaurus, and reducing the dimension. Therefore, we explore the Chinese text authorship identification with the KNN classifier that adopts the HNC conceptual primitives as the features carrier. We explain the HNC conceptual primitives and the semantic expression in the following firstly. 2. The HNC conceptual primitives and the semantic expression The network of primitives provides a necessary semantic description system in HNC, which expresses the semantic with formalization and conceptualization. The basic unite of the primitives network is conceptual tree, the node in the tree is the element for semantic expression, the relation between trees and the nodes in one tree provides the relevancy of the primitives. There are 456 trees, which are defined in HNC. Fig. 1 is an example for the conceptual tree. This branch provides the definition and reference for its dominative category. It defines part concepts of the professional activities in detail. The letter and the string, such as a, a1, a119, are the concept identifier for the node, and the content in the brackets is the comment for the conceptual node. All these are merely abstract definition, not for the concrete instance. When expressing the concrete instance, we need to combine more concepts according its meaning. For example, the “U.S.A. Government” can be expressed in “a119+fpj2*304”, the meaning of the “a119” can be found in the Fig. 1, and the “fpj2*304” refers in particular to the United Stats. The goal of the HNC conceptual primitives is to take out and generalize the commonness of the things, and identify them with a well-defined string. The network just describes the meaning of the words. As to the denotative function of a word, the HNC introduces five elements for the abstract concepts: v (verb), g (noun), u (attribute), z 516  (value), and r (effect); two elements for the concrete concepts: w (matter) and p (people); and one element between the abstract and concrete concept: x (quality). These elements are called “classification of concept”. They are also primitives, and need to combine to express a word denotative function.  Figure 1: A branch of the conceptual tree.  Table 1: Chinese word in HNC primitives expression.  Word 哀愁  Classification v,g  Conceptual meaing v7132+v7200e72  Sad and worry  巡警  p  pa41+va11  Cop 诱发  v  ((va71,v9441)#(v900;v80))  Arouse  We will adopt the classification of concept as categorization feature. The basic condition of the categorization feature is that the concepts which belong to one classification must not be too few, and the distinguish ability between the classifications is enough. Then we use the 12 kinds classification as feature.  Table 2: The 12 kinds classification for the feature.  No.  Name  
* I am grateful for the crucial comments from the anonymous reviewers, which help sharpen and clarify several main points in this paper. Special thanks go to Professor Miao-Hsia Chang and Professor HsuehO Lin for the critical comments and suggestions. Also, I thank my grandfather, Zhu-Mu Zhao, my parents, Su-Ying Chen and Shan-Long Zhao, my brother, Shi-Jie Zhao and my sister Ya-Ling Zhao for the encouragements on all occasions. Copyright 2007 by Yi-jing Zhao 522  distal deixis nage is being grammaticalized as a definite article in Mandarin Chinese. Latter, Liu investigates the complexity of proximal deixis and it is emphasized that zheyang(zi) has been grammaticalized from a demonstrative to a discourse boundary marker in natural conversations. Demonstratives in Taiwanese, however, have not been fully investigated. Specifically, previous studies relating to Taiwanese demonstratives frequently restrict their analyses to syntactic patterns or semantic diversities. Very few of them can take a discourse-pragmatic perspective to explore versatile usages of Taiwanese demonstratives in natural conversations. Li (1999) and Chang (2002) are among the few scholars who can take a discourse-pragmatic approach to examine demonstratives in Taiwanese. Li stresses an interactive function of He and Che in Taiwanese and a grammaticalization development of He and Che are also revealed in the study. Chang takes a parallel approach as Li. She classifies Anne in Taiwanese into six categories in accordance with its functions and syntactic patterns and some grammaticalization effects are observed in the study. However, a detailed study of hitlo ‘that’ in Taiwanese has not yet been conducted and it performs functions which are not found in the aforementioned deictic elements. Hitlo, a distal demonstrative, is frequently used in Taiwanese Southern Min. The purpose of this paper, hence, is to analyze the use of distal deixis hitlo in Taiwanese spoken discourse from a discourse-pragmatic perspective. The focus will be on various functions which hitlo can serve in spoken discourse. In particular, the study will show that hitlo seems to serve an interactive function in Taiwanese Southern Min. Moreover, the present study will reveal a grammaticalization process which hitlo is probably undergoing in Taiwanese spoken discourse. The data for this study consist of 27 stretches of talk drawn from audio recordings of daily conversations, telephone conversations, radio interviews, TV drama series and random examples. A total of 172 tokens of hitlo are identified in the data bank. The data are transcribed into intonation units (IU) and the transcription notations proposed by Du Bois et al. (1993) are followed. Following this section, section 2 will provide a review of previous relevant works on which the analysis and discussion are based. Section 3 discusses versatile functions of hitlo in Taiwanese. Section 4 reveals the grammaticalization process of hitlo. Section 5 is the conclusion. 2. Review of literature This section is divided into two parts. First, previous works related to demonstratives. Due to a lack of direct studies on hitlo, works on demonstratives will be discussed instead. Also, the grammaticalizational phenomenon will be reviewed in general. The latter will serve as the theoretical background knowledge of the present study. 2.1.Demonstratives in previous studies Traditional analyses for demonstratives are usually restricted to identify their meanings and distributions at sentential level. Chao (1968) classified uses of demonstratives into referential and non-referential usages. Cheng (1989) also investigated various functions of deixis. He found that distal deixis in Taiwanese can function as a proform of a predicate. This specific use of distal deixis, according to Cheng, is to “substitute for adjectives which the speaker is unwilling to explicitly state (1).” (1) 你 抵即 的 態度 實在 有 一點 傷過 彼 lo 啦 you just ASSC attitude really have a.little too that PR ‘Your attitude is really too that.’ This proform usage is similar to the hedging expression of hitlo which will be discussed in detail in section 3. Compared to Chao (1968), much more functions of demonstratives are 523  scrutinized in Cheng’s study. However, both studies focus on sentence level and examples used in both analyses are not extracted from spoken discourse. Their analyses, hence, may not be enough to provide a persuasive and satisfying overview of multiple functions that demonstratives can fulfill. Not until recently have versatile functions of demonstratives been investigated at discourse level (Biq 1990; Tao 1994, 1999; Huang 1999; Lee 1999; Chang 2002). Biq (1990) studies the use of na(me) in Chinese. She tries to argue that as a discourse connector, na(me) is used to mark a connection of units of talks and thus establish the relevance of the following talks to the prior. Tao (1994, 1999) emphasizes the intricacy of demonstrative discourse usages in Chinese in order to argue against the restricted view of demonstrative usages proposed in traditional studies. Huang (1999) identifies eight discourse functions of Chinese demonstratives; a more significant finding is that the distal demonstrative nage may undergo a grammaticalizational effect and may evolve into a definite article in spoken Chinese. Compared to Biq, Tao and Huang, who investigate Chinese demonstratives, Li and Chang explore the versatile discourse functions of demonstratives in Taiwanese. Li (1999) takes a pragmatic-discoursal approach to study Taiwanese proximal deixis che and distal deixis he. In her study, Li holds that the meaning contrast between che and he does not depend primarily on the spatial distinction, but on an interactive aspect, che signaling speaker’s own involvement, while he being a mark of addressee’s involvement. Chang (2002) adopts the same approach to examine multiple functions of Taiwanese proximal deixis anne and classifies its usages into six categories based on its discourse functions and syntactic patterns. Furthermore, both Li and Chang have argued that a grammaticalizational effect concerning semantic shift is demonstrated in the use of Taiwanese proximal deixis. In contrast to the traditional analyses, all of these studies base their arguments on spoken data beyond sentence level, making their analyses more convincing. However, their studies still do not illustrate overall functions that demonstratives can fulfill in Taiwanese, for example, filled pause and hedge expression (These usages will be discussed in section 3 in detail); more specifically speaking, studies relating to Taiwanese demonstratives are still insufficient. The investigation of hitlo in the present study, hence, aims to supplement the limitations of previous studies. 2.2.Theoretical assumption The theoretical assumption of this study is that the semantic and pragmatic change of a lexical item is a dynamic and unidirectional process which follows a path from deictic through textual to communicative functions (Traugott 1982, 1989, 1990, 1991). Traugott in 1989 revised this path to the following three tendencies: 1. Semantic-pragmatic Tendency I: Meanings based in the external described situation → meanings based in the internal (evaluative/ perceptual/ cognitive) situation 2. Semantic-pragmatic Tendency II: Meanings based in the described external or internal situation → meanings based in the textual situation 3. Semantic-pragmatic Tendency III: Meanings tend to become increasingly situated in the speaker’s subjective belief-state/attitude toward the situation Note that all three tendencies proposed by Traugott suggest that the directionality of grammaticalization is from concrete to abstract. The shift of while in English from a noun or adverb meaning of ‘period, time’ through a conjunction meaning ‘at the time’ to a marker denoting adversative or concessive meaning is an obvious example of this kind of grammaticalization path. 524  3. Discussion 3.1.Exophoric use and endophoric use A demonstrative may refer to an entity located in an immediate situation within which the speech event takes place. When used to refer to an object in an extralinguistic situation, demonstratives often accompany gestures or eye contact. In addition to its exophoric usage, distal deixis hitlo is also used to refer to a referent in previous discourse. The two common usages are not the focus of this paper, so examples will not be provided. 3.2.Referent-introducing function According to Huang (1999), the distal deixis nage in Chinese can introduce a new but familiar and identifiable referent into the discourse. In addition, the referent being introduced usually has topical significance. Distal deixis hitlo is found to fulfill the same function in Taiwanese. The following extract is an illustration of the referent-introducing function served by hitlo: (4) (Speaker F and M is talking about what kinds of dishes people should eat in January 1st according to the lunar calendar.) F1:(0)你 你 正月 初一, you you January first ...有 吃 什麼 菜 bo. have eat what dish PR F1: ‘What kind of dish do you eat on January first?’ M1:..eN, FP → ..阮 正月 初一 是 吃 彼 lo, we January first be eat hitlo ..<L2 什錦菜 L2>. Chowchow M1: ‘We usually eat chowchow on January first.’ F2:...喔. RT ..宛若 有 吃 <L2 什錦菜 L2> 喔. also have eat chowchow RT F2: ‘You also eat chowchow.’ M2:(0)heN 阿. yes PR ..十四 項. fourteen CL ..裏底 是 十四 項. inside be fourteen CL M2: ‘Yes. There are fourteen ingredients.’ F3:..有影 喔. really PR F3: ‘Really.’ M3:(0)heN. RT M3: ‘Yes.’ F4:...喔=. RT ...按呢 復 卡 厲害. 525  anne still more great F4: ‘This is better than us.’ M4:..heN. RT M4: ‘Yes.’ In (4), Speaker F and M is talking about a kind of dish which people used to eat in January 1st. In (M1), the dish, chowchow, is introduced by hitlo into the discourse and is instantly accepted by F. Then, they go on to talk about this dish for a total of 13 intonation units. Notice that the occurrence of hitlo in (4) can be omitted without affecting the addressee’ understanding of the intended meaning by the speaker. This forces us to consider that why speaker M selects to add the seemingly redundant distal deixis here. A closer observation on the context seems to suggest we should adopt a different perspective to view the choice of hitlo by the speaker. If a need to ensure interpersonal involvement is taken into account, the question is probably not that difficult to answer. Cheshire (1996) defines ‘involvement’ as “an assumption that spoken discourse is a collaborative production, with speakers and addressees working together to produce meaning as the discourse unfolds.” Viewing hitlo from this perspective, then, the occurrence of hitlo in (4) can be explained more easily and thoroughly. Now, let’s reconsider example (4). In (4), speaker F asks what kind of dish speaker M usually eats in January 1st. Speaker M in M1 answers with a common dish which people usually eat in January 1st. Obviously, speaker M assumes that speaker F must know this dish for it is a culturally related concept and a kind of shared understanding. The use of hitlo overtly manifests the speaker’s intention to invite the addressee to concentrate on their shared knowledge with an aim to achieving a common understanding. The use of hitlo here, thus, indicates an interpersonal involvement. Cheshire (1996) and Li (1999) are among the few researchers that recognize the interactive function of demonstratives. 3.3.Hedging expression Hedging expressions are elements in language “which makes messages indeterminate, that is, convey inexactitude, or in one way or another mitigates or reduces the strength of the assertions that speakers or writers make” (Mauranen 2004). These expressions fall into two main types: vagueness indicators and mitigators. Vagueness indicators are elements indicating fuzziness, imprecision, approximation and so on (2004: 179). Though Cheng (1989) senses this function which hitlo can perform, he did not explain it in detail and discuss it in discourse level. The following example aims to supplement the deficiency. (5) (Speaker E, an English teacher, is talking with the other three speakers about English education.) E:按呢 就 勿會— anne TOH not 就 有夠 a la.\ TOH enough PR PR ..<L2 那%L2>-that 你 若 講 欲 <L2 再 學得 更 深 L2>,_ you if say want more learn even in-depth 參像講— for example <L2 外面 L2>就 是 講 我 質馬 在— outside TOH be say I now at 526  ...<L2^比喻 L2> e 彼 寡 彼 lo hoN,\ similitude NM that some hitlo PT ...<L2 幼兒 L2> e 彼 寡 <L2 英文 L2> e=-children ASSC that some English ASSC hoN,\ PR 彼 類 彼 寡 彼._ That kind that some that ...<L2 何嘉仁 L2> la hoN,_ Hess PR PR 啊 足濟 e la.\ PR many ASSC PR 替 伊 廣告 沒 要緊 la,_ for it advertise no matter PR E: ‘This is enough. If you want to learn more, there are many ways, for example, English cram schools for children or something like that. Hess or something. There are many cram schools outside. It does not matter to advertise for it (Hess).’ In this excerpt, speaker E is talking about English cram schools for children. However, she does not want to specify these cram schools for she is afraid that her audience will think she is advertising for these cram schools. To avoid being regarded as advertising in a public place, speaker E uses many vague category markers such as 彼寡 ‘some’ and 彼類 ‘that kind’, together with 彼 lo ‘that’, to indicate that an unspecified category is being talked about. These vague expressions together convey the meaning that it is not important to know each instance in this category for it is not the focus of the present conversation. The use of hitlo helps the addressee interpreting utterances of the speaker more easily, and directs the addressees’ attention on the vague category. Hitlo, hence, expresses the speaker’s intention to get her addressees into the conversation.  3.4.Condition introducing marker Some hitlos in the data occur in the context that the speaker wants to express a condition. These hitlos seem to designate an upcoming conditional sentence.  (6) (A is talking about fruits.) A: (0) 其實 hoN,  Actually PR 你 講 柳柳丁 hoN,  you say oranges PR  → ..柳丁 若 彼 lo hoN,  
* I am grateful for the crucial comments from the anonymous reviewers, which help sharpen and clarify several main points in this paper. Special thanks go to Professor Chun-Yin Chen for the critical comments and suggestions. Also, I thank my grandfather, Zhu-Mu Zhao, my parents, Su-Ying Chen and Shan-Long Zhao, my brother, Shi-Jie Zhao and my sister Ya-Ling Zhao for the encouragements on all occasions. Copyright 2007 by Yi-jing Zhao 532  One thing worth mentioning is that these deictic terms usually appear along with all kinds of physical expressions. In actual language use, speakers make use of a variety of nonlinguistic clues such as eye gaze and gestures while producing the deictic words. Previous studies also prove that from about age one onwards children can and do utilize the non-linguistic clues of the speaker to identify what is the intended joint attention (Leung and Pheingold 1981; Tomasello, Call, and Gluckman 1997; Moore and Dunham 1995). If this is the case, we should expect that these physical cues should play an important role in children’s acquisition of certain deictic terms. This expectation will be examined in the present study. Children’s acquisition of demonstratives in Mandarin Chinese, however, has not been investigated in literature so far as I know. Previous studies relating to children’s acquisition of demonstratives usually limit their analyses to western languages (e.g. English or Turkish) and to an adjective-like function (e.g. Make ‘this’ chicken hop). Also, age-grading of participants in previous studies is usually not broadly-based. To understand children’s acquisition of demonstratives and of different functions of demonstratives, more studies are required. The purpose of this study, hence, is to investigate children’s acquisition of proximal demonstrative ‘zhege’ (this) and distal demonstrative ‘nage’ (that) in Mandarin Chinese. The focus will be on the pronominal function which the two words are found to serve in Chinese. Four age groups ranging from three to six years old are involved in the study. Five research questions will be addressed in the present study: (1) How do children work out the meaning of DPs? (2) Can H. Clark’s (1973) marking hypothesis be applied to children’s acquisition of DPs? (3) What are the developmental stages and the corresponding age grading of performance? (4) Does Piaget’s (1926) egocentrism hypothesis do exist? (5) Is there any age effect in children’s comprehension and production of these DPs? Following this section, section 2 will provide some basic properties of demonstratives in Mandarin Chinese. Section 3 offers an issue review of relevant theories and works on which the analysis and discussion are based. Section 4 is the methodology. Section 5 is a combination of results and general discussion. Section 6 concludes the finding and reveals some limitations. 2. Properties of Chinese demonstrative pronouns This section illustrates some basic properties of demonstrative pronouns (DPs) ‘zhege’ (this) and ‘nage’ (that) in Mandarin Chinese. In literature, a spatial distinction is often used to differentiate between ‘zhege’ and ‘nage’ in Mandarin Chinese. Generally speaking, ‘zhege’ is defined as a proximal DP whose major function is to indicate a referent near the speaker while ‘nage’ is a distal one used to refer to an object far away from the speaker: (1) wo yao chi zhe-ge I want eat this-CL ‘I want to eat this.’ (2) wo yao chi na-ge I want eat that-CL ‘I want to eat that.’ This spatial distinction has been recognized in works such as Chao (1968) and Cheng (1989). Chao glosses ‘zhege’ and ‘nage’ as demonstrative-measure compounds used to refer to an object either proximal (‘zhege’) or distal (‘nage’) to the speaker. Cheng makes similar remarks on the two DPs. In general, to master demonstrative pronouns ‘zhege’ and ‘nage’ in Mandarin Chinese, children at least have to learn two things: (1) the speaker is the deictic center and (2) the proximity distinction is a major contrast between the pair. 533  3. Literature review 3.1.Piaget’s egocentrism hypothesis (1926) The egocentrism hypothesis is first proposed by Piaget. According to Piaget (1926, 1928, 1929, 1956, 1958), children think in an ego-centric way up to the age of 7. That is, young children believe that they themselves are the center of the universe, and that all the others think in the same way like them. Thus, they fail to adopt points of view other than their own. The mastery of deictic terms, however, requires children to incorporate interlocutors’ viewpoint into consideration. That is to say, they have to know that the speaker is the deictic center. If children do think in an ego-centric way, they may have difficulty in shifting the deictic center. This hypothesis has been received a wide range of discussion in the literature. Some studies prove that egocentrism do exist in children’s minds. Webb and Abrahamson (1975) examine children’s comprehension of some deictic pairs and the result shows that children perform better when their perspective is the same as the experimenter, suggesting that egocentrism may be a central factor in the experiment. Clark and Sengul (1977) argue that in acquiring deictic contrasts, children tend to choose themselves as the deictic center. Other studies, on the other hand, provide evidence to challenge the egocentric view. In De Villiers and De Villiers’ (1974) study, they conclude that at the age of 3 or 4 years old, children are capable of adopting speaker as a point of reference in the comprehension and production of certain deictic terms. These studies show that the notion of egocentrism is still a controversial one and more researches are required to provide additional tests for the validity of the hypothesis. 3.2.H. Clark’s marking hypothesis (1973) The marking hypothesis is established in H. Clark’s (1973) study which states that an unmarked member of a pair will be acquired before a marked one. For spatial demonstratives, ‘that’ is often identified as the unmarked term of the pair ‘this’ and ‘that’ (Kuroda 1968; H. Clark 1973). The prediction that ‘that’ will be learned earlier than ‘this’ then follows. This hypothesis is still under examination. Donaldson and Wales (1970) study children’s acquisition of certain relation terms. Their results support the marking hypothesis in which the positive terms of word pairs are learned first by subjects. A similar result is obtained in De Villiers & de Villers’s (1974) study. Generally speaking, subjects in their study did better on distal terms than on proximal ones. Opposite results, however, are demonstrated in the following researches. As shown in Webb and Abrahamson’s (1975) experiments, children actually perform better on the word ‘this’ than ‘that’. Kuczaj & Maratsos (1975) argue that children demonstrate no difference in their comprehension of ‘front’ and ‘back’. Tanz (1980) also obtains an opposite result in observing children’s acquisition of ‘far’ and ‘close’. She illustrates that the marked member, ‘close’, appears to be acquired earlier than ‘far’. The controversy over this issue suggests that more researches are needed to examine the legitimacy of this hypothesis. 3.3.Children’s acquisition of demonstratives in previous studies Children’s acquisition of demonstratives is a prevalent topic in linguistic inquiry. To gain a complete understanding of children’s acquisition of demonstratives, scholars conduct various kinds of experiments. These experiments do help readers to grasp a general concept on this issue. Webb and Abrahamson (1975), for example, investigate children’s use of ‘this’ and ‘that’, and propose that children first learn the proximity distinction between the two words and then lean that the speaker is the point of reference. Latter, Clark and Sengul (1977) argue that in acquiring deictic contrasts, children go through three phases: from ‘No Contrast’ through ‘Partial Contrast’ to ‘Full Contrast’. Recently, Kuntay et al. (2006) explore children’s acquisition of Turkish demonstrative system which necessarily encodes both proximity distinction and the addressee’s visual attention (present or absent on the referent), and reveal that the ability to take the hearer’s visual attention into consideration during conversation is not 534  obtained even by the age of six. The aforementioned studies, however, suffer from some limitations. These studies are all about western languages (e.g. English and Turkish) and their major concern is the adjective like usage of demonstratives (e.g. Pick up ‘this’ candy). Moreover, the age range of subjects participated in these researches is not broadly-based. To get a full understanding of children’s acquisition of versatile usages of demonstratives, more studies are required. The investigation of children’s acquisition of demonstrative pronouns in Mandarin Chinese, hence, aims to supplement the limitations of previous studies. 4. Method 4.1.Subjects The participants were eight nursery school children in Taipei city. The choice of the participants was based on previous studies. It is generally held that deictic words like this and that are usually present by the age of two and a half (Grant 1915; Nice 1915; Rodrigo 2004) and not until the age of six or seven can children demonstrate adult-like competence in their use of demonstratives (Clark and Sengul 1977; Kuntay 2006). To examine children’s developmental stages and age differences in their use of DPs, eight children aged between three and six were chosen. They were divided into four age groups, each containing two children (n=2): Group 1 three-year-olds, Group 2 four-year-olds, Group 3 five-year-olds, and Group 4 six-year-olds. All the participants were monolingual Mandarin Chinese speakers and all were individually tested in a quiet classroom at their schools. 4.2.Comprehension task 4.2.1.Stimuli The stimuli included twelve sentences: four with the word ‘zhege’, four with the word ‘nage’, and four fillers. The testing order of these sentences was randomized in the task. 4.2.2.Materials Two identical small cartons, one with candies inside and the other with cookies, and 12 puppets were used to test children’s comprehension of DPs. The 12 puppets were animals familiar to children, for instance, monkey, bear, and koala. 4.2.3.Procedure The task was composed of two trials, one with the experimenter sitting beside the child in front of a desk (the same perspective trial), and the other with the experimenter sitting at the opposite side of the desk (the different perspective trial). On the desk, there were two identical cartons, one with candies and the other with cookies; the candy carton was placed nearer the child’s side of the desk, and the other nearer the opposite side of the desk. Each trial required children to make a two-alternative, forced-choice decision between candy and cookie. The child first received six testing sentences (randomized order) in the same perspective trial, two of ‘zhege’ (one with a non-linguistic cue and the other without), two of ‘nage’ (with or without a nonlinguistic cue) and two fillers, and then s/he received the same six sentences in the different perspective trial. To start the experiment, the experimenter first explained to the child that they were to play a role-play game in which the child had to pick up either candy or cookie to testing sentences like ‘我要買這個’ (I want to buy this) or ‘我要買那個’ (I want to buy that) (see Table 1). Notice that if the testing sentence is the one with a non-linguistic cue, the experimenter will give a clear eye gaze at the correct object; if it is the one without a nonlinguistic cue, the experimenter will look directly at the child, avoiding any gestural clues or eye-gazing. As shown in Figure 1, to start the task, the experimenter first sat at position 1 (the 535  same perspective trial) and then after the same perspective trial, the experimenter moved to position 2 to begin the different perspective trial. Note that the two cartons were placed at points equidistant from the child, about an arm’s reach, to avoid any possible bias. Children’s responses will be recorded by an assistant to the experimenter. Each child was recorded by a digital recorder during the task, respectively.  Table 1: Testing sentences used in the comprehension task. Test Sentence  Same perspective trial  With nonlinguistic cue Without nonlinguistic cue  這個 那個 這個 那個  Filler  With nonlinguistic cue  這個 那個  Different perspective trial  Without nonlinguistic cue  這個 那個  Filler  我要買這個 (with eye gaze) 我要買那個 (with eye gaze) 我要買這個 (without eye gaze) 我要買那個 (without eye gaze) 我要買糖果 我要買餅乾 我要買這個 (with eye gaze) 我要買那個 (with eye gaze) 我要買這個 (without eye gaze) 我要買那個 (without eye gaze) 我要買糖果 我要買餅乾  Correct Object Object chosen Candy Cookie Candy Cookie Cookie Candy Cookie Candy  536  Figure 1: Situation used in the comprehension task. 5. Results and discussion This section provides detailed results and general discussion of the experiment in the order of the research questions. Note that one subject in Group 2 was dropped in the experiment because she seems to always choose her preferred snack but not the one required by the experimenter, causing the inconsistency of the result. Mean scores of correct responses with and without eye gaze for each age group were shown in Table 2 and 3, respectively. As can be seen, the existence of a non-linguistic cue does assist children in comprehending these DPs. Compare Table 2 and 3, children achieved a higher score for instructions with a clear eye gaze than for instructions without any physical clues. Table 3 illustrates that without any physical clue children did slightly on ‘zhege’ than ‘nage’, and on same than different perspective trial in general. However, with the aid of a clear eye gaze, as shown in Table 2, children did almost equally well in same and different perspective trials, and in ‘zhege’ and ‘nage’ across the four age groups. Actually the effect of this non-linguistic clue was not apparent on Group 4, and on ‘zhege’ in same perspective trial compared with Table 3; however, if the scores of ‘nage’, of ‘zhege’ in different perspective trial, and of the other three younger groups were considered, the effect of this physical clue was significant. The high scores in instructions with eye gaze seem to suggest that children depend highly on physical clues to figure out the meanings of deictic terms. This argumentation is not unfamiliar in the linguistic field. Clark (1973) and Donaldson and McGarrigle (1974), though not examining DPs, also argue that children’s beginning assumption toward word meanings depend highly on context. At first, children may not know the precise meanings of these deictic terms and make use of all kinds of non-linguistic clues such as eye gaze and gestures to determine the intended focus of the speaker. Latter, they gradually grasp the impression that ‘zhege’ is used specifically under the context that when the speaker wants to refer to a near object while ‘nage’ is used to refer to a far object. In other words, physical expressions may play a central role in assisting children to work out meanings of certain deictic terms. To answer our second research question, let’s consider Table 3 again. The scores in Table 3 reveal that children’s performance was slightly better on the proximal term than the distal one especially in same perspective trial. Compare this result with the scores of production task in Table 4. The results seem to correspond to each other. As illustrates in Table 4, children did significantly perform better on ‘zhege’ than ‘nage’. 537  Both results, hence, seem to challenge H. Clark’s marking hypothesis. H. Clark (1973) argued that children should acquire the positive, unmarked member of a word pair first. However, in the present study children actually performed better on ‘zhege’ than ‘nage’. In fact, this hypothesis has been challenged by results of some empirical studies and some alternative explanations have been provided in the literature (Kuczaj and Maratsos, 1975; Clark and Sengul, 1977; Tanz, 1980). In Kuczaj and Maratsos’s (1975) study, they propose that the order of acquiring members of a pair of demonstratives may correlate with the degree of shifting of reference of the members, that is, the more shifting, the harder to learn. For ‘this’ and ‘that’, they suggest that ‘that’ shifts reference more than ‘this’; this may also account for the better performance on ‘zhege’ than ‘nage’ in this study. Due to a limitation of subject numbers, the overall results for the task were not significant enough to let us judge children’s developmental stages. For that, the children were divided into subgroups based on their individual response patterns in instructions without eye gaze. The representation and classification of response patterns and the grouping of children patterned after Clark and Sengul’ (1977) study with some modification. Following their study, each child’s pattern was represented as a quadruple of ones and zeros. Take the pattern 1001 of Group 1 as an example, the first two digits, 1 and 0, encode the child’s reply to ‘zhege’ and ‘nage’ in same perspective trial correspondingly. The next two digits, 0 and 1, encode his/her response to ‘zhege’ and ‘nage’ in different perspective trial. The pattern 1001, thus, shows that the child was correct on ‘zhege’ in same perspective trial while on ‘nage’ in different perspective trial. Based on individual response patterns, Clark and Sengul divided children into three subgroups, No contrast, Partial contrast, and Full contrast, corresponding to their developmental stages. As shown in Table 5, participants in this study can be assigned to different stages, too. Three subjects were at the No contrast stage. They are all in the younger groups, Group 1 and 2. These subjects seem to consider the two words as pure deictic terms without any meaning contrast. Three subjects were at the Partial contrast stage. Two of them are from Group 3 and one from Group 4. They seem to have a rough idea that the two terms have a distance contrast and hence can differentiate them under some circumstances but not all. The final subject was at the Full contrast stage, having a full control over the two words. The result suggests that in acquiring Chinese DPs, children also go through three successive developmental stages like western children, from No contrast through Partial contrast to Full contrast. Although the developmental stages were proposed in Clark and Sengul’s (1977) study, the corresponding age grading was not clearly provided in their study. The result of this study seems to suggest that children under or equal to the age of four are still in the No contrast stage; from five to six years old, they may pass into the Partial contrast stage and until the age of six or seven they may have gained a full control over these DPs. These successive stages also indicate that the acquisition of deictic meanings is a gradual process. As for Piaget’s egocentrism hypothesis, the result of this study seems to support the hypothesis. Consider children’s response patterns shown in Table 5 again. According to Clark and Sengul (1977), children with the patter 1001, 0110 and 1100 are childcentered while 1011 and 1111 are speak-centered. Based on their assumption, Group 1, 2 and 3 in the present study are all child-centered and Group 4 is speaker-centered. Thus, under the age of six children may still not know that the use of these DPs requires a constant shifting of reference point. The result, hence, suggests that egocentrism do exist in young children’s minds (Webb and Abrahamsom, 1975; Clark and Sengul, 1977) and they seem to apply this bias to language learning, too. As for the age effect, there was no clear evidence shown in the overall comprehension scores. Though Group 4 did perform better than the other three younger groups in instructions without eye gaze, the effect was not clear in the other three groups. However, if the scores of production task (see Table 4) were considered, a patent age effect was revealed. Though the performance of ‘zhege’ did not reveal any considerable difference across age groups, the scores 538  of ‘nage’ did suggest a clear age effect. Children’s ability in producing ‘nage’ seems to increase with age. The performance of Group 1 and 2 were the worst; Group 3 did slightly better and Group 4 gained a full control over the production of the two terms. Also, note that children at the No contrast stage were younger than those at the Partial and Full contrast stages; children at the Partial contrast stage were slightly younger than the one at the Full contrast stage (see Table 5). The subjects in the present study, hence, showed a steady improvement with age. Thus, age does improve children’s learning capacity. It is an important factor in learning deictic terms (De Villiers and De Villiers, 1974; Webb and Abrahamson, 1975; Clark and Sengul, 1977).  Table 2: Mean scores of correct responses for instructions with eye gaze in each age group  Age group (n=2) 
This paper compares two approaches to lexical compound word reconstruction from a speech recognizer output where compound words are decomposed. The ﬁrst method has been proposed earlier and uses a dedicated language model that models compound tails in the context of the preceding words and compound heads only in the context of the tail. A novel approach models imaginable compound particle connectors as hidden events and predicts such events using a simple N -gram language model. Experiments on two Estonian speech recognition tasks show that the second approach performs consistently better and achieves high accuracy. 
Although phrase structure grammars have turned out to be a more popular approach for analysis and representation of the natural language syntactic structures, dependency grammars are often considered as being more appropriate for free word order languages. While building a parser for Latvian, a language with a rather free word order, we found (similarly to TIGER project for German and Talbanken05 for Swedish) that none of these models alone is adequate. Instead, we are proposing an original hybrid formalism that is strongly built on top of the dependency model borrowing the concept of a constituent from the phrase structure approach for representing analytical (multiword) forms. The proposed model has been implemented in an experimental parser and is being successfully applied for description of a wide coverage grammar for Latvian. 
This paper presents a rule-based Norwegian-English MT system. Exploiting the closeness of Norwegian and Danish, and the existence of a well-performing Danish-English system, Danish is used as an «interlingua». Structural analysis and polysemy resolution are based on Constraint Grammar (CG) function tags and dependency structures. We describe the semiautomatic construction of the necessary Norwegian-Danish dictionary and evaluate the method used as well as the coverage of the lexicon. 
This paper describes a new Norwegian speech corpus – The NoTa Corpus – that exhibits a variety of useful and advanced features. It contains 900 000 words of transcribed, lemmatised and POS tagged Oslo speech (carefully selected to cover many speech varieties), which is linked directly to audio and video. It has advanced search interfaces both for searches and results presentations. Since corpora of this kind are aimed at linguists and non-technical users, our guideline has been to keep user-interfaces maximally simple at all levels. The paper describes the contents of the corpus, and focuses on some nice features of its search interface. Some problems and solutions w.r.t. transcription are discussed, and the corpus is compared with five other speech corpora. 
This paper describes a system to extract events and time information from football match reports generated through minute-byminute reporting. We describe a method that uses regular expressions to ﬁnd the events and divides them into different types to determine in which order they occurred. In addition, our system detects time expressions and we present a way to structure the collected data using XML. 
Being able to search for relevant information within a collection of documents is vital for the effective use of any kind of storage media. The main body of Spoken Document Retrieval research has concentrated on the English language, leaving other languages – and information produced in them – without the benefit of existing SDR systems. Especially the performance of vocabulary based speech recognition suffers from inflection. This also affects the usability of retrieval systems based on vocabulary-based recognition techniques. We discuss a method for rapid phoneme filtering to facilitate fast searches for words unknown by large vocabulary speech recognizers. The method is evaluated against a speech database in Finnish, which is a highly inflected language.  (SDR) research has mainly focused on retrieval of English speech. Since English is a language with very limited inflection, current SDR has not adequately dealt with the implications inflection has on neither speech recognition nor its effects on the retrieval task. Compared to English, Finnish is an exceptionally highly inflected language (Karlsson 1983). This does not, however, limit the value of this kind of research only to Finnish. In fact, looking at the languages spoken in Europe, English is one of the least inflectional languages (Lamel 2002). Current SDR methods developed for English do not take into account various difficulties in speech recognition and indexing that arise from inflection. In this paper, we will examine the following questions: 1) How does inflection affect speech recognition and SDR? 2) What solutions are there to deal with inflection in SDR? 3) How to filter/retrieve spoken documents in a highly inflected language? And 4) what is the effectiveness of Spoken Document Filtering on a Finnish test database? How does it compare to text-based retrieval under the same conditions?  
In many language technology applications, we need to map wordforms to a citation form or baseform, or the other way around, e.g. for lexicon lookup or for representational purposes. In this paper, we used a sufﬁx trie mapper with sufﬁx-change probabilities, and computed wordform-baseform and baseformwordform models from eight subsets of a ranked Swedish vocabulary. All models were evaluated for both directions on a testset, and four of the models were also evaluated for wordform-baseform mapping on ﬁve unseen texts. For wordform-baseform mapping, the best models performed on par with state-ofthe-art systems. Most models were useful for some situation—given mapping direction, and time and space restrictions—but no model was best for all situations.  Thus, there is a need for a wordform-baseform mapper. Going in the other direction—from baseform to wordform—could also be useful for applications such as natural language generation, or query expansion in information retrieval. A mapper that can handle both directions reasonably well, perhaps with different underlying language models, would be an extra treat: two applications for the price of one. But what kind of information, and how much, should such models contain? In this paper, we test two assumptions: 1) that irregular wordforms either are among the most top-frequent words in a vocabulary, or so rarely used that they are insigniﬁcant for a robust application, and 2) that rules for regular forms can be induced from a limited number of examples. We describe the induction of various wordformbaseform mapping models (Section 3.3) from subsets of a Swedish vocabulary pool (Section 3.1), in the framework of Wicentowski’s Base Model (Section 3.2). The models are evaluated both on a testset (Section 4.1) and on unseen texts (Section 4.2). 2 Background  
Estonian institutional calls are analyzed with the further aim to develop a dialogue system. The analysis is based on the Estonian Dialogue Corpus. Four types of dialogues are considered: calls to travel agencies and outpatients’ offices, ordering a taxi, and directory inquiries. A customer’s goal is either to get information or to trigger an action by the operator. This goal is achieved in collaboration with the operator. Sub-dialogues are initiated both by the customer and operator in order to achieve sub-goals of the initial goal. A stack is an appropriate data structure for saving goals and sub-goals. 
This paper describes the development of the first text-to-speech (TTS) synthesizer for Latvian language. It provides an overview of the project background and describes the general approach, the choices and particular implementation aspects of the principal TTS components: NLP, prosody and waveform generation. A novelty for waveform synthesis is the combination of corpusbased unit selection methods with traditional diphone synthesis. We conclude that the proposed combination of rather simple language models and synthesis methods yields a cost effective TTS synthesizer of adequate quality. 
This paper describes a system to deﬁne and evaluate development stages in second language French. The identiﬁcation of such stages can be formulated as determining the frequency of some lexical and grammatical features in the learners’ production and how they vary over time. The problems in this procedure are threefold: identify the relevant features, decide on cutoff points for the stages, and evaluate the degree of success of the model. The system addresses these three problems. It consists of a morphosyntactic analyzer called Direkt Proﬁl and a machine-learning module connected to it. We ﬁrst describe the usefulness and rationale behind its development. We then present the corpus we used to develop the analyzer. Finally, we present new and substantially improved results on training machine-learning classiﬁers compared to previous experiments (Granfeldt et al., 2006). We also introduce a method to select attributes in order to identify the most relevant grammatical features. 
Syntactic parsers and generators need highquality grammars of coordination and coordinate ellipsis—structures that occur very frequently but are much less well understood theoretically than many other domains of grammar. Modern grammars of coordinate ellipsis are based nearly exclusively on linguistic judgments (intuitions). The extent to which grammar rules based on this type of empirical evidence generate all and only the structures in text corpora, is unknown. As part of a project on the development of a grammar and a generator for coordinate ellipsis in German, we undertook an extensive exploration of the TIGER treebank—a syntactically annotated corpus of about 50,000 newspaper sentences. We report (1) frequency data for the various patterns of coordinate ellipsis, and (2) several rarely (but regularly) occurring ‘fringe deviations’ from the intuition-based rules for several ellipsis types. This information can help improve parser and generator performance. 
We investigate diﬀerent areas of the highdimensional vector space built by the automatic text summarizer HolSum, which evaluates sets of summary candidates using their similarity to the original text. Previously, the search for a good summary was constrained to a very limited area of the summary space. Since an exhaustive search is not reasonable we have sampled new parts of the space using randomly chosen starting points. We also replaced the simple greedy search with simulated annealing. A greedy search from the leading sentences still ﬁnds the best summary. Finally, we also evaluated a new word weighting scheme: the standard deviation of word distances, comparing it to the previously used tf · log(idf) weighting. Diﬀerent weighting schemes perform similarly, though the term frequency contributes more than other factors. 
This article presents a comparison of the accuracy of a number of different approaches for identifying cross language term equivalents (translations). The methods investigated are on the one hand associative measures, commonly used in word-space models or in Information Retrieval and on the other hand a Statistical Machine Translation (SMT) approach. I have performed tests on six language pairs, using the JRC-Acquis parallel corpus as training material and Eurovoc as a gold standard. The SMT approach is shown to be more effective than the associative measures. The best results are achieved by taking a weighted average of the scores of the SMT approach and disparate associative measures. 
We describe a new method to convert English constituent trees using the Penn Treebank annotation style into dependency trees. The new format was inspired by annotation practices used in other dependency treebanks with the intention to produce a better interface to further semantic processing than existing methods. In particular, we used a richer set of edge labels and introduced links to handle long-distance phenomena such as wh-movement and topicalization. The resulting trees generally have a more complex dependency structure. For example, 6% of the trees contain at least one nonprojective link, which is difﬁcult for many parsing algorithms. As can be expected, the more complex structure and the enriched set of edge labels make the trees more difﬁcult to predict, and we observed a decrease in parsing accuracy when applying two dependency parsers to the new corpus. However, the richer information contained in the new trees resulted in a 23% error reduction in a baseline FrameNet semantic role labeler that relied on dependency arc labels only. 
Self-organizing map (SOM) and multidimensional scaling (MDS) are the methods of data analysis that reduce dimensionality of the input data and visualize the structure of multidimensional data by means of projection. Both methods are widely used in different research areas. In the studies of emotion vocabulary and other psycho-lexical surveys the MDS has been prevalent. In this paper both of the methods are introduced and as an illustration they are applied to a case study of Estonian emotion concepts. There is a need to introduce some new methods to the field because exploiting only one analytical tool may tend to reveal only specific properties of data and thus have an unwanted impact on the results. 
Text-to-scene conversion systems need to share the spatial descriptions between natural language and the 3D scene. Such applications are the ideal candidates for the extraction of spatial relations from free texts, in which the extraction to trajectories that are focus objects in spatial descriptions is an essential problem. We present an analysis of how the space relations are described in Chinese. Based on this study, we propose a method where the extraction of trajectories is modeled as a binary classification problem and resolved based on a linear classifier with syntactic features. Moreover, experimental results are analyzed in detail to demonstrate the effectiveness of the linear classifier to the extraction problem of the trajectory concept. 
We describe and evaluate an incremental ﬁnite-state parser for Icelandic – the ﬁrst parser published for the language. Input to the parser is POS tagged text and it generates output according to a shallow syntactic annotation scheme, speciﬁcally designed for this project. The parser consists of a phrase structure module and a syntactic functions module. Both modules comprise a sequence of ﬁnite-state transducers, each of which adds syntactic information into substrings of the input text. F-measure for constituents and syntactic functions is 96.7% and 84.3%, respectively. These results are good, because Icelandic has a relatively free word order which can be difﬁcult to account for in a parser. Moreover, of the various morphological features available in the rich POS tags, the transducers only use the case feature in their patterns. 
We present a Swedish-Turkish parallel corpus and the automatic annotation procedure with tools that we have been using in order to build the corpus efficiently. The method presented here can be transferred directly to build other parallel corpora. 
This paper describes a new method for compensating bandwidth mismatch for automatic speech recognition using multivariate linear combinations of feature vector components. It is shown that multivariate compensation is superior to methods based on linear compensations of individual features. Performance is evaluated on a real microphone-telephone mismatch condition (this involves noise compensation and bandwidth extension of real data), as well as on several artificial bandwidth limitations. Speech recognition accuracy using this approach is similar to that of acoustic model compensation methods for small to moderate mismatches, and allows keeping active a single acoustic model set for multiple bandwidth limitations. 
The question of grammar coverage in a treebank is addressed from the perspective of language description, not corpus description. We argue that a treebanking methodology based on parsing a corpus does not necessarily imply worse coverage than grammar induction based on a manually annotated corpus. 
We have investigated utterance-initial duration of non-plosive consonants in two qualitatively different Finnish speech corpora. The goal has been to identify any possible lengthening or shortening effects the domain edge (here, the beginning of an utterance) might have on segmental duration. Duration was observed at phone level. The results indicate that cases of lengthening, shortening, and absence of any effect all occur. Those are determined by the speech sounds phonemic identity, and the results were similar in both corpora. For instance /s/ and /r/ are lengthened while /j/ and /m/ are shortened. Contrasted with previous research on various languages, the phonetic universality associated with final lengthening does not apply for initial duration processes. 
This paper presents results of a pilot project for the development of a foreign text comprehension assistant. This tool provides word, phrase and simple sentence translation between the languages of the Baltic countries (Estonian, Latvian and Lithuanian) and widely used European languages (English, German, French and Russian). The paper presents the general architecture of the system, describes its main constituents and outlines difficulties in multilingual phrase translation. The system demonstrates original adaptation of rule based techniques and statistical methods to deal with language specificities, such as inflectional word forms, free word order, and the lack of sizeable, sufficiently representative parallel corpora. 
We introduce a method for the automatic construction of noun entries in a semantic lexicon. Using the entries already present in the lexicon, semantic features are inherited from known to yet unknown words along similar contexts. As contexts, we use three speciﬁc syntactic-semantic relations: modifying adjective, verb-deep-subject and verbdeep-object. The combination of evidences from different contexts yields very high precision for most semantic features, giving rise to the fully automatic incorporation into the lexicon. 
Totally unordered or discontinuous composition blows up chart size on most setups. This paper analyzes the effects of total unordering to type 2 grammars and simple attribute-value grammars (sAVGs). In both cases, charts turn exponential in size. It is shown that the k-ambiguity constraint turns charts polynomial, even for s-AVGs. Consequently, tractable parsing can be deviced. 
The correct attachment of prepositional phrases (PPs) is a central disambiguation problem when parsing natural languages. This paper compares the baseline situation for French as exempliﬁed in the Le Monde treebank with earlier ﬁndings for English, German and Swedish. We perform uniform treebank queries and show that the noun attachment rate for French prepositions is strongly inﬂuenced by the preposition de which is by far the most frequent preposition and has a strong tendency for noun attachment. We therefore also compute the noun attachment rate for the other prepositions separately as well as for the many complex prepositions that are explicitly marked in this treebank. 
In our work with conversational recommender systems we have derived two dialogue strategies called interview and delivery. We explore the symmetry between preferential interview and traditional clariﬁcation questions, and arrive at basic interview and delivery strategies suitable for conversational recommender system implementations. The strategies are based on a corpus analysis of recommendation dialogues in the movie domain. We illustrate the strategies in a conversational music recommender system called CORESONG. 
This paper describes TEXTSIM, a system for determining the similarity between texts. Further, we show the results of a comparison between two various conﬁgurations of TEXTSIM; one with and one without any deeper linguistic analysis. To evaluate and compare the two models of TEXTSIM we used two sets of examples: a set of automatically generated examples and a set of examples acquired from two assessors. Depending on the type of documents, we found the model using linguistic analysis to perform equally well or better than the model not using linguistic analysis. 
This paper presents the construction of a Greek-English bilingual dictionary from parallel corpora that were created manually by collecting documents retrieved from the Internet. The parallel corpora processing was performed by the Uplug word alignment system without the use of language specific information. A sample was extracted from the population of suggested translations and was included in questionnaires that were sent out to Greek-English speakers who evaluated the sample based on the quality of the translation pairs. For the suggested translation pairs of the sample belonging to the stratum with the higher frequency of occurrence, 67.11% correct translations were achieved. With an overall 50.63% of correct translations of the sample, the results were promising considering the minimal optimisation of the corpus and the differences between the two languages. 
We present the first extension of the DataOriented Parsing paradigm (Bod 1998a) to Natural Language Generation: Unmediated Data-Oriented Generation, or UDOG. It is “unmediated” because instead of using a logic-like amodal representation of meaning as a basis for semantics (Van den Berg et al 1994), it exploits direct connections between exemplars in linguistic and nonlinguistic (in this case visual) modalities as the basis for meaning. 
Swedish morphology differs signiﬁcantly from English in several ways. This is something which makes natural language processing based on the English language not always applicable for Swedish material. One area where there is a difference is compounding. The word-forming process of compounding is very productive in Swedish. The compounds are mostly written as one word, without the segmentation point marked in any way. Thus segmentation has to be done in order to interpret the compounds. In this study I have implemented a decomposer which ﬁnds the segmentation point in Swedish compounds, making it easier to handle compounds in natural language processing. Brodda’s algorithm for heuristic compound segmentation guided the work. The decomposer is implemented in TiMBL, a memory-based learner.  in natural language processing tasks which have to be dealt with. Areas where this is important are for example information retrieval, machine translation and speech synthesis. In information retrieval a search query may contain a term which in a certain document only occurs hidden in a compound. Or it can be the other way around. The query may contain a compound but the concepts of any or both of the constituents may in a certain document only occur as simplex words. In order to make use of the constituents of Swedish compounds, one must ﬁrst determine which they are. One problem for languages where compounds are written with no white space between the parts, as in Swedish, is to ﬁnd the parts, that is to determine the compound segmentation point. In this paper I describe an experiment using memory-based learning for compound segmentation. The hypothesis is that this should work because n-grams around a segmentation point tend to contain grapheme clusters that do not occur in simplex words (Brodda, 1979). The learner is thus trained to distinguish clusters appearing around segmentation points from other clusters.  
A basic task in machine translation is to choose the right translation for source words with several possible translations in the target language. In this paper we treat word translation as a word sense disambiguation problem and train memory-based classifiers on words with alternative translations. The training data was automatically labeled with the corresponding translations by word-aligning a parallel corpus. Results show that many words were translated with accuracy above the baseline. 
We argue that ﬁnite clauses should be regarded as the basic unit in syntactic analysis of spoken language, and describe a method that automatically detects clause boundaries by classifying coordinating conjunctions in spoken language discourse as belonging to either the syntactic level or the discourse level of analysis. The method exploits the special role that coordinating conjunctions play in organizing spoken language discourse, and that coordinating conjunctions at discourse level mark clause boundaries. 
Spoken language contains disﬂuencies that, because of their irregular nature, may lead to reduced performance of data-driven parsers. This paper describes an experiment that quantiﬁes the effects of disﬂuency detection and disﬂuency removal on data-driven parsing of spoken language data. The experiment consists of creating two reduced versions from a spoken language treebank, the Switchboard Corpus, mimicking a speechrecognizer output with and without disﬂuency detection and deletion. Two datadriven parsers are applied on the new data, and the parsers’ output is evaluated and compared. 
This paper describes work on the grammatical tagging of a newly created Norwegian speech corpus: the ﬁrst corpus of modern Norwegian speech. We use an iterative procedure to perform computer-aided manual tagging of a part of the corpus. This material is then used to train the ﬁnal taggers, which are applied to the rest of the corpus. We experiment with taggers that are based on three different data-driven methods: memory-based learning, decision trees, and hidden Markov models, and ﬁnd that the decision tree tagger performs best. We also test the effects of removing pauses and/or hesitations from the material before training and applying the taggers. We conclude that these attempts at cleaning up hurt the performance of the taggers, indicating that such material, rather than functioning as noise, actually contributes important information about the grammatical function of the words in their nearest context. 
This paper presents a short description of work recently done at University of Tartu to construct a word–based speech recognition system. Simple bigram and trigram language models with cross–word triphone acoustic models are used by a one–pass best hypothesis recognizer to perform decoding of test data. The lowest word error rate of 37.5% reported in this paper is a common ﬁgure for word–based speech recognition of languages like Estonian. 
Rule-based multilingual natural language processing (NLP) applications such as machine translation systems require the development of grammars for multiple languages. Grammar writing, however, is often a slow and laborious process. In this paper we describe a methodology for multilingual and multipurpose grammar development based on grammar sharing. This paper presents the first step towards a language independent core grammar used for recognition, analysis and generation of English, Japanese and Finnish used in a domain specific spoken language translation system. The paper focuses on the grammar architecture and rule writing principles. Evaluation on analysis and generation has shown that two thirds of the rules are shared between these three typologically different languages. 
 2 Experiment  This paper is part of an extended study on system architectures, the long term aim being to determine if a unidirectional, a bidirectional or a fixed-phrase architecture is more suitable in the context of the spoken language translator in the medical domain (MedSLT). Our aim here is to compare data collected during a Wizard of Oz (WOz) experiment with data collected using a beta bidirectional version of our system. 
Extraction of temporal expressions from an input text is an important step in natural language processing tasks. Automated extraction of temporal expressions can be used in dialogue systems where temporal constraints need to be enforced. The paper proposes an algorithm for processing temporal information in natural language. The algorithm was implemented as a standalone rule-based temporal expression recognizer and was made available as a web-service. Finally the implemented module was partially integrated into a spoken language dialogue system that is an interface to a theater information database.  The paper proposes an algorithm for processing temporal information in natural language. The algorithm was implemented to work on Estonian texts and partially integrated to an Estonian spoken language dialogue system that is an interface to a theater information database (Treumuth, et al., 2006). The extraction tool of time expressions was implemented as a standalone non-domain-specific module, and was made available as a web-service, that can be plugged into dialogue systems with some minor adjustments. The time expression recognizer could be a useful software tool in the following list of currently available Estonian language technology software tools: • Text-to-speech synthesizer (Meister, et al, 2003) (Mihkla, et al, 1999)  There is no evaluation as this work is still in progress.  • Speech recognizer (speech-to-text): experimental version (Alumäe, 2004)  
This paper presents an English-Swedish Parallel Treebank, LinES, that is currently under development. LinES is intended as a resource for the study of variation in translation of common syntactic constructions from English to Swedish. For this reason, annotation in LinES is syntactically oriented, multi-level, complete and manually reviewed according to guidelines. Another aim of LinES is to support queries made in terms of types of translation shifts. 
In this paper, we present improved wordlevel confidence measures based on posterior probabilities for children’s oral reading continuous speech recognition. Initially we compute posterior probability based confidence measures on word graphs using a forward-backward algorithm. We study how an increase of the word graph density affects the quality of these confidence measures. For this purpose we merge word graphs obtained using three different language models and compute the previous confidence measures over the resulting word graph. This produces a relative error reduction of 8% in Confidence Error Rate compared to the baseline confidence measure. Moreover the system operating range is increased significantly. 
This paper describes the experiments that apply phrase-based statistical machine translation to Estonian. The work has two main aims: the first one is to define the main problems in the output of EstonianEnglish statistical machine translation and set a baseline for further experiments with this language pair. The second is to compare the two available corpora of translated legislation texts and test them for compatibility. The experiment results show that statistical machine translation works well with that kind of text. The corpora appear to be compatible, and their combining – beneficial. 
We present a data-driven parser that derives both constituent structures and dependency structures, alone or in combination, in one and the same process. When trained and tested on data from the Swedish treebank Talbanken05, the parser achieves a labeled dependency accuracy of 82% and a labeled bracketing F-score of 75%. 
We introduce an Icelandic corpus of more than 250 million running words and describe the methodology to build it. The resource is available for use free of charge. We provide automatically generated monolingual lexicon entries, comprising frequency statistics, samples of usage, cooccurring words and a graphical representation of the word’s semantic neighbourhood. 
This paper surveys work on unsupervised learning of morphology. A fairly broad demarcation of the area is given, and a hierarchy of subgoals is established in order to properly characterize each line of work. All the minor and major lines of work are mentioned with a reference and a brief characterization. Different approaches that have been prevalent in the ﬁeld as a whole are highlighted and critically discussed. The general picture resulting from the survey is that much work has been repeated over and over, with little exchange and evolution of techniques. All in all, the contribution of this paper is a very brief but comprehensive umbrellic synopsis to the research area. 
Danish phonetics and Norwegian phone-tics are not all that different. This fact is exploited in an ongoing project establishing a phonetic transcription algorithm for (East-) Norwegian. Using methods known from machine learning we exploit a publicly available phonetic database for Danish (based on the Danish PAROLE corpus) arriving at a costcompetitive phonetic database for Norwegian. While the ultimate goal of this enterprise is a low-budget complete phonetic transcription of the NoTa corpus of Norwegian spontaneous speech, this paper presents the subparts related to Danish phonetics. 
In this paper we describe DISCUS, a research tool for developing a context model and update algorithm for dialogue management. The model builds on Dynamic Interpretation Theory (DIT), in which dialogue is modelled in terms of dialogue acts operating on the information state of the dialogue participants. On the basis of dialogue act speciﬁcations of both system and user utterances, DISCUS performs the update of the system’s context model. The context model is structured into several components and contains complex elements involving the beliefs and goals of both system and user. We will present simulations of two dialogues, one for demonstrating the context update model, and another in which the system utterances are generated automatically. 
This paper presents a new management method for morphological variation of keywords. The method is called FCG, Frequent Case Generation. It is based on the skewed distributions of word forms in natural languages and is suitable for languages that have either fair amount of morphological variation or are morphologically very rich. The proposed method has been evaluated so far with four languages, Finnish, Swedish, German and Russian, which show varying degrees of morphological complexity. 
We present an automatically created Swedish-Thai lexicon. The lexicon was created by matching the English translations in a Thai-English and a Swedish-English lexicon. The search interface to the lexicon includes several NLP tools to help the target group: second language learners of Swedish. These include automatic generation of inﬂectional forms of words, automatic spelling correction, lemmatization and compound analysis of queries. A user study was performed and showed that while erroneous translations sometimes fool the users, they still ﬁnd the lexicon good enough to be useful. They also like the NLP tools, though some grammatical information is presented in a hard to understand way. The lexicon and the interface tools were built using commonly available NLP tools. 
In the era of the Electronic Health Record the release of medical narrative textual data for research, for health care statistics, for monitoring of new diagnostic tests and for tracking disease outbreak alerts imposes tough restrictions by various public authority bodies for the protection of (patient) privacy. In this paper we present a system for automatic identification of named entities in Swedish clinical free text, in the form of discharge letters, by applying generic named entity recognition technology with minor adaptations. 
This paper reports on a corpus-based, contrastive study of the Swedish and English medical language in the cancer sub-domain. It is focused on the examination of a number of linguistic parameters differentiating two types of cancer-related textual material, one intended for medical experts and one for laymen. Language-dependent and language independent characteristics of the textual data between the two languages and the two registers are examined and compared. The aim of the work is to gain insights into the differences between lay and expert texts in order to support natural language generation (NLG) systems. 
This document describes an XML-based data model for annotated, modular text corpora along with a WWW-interface for browsing such corpora, reading the texts, searching for examples, and extracting information of word usages. The interface is based solely on programs and techniques belonging to the XML-family. The corpus model is designed in such a way that new parts (texts, sub-corpora) can be easily plugged in the system as far as they ﬁt into the model. Furthermore, the model includes slots for such parts that are not conventionally included in text corpora. These may include (digitized) originals of the texts and links to other relevant documents. The searching interface for the model is based on XML query language that enables the developers to add queries to the system for extracting detailed linguistic information from the texts, depending on their annotation level. The corpus model and its interface can be seen as a step towards a general quantitative tool for text linguistic research, including the data model and programs for browsing, querying, and analyzing the texts. 
This paper deals with the perceptual assessment of Russian-accented Estonian. Speech samples were recorded from 20 speakers with a Russian background; clips of about 20 seconds from each speaker were selected for this perceptual study. The accentedness was rated in two tests: first, 20 native Estonian speakers judged the samples and rated the degree of foreign accent on a six-point interval scale; secondly, two experienced phoneticians carried out a perceptual study of the same samples and compiled the list of pronunciations errors. The results of both listening tests were highly correlated – the higher the degree of accentedness given to a L2-speaker by naïve listeners, the more pronunciation errors were found by trained experts. The classification of most frequent pronunciation errors based on acoustic-phonetic features is given, as well.  language (L2) speaker deviates from that of L1speakers (Southwood & Flege, 1999). On the contrary, a trained phonetician should be able to identify and classify different accent phenomena as well as describe them in terms of deviations of acoustic-phonetic features. Following the findings and methodology presented in a recent paper (Meister, 2006; for methods employed in different studies see Jesney, 2004) on the accentedness rating of foreign-accented Estonian, two further listening experiments have been designed. The aim of these experiments is to compare the accentedness ratings given by naïve listeners, and the results of perceptual analysis of pronunciation errors carried out by experienced phoneticians. It is expected that the results of these two groups of raters harmonize quite well, i.e., the higher the accentedness ratings by naïve listeners of L2 speakers are, the more pronunciation errors are listed by experts. The study serves also a longterm goal – the development of criteria for speaking proficiency assessment, including the degree of FA. 2 Method  
This paper presents how word alignment techniques could be used for building standardized term banks. It is shown that time and effort could be saved by a relatively simple evaluation metric based on frequency data from term pairs, and source and target distributions inside the alignment results. The proposed Q-value metric is shown to outperform other tested metrics such as Dice’s coefficient, and simple pair frequency. 
This paper proposes representing the semantics of natural-language calendar expressions as a sequence of compositions of ﬁnite-state transducers (FSTs) that bracket the denoted periods of time on a ﬁnite timeline of nested calendar periods. In addition to simple dates and times of the day, the approach covers more complex calendar expressions. The paper illustrates the model by walking through the representation of the calendar expression January to March and May 2007. The representation of the expressions considered is compositional with reference to their subexpressions. The paper also outlines possible applications of the model, based on ﬁnding the common periods of time denoted by two calendar expressions. 
An experiment with an Estonian Constraint Grammar based syntactic analyzer is conducted, analyzing transcribed speech. In this paper the problems encountered during parsing disfluencies are analyzed. In addition, the amount by which the manual normalization of disfluencies improved the results of recall and precision was compared to non-normalized utterances. 
The article reports the development of a speech corpus for Estonian text-to-speech synthesis based on unit selection. Introduced are the principles of the corpus as well as the procedure of its creation, from text compilation to corpus analysis and text recording. Also described are the choices made in the process of producing a text of 400 sentences, the relevant lexical and morphological preferences, and the way to the most natural sentence context for the words used. 
This paper presents a thorough examination of the validity of three evaluation measures on parser output. We assess parser performance of an unlexicalised probabilistic parser trained on two German treebanks with different annotation schemes and evaluate parsing results using the PARSEVAL metric, the Leaf-Ancestor metric and a dependency-based evaluation. We reject the claim that the Tu¨Ba-D/Z annotation scheme is more adequate then the TIGER scheme for PCFG parsing and show that PARSEVAL should not be used to compare parser performance for parsers trained on treebanks with different annotation schemes. An analysis of speciﬁc error types indicates that the dependency-based evaluation is most appropriate to reﬂect parse quality. 
We report on the creation of a Modern Greek broadcast-news corpus as a pre-requisite to build a large-vocabulary continuous-speech recognition system. We discuss lexical modelling with respect to pronuciation generation and examine the effects of the lexicon size on word accuracies. Peculiarities of Modern Greek as a highly inﬂectional language and their challenges for speech recognition are discussed. 
The present study examines the difference between categorization and goodness ratings in Udmurt (Finno-Ugric language) using different sets of spectral attributes. The tendency to have two areas of /u/ vowels was observed in languages with unrounded non-front closed vowels during the TURVOTES project. Our study explores whether this can be due to the different acoustic attributes used in the identification and goodness ratings of these vowels. The identification and goodness-rating of Udmurt close vowels confirmed the observation. The model using only formants was not significant for identification data, but did explain the goodness rating data. The spectral moments explained both identification and goodness ratings. 
We present a program that recreates split compound errors with amusing eﬀects in written Swedish. Two useful criteria for funniness is that the result should be grammatical and that the compound words should not be split into many short components. 
 2 The Question Classiﬁcation Task  This paper presents a re-examiniation of previous work on machine learning techniques for questions classiﬁcation, as well as results from new experiments. The results suggest that some of the work done in the ﬁeld have yielded biased results. The results also suggest that Na¨ıve Bayes, Decision Trees and Support Vector Machines perform on par with each other when faced with actual users’ questions. 
Estonian institutional phone calls are analyzed with the further aim to develop a human-computer dialogue system. The analysis is based on the Estonian Dialogue Corpus. Linguistic cues of yes/no questions are found out that can be used for their automatic recognition. 
In this paper, we present concise but robust rules for dependency-based logical form identiﬁcation with high accuracy. We describe our approach from an intuitive and formalized perspective, which we believe overcomes much of the complexity. In comparison to previous work, we believe ours is more compact and involves less rules and exceptions. We also provide the reader with a comparison of the respective impacts of the most essential rules on the logical form identiﬁcation task of the 2004 Senseval 3 test set. 
The paper discusses quality of service evaluation which emphasises the user’s experience in the evaluation of system functionality and efficiency. For NLG systems, an important quality feature is communicatively adequate language generation, which affects the users’ perception of the system and consequently, evaluation results. The paper drafts an evaluation task that aims at measuring quality of service, taking the system’s communicative competence into account. 
Generation of Referring Expressions is a thriving subﬁeld of Natural Language Generation which has traditionally focused on the task of selecting a set of attributes that unambiguously identify a given referent. In this paper, we address the complementary problem of generating repeated, potentially different referential expressions that refer to the same entity in the context of a piece of discourse longer than a sentence. We describe a corpus of short encyclopaedic texts we have compiled and annotated for reference to the main subject of the text, and report results for our experiments in which we set human subjects and automatic methods the task of selecting a referential expression from a wide range of choices in a full-text context. We ﬁnd that our human subjects agree on choice of expression to a considerable degree, with three identical expressions selected in 50% of cases. We tested automatic selection strategies based on most frequent choice heuristics, involving different combinations of information about syntactic MSR type and domain type. We ﬁnd that more information generally produces better results, achieving a best overall test set accuracy of 53.9% when both syntactic MSR type and domain type are known. 
We present a log-linear model that is used for ranking the string realisations produced for given corpus f-structures by a reversible broadcoverage LFG for German and compare its results with the ones achieved by the application of a language model (LM). Like other authors that have developed log-linear models for realisation ranking, we use a hybrid model that uses linguistically motivated learning features and a LM (whose score is simply integrated into the log-linear model as an additional feature) for the task of realisation ranking. We carry out a large evaluation of the model, training on over 8,600 structures and testing on 323. We observe that the contribution that the structural features make to the quality of the output is slightly greater in the case of a free word order language like German than it is in the case of English. The exact match metric improves from 27% to 37% when going from the LM-based realisation ranking to the hybrid model, BLEU score improves from 0.7306 to 0.7939. 
In this paper we present a view of natural language generation in which the control structure of the generator is clearly separated from the content decisions made during generation, allowing us to explore and compare different control strategies in a systematic way. Our approach factors control into two components, a ‘generation tree’ which maps out the relationships between different decisions, and an algorithm for traversing such a tree which determines which choices are actually made. We illustrate the approach with examples of stylistic control and automatic text revision using both generative and empirical techniques. We argue that this approach provides a useful basis for the theoretical study of control in generation, and a framework for implementing generators with a range of control strategies. We also suggest that this approach can be developed into tool for analysing and adapting control aspects of other advanced wide-coverage generation systems. 
We investigate two methods for enhancing variation in the output of a stochastic surface realiser: choosing from among the highest-scoring realisation candidates instead of taking the single highestscoring result (ε-best sampling), and penalising the words from earlier sentences in a discourse when generating later ones (anti-repetition scoring). In a human evaluation study, subjects were asked to compare texts generated with and without the variation enhancements. Strikingly, subjects judged the texts generated using these two methods to be better written and less repetitive than the texts generated with optimal n-gram scoring; at the same time, no signiﬁcant difference in understandability was found between the two versions. In analysing the two methods, we show that the simpler ε-best sampling method is considerably more prone to introducing dispreferred variants into the output, indicating that best results can be obtained using antirepetition scoring with strict or no ε-best sampling. 
We present a method for quickly spotting overgeneration suspects (i.e., likely cause of overgeneration) in hand-coded grammars. The method is applied to a medium size Tree Adjoining Grammar (TAG) for French and is shown to help reduce the number of outputs by 70% almost all of it being overgeneration. 
Despite being the focus of intensive research, evaluation of algorithms that generate referring expressions is still in its infancy. We describe a corpusbased evaluation methodology, applied to a number of classic algorithms in this area. The methodology focuses on balance and semantic transparency to enable comparison of human and algorithmic output. Although the Incremental Algorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difﬁcult to predict. 
Politeness is an integral part of human language variation, e.g. consider the difference in the pragmatic effect of realizing the same communicative goal with either “Get me a glass of water mate!” or “I wonder if I could possibly have some water please?” This paper presents POLLy (Politeness for Language Learning), a system which combines a natural language generator with an AI Planner to model Brown and Levinson’s theory of politeness (B&L) in collaborative task­oriented dialogue, with the ultimate goal of providing a fun and stimulating environment for learning English as a second language. An evaluation of politeness perceptions of POLLy’s output shows that: (1) perceptions are generally consistent with B&L’s predictions for choice of form and for discourse situation, i.e. utterances to strangers need to be much more polite than those to friends; (2) our indirect strategies which should be the politest forms, are seen as the rudest; and (3) English and Indian native speakers of English have different perceptions of the level of politeness needed to mitigate particular face threats. Introduction Politeness is an integral part of human language variation in conversation, e.g. consider the difference in the pragmatic effect of realizing the same communicative goal with either “Get me a glass of water mate!” or “I wonder if I could possibly have some water please?”, with choices  of these different forms driven by sociological norms among human speakers (Brown & Levinson, 1987). Recent work on conversational agents suggests that such norms are an important aspect of language generation for human­computer conversation as well (Walker et al., 1997; André et al., 2000; Reeves & Nass, 1996; Cassell & Bickmore, 2003; Porayska­Pomsta, 2003; Johnson et al., 2004). (Walker et al., 1997) were the first to propose and implement Brown & Levinson’s (1987) theory of politeness, henceforth B&L, in conversational agents. Their goal was to provide interesting variations of character and personality in an interactive narrative application. Subsequent work has shown the value of politeness strategies based on B&L in many conversational applications, e.g. tutorial dialogue (Porayska­Pomsta, 2003; Johnson et al., 2004), animated presentation teams (André et al.,2000; Rehm and Andre, 2007), real estate sales (Cassell & Bickmore, 2003), and has also shown that the cross­cultural claims of B&L hold up in these contexts (Johnson et al., 2005). This paper presents POLLy (Politeness for Language Learning), a system which combines a natural language generator with an AI Planner to model B&L’s theory of politeness in task­ oriented dialogue. Our hypothesis is that politeness forms are difficult for non­native speakers to learn, and that a virtual environment where learners can interact with virtual agents embodying different politeness strategies in different discourse contexts, will provide a fun and stimulating environment for learning English as a second language (ESL). As a first step, we evaluate the use of different politeness strategies in task­oriented dialogues in a  57  Figure 1: Complete System Architecture  collaborative task domain of cooking, where subjects are asked to collaborate with another person to make a recipe. We show that: (1) politeness perceptions of POLLy’s output are generally consistent with B&L’s predictions for choice of form and for discourse situation, i.e. utterances to strangers need to be more polite than those to friends; (2) our indirect strategies which should be the politest forms, are seen as the rudest; and (3) English and Indian speakers of English have different perceptions of the level of politeness needed to mitigate particular face threats. Section 1 describes POLLy’s architecture and functionality. Section 2 describes an experiment to evaluate user’s perceptions of automatically generated task­ oriented polite language and Section 3 presents the experimental results. Section 4 sums up and compares our results with previous work. 
The potential of sentence generators as engines in Intelligent Computer-Assisted Language Learning and teaching (ICALL) software has hardly been explored. We sketch the prototype of COMPASS, a system that supports integrated writing and grammar curricula for 10 to 14 year old elementary or secondary schoolers. The system enables first- or second-language teachers to design controlled writing exercises, in particular of the “sentence combining” variety. The system includes facilities for error diagnosis and on-line feedback. Syntactic structures built by students or system can be displayed as easily understood phrase-structure or dependency trees, adapted to the student’s level of grammatical knowledge. The heart of the system is a specially designed generator capable of lexically guided sentence generation, of generating syntactic paraphrases, and displaying syntactic structures visually. 
This paper describes a model of the choice of modal verbs and modal particles. The choice mechanism does not require a modality-specific input as, e.g., a modal logical formula. Instead semantic (modal force) and pragmatic constraints (speech act marking) are applied to the available information on the whole and constrain the set of modal candidates to those that are appropriate in the respective contexts. The choice model is realized in the CAN system that generates recommendations about courses of study. 
 While a good ordering is essential for summary  The issue of sentence ordering is an important one for natural language tasks such as multi-document summarization, yet there has not been a quantitative exploration of the range of acceptable sentence orderings for short texts. We present results of a sentence reordering experiment with three experimental conditions. Our ﬁndings indicate a very high degree of variability in the orderings that the eighteen subjects produce. In addition, the variability of reorderings is signiﬁcantly greater when the initial ordering seen by subjects is different from the original summary. We conclude that evaluation of sentence ordering should use multiple reference orderings. Our evaluation presents several metrics that might prove useful in assessing against multiple references. We conclude with a deeper set of questions: (a) what sorts of independent assessments of quality of the different reference orderings could be made and (b) whether a large enough test set would obviate the need for such independent means of quality assessment.  comprehension (Barzilay et al., 2002), and recent work on sentence ordering (Bollegala et al., 2006) does show promise, it is important to note that determining an optimal sentence ordering for a given summary may not be feasible. The question for evaluation of ordering is whether there is a single best ordering that humans will converge on, or that would lead to maximum reading comprehension, or that would maximize another extrinsic summary evaluation measure. On texts of approximately the same length as summaries we look at here, Karamanis et al. (2005) found that experts produce different sentence orderings for expressing database facts about archaeology. We ﬁnd that summaries of newswire have a relatively larger set of coherent orderings. We conducted an experiment where human subjects were asked to reorder multi-document summaries in order to maximize their coherence. The summaries used in this experiment were originally produced by a different set of human summarizers as part of a multi-document summarization task that  
In this paper we introduce a method for generating interactive documents which exploits the visual features of hypertext to represent discourse structure. We explore the consistent and principled use of graphics and animation to support navigation and comprehension of non-linear text, where textual discourse markers do not always work effectively. 
Existing generation systems use verbs almost exclusively to describe actions/events or to ascribe properties. In doing so, they achieve a direct concrete style of the kind often recommended in style manuals. However in many genres, including academic writing, it is common to ﬁnd verbs expressing abstract relationships, with events etc. pushed down into nominalisations. This paper illustrates two important classes of abstract verb, one expressing discourse relations, the other expressing participant roles, and discusses some theoretical and practical reasons for studying such verbs and including them in generation systems. 
This paper introduces our domain independent approach to “free generation” from single RDF triples without using any domain dependent knowledge. Our approach is developed based on our argument that RDF representations carry rich linguistic information, which can be used to achieve readable domain independent generation. In order to examine to what extent our argument is realistic, we carry out an evaluation experiment, which is the first evaluation of this kind of domain independent generation in the field. Introduction In the Semantic Web, both instance data and ontological data1 are represented as graphs based on the Resource Description Framework (RDF) (W3C 2004). In order to facilitate non-technician users to access the knowledge and information coded in RDF, we are eventually aiming at developing a domain independent approach to presenting RDF graphs in natural language, which can greatly reduce the cost of applying NLG techniques to various RDF domains (e.g., medical RDF data and chemical RDF data). In this paper we introduce our domain independent approach to generating phrases or sentences from single RDF triples2 without using any domain knowledge but only generic linguistic knowledge sources. This contrasts with almost all existing work generating natural language from 
We present the Narrator, an NLG component used for the generation of narratives in a digital storytelling system. We describe how the Narrator works and show some examples of generated stories. 
Almost all existing referring expression generation algorithms aim to ﬁnd one best referring expression for a given intended referent. However, human-produced data demonstrates that, for any given entity, many perfectly acceptable referring expressions exist. At the same time, it is not the case that all logically possible descriptions are acceptable; so, if we remove the requirement to produce only one best solution, how do we avoid generating undesirable descriptions? Our aim in this paper is to sketch a framework that allows us to capture constraints on referring expression generation, so that the set of logically possible descriptions can be reduced to just those that are acceptable. 
We present an empirical approach to adaptively selecting a tutoring system’s remediation strategy based on an annotated corpus of human-human tutorial dialogues. We are interested in the remediation selection problem, that of generating the best remediation strategy given a diagnosis for an incorrect answer and the current problem solving context. By comparing the use of individual remediation strategies to their success in varying contexts, we can empirically extract and implement tutoring rules for the content planner of an intelligent tutoring system. We describe a methodology for analyzing a tutoring corpus and using the resulting data to inform a content planning model. 
This paper discusses an implemented dialogue system which generates the meanings of utterances by taking into account: the surface mood of the user’s last utterance; the meanings of all the user’s utterances from the current discourse; the system’s expert knowledge; and the system’s beliefs about the current situation arising from the discourse (including its beliefs about the user and her beliefs, and its beliefs about what is ‘common knowledge’). The system formulates the content of its responses by employing an epistemic theorem prover to do deep reasoning. During the reasoning process, it remembers the proof tree it constructs, and from this derives the meaning of an explanatory response. 
This paper reports on work in progress on extending the entity-based approach on measuring coherence (Barzilay & Lapata, 2005; Lapata & Barzilay, 2005) from coreference to semantic relatedness. We use a corpus of manually annotated German newspaper text (Tu¨Ba-D/Z) and aim at improving the performance by grouping related entities with the WikiRelate! API (Strube & Ponzetto, 2006). 
We introduce Naturalowl, an open-source multilingual natural language generator that produces descriptions of instances and classes, starting from a linguistically annotated ontology. The generator is heavily based on ideas from ilex and m-piro, but it is in many ways simpler and it provides full support for owl dl ontologies with rdf linguistic annotations. Naturalowl is written in Java, and it is supported by m-piro’s authoring tool, as well as an alternative plug-in for the Prot´eg´e ontology editor. 
This paper discusses the generation of cryptic crossword clues: a task that involves generating texts that have both a surface reading, based on a natural language interpretation of the words, and a hidden meaning in which the strings that form the text can be interpreted as a puzzle. The process of clue generation realizes a representation of the hidden, puzzle meaning of the clue through the aggregation of chunks of text. As these chunks are combined, syntactic and semantic selectional constraints are explored, and through this language understanding task a meaningful surface reading is recovered. This hybrid language generation/language understanding process transforms a representation of the clue as a word puzzle into a representation of some meaningful assertion in the domain of the real world, mediated through the generated multi-layered text; a text which has two separate readings. 
We describe a new application for NLG technology: the generation of indicative, abstractive summaries of multi-party meetings. Based on the freely available AMI corpus of 100 hours of recorded meetings, we are developing a summarizer that uses the rich annotations in the AMI corpus. 
We assess the use of hedge phrases in “affective” NLG texts. A simple experiment suggests non-native speakers prefer texts that contain hedge phrases, but native speakers prefer texts that do not contain hedge phrases. 
An existing taxonomy of Dutch cue phrases, designed for use in story generation, was validated by analysing cue phrase usage in a corpus of classical fairy tales. The analysis led to some adaptations of the original taxonomy. 
Geo-referenced data which are often communicated via maps are inaccessible to the visually impaired population. We summarise existing approaches to improving accessibility of geo-referenced data and present the Atlas.txt project which aims to produce textual summaries of such data which can be read out via a screenreader. We outline issues involved in generating descriptions of geo-referenced data and present initial work on content determination based on knowledge acquisition from both parallel corpus analysis and input from visually impaired people. In our corpus analysis we build an ontology containing abstract representations of expert-written sentences which we associate with macros containing sequences of data analysis methods. This helps us to identify which data analysis methods need to be applied to generate text from data. 
Medical information is notoriously diﬃcult to convey to patients because the content is complex, emotionally sensitive, and hard to explain without recourse to technical terms. We describe a pilot system for communicating the contents of electronic health records (EHRs) to patients. It generates two alternative presentations, which we have compared in a preliminary evaluation study: the ﬁrst takes the form of a monologue, which elaborates the information taken from the patient’s EHR by adding explanations of some concepts and procedures; the second takes the form of a scripted dialogue, in which the content is recast as a series of questions, answers and statements assigned to two characters in the dialogue, a senior and a junior nurse. Our discourse planning method designs these presentations in tandem, ﬁrst producing a monologue plan which is then elaborated into a dialogue plan. 
A method is described to incorporate bilexical preferences between phrase heads, such as selection restrictions, in a MaximumEntropy parser for Dutch. The bilexical preferences are modelled as association rates which are determined on the basis of a very large parsed corpus (about 500M words). We show that the incorporation of such selftrained preferences improves parsing accuracy signiﬁcantly. 
This paper describes an effective approach to adapting an HPSG parser trained on the Penn Treebank to a biomedical domain. In this approach, we train probabilities of lexical entry assignments to words in a target domain and then incorporate them into the original parser. Experimental results show that this method can obtain higher parsing accuracy than previous work on domain adaptation for parsing the same data. Moreover, the results show that the combination of the proposed method and the existing method achieves parsing accuracy that is as high as that of an HPSG parser retrained from scratch, but with much lower training cost. We also evaluated our method in the Brown corpus to show the portability of our approach in another domain. 
We compare the accuracy of a statistical parse ranking model trained from a fully-annotated portion of the Susanne treebank with one trained from unlabeled partially-bracketed sentences derived from this treebank and from the Penn Treebank. We demonstrate that conﬁdence-based semi-supervised techniques similar to self-training outperform expectation maximization when both are constrained by partial bracketing. Both methods based on partially-bracketed training data outperform the fully supervised technique, and both can, in principle, be applied to any statistical parser whose output is consistent with such partial-bracketing. We also explore tuning the model to a diﬀerent domain and the eﬀect of in-domain data in the semi-supervised training processes. 
We introduce a set of 1,000 gold standard parse trees for the British National Corpus (BNC) and perform a series of self-training experiments with Charniak and Johnson’s reranking parser and BNC sentences. We show that retraining this parser with a combination of one million BNC parse trees (produced by the same parser) and the original WSJ training data yields improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set. 
As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007), we were asked to discuss our perspectives on the role of current trends in deep linguistic processing for parsing technology. We are particularly interested in the ways in which efﬁcient, broad coverage parsing systems for linguistically expressive grammars can be built and integrated into applications which require richer syntactic structures than shallow approaches can provide. This often requires hybrid technologies which use shallow or statistical methods for pre- or post-processing, to extend coverage, or to disambiguate the output. 
The C&C CCG parser is a highly efﬁcient linguistically motivated parser. The efﬁciency is achieved using a tightly-integrated supertagger, which assigns CCG lexical categories to words in a sentence. The integration allows the parser to request more categories if it cannot ﬁnd a spanning analysis. We present several enhancements to the CKY chart parsing algorithm used by the parser. The ﬁrst proposal is chart repair, which allows the chart to be efﬁciently updated by adding lexical categories individually, and we evaluate several strategies for adding these categories. The second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage. 
We extend a recently proposed algorithm for n-best unpacking of parse forests to deal efﬁciently with (a) Maximum Entropy (ME) parse selection models containing important classes of non-local features, and (b) forests produced by uniﬁcation grammars containing signiﬁcant proportions of globally inconsistent analyses. The new algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature nonlocality; in addition, compared with agendadriven best-ﬁrst parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.† 
This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply deﬁned as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as deﬁned in the CCG/HPSG/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well deﬁned. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the ﬁrst model which properly incorporates the supertagging probabilities into parse tree’s probabilistic model. 
Writing aids such as spelling and grammar checkers are often based on texts by adult writers and are not sufficiently targeted to support children in their writing process. This paper reports on the development of a writing tool based on a corpus of Swedish text written by children and on the parsing methods developed to handle text containing errors. The system uses finite state techniques for finding grammar errors without actually specifying the error. The ‘broadness’ of the grammar and the lexical ambiguity in words, necessary for parsing text containing errors, also yields ambiguous and/or alternative phrase annotations. We block some of the (erroneous) alternative parses by the order in which phrase segments are selected, which causes bleeding of some rules and more ‘correct’ parsing results are achieved. The technique shows good coverage results for agreement and verb selection phenomena. 
Despite the popularity of stochastic parsers, symbolic parsing still has some advantages, but is not practical without an effective mechanism for selecting among alternative analyses. This paper describes the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks. The hybrid currently equals or exceeds most stochastic parsers in speed and is approaching them in accuracy. The preference system is novel in using a simple, three-valued scoring method (-1, 0, or +1) for assigning preferences to constituents viewed in the context of their containing constituents. The approach addresses problems associated with earlier preference systems, and has considerably facilitated development. It is ultimately based on viewing preference scoring as an engineering mechanism, and only indirectly related to cognitive principles or corpus-based frequencies. 
In this paper, we present a method which, in practice, allows to use parsers for languages deﬁned by very large context-free grammars (over a million symbol occurrences). The idea is to split the parsing process in two passes. A ﬁrst pass computes a sub-grammar which is a specialized part of the large grammar selected by the input text and various ﬁltering strategies. The second pass is a traditional parser which works with the subgrammar and the input text. This approach is validated by practical experiments performed on a Earley-like parser running on a test set with two large context-free grammars. 
This paper identiﬁes two orthogonal dimensions of context sensitivity, the ﬁrst being context sensitivity in concurrency and the second being structural context sensitivity. We present an example from natural language which seems to require both types of context sensitivity, and introduce partially ordered multisets (pomsets) mcfgs as a formalism which succintly expresses both. Introduction Researchers in computer science and formal language theory have separately investigated context sensitivity of languages, addressing disjoint dimensions of context sensitivity. Researchers in parallel computing have explored the addition of concurrency and free word order to context free languages, i.e. a concurrency context sensitivity (Gischer, 1981; Warmuth and Haussler, 1984; Pratt, 1985; Pratt, 1986; Lodaya and Weil, 2000). Computational linguistis have explored adding crossing dependency and discontinuous constituency, i.e. a structural context sensitivity (Seki et al., 1991; Vijay-Shanker et al., 1987; Stabler, 1996). Research considering the combination of two dimensions of expressing context sensitivity have been sparse, e.g. (Becker et al., 1991), with research dedicated to this topic virtually nonexistent. Natural languages are not well expressed by either form of context sensitivity alone. For example, in Table 1, sentences 1-8 are valid, but 9, 10 are invalid constructions of Norwegian. In addition to the crossing dependency between the determiner and adverb phrase, this example can be described by either  Derfor ga Jens Kari kyllingen tydeligvis ikke lenger kald Therefore gave Jens Kari the chicken evidently not longer cold Derfor ga Jens Kari tydeligvis kyllingen ikke lenger kald Derfor ga Jens tydeligvis Kari kyllingen ikke lenger kald Derfor ga Jens tydeligvis Kari ikke kyllingen lenger kald Derfor ga Jens tydeligvis Kari ikke lenger kyllingen kald Derfor ga Jens tydeligvis ikke lenger Kari kyllingen kald Derfor ga tydeligvis Jens ikke lenger Kari kyllingen kald Derfor ga tydeligvis ikke Jens lenger Kari kyllingen kald * Derfor ga Jens ikke tydeligvis Kari lenger kyllingen kald * Derfor ga Jens ikke tydeligvis kyllingen lenger Kari kald Table 1: Bobaljik’s paradox/shape conservation example Bobaljik’s paradox (Bobaljik, 1999), which asserts that relative ordering of clausal constituents are not unambiguously determined by the phrase structure, or shape conservation (Mu¨ller, 2000), i.e. that linear precedence is preserved despite movement operations. In other words, the two structurally context sensitive components (due to the crossing dependency between them) can be shufﬂed arbitrarily, leading to concurrent context sensitivity. This paper proposes pomset mcfgs as a formalism for perspicuously expressing both types of context sensitivity. 1 The rest of the paper is organized as follows. Section 1 introduces pomsets, pomset operations, and pomset properties. Section 2 provides a deﬁnition of pomset mcfgs by extending the standard deﬁnition of mcfgs, deﬁned over tuples of strings, to tuples of pomsets. Section 3 discusses pomset mcfg parsing. 1Other pomset based formalisms (Lecomte and Retore, 1995; Basten, 1997; Nederhof et al., 2003) have been limited to the use of pomsets in context free grammars only.  106 Proceedings of the 10th Conference on Parsing Technologies, pages 106–108, Prague, Czech Republic, June 2007. c 2007 Association for Computational Linguistics  
In functional and logic programming, parsers can be built as modular executable speciﬁcations of grammars, using parser combinators and deﬁnite clause grammars respectively. These techniques are based on top-down backtracking search. Commonly used implementations are inefﬁcient for ambiguous languages, cannot accommodate left-recursive grammars, and require exponential space to represent parse trees for highly ambiguous input. Memoization is known to improve efﬁciency, and work by other researchers has had some success in accommodating left recursion. This paper combines aspects of previous approaches and presents a method by which parsers can be built as modular and efﬁcient executable speciﬁcations of ambiguous grammars containing unconstrained left recursion. 
In this paper we investigate several nonprojective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model. We also investigate algorithms for non-projective parsing that account for nonlocal information, and present several hardness results. This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model. 
This paper investigates new design options for the feature space of a dependency parser. We focus on one of the simplest and most efﬁcient architectures, based on a deterministic shift-reduce algorithm, trained with the perceptron. By adopting second-order feature maps, the primal form of the perceptron produces models with comparable accuracy to more complex architectures, with no need for approximations. Further gains in accuracy are obtained by designing features for parsing extracted from semantic annotations generated by a tagger. We provide experimental evaluations on the Penn Treebank. 
We propose a generative dependency parsing model which uses binary latent variables to induce conditioning features. To deﬁne this model we use a recently proposed class of Bayesian Networks for structured prediction, Incremental Sigmoid Belief Networks. We demonstrate that the proposed model achieves state-of-the-art results on three different languages. We also demonstrate that the features induced by the ISBN’s latent variables are crucial to this success, and show that the proposed model is particularly good on long dependencies. 
Current parameters of accurate unlexicalized parsers based on Probabilistic ContextFree Grammars (PCFGs) form a twodimensional grid in which rewrite events are conditioned on both horizontal (headoutward) and vertical (parental) histories. In Semitic languages, where arguments may move around rather freely and phrasestructures are often shallow, there are additional morphological factors that govern the generation process. Here we propose that agreement features percolated up the parse-tree form a third dimension of parametrization that is orthogonal to the previous two. This dimension differs from mere “state-splits” as it applies to a whole set of categories rather than to individual ones and encodes linguistically motivated co-occurrences between them. This paper presents extensive experiments with extensions of unlexicalized PCFGs for parsing Modern Hebrew in which tuning the parameters in three dimensions gradually leads to improved performance. Our best result introduces a new, stronger, lower bound on the performance of treebank grammars for parsing Modern Hebrew, and is on a par with current results for parsing Modern Standard Arabic obtained by a fully lexicalized parser trained on a much larger treebank.  
The non-verbal behaviour of an embodied conversational agent is normally based on recorded human behaviour. There are two main ways that the mapping from human behaviour to agent behaviour has been implemented. In some systems, human behaviour is analysed, and then rules for the agent are created based on the results of that analysis; in others, the recorded behaviour is used directly as a resource for decision-making, using data-driven techniques. In this paper, we implement both of these methods for selecting the conversational facial displays of an animated talking head and compare them in two user evaluations. In the ﬁrst study, participants were asked for subjective preferences: they tended to prefer the output of the data-driven strategy, but this trend was not statistically signiﬁcant. In the second study, the data-driven facial displays affected the ability of users to perceive user-model tailoring in synthesised speech, while the rulebased displays did not have any effect. 
Japanese backchannel utterances, aizuti, in a multi-party design conversation were examined, and aizuti functions were analyzed in comparison with its functions in twoparty dialogues. In addition to the two major functions, signaling acknowledgment and turn-management, it was argued that aizuti in multi-party conversations are involved in joint construction of design plans through management of the ﬂoor structure, and display of participants’ readiness to engage in collaborative elaboration of jointly constructed proposals. 
In multi-party conversations it may not always be obvious who is talking to whom. Backchannels may provide a partial answer to this question, possibly in combination with some other events, such as gaze behaviors of the interlocutors. We look at some patterns in multi-party interaction relating features of backchannel behaviours to aspects of the partipation framework. 
In this paper we describe an experiment aimed at determining the most effective and natural orientation of a virtual guide that gives route directions in a 3D virtual environment. We hypothesized that, due to the presence of mirrored gestures, having the route provider directly face the route seeker would result in a less effective and less natural route description than having the route provider adapt his orientation to that of the route seeker. To compare the effectiveness of the different orientations, after having received a route description the participants in our experiment had to ‘virtually’ traverse the route using prerecorded route segments. The results showed no difference in effectiveness between the two orientations, but suggested that the orientation where the speaker directly faces the route seeker is more natural. 
In this paper we explore the possibilities that conversational agent technology offers for the improvement of the quality of human-machine interaction in a concrete area of application: the multimodal biometric authentication system. Our approach looks at the user perception effects related to the system interface rather than to the performance of the biometric technology itself. For this purpose we have created a multibiometric user test environment with two different interfaces or interaction metaphors: one with an embodied conversational agent and the other with on-screen text messages only. We present the results of an exploratory experiment that reveals interesting effects, related to the presence of a conversational agent, on the user’s perception of parameters such as privacy, ease of use, invasiveness or system security. 
We investigate the role of increasing friendship in dialogue, and propose a first step towards a computational model of the role of long-term relationships in language use between humans and embodied conversational agents. Data came from a study of friends and strangers, who either could or could not see one another, and who were asked to give directions to one-another, three subsequent times. Analysis focused on differences in the use of dialogue acts and non-verbal behaviors, as well as cooccurrences of dialogue acts, eye gaze and head nods, and found a pattern of verbal and nonverbal behavior that differentiates the dialogue of friends from that of strangers, and differentiates early acquaintances from those who have worked together before. Based on these results, we present a model of deepening rapport which would enable an ECA to begin to model patterns of human relationships. 
We describe the implementation and evaluation of a prototype American Sign Language (ASL) generation component that produces animations of ASL classifier predicates, some frequent and complex spatial phenomena in ASL that no previous generation system has produced. We discuss some challenges in evaluating ASL systems and present the results of a userbased evaluation study of our system. 
For embodied agents to engage in realistic multiparty conversation, they must stand in appropriate places with respect to other agents and the environment. When these factors change, for example when an agent joins a conversation, the agents must dynamically move to a new location and/or orientation to accommodate. This paper presents an algorithm for simulating the movement of agents based on observed human behavior using techniques developed for pedestrian movement in crowd simulations. We extend a previous group conversation simulation to include an agent motion algorithm. We examine several test cases and show how the simulation generates results that mirror real-life conversation settings. 
In this paper we present validation tests that we have carried out on gestures that we have designed for an embodied conversational agent (ECAs), to assess their soundness with a view to applying said gestures in a forthcoming experiment to explore the possibilities ECAs can offer to overcome typical robustness problems in spoken language dialogue systems (SLDSs). The paper is divided into two parts: First we carry our a literature review to acquire a sense of the extent to which ECAs can help overcome user frustration during human-machine interaction. Then we associate tentative, yet specific, ECA gestural behaviour with each of the main dialogue stages, with special emphasis on problem situations. In the second part we describe the tests we have carried out to validate our ECA’s gestural repertoire. The results obtained show that users generally understand and naturally accept the gestures, to a reasonable degree. This encourages us to proceed with the next stage of research: evaluating the gestural strategy in real dialogue situations with the aim of learning about how to favour a more efficient and pleasant dialogue flow for the users.  
This paper describes how grammar-based language models for speech recognition systems can be generated from Grammatical Framework (GF) grammars. Context-free grammars and ﬁnite-state models can be generated in several formats: GSL, SRGS, JSGF, and HTK SLF. In addition, semantic interpretation code can be embedded in the generated context-free grammars. This enables rapid development of portable, multilingual and easily modiﬁable speech recognition applications. 
We present an algorithm for converting Grammatical Framework grammars (Ranta, 2004) into the Regulus uniﬁcation-based framework (Rayner et al., 2006). The main purpose is to take advantage of the Regulusto-Nuance compiler for generating optimized speech recognition grammars. But there is also a theoretical interest in knowing how similar the two grammar formalisms are. Since Grammatical Framework is more expressive than Regulus, the resulting Regulus grammars can be overgenerating. We therefore describe a subclass of Grammatical Framework for which the algorithm results in an equivalent Regulus grammar. 
We present two experiments in the localization of spoken dialogue systems. The domain of the dialogue system is an MP3 application for automobiles. In the ﬁrst experiment, a grammar in Nuance GSL format was rewritten in Grammatical Framework (GF). Within GF, the grammar was extended from two to six languages, giving a baseline for semantically complete grammars. In the second experiment, the German version of this baseline GF grammar was extended with the goal to restore the coverage of the original Nuance grammar. 
This paper shows how we can combine the art of grammar writing with the power of statistics by bootstrapping statistical language models (SLMs) for Dialogue Systems from grammars written using the Grammatical Framework (GF) (Ranta, 2004). Furthermore, to take into account that the probability of a user’s dialogue moves is not static during a dialogue we show how the same methodology can be used to generate dialogue move speciﬁc SLMs where certain dialogue moves are more probable than others. These models can be used at different points of a dialogue depending on contextual constraints. By using grammar generated SLMs we can improve both recognition and understanding performance considerably over using the original grammar. With dialogue move speciﬁc SLMs we would be able to get a further improvement if we had an optimal way of predicting the correct language model. 
In command and control (C&C) speech interaction, users interact by speaking commands or asking questions typically specified in a context-free grammar (CFG). Unfortunately, users often produce out-ofgrammar (OOG) commands, which can result in misunderstanding or nonunderstanding. We explore a simple approach to handling OOG commands that involves generating a backoff grammar from any CFG using filler models, and utilizing that grammar for recognition whenever the CFG fails. Working within the memory footprint requirements of a mobile C&C product, applying the approach yielded a 35% relative reduction in semantic error rate for OOG commands. It also improved partial recognitions for enabling clarification dialogue. 
Information Extraction (IE) often involves some amount of partial syntactic processing. This is clear in cases of interesting highlevel IE tasks, such as ﬁnding information about who did what to whom (when, where, how and why), but it is also true in case of simpler IE tasks, such as ﬁnding company names in texts. The aim of this paper is to give an overview of Slavonic phenomena which pose particular problems for IE and partial parsing, and some phenomena which seem easier to treat in Slavonic than in Germanic or Romance; I also mention various tools which have been used for the partial processing of Slavonic. 
In this paper a system for Named Entity Recognition and Classification in Croatian language is described. The system is composed of the module for sentence segmentation, inflectional lexicon of common words, inflectional lexicon of names and regular local grammars for automatic recognition of numerical and temporal expressions. After the first step (sentence segmentation), the system attaches to each token its full morphosyntactic description and appropriate lemma and additional tags for potential categories for names without disambiguation. The third step (the core of the system) is the application of a set of rules for recognition and classification of named entities in already annotated texts. Rules based on described strategies (like internal and external evidence) are applied in cascade of transducers in defined order. Although there are other classification systems for NEs, the results of our system are annotated NEs which are following MUC-7 specification. System is applied on informative and noninformative texts and results are compared. F-measure of the system applied on informative texts yields over 90%. 
We present a language independent approach for ﬁne-grained categorization and discrimination of names on the basis of text semantic similarity information. The experiments are conducted for languages from the Romance (Spanish) and Slavonic (Bulgarian) language groups. Despite the fact that these languages have speciﬁc characteristics as word-order and grammar, the obtained results are encouraging and show that our name entity method is scalable not only to different categories, but also to different languages. In an exhaustive experimental evaluation, we have demonstrated that our approach yields better results compared to a baseline system. 
The paper presents two techniques for lemmatization of Polish person names. First, we apply a rule-based approach which relies on linguistic information and heuristics. Then, we investigate an alternative knowledge-poor method which employs string distance measures. We provide an evaluation of the adopted techniques using a set of newspaper texts. 
The paper presents a rule-based information extraction (IE) system for Polish medical texts. We select the most important information from diabetic patients’ records. Most data being processed are free-form texts, only a part is in table form. The work has three goals: to test classical IE methods on texts in Polish, to create relational database containing the extracted data, and to prepare annotated data for further IE experiments. 
This paper presents the results of the preliminary experiments in the automatic extraction of deﬁnitions (for semi-automatic glossary construction) from usually unstructured or only weakly structured e-learning texts in Bulgarian, Czech and Polish. The extraction is performed by regular grammars over XML-encoded morphosyntacticallyannotated documents. The results are less than satisfying and we claim that the reason for that is the intrinsic difﬁculty of the task, as measured by the low interannotator agreement, which calls for more sophisticated deeper linguistic processing, as well as for the use of machine learning classiﬁcation techniques. 
This paper describes a study on performance of existing unsupervised algorithms of text documents topical segmentation when applied to Polish plain text documents. For performance measurement ﬁve existing topical segmentation algorithms were selected, three different Polish test collections were created and seven approaches to text preprocessing were implemented. Based on quantitative results (Pk and WindowDiff metrics) use of speciﬁc algorithm was recommended and impact of pre-processing strategies was assessed. Thanks to use of standardized metrics and application of previously described methodology for test collection development, comparative results for Polish and English were also obtained. 
The goal of this paper is to compile a method for multi-word term extraction, taking into account both the linguistic properties of Bulgarian terms and their statistical rates. The method relies on the extraction of term candidates matching given syntactic patterns followed by statistical (by means of Log-likelihood ratio) and linguistically (by means of inflectional clustering) based filtering aimed at improving the coverage and the precision of multi-word term extraction. 
Several hybrid disambiguation methods are described which combine the strength of hand-written disambiguation rules and statistical taggers. Three different statistical (HMM, Maximum-Entropy and Averaged Perceptron) taggers are used in a tagging experiment using Prague Dependency Treebank. The results of the hybrid systems are better than any other method tried for Czech tagging so far. 
In the paper we describe enriching Czech WordNet with the derivational relations that in highly inflectional languages like Czech form typical derivational nests (or subnets). Derivational relations are mostly of semantic nature and their regularity in Czech allows us to add them to the WordNet almost automatically. For this purpose we have used the derivational version of morphological analyzer Ajka that is able to handle the basic and most productive derivational relations in Czech. Using a special derivational interface developed in our NLP Lab we have explored the semantic nature of the selected noun derivational suffixes and established a set of the semantically labeled derivational relations – presently 14. We have added them to the Czech WordNet and in this way enriched it with approx. 30 000 new Czech synsets. A similar enrichment for Princeton WordNet has been reported in its recently released version 3.0, we will comment on the partial similarities and differences. 
We describe a study that evaluates an approach to Word Sense Discrimination on three languages with different linguistic structures, English, Hebrew, and Russian. The goal of the study is to determine whether there are signiﬁcant performance differences for the languages and to identify language-speciﬁc problems. The algorithm is tested on semantically ambiguous words using data from Wikipedia, an online encyclopedia. We evaluate the induced clusters against sense clusters created manually. The results suggest a correlation between the algorithm’s performance and morphological complexity of the language. In particular, we obtain FScores of 0.68 , 0.66 and 0.61 for English, Hebrew, and Russian, respectively. Moreover, we perform an experiment on Russian, in which the context terms are lemmatized. The lemma-based approach significantly improves the results over the wordbased approach, by increasing the FScore by 16%. This result demonstrates the importance of morphological analysis for the task for morphologically rich languages like Russian. 
Named entity recognition (NER) is a subtask of information extraction (IE) which can be used further on for different purposes. In this paper, we discuss named entity recognition for Ukrainian language, which is a Slavonic language with a rich morphology. The approach we follow uses a restricted number of features. We show that it is feasible to boost performance by considering several heuristics and patterns acquired from the Web data. 
As the development of information technologies makes progress, large morphologically annotated corpora become a necessity, as they are necessary for moving onto higher levels of language computerisation (e. g. automatic syntactic and semantic analysis, information extraction, machine translation). Research of morphological disambiguation and morphological annotation of the 100 million word Lithuanian corpus are presented in the article. Statistical methods have enabled to develop the automatic tool of morphological annotation for Lithuanian, with the disambiguation precision of 94%. Statistical data about the distribution of parts of speech, most frequent wordforms, and lemmas, in the annotated Corpus of The Contemporary Lithuanian Language is also presented. 
 This paper presents a corpus-based method for automatic evaluation of geometric constraints on projective prepositions. The method is used to ﬁnd an appropriate model of geometric constraints for a twodimensional domain. Two simple models are evaluated against the uses of projective prepositions in a corpus of natural language dialogues to ﬁnd the best parameters of these models. Both models cover more than 96% of the data correctly. An extra treatment of negative uses of projective prepositions (e.g. A is not above B) improves both models getting close to full coverage. 
In order for automated navigation systems to operate effectively, the route instructions they produce must be clear, concise and easily understood by users. In order to incorporate a landmark within a coherent sentence, it is necessary to ﬁrst understand how that landmark is conceptualised by travellers — whether it is perceived as point-like, linelike or area-like. This paper investigates the viability of automatically classifying the conceptualisation of landmarks relative to a given city context. We use web data to learn the default conceptualisation of those landmarks, crucially analysing preposition and verb collocations in the classiﬁcation. 
PrepLex is a lexicon of French prepositions which provides all the syntactic information needed for parsing. It was built by comparing and merging several authoritative lexical sources. This lexicon also includes information about the prepositions or classes of prepositions that appear in French verb subcategorization frames. This resource has been developed as a ﬁrst step in making current French preposition lexicons available for effective natural language processing. 
This paper presents ongoing work on the detection of preposition errors of non-native speakers of English. Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students. To address this problem, we use a maximum entropy classiﬁer combined with rule-based ﬁlters to detect preposition errors in a corpus of student essays. Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3. 
We determine the productivity of determinerless PPs in German quantitatively, restricting ourselves to the preposition unter. The study is based on two German newspaper corpora, comprising some 210 million words. The problematic construction, i.e. unter followed by a determinerless singular noun occurs some 16.000 times in the corpus. To clarify the empirical productivity of the construction, we apply a productivity measure developed by Baayen (2001) to the syntactic domain by making use of statistical models suggested in Evert (2004). We compare two different models and suggest a gradient descent search for parameter estimation. Our results show that the combination of unter+noun must in fact be characterized as productive, and hence that a syntactic treatment is required. Kiss (2006),Kiss (2007),Li (1992), Zipf (1949) 
In this work we report on the results of a preliminary corpus study of Italian on the semantics of temporal prepositions, which is part of a wider project on the automatic recognition of temporal relations. The corpus data collected supports our hypothesis that each temporal preposition can be associated with one prototypical temporal relation, and that deviations from the prototype can be explained as determined by the occurrence of different semantic patterns. The motivation behind this approach is to improve methods for temporal annotation of texts for content based access to information. The corpus study described in this paper led to the development of a preliminary set of heuristics for automatic annotation of temporal relations in text/discourse. 
This paper proposes a machine-learning based approach to predict accurately, given a syntactic and semantic context, which preposition is most likely to occur in that context. Each occurrence of a preposition in an English corpus has its context represented by a vector containing 307 features. The vectors are processed by a voted perceptron algorithm to learn associations between contexts and prepositions. In preliminary tests, we can associate contexts and prepositions with a success rate of up to 84.5%. 
The paper describes an approach to automatically select from Indian Language the appropriate lexical correspondence of English simple preposition. The paper describes this task from a Machine Translation (MT) perspective. We use the properties of the head and complement of the preposition to select the appropriate sense in the target language. We later show that the results obtained from this approach are promising. 
In this paper we describe the Graph Annotation Format (GrAF) and show how it is used represent not only independent linguistic annotations, but also sets of merged annotations as a single graph. To demonstrate this, we have automatically transduced several different annotations of the Wall Street Journal corpus into GrAF and show how the annotations can then be merged, analyzed, and visualized using standard graph algorithms and tools. We also discuss how, as a standard graph representation, it allows for the application of well-established graph traversal and analysis algorithms to produce information about interactions and commonalities among merged annotations. GrAF is an extension of the Linguistic Annotation Framework (LAF) (Ide and Romary, 2004, 2006) developed within ISO TC37 SC4 and as such, implements state-of-the-art best practice guidelines for representing linguistic annotations. 
With ever-increasing demands on the diversity of annotations of language data, the need arises to reduce the amount of efforts involved in generating such value-added language resources. We introduce here the Jena ANnotation Environment (JANE), a platform that supports the complete annotation lifecycle and allows for ‘focused’ annotation based on active learning. The focus we provide yields signiﬁcant savings in annotation efforts by presenting only informative items to the annotator. We report on our experience with this approach through simulated and real-world annotations in the domain of immunogenetics for NE annotations. 
This paper presents a uniform approach to data extraction from syntactically annotated corpora encoded in XML. XQuery, which incorporates XPath, has been designed as a query language for XML. The combination of XPath and XQuery offers ﬂexibility and expressive power, while corpus speciﬁc functions can be added to reduce the complexity of individual extraction tasks. We illustrate our approach using examples from dependency treebanks for Dutch.  this task are often developed with only a single application in mind (mostly corpus linguistics) or are developed in an ad-hoc fashion, as part of a speciﬁc application. We propose a more principled approach, based on two observations: • XML is widely used to encode syntactic annotation. Syntactic annotation is not more complex that some other types of information that is routinely stored in XML. This suggests that XML technology can be used to process syntactically annotated corpora.  
We present an annotated corpus of conversational facial displays designed to be used for generation. The corpus is based on a recording of a single speaker reading scripted output in the domain of the target generation system. The data in the corpus consists of the syntactic derivation tree of each sentence annotated with the full syntactic and pragmatic context, as well as the eye and eyebrow displays and rigid head motion used by the the speaker. The behaviours of the speaker show several contextual patterns, many of which agree with previous ﬁndings on conversational facial displays. The corpus data has been used in several studies exploring different strategies for selecting facial displays for a synthetic talking head. 
We introduce an annotation type system for a data-driven NLP core system. The speciﬁcations cover formal document structure and document meta information, as well as the linguistic levels of morphology, syntax and semantics. The type system is embedded in the framework of the Unstructured Information Management Architecture (UIMA). 
This paper introduces a new, reversible method for converting syntactic structures with discontinuous constituents into traditional syntax trees. The method is applied to the Tiger Corpus of German and results for PCFG parsing requiring such contextfree trees are provided. A labeled dependency evaluation shows that the new conversion method leads to better results by preserving local relationships and introducing fewer inconsistencies into the training data. 
This paper describes an annotation system for Sa´mi language corpora, which consists of structured, running texts. The annotation of the texts is fully automatic, starting from the original documents in different formats. The texts are ﬁrst extracted from the original documents preserving the original structural markup. The markup is enhanced by a document-speciﬁc XSLT script which contains document-speciﬁc formatting instructions. The overall maintenance is achieved by system-wide XSLT scripts. 
In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance. Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a caseby-case basis rather than applying a strict methodology. Determining appropriate criteria for clustering is based primarily on the needs of annotators. 
We investigate a way to partially automate corpus annotation for named entity recognition, by requiring only binary decisions from an annotator. Our approach is based on a linear sequence model trained using a k-best MIRA learning algorithm. We ask an annotator to decide whether each mention produced by a high recall tagger is a true mention or a false positive. We conclude that our approach can reduce the effort of extending a seed training corpus by up to 58%. 
This paper presents a multimodal corpus of comparable pack messages and the concordancer that has been built to query it. The design of the corpus and its annotation is introduced. This is followed by a description of the concordancer’s interface, implementation and concordance display. Finally, some ideas for future work are outlined. 
The linguistic quality of a parallel treebank depends crucially on the parallelism between the source and target language annotations. We propose a linguistic notion of translation units and a quantitative measure of parallelism for parallel dependency treebanks, and demonstrate how the proposed translation units and parallelism measure can be used to compute transfer rules, spot annotation errors, and compare different annotation schemes with respect to each other. The proposal is evaluated on the 100,000 word Copenhagen Danish-English Dependency Treebank. 
We present an approach to automatic semantic role labeling (SRL) carried out in the context of the Dutch Language Corpus Initiative (D-Coi) project. Adapting earlier research which has mainly focused on English to the Dutch situation poses an interesting challenge especially because there is no semantically annotated Dutch corpus available that can be used as training data. Our automatic SRL approach consists of three steps: bootstrapping from a syntactically annotated corpus by means of a rulebased tagger developed for this purpose, manual correction on the basis of the PropBank guidelines which have been adapted to Dutch and training a machine learning system on the manually corrected data. 
This paper describes a tool for aligning and searching parallel treebanks. Such treebanks are a new type of parallel corpora that come with syntactic annotation on both languages plus sub-sentential alignment. Our tool allows the visualization of tree pairs and the comfortable annotation of word and phrase alignments. It also allows monolingual and bilingual searches including the speciﬁcation of alignment constraints. We show that the TIGER-Search query language can easily be combined with such alignment constraints to obtain a powerful cross-lingual query language. 
The Appraisal framework is a theory of the language of evaluation, developed within the tradition of systemic functional linguistics. The framework describes a taxonomy of the types of language used to convey evaluation and position oneself with respect to the evaluations of other people. Accurate automatic recognition of these types of language can inform an analysis of document sentiment. This paper describes the preparation of test data for algorithms for automatic Appraisal analysis. The difﬁculty of the task is assessed by way of an inter-annotator agreement study, based on measures analogous to those used in the MUC-7 evaluation. 
In the construction of a part-of-speech annotated corpus, we are constrained by a fixed budget. A fully annotated corpus is required, but we can afford to label only a subset. We train a Maximum Entropy Markov Model tagger from a labeled subset and automatically tag the remainder. This paper addresses the question of where to focus our manual tagging efforts in order to deliver an annotation of highest quality. In this context, we find that active learning is always helpful. We focus on Query by Uncertainty (QBU) and Query by Committee (QBC) and report on experiments with several baselines and new variations of QBC and QBU, inspired by weaknesses particular to their use in this application. Experiments on English prose and poetry test these approaches and evaluate their robustness. The results allow us to make recommendations for both types of text and raise questions that will lead to further inquiry. 
We present MAIS, a UIMA-based environment for combining information from various annotated resources. Each resource contains one mode of linguistic annotation and remains independent from the other resources. Interactions between annotations are deﬁned based on use cases. 
XARA is a rule-based PropBank labeler for Alpino XML ﬁles, written in Java. I used XARA in my research on semantic role labeling in a Dutch corpus to bootstrap a dependency treebank with semantic roles. Rules in XARA are based on XPath expressions, which makes it a versatile tool that is applicable to other treebanks as well. In addition to automatic role annotation, XARA is able to extract training instances (sets of features) from an XML based treebank. Such an instance base can be used to train machine learning algorithms for automatic semantic role labeling (SRL). In my semantic role labeling research, I used the Tilburg Memory Learner (TiMBL) for this purpose. 
In this paper, we present a treebank annotation tool developed for processing Turkish sentences. The tool consists of three different annotation stages; morphological analysis, morphological disambiguation and syntax analysis. Each of these stages are integrated with existing analyzers in order to guide human annotators. Our semiautomatic treebank annotation tool is currently used both for creating new data sets and correcting the existing Turkish treebank. 
We present two web-based, interactive tools for creating and visualizing sub-sentential alignments of parallel text. Yawat is a tool to support distributed, manual word- and phrase-alignment of parallel text through an intuitive, web-based interface. Kwipc is an interface for displaying words or bilingual word pairs in parallel, word-aligned context. A key element of the tools presented here is the interactive visualization: alignment information is shown only for one pair of aligned words or phrases at a time. This allows users to explore the alignment space interactively without being overwhelmed by the amount of information available. 
This paper presents the building procedure of a Chinese sense annotated corpus. A set of software tools is designed to help human annotator to accelerate the annotation speed and keep the consistency. The software tools include 1) a tagger for word segmentation and POS tagging, 2) an annotating interface responsible for the sense describing in the lexicon and sense annotating in the corpus, 3) a checker for consistency keeping, 4) a transformer responsible for the transforming from text file to XML format, and 5) a counter for sense frequency distribution calculating. 
In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1, which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 
Annotating large text corpora is a timeconsuming effort. Although single-user annotation tools are available, web-based annotation applications allow for distributed annotation and ﬁle access from different locations. In this paper we present the webbased annotation application Serengeti for annotating anaphoric relations which will be extended for the annotation of lexical chains. 
The LUNA corpus is a multi-lingual, multidomain spoken dialogue corpus currently under development that will be used to develop a robust natural spoken language understanding toolkit for multilingual dialogue services. The LUNA corpus will be annotated at multiple levels to include annotations of syntactic, semantic, and discourse information; specialized annotation tools will be used for the annotation at each of these levels. In order to synchronize these multiple layers of annotation, the PAULA standoff exchange format will be used. In this paper, we present the corpus and its PAULA-based architecture.1 
This document outlines minimal design principles underlying annotation of coreference relations in PoCoS, a scheme for cross-linguistic anaphoric annotation. We identify language-independent principles for markable identification which are essential for comparability of annotations produced for different languages. We further suggest a clear and motivated structure of annotation stages, the separation of a coarse-grained core and a family of more elaborate extended schemes, and strategies for the systematic treatment of ambiguity. Explicit mark-up of ambiguities is a novel feature. We implemented three instantiations of PoCoS for German, English and Russian applied to corpora of newspaper texts. 
Whilst the degree to which a treebank subscribes to a speciﬁc linguistic theory limits the usefulness of the resource, the availability of more formats for the same resource plays a crucial role both in NLP and linguistics. Conversion tools and multi-format treebanks are useful for investigating portability of NLP systems and validity of annotation. Unfortunately, conversion is a quite complex task since it involves grammatical rules and linguistic knowledge to be incorporated into the converter program. The paper focusses on a methodology for treebank conversion which consists in splitting the process in steps corresponding to the kinds of information that have to be converted, i.e. morphological, structural or relational syntactic. The advantage is the generation of a set of parallel treebanks featuring progressively differentiated formats. An application to the case of an Italian dependency-based treebank in a Penn like format is described. 
This paper presents observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system. The system relies on cross-linguistic evidence from a set of ﬁve Romance languages: Spanish, Italian, French, Portuguese, and Romanian. Given a training set of English noun phrases in context along with their translations in the ﬁve Romance languages, our algorithm automatically learns a classiﬁcation function that is later on applied to unseen test instances for semantic interpretation. As training and test data we used two text collections of different genre: Europarl and CLUVI. The training data was annotated with contextual features based on two stateof-the-art classiﬁcation tag sets. 
We propose a new XML format for representing interlinearized glossed text (IGT), particularly in the context of the documentation and description of endangered languages. The proposed representation, which we call IGT-XML, builds on previous models but provides a more loosely coupled and ﬂexible representation of different annotation layers. Designed to accommodate both selective manual reannotation of individual layers and semi-automatic extension of annotation, IGT-XML is a ﬁrst step toward partial automation of the production of IGT. 
We seek to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: (1) the comparison of annotation schemes; (2) the merging of information represented by various annotation schemes; (3) the emergence of NLP systems that use information in multiple annotation schemes; and (4) the adoption of various types of best practice in corpus annotation. Such best practices would include: (a) clearer demarcation of phenomena being annotated; (b) the use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the “Controversial” portions of the WikipediaXML corpus (Denoyer and  Gallinari, 2006). 
Dept. of Linguistics Computer Laboratory  Univ. of Texas at Austin Univ. of Cambridge  bjreese@mail.utexas.edu sht25@cl.cam.uk  Bonnie Webber School of Informatics Univ. of Edinburgh bonnie@inf.ed.ac.uk  Theresa Wilson Dept. of Comp. Science Univ. of Pittsburgh twilson@cs.pitt.edu  
This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year’s dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges. 1.1 The RTE challenges The goal of the RTE challenges has been to create a benchmark task dedicated to textual entailment – recognizing that the meaning of one  text is entailed, i.e. can be inferred, by another1. In the recent years, this task has raised great interest since applied semantic inference concerns many practical Natural Language Processing (NLP) applications, such as Question Answering (QA), Information Extraction (IE), Summarization, Machine Translation and Paraphrasing, and certain types of queries in Information Retrieval (IR). More specifically, the RTE challenges have aimed to focus research and evaluation on this common underlying semantic inference task and separate it from other problems that different NLP applications need to handle. For example, in addition to textual entailment, QA systems need to handle issues such as answer retrieval and question type recognition. By separating out the general problem of textual entailment from these task-specific problems, progress on semantic inference for many application areas can be promoted. Hopefully, research on textual entailment will finally lead to the development of entailment “engines”, which can be used as a standard module in many applications (similar to the role of part-of-speech taggers and syntactic parsers in current NLP applications). In the following sections, a detailed description of RTE-3 is presented. After a quick review  
This paper discusses our contribution to the third RTE Challenge – the SALSA RTE system. It builds on an earlier system based on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap. We evaluate their (combined) performance on various data sets. However, earlier observations that the combination of features improves the overall accuracy could be replicated only partly. 
This paper describes our system as used in the RTE3 task. The system maps premise and hypothesis pairs into an abstract knowledge representation (AKR) and then performs entailment and contradiction detection (ECD) on the resulting AKRs. Two versions of ECD were used in RTE3, one with strict ECD and one with looser ECD. 
This paper reports on LCC’s participation at the Third PASCAL Recognizing Textual Entailment Challenge. First, we summarize our semantic logical-based approach which proved successful in the previous two challenges. Then we highlight this year’s innovations which contributed to an overall accuracy of 72.25% for the RTE 3 test data. The novelties include new resources, such as eXtended WordNet KB which provides a large number of world knowledge axioms, event and temporal information provided by the TARSQI toolkit, logic form representations of events, negation, coreference and context, and new improvements of lexical chain axiom generation. Finally, the system’s performance and error analysis are discussed. 
This paper describes on-going efforts to annotate a corpus of almost 16000 answer pairs with an estimated 69000 fine-grained entailment relationships. We illustrate the need for more detailed classification than currently exists and describe our corpus and annotation scheme. We discuss early statistical analysis showing substantial inter-annotator agreement even at the finegrained level. The corpus described here, which is the only one providing such detailed annotations, will be made available as a public resource later this year (2007). This is expected to enable application development that is currently not practical. 
We present a novel approach to RTE that exploits a structure-oriented sentence representation followed by a similarity function. The structural features are automatically acquired from tree skeletons that are extracted and generalized from dependency trees. Our method makes use of a limited size of training data without any external knowledge bases (e.g. WordNet) or handcrafted inference rules. We have achieved an accuracy of 71.1% on the RTE-3 development set performing a 10-fold cross validation and 66.9% on the RTE-3 test data. 
We present the system that we submitted to the 3rd Pascal Recognizing Textual Entailment Challenge. It uses four Support Vector Machines, one for each subtask of the challenge, with features that correspond to string similarity measures operating at the lexical and shallow syntactic level. 
We present VENSES, a linguistically-based approach for semantic inference which is built around a neat division of labour between two main components: a grammatically-driven subsystem which is responsible for the level of predicatearguments well-formedness and works on the output of a deep parser that produces augmented head-dependency structures. A second subsystem fires allowed logical and lexical inferences on the basis of different types of structural transformations intended to produce a semantically valid meaning correspondence. In the current challenge, we produced a new version of the system, where we do away with grammatical relations and only use semantic roles to generate weighted scores. We also added a number of additional modules to cope with fine-grained inferential triggers which were not present in previous dataset. Different levels of argumenthood have been devised in order to cope with semantic uncertainty generated by nearly-inferrable Text-Hypothesis pairs where the interpretation needs reasoning. RTE3 has introduced texts of paragraph length: in turn this has prompted us to upgrade VENSES by the addition of a discourse level anaphora resolution module, which is paramount to allow entailment in pairs where the relevant portion of text contains pronominal expressions. We present the system, its relevance to the task at hand and an evaluation. 
To score well in RTE3, and even more so to create good justifications for entailments, substantial lexical and world knowledge is needed. With this in mind, we present an analysis of a sample of the RTE3 positive entailment pairs, to identify where and what kinds of world knowledge are needed to fully identify and justify the entailment, and discuss several existing resources and their capacity for supplying that knowledge. We also briefly sketch the path we are following to build an RTE system (Our implementation is very preliminary, scoring 50.9% at the time of RTE). The contribution of this paper is thus a framework for discussing the knowledge requirements posed by RTE and some exploration of how these requirements can be met. 
This paper describes our experiments on Textual Entailment in the context of the Third Pascal Recognising Textual Entailment (RTE-3) Evaluation Challenge. Our system uses a Machine Learning approach with Support Vector Machines and AdaBoost to deal with the RTE challenge. We perform a lexical, syntactic, and semantic analysis of the entailment pairs . From this information we compute a set of semanticbased distances between sentences. The results look promising specially for the QA entailment task. 
The textual entailment recognition system that we discuss in this paper represents a perspective-based approach composed of two modules that analyze text-hypothesis pairs from a strictly lexical and syntactic perspectives, respectively. We attempt to prove that the textual entailment recognition task can be overcome by performing individual analysis that acknowledges us of the maximum amount of information that each single perspective can provide. We compare this approach with the system we presented in the previous edition of PASCAL Recognising Textual Entailment Challenge, obtaining an accuracy rate 17.98% higher. 
In this paper, we brieﬂy describe two enhancements of the cross-pair similarity model for learning textual entailment rules: 1) the typed anchors and 2) a faster computation of the similarity. We will report and comment on the preliminary experiments and on the submission results. 
This document contains the description of the experiments carried out by SINAI group. We have developed an approach based on several lexical and syntactic measures integrated by means of different machine learning models. More precisely, we have evaluated three features based on lexical similarity and 11 features based on syntactic tree comparison. In spite of the relatively straightforward approach we have obtained more than 60% for accuracy. Since this is our ﬁrst participation we think we have reached a good result.  Figure 1: Training processes Figure 2: Classiﬁcation processes  
This paper addresses syntax-based paraphrasing methods for Recognizing Textual Entailment (RTE). In particular, we describe a dependency-based paraphrasing algorithm, using the DIRT data set, and its application in the context of a straightforward RTE system based on aligning dependency trees. We ﬁnd a small positive effect of dependency-based paraphrasing on both the RTE3 development and test sets, but the added value of this type of paraphrasing deserves further analysis. 
This paper describes the experiments developed and the results obtained in the participation of UNED in the Third Recognising Textual Entailment (RTE) Challenge. The experiments are focused on the study of the effect of named entities in the recognition of textual entailment. While Named Entity Recognition (NER) provides remarkable results (accuracy over 70%) for RTE on QA task, IE task requires more sophisticated treatment of named entities such as the identiﬁcation of relations between them. 
The primary focuses of this entry this year was firstly, to develop a framework to allow multiple researchers from our group to easily contribute metrics measuring textual entailment, and secondly, to provide a baseline which we could use in our tools to evaluate and compare new metrics. A development environment tool was created to quickly allow for testing of various metrics and to easily randomize the development and test sets. For each test, this RTE tool calculated two sets of results by applying the metrics to both a univariate Gaussian density and by maximizing a linear discriminant function. The metrics used for the submission were a lexical similarity metric and a lexical similarity metric using synonym and antonym replacement. The two submissions for RTE 2007 scored an accuracy of 61.00% and 62.62%.  may or may not need different techniques to determine entailment, and for the purposes of the RTE tool developed, are considered separate problems. 2 RTE Development Environment Tool Our research group decided to begin focusing on the Recognizing Textual Entailment challenge this year in February and to continue our participation for years to come. It was decided to create a development environment from which our researchers could attempt different techniques of examining a Text-Hypothesis pair and yet all metrics resulting from those techniques could be used in calculating the final results. The RTE tool also randomly generates training and testing sets from the 800 Text-Hypothesis pairs provided for development by the competition to avoid overfitting the data during the training stage.  
Recent research suggests that sentence structure can improve the accuracy of recognizing textual entailments and paraphrasing. Although background knowledge such as gazetteers, WordNet and custom built knowledge bases are also likely to improve performance, our goal in this paper is to characterize the syntactic features alone that aid in accurate entailment prediction. We describe candidate features, the role of machine learning, and two final decision rules. These rules resulted in an accuracy of 60.50 and 65.87% and average precision of 58.97 and 60.96% in RTE3Test and suggest that sentence structure alone can improve entailment accuracy by 9.25 to 14.62% over the baseline majority class. 
We compare two approaches to the problem of Textual Entailment: SLIM, a compositional approach modeling the task based on identifying relations in the entailment pair, and BoLI, a lexical matching algorithm. SLIM’s framework incorporates a range of resources that solve local entailment problems. A search-based inference procedure uniﬁes these resources, permitting them to interact ﬂexibly. BoLI uses WordNet and other lexical similarity resources to detect correspondence between related words in the Hypothesis and the Text. In this paper we describe both systems in some detail and evaluate their performance on the 3rd PASCAL RTE Challenge. While the lexical method outperforms the relation-based approach, we argue that the relation-based model offers better long-term prospects for entailment recognition. 
Latent Semantic Analysis has only recently been applied to textual entailment recognition. However, these efforts have suffered from inadequate bag of words vector representations. Our prototype implementation for the Third Recognising Textual Entailment Challenge (RTE-3) improves the approach by applying it to vector representations that contain semi-structured representations of words. It uses variable size n-grams of word stems to model independently verbs, subjects and objects displayed in textual statements. The system performance shows positive results and provides insights about how to improve them further. 
This paper presents two systems for textual entailment, both employing decision trees as a supervised learning algorithm. The ﬁrst one is based primarily on the concept of lexical overlap, considering a bag of words similarity overlap measure to form a mapping of terms in the hypothesis to the source text. The second system is a lexicosemantic matching between the text and the hypothesis that attempts an alignment between chunks in the hypothesis and chunks in the text, and a representation of the text and hypothesis as two dependency graphs. Their performances are compared and their positive and negative aspects are analyzed. 
Based on the core approach of the tree edit distance algorithm, the system central module is designed to target the scope of TE – semantic variability. The main idea is to transform the hypothesis making use of extensive semantic knowledge from sources like DIRT, WordNet, Wikipedia, acronyms database. Additionally, we built a system to acquire the extra background knowledge needed and applied complex grammar rules for rephrasing in English. 
We present a new framework for textual entailment, which provides a modular integration between knowledge-based exact inference and cost-based approximate matching. Diverse types of knowledge are uniformly represented as entailment rules, which were acquired both manually and automatically. Our proof system operates directly on parse trees, and infers new trees by applying entailment rules, aiming to strictly generate the target hypothesis from the source text. In order to cope with inevitable knowledge gaps, a cost function is used to measure the remaining “distance” from the hypothesis. 
We introduce a system for textual entailment that is based on a probabilistic model of entailment. The model is deﬁned using some calculus of transformations on dependency trees, which is characterized by the fact that derivations in that calculus preserve the truth only with a certain probability. We also describe a possible set of transformations (and with it implicitly a calculus) that was successfully applied to the RTE3 challenge data. However, our system can be improved in many ways and we see it as the starting point for a promising new approach to textual entailment.  Magnini, 2005; Kouylekov and Magnini, 2006; Tatu et al., 2006; Adams, 2006). However, instead of deﬁning some distance based on edits, we will generate derivations in some calculus that is able to transform dependency parse trees. The special property of our calculus is that the truth is only preserved with a certain probability along its derivations. This might sound like a disadvantage. However, in commonsense reasoning there is usual a lot of uncertainty due the fact that it is impossible to formalize all world knowledge. We think that probabilities might help us in such situations where it is impossible to include everything into the model, but in which nonetheless we want to do reasoning. 2 Main idea  
We describe a preliminary version of Mutaphrase, a system that generates paraphrases of semantically labeled input sentences using the semantics and syntax encoded in FrameNet, a freely available lexicosemantic database. The algorithm generates a large number of paraphrases with a wide range of syntactic and semantic distances from the input. For example, given the input “I like eating cheese”, the system outputs the syntactically distant “Eating cheese is liked by me”, the semantically distant “I fear sipping juice”, and thousands of other sentences. The wide range of generated paraphrases makes the algorithm ideal for a range of statistical machine learning problems such as machine translation and language modeling as well as other semanticsdependent tasks such as query and language generation.  document retrieval (Zukerman and Raskutti, 2002), and many others. Most of the reported work on paraphrase generation from arbitrary input sentences uses machine learning techniques trained on sentences that are known or can be inferred to be paraphrases of each other (Bannard and Callison-Burch, 2005; Barzilay and Lee, 2003; Barzilay and McKeown, 2001; Callison-Burch et al., 2006; Dolan et al., 2004; Ibrahim et al., 2003; Lin and Pantel, 2001; Pang et al., 2003; Quirk et al., 2004; Shinyama et al., 2002). Mutaphrase instead generates paraphrases algorithmically using an input sentence and FrameNet, a freely available lexico-semantic resource (information regarding FrameNet, including relevant terminology, is presented in Section 2).  ) FEAR SIPPING JUICE  $IFFERENT  4O SIP ON JUICE DISTURBS ME  ) LIKE TO SNACK ON BREAD  3EMANTICS  
To enhance the technology for computing semantic equivalence, we introduce the notion of phrasal thesaurus which is a natural extension of conventional word-based thesaurus. Among a variety of phrases that conveys the same meaning, i.e., paraphrases, we focus on syntactic variants that are compositionally explainable using a small number of atomic knowledge, and develop a system which dynamically generates such variants. This paper describes the proposed system and three sorts of knowledge developed for dynamic phrasal thesaurus in Japanese: (i) transformation pattern, (ii) generation function, and (iii) lexical function. 
We describe an approach to textual inference that improves alignments at both the typed dependency level and at a deeper semantic level. We present a machine learning approach to alignment scoring, a stochastic search procedure, and a new tool that ﬁnds deeper semantic alignments, allowing rapid development of semantic features over the aligned graphs. Further, we describe a complementary semantic component based on natural logic, which shows an added gain of 3.13% accuracy on the RTE3 test set. 
In this paper, we introduce a new framework for recognizing textual entailment which depends on extraction of the set of publiclyheld beliefs – known as discourse commitments – that can be ascribed to the author of a text or a hypothesis. Once a set of commitments have been extracted from a t-h pair, the task of recognizing textual entailment is reduced to the identiﬁcation of the commitments from a t which support the inference of the h. Promising results were achieved: our system correctly identiﬁed more than 80% of examples from the RTE-3 Test Set correctly, without the need for additional sources of training data or other web-based resources. 
As shown in the formal semantics literature, adjectives can display very different inferential patterns depending on whether they are intersective, privative, subsective or plain non-subsective. Moreover, many of these classes are often described using second order constructs. In this paper, we adopt Hobbs’s ontologically promiscuous approach and present a ﬁrst order treatment of adjective semantics which opens the way for a sophisticated treatment of adjectival inference. The approach was implemented and tested using ﬁrst order automated reasoners. 
This paper presents the ﬁrst use of a computational model of natural logic—a system of logical inference which operates over natural language—for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacriﬁcing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on ﬁrst-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system ﬁnds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the ﬁrst reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields signiﬁcant performance gains. 
We introduce the proceedings from the workshop ‘Computing and Historical Phonology: 9th Meeting of the ACL Special Interest Group for Computational Morphology and Phonology’. 
Phylogenetic analyses of languages need to explicitly address whether the languages under consideration are related to each other at all. Recently developed permutation tests allow this question to be explored by testing whether words in one set of languages are signiﬁcantly more similar to those in another set of languages when paired up by semantics than when paired up at random. Seven different phonetic similarity metrics are implemented and evaluated on their effectiveness within such multilateral comparison systems when deployed to detect genetic relations among the Indo-European and Uralic language families. 
This present work deliberately abandons the purpose of capturing the global resemblance between languages and the ambition of giving a rational foundation to probability of changes in linguistics, to focus instead on cladistic approach, which was applied to different dialects and data (gallo-romance, southern italo-romance) through an original coding of philological derivations. Results show good congruence with linguistic classification and provide new insight on how tackle various dialectological problems as borrowings. 
In this paper we use the Reeks Nederlandse Dialectatlassen as a source for the reconstruction of a ‘proto-language’ of Dutch dialects. We used 360 dialects from locations in the Netherlands, the northern part of Belgium and French-Flanders. The density of dialect locations is about the same everywhere. For each dialect we reconstructed 85 words. For the reconstruction of vowels we used knowledge of Dutch history, and for the reconstruction of consonants we used well-known tendencies found in most textbooks about historical linguistics. We validated results by comparing the reconstructed forms with pronunciations according to a proto-Germanic dictionary (Köbler, 2003). For 46% of the words we reconstructed the same vowel or the closest possible vowel when the vowel to be reconstructed was not found in the dialect material. For 52% of the words all consonants we reconstructed were the same. For 42% of the words, only one consonant was differently reconstructed. We measured the divergence of Dutch dialects from their ‘proto-language’. We measured pronunciation distances to the protolanguage we reconstructed ourselves and correlated them with pronunciation distances we measured to proto-Germanic based on the dictionary. Pronunciation distances were measured using Levenshtein distance, a string edit distance measure. We found a relatively strong correlation (r=0.87).  
Quantitative measurement of inter-language distance is a useful technique for studying diachronic and synchronic relations between languages. Such measures have been used successfully for purposes like deriving language taxonomies and language reconstruction, but they have mostly been applied to handcrafted word lists. Can we instead use corpus based measures for comparative study of languages? In this paper we try to answer this question. We use three corpus based measures and present the results obtained from them and show how these results relate to linguistic and historical knowledge. We argue that the answer is yes and that such studies can provide or validate linguistic and computational insights. 
Pair Hidden Markov Models (PairHMMs) are trained to align the pronunciation transcriptions of a large contemporary collection of Dutch dialect material, the GoemanTaeldeman-Van Reenen-Project (GTRP, collected 1980–1995). We focus on the question of how to incorporate information about sound segment distances to improve sequence distance measures for use in dialect comparison. PairHMMs induce segment distances via expectation maximisation (EM). Our analysis uses a phonologically comparable subset of 562 items for all 424 localities in the Netherlands. We evaluate the work ﬁrst via comparison to analyses obtained using the Levenshtein distance on the same dataset and second, by comparing the quality of the induced vowel distances to acoustic differences. 
This paper discusses the reconstruction of the Elamite language’s phonology from its orthography using the Gradual Learning Algorithm, which was re-purposed to “learn” underlying phonological forms from surface orthography. Practical issues are raised regarding the difﬁculty of mapping between orthography and phonology, and Optimality Theory’s neglected Lexicon Optimization module is highlighted. 
The verb inﬂections of Bengali underwent a series of phonological change between 10th and 18th centuries, which gave rise to several modern dialects of the language. In this paper, we offer a functional explanation for this change by quantifying the functional pressures of ease of articulation, perceptual contrast and learnability through objective functions or constraints, or both. The multi-objective and multi-constraint optimization problem has been solved through genetic algorithm, whereby we have observed the emergence of Pareto-optimal dialects in the system that closely resemble some of the real ones. 
With the supply of 8 closely interpreted dialectometrical maps, this paper analyses the linguistic change of the geolinguistic deep structures in Northern France (Domaine d’Oïl) between 1300 and 1900. As a matter of fact, the result will show – with one exception – the great stability of these deep structures. 
This paper describes the development and use of an interface for visually evaluating distance measures. The combination of multidimensional scaling plots, histograms and tables allows for different stages of overview and detail. The interdisciplinary project Rule-based search in text databases with nonstandard orthography develops a fuzzy full text search engine and uses distance measures for historical text document retrieval. This engine should provide easier text access for experts as well as interested amateurs. 
In this work, we attempt to capture patterns of co-occurrence across vowel systems and at the same time ﬁgure out the nature of the force leading to the emergence of such patterns. For this purpose we deﬁne a weighted network where the vowels are the nodes and an edge between two nodes (read vowels) signify their co-occurrence likelihood over the vowel inventories. Through this network we identify communities of vowels, which essentially reﬂect their patterns of co-occurrence across languages. We observe that in the assortative vowel communities the constituent nodes (read vowels) are largely uncorrelated in terms of their features indicating that they are formed based on the principle of maximal perceptual contrast. However, in the rest of the communities, strong correlations are reﬂected among the constituent vowels with respect to their features indicating that it is the principle of feature economy that binds them together. 
We use an iterative process of multi-gram alignment between associated words in different languages in an attempt to identify cognates. To maximise the amount of data, we use practical orthographies instead of consistently coded phonetic transcriptions. First results indicate that using practical orthographies can be useful, the more so when dealing with large amounts of data. 
Paradigms provide an inherent organizational structure to natural language morphology. ParaMor, our minimally supervised morphology induction algorithm, retrusses the word forms of raw text corpora back onto their paradigmatic skeletons; performing on par with state-ofthe-art minimally supervised morphology induction algorithms at morphological analysis of English and German. ParaMor consists of two phases. Our algorithm first constructs sets of affixes closely mimicking the paradigms of a language. And with these structures in hand, ParaMor then annotates word forms with morpheme boundaries. To set ParaMor’s few free parameters we analyze a training corpus of Spanish. Without adjusting parameters, we induce the morphological structure of English and German. Adopting the evaluation methodology of Morpho Challenge 2007 (Kurimo et al., 2007), we compare ParaMor’s morphological analyses with Morfessor (Creutz, 2006), a modern minimally supervised morphology induction system. ParaMor consistently achieves competitive F1 measures. 
This paper reports the results of a research project that experiments with crosstabulation in aiding phonemic reconstruction. Data from the Tup´ı stock was used, and three tests were conducted in order to determine the efﬁcacy of this application: the conﬁrmation and challenging of a previously established reconstruction in the family; testing a new reconstruction generated by our model; and testing the upper limit of simultaneous, multiple correspondences across several languages. Our conclusion is that the use of cross tabulations (implemented within a database as pivot tables) offers an innovative and effective tool in comparative study and sound reconstruction. 
We apply algorithms for the identiﬁcation of cognates and recurrent sound correspondences proposed by Kondrak (2002) to the Totonac-Tepehua family of indigenous languages in Mexico. We show that by combining expert linguistic knowledge with computational analysis, it is possible to quickly identify a large number of cognate sets within the family. Our objective is to provide tools for rapid construction of comparative dictionaries for relatively unfamiliar language families. 
Recent work identifies two properties that appear particularly relevant to the characterization of graph-based dependency models of syntactic structure1: the absence of interleaving substructures (well-nestedness) and a bound on a type of discontinuity (gap-degree ≤ 1) successfully describe more than 99% of the structures in two dependency treebanks (Kuhlmann and Nivre 2006).2 Bodirsky et al. (2005) establish that every dependency structure with these two properties can be recast as a lexicalized Tree Adjoining Grammar (LTAG) derivation and vice versa. However, multicomponent extensions of TAG (MC-TAG), argued to be necessary on linguistic grounds, induce dependency structures that do not conform to these two properties (Kuhlmann and Möhl 2006). In this paper, we observe that several types of MC-TAG as used for linguistic analysis are more restrictive than the formal system is in principle. In particular, tree-local MC-TAG, tree-local MC-TAG with flexible composi- 
This paper investigates perceptron training for a wide-coverage CCG parser and compares the perceptron with a log-linear model. The CCG parser uses a phrase-structure parsing model and dynamic programming in the form of the Viterbi algorithm to ﬁnd the highest scoring derivation. The difﬁculty in using the perceptron for a phrase-structure parsing model is the need for an efﬁcient decoder. We exploit the lexicalized nature of CCG by using a ﬁnite-state supertagger to do much of the parsing work, resulting in a highly efﬁcient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and ﬁnd that order does not signiﬁcantly affect the results. 
We present a log-linear model for the disambiguation of the analyses produced by a German broad-coverage LFG, focussing on the properties (or features) this model is based on. We compare this model to an initial model based only on a part of the properties provided to the ﬁnal model and observe that the performance of a log-linear model for parse selection depends heavily on the types of properties that it is based on. In our case, the error reduction achieved with the log-linear model based on the extended set of properties is 51.0% and thus compares very favorably to the error reduction of 34.5% achieved with the initial model. 
In this paper we present a framework for experimentation on parse selection using syntactic and semantic features. Results are given for syntactic features, dependency relations and the use of semantic classes. 
In this paper, we propose a new syntaxbased machine translation (MT) approach based on reducing the MT task to a treelabeling task, which is further decomposed into a sequence of simple decisions for which discriminative classiﬁers can be trained. The approach is very ﬂexible and we believe that it is particularly well-suited for exploiting the linguistic knowledge encoded in deep grammars whenever possible, while at the same time taking advantage of data-based techniques that have proven a powerful basis for MT, as recent advances in statistical MT show. A full system using the Lexical-Functional Grammar (LFG) parsing system XLE and the grammars from the Parallel Grammar development project (ParGram; (Butt et al., 2002)) has been implemented, and we present preliminary results on English-toGerman translation with a tree-labeling system trained on a small subsection of the Europarl corpus. 
This paper discusses how lexical resources based on semantic roles (i.e. FrameNet, PropBank, VerbNet) can be used for Question Answering, especially Web Question Answering. Two algorithms have been implemented to this end, with quite different characteristics. We discuss both approaches when applied to each of the resources and a combination of these and give an evaluation. We argue that employing semantic roles can indeed be highly beneﬁcial for a QA system. 
We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems. The goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application ontology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning. 
This paper proposes a design strategy for deep language processing grammars to appropriately handle language variants. It allows a grammar to be restricted as to what language variant it is tuned to, but also to detect the variant a given input pertains to. This is evaluated and compared to results obtained with an alternative strategy by which the relevant variant is detected with current language identiﬁcation methods in a preprocessing step. 
The demand for deep linguistic analysis for huge volumes of data means that it is increasingly important that the time taken to parse such data is minimized. In the XLE parsing model which is a hand-crafted, uniﬁcation-based parsing system, most of the time is spent on uniﬁcation, searching for valid f-structures (dependency attributevalue matrices) within the space of the many valid c-structures (phrase structure trees). We carried out an experiment to determine whether pruning the search space at an earlier stage of the parsing process results in an improvement in the overall time taken to parse, while maintaining the quality of the f-structures produced. We retrained a stateof-the-art probabilistic parser and used it to pre-bracket input to the XLE, constraining the valid c-structure space for each sentence. We evaluated against the PARC 700 Dependency Bank and show that it is possible to decrease the time taken to parse by ∼18% while maintaining accuracy. 
We discuss semantic composition in Minimal Recursion Semantics (MRS) and Robust Minimal Recursion Semantics (RMRS). We demonstrate that a previously deﬁned formal algebra applies to grammar engineering across a much greater range of frameworks than was originally envisaged. We show how this algebra can be adapted to composition in grammar frameworks where a lexicon is not assumed, and how this underlies a practical implementation of semantic construction for the RASP system. 
Several recent approaches to Information Extraction (IE) have used dependency trees as the basis for an extraction pattern representation. These approaches have used a variety of pattern models (schemes which deﬁne the parts of the dependency tree which can be used to form extraction patterns). Previous comparisons of these pattern models are limited by the fact that they have used indirect tasks to evaluate each model. This limitation is addressed here in an experiment which compares four pattern models using an unsupervised learning algorithm and a standard IE scenario. It is found that there is a wide variation between the models’ performance and suggests that one model is the most useful for IE. 
The lack of a large annotated systemic functional grammar (SFG) corpus has posed a signiﬁcant challenge for the development of the theory. Automating SFG annotation is challenging because the theory uses a minimal constituency model, allocating as much of the work as possible to a set of hierarchically organised features. In this paper we show that despite the unorthodox organisation of SFG, adapting existing resources remains the most practical way to create an annotated corpus. We present and analyse SFGBank, an automated conversion of the Penn Treebank into systemic functional grammar. The corpus is comparable to those available for other linguistic theories, offering many opportunities for new research. 
In the paper, we describe methods for exploitation of a new lexical database of valency frames (VerbaLex) in relation to Transparent Intensional Logic (TIL). We present a detailed description of the Complex Valency Frames (CVF) as they appear in VerbaLex including basic ontology of the VerbaLex semantic roles. TIL is a typed logical system developed for natural language semantic representation using TIL logical forms known as constructions. TIL is well suited to handle the difficult language phenomena such as temporal relations, intensionality and propositional attitudes. Here we make use of the long-term development of the Normal Translation Algorithm aimed at automatic translation of natural language sentences into TIL constructions. We examine the relations between CVFs and TIL constructions of predicateargument structures and discuss the procedure of automatic acquisition of the verbal object constructions. The exploitation of CVFs in the syntactic parsing is also briefly mentioned. 
This paper describes work on the development of an open-source HPSG grammar for Spanish implemented within the LKB system. Following a brief description of the main features of the grammar, we present our approach for pre-processing and ongoing research on automatic lexical acquisition.1 
We examine the feasibility of harvesting a wide-coverage lexicon of English verbs from the FrameNet semantically annotated corpus, intended for use in a practical natural language understanding (NLU) system. We identify a range of constructions for which current annotation practice leads to problems in deriving appropriate lexical entries, for example imperatives, passives and control, and discuss potential solutions. 
The development of robust “deep” linguistic parsers is known to be a difﬁcult task. Few such systems can claim to satisfy the needs of large-scale NLP applications in terms of robustness, efﬁciency, granularity or precision. Adapting such systems to more than one language makes the task even more challenging. This paper describes some of the properties of Fips, a multilingual parsing system that has been for a number of years (and still is) under development at LATL. Based on Chomsky’s generative grammar for its grammatical aspects, and on objectoriented (OO) sofware engineering techniques for its implementation, Fips is designed to efﬁciently parse the four Swiss “national” languages (German, French, Italian and English) to which we also added Spanish and (more recently) Greek. 
This paper presents an approach to partial parse selection for robust deep processing. The work is based on a bottom-up chart parser for HPSG parsing. Following the definition of partial parses in (Kasper et al., 1999), different partial parse selection methods are presented and evaluated on the basis of multiple metrics, from both the syntactic and semantic viewpoints. The application of the partial parsing in spontaneous speech texts processing shows promising competence of the method. 
We present a validation methodology for a cross-linguistic grammar resource which produces output in the form of small grammars based on elicited typological descriptions. Evaluating the resource entails sampling from a very large space of language types, the type and range of which preclude the use of standard test suites development techniques. We produce a database from which gold standard test suites for these grammars can be generated on demand, including well-formed strings paired with all of their valid semantic representations as well as a sample of ill-formed strings. These string-semantics pairs are selected from a set of candidates by a system of regularexpression based ﬁlters. The ﬁlters amount to an alternative grammar building system, whose generative capacity is limited compared to the actual grammars. We perform error analysis of the discrepancies between the test suites and grammars for a range of language types, and update both systems appropriately. The resulting resource serves as a point of comparison for regression testing in future development. 
We report on recent advances in HPSG parsing of German with local ambiguity packing (Oepen and Carroll, 2000), achieving a speed-up factor of 2 on a balanced test-suite. In contrast to earlier studies carried out for English using the same packing algorithm, we show that restricting semantic features only is insufﬁcient for achieving acceptable runtime performance with a German HPSG grammar. In a series of experiments relating to the three different types of discontinuities in German (head movement, extraction, extraposition), we examine the effects of restrictor choice, ultimately showing that extraction and head movement require partial restriction of the respective features encoding the dependency, whereas full restriction gives best results for extraposition. 
This paper is concerned with the standardisation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual parser performance. Speciﬁcally, we investigate the impact that lexicons at varying levels of lexical item precision and recall have on the performance of pre-existing broad-coverage precision grammars in parsing, i.e., on their coverage and accuracy. The grammars used for the experiments reported here are the LinGO English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002), precision grammars of English and Japanese, respectively. Our results show convincingly that traditional Fscore-based evaluation of lexical acquisition does not correlate with actual parsing performance. What we argue for, therefore, is a recall-heavy interpretation of F-score in designing and optimising automated lexical acquisition algorithms. 
Natural languages contain many multi-word sequences that do not display the variety of syntactic processes we would expect given their phrase type, and consequently must be included in the lexicon as multiword units. This paper describes a method for identifying such items in corpora, focussing on English verb-noun combinations. In an evaluation using a set of dictionary-published MWEs we show that our method achieves greater accuracy than existing MWE extraction methods based on lexical association. 
We identify several classes of multiword expressions that each require a different encoding in a (computational) lexicon, as well as a different treatment within a computational system. We examine linguistic properties pertaining to the degree of semantic idiosyncrasy of these classes of expressions. Accordingly, we propose statistical measures to quantify each property, and use the measures to automatically distinguish the classes. 
This paper describes the design and implementation of a lexicon of Dutch multiword expressions (MWEs). No exhaustive research on a standard lexical representation of MWEs has been done for Dutch before. The approach taken is innovative, since it is based on the Equivalence Class Method. Furthermore, the selection of the lexical entries and their properties is corpus-based. The design of the lexicon and the standard representation will be tested in Dutch NLP systems. The purpose of the current paper is to give an overview of the decisions made in order to come to a standard lexical representation and to discuss the description ﬁelds this representation comprises. 
This paper describes a fully unsupervised and automated method for large-scale extraction of multiword expressions (MWEs) from large corpora. The method aims at capturing the non-compositionality of MWEs; the intuition is that a noun within a MWE cannot easily be replaced by a semantically similar noun. To implement this intuition, a noun clustering is automatically extracted (using distributional similarity measures), which gives us clusters of semantically related nouns. Next, a number of statistical measures – based on selectional preferences – is developed that formalize the intuition of non-compositionality. Our approach has been tested on Dutch, and automatically evaluated using Dutch lexical resources. 
This paper presents an electronic dictionary of Spanish adverbial frozen expressions. It focuses on their formal description in view of natural language processing and presents an experiment on the automatic application of this data to real texts using finite-state techniques. The paper makes an assessment of the advantages and limitations of this method for the identification of these multiword units in texts. 
Much work on idioms has focused on type identiﬁcation, i.e., determining whether a sequence of words can form an idiomatic expression. Since an idiom type often has a literal interpretation as well, token classiﬁcation of potential idioms in context is critical for NLP. We explore the use of informative prior knowledge about the overall syntactic behaviour of a potentially-idiomatic expression (type-based knowledge) to determine whether an instance of the expression is used idiomatically or literally (tokenbased knowledge). We develop unsupervised methods for the task, and show that their performance is comparable to that of state-of-the-art supervised techniques. 
In this paper we investigate the role of the placement of pauses in automatically extracted multi-word expression (MWE) candidates from a learner corpus. The aim is to explore whether the analysis of pauses might be useful in the validation of these candidates as MWEs. The study is based on the assumption advanced in the area of psycholinguistics that MWEs are stored holistically in the mental lexicon and are therefore produced without pauses in naturally occurring discourse. Automatic MWE extraction methods are unable to capture the criterion of holistic storage and instead rely on statistics and raw frequency in the identification of MWE candidates. In this study we explore the possibility of a combination of the two approaches. We report on a study in which we analyse the placement of pauses in various instances of two very frequent automatically extracted MWE candidates from a learner corpus, i.e. the n-grams I don’t know and I think I. Intuitively, they are judged differently in terms of holistic storage. Our study explores whether pause analysis can be used as an objective empirical criterion to support this intuition. A corpus of interview data of language learners of English forms the basis of this study. 
Contextual information extracted from corpora is frequently used to model semantic similarity. We discuss distinct classes of context types and compare their effectiveness for compound noun interpretation. Contexts corresponding to word-word similarity perform better than contexts corresponding to relation similarity, even when relational co-occurrences are extracted from a much larger corpus. Combining wordsimilarity and relation-similarity kernels further improves SVM classiﬁcation performance. 
This paper proposes an approach of processing Japanese compound functional expressions by identifying them and analyzing their dependency relations through a machine learning technique. First, we formalize the task of identifying Japanese compound functional expressions in a text as a machine learning based chunking problem. Next, against the results of identifying compound functional expressions, we apply the method of dependency analysis based on the cascaded chunking model. The results of experimental evaluation show that, the dependency analysis model achieves improvements when applied after identifying compound functional expressions, compared with the case where it is applied without identifying compound functional expressions. 
This paper discusses the semantic interpretation of compound nominalizations in Chinese. We propose four coarse-grained semantic roles of the noun modiﬁer and use a Maximum Entropy Model to label such relations in a compound nominalization. The feature functions used for the model are web-based statistics acquired via role related paraphrase patterns, which are formed by a set of word instances of prepositions, support verbs, feature nouns and aspect markers. By applying a sub-linear transformation and discretization of the raw statistics, a rate of approximately 77% is obtained for classiﬁcation of the four semantic relations. 
We consider the diagnostic utility of various syntactic complexity measures when extracted from spoken language samples of healthy and cognitively impaired subjects. We examine measures calculated from manually built parse trees, as well as the same measures calculated from automatic parses. We show statistically signiﬁcant differences between clinical subject groups for a number of syntactic complexity measures, and these differences are preserved with automatic parsing. Different measures show different patterns for our data set, indicating that using multiple, complementary measures is important for such an application. 
This paper demonstrates a method for determining the syntactic structure of medical terms. We use a model-ﬁtting method based on the Log Likelihood Ratio to classify three-word medical terms as right or left-branching. We validate this method by computing the agreement between the classiﬁcation produced by the method and manually annotated classiﬁcations. The results show an agreement of 75% - 83%. This method may be used effectively to enable a wide range of applications that depend on the semantic interpretation of medical terms including automatic mapping of terms to standardized vocabularies and induction of terminologies from unstructured medical text. 
This paper investigates the roles of named entities (NE’s) in annotated biomedical text classiﬁcation. In the annotation schema of BioCaster, a text mining system for public health protection, important concepts that reﬂect information about infectious diseases were conceptually analyzed with a formal ontological methodology. Concepts were classiﬁed as Types, while others were identiﬁed as being Roles. Types are speciﬁed as NE classes and Roles are integrated into NEs as attributes. We focus on the Roles of NEs by extracting and using them in different ways as features in the classiﬁer. Experimental results show that: 1) Roles for each NE greatly helped improve performance of the system, 2) combining information about NE classes with their Roles contribute signiﬁcantly to the improvement of performance. We discuss in detail the effect of each Role on the accuracy of text classiﬁcation. 
Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such uniﬁcation by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application-oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the uniﬁcation of the syntactic annotations of BioInfer and the GENIA Treebank. We ﬁnd that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction. 
We propose an unsupervised method to automatically extract domain-speciﬁc preﬁxes and sufﬁxes from biological corpora based on the use of PATRICIA tree. The method is evaluated by integrating the extracted afﬁxes into an existing learning-based biological term annotation system. The system based on our method achieves comparable experimental results to the original system in locating biological terms and exact term matching annotation. However, our method improves the system efﬁciency by signiﬁcantly reducing the feature set size. Additionally, the method achieves a better performance with a small training data set. Since the afﬁx extraction process is unsupervised, it is assumed that the method can be generalized to extract domain-speciﬁc afﬁxes from other domains, thus assisting in domain-speciﬁc concept recognition. 
Gene names and symbols are important biomedical entities, but are highly ambiguous. This ambiguity affects the performance of both information extraction and information retrieval systems in the biomedical domain. Existing knowledge sources contain different types of information about genes and could be used to disambiguate gene symbols. In this paper, we applied an information retrieval (IR) based method for human gene symbol disambiguation and studied different methods to combine various types of information from available knowledge sources. Results showed that a combination of evidence usually improved performance. The combination method using coefficients obtained from a logistic regression model reached the highest precision of 92.2% on a testing set of ambiguous human gene symbols. 
We present a corpus-driven method for building a lexicon of semantically equivalent pairs of technical and lay medical terms. Using a parallel corpus of abstracts of clinical studies and corresponding news stories written for a lay audience, we identify terms which are good semantic equivalents of technical terms for a lay audience. Our method relies on measures of association. Results show that, despite the small size of our corpus, a promising number of pairs are identiﬁed. 
We describe the annotation of chemical named entities in scientiﬁc text. A set of annotation guidelines deﬁnes 5 types of named entities, and provides instructions for the resolution of special cases. A corpus of fulltext chemistry papers was annotated, with an inter-annotator agreement score of 93%. An investigation of named entity recognition using LingPipe suggests that scores of 63% are possible without customisation, and scores of 74% are possible with the addition of custom tokenisation and the use of dictionaries. 
Although recent named entity (NE) annotation efforts involve the markup of nested entities, there has been limited focus on recognising such nested structures. This paper introduces and compares three techniques for modelling and recognising nested entities by means of a conventional sequence tagger. The methods are tested and evaluated on two biomedical data sets that contain entity nesting. All methods yield an improvement over the baseline tagger that is only trained on ﬂat annotation. 
This paper presents the results of a pilot usability study of a novel approach to search user interfaces for bioscience journal articles. The main idea is to support search over ﬁgure captions explicitly, and show the corresponding ﬁgures directly within the search results. Participants in a pilot study expressed surprise at the idea, noting that they had never thought of search in this way. They also reported strong positive reactions to the idea: 7 out of 8 said they would use a search system with this kind of feature, suggesting that this is a promising idea for journal article search. 
Applications using automatically indexed clinical conditions must account for contextual features such as whether a condition is negated, historical or hypothetical, or experienced by someone other than the patient. We developed and evaluated an algorithm called ConText, an extension of the NegEx negation algorithm, which relies on trigger terms, pseudo-trigger terms, and termination terms for identifying the values of three contextual features. In spite of its simplicity, ConText performed well at identifying negation and hypothetical status. ConText performed moderately at identifying whether a condition was experienced by someone other than the patient and whether the condition occurred historically. 
The vast number of published medical documents is considered a vital source for relationship discovery. This paper presents a statistical unsupervised system, called BioNoculars, for extracting protein-protein interactions from biomedical text. BioNoculars uses graph-based mutual reinforcement to make use of redundancy in data to construct extraction patterns in a domain independent fashion. The system was tested using MEDLINE abstract for which the protein-protein interactions that they contain are listed in the database of interacting proteins and proteinprotein interactions (DIPPPI). The system reports an F-Measure of 0.55 on test MEDLINE abstracts. 
This paper describes the application of an ensemble of indexing and classification systems, which have been shown to be successful in information retrieval and classification of medical literature, to a new task of assigning ICD-9-CM codes to the clinical history and impression sections of radiology reports. The basic methods used are: a modification of the NLM Medical Text Indexer system, SVM, k-NN and a simple pattern-matching method. The basic methods are combined using a variant of stacking. Evaluated in the context of a Medical NLP Challenge, fusion produced an Fscore of 0.85 on the Challenge test set, which is considerably above the mean Challenge F-score of 0.77 for 44 participating groups. 
This paper describes a system capable of semi-automatically filling an XML template from free texts in the clinical domain (practice guidelines). The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and actions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present a system developed for this task. We show that it yields good performance when applied to the analysis of French practice guidelines. 
This paper describes experiments in classifying sentences of medical abstracts into a number of semantic classes given by section headings in structured abstracts. Using conditional random ﬁelds, we obtain F -scores ranging from 0.72 to 0.97. By using a small set of sentences that appear under the PARTICPANTS heading, we demonstrate that it is possible to recognize sentences that describe population characteristics of a study. We present a detailed study of the structure of abstracts of randomized clinical trials, and examine how sentences labeled under PARTICIPANTS could be used to summarize the population group. 
Code assignment is important for handling large amounts of electronic medical data in the modern hospital. However, only expert annotators with extensive training can assign codes. We present a system for the assignment of ICD-9-CM clinical codes to free text radiology reports. Our system assigns a code conﬁguration, predicting one or more codes for each document. We combine three coding systems into a single learning system for higher accuracy. We compare our system on a real world medical dataset with both human annotators and other automated systems, achieving nearly the maximum score on the Computational Medicine Center’s challenge. 
We propose a methodology using underspecified semantic interpretation to process comparative constructions in MEDLINE citations, concentrating on two structures that are prevalent in the research literature reporting on clinical trials for drug therapies. The method exploits an existing semantic processor, SemRep, which constructs predications based on the Unified Medical Language System. Results of a preliminary evaluation were recall of 70%, precision of 96%, and F-score of 81%. We discuss the generalization of the methodology to other entities such as therapeutic and diagnostic procedures. The available structures in computable format are potentially useful for interpreting outcome statements in MEDLINE citations. 
There has been much recent interest in the extraction of PPIs (protein-protein interactions) from biomedical texts, but in order to assist with curation efforts, the PPIs must be enriched with further information of biological interest. This paper describes the implementation of a system to extract and enrich PPIs, developed and tested using an annotated corpus of biomedical texts, and employing both machine-learning and rulebased techniques. 
Many approaches for named entity recognition rely on dictionaries gathered from curated databases (such as Entrez Gene for gene names.) Strategies for matching entries in a dictionary against arbitrary text use either inexact string matching that allows for known deviations, dictionaries enriched according to some observed rules, or a combination of both. Such reﬁned dictionaries cover potential structural, lexical, orthographical, or morphological variations. In this paper, we present an approach to automatically analyze dictionaries to discover how names are composed and which variations typically occur. This knowledge can be constructed by looking at single entries (names and synonyms for one gene), and then be transferred to entries that show similar patterns in one or more synonyms. For instance, knowledge about words that are frequently missing in (or added to) a name (“antigen”, “protein”, “human”) could automatically be extracted from dictionaries. This paper should be seen as a vision paper, though we implemented most of the ideas presented and show results for the task of gene name recognition. The automatically extracted name composition rules can easily be included in existing approaches, and provide valuable insights into the biomedical sub-language.  
This paper describes a preliminary analysis of issues involved in the production of reports aimed at patients from Electronic Patient Records. We present a system prototype and discuss the problems encountered. 
The names of named entities very often occur as constituents of larger noun phrases which denote different types of entity. Understanding the structure of the embedding phrase can be an enormously beneﬁcial ﬁrst step to enhancing whatever processing is intended to follow the named entity recognition in the ﬁrst place. In this paper, we examine the integration of general purpose linguistic processors together with domain speciﬁc named entity recognition in order to carry out the task of baseNP detection. We report a best F-score of 87.17% on this task. We also report an inter-annotator agreement score of 98.8 Kappa on the task of baseNP annotation of a new data set. 
At present, most biomedical Information Retrieval and Extraction tools process abstracts rather than full-text articles. The increasing availability of full text will allow more knowledge to be extracted with greater reliability. To investigate the challenges of full-text processing, we manually annotated a corpus of cited articles from a Molecular Interaction Map (Kohn, 1999). Our analysis demonstrates the necessity of full-text processing; identiﬁes the article sections where interactions are most commonly stated; and quantiﬁes both the amount of external knowledge required and the proportion of interactions requiring multiple or deeper inference steps. Further, it identiﬁes a range of NLP tools required, including: identifying synonyms, and resolving coreference and negated expressions. This is important guidance for researchers engineering biomedical text processing systems. 
{jmiller,vijay}@cis.udel.edu  2Biostatistics, Bioinformatics and Biomathematics Georgetown University Medical Center Washington, DC 20057 mt352@georgetown.edu  
The paper presents two rule-based information extraction (IE) from two types of patients’ documentation in Polish. For both document types, values of sets of attributes were assigned using specially designed grammars. 
The shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized. In the biomedical domain, continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as MEDLINE®. In this paper, we evaluate two statistical methods of producing MeSH® indexing recommendations for the genetics literature, including recommendations involving subheadings, which is a novel application for the methods. We show that a generic representation of the documents yields both better precision and recall. We also find that a domainspecific representation of the documents can contribute to enhancing recall. 
This paper proposes a machine learning approach to the task of assigning the international standard on classiﬁcation of diseases ICD-9-CM codes to clinical records. By treating the task as a text categorisation problem, a classiﬁcation system was built which explores a variety of features including negation, different strategies of measuring gloss overlaps between the content of clinical records and ICD-9-CM code descriptions together with expansion of the glosses from the ICD-9-CM hierarchy. The best classiﬁer achieved an overall F1 value of 88.2 on a data set of 978 free text clinical records, and was better than the performance of two out of three human annotators. 
We report on an empirical study that deals with the quantity of different kinds of referring expressions in biomedical abstracts. 
This paper is concerned with the evaluation of biomedical named entity recognition systems. We compare two such systems, one based on a Hidden Markov Model and one based on Conditional Random Fields and syntactic parsing. In our experiments we used automatically generated data as well as manually annotated material, including a new dataset which consists of biomedical full papers. Through our evaluation, we assess the strengths and weaknesses of the systems tested, as well as the datasets themselves in terms of the challenges they present to the systems. 
Morphological analysis as applied to English has generally involved the study of rules for inflections and derivations. Recent work has attempted to derive such rules from automatic analysis of corpora. Here we study similar issues, but in the context of the biological literature. We introduce a new approach which allows us to assign probabilities of the semantic relatedness of pairs of tokens that occur in text in consequence of their relatedness as character strings. Our analysis is based on over 84 million sentences that compose the MEDLINE database and over 2.3 million token types that occur in MEDLINE and enables us to identify over 36 million token type pairs which have assigned probabilities of semantic relatedness of at least 0.7 based on their similarity as strings. 
This paper provides a description and evaluation of a generic named-entity recognition (NER) system for Swedish applied to electronic versions of Swedish literary classics from the 19th century. We discuss the challenges posed by these texts and the necessary adaptations introduced into the NER system in order to achieve accurate results, useful both for metadata generation, but also for the enhancement of the searching and browsing capabilities of Litteraturbanken, the Swedish Literature Bank, an ongoing cultural heritage project which aims to digitize significant works of Swedish literature. 
An alignment method based on the Viterbi algorithm is proposed to ﬁnd mappings between word images of a given handwritten document and their respective (ASCII) words on its transcription. The approach takes advantage of the underlying segmentation made by Viterbi decoding in handwritten text recognition based on Hidden Markov Models (HMMs). Two HMMs modelling schemes are evaluated: one using 78-HMMs (one HMM per character class) and other using a unique HMM to model all the characters and another to model blank spaces. According to various metrics used to measure the quality of the alignments, encouraging results are obtained. 
Importing large amounts of data into databases does not always go without the loss of important information. In this work, methods are presented that aim to rediscover this information by inferring it from the information that is available in the database. From and animal specimen database, the information to which expedition an animal that was found belongs is rediscovered. While the work is in an early stage, the obtained results are promising, and prove that it is possible to rediscover expedition information from the database. 
We address the problem of mining text for relevant image metadata. Our work is situated in the art and architecture domain, where highly specialized technical vocabulary presents challenges for NLP techniques. To extract high quality metadata, the problem of word sense disambiguation must be addressed in order to avoid leading the searcher to the wrong image as a result of ambiguous — and thus faulty — metadata. In this paper, we present a disambiguation algorithm that attempts to select the correct sense of nouns in textual descriptions of art objects, with respect to a rich domain-specific thesaurus, the Art and Architecture Thesaurus (AAT). We performed a series of intrinsic evaluations using a data set of 600 subject terms extracted from an online National Gallery of Art (NGA) collection of images and text. Our results showed that the use of external knowledge sources shows an improvement over a baseline. 1. Introduction We describe an algorithm that takes noun phrases and assigns a sense to the head noun or phrase, given a large domain-specific thesaurus, the Art and Architecture Thesaurus1 (published by the Getty Research Institute). This research is part of the Computational Linguistics for Metadata 1http://www.getty.edu/research/conducting_research/vocabul aries/aat/  Building (CLiMB) project (Klavans 2006, Klavans in preparation), which aims to improve image access by automatically extracting metadata from text associated with images. We present here a component of an overall architecture that automatically mines scholarly text for metadata terms. In order to filter and associate a term with a related concept, ambiguous terms must be clarified. The disambiguation of terms is a basic challenge in computational linguistics (Ide and Veronis 1990, Agirre and Edmonds 2006). As more non-specialists in digital libraries search for images, the need for subject term access has increased. Subject terms enrich catalog records with valuable broad-reaching metadata and help improve image access (Layne 1994). Image seekers will receive more relevant results if image records contain terms that reflect conceptual, semantic, and ontological relationships. Furthermore, subject terms associated with hierarchical and faceted thesaural senses promise to further improve precision in image access. Such terms map to standardized thesaurus records that include the term’s preferred, variant, and related names, including both broader and specific concepts, and other related concepts. This information can then be filtered, linked, and subsequently tested for usefulness in performing richer image access. As with other research on disambiguation, our hypothesis is that accurate assignment of senses to metadata index terms will results in higher precision for searchers. This hypothesis will be fully tested as we incorporate the disambiguation module in our end-to-end CLiMB Toolkit, and as we perform user studies. Finding subject terms and mapping them to a thesaurus is a time-intensive task for catalogers  25 Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 25–32, Prague, 28 June 2007. c 2007 Association for Computational Linguistics  (Rasmussen 1997, Ferguson and Intner 1998). Doing so typically involves reading image-related text or other sources to find subject terms. Even so, the lack of standard vocabulary in extensive subject indexing means that the enriched number of subject terms could be inadvertently offset by the vocabulary naming problem (Baca 2002). This paper reports on our results using the subject terms in the AAT; the CLiMB project is also using the Thesaurus of Geographic Names (TGN) and the Union List of Artist Names (ULAN). Since the focus of this paper is on disambiguation of common nouns rather than proper nouns, the AAT is our primary resource. 2. Resources 2.1 Art and Architecture Thesaurus (AAT) The AAT is a widely-used multi-faceted thesaurus of terms for the cataloging and indexing of art, architecture, artifactual, and archival materials. Since the AAT offers a controlled vocabulary for recording and retrieval of data in object, bibliographic, and visual databases, it is of interest to a wide community. In the AAT, each concept is described through a record which has a unique ID, preferred name, record description, variant names, broader, narrower, and related terms. In total, AAT has 31,000 such records. For the purpose of this article, a record can be viewed as synonymous with sense. Within the AAT, there are 1,400 homonyms, i.e., records with same preferred name. For example, the term wings has five senses in the AAT (see Figure 1 below). Wings (5 senses): • Sense#1: Used for accessories that project outward from the shoulder of a garment and are made of cloth or metal. • Sense#2: Lateral parts or appendages of a work of art, such as those found on a triptych. • Sense#3: The areas offstage and to the side of the acting area. • Sense#4: The two forward extensions to the sides of the back on an easy chair. • Sense#5: Subsidiary parts of buildings extending out from the main portion. Figure 1: Selection of AAT records for term “wings”  Table 1 shows the breakdown of the AAT vocabulary by number of senses with a sample lexical item for each frequency.  # of Senses 2  # of Homonyms 1097  Example bells  3  215  painting  4  50  alabaster  5  39  wings  6  9  boards  7  5  amber  8  2  emerald  9  
This paper describes the mutually beneﬁcial relationship between a cultural heritage digital library and a historical treebank: an established digital library can provide the resources and structure necessary for efﬁciently building a treebank, while a treebank, as a language resource, is a valuable tool for audiences traditionally served by such libraries. 
This article presents a method to extract and query Cultural Heritage (CH) textual digital resources. The extraction and querying phases are linked by a common ontological representation (CIDOC-CRM). A transport format (RDF) allows the ontology to be queried in a suitable query language (SPARQL), on top of which an interface makes it possible to formulate queries in Natural Language (NL). The extraction phase exploits the propositional nature of the ontology. The query interface is based on the Generate and Select principle, where potentially suitable queries are generated to match the user input, only for the most semantically similar candidate to be selected. In the process we evaluate data extracted from the description of a medieval city (Wolfenbu¨ttel), transform and develop two methods of computing similarity between sentences based on WordNet. Experiments are described that compare the pros and cons of the similarity measures and evaluate them. 
This research is concerned with making recommendations to museum visitors based on their history within the physical environment, and textual information associated with each item in their history. We investigate a method of providing such recommendations to users through a combination of language modelling techniques, geospatial modelling of the physical space, and observation of sequences of locations visited by other users in the past. This study compares and analyses different methods of path prediction including an adapted naive Bayes method, document similarity, visitor feedback and measures of lexical similarity. 
In this paper, we argue on the interest of anchoring Dutch Cultural Heritage controlled vocabularies to WordNet, and demonstrate a reusable methodology for achieving this anchoring. We test it on two controlled vocabularies, namely the GTAA thesaurus, used at the Netherlands Institute for Sound and Vision (the Dutch radio and television archives), and the GTT thesaurus, used to index books of the Dutch National Library. We evaluate the two anchorings having in mind a concrete use case, namely generic alignment scenarios where concepts from one thesaurus must be aligned to concepts from the other.  collections using the concepts that appear in the metadata. The Netherlands Institute for Sound and Vision2, for example, uses the GTAA thesaurus for indexing public radio and TV programs – GTAA is a Dutch abbreviation for “Common Thesaurus [for] Audiovisual Archives”. Its hierarchy of subjects contains about 3800 Preferred Terms and 2000 Non Preferred terms. A second example is the GTT thesaurus, which contains 35000 concepts, gathering 50000 preferred and non-preferred Dutch terms. This thesaurus is used to index and retrieve books from the Dutch National Library3 – GTT is a Dutch abbreviation for “GOO keyword thesaurus”, GOO referring to the Joint Subject Indexing system used by many Dutch libraries.  
We describe a system which enhances the experience of museum visits by providing users with language-technology-based information retrieval capabilities. The system consists of a cross-lingual search engine, augmented by state of the art semantic expansion technology, speciﬁcally designed for the domain of the museum (history and archaeology of Israel). We discuss the technology incorporated in the system, its adaptation to the speciﬁc domain and its contribution to cultural heritage appreciation. 
Cultural heritage, and other special domains, pose a particular problem for information retrieval: evaluation requires a dedicated test collection that takes the particular documents and information requests into account, but building such a test collection requires substantial human effort. This paper investigates methods of generating a document retrieval test collection from a search engine’s transaction log, based on submitted queries and user-click data. We test our methods on a museum’s search log ﬁle, and compare the quality of the generated test collections against a collection with manually generated and judged known-item topics. Our main ﬁndings are the following. First, the test collection derived from a transaction log corresponds well to the actual search experience of real users. Second, the ranking of systems based on the derived judgments corresponds well to the ranking based on the manual topics. Third, deriving pseudo-relevance judgments from a transaction log ﬁle is an attractive option in domains where dedicated test collections are not readily available. 
The linguistic features of material in Cultural Heritage (CH) archives may be in various languages requiring a facility for effective multilingual search. The specialised language often associated with CH content introduces problems for automatic translation to support search applications. The MultiMatch project is focused on enabling users to interact with CH content across different media types and languages. We present results from a MultiMatch study exploring various translation techniques for the CH domain. Our experiments examine translation techniques for the English language CLEF 2006 Cross-Language Speech Retrieval (CL-SR) task using Spanish, French and German queries. Results compare effectiveness of our query translation against a monolingual baseline and show improvement when combining a domain-speciﬁc translation lexicon with a standard machine translation system. 
Functional Arabic Morphology is a formulation of the Arabic inﬂectional system seeking the working interface between morphology and syntax. ElixirFM is its high-level implementation that reuses and extends the Functional Morphology library for Haskell. Inﬂection and derivation are modeled in terms of paradigms, grammatical categories, lexemes and word classes. The computation of analysis or generation is conceptually distinguished from the general-purpose linguistic model. The lexicon of ElixirFM is designed with respect to abstraction, yet is no more complicated than printed dictionaries. It is derived from the open-source Buckwalter lexicon and is enhanced with information sourcing from the syntactic annotations of the Prague Arabic Dependency Treebank. 
The numeral system of Arabic is rich in its morphosyntactic variety yet suffers from the lack of a good computational resource that describes it in a reusable way. This implies that applications that require the use of rules of the Arabic numeral system have to either reimplement them each time, which implies wasted resources, or use simpliﬁed, imprecise rules that result in low quality applications. A solution has been devised with Grammatical Framework (GF) to use language constructs and grammars as libraries that can be written once and reused in various applications. In this paper, we describe our implementation of the Arabic numeral system, as an example of a bigger implementation of a grammar library for Arabic. We show that users can reuse our system by accessing a simple language-independent API rule. 
Named entity recognition (NER) is nowadays an important task, which is responsible for the identification of proper names in text and their classification as different types of named entity such as people, locations, and organizations. In this paper, we present our attempt at the recognition and extraction of the most important proper name entity, that is, the person name, for the Arabic language. We developed the system, Person Name Entity Recognition for Arabic (PERA), using a rule-based approach. The system consists of a lexicon, in the form of gazetteer name lists, and a grammar, in the form of regular expressions, which are responsible for recognizing person name entities. The PERA system is evaluated using a corpus that is tagged in a semi-automated way. The system performance results achieved were satisfactory and confirm to the targets set forth for the precision, recall, and fmeasure. 
This paper presents a machine learning approach based on an SVM classifier coupled with preprocessing rules for crossdocument named entity normalization. The classifier uses lexical, orthographic, phonetic, and morphological features. The process involves disambiguating different entities with shared name mentions and normalizing identical entities with different name mentions. In evaluating the quality of the clusters, the reported approach achieves a cluster F-measure of 0.93. The approach is significantly better than the two baseline approaches in which none of the entities are normalized or entities with exact name mentions are normalized. The two baseline approaches achieve cluster F-measures of 0.62 and 0.74 respectively. The classifier properly normalizes the vast majority of entities that are misnormalized by the baseline system. 1. Introduction: Much recent attention has focused on the extraction of salient information from unstructured text. One of the enabling technologies for information extraction is Named Entity Recognition (NER), which is concerned with identifying the names of persons, organizations, locations, expressions of times, quantities, ... etc. (Chinchor, 1999; Maynard et al., 2001; Sekine, 2004; Joachims, 2002). The NER task is challenging due to the ambiguity of natural language and to the lack of uniformity in writing  styles and vocabulary used across documents (Solorio, 2004). Beyond NER, considerable work has focused on the tracking and normalization of entities that could be mentioned using different names (e.g. George Bush, Bush) or nominals (e.g. the president, Mr., the son) (Florian et al., 2004). Most of the named entity tracking work has focused on intra-document normalization with very limited work on cross-documents normalization. Recognizing and tracking entities of type “Person Name” are particularly important for information extraction. Yet they pose interesting challenges that require special attention. The problems can result from: 1. A Person’s name having many variant spellings (especially when it is transliterated into a foreign language). These variations are typically limited in the same document, but are very common across different documents from different sources (e.g. Mahmoud Abbas = Mahmod Abas, Mohamed El-Baradei = Muhammad AlBaradey … etc). 2. A person having more than one name (e.g. Mahmoud Abbas = Abu Mazen). 3. Some names having very similar or identical names but refer to completely different persons (George H. W. Bush ≠ George W. Bush). 4. Single token names (e.g. Bill Clinton = Clinton ≠ Hillary Clinton). This paper will focus on Arabic cross-document normalization of named entities of type “person name,” which would involve resolving the aforementioned problems. As illustrated in Figure 1, the task involves normalizing a set of person entities into a set of classes each of which is  25 Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25–32, Prague, Czech Republic, June 2007. c 2007 Association for Computational Linguistics  E1  E2  E3 E4  E6  E7 E5 E8  Normalization  E1 E4 E8 E7 E3  E2 E5 E6  Figure 1 Normalization Model  formed of at least one entity. For N input entities, the output of normalization process will be M classes, where M ≤ N. Each class would refer to only one person and each class would contain all entities referring to that person. For this work, intra-document normalization is assumed and an entity refers to a normalized set of name mentions and nominals referring to a single person in a single document. Florian et al. (2004) were kind enough to provide the authors access to an updated version of their state-of-the-art Named Entity Recognition and Tracking (NERT) system, which achieves an F-measure of 0.77 for NER, and an F-measure of 0.88 for intra-document normalization assuming perfect NER. Although the NERT systems is efficient for relatively short documents, it is computational impractical for large documents, which precludes using the NERT system for cross-document normalization through combining the documents into one large document. The main challenges of this work stem from large variations in the spelling of transliterated foreign names and the presence of many common Arabic names (such as Muhammad, Abdullah, Ahmed …etc.), which increases the ambiguity in identifying the person referred to by the mentioned name. Further, the NERT system output system contains many NER errors and intra-document normalization errors. In this paper, cross-document normalization system employs a two-step approach. In the first step, preprocessing rules are used to remove errant named entities. In the second step, a support vector machine (SVM) classifier is used to determine if two entities from two different documents need to be normalized. The classifier is trained on lexical, orthographic, phonetic, and morphological features. The paper is organized as follows: Section 2 provides a background on cross-document NE  normalization; Section 3 describes the preprocessing steps and data used for training and testing; Section 4 describes the normalization methodology; Section 5 describes the experimental setup; Section 6 reports and discusses experimental results; and Section 7 concludes the paper and provides possible future directions. 2. Background While considerable work has focused on named entity normalization within a single document, little work has focused on the challenges associated with resolving person name references across multiple documents. Most of the work done in cross-document normalization focused on the problem of determining if two instances with the same name from different documents referring to the same person (Fleischman and Hovy, 2004). Fleischman and Hovy (2004) focused on distinguishing between individuals having identical names, but they did not extend normalization to different names referring to the same individual. Their task is a subtask of what is examined in this paper. They used a large number of features to accomplish their work, depending mostly on language specific dictionaries and wordnet. Some these resources are not available for Arabic and many other languages. Mann and Yarowsky (Mann and Yarowsky, 2003) examined the same problem but they treated it as a clustering task. They focused on information extraction to build biographical profiles (date of birth, place of birth, etc.), and they wanted to disambiguate biographies belonging to different authors with identical names. Dozier and Zielund (Dozier and Zielund, 2004) reported on cross-document person name normalization in the legal domain. They used a  26  finite state machine that identifies paragraphs in a document containing the names of attorneys, judges, or experts and a semantic parser that extracts from the paragraphs template information about each named individual. They relied on reliable biographies for each individual. A biography would typically contain a person’s first name, middle name, last name, firm, city, state, court, and other information. They used a Bayesian network to match the name mentions to the biographical records. Bhattacharya and Getoor (Bhattacharya and Getoor, 2006) introduced a collective decision algorithm for author name entity resolution, where decisions are not considered on an independent pairwise basis. They focused on using relational links among the references and co-author relationships to infer collaboration groups, which would disambiguate entity names. Such explicit links between co-authors can be extracted directly. However, implicit links can be useful when looking at completely unstructured text. Other work has extended beyond entities of type “person name” to include the normalization of location names (Li et al., 2002) and organizations (Ji and Grishman. 2004). 3. Preprocessing and the Data Set For this work, a set of 7,184 person name entities was constructed. Building new training and test sets is warranted, because the task at hand is sufficiently different from previously reported tasks in the literature. The entities were recognized from 2,931 topically related documents (relating to the situation in the Gaza and Lebanon during July of 2006) from different Arabic news Figure 2 Entity Description  sources (obtained from searching the Arabic version of news.google.com). The entities were recognized and normalized (within document) using the NERT system of Florian et al (2004). As shown in Figure 2, each entity is composed of a set of name mentions (one or more) and a set of nominal mentions (zero or more). The NERT system achieves an F-score of 0.77 with precision of 0.82 and recall of 0.73 for person name mention and nominal recognition and an Fscore of 0.88 for tracking (assuming 100% recognition accuracy). The produced entities may suffer from the following: 1. Errant name mentions: Two name mentions referring to two different entities are concatenated into an errant name mention (e.g. “Bush Blair”, “Ahmadinejad Bush”). These types of errors stem from phrases such as “The meeting of Bush Blair” and generally due to lack of sufficient punctuation marks. 2. NE misrecognitions: Regular words are recognized as person name mentions and are embedded into person entities (e.g. Bush = George Bush = said). 3. Errant entity tracking: name mentions of different entities are recognized as different mentions of the same entity (e.g. Bush = Clinton = Ahmadinejad). 4. Lack of nominal mentions: Many entities do not contain any nominal mentions, which increases the entity ambiguity (especially when there is only one name mention composed of a single token). To overcome these problems, entities were preprocessed as follows: 1. Errant name mentions such as “Bush Blair” were automatically removed. In this step, a dictionary of person name mentions was built from the 2,931 documents collection from which the entities were recognized and normalized along with the frequency of appearance in the collection. For each entity, all its name mentions are checked in the dictionary and their frequencies are compared to each other. Any name mention with a frequency less than 1/30 of the frequency of the name mention with the highest frequency is automatically removed (1/30 was picked based on manual examination of the training set).  27  2. Name mentions formed of a single token consisting of less than 3 characters are removed. Such names are almost always misrecognized name entities. 3. Name entities with 10 or more different name mentions are automatically removed. The NERT system often produces entities that include many different name mentions referring to different persons as one. Such entities are errant because they over normalize name mentions. Persons are referred to using a limited number of name mentions. 4. Nominal mentions are stemmed using a context sensitive Arabic stemmer (Lee et al. 2003) to overcome the morphological complexity of Arabic. For example, “ ‫= ”ر‬ “president”, “ ‫“ = ”ا‬the president”, “ ‫“ = ”وا‬and the president”, “ ‫“ = ”ر‬its presidents” … etc are stemmed to “ ‫= ”ر‬ “president”.  Cross-document entities are compared in a  pairwise manner and binary decision is taken on  whether they are the same. Therefore, the  available 7,184 entities lead to nearly 26 million  pairwise comparisons (For N entities, the number  of  pair  wise  comparisons  =  N  (  N 2  −  1)  ).  Entity pairs were chosen to be included in the  training set if they match any of the following  criteria:  1. Both entities have one shared name mention.  2. Both entities have shared nominal mentions.  3. A name mention in one of the entities is a  substring of a name mention in the other  entity.  4. Both entities have nearly identical name  mentions (small edit distance between both  mentions).  The resulting set was composed of 19,825  pairs, which were manually judged to determine if  they should be normalized or not. These criteria  skew the selection of pairs towards more  ambiguous cases, which would be better  candidates to train the intended SVM classifier,  where the items near the boundary dividing the  hyperplane are the most important. For the  training set, 18,503 pairs were normalized, and  1,322 pairs were judged as different.  Unfortunately, the training set selection criteria  skewed the distribution of training examples heavily in favor of positive examples. It would interesting to examine other training sets where the distribution of positives and negatives is balanced or skewed in favor of negatives. The test set was composed of 470 entities that were manually normalized into 253 classes, of which 304 entities were normalized to 87 classes and 166 entities remained unnormalized (forming single-entity classes). Using 470 entities leads to 110,215 pairwise comparisons. The test set, which was distinct from the training set, was chosen using the same criteria as the training set. Further, all duplicate (identical) entities were removed from the test set. The selection criteria insure that the test set is skewed more towards ambiguous cases. Randomly choosing entities would have made the normalization too easy.  4. Normalization Methodology  SVMLight, an SVM classifier (Joachims, 2002),  was used for classification with a linear kernel and  default parameters. The following training  features were employed:  1. The percentage of shared name mentions between two entities calculated as:  Name Commonality =  ∑ ∑ ∑ <common names> min  f1i , f1 j  f 2i f2 j    where f1i is the frequency of the shared name mention in first entity, and f2i is the frequency of the shared name mention in the second entity. ∑ f1i is the number of name mentions appearing in the entity. 2. The maximum number of tokens in the shared name mentions, i.e. if there exists more than one shared name mention then this feature is the number of tokens in the longest shared name mention. 3. The percentage of shared nominal mentions between two entities, and it is calculated as the name commonality but for nominal mentions. 4. The smallest minimum edit distance (Levenshtein distance with uniform weights) between any two name mentions in both entities (Cohen et al., 2003) and this feature is only enabled when name commonality between both entities equals to zero.  28  5. Phonetic edit distance, which is similar to edit distance except that phonetically similar characters, namely {(‫ – ت‬t, ‫ – ط‬T), (‫ – ك‬k, ‫– ق‬ q),(‫ – د‬d, ‫ – ض‬D),(‫ – ث‬v, ‫ – س‬s, ‫ – ص‬S), (‫* – ذ‬, ‫ – ز‬z, ‫ – ظ‬Z),(‫ – ج‬j, ‫ – غ‬g),( ‫ – ـ‬p, ‫ – ـ‬h),(‫< – إ‬, – |, ‫> – أ‬, ‫ – ا‬A)1}, are normalized, vowels are removed, and spaces between tokens are removed. 6. The number of tokens in the pair of name mentions that lead to the minimum edit distance. Some of the features might seem duplicative. However, the edit distance and phonetic edit distance are often necessary when names are transliterated into Arabic and hence may have different spellings and consequently no shared name mentions. Conversely, given a shared name mention between a pair of entities will lead to zero edit distance, but the name commonality may also be very low indicating two different persons may have a shared name mention. For example “Abdullah the second” and “Abdullah bin Hussein” have the shared name mention “Abdullah” that leads to zero edit distance, but they are in fact two different persons. In this case, the name commonality feature can be indicative of the difference. Further, nominals are important in differentiating between identical name mentions that in fact refer to different persons (Fleischman and Hovy, 2004). The number of tokens feature indicates the importance of the presence of similarity between two name mentions, as the similarity between name mentions formed of one token cannot be indicative for similarity when the number of tokens is more than one. Further, it is assumed that entities are transitive and are not available all at once, but rather the system has to normalize entities incrementally as they appear. Therefore, for a given set of entity pairs, if the classifier deems that Entityi = Entityj and Entityj = Entityk, then Entityi is set to equal Entityk even if the classifier indicates that Entityi ≠ Entityk, and all entities (i, j, and k) are merged into one class. 
Amharic is the Semitic language that has the second large number of speakers after Arabic (Hayward and Richard 1999). Its writing system is syllabic with Consonant-Vowel (CV) syllable structure. Amharic orthography has more or less a one to one correspondence with syllabic sounds. We have used this feature of Amharic to develop a CV syllable-based speech recognizer, using Hidden Markov Modeling (HMM), and achieved 90.43% word recognition accuracy. 
We describe the adaptation for Arabic of the grammar-based MedSLT medical speech system. The system supports simple medical diagnosis questions about headaches using vocabulary of 322 words. We show that the MedSLT architecture based on motivated general grammars produces very good results, with a limited effort. Based on the grammars for other languages covered by the system, it is in fact very easy to develop an Arabic grammar and to specialize it efficiently for the different system tasks. In this paper, we focus on generation. 
Transliteration of a word into another language often leads to multiple spellings. Unless an information retrieval system recognises different forms of transliterated words, a signiﬁcant number of documents will be missed when users specify only one spelling variant. Using two different datasets, we evaluate several approaches to ﬁnding variants of foreign words in Arabic, and show that the longest common subsequence (LCS) technique is the best overall. 
Computational linguistics methods are typically ﬁrst developed and tested in English. When applied to other languages, assumptions from English data are often applied to the target language. One of the most common such assumptions is that a “standard” part-of-speech (POS) tagset can be used across languages with only slight variations. We discuss in this paper a speciﬁc issue related to the deﬁnition of a POS tagset for Modern Hebrew, as an example to clarify the method through which such variations can be deﬁned. It is widely assumed that Hebrew has no syntactic category of modals. There is, however, an identiﬁed class of words which are modal-like in their semantics, and can be characterized through distinct syntactic and morphologic criteria. We have found wide disagreement among traditional dictionaries on the POS tag attributed to such words. We describe three main approaches when deciding how to tag such words in Hebrew. We illustrate the impact of selecting each of these approaches on agreement among human taggers, and on the accuracy of automatic POS taggers induced for each method. We ﬁnally recommend the use of a “modal” tag in Hebrew and provide detailed guidelines for this tag. Our overall conclusion is that tagset deﬁnition is a complex task which deserves appropriate methodology.  
Tokenization is a necessary and non-trivial step in natural language processing. In the case of Arabic, where a single word can comprise up to four independent tokens, morphological knowledge needs to be incorporated into the tokenizer. In this paper we describe a rule-based tokenizer that handles tokenization as a full-rounded process with a preprocessing stage (white space normalizer), and a post-processing stage (token filter). We also show how it handles multiword expressions, and how ambiguity is resolved. 
Sentence alignment consists in estimating which sentence or sentences in the source language correspond with which sentence or sentences in a target language. We present in this paper a new approach to aligning sentences from a parallel corpus based on a cross-language information retrieval system. This approach consists in building a database of sentences of the target text and considering each sentence of the source text as a "query" to that database. The cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents to be indexed. This system is composed of a multilingual linguistic analyzer, a statistical analyzer, a reformulator, a comparator and a search engine. The multilingual linguistic analyzer includes a morphological analyzer, a partof-speech tagger and a syntactic analyzer. The linguistic analyzer processes both documents to be indexed and queries to produce a set of normalized lemmas, a set of named entities and a set of nominal compounds with their morpho-syntactic tags. The statistical analyzer computes for documents to be indexed concept weights based on concept database frequencies. The comparator computes intersections between queries and documents and provides a relevance weight for each intersection. Before this comparison, the reformulator expands  queries during the search. The expansion is used to infer from the original query words other words expressing the same concepts. The search engine retrieves the ranked, relevant documents from the indexes according to the corresponding reformulated query and then merges the results obtained for each language, taking into account the original words of the query and their weights in order to score the documents. The sentence aligner has been evaluated on the MD corpus of the ARCADE II project which is composed of news articles from the French newspaper "Le Monde Diplomatique". The part of the corpus used in evaluation consists of the same subset of sentences in Arabic and French. Arabic sentences are aligned to their French counterparts. Results showed that alignment has correct precision and recall even when the corpus is not completely parallel (changes in sentence order or missing sentences). 
We describe a Slot Grammar (SG) parser for Arabic, ASG, and new features of SG designed to accommodate Arabic as well as the European languages for which SGs have been built. We focus on the integration of BAMA with ASG, and on a new, expressive SG grammar formalism, SGF, and illustrate how SGF is used to advantage in ASG. 
Base Phrase Chunking (BPC) or shallow syntactic parsing is proving to be a task of interest to many natural language processing applications. In this paper, A BPC system is introduced that improves over state of the art performance in BPC using a new part of speech tag (POS) set. The new POS tag set, ERTS, reﬂects some of the morphological features speciﬁc to Modern Standard Arabic. ERTS explicitly encodes deﬁniteness, number and gender information increasing the number of tags from 25 in the standard LDC reduced tag set to 75 tags. For the BPC task, we introduce a more language speciﬁc set of deﬁnitions for the base phrase annotations. We employ a support vector machine approach for both the POS tagging and the BPC processes. The POS tagging performance using this enriched tag set, ERTS, is at 96.13% accuracy. In the BPC experiments, we vary the feature set along two factors: the POS tag set and a set of explicitly encoded morphological features. Using the ERTS POS tagset, BPC achieves the highest overall Fβ=1 of 96.33% on 10 different chunk types outperforming the use of the standard POS tag set even when explicit morphological features are present. 
We propose an enhanced Part-of-Speech (POS) tagger of Semitic languages that treats Modern Standard Arabic (henceforth Arabic) and Modern Hebrew (henceforth Hebrew) using the same probabilistic model and architectural setting. We start out by porting an existing Hidden Markov Model POS tagger for Hebrew to Arabic by exchanging a morphological analyzer for Hebrew with Buckwalter's (2002) morphological analyzer for Arabic. This gives state-of-theart accuracy (96.12%), comparable to Habash and Rambow’s (2005) analyzerbased POS tagger on the same Arabic datasets. However, further improvement of such analyzer-based tagging methods is hindered by the incomplete coverage of standard morphological analyzer (Bar Haim et al., 2005). To overcome this coverage problem we supplement the output of Buckwalter's analyzer with synthetically constructed analyses that are proposed by a model which uses character information (Diab et al., 2004) in a way that is similar to Nakagawa's (2004) system for Chinese and Japanese. A version of this extended model that (unlike Nakagawa) incorporates synthetically constructed analyses also for known words achieves 96.28% accuracy on the standard Arabic test set.  
Stemming is an important analysis step in a number of areas such as natural language processing (NLP), information retrieval (IR), machine translation(MT) and text classiﬁcation. In this paper we present the development of a stemmer for Amharic that reduces words to their citation forms. Amharic is a Semitic language with rich and complex morphology. The application of such a stemmer is in dictionary based cross language IR, where there is a need in the translation step, to look up terms in a machine readable dictionary (MRD). We apply a rule based approach supplemented by occurrence statistics of words in a MRD and in a 3.1M words news corpus. The main purpose of the statistical supplements is to resolve ambiguity between alternative segmentations. The stemmer is evaluated on Amharic text from two domains, news articles and a classic ﬁction text. It is shown to have an accuracy of 60% for the old fashioned ﬁction text and 75% for the news articles. 
Today's statistical machine translation systems generalize poorly to new domains. Even small shifts can cause precipitous drops in translation quality. Phrasal systems rely heavily, for both reordering and contextual translation, on long phrases that simply fail to match outof-domain text. Hierarchical systems attempt to generalize these phrases but their learned rules are subject to severe constraints. Syntactic systems can learn lexicalized and unlexicalized rules, but the joint modeling of lexical choice and reordering can narrow the applicability of learned rules. The treelet approach models reordering separately from lexical choice, using a discriminatively trained order model, which allows treelets to apply broadly, and has shown better generalization to new domains, but suffers a factorially large search space. We introduce a new reordering model based on dependency order templates, and show that it outperforms both phrasal and treelet systems on in-domain and out-of-domain text, while limiting the search space. 
 there have been a few syntax-based models that  Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in ﬂat structured phrase-based models is largely due to better local reorderings.  show performance comparable to the phrase-based models (Chiang, 2005; Marcu et al., 2006). However, reliably learning powerful rules from parallel data is very difﬁcult and prone to problems with sparsity and noise in the data. These models also suffer from a large search space when decoding with an integrated language model, which can lead to search errors (Chiang, 2005). In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the strengths of both the phrase-based models and syntactic structures. This is done using CCG supertags, which provide a rich source of syntactic information. CCG contains most of the structure of  
We provide an in-depth analysis of the integration of an Arabic-to-English transliteration system into a general-purpose phrase-based statistical machine translation system. We study the integration from different aspects and evaluate the improvement that can be attributed to the integration using the BLEU metric. Our experiments show that a transliteration module can help significantly in the situation where the test data is rich with previously unseen named entities. We obtain 70% and 53% of the theoretical maximum improvement we could achieve, as measured by an oracle on development and test sets respectively for OOV words (out of vocabulary source words not appearing in the phrase table). 
 We investigate different representational granularities for sub-lexical representation in statistical machine translation work from English to Turkish. We ﬁnd that (i) representing both Turkish and English at the morpheme-level but with some selective morpheme-grouping on the Turkish side of the training data, (ii) augmenting the training data with “sentences” comprising only the content words of the original training data to bias root word alignment, (iii) reranking the n-best morpheme-sequence outputs of the decoder with a word-based language model, and (iv) using model iteration all provide a non-trivial improvement over a fully word-based baseline. Despite our very limited training data, we improve from 20.22 BLEU points for our simplest model to 25.08 BLEU points for an improvement of 4.86 points or 24% relative. 
Current statistical machine translation systems handle the translation process as the transformation of a string of symbols into another string of symbols. Normally the symbols dealt with are the words in different languages, sometimes with some additional information included, like morphological data. In this work we try to push the approach to the limit, working not on the level of words, but treating both the source and target sentences as a string of letters. We try to ﬁnd out if a nearly unmodiﬁed state-of-the-art translation system is able to cope with the problem and whether it is capable to further generalize translation rules, for example at the level of word sufﬁxes and translation of unseen words. Experiments are carried out for the translation of Catalan to Spanish. 
This paper describes a novel model using dependency structures on the source side for syntax-based statistical machine translation: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependency structures to target strings. In this model translation pairs of source treelets and target strings with their word alignments are learned automatically from the parsed and aligned corpus. The DTSC model allows source treelets and target strings with variables so that the model can generalize to handle dependency structures with the same head word but with different modiﬁers and arguments. Additionally, target strings can be also discontinuous by using gaps which are corresponding to the uncovered nodes which are not included in the source treelets. A chart-style decoding algorithm with two basic operations– substituting and attaching–is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation. We ﬁnally evaluate our current implementation of a simpliﬁed version of DTSC for statistical machine translation. 
Evaluation and error analysis of machine translation output are important but difﬁcult tasks. In this work, we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of Word Error Rate (WER) and Position independent word Error Rate (PER) over different Partof-Speech (POS) classes. Furthermore, we investigate two possible aspects of the use of these decompositions for automatic error analysis: estimation of inﬂectional errors and distribution of missing words over POS classes. The obtained results are shown to correspond to the results of a human error analysis. The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system. 
In order to simultaneously translate speech into multiple languages an extension of stochastic ﬁnite-state transducers is proposed. In this approach the speech translation model consists of a single network where acoustic models (in the input) and the multilingual model (in the output) are embedded. The multi-target model has been evaluated in a practical situation, and the results have been compared with those obtained using several mono-target models. Experimental results show that the multi-target one requires less amount of memory. In addition, a single decoding is enough to get the speech translated into multiple languages. 
We propose a novel syntax-based model for statistical machine translation in which meta-structure (MS) and meta-structure sequence (SMS) of a parse tree are defined. In this framework, a parse tree is decomposed into SMS to deal with the structure divergence and the alignment can be reconstructed at different levels of recombination of MS (RM). RM pairs extracted can perform the mapping between the substructures across languages. As a result, we have got not only the translation for the target language, but an SMS of its parse tree at the same time. Experiments with BLEU metric show that the model significantly outperforms Pharaoh, a state-art-theart phrase-based system. 
Modern statistical machine translation systems may be seen as using two components: feature extraction, that summarizes information about the translation, and a log-linear framework to combine features. In this paper, we propose to relax the linearity constraints on the combination, and hence relaxing constraints of monotonicity and independence of feature functions. We expand features into a non-parametric, non-linear, and high-dimensional space. We extend empirical Bayes reward training of model parameters to meta parameters of feature generation. In effect, this allows us to trade away some human expert feature design for data. Preliminary results on a standard task show an encouraging improvement. 
In this paper, we present a Bayesian Learning based method to train word dependent transition models for HMM based word alignment. We present word alignment results on the Canadian Hansards corpus as compared to the conventional HMM and IBM model 4. We show that this method gives consistent and significant alignment error rate (AER) reduction. We also conducted machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 
Statistical machine translation, as well as other areas of human language processing, have recently pushed toward the use of large scale n-gram language models. This paper presents efﬁcient algorithmic and architectural solutions which have been tested within the Moses decoder, an open source toolkit for statistical machine translation. Experiments are reported with a high performing baseline, trained on the Chinese-English NIST 2006 Evaluation task and running on a standard Linux 64-bit PC architecture. Comparative tests show that our representation halves the memory required by SRI LM Toolkit, at the cost of 44% slower translation speed. However, as it can take advantage of memory mapping on disk, the proposed implementation seems to scale-up much better to very large language models: decoding with a 289-million 5-gram language model runs in 2.1Gb of RAM. 
We introduce a novel evaluation scheme for the human evaluation of different machine translation systems. Our method is based on direct comparison of two sentences at a time by human judges. These binary judgments are then used to decide between all possible rankings of the systems. The advantages of this new method are the lower dependency on extensive evaluation guidelines, and a tighter focus on a typical evaluation task, namely the ranking of systems. Furthermore we argue that machine translation evaluations should be regarded as statistical processes, both for human and automatic evaluation. We show how conﬁdence ranges for state-of-the-art evaluation measures such as WER and TER can be computed accurately and efﬁciently without having to resort to Monte Carlo estimates. We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign. 
We present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser. Our dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. 
Attempts to estimate phrase translation probablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by Koehn, et al. (2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.’s model. Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time. 
 Most state-of-the-art statistical machine translation systems use log-linear models, which are deﬁned in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a signiﬁcant decrease in translation quality.  
We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system. 
This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies. 
In this work we revise the application of discriminative learning to the problem of phrase selection in Statistical Machine Translation. Inspired by common techniques used in Word Sense Disambiguation, we train classiﬁers based on local context to predict possible phrase translations. Our work extends that of Vickrey et al. (2005) in two main aspects. First, we move from word translation to phrase translation. Second, we move from the ‘blank-ﬁlling’ task to the ‘full translation’ task. We report results on a set of highly frequent source phrases, obtaining a signiﬁcant improvement, specially with respect to adequacy, according to a rigorous process of manual evaluation. 
This paper describes the 2007 Ngram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the previous years system, being highlighted and empirically compared. Mainly, these include a novel word ordering strategy based on: (1) statistically monotonizing the training source corpus and (2) a novel reordering approach based on weighted reordering graphs. In addition, this system introduces a target language model based on statistical classes, a feature for out-of-domain units and an improved optimization procedure. The paper provides details of this system participation in the ACL 2007 SECOND WORKSHOP ON STATISTICAL MACHINE TRANSLATION. Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 
One main challenge of statistical machine translation (SMT) is dealing with word order. The main idea of the statistical machine reordering (SMR) approach is to use the powerful techniques of SMT systems to generate a weighted reordering graph for SMT systems. This technique supplies reordering constraints to an SMT system, using statistical criteria. In this paper, we experiment with different graph pruning which guarantees the translation quality improvement due to reordering at a very low increase of computational cost. The SMR approach is capable of generalizing reorderings, which have been learned during training, by using word classes instead of words themselves. We experiment with statistical and morphological classes in order to choose those which capture the most probable reorderings. Satisfactory results are reported in the WMT07 Es/En task. Our system outperforms in terms of BLEU the WMT07 Ofﬁcial baseline system. 
Mixture modelling is a standard technique for density estimation, but its use in statistical machine translation (SMT) has just started to be explored. One of the main advantages of this technique is its capability to learn speciﬁc probability distributions that better ﬁt subsets of the training dataset. This feature is even more important in SMT given the difﬁculties to translate polysemic terms whose semantic depends on the context in which that term appears. In this paper, we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a state-of-the-art phrase-based system. Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling. 
We describe an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup. We use a variant of standard SMT technology to align translations from one or more RBMT systems with the source text. We incorporate phrases extracted from these alignments into the phrase table of the SMT system and use the open-source decoder Moses to ﬁnd good combinations of phrases from SMT training data with the phrases derived from RBMT. First experiments based on this hybrid architecture achieve promising results. 
This paper describes experiments with English-to-Czech phrase-based machine translation. Additional annotation of input and output tokens (multiple factors) is used to explicitly model morphology. We vary the translation scenario (the setup of multiple factors) and the amount of information in the morphological tags. Experimental results demonstrate signiﬁcant improvement of translation quality in terms of BLEU. 
The paper proposes formulating MT evaluation as a ranking problem, as is often done in the practice of assessment by human. Under the ranking scenario, the study also investigates the relative utility of several features. The results show greater correlation with human assessment at the sentence level, even when using an n-gram match score as a baseline feature. The feature contributing the most to the rank order correlation between automatic ranking and human assessment was the dependency structure relation rather than BLEU score and reference language model feature. 
This paper studies the impact that difficult-totranslate source-language phrases might have on the machine translation process. We formulate the notion of difficulty as a measurable quantity; we show that a classifier can be trained to predict whether a phrase might be difficult to translate; and we develop a framework that makes use of the classifier and external resources (such as human translators) to improve the overall translation quality. Through experimental work, we verify that by isolating difficult-to-translate phrases and processing them as special cases, their negative impact on the translation of the rest of the sentences can be reduced. 
Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon. The reason is that, while MT quality aspects are diverse, BLEU limits its scope to the lexical dimension. In this work, we suggest using metrics which take into account linguistic features at more abstract levels. We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature. 
Unsupervised Data-Oriented Parsing models (U-DOP) represent a class of structure bootstrapping models that have achieved some of the best unsupervised parsing results in the literature. While U-DOP was originally proposed as an engineering approach to language learning (Bod 2005, 2006a), it turns out that the model has a number of properties that may also be of linguistic and cognitive interest. In this paper we will focus on the original U-DOP model proposed in Bod (2005) which computes the most probable tree from among the shortest derivations of sentences. We will show that this U-DOP model can learn both rule-based and exemplar-based aspects of language, ranging from agreement and movement phenomena to discontiguous contructions, provided that productive units of arbitrary size are allowed. We argue that our results suggest a rapprochement between nativism and empiricism. 
 even when accounting for differences in content and  We apply machine learning techniques to study language transfer, a major topic in the theory of Second Language Acquisition (SLA). Using an SVM for the problem of native language classiﬁcation, we show that a careful analysis of the effects of various features can lead to scientiﬁc insights. In particular, we demonstrate that character bigrams alone allow classiﬁcation levels of about 66% for a 5-class task, even when content and function word differences are accounted for. This may show that native language has a strong effect on the word choice of people writing in a second language.  function words. This result leads us to form a novel hypothesis on the role of language transfer in SLA: that the choice of words people make when writing in a second language is strongly inﬂuenced by the phonology of their native language. As far as we know, this is the ﬁrst time that such a hypothesis has beed formulated. Moreover, this is the ﬁrst statistical learning-supported hypothesis in language transfer. Our results should be further substantiated by additional psycholinguistic and computational experiments; nonetheless, we provide a strong starting point. The next section provides some essential background. In Section 3 we describe our experimen-  
This paper discusses a new, open-source software program, called Phon, that is designed for the transcription, coding, and analysis of phonological corpora. Phon provides support for multimedia data linkage, segmentation, multiple-blind transcription, transcription validation, syllabification, alignment of target and actual forms, and data analysis. All of these functions are available through a user-friendly graphical interface. Phon, available on most computer platforms, supports data exchange among researchers with the TalkBank XML document format and the Unicode character set.. This program provides the basis for the elaboration of PhonBank, a database project that seeks to broaden the scope of CHILDES into phonological development and disorders. 
 viding a layer of morphological information. In par-  Corpora of child language are essential for psycholinguistic research. Linguistic annotation of the corpora provides researchers with better means for exploring the development of grammatical constructions and their usage. We describe an ongoing project that aims to annotate the English section of the CHILDES database with grammatical relations in the form of labeled dependency structures. To date, we have produced a corpus of over 65,000 words with manually curated gold-standard grammatical relation annotations. Using this corpus, we have developed a highly accurate data-driven parser for English CHILDES data. The parser and the manually annotated data are freely available for research purposes.  ticular, the English section of the database is augmented by part of speech (POS) tags for each word. However, this information is usually insufﬁcient for investigations dealing with the syntactic, semantic or pragmatic aspects of the data. In this paper we describe an ongoing effort aiming to annotate the English portion of the CHILDES database with syntactic information based on grammatical relations represented as labeled dependency structures. Although an annotation scheme for syntactic information in CHILDES data has been proposed (Sagae et al., 2004), until now no signiﬁcant amount of annotated data had been made publicly available. In the process of manually annotating several thousands of words, we updated the annotation scheme, mostly by extending it to cover syntactic phenomena that occur in real data but were unaccounted for in the original annotation scheme.  
 tween children and parents over 25 human languages.  Empirical data regarding the syntactic complexity of children’s speech is important for theories of language acquisition. Currently much of this data is absent in the annotated versions of the CHILDES database. In this perliminary study, we show that a state-ofthe-art subcategorization acquisition system of Preiss et al. (2007) can be used to extract largescale subcategorization (frequency) information from the (i) child and (ii) child-directed speech within the CHILDES database without any domain-speciﬁc tuning. We demonstrate that the acquired information is sufﬁciently accurate to conﬁrm and extend previously reported research ﬁndings. We also report qualitative results which can be used to further improve parsing and lexical acquisition technology for child language data in the future.  CHILDES is currently available in raw, part-of-speechtagged and lemmatized formats. However, adequate investigation of syntactic complexity requires deeper annotations related to e.g. syntactic parses, subcategorization frames (SCFs), lexical classes and predicateargument structures. Although manual syntactic annotation is possible, it is extremely costly. The alternative is to use natural language processing (NLP) techniques for annotation. Automatic techniques are now viable, cost effective and, although not completely error-free, are sufﬁciently accurate to yield annotations useful for linguistic purposes. They also gather important qualitative and quantitative information, which is difﬁcult for humans to obtain, as a side-effect of the acquisition process. For instance, state-of-the-art statistical parsers, e.g. (Charniak, 2000; Briscoe et al., 2006), have wide coverage and yield grammatical representations capa-  
 over all the classes that can occur in that position.  We present a cognitive model of inducing verb selectional preferences from individual verb usages. The selectional preferences for each verb argument are represented as a probability distribution over the set of semantic properties that the argument can possess—a semantic proﬁle. The semantic proﬁles yield verb-speciﬁc conceptualizations of the arguments associated with a syntactic position. The proposed model can learn appropriate verb proﬁles from a small set of noisy training data, and can use them in simulating human plausibility judgments and analyzing implicit object alternation.  Resnik’s model was proposed as a model of human learning of selectional preferences that made minimal representational assumptions; it showed how such preferences could be acquired from usage data and an existing conceptual hierarchy. However, his and later computational models (see Section 2) have properties that do not match with certain cognitive plausibility criteria for a child language acquisition model. All these models use the training data in “batch mode”, and most of them use information theoretic measures that rely on total counts from a corpus. Therefore, it is not clear how the representation of selectional preferences could be updated incrementally in these models as the person receives more data. Moreover, the assumption that children  
 the whole input corpus, and then build a model at  We introduce Incremental Semantic Analysis, a fully incremental word space model, and we test it on longitudinal child-directed speech data. On this task, ISA outperforms the related Random Indexing algorithm, as well as a SVD-based technique. In addition, the model has interesting properties that might also be characteristic of the semantic space of children.  once via matrix algebra operations. Admittedly, this is hardly a plausible simulation of how children learn word meanings incrementally by being exposed to short sentences containing a relatively small number of different words. The lack of incrementality of several models appears conspicuous especially given their explicit claim to solve old theoretical issues about the acquisition of language (e.g., (Landauer and Dumais, 1997)). Other extant models display some degree if incrementality. For instance,  
 The word-learning mechanisms used at this early  Naming requires recognition. Recognition requires the ability to categorize objects and events. Infants under six months of age are capable of making ﬁne-grained discriminations of object boundaries and three-dimensional space. At 8 to 10 months, a child’s object categories are sufﬁciently stable and ﬂexible to be used as the foundation for labeling and referencing actions. What mechanisms in the brain underlie the unfolding of these capacities? In this article, we describe a neural network model which attempts to simulate, in a biologically plausible way, the process by which infants learn how to recognize objects and words through exposure to visual stimuli and vocal sounds. 
A testing procedure is proposed to reevaluate the syntactic burst in children over age two. The experimentation is based on the children’s capacities in perception, memory, association and cognition, and does not presuppose any specific innate grammatical capacities. The procedure is tested using the large Manchester corpus in the CHILDES database. The results showed that young children grammatical capabilities (before age three) could be the results of simple mechanisms and that complex linguistic mastery does not need to be available so early in the course of language development. 
 tic space, not its generation or properties that exist  at a whole-network level.  Semantic networks have been used suc-  Topological analyses, looking at the statistical  cessfully to explain access to the men-  regularities of whole semantic networks, can be  tal lexicon. Topological analyses of these  used to model phenomena not easily explained from  networks have focused on acquisition and  the smaller scale data found in human experiments.  generation. We extend this work to look  These networks are typically formed from corpora,  at models that distinguish semantic rela-  expert compiled lexical resources, or human word-  tions. We ﬁnd the scale-free properties  association data.  of association networks are not found in synonymy-homonymy networks, and that this is consistent with studies of childhood acquisition of these relationships. We further ﬁnd that distributional models of language acquisition display similar topological properties to these networks.  Existing work has focused language acquisition (Steyvers and Tenenbaum, 2005) and generation (Cancho and Sole´, 2001). These models use the general notion of semantic association which subsumes all speciﬁc semantic relations, e.g. synonymy. There is evidence that there are distinct cognitive processes for different semantic relations (e.g.  Casenhiser, 2005). We perform a graph analysis  
 restricting GEN, creating an alternative closer to  We compare three recent proposals adding a topology to OT: McCarthy’s Persistent OT, Smolensky’s ICS and B´ıro´’s SA-OT. To test their learnability, constraint rankings are learnt from SA-OT’s output. The errors in the output, being more than mere noise, follow from the topology. Thus, the learner has to reconstructs her competence having access only to the teacher’s performance.  standard derivations. Based the iterative syllabiﬁcation in Imdlawn Tashlhiyt Berber, they suggest: “some general procedure (Do-α) is allowed to make a certain single modiﬁcation to the input, producing the candidate set of all possible outcomes of such modiﬁcation.” The outputs of Do-α are “neighbours” of its input, so Do-α deﬁnes a topology. Subsequently, EVAL ﬁnds the most harmonic element of this restricted candidate set, which then serves again as the input of Do-α. Repeating this  
 from the fact that the semantics of noun-noun com-  The ability to correctly interpret and produce noun-noun compounds such as WIND FARM or CARBON TAX is an important part of the acquisition of language in various domains of discourse. One approach to the interpretation of noun-noun compounds assumes that people make use of distributional information about how the constituent words of compounds tend to combine; another assumes that people make use of information about the two constituent concepts’ features to produce interpretations. We present an experiment that examines how people acquire both the distributional information and conceptual information relevant to compound interpretation. A plausible model of the interpretation process is also presented. 
In this paper, we describe a sourceside reordering method based on syntactic chunks for phrase-based statistical machine translation. First, we shallow parse the source language sentences. Then, reordering rules are automatically learned from source-side chunks and word alignments. During translation, the rules are used to generate a reordering lattice for each sentence. Experimental results are reported for a Chinese-to-English task, showing an improvement of 0.5%–1.8% BLEU score absolute on various test sets and better computational efﬁciency than reordering during decoding. The experiments also show that the reordering at the chunk-level performs better than at the POS-level. 
We present a proposal for the structure of noun phrases in Synchronous TreeAdjoining Grammar (STAG) syntax and semantics that permits an elegant and uniform analysis of a variety of phenomena, including quantiﬁer scope and extraction phenomena such as wh-questions with both moved and in-place wh-words, pied-piping, stranding of prepositions, and topicalization. The tight coupling between syntax and semantics enforced by the STAG helps to illuminate the critical relationships and ﬁlter out analyses that may be appealing for either syntax or semantics alone but do not allow for a meaningful relationship between them. 
We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models. This syntactic model is similar to its ﬂatstring phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training. We demonstrate that the consistency constraints that allow ﬂat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm. We also show that the phrasal translation tables produced by the ITG are superior to those of the ﬂat joint phrasal model, producing up to a 2.5 point improvement in BLEU score. Finally, we explore, for the ﬁrst time, the utility of a joint phrasal translation model as a word alignment method. 
Factoring a Synchronous Context-Free Grammar into an equivalent grammar with a smaller number of nonterminals in each rule enables synchronous parsing algorithms of lower complexity. The problem can be formalized as searching for the tree-decomposition of a given permutation with the minimal branching factor. In this paper, by modifying the algorithm of Uno and Yagiura (2000) for the closely related problem of ﬁnding all common intervals of two permutations, we achieve a linear time algorithm for the permutation factorization problem. We also use the algorithm to analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 
Binarization is essential for achieving polynomial time complexities in parsing and syntax-based machine translation. This paper presents a new binarization scheme, target-side binarization, and compares it with source-side and synchronous binarizations on both stringbased and tree-based systems using synchronous grammars. In particular, we demonstrate the effectiveness of targetside binarization on a large-scale tree-tostring translation system. 
We present the main ideas behind a new syntax-based machine translation system, based on reducing the machine translation task to a tree-labeling task. This tree labeling is further reduced to a sequence of decisions (of four varieties), which can be discriminatively trained. The optimal tree labeling (i.e. translation) is then found through a simple depth-ﬁrst branch-andbound search. An early system founded on these ideas has been shown to be competitive with Pharaoh when both are trained on a small subsection of the Europarl corpus. 
Discriminative approaches for word alignment have gained popularity in recent years because of the ﬂexibility that they offer for using a large variety of features and combining information from various sources. But, the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure (the set of alignment links) and the syntactic divergence between sentences in the parallel text. This is primarily because of the limitation of their search techniques. In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively. These features are particularly useful for language pairs with high structural divergence (like English-Hindi, EnglishJapanese). We have shown that by using the structural features, we have obtained a decrease of 2.3% in the absolute value of alignment error rate (AER). When we add the cooccurence probabilities obtained from IBM model-4 to our features, we achieved the best AER (50.50) for the English-Hindi parallel corpus. 
In this paper we explore a generative model for recovering surface syntax and strings from deep-syntactic tree structures. Deep analysis has been proposed for a number of language and speech processing tasks, such as machine translation and paraphrasing of speech transcripts. In an effort to validate one such formalism of deep syntax, the Praguian Tectogrammatical Representation (TR), we present a model of synthesis for English which generates surface-syntactic trees as well as strings. We propose a generative model for function word insertion (prepositions, deﬁnite/indeﬁnite articles, etc.) and subphrase reordering. We show by way of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees. 
The purpose of this work is to explore the integration of morphosyntactic information into the translation model itself, by enriching words with their morphosyntactic categories. We investigate word disambiguation using morphosyntactic categories, n-best hypotheses reranking, and the combination of both methods with word or morphosyntactic n-gram language model reranking. Experiments are carried out on the English-to-Spanish translation task. Using the morphosyntactic language model alone does not results in any improvement in performance. However, combining morphosyntactic word disambiguation with a word based 4-gram language model results in a relative improvement in the BLEU score of 2.3% on the development set and 1.9% on the test set. 
This paper seeks to complement the current trend of adding more structure to Statistical Machine Translation systems, by exploring the opposite direction: adding statistical components to a Transfer-Based MT system. Initial results on the BTEC data show significant improvement according to three automatic evaluation metrics (BLEU, NIST and METEOR). 
We present a novel method for evaluating the output of Machine Translation (MT), based on comparing the dependency structures of the translation and reference rather than their surface string forms. Our method uses a treebank-based, widecoverage, probabilistic Lexical-Functional Grammar (LFG) parser to produce a set of structural dependencies for each translation-reference sentence pair, and then calculates the precision and recall for these dependencies. Our dependencybased evaluation, in contrast to most popular string-based evaluation metrics, will not unfairly penalize perfectly valid syntactic variations in the translation. In addition to allowing for legitimate syntactic differences, we use paraphrases in the evaluation process to account for lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. An experiment with two translations of 4,000 sentences from Spanish-English Europarl shows that, in contrast to most other metrics, our method does not display a high bias towards statistical models of translation. 
We provide a conceptual basis for thinking of machine translation in terms of synchronous grammars in general, and probabilistic synchronous tree-adjoining grammars in particular. Evidence for the view is found in the structure of bilingual dictionaries of the last several millennia. 
Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translation relies on (local) associations of target words/phrases with source words/phrases for lexical selection. In contrast, in this paper, we present a novel approach to lexical selection where the target words are associated with the entire source sentence (global) without the need for local associations. This technique is used by three models (Bag–of–words model, sequential model and hierarchical model) which predict the target language words given a source sentence and then order the words appropriately. We show that a hierarchical model performs best when compared to the other two models. 
This paper describes a new method to compare reordering constraints for Statistical Machine Translation. We investigate the best possible (oracle) BLEU score achievable under different reordering constraints. Using dynamic programming, we efﬁciently ﬁnd a reordering that approximates the highest attainable BLEU score given a reference and a set of reordering constraints. We present an empirical evaluation of popular reordering constraints: local constraints, the IBM constraints, and the Inversion Transduction Grammar (ITG) constraints. We present results for a German-English translation task and show that reordering under the ITG constraints can improve over the baseline by more than 7.5 BLEU points. 
This paper reports on progress applying partially observable Markov decision processes (POMDPs) to a commercial dialog domain: troubleshooting. In the troubleshooting domain, a spoken dialog system helps a user to ﬁx a product such as a failed DSL connection. Past work has argued that a POMDP is a principled approach to building spoken dialog systems in the simpler slot-ﬁlling domain; this paper explains how the POMDPs formulation can be extended to the more complex troubleshooting domain. Results from dialog simulation verify that a POMDP outperforms a handcrafted baseline. 
The multimodal presentation dashboard allows users to control and browse presentation content such as slides and diagrams through a multimodal interface that supports speech and pen input. In addition to control commands (e.g. “take me to slide 10”), the system allows multimodal search over content collections. For example, if the user says “get me a slide about internet telephony,” the system will present a ranked series of candidate slides that they can then select among using voice, pen, or a wireless remote. As presentations are loaded, their content is analyzed and language and understanding models are built dynamically. This approach frees the user from the constraints of linear order allowing for a more dynamic and responsive presentation style.  
The goal of this paper is to give a description of the state of the art, the issues, the problems, and the solutions related to industrial dialog systems for the automation of technical support. After a general description of the evolution of the spoken dialog industry, and the challenges in the development of technical support applications, we will discuss two specific problems through a series of experimental results. The first problem is the identification of the call reason, or symptom, from loosely constrained user utterances. The second is the use of data for the experimental optimization of the Voice User Interface (VUI). 
We introduce Olympus, a freely available framework for research in conversational interfaces. Olympus’ open, transparent, flexible, modular and scalable nature facilitates the development of large-scale, real-world systems, and enables research leading to technological and scientific advances in conversational spoken language interfaces. In this paper, we describe the overall architecture, several systems spanning different domains, and a number of current research efforts supported by Olympus. 
This paper describes our experiences of collecting a corpus of 42,000 dialogues for a call-routing application using a Wizard-of-Oz approach. Contrary to common practice in the industry, we did not use the kind of automated application that elicits some speech from the customers and then sends all of them to the same destination, such as the existing touch-tone menu, without paying attention to what they have said. Contrary to the traditional Wizard-of-Oz paradigm, our data-collection application was fully integrated within an existing service, replacing the existing touch-tone navigation system with a simulated callrouting system. Thus, the subjects were real customers calling about real tasks, and the wizards were service agents from our customer care. We provide a detailed exposition of the data collection as such and the application used, and compare our approach to methods previously used. 
Statistical classification techniques for natural-language call routing systems have matured to the point where it is possible to distinguish between several hundreds of semantic categories with an accuracy that is sufficient for commercial deployments. For category sets of this size, the problem of maintaining consistency among manually tagged utterances becomes limiting, as lack of consistency in the training data will degrade performance of the classifier. It is thus essential that the set of categories be structured in a way that alleviates this problem, and enables consistency to be preserved as the domain keeps changing. In this paper, we describe our experiences of using a twolevel multi-slot semantics as a way of meeting this problem. Furthermore, we explore the ramifications of the approach with respect to classification, evaluation and dialogue design for call routing systems. 
This paper presents a series of measurements of the accuracy of speech understanding when grammar-based or robust approaches are used. The robust approaches considered here are based on statistical language models (SLMs) with the interpretation being carried out by phrasespotting or robust parsing methods. We propose a simple process to leverage existing grammars and logged utterances to upgrade grammar-based applications to become more robust to out-of-coverage inputs. All experiments herein are run on data collected from deployed directed dialog applications and show that SLMbased techniques outperform grammarbased ones without requiring any change in the application logic. 
In this paper, we present the WIRE system for human intelligence reporting and discuss challenges of deploying spoken language understanding systems for the military, particularly for dismounted warfighters. Using the PARADISE evaluation paradigm, we show that performance models derived using standard metrics can account for 68% of the variance of User Satisfaction. We discuss the implication of these results and how the evaluation paradigm may be modified for the military domain. 
A chatbot is a software system, which can interact or “chat” with a human user in natural language such as English. For the annual Loebner Prize contest, rival chatbots have been assessed in terms of ability to fool a judge in a restricted chat session. We are investigating methods to train and adapt a chatbot to a specific user’s language use or application, via a usersupplied training corpus. We advocate open-ended trials by real users, such as an example Afrikaans chatbot for Afrikaansspeaking researchers and students in South Africa. This is evaluated in terms of “glass box” dialogue efficiency metrics, and “black box” dialogue quality metrics and user satisfaction feedback. The other examples presented in this paper are the Qur'an and the FAQchat prototypes. Our general conclusion is that evaluation should be adapted to the application and to user needs. 
 WCG  Article Graph  In this paper, we discuss two graphs in Wikipedia (i) the article graph, and (ii) the category graph. We perform a graphtheoretic analysis of the category graph, and show that it is a scale-free, small world graph like other well-known lexical semantic networks. We substantiate our ﬁndings by transferring semantic relatedness algorithms deﬁned on WordNet to the Wikipedia category graph. To assess the usefulness of the category graph as an NLP resource, we analyze its coverage and the performance of the transferred semantic relatedness algorithms. 
This paper introduces multi-level association graphs (MLAGs), a new graph-based framework for information retrieval (IR). The goal of that framework is twofold: First, it is meant to be a meta model of IR, i.e. it subsumes various IR models under one common representation. Second, it allows to model different forms of search, such as feedback, associative retrieval and browsing at the same time. It is shown how the new integrated model gives insights and stimulates new ideas for IR algorithms. One of these new ideas is presented and evaluated, yielding promising experimental results. 
In this article we address the usefulness of linguistic-independent methods in extractive Automatic Summarization, arguing that linguistic knowledge is not only useful, but may be necessary to improve the informativeness of automatic extracts. An assessment of four diverse AS methods on Brazilian Portuguese texts is presented to support our claim. One of them is Mihalcea’s TextRank; other two are modified versions of the former through the inclusion of varied linguistic features. Finally, the fourth method employs machine learning techniques, tackling more profound and language-dependent knowledge. 
Current graph-based approaches to automatic text summarization, such as LexRank and TextRank, assume a static graph which does not model how the input texts emerge. A suitable evolutionary text graph model may impart a better understanding of the texts and improve the summarization process. We propose a timestamped graph (TSG) model that is motivated by human writing and reading processes, and show how text units in this model emerge over time. In our model, the graphs used by LexRank and TextRank are specific instances of our timestamped graph with particular parameter settings. We apply timestamped graphs on the standard DUC multi-document text summarization task and achieve comparable results to the state of the art. 
We propose to use graph-based diffusion techniques with data-dependent kernels to build unigram language models. Our approach entails building graphs, where each vertex corresponds uniquely to a word from a closed vocabulary, and the existence of an edge (with an appropriate weight) between two words indicates some form of similarity between them. In one of our constructions, we place an edge between two words if the number of times these words were seen in a training set differs by at most one count. This graph construction results in a similarity matrix with small intrinsic dimension, since words with the same counts have the same neighbors. Experimental results from a benchmark task from language modeling show that our method is competitive with the Good-Turing estimator. 
We extend the Blum and Chawla (2001) graph min-cut algorithm to structured problems. This extension can alternatively be viewed as a joint inference method over a set of training and test instances where parts of the instances interact through a prespeciﬁed associative network. The method has has an eﬃcient approximation through a linear-programming relaxation. On small training data sets, the method achieves up to 34.8% relative error reduction. 
This paper presents latent semantic grammars for the unsupervised induction of English grammar. Latent semantic grammars were induced by applying singular value decomposition to n-gram by context-feature matrices. Parsing was used to evaluate performance. Experiments with context, projectivity, and prior distributions show the relative performance effects of these kinds of prior knowledge. Results show that prior distributions, projectivity, and part of speech information are not necessary to beat the right branching baseline. 
We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures, and illustrate how this allows one to view NLP tasks as graph transformations. We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identiﬁcation of non-local depenencies (using Penn Treebank data) and semantic role labeling (using Proposition Bank data). 
This paper describes the analysis of weak local coherence utterances during humancomputer conversation through the application of an emergent data mining technique, data crystallization. Results reveal that by adding utterances with weak local relevance the performance of a baseline conversational partner, in terms of user satisfaction, showed betterment. 
We study the correlations in the connectivity patterns of large scale syntactic dependency networks. These networks are induced from treebanks: their vertices denote word forms which occur as nuclei of dependency trees. Their edges connect pairs of vertices if at least two instance nuclei of these vertices are linked in the dependency structure of a sentence. We examine the syntactic dependency networks of seven languages. In all these cases, we consistently obtain three ﬁndings. Firstly, clustering, i.e., the probability that two vertices which are linked to a common vertex are linked on their part, is much higher than expected by chance. Secondly, the mean clustering of vertices decreases with their degree — this ﬁnding suggests the presence of a hierarchical  network organization. Thirdly, the mean degree of the nearest neighbors of a vertex x tends to decrease as the degree of x grows — this ﬁnding indicates disassortative mixing in the sense that links tend to connect vertices of dissimilar degrees. Our results indicate the existence of common patterns in the large scale organization of syntactic dependency networks. 
In this paper we attempt to deduce textual entailment based on syntactic dependency trees of a given text-hypothesis pair. The goals of this project are to provide an accurate and fast system, which we have called DLSITE-2, that can be applied in software systems that require a near-realtime interaction with the user. To accomplish this we use MINIPAR to parse the phrases and construct their corresponding trees. Later on we apply syntacticbased techniques to calculate the semantic similarity between text and hypothesis. To measure our method’s precision we used the test text corpus set from Second PASCAL Recognising Textual Entailment Challenge (RTE-2), obtaining an accuracy rate of 60.75%. 
The difﬁculties involved in spelling error detection and correction in a language have been investigated in this work through the conceptualization of SpellNet – the weighted network of words, where edges indicate orthographic proximity between two words. We construct SpellNets for three languages - Bengali, English and Hindi. Through appropriate mathematical analysis and/or intuitive justiﬁcation, we interpret the different topological metrics of SpellNet from the perspective of the issues related to spell-checking. We make many interesting observations, the most signiﬁcant among them being that the probability of making a real word error in a language is propotionate to the average weighted degree of SpellNet, which is found to be highest for Hindi, followed by Bengali and English. 
Degree distributions for word forms cooccurrences for large Russian text collections are obtained. Two power laws fit the distributions pretty good, thus supporting Dorogovtsev-Mendes model for Russian. Few different Russian text collections were studied, and statistical errors are shown to be negligible. The model exponents for Russian are found to differ from those for English, the difference probably being due to the difference in the collections structure. On the contrary, the estimated size of the supposed kernel lexicon appeared to be almost the same for the both languages, thus supporting the idea of importance of word forms for a perceptual lexicon of a human. 
Speakers and listeners make use of a variety of pragmatic factors to produce and identify sarcastic statements. It is also possible that lexical factors play a role, although this possibility has not been investigated previously. College students were asked to read excerpts from published works that originally contained the phrase said sarcastically, although the word sarcastically was deleted. The participants rated the characters’ statements in these excerpts as more likely to be sarcastic than those from similar excerpts that did not originally contain the word sarcastically. The use of interjections, such as gee or gosh, predicted a significant amount of the variance in the participants’ ratings of sarcastic intent. This outcome suggests that sarcastic statements may be more formulaic than previously realized. It also suggests that computer software could be written to recognize such lexical factors, greatly increasing the likelihood that nonliteral intent could be correctly interpreted by such programs, even if they are unable to identify the pragmatic components of nonliteral language. 
The paper presents a corpus-based method for ﬁnding metaphorically used lexemes and prevailing semantico-conceptual source domains, given a target domain corpus. It is exempliﬁed by a case study on the target domain of European politics, based on a French 800,000 token corpus.  are grouped into semantico-conceptual domains for which, in a ﬁnal step, additional lexical instantiations are searched (Section 5). Two important source domains (BUILDING and MOTION) are detected, which are supported by over 1,000 manual corpus annotations. The domains can be characterized as small networks of EuroWordNet synsets (nodes) and lexical as well as conceptual relations (Section 6). Section 7 concludes the paper.  
In this paper we propose algorithms to automatically classify sentences into metaphoric or normal usages. Our algorithms only need the WordNet and bigram counts, and does not require training. We present empirical results on a test set derived from the Master Metaphor List. We also discuss issues that make classiﬁcation of metaphors a tough problem in general. 
In this paper we present an active learning approach used to create an annotated corpus of literal and nonliteral usages of verbs. The model uses nearly unsupervised word-sense disambiguation and clustering techniques. We report on experiments in which a human expert is asked to correct system predictions in different stages of learning: (i) after the last iteration when the clustering step has converged, or (ii) during each iteration of the clustering algorithm. The model obtains an f-score of 53.8% on a dataset in which literal/nonliteral usages of 25 verbs were annotated by human experts. In comparison, the same model augmented with active learning obtains 64.91%. We also measure the number of examples required when model conﬁdence is used to select examples for human correction as compared to random selection. The results of this active learning system have been compiled into a freely available annotated corpus of literal/nonliteral usage of verbs in context. 
The majority of the state-of-the-art approaches to recognizing textual entailment focus on deﬁning a generic approach to RTE. A generic approach never works well for every single entailment pair: there are entailment pairs that are recognized poorly by all the generic systems. Automatic identiﬁcation of such entailment pairs and applying to them an RTE algorithm that is speciﬁc to them could thus increase an overall performance of an entailment engine (that in this case will combine a generic RTE algorithm with a number of RTE algorithms for the problematic entailment pairs). We identify one subtype of entailment pairs and develop a two-part probabilistic model for their classiﬁcation into true and false entailments and evaluate it relative both to a baseline and to the RTE systems. We show that the model performs better than the baseline and the average of the systems from the RTE2 on both the balanced and unbalanced datasets we have created for evaluation. 
Data that has been annotated by linguists is often considered a gold standard on many tasks in the NLP ﬁeld. However, linguists are expensive so researchers seek automatic techniques that correlate well with human performance. Linguists working on the ScamSeek project were given the task of deciding how many and which document classes existed in this previously unseen corpus. This paper investigates whether the document classes identiﬁed by the linguists correlate signiﬁcantly with Latent Dirichlet Allocation (LDA) topics induced from that corpus. Monte-Carlo simulation is used to measure the statistical signiﬁcance of the correlation between LDA models and the linguists’ characterisations. In experiments, more than 90% of the linguists’ classes met the level required to declare the correlation between linguistic insights and LDA models is signiﬁcant. These results help verify the usefulness of the LDA model in NLP and are a ﬁrst step in showing that the LDA model can replace the efforts of linguists in certain tasks like subdividing a corpus into classes. 
This paper reports on the application of the Text Attribution Tool (TAT) to proﬁling the authors of Arabic emails. The TAT system has been developed for the purpose of language-independent author proﬁling and has now been trained on two email corpora, English and Arabic. We describe the overall TAT system and the Machine Learning experiments resulting in classiﬁers for the different author traits. Predictions for demographic and psychometric author traits show improvements over the baseline for some of the author traits with both the English and the Arabic data. Arabic presents particular challenges for NLP and this paper describes more speciﬁcally the text processing components developed to handle Arabic emails. 
Near-synonyms are words that mean approximately the same thing, and which tend to be assigned to the same leaf in ontologies such as WordNet. However, they can differ from each other subtly in both meaning and usage—consider the pair of nearsynonyms frugal and stingy—and therefore choosing the appropriate near-synonym for a given context is not a trivial problem. Initial work by Edmonds (1997) suggested that corpus statistics methods would not be particularly effective, and led to subsequent work adopting methods based on speciﬁc lexical resources. In earlier work (Gardiner and Dras, 2007) we discussed the hypothesis that some kind of corpus statistics approach may still be effective in some situations, particularly if the near-synonyms differ in sentiment from each other, and we presented some preliminary conﬁrmation of the truth of this hypothesis. This suggests that problems involving this type of nearsynonym may be particularly amenable to corpus statistics methods. In this paper we investigate whether this result extends to a different corpus statistics method and in addition we analyse the results with respect to a possible confounding factor discussed in the previous work: the skewness of the sets of near synonyms. Our results show that the relationship between success in prediction and the nature of the near-synonyms is method dependent and that skewness is a more signiﬁcant factor.  
Large quantities of data are an increasingly essential resource for many Natural Language Processing techniques. The Web 1T corpus, a massive resource containing ngram frequencies produced from one trillion words drawn from the World Wide Web, is a relatively new corpus whose size will increase performance on many data-hungry applications. In addition, a ﬁxed resource of this kind reduces reliance on using web results as experimental data, increasing replicability of researchers’ results. However, effectively utilising a resource of this size presents signiﬁcant challenges. We discuss the challenges of using a data source of this magnitude, and describe strategies for overcoming these, including efﬁcient extraction of queries including wildcards, and specialised data compression. We present a software suite, “Get 1T”, implementing these techniques, released as free software for use by the natural language research community, and others. 
Question answering on speech transcripts (QAst) is a pilot track of the CLEF competition. In this paper we present our contribution to QAst, which is centred on a study of Named Entity (NE) recognition on speech transcripts, and how it impacts on the accuracy of the ﬁnal question answering system. We have ported AFNER, the NE recogniser of the AnswerFinder questionanswering project, to the set of answer types expected in the QAst track. AFNER uses a combination of regular expressions, lists of names (gazetteers) and machine learning to ﬁnd NeWS in the data. The machine learning component was trained on a development set of the AMI corpus. In the process we identiﬁed various problems with scalability of the system and the existence of errors of the extracted annotation, which lead to relatively poor performance in general. Performance was yet comparable with state of the art, and the system was second (out of three participants) in one of the QAst subtasks. 
Mutual Exclusion Bootstrapping (MEB) was designed to overcome the problem of semantic drift suffered by iterative bootstrapping, where the meaning of extracted terms quickly drifts from the original seed terms (Curran et al., 2007). MEB works by extracting mutually exclusive classes in parallel which constrain each other. In this paper we explore the strengths and limitations of MEB by applying it to two novel lexical-semantic extraction tasks: extracting bigram named entities and WordNet lexical ﬁle classes (Fellbaum, 1998) from the Google Web 1T 5-grams. 
Different parsers trained on the same corpus deliver different results, both in terms of overall performance and in terms of the individual analyses they provide. In particular, for any given sentence, one parser may provide a correct analysis, while another will produce an incorrect analysis; but when faced with a different sentence, the ﬁrst parser may be in error while the second is correct. In this paper, we leverage this observation by exploring how the results of a number of different parsers may be combined to provide a better performance than any single parser. The method involves constructing a chart that contains edges contributed by a collection of parsers, with a simple voting mechanism to choose the most preferred constituents; this provides a significant improvement in performance over any individual parser. More sophisticated voting mechanisms are also discussed. 
This paper proposes the use of a language representation that speciﬁes the relationship between terms of a sentence using question words. The proposed representation is tailored to help the search for documents containing an answer for a natural language question. This study presents the construction of this language model, the framework where it is used, and its evaluation. 
Abbreviations are commonly found instances of synonymy in Biomedical journal papers. Information retrieval systems that index paragraphs rather than full-text articles are more susceptible to term variation of this kind, since abbreviations are typically only deﬁned once at the beginning of the text. One solution to this problem is to expand the user query automatically with all possible abbreviation instances for each query term. In this paper, we compare the effectiveness of two abbreviation expansion techniques on the TREC 2006 Genomics Track queries and collection. Our results show that for highly ambiguous abbreviations the query collocation effect isn’t strong enough to deter the retrieval of erroneous passages. We conclude that full-text abbreviation resolution prior to passage indexing is the most appropriate approach to this problem. 
Collins’ widely-used parsing models treat noun phrases (NPs) in a different manner to other constituents. We investigate these differences, using the recently released internal NP bracketing data (Vadas and Curran, 2007a). Altering the structure of the Treebank, as this data does, has a number of consequences, as parsers built using Collins’ models assume that their training and test data will have structure similar to the Penn Treebank’s. Our results demonstrate that it is difﬁcult for Collins’ models to adapt to this new NP structure, and that parsers using these models make mistakes as a result. This emphasises how important treebank structure itself is, and the large amount of inﬂuence it can have. 
The utility of syntactic dependencies in computing distributional similarity has not yet been fully investigated. Most research based on syntactically conditioned co-occurrences simply ignores the salience of grammatical relations and effectively merges syntactic dependencies into one ‘context’. Through calculating distributional similarity, we design two experiments to explore and evaluate the four major types of contexts that are conditioned on grammatical relations. The consistent results show that the headmodifier dependency plays an important role in predicting the semantic features of nouns and verbs, in contrast to other dependencies. 
This paper proposes a method for automatically sense-to-sense aligning dictionaries in different languages (focusing on Japanese and English), based on structural data in the respective dictionaries. The basis of the proposed method is sentence similarity of the sense deﬁnition sentences, using a bilingual Japanese-to-English dictionary as a pivot during the alignment process. We experiment with various embellishments to the basic method, including term weighting, stemming/lemmatisation, and ontology expansion. 
Morphological analysis is often used during preprocessing in Statistical Machine Translation. Existing work suggests that the beneﬁt would be greater for more highly inﬂected languages, although to our knowledge this has not been systematically tested on languages with comparable morphology. In this paper, two comparable languages with different amounts of inﬂection are tested, to see if the beneﬁts of morphology used during the translation process, depends on the morphological richness of the language. For this work we use indigenous Australian languages: most Australian Aboriginal languages are highly inﬂected, where words can take a considerable number of postﬁxes when compared to Indo-European languages, and for languages in the same (Pama Nyungan) family, the morphological system works similarly. We show in this preliminary work that morphological analysis clearly beneﬁts the richer of the two languages investigated, but is more equivocal in the case of the other. 
Most existing systems for automatically extracting lexical-semantic resources neglect multi-word expressions (MWEs), even though approximately 30% of gold-standard thesauri entries are MWEs. We present a distributional similarity system that identiﬁes synonyms for MWEs. We extend Grefenstette’s SEXTANT shallow parser to ﬁrst identify bigram MWEs using collocation statistics from the Google WEB1T corpus. We extract contexts from WEB1T to increase coverage on the sparser bigrams. 
CCGbank is an automatic conversion of the Penn Treebank to Combinatory Categorial Grammar (CCG). We present two extensions to CCGbank which involve manipulating its derivation and category structure. We discuss approaches for the automatic re-insertion of removed quote symbols and evaluate their impact on the performance of the C&C CCG parser. We also analyse CCGbank to extract a multi-modal CCG lexicon, which will allow the removal of hardcoded language-speciﬁc constraints from the C&C parser, granting beneﬁts to parsing speed and accuracy. 
This paper presents a combined supervised and unsupervised approach for multidocument person name disambiguation. Based on feature vectors reﬂecting pairwise comparisons between web pages, a classiﬁcation algorithm provides linking information about document pairs, which leads to initial clusters. In addition, two different clustering algorithms are fed with matrices of weighted keywords. In a ﬁnal step the “seed” clusters are combined with the results of the clustering algorithms. Results on the validation data show that a combined classiﬁcation and clustering approach doesn’t always compare favorably to those obtained by the different algorithms separately. 
The increasing number of web sources is exacerbating the named-entity ambiguity problem. This paper explores the use of various token-based and phrase-based features in unsupervised clustering of web pages containing personal names. From these experiments, we find that the use of rich features can significantly improve the disambiguation performance for web personal names. 
We approached the temporal relation identiﬁcation tasks of TempEval 2007 as pair-wise classiﬁcation tasks. We introduced a variety of syntactically and semantically motivated features, including temporal-logicbased features derived from running our Task B system on the Task A and C data. We trained support vector machine models and achieved the second highest accuracies on the tasks: 61% on Task A, 75% on Task B and 54% on Task C. 
We propose an IE based approach to people disambiguation. We assume the mentioning of NEs and the relational context of a person in the text to be important discriminating features in order to distinguish different people sharing a name. 
We present a corpus-based supervised learning system for coarse-grained sense disambiguation. In addition to usual features for training in word sense disambiguation, our system also uses Base Level Concepts automatically obtained from WordNet. Base Level Concepts are some synsets that generalize a hyponymy sub–hierarchy, and provides an extra level of abstraction as well as relevant information about the context of a word to be disambiguated. Our experiments proved that using this type of features results on a signiﬁcant improvement of precision. Our system has achieved almost 0.8 F1 (ﬁfth place) in the coarse–grained English all-words task using a very simple set of features plus Base Level Concepts annotation. 
In this paper, we attempt to use a sequence labeling model with features from dependency parsed tree for temporal relation identiﬁcation. In the sequence labeling model, the relations of contextual pairs can be used as features for relation identiﬁcation of the current pair. Head-modiﬁer relations between pairs of words within one sentence can be also used as the features. In our preliminary experiments, these features are effective for the temporal relation identiﬁcation tasks. 
Optimal ensembling (OE) is a word sense disambiguation (WSD) method using word-specific training factors (average positive vs negative training per sense, posex and negex) to predict best system (classifier algorithm / applicable feature set) for given target word. Our official entry (OE1) in Senseval-4 Task 17 (coarse-grained English lexical sample task) contained many design flaws and thus failed to show the whole potential of the method, finishing -4.9% behind top system (+0.5 gain over best base system). A fixed system (OE2) finished only -3.4% behind (+2.0% net gain). All our systems were 'closed', i.e. used the official training data only (average 56 training examples per each sense). We also show that the official evaluation measure tends to favor systems that do well with high-trained words. 
This article introduces an unsupervised word sense disambiguation algorithm that is inspired by the lexical attraction models of Yuret (1998). It is based on the assumption that the meanings of the words that form a sentence can be best assigned by constructing an interpretation of the whole sentence. This interpretation is facilitated by a dependency-like context speciﬁcation of a content word within the sentence. Thus, ﬁnding the context words of a target word is a matter of ﬁnding a pseudo-syntactic dependency analysis of the sentence, called a linkage. 
This paper reports on a experiment to identify the emotional loading (the “valence”) of news headlines. The experiment reported is based on a resource-thrifty approach for valence annotation based on a word-space model and a set of seed words. The model was trained on newsprint, and valence was computed using proximity to one of two manually deﬁned points in a highdimensional word space — one representing positive valence, the other representing negative valence. By projecting each headline into this space, choosing as valence the similarity score to the point that was closer to the headline, the experiment provided results with high recall of negative or positive headlines. These results show that working without a high-coverage lexicon is a viable approach to content analysis of textual data. 
We present two systems that pick the ten most appropriate substitutes for a marked word in a test sentence. The ﬁrst system scores candidates based on how frequently their local contexts match that of the marked word. The second system, an enhancement to the ﬁrst, incorporates cosine similarity using unigram features. The core of both systems bypasses intermediate sense selection. Our results show that a knowledge-light, direct method for scoring potential replacements is viable. 
Most of the previous works that disambiguate personal names in Web search results employ agglomerative clustering approaches. However, these approaches tend to generate clusters that contain a single element depending on a certain criterion of merging similar clusters. In contrast to such previous works, we have adopted a semisupervised clustering approach to integrate similar documents into a labeled document. Moreover, our proposed approach is characterized by controlling the ﬂuctuation of the centroid of a cluster in order to generate more accurate clusters. 
Words in the context of a target word have long been used as features by supervised word-sense classiﬁers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words—the distributional proﬁle of a concept (DPC)—without the use of manually annotated data. We implemented an unsupervised na¨ıve Bayes word sense classiﬁer using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese– English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classiﬁer to attempt the English Lexical Substitution Task (task #10); however, its performance was poor. 
This paper presents a headline emotion classiﬁcation approach based on frequency and co-occurrence information collected from the World Wide Web. The content words of a headline (nouns, verbs, adverbs and adjectives) are extracted in order to form different bag of word pairs with the joy, disgust, fear, anger, sadness and surprise emotions. For each pair, we compute the Mutual Information Score which is obtained from the web occurrences of an emotion and the content words. Our approach is based on the hypothesis that group of words which co-occur together across many documents with a given emotion are highly probable to express the same emotion. 
This paper presents an approach for web page clustering. The different underlying meanings of a name are discovered on the basis of the title of the web page, the body content, the common named entities across the documents and the sub-links. This information is feeded into a K-Means clustering algorithm which groups together the web pages that refer to the same individual. 
For our system we use the SMO implementation of a support vector machine provided with the WEKA machine learning toolkit. As with all machine learning approaches, the most important step is to choose a set of features which reliably help to predict the label of the example. We used 76 features drawn from two very different knowledge sources. The first 48 features are boolean values indicating whether or not each of the nominals in the sentence are linked to certain other words in the WordNet hypernym and meronym networks. The remaining 28 features are web frequency counts for the two nominals joined by certain common prepositions and verbs. Our system performed well on all but two of the relations; theme-tool and origin entity. 
We describe a supervised learning approach to categorizing inter-noun relations, based on Support Vector Machines, that builds a different classifier for each of seven semantic relations. Each model uses the same learning strategy, while a simple voting procedure based on five trained discriminators with various blends of features determines the final categorization. The features that characterize each of the noun pairs are a blend of lexicalsemantic categories extracted from WordNet and several flavors of syntactic patterns extracted from various corpora, including Wikipedia and the WMTS corpus. 
SenseClusters is a freely–available open– source system that served as the University of Minnesota, Duluth entry in the SENSEVAL-4 sense induction task. For this task SenseClusters was conﬁgured to construct representations of the instances to be clustered using the centroid of word cooccurrence vectors that replace the words in an instance. These instances are then clustered using k–means where the number of clusters is discovered automatically using the Adapted Gap Statistic. In these experiments SenseClusters did not use any information outside of the raw untagged text that was to be clustered, and no tuning of the system was performed using external corpora.  Strong Contextual Hypothesis (Miller and Charles, 1991). SenseClusters has been in active development at the University of Minnesota, Duluth since 2002. It is an open–source project that is freely–available from sourceforge, and has been been described in detail in numerous publications (e.g., (Purandare and Pedersen, 2004), (Pedersen et al., 2005), (Pedersen and Kulkarni, 2007)). SenseClusters supports a variety of techniques for selecting lexical features, representing contexts to be clustered, determining the appropriate number of cluster automatically, clustering, labeling of clusters, and evaluating cluster quality. The conﬁguration used in SENSEVAL-4 was just one possible combination of these techniques. 2 Methodology in Sense Induction Task  
We describe the SUPERSENSELEARNER system that participated in the English allwords disambiguation task. The system relies on automatically-learned semantic models using collocational features coupled with features extracted from the annotations of coarse-grained semantic categories generated by an HMM tagger. 
We describe the Shefﬁeld system used in TempEval-2007. Our system takes a machine-learning (ML) based approach, treating temporal relation assignment as a simple classiﬁcation task and using features easily derived from the TempEval data, i.e. which do not require ‘deeper’ NLP analysis. We aimed to explore three questions: (1) How well would a ‘lite’ approach of this kind perform? (2) Which features contribute positively to system performance? (3) Which ML algorithm is better suited for the TempEval tasks? We used the Weka ML workbench to facilitate experimenting with different ML algorithms. The paper describes our system and supplies preliminary answers to the above questions. 
Although researchers have shown increasing interest in extracting/classifying semantic relations, most previous studies have basically relied on lexical patterns between terms. This paper proposes a novel way to accomplish the task: a system that captures a physical size of an entity. Experimental results revealed that our proposed method is feasible and prevents the problems inherent in other methods. 
The system we propose to learning semantic relations consists of two parallel components. For our ﬁnal submission we used components based on the similarity measures deﬁned over WordNet and the patterns extracted from the Web and WMTS. Other components using syntactic structures were explored but not used for the ﬁnal run. 
We present the system we used for the TempEval competition. This system relies on a deep syntactic analyzer that has been extended for the treatment of temporal expressions, thus making temporal processing a complement to a better general purpose text understanding system. 
Recent work by Nerbonne and Wiersma (2006) has provided a foundation for measuring syntactic differences between corpora. It uses part-of-speech trigrams as an approximation to syntactic structure, comparing the trigrams of two corpora for statistically signiﬁcant differences. This paper extends the method and its application. It extends the method by using leafpath ancestors of Sampson (2000) instead of trigrams, which capture internal syntactic structure—every leaf in a parse tree records the path back to the root. The corpus used for testing is the International Corpus of English, Great Britain (Nelson et al., 2002), which contains syntactically annotated speech of Great Britain. The speakers are grouped into geographical regions based on place of birth. This is different in both nature and number than previous experiments, which found differences between two groups of Norwegian L2 learners of English. We show that dialectal variation in eleven British regions from the ICEGB is detectable by our algorithm, using both leaf-ancestor paths and trigrams. 
This paper proposes a novel approach to the induction of Combinatory Categorial Grammars (CCGs) by their potential afﬁnity with the Genetic Algorithms (GAs). Speciﬁcally, CCGs utilize a rich yet compact notation for lexical categories, which combine with relatively few grammatical rules, presumed universal. Thus, the search for a CCG consists in large part in a search for the appropriate categories for the data-set’s lexical items. We present and evaluates a system utilizing a simple GA to successively search and improve on such assignments. The ﬁtness of categorial-assignments is approximated by the coverage of the resulting grammar on the data-set itself, and candidate solutions are updated via the standard GA techniques of reproduction, crossover and mutation. 
The aim of this paper is to present a simple yet efﬁcient implementation of a tool for simultaneous rule-based morphosyntactic tagging and partial parsing formalism. The parser is currently used for creating a treebank of partial parses in a valency acquisition project over the IPI PAN Corpus of Polish. 
The paper proposes a methodology for dealing with multiword expressions in natural language processing applications. It provides a practically justiﬁed taxonomy of such units, and suggests the ways in which the individual classes can be processed computationally. While the study is currently limited to Polish and English, we believe our ﬁndings can be successfully employed in the processing of other languages, with emphasis on inﬂectional ones. 
This paper describes an algorithm to automatically generate a list of cognates in a target language by means of Support Vector Machines. While Levenshtein distance was used to align the training file, no knowledge repository other than an initial list of cognates used for training purposes was input into the algorithm. Evaluation was set up in a cognate production scenario which mimed a reallife situation where no word lists were available in the target language, delivering the ideal environment to test the feasibility of a more ambitious project that will involve language portability. An overall improvement of 50.58% over the baseline showed promising horizons. 
Effectively identifying events in unstructured text is a very difﬁcult task. This is largely due to the fact that an individual event can be expressed by several sentences. In this paper, we investigate the use of clustering methods for the task of grouping the text spans in a news article that refer to the same event. The key idea is to cluster the sentences, using a novel distance metric that exploits regularities in the sequential structure of events within a document. When this approach is compared to a simple bag of words baseline, a statistically signiﬁcant increase in performance is observed. 
This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts. We assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR, but with some distinct features. The experiments showed that the best results were obtained from the combination of character-based processing, dictionary look up (maximum matching) and a negation check.  volved in international trade or international relations. The sentiment classiﬁcation experiments presented in this paper were done in the context of Opinionated Information Retrieval which is planned to be a module in a Cross-Language Opinion Extraction system (CLOE). The main goal of this system is to provide access to opinionated information on any topic ad-hoc in a language different to the language of a query. To implement the idea the CLOE system which is the context for the experiments described in the paper will consist of four main modules:  
I review a number of grammar induction algorithms (ABL, Emile, Adios), and test them on the Eindhoven corpus, resulting in disappointing results, compared to the usually tested corpora (ATIS, OVIS). Also, I show that using neither POS-tags induced from Biemann’s unsupervised POS-tagging algorithm nor hand-corrected POS-tags as input improves this situation. Last, I argue for the development of entirely incremental grammar induction algorithms instead of the approaches of the systems discussed before. 
We investigate a family of update methods for online machine learning algorithms for cost-sensitive multiclass and structured classiﬁcation problems. The update rules are based on multinomial logistic models. The most interesting question for such an approach is how to integrate the cost function into the learning paradigm. We propose a number of solutions to this problem. To demonstrate the applicability of the algorithms, we evaluated them on a number of classiﬁcation tasks related to incremental dependency parsing. These tasks were conventional multiclass classiﬁcation, hiearchical classiﬁcation, and a structured classiﬁcation task: complete labeled dependency tree prediction. The performance ﬁgures of the logistic algorithms range from slightly lower to slightly higher than margin-based online algorithms. 
This paper compares different measures of graphemic similarity applied to the task of bilingual lexicon induction between a Swiss German dialect and Standard German. The measures have been adapted to this particular language pair by training stochastic transducers with the ExpectationMaximisation algorithm or by using handmade transduction rules. These adaptive metrics show up to 11% F-measure improvement over a static metric like Levenshtein distance. 
The aim of this paper is to present a new method for identifying linguistic structure in the aggregate analysis of the language variation. The method consists of extracting the most frequent sound correspondences from the aligned transcriptions of words. Based on the extracted correspondences every site is compared to all other sites, and a correspondence index is calculated for each site. This method enables us to identify sound alternations responsible for dialect divisions and to measure the extent to which each alternation is responsible for the divisions obtained by the aggregate analysis. 
I propose a computational treatment of superlatives, starting with superlative constructions and the main challenges in automatically recognising and extracting their components. Initial experimental evidence is provided for the value of the proposed work for Question Answering. I also briefly discuss its potential value for Sentiment Detection and Opinion Extraction. 
There is little consensus on a standard experimental design for the compound interpretation task. This paper introduces wellmotivated general desiderata for semantic annotation schemes, and describes such a scheme for in-context compound annotation accompanied by detailed publicly available guidelines. Classiﬁcation experiments on an open-text dataset compare favourably with previously reported results and provide a solid baseline for future research. 
This paper investigates the use of machine learning algorithms to label modifier-noun compounds with a semantic relation. The attributes used as input to the learning algorithms are the web frequencies for phrases containing the modifier, noun, and a prepositional joining term. We compare and evaluate different algorithms and different joining phrases on Nastase and Szpakowicz’s (2003) dataset of 600 modifier-noun compounds. We find that by using a Support Vector Machine classifier we can obtain better performance on this dataset than a current state-of-the-art system; even with a relatively small set of prepositional joining terms. 
This paper describes a new method for computing lexical chains. These are sequences of semantically related words that reflect a text’s cohesive structure. In contrast to previous methods, we are able to select chains based on their cohesive strength. This is achieved by analyzing the connectivity in graphs representing the lexical chains. We show that the generated chains significantly improve performance of automatic text summarization and keyphrase indexing. 
Our paper reports an attempt to apply an unsupervised clustering algorithm to a Hungarian treebank in order to obtain semantic verb classes. Starting from the hypothesis that semantic metapredicates underlie verbs’ syntactic realization, we investigate how one can obtain semantically motivated verb classes by automatic means. The 150 most frequent Hungarian verbs were clustered on the basis of their complementation patterns, yielding a set of basic classes and hints about the features that determine verbal subcategorization. The resulting classes serve as a basis for the subsequent analysis of their alternation behavior. 
 This paper describes MIMUS, a multimodal and multilingual dialogue system for the in– home scenario, which allows users to control some home devices by voice and/or clicks. Its design relies on Wizard of Oz experiments and is targeted at disabled users. MIMUS follows the Information State Update approach to dialogue management, and supports English, German and Spanish, with the possibility of changing language on–the– ﬂy. MIMUS includes a gestures–enabled talking head which endows the system with a human–like personality. 
We are currently developing a translation aid system specially designed for Englishto-Japanese volunteer translators working mainly online. In this paper we introduce the stratiﬁed reference lookup interface that has been incorporated into the source text area of the system, which distinguishes three user awareness levels depending on the type and nature of the reference unit. The different awareness levels are assigned to reference units from a variety of reference sources, according to the criteria of “composition”, “difﬁculty”, “speciality” and “resource type”. 
A multimedia blog creation system is described that uses Japanese dialogue with an intelligent robot. Although multimedia blogs are increasing in popularity, creating blogs is not easy for users who lack highlevel information literacy skills. Even skilled users have to waste time creating and assigning text descriptions to their blogs and searching related multimedia such as images, music, and illustrations. To enable effortless and enjoyable creation of multimedia blogs, we developed the system on a prototype robot called PaPeRo. Video messages are recorded and converted into text descriptions by PaPeRo using continuous speech recognition. PaPeRo then searches for suitable multimedia contents on the internet and databases, and then, based on the search results, chooses appropriate sympathetic comments by using natural language text retrieval. The retrieved contents, PaPeRo's comments, and the video recording on the user's blog is automatically uploaded and edited. The system was evaluated by 10 users for creating travel blogs and proved to be helpful for both inexperienced and experienced users. The system enabled easy multimediarich blog creation and even provided users the pleasure of chatting with PaPeRo.  ganizations. A multimedia blog, which contains videos, music, and illustrations, is increasing in popularity because it enables users to express their thoughts creatively. However, users are unsatisfied with the current multimedia blog creation methods. Users have three requirements. First, they need easier methods to create blogs. Most multimedia blogs are created in one of two ways: 1) A user creates audio-visual contents by cameras and or some other recording devices, and then assigns a text description to the contents as indexes. 2) A user creates a text blog, and then searches for multimedia contents on the internet and databases to attach them to his blog. Both methods require high-level information literacy skills. Second, they would like to reduce their blog-creation time. Even skilled users have to waste time assigning text description and searching related multimedia contents. Third, they like to be encouraged by other peoples’ comments on their blogs. Although some users utilize pet-type agents making automatic comments to their blogs, the agents do not always satisfy them because the comments do not consider users' moods. To meet the three requirements, we developed a multimedia blog creation system using Japanese dialogue with an intelligent robot. The system was developed on a prototype robot called PaPeRo (Fujita, 2002), which has the same CPU and memory as a mobile PC. In this paper, we describe the multimedia blog creation method and the evaluation results in a practical setting. 2 Multimedia Blog Creation  
 The structure of the paper is as follows. First,  In this paper, we introduce SEMTAG, a free and open software architecture for the development of Tree Adjoining Grammars integrating a compositional semantics. SEMTAG differs from XTAG in two main ways. First, it provides an expressive grammar formalism and compiler for factorising and specifying TAGs. Second, it supports semantic construction.  we brieﬂy introduce the syntactic and semantic formalisms that are being handled (section 2). Second, we situate our approach with respect to other possible ways of doing TAG based semantic construction (section 3). Third, we show how XMG, the linguistic formalism used to specify the grammar (section 4) differs from existing computational frameworks for specifying a TAG and in particular, how it supports the integration of semantic information. Finally, section 5 focuses on the semantic construction module  
In this paper, we will describe ODIE, the On-Demand Information Extraction system. Given a user’s query, the system will produce tables of the salient information about the topic in structured form. It produces the tables in less than one minute without any knowledge engineering by hand, i.e. pattern creation or paraphrase knowledge creation, which was the largest obstacle in traditional IE. This demonstration is based on the idea and technologies reported in (Sekine 06). A substantial speed-up over the previous system (which required about 15 minutes to analyze one year of newspaper) was achieved through a new approach to handling pattern candidates; now less than one minute is required when using 11 years of newspaper corpus. In addition, functionality was added to facilitate investigation of the extracted information. 
 Vogel, 2004) and ontologies engineering (Klein,  This paper describes the main features of our tool called “Legal Taxonomy Syllabus”. The system is an ontology based tool designed to annotate and recover multi-lingua legal information and build conceptual dictionaries on European Directives. 
This paper describes the latest version of speech-to-speech translation systems developed by the team of NICT-ATR for over twenty years. The system is now ready to be deployed for the travel domain. A new noise-suppression technique notably improves speech recognition performance. Corpus-based approaches of recognition, translation, and synthesis enable coverage of a wide variety of topics and portability to other languages. 
 models can also be used to quantify the relative  We introduce the zipfR package, a power-  productivity of two morphological processes (as illustrated below) or of two rival syntactic construc-  ful and user-friendly open-source tool for  tions by looking at their vocabulary growth rate as  LNRE modeling of word frequency distribu-  sample size increases. Practical NLP applications  tions in the R statistical environment. We  include making informed guesses about type counts  give some background on LNRE models,  in very large data sets (e.g., How many typos are  discuss related software and the motivation  there on the Internet?) and determining the “lexical  for the toolkit, describe the implementation,  richness” of texts belonging to different genres. Last  and conclude with a complete sample ses-  but not least, LNRE models play an important role  sion showing a typical LNRE analysis.  as a population model for Bayesian inference and  
 University of Sydney  Oxford University Universita` di Roma “La Sapienza”  NSW 2006, Australia Wolfson Building, Parks Road  via Salaria 113  james@it.usyd.edu.au  Oxford, OX1 3QD, UK  00198 Roma, Italy  stephen.clark@comlab.ox.ac.uk  bos@di.uniroma1.it  
We demonstrate one aspect of an affectextraction system for use in intelligent conversational agents. This aspect performs a degree of affective interpretation of some types of metaphorical utterance.  of metaphor, with a signiﬁcant degree of linguistic open-endedness. Also, note that our overarching research aim is to study metaphor as such, not just how it arises in e-drama. This increases our need for systematic, open-ended methods. 2 Metaphor and Affect  
 A thesaurus is created by  Gorman and Curran (2006) argue that the-  • taking a corpus  saurus generation for billion+-word corpora is problematic as the full computation takes  • identifying contexts for each word  many days. We present an algorithm with which the computation takes under two hours. We have created, and made publicly available, thesauruses based on large corpora for (at time of writing) seven major world languages. The development is implemented in the Sketch Engine (Kilgarriff et al., 2004). Another innovative development in the same tool is the presentation of the grammatical behaviour of a word against the background of how all other words of the same word class behave. Thus, the English noun constraint occurs 75% in the plural. Is this a salient lexical fact? To form a judgement, we need to know the distribution for all nouns. We use histograms to present the distribution in a way that is easy to grasp.  • identifying which words share contexts. For each word, the words that share most contexts (according to some statistic which also takes account of their frequency) are its nearest neighbours. Thesauruses generally improve in accuracy with corpus size. The larger the corpus, the more clearly the signal (of similar words) will be distinguished from the noise (of words that just happen to share a few contexts). Lin’s was based on around 300M words and (Curran, 2004) used 2B (billion). A direct approach to thesaurus computation looks at each word and compares it with each other word, checking all contexts to see if they are shared. Thus, complexity is O(n2m) where n in the number of types and m is the size of the context vector. The number of types increases with the corpus size, and (Ravichandran et al., 2005) propose heuristics for thesaurus building without undertaking the complete  
We describe the semantic enrichment of journal articles with chemical structures and biomedical ontology terms using Oscar, a program for chemical named entity recognition (NER). We describe how Oscar works and how it can been adapted for general NER. We discuss its implementation in a real publishing workﬂow and possible applications for enriched articles. 
 3 The Application Programming Interface  We present an API for computing the semantic relatedness of words in Wikipedia. 
A distributional method for part-of-speech induction is presented which, in contrast to most previous work, determines the part-of-speech distribution of syntactically ambiguous words without explicitly tagging the underlying text corpus. This is achieved by assuming that the word pair consisting of the left and right neighbor of a particular token is characteristic of the part of speech at this position, and by clustering the neighbor pairs on the basis of their middle words as observed in a large corpus. The results obtained in this way are evaluated by comparing them to the part-of-speech distributions as found in the manually tagged Brown corpus. 
This paper presents the use of Support Vector Machines (SVM) to detect relevant information to be included in a queryfocused summary. Several SVMs are trained using information from pyramids of summary content units. Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation. 
This paper presents noisy-channel based Korean preprocessor system, which corrects word spacing and typographical errors. The proposed algorithm corrects both errors simultaneously. Using Eojeol transition pattern dictionary and statistical data such as Eumjeol n-gram and Jaso transition probabilities, the algorithm minimizes the usage of huge word dictionaries. 
Kernel methods such as support vector machines (SVMs) have attracted a great deal of popularity in the machine learning and natural language processing (NLP) communities. Polynomial kernel SVMs showed very competitive accuracy in many NLP problems, like part-of-speech tagging and chunking. However, these methods are usually too inefficient to be applied to large dataset and real time purpose. In this paper, we propose an approximate method to analogy polynomial kernel with efficient data mining approaches. To prevent exponential-scaled testing time complexity, we also present a new method for speeding up SVM classifying which does independent to the polynomial degree d. The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively. 
This paper addresses two remaining challenges in Chinese word segmentation. The challenge in HLT is to ﬁnd a robust segmentation method that requires no prior lexical knowledge and no extensive training to adapt to new types of data. The challenge in modelling human cognition and acquisition it to segment words efﬁciently without using knowledge of wordhood. We propose a radical method of word segmentation to meet both challenges. The most critical concept that we introduce is that Chinese word segmentation is the classiﬁcation of a string of character-boundaries (CB’s) into either word-boundaries (WB’s) and non-word-boundaries. In Chinese, CB’s are delimited and distributed in between two characters. Hence we can use the distributional properties of CB among the background character strings to predict which CB’s are WB’s. 
On a multi-dimensional text categorization task, we compare the effectiveness of a feature based approach with the use of a stateof-the-art sequential learning technique that has proven successful for tasks such as “email act classification”. Our evaluation demonstrates for the three separate dimensions of a well established annotation scheme that novel thread based features have a greater and more consistent impact on classification performance. 
 low-dimensional space, and improves both cluster-  In this paper, we propose a new ensemble document clustering method. The novelty of our method is the use of Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase. In our experiment, we compared our method with some clustering methods. Our method achieved the best results.  ing accuracy and speed. NMF is a dimensional reduction method (Xu et al., 2003) that is based on the “aspect model” used in the Probabilistic Latent Semantic Indexing (Hofmann, 1999). Because the axis in the reduced space by NMF corresponds to a topic, the reduced vector represents the clustering result. For a given termdocument matrix and cluster number, we can obtain the NMF result with an iterative procedure (Lee and Seung, 2000). However, this iteration does not al-  
In this work, we investigate the use of error-correcting output codes (ECOC) for boosting centroid text classifier. The implementation framework is to decompose one multi-class problem into multiple binary problems and then learn the individual binary classification problems by centroid classifier. However, this kind of decomposition incurs considerable bias for centroid classifier, which results in noticeable degradation of performance for centroid classifier. In order to address this issue, we use Model-Refinement to adjust this so-called bias. The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids of text data. The experimental results reveal that Model-Refinement can dramatically decrease the bias introduced by ECOC, and the combined classifier is comparable to or even better than SVM classifier in performance. 1. Introduction In recent years, ECOC has been applied to boost the naïve bayes, decision tree and SVM classifier for text data (Berger 1999, Ghani 2000, Ghani 2002, Rennie et al. 2001). Following this research direction, in this work, we explore the use of ECOC to enhance the performance of centroid classifier (Han et al. 2000). To the best of our knowledge, no previous work has been conducted on exactly this problem. The framework we adopted is to decompose one multi-class problem into multiple binary problems and then use centroid classifier to learn the individual binary classification problems. However, this kind of decomposition incurs considerable bias (Liu et al. 2002) for centroid classifier. In substance, centroid classifier (Han et  al. 2000) relies on a simple decision rule that a given document should be assigned a particular class if the similarity (or distance) of this document to the centroid of the class is the largest (or smallest). This decision rule is based on a straightforward assumption that the documents in one category should share some similarities with each other. However, this hypothesis is often violated by ECOC on the grounds that it ignores the similarities of original classes when disassembling one multi-class problem into multiple binary problems. In order to attack this problem, we use ModelRefinement (Tan et al. 2005) to reduce this socalled bias. The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids. This technique is very flexible, which only needs one classification method and there is no change to the method in any way. To examine the performance of proposed method, we conduct an extensive experiment on two commonly used datasets, i.e., Newsgroup and Industry Sector. The results indicate that ModelRefinement can dramatically decrease the bias introduce by ECOC, and the resulted classifier is comparable to or even better than SVM classifier in performance. 2. Error-Correcting Output Coding Error-Correcting Output Coding (ECOC) is a form of combination of multiple classifiers (Ghani 2000). It works by converting a multiclass supervised learning problem into a large number (L) of two-class supervised learning problems (Ghani 2000). Any learning algorithm that can handle two-class learning problems, such as Naïve Bayes (Sebastiani 2002), can then be applied to learn each of these L problems. L can then be thought of as the length of the codewords  81 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 81–84, Prague, June 2007. c 2007 Association for Computational Linguistics  with one bit in each codeword for each classifier. The ECOC algorithm is outlined in Figure 1. TRAINING 1 Load training data and parameters, i.e., the length of code L and training class K. 2 Create a L-bit code for the K classes using a kind of coding algorithm. 3 For each bit, train the base classifier using the binary class (0 and 1) over the total training data. TESTING 1 Apply each of the L classifiers to the test example. 2 Assign the test example the class with the largest votes. Figure 1: Outline of ECOC  Let’s take a simple multi-class classification task with 12 classes. After coding the original classes, we obtain the dataset as Figure 2. Class 0 consists of 6 original categories, and class 1 contains another 6 categories. Then we calculate the centroids of merged class 0 and merged class 1 using formula (1), and draw a Middle Line that is the perpendicular bisector of the line between the two centroids.  Class 0  Middle Line Class 1  3. Methodology  3.1 The bias incurred by ECOC for centroid classifier Centroid classifier is a linear, simple and yet efficient method for text categorization. The basic idea of centroid classifier is to construct a centroid Ci for each class ci using formula (1) where d denotes one document vector and |z| indicates the cardinality of set z. In substance, centroid classifier makes a simple decision rule (formula (2)) that a given document should be assigned a particular class if the similarity (or distance) of this document to the centroid of the class is the largest (or smallest). This rule is based on a straightforward assumption: the documents in one category should share some similarities with each other.  C = 1 ∑d  (1)  i  c d∈ci  i  c  =  arg  max c i  ⎜⎜⎝⎛  d ⋅C i  dC  2  i  ⎟⎞ 2 ⎟⎠  (2)  For example, the single-topic documents involved with “sport” or “education” can meet with the presumption; while the hybrid documents involved with “sport” as well as “education” break this supposition.  As such, ECOC based centroid classifier also breaks this hypothesis. This is because ECOC ignores the similarities of original classes when producing binary problems. In this scenario, many different classes are often merged into one category. For example, the class “sport” and “education” may be assembled into one class. As a result, the assumption will inevitably be broken.  82  d  C0  C1  Figure 2: Original Centroids of Merged Class 0 and  Class 1  According to the decision rule (formula (2)) of centroid classifier, the examples of class 0 on the right of the Middle Line will be misclassified into class 1. This is the mechanism why ECOC can bring bias for centroid classifier. In other words, the ECOC method conflicts with the assumption of centroid classifier to some degree.  3.2 Why Model-Refinement can reduce this bias? In order to decrease this kind of bias, we employ the Model-Refinement to adjust the class representative, i.e., the centroids. The basic idea of Model-Refinement is to make use of training errors to adjust class centroids so that the biases can be reduced gradually, and then the trainingset error rate can also be reduced gradually.  
This paper presents recent extensions to Poliqarp, an open source tool for indexing and searching morphosyntactically annotated corpora, which turn it into a tool for indexing and searching certain kinds of treebanks, complementary to existing treebank search engines. In particular, the paper discusses the motivation for such a new tool, the extended query syntax of Poliqarp and implementation and efﬁciency issues. 
 2 Corpus Annotation  Opinion analysis is an important research topic in recent years. However, there are no common methods to create evaluation corpora. This paper introduces a method for developing opinion corpora involving multiple annotators. The characteristics of the created corpus are discussed, and the methodologies to select more consistent testing collections and their corresponding gold standards are proposed. Under the gold standards, an opinion extraction system is evaluated. The experiment results show some interesting phenomena. 
The AMI Meeting Corpus is now publicly available, including manual annotation ﬁles generated in the NXT XML format, but lacking explicit metadata for the 171 meetings of the corpus. To increase the usability of this important resource, a representation format based on relational databases is proposed, which maximizes informativeness, simplicity and reusability of the metadata and annotations. The annotation ﬁles are converted to a tabular format using an easily adaptable XSLT-based mechanism, and their consistency is veriﬁed in the process. Metadata ﬁles are generated directly in the IMDI XML format from implicit information, and converted to tabular format using a similar procedure. The results and tools will be freely available with the AMI Corpus. Sharing the metadata using the Open Archives network will contribute to increase the visibility of the AMI Corpus. 
This paper focuses on the exploration of term dependence in the application of sentence retrieval. The adjacent terms appearing in query are assumed to be related with each other. These assumed dependences among query terms will be further validated for each sentence and sentences, which present strong syntactic relationship among query terms, are considered more relevant. Experimental results have fully demonstrated the promising of the proposed models in improving sentence retrieval effectiveness. 
 The remaining part of this paper is structured as  We present a Minimum Bayes Risk (MBR) decoder for statistical machine translation. The approach aims to minimize the expected loss of translation errors with regard to the  follows: after a short overview of related work in Sec. 2, we describe the MBR decoder in Sec. 3. We present the experimental results in Sec. 4 and conclude in Sec. 5.  BLEU score. We show that MBR decoding on N -best lists leads to an improvement of translation quality. We report the performance of the MBR decoder on four different tasks: the TCSTAR EPPS Spanish-English task 2006, the NIST Chinese-English task 2005 and the GALE Arabic-English and Chinese-English task 2006. The absolute improvement of the BLEU score is between 0.2% for the TCSTAR task and 1.1% for the GALE ChineseEnglish task. 
 tecting the fact that a task is being assigned, and in  We describe an algorithm for a novel task: disambiguating the pronoun you in conversation. You can be generic or referential; ﬁnding referential you is important for tasks such as addressee identiﬁcation or extracting ‘owners’ of action items. Our classiﬁer achieves 84% accuracy in two-person conversations; an initial study shows promising performance even on more complex multi-party meetings. 
 and NECESSITIES AS FOOD. However, these  In this paper we provide a formalization of a set of default rules that we claim are required for the transfer of information such as causation, event rate and duration in the interpretation of metaphor. Such rules are domain-independent and are identiﬁed as invariant adjuncts to any conceptual metaphor. We also show a way of embedding the invariant mappings in a semantic framework.  metaphorical views would not contain any relationship that maps the speciﬁc manner of dying that constitutes being starved to death (we say that “starving” is a map-transcending entity). Yet one could argue that the manner of Connors’s death is a crucial part of the informational contribution of (1). A possible solution would be to create a new view-speciﬁc mapping that goes from the form of killing involved in starving to death to some process in sport, but such enrichment of mappings would be  
Live closed-captions for deaf and hard of hearing audiences are currently produced by stenographers, or by voice writers using speech recognition. Both techniques can produce captions with errors. We are currently developing a correction module that allows a user to intercept the real-time caption stream and correct it before it is broadcast. We report results of preliminary experiments on correction rate and actual user performance using a prototype correction module connected to the output of a speech recognition captioning system. 
 also enables users to easily come up with answer  This paper proposes the idea of ranking definitions of a person (a set of biographical facts) to automatically generate “Who is this?” quizzes. The deﬁnitions are ordered according to how difﬁcult they make it to name the person. Such ranking would enable users to interactively learn about a person through dialogue with a system with improved understanding and lasting motivation, which is useful for educational systems. In our approach, we train a ranker that learns from data the appropriate ranking of deﬁnitions based on features that encode the importance of keywords in a deﬁnition as well as its content. Experimental results show that our approach is signiﬁcantly better in ranking deﬁnitions than baselines that use conventional information retrieval measures such as tf*idf and pointwise mutual information (PMI). 
The aim of this paper is to develop animated agents that can control multimodal instruction dialogues by monitoring user’s behaviors. First, this paper reports on our Wizard-of-Oz experiments, and then, using the collected corpus, proposes a probabilistic model of fine-grained timing dependencies among multimodal communication behaviors: speech, gestures, and mouse manipulations. A preliminary evaluation revealed that our model can predict a instructor’s grounding judgment and a listener’s successful mouse manipulation quite accurately, suggesting that the model is useful in estimating the user’s understanding, and can be applied to determining the agent’s next action. 
Assessing the quality of user generated content is an important problem for many web forums. While quality is currently assessed manually, we propose an algorithm to assess the quality of forum posts automatically and test it on data provided by Nabble.com. We use state-of-the-art classiﬁcation techniques and experiment with ﬁve feature classes: Surface, Lexical, Syntactic, Forum speciﬁc and Similarity features. We achieve an accuracy of 89% on the task of automatically assessing post quality in the software domain using forum speciﬁc features. Without forum speciﬁc features, we achieve an accuracy of 82%.  percentage of manually rated posts is very low (0.1% in Nabble). Departing from this, the main idea explored in the present paper is to investigate the feasibility of automatically assessing the perceived quality of user generated content. We test this idea for online forum discussions in the domain of software. The perceived quality is not an objective measure. Rather, it models how the community at large perceives post quality. We choose a machine learning approach to automatically assess it. Our main contributions are: (1) An algorithm for automatic quality assessment of forum posts that learns from human ratings. We evaluate the system on online discussions in the software domain. (2) An analysis of the usefulness of different classes of features for the prediction of post quality.  
 model can improve recognition, where the amount  This paper presents the application of WordNet-based semantic relatedness measures to Automatic Speech Recognition (ASR) in multi-party meetings. Different word-utterance context relatedness measures and utterance-coherence measures are deﬁned and applied to the rescoring of N best lists. No signiﬁcant improvements in terms of Word-Error-Rate (WER) are achieved compared to a large word-based ngram baseline model. We discuss our results  of improvement varies with context length and sentence length. Thereby it was shown that these models can make use of long-term information. In this paper the best performing measures from (Pucher, 2005), which outperform baseline models on word prediction for conversational telephone speech are used for Automatic Speech Recognition (ASR) in multi-party meetings. Thereby we want to investigate if WordNet-based models can be used for rescoring of ‘real’ N -best lists in a difﬁcult task.  and the relation to other work that achieved an improvement with such models for simpler tasks.  1.1 Word prediction by semantic similarity The standard n-gram approach in language modeling for speech recognition cannot cope with  
An emotion lexicon is an indispensable resource for emotion analysis. This paper aims to mine the relationships between words and emotions using weblog corpora. A collocation model is proposed to learn emotion lexicons from weblog articles. Emotion classification at sentence level is experimented by using the mined lexicons to demonstrate their usefulness. 
 2 Two Issues  For natural language understanding, it is essential to reveal semantic relations between words. To date, only the IS-A relation has been publicly available. Toward deeper natural language understanding, we semiautomatically constructed the domain dictionary that represents the domain relation between Japanese fundamental words. This  You have to address two issues. One is what domains to assume, and the other is how to associate words with domains without document collections. The former is paraphrased as how people categorize the real world, which is really a hard problem. In this study, we avoid being too involved in the problem and adopt a simple domain system that most people can agree on, which is as follows:  is the ﬁrst Japanese domain resource that is fully available. Besides, our method does not require a document collection, which is indispensable for keyword extraction techniques but is hard to obtain. As a task-based  CULTURE RECREATION SPORTS HEALTH  LIVING DIET TRANSPORTATION EDUCATION  SCIENCE BUSINESS MEDIA GOVERNMENT  evaluation, we performed blog categorization. Also, we developed a technique for estimating the domain of unknown words.  It has been created based on web directories such as Open Directory Project with some adjustments. In addition, NODOMAIN was prepared for those  
At least two kinds of relations exist among related words: taxonomical relations and thematic relations. Both relations identify related words useful to language understanding and generation, information retrieval, and so on. However, although words with taxonomical relations are easy to identify from linguistic resources such as dictionaries and thesauri, words with thematic relations are difficult to identify because they are rarely maintained in linguistic resources. In this paper, we sought to extract thematically (non-taxonomically) related word sets among words in documents by employing case-marking particles derived from syntactic analysis. We then verified the usefulness of word sets with non-taxonomical relation that seems to be a thematic relation for information retrieval. 1. Introduction Related word sets are useful linguistic resources for language understanding and generation, information retrieval, and so on. In previous research on natural language processing, many methodologies for extracting various relations from corpora have been developed, such as the “is-a” relation (Hearst 1992), “part-of” relation (Berland and Charniak 1999), causal relation (Girju 2003), and entailment relation (Geffet and Dagan 2005). Related words can be used to support retrieval in order to lead users to high-quality information. One simple method is to provide additional words related to the key words users have input, such as an input support function within the Google search  engine. What kind of relation between the key words that have been input and the additional word is effective for information retrieval? As for the relations among words, at least two kinds of relations exist: the taxonomical relation and the thematic relation. The former is a relation representing the physical resemblance among objects, which is typically a semantic relation such as a hierarchal, synonymic, or antonymic relation; the latter is a relation between objects through a thematic scene, such as “milk” and “cow” as recollected in the scene “milking a cow,” and “milk” and “baby,” as recollected in the scene “giving baby milk,” which include causal relation and entailment relation. Wisniewski and Bassok (1999) showed that both relations are important in recognizing those objects. However, while taxonomical relations are comparatively easy to identify from linguistic resources such as dictionaries and thesauri, thematic relations are difficult to identify because they are rarely maintained in linguistic resources. In this paper, we sought to extract word sets with a thematic relation from documents by employing case-marking particles derived from syntactic analysis. We then verified the usefulness of word sets with non-taxonomical relation that seems to be a thematic relation for information retrieval. 2. Method In order to derive word sets that direct users to obtain information, we applied a method based on the Complementary Similarity Measure (CSM), which can determine a relation between two words in a corpus by estimating inclusive relations between two vectors representing each appearance pattern for each words (Yamamoto et al. 2005).  141 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 141–144, Prague, June 2007. c 2007 Association for Computational Linguistics  We first extracted word pairs having an inclusive relation between the words by calculating the CSM values. Extracted word pairs are expressed by a tuple <wi, wj>, where CSM(Vi, Vj) is greater than CSM(Vj, Vi) when words wi and wj have each appearance pattern represented by each binary vector Vi and Vj. Then, we connected word pairs with CSM values greater than a certain threshold and constructed word sets. A feature of the CSM-based method is that it can extract not only pairs of related words but also sets of related words because it connects tuples consistently. Suppose we have <A, B>, <B, C>, <Z, B>, <C, D>, <C, E>, and <C, F> in the order of their CSM values, which are greater than the threshold. For example, let <B, C> be an initial word set {B, C}. First, we find the tuple with the greatest CSM value among the tuples in which the word C at the tail of the current word set is the left word, and connect the right word behind C. In this example, word “D” is connected to {B, C} because <C, D> has the greatest CSM value among the three tuples <C, D>, <C, E>, and <C, F>, making the current word set {B, C, D}. This process is repeated until no tuples exist. Next, we find the tuple with the greatest CSM value among the tuples in which the word B at the head of the current word set is the right word, and connect the left word before B. This process is repeated until no tuples exist. In this example, we obtain the word set {A, B, C, D}. Finally, we removed ones with a taxonomical relation by using thesaurus. The rest of the word sets have a non-taxonomical relation — including a thematic relation — among the words. We then extracted those word sets that do not agree with the thesaurus as word sets with a thematic relation.  We extracted word sets by utilizing inclusive relations of the appearance pattern between words based on a modified/modifier relationship in documents. The Japanese language has casemarking particles that indicate the semantic relation between two elements in a dependency relation. Then, we collected from documents dependency relations matching the following five patterns; “A <no (of)> B,” “P <wo (object)> V,” “Q <ga (subject)> V,” “R <ni (dative)> V,” and “S <ha (topic)> V,” where A, B, P, Q, R, and S are nouns, V is a verb, and <X> is a case-marking particle. From such collected dependency relations, we compiled the following types of experimental data; NN-data based on co-occurrence between nouns for each sentence, NV-data based on a dependency relation between noun and verb for each case-marking particle <wo>, <ga>, <ni>, and <ha>, and SO-data based on a collocation between subject and object that depends on the same verb V as the subject. These data are represented with a binary vector which corresponds to the appearance pattern of a noun and these vectors are used as arguments of CSM. We translated descriptors in the MeSH thesaurus into Japanese and used them as Japanese medical terms. The number of terms appearing in this experiment is 2,557 among them. We constructed word sets consisting of these medical terms. Then, we chose 977 word sets consisting of three or more terms from them, and removed word sets with a taxonomical relation from them with the MeSH thesaurus in order to obtain the rest 847 word sets as word sets with a thematic relation. 4. Verification  3. Experiment In our experiment, we used domain-specific Japanese documents within the medical domain (225,402 sentences, 10,144 pages, 37MB) gathered from the Web pages of a medical school and the 2005 Medical Subject Headings (MeSH) thesaurus 1 . Recently, there has been a study on query expansion with this thesaurus as domain information (Friberg 2007). 
 mantics of the descriptions should be defined based on a shared ontology.  This paper introduces conceptual framework of an ontology for describing linguistic services on network-based language infrastructures. The ontology defines a taxonomy of processing resources and the associated static language resources. It also develops a sub-ontology for abstract linguistic objects such as expression, meaning, and description; these help define functionalities of a linguistic service. The proposed ontology is expected to serve as a solid basis for the interoperability of technical elements in language infrastructures. 
 larity measure could be deﬁned so that, for exam-  The ability to detect similarity in conjunct heads is potentially a useful tool in helping to disambiguate coordination structures - a difﬁcult task for parsers. We propose a distributional measure of similarity designed for such a task. We then compare several different measures of word similarity by testing whether they can empirically detect similarity in the head nouns of noun phrase conjuncts in the Wall Street Journal (WSJ) treebank. We demonstrate that several measures of word similarity can successfully detect conjunct head similarity and suggest that the measure proposed in this paper is the most appropriate for this task.  ple: sim(executives, spouses) > sim(busloads, spouses) then it is potentially useful for coordination disambiguation. The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in (Riloff and Shepherd, 1997) and used effectively to automatically cluster semantically similar words (Roark and Charniak, 1998; Caraballo, 1999; Widdows and Dorow, 2002). The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by Resnik (1999) who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase. In this paper we look at different measures of  
Identiﬁcation of transliterated names is a particularly difﬁcult task of Named Entity Recognition (NER), especially in the Chinese context. Of all possible variations of transliterated named entities, the difference between PRC and Taiwan is the most prevalent and most challenging. In this paper, we introduce a novel approach to the automatic extraction of diverging transliterations of foreign named entities by bootstrapping cooccurrence statistics from tagged and segmented Chinese corpus. Preliminary experiment yields promising results and shows its potential in NLP applications. 
This paper proposes a supervised learning method for detecting a semantic relation between a given pair of named entities, which may be located in different sentences. The method employs newly introduced contextual features based on centering theory as well as conventional syntactic and word-based features. These features are organized as a tree structure and are fed into a boosting-based classiﬁcation algorithm. Experimental results show the proposed method outperformed prior methods, and increased precision and recall by 4.4% and 6.7%. 
This paper describes a work in progress aiming at linking the two largest Italian lexical-semantic databases ItalWordNet and PAROLE-SIMPLE-CLIPS. The adopted linking methodology, the software tool devised and implemented for this purpose and the results of the first mapping phase regarding 1stOrderEntities are illustrated here. 
 English. Little work has been done for other lan-  We apply pattern-based methods for collecting hypernym relations from the web. We compare our approach with hypernym extraction from morphological clues and from large text corpora. We show that the abundance of available data on the web enables obtaining good results with relatively unsophisticated techniques.  guages. IJzereef (2004) used ﬁxed patterns to extract Dutch hypernyms from text and encyclopedias. Van der Plas and Bouma (2005) employed noun distribution characteristics for extending the Dutch part of EuroWordNet. In earlier work, different techniques have been applied to large and very large text corpora. Today, the web contains more data than the largest available text corpus. For this reason, we are interested in em-  
 As far as possible GOLD uses language-neutral  The paper presents an OWL ontology for HPSG. The HPSG ontology is integrated with an existing OWL ontology, GOLD, as a community of practice extension. The basic ideas are illustrated by visualizations of type hierarchies for parts of speech.  and theory-neutral terminology. For instance, parts of speech are subclasses of gold:GrammaticalUnit as shown in Figure 1. As GOLD is language-neutral, a wide range of parts of speech are included. For example, both Preposition and Postposition are included as subclasses of Adposition. The classes in the OWLViz graphical visualization (on the right in  
 automatically from raw text that not only dupli-  This paper describes a fully automatic twostage machine learning architecture that learns temporal relations between pairs of events. The ﬁrst stage learns the temporal attributes of single event descriptions, such as tense, grammatical aspect, and aspectual class. These imperfect guesses, combined with other linguistic features, are then used in a second stage to classify the temporal relationship between two events. We present both an analysis of our new features and re-  cates this performance, but surpasses its accuracy by 3%. We do so through advanced linguistic features and a surprising ﬁnding that using automatic rather than hand-labeled tense and aspect knowledge causes only a slight performance degradation. We brieﬂy describe current work on temporal ordering in section 2. Section 4 describes the ﬁrst stage of basic temporal extraction, followed by a full description of the second stage in 5. The evaluation and results on Timebank then follow in section 6. 2 Previous Work  sults on the TimeBank Corpus that is 3% higher than previous work that used perfect human tagged features.  Mani et. al (2006) built a MaxEnt classiﬁer that assigns each pair of events one of 6 relations from an augmented Timebank corpus. Their classiﬁer relies  
Richard Zens RWTH Aachen4  Marcello Federico Nicola Bertoldi ITC-irst2  Brooke Cowan Wade Shen Christine Moran MIT3  Chris Dyer  Ondřej Bojar  University of Maryland5 Charles University6  Alexandra Constantin Williams College7  Evan Herbst Cornell8  
Data sparseness is one of the factors that degrade statistical machine translation (SMT). Existing work has shown that using morphosyntactic information is an effective solution to data sparseness. However, fewer efforts have been made for Chinese-to-English SMT with using English morpho-syntactic analysis. We found that while English is a language with less inﬂection, using English lemmas in training can signiﬁcantly improve the quality of word alignment that leads to yield better translation performance. We carried out comprehensive experiments on multiple training data of varied sizes to prove this. We also proposed a new effective linear interpolation method to integrate multiple homologous features of translation models. 
Event-based summarization extracts and organizes summary sentences in terms of the events that the sentences describe. In this work, we focus on semantic relations among event terms. By connecting terms with relations, we build up event term graph, upon which relevant terms are grouped into clusters. We assume that each cluster represents a topic of documents. Then two summarization strategies are investigated, i.e. selecting one term as the representative of each topic so as to cover all the topics, or selecting all terms in one most significant topic so as to highlight the relevant information related to this topic. The selected terms are then responsible to pick out the most appropriate sentences describing them. The evaluation of clustering-based summarization on DUC 2001 document sets shows encouraging improvement over the well-known PageRank-based summarization. 
 We present an approach to MT between Turkic languages and present results from an implementation of a MT system from Turkmen to Turkish. Our approach relies on ambiguous lexical and morphological transfer augmented with target side rule-based repairs and rescoring with statistical language models.  2 Related Work Studies on machine translation between close languages are generally concentrated around certain Slavic languages (e.g., Czech→Slovak, Czech→Polish, Czech→Lithuanian (Hajic et al., 2003)) and languages spoken in the Iberian Peninsula (e.g., Spanish↔Catalan (Canals et al., 2000), Spanish↔Galician (Corbi-Bellot et al., 2003) and  Spanish↔Portugese (Garrido-Alenda et al., 2003).  
 marization, with generally better results from log-  The increasing complexity of summarization systems makes it difﬁcult to analyze exactly which modules make a difference in performance. We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization: raw frequency (word probability) and log-likelihood ratio. We demonstrate that the advantages of log-likelihood ratio come from its known distributional properties which allow for the identiﬁcation of a set of words that in its entirety deﬁnes the aboutness of the input. We also ﬁnd that LLR is more suitable for query-focused summarization since, unlike raw frequency, it is more sensitive to the integration of the information need deﬁned by the user.  likelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More signiﬁcantly, previous studies show that generic and focused systems perform very similarly to each other in query-focused summarization (Nenkova, 2005) and it is of interest to ﬁnd out why. To address these questions we examine the two  
 pivot language, and a large pivot-destination dictio-  We propose a novel method to expand a small existing translation dictionary to a large translation dictionary using a pivot language. Our method depends on the assumption that it is possible to ﬁnd a pivot language for a given language pair on condition that there are both a large translation dictionary from the source language to the pivot language, and a large translation dictionary from the pivot language to the destination language. Experiments that expands the Indonesian-Japanese dictionary using the English language as a pivot language shows that the proposed method can improve performance of a real CLIR system. 
We present a formalization of dependency labeling with Integer Linear Programming. We focus on the integration of subcategorization into the decision making process, where the various subcategorization frames of a verb compete with each other. A maxi-  tion. More formally, the dependency lab¢el¡ ing probal e¦nmd¡Pi§sP: £©gc¤ ihvu)ennwkasit1hs,ea£¥ntd¤ een,pceleanbdweelinthaclyl(irp)eavliaertsribo(sn¦, (¡¨in§c,lu£©(di¤ii)nNg Pa class for the null assignment) such that all chunks get attached and for each verb exactly one subcategorization frame is instantiated.  mum entropy model provides the weights for ILP optimization. 
 instead of phrase-structure parsing. Usual depen-  Dependency structures do not have the information of phrase categories in phrase structure grammar. Thus, dependency parsing relies heavily on the lexical information of words. This paper discusses our investigation into the effectiveness of lexicalization in dependency parsing. Speciﬁcally, by restricting the degree of lexicalization in the training phase of a parser, we examine the change in the accuracy of dependency relations. Experimental results indicate that minimal or low lexicalization is sufﬁcient for parsing accuracy. 
In the world of non-proprietary NLP software the standard, and perhaps the best, HMM-based POS tagger is TnT (Brants, 2000). We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT’s peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages. We present HunPos1, a free and open source (LGPL-licensed) alternative, which can be tuned by the user to fully utilize the potential of HMM architectures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process.  ing system. Though taggers based on dependency networks (Toutanova et al., 2003), SVM (Gime´nez and Ma`rquez, 2003), MaxEnt (Ratnaparkhi, 1996), CRF (Smith et al., 2005), and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer. A ubiquitous problem in HMM tagging originates from the standard way of calculating lexical probabilities by means of a lexicon generated during training. In highly inﬂecting languages considerably more unseen words will be present in the test data than in more isolating languages, which largely accounts for the drop in the performance of n-gram taggers when moving away from English. To mitigate the effect one needs a morphological dictionary (Hajicˇ et al., 2001) or a morphological analyzer (Hakkani-Tu¨r et al., 2000), but if the implementation source is closed there is no handy way to incorporate morphological knowledge in the tagger.  0 Introduction  Even without a formal survey it is clear that TnT (Brants, 2000) is used widely in research labs throughout the world: Google Scholar shows over 400 citations. For research purposes TnT is freely available, but only in executable form (closed source). Its greatest advantage is its speed, important both for a fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer-  The paper is structured as follows. In Section 1 we present our own system, HunPos, while in Section 2 we describe some of the implementation details of TnT that we believe inﬂuence the performance of a HMM based tagging system. We evaluate the system and compare it to TnT on a variety of tasks in Section 3. We don’t necessarily consider HunPos to be signiﬁcantly better than TnT, but we argue that we could reach better results, and so could others coming after us, because the system is open to explore all kinds of ﬁne-tuning strategies. Some  1http://mokk.bme.hu/resources/hunpos/  concluding remarks close the paper in Section 4.  209 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 209–212, Prague, June 2007. c 2007 Association for Computational Linguistics  
In this paper we present several extensions of MARIE1, a freely available N -gram-based statistical machine translation (SMT) decoder. The extensions mainly consist of the ability to accept and generate word graphs and the introduction of two new N -gram models in the loglinear combination of feature functions the decoder implements. Additionally, the decoder is enhanced with a caching strategy that reduces the number of N -gram calls improving the overall search efﬁciency. Experiments are carried out over the Eurpoean Parliament Spanish-English translation task. 
 2 Hybrid Method for Word Segmentation and POS Tagging  In this paper, we present a hybrid method for word segmentation and POS tagging. The target languages are those in which word boundaries are ambiguous, such as Chinese and Japanese. In the method, word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously. Experimental results on multiple corpora show that the integrated method has high accuracy. 
 HMM model to achieve higher accuracy (Cutting  et al., 1992). The semi-supervised model de-  This paper describes our work on build-  scribed in Cutting et al. (1992), makes use of  ing Part-of-Speech (POS) tagger for  both labeled training text and some amount of  Bengali. We have use Hidden Markov  unlabeled text. Incorporating a diverse set of  Model (HMM) and Maximum Entropy  overlapping features in a HMM-based tagger is  (ME) based stochastic taggers. Bengali is  difficult and complicates the smoothing typically  a morphologically rich language and our  used for such taggers. In contrast, methods based  taggers make use of morphological and  on Maximum Entropy (Ratnaparkhi, 1996),  contextual information of the words.  Conditional Random Field (Shrivastav, 2006)  Since only a small labeled training set is  etc. can deal with diverse, overlapping features.  available (45,000 words), simple stochastic approach does not yield very good results. In this work, we have studied the  1.1 Previous Work on Indian Language POS Tagging  effect of using a morphological analyzer  Although some work has been done on POS tag-  to improve the performance of the tagger.  ging of different Indian languages, the systems  We find that the use of morphology helps  are still in their infancy due to resource poverty.  improve the accuracy of the tagger espe-  Very little work has been done previously on  cially when less amount of tagged cor-  POS tagging of Bengali. Bengali is the main  pora are available.  language spoken in Bangladesh, the second most  
 of Web 2.0. Such documents do not use controlled  The amount of documents directly published by end users is increasing along with the growth of Web 2.0. Such documents often contain spoken-style expressions, which are difﬁcult to analyze using conventional parsers. This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions. One characteristic of our method is that it can parse selfdependent (independent) segments using sequential labeling.  written language and contain ﬁllers and emoticons. This implies that analyzing such documents is difﬁcult for conventional parsers. This paper presents a new method of Japanese dependency parsing that utilizes sequential labeling based on conditional random ﬁelds (CRFs) in order to analyze semi-spoken language. Concretely, sequential labeling assigns each segment a dependency label that indicates its relative position of dependency. If the label set includes self-dependency, the ﬁllers and emoticons would be analyzed as segments depending on themselves. Therefore, since it  
 most relaxed IBM Model-1, which assumes that any  We present a general framework to incorporate prior knowledge such as heuristics or linguistic features in statistical generative word alignment models. Prior knowledge plays a role of probabilistic soft constraints between bilingual word pairs that shall be used to guide word alignment model training. We investigate knowledge that can be derived automatically from entropy principle and bilingual latent semantic analysis and show how they can be applied to improve translation performance.  source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al., 1996), or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models (Brown et al., 1993). Following the path, we shall put more constraints on word alignment models and investigate ways of implementing them in a statistical framework. We have seen examples showing that names tend to align to names and function words are likely to be linked to function words. These observations are  
We present a global discriminative statistical word order model for machine translation. Our model combines syntactic movement and surface movement information, and is discriminatively trained to choose among possible word orders. We show that combining discriminative training with features to detect these two different kinds of movement phenomena leads to substantial improvements in word ordering performance over strong baselines. Integrating this word order model in a baseline MT system results in a 2.4 points improvement in BLEU for English to Japanese translation. 
 and Dorr, 2006). However, few of these methods  Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efﬁciency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacriﬁcing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.  have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daume´ III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and word alignments. Rules are extracted from target side constituents that can be projected onto contiguous spans of the source sentence via the word alignment. Constituents that project onto non-contiguous spans of the source sentence do not yield transducer rules themselves, and can only be incorporated into larger  
 performance. We will show how such corpora can  Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language. In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality. We propose several algorithms with this aim, and present the strengths and weaknesses of each one. We present detailed experimental evaluations on the French–English EuroParl data set and on data from the NIST Chinese–English largedata track. We show a signiﬁcant improvement in translation quality on both tasks.  be used to achieve higher translation quality. Even if large amounts of bilingual text are given, the training of the statistical models usually suffers from sparse data. The number of possible events, i.e. phrase pairs or pairs of subtrees in the two languages, is too big to reliably estimate a probability distribution over such pairs. Another problem is that for many language pairs the amount of available bilingual text is very limited. In this work, we will address this problem and propose a general framework to solve it. Our hypothesis is that adding information from source language text can also provide improvements. Unlike adding target language text, this hypothesis is a natural semi-supervised learning problem. To tackle this problem, we propose algorithms for transductive semi-supervised learn-  
 To perform translation, state-of-the-art MT sys-  Recent research presents conﬂicting evidence on whether word sense disambiguation (WSD) systems can help to improve the performance of statistical machine translation (MT) systems. In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero. We show for the ﬁrst time that integrating a WSD system improves the performance of a state-ofthe-art statistical MT system on an actual translation task. Furthermore, the improvement is statistically signiﬁcant. 
We present a novel approach to the word sense disambiguation problem which makes use of corpus-based evidence combined with background knowledge. Employing an inductive logic programming algorithm, the approach generates expressive disambiguation rules which exploit several knowledge sources and can also model relations between them. The approach is evaluated in two tasks: identification of the correct translation for a set of highly ambiguous verbs in EnglishPortuguese translation and disambiguation of verbs from the Senseval-3 lexical sample task. The average accuracy obtained for the multilingual task outperforms the other machine learning techniques investigated. In the monolingual task, the approach performs as well as the state-of-the-art systems which reported results for the same set of verbs. 
 contains sentences from two different corpora,  When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed. This highlights the importance of domain adaptation for word sense disambiguation. In this paper, we ﬁrst show that an active learning approach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.  namely Brown Corpus (BC) and Wall Street Journal (WSJ). They found that training a WSD system on one part (BC or WSJ) of the DSO corpus, and applying it to the other, can result in an accuracy drop of more than 10%, highlighting the need to perform domain adaptation of WSD systems to new domains. Escudero et al. (2000) pointed out that one of the reasons for the drop in accuracy is the difference in sense priors (i.e., the proportions of the different senses of a word) between BC and WSJ. When the authors assumed they knew the sense priors of each word in BC and WSJ, and adjusted these two datasets such that the proportions of the different senses of each word were the same between BC and WSJ, accuracy improved by 9%. In this paper, we explore domain adaptation of  
 2004) but to the underlying ontological structure it-  Human categorization is neither a binary nor a context-free process. Rather, some concepts are better examples of a category than others, while the criteria for category membership may be satisﬁed to different degrees by different concepts in different contexts. In light of these empirical facts, WordNet’s static category structure appears both excessively rigid and unduly fragile for processing real texts. In this paper we describe a syntagmatic, corpus-based approach to redeﬁning WordNet’s categories in a functional, gradable and context-sensitive fashion. We describe how the diagnostic properties for these deﬁnitions are automatically acquired from the web, and how the increased ﬂexibility in categorization that arises from these redeﬁnitions offers a robust account of metaphor comprehension in the mold of Glucksberg’s (2001) theory of category-inclusion. Furthermore, we demonstrate how this competence with ﬁgurative categorization can effectively be governed by automatically-generated ontological constraints, also acquired from the web.  self (see Cimiano, Hotho and Staab, 2005). The most revealing variations are syntagmatic in nature, which is to say, they look beyond individual word forms to larger patterns of contiguous usage (Hanks, 2004). In most contexts, the similarity between chocolate, say, and a narcotic like heroin will meagerly reﬂect the simple ontological fact that both are kinds of substances; certainly, taxonomic measures of similarity as discussed in Budanitsky and Hirst (2006) will capture little more than this commonality. However, in a context in which the addictive properties of chocolate are very salient (e.g., an online dieting forum), chocolate is more likely to be categorized as a drug and thus be considered more similar to heroin. Look, for instance, at the similar ways in which these words can be used: one can be ”chocolate-crazed” or ”chocolate-addicted” and suffer ”chocolate-induced” symptoms (e.g., each of these uses can be found in the pages of Wikipedia). In a context that gives rise to these expressions, it is unsurprising that chocolate should appear altogether more similar to a harmful narcotic. In this paper we computationally model this idea that language use reﬂects category structure. As noted by De Leenheer and de Moor (2005), ontologies are lexical representations of concepts, so we  
 at all pairs of features (typically several hundred) is  A standard form of analysis for linguistic typology is the universal implication. These implications state facts about the range of extant languages, such as “if objects come after verbs, then adjectives come after nouns.” Such implications are typically discovered by painstaking hand analysis over a small sample of languages. We propose a computational model for assisting at this process. Our model is able to discover both well-known implications as well as some novel implications that deserve further study. Moreover, through a careful application of hierarchical analysis, we are able to cope with the well-known sampling problem: languages are not independent.  virtually impossible by hand. Moreover, it is insufﬁcient to simply look at counts. For instance, results presented in the form “verb precedes object implies prepositions in 16/19 languages” are nonconclusive. While compelling, this is not enough evidence to decide if this is a statistically well-founded implication. For one, maybe 99% of languages have prepositions: then the fact that we’ve achieved a rate of 84% actually seems really bad. Moreover, if the 16 languages are highly related historically or areally (geographically), and the other 3 are not, then we may have only learned something about geography. In this work, we propose a statistical model that deals cleanly with these difﬁculties. By building a computational model, it is possible to apply it to a very large typological database and search over many thousands of pairs of features. Our model  
 The most widely used LM is a probabilistic lan-  guage model (PLM), which assigns a probability to  In this paper, we propose a novel discrim-  a sentence or a word sequence. In particular, N-  inative language model, which can be ap-  grams with maximum likelihood estimation (NLMs)  plied quite generally. Compared to the  are often used. Although NLMs are simple, they are  well known N-gram language models, dis-  effective for many applications.  criminative language models can achieve more accurate discrimination because they can employ overlapping features and nonlocal information. However, discriminative language models have been used only for re-ranking in speciﬁc applications because negative examples are not available. We propose sampling pseudo-negative examples taken from probabilistic language models. However, this approach requires prohibitive computational cost if we are dealing with quite a few features and training samples. We tackle the problem by estimating the latent information in sentences using a semiMarkov class model, and then extracting features from them. We also use an online margin-based algorithm with efﬁcient kernel computation. Experimental results show that pseudo-negative examples can be treated as real negative examples and our model can classify these sentences correctly.  However, NLMs cannot determine correctness of a sentence independently because the probability depends on the length of the sentence and the global frequencies of each word in it. For example, Ô´Ë½µ Ô´Ë¾µ, where Ô´Ëµ is the probability of a sentence Ë given by an NLM, does not always mean that Ë¾ is more correct, but instead could occur when Ë¾ is shorter than Ë½, or if Ë¾ has more common words than Ë½. Another problem is that NLMs cannot handle overlapping information or non-local information easily, which is important for more accurate sentence classiﬁcation. For example, a NLM could assign a high probability to a sentence even if it does not have a verb. Discriminative language models (DLMs) have been proposed to classify sentences directly as correct or incorrect (Gao et al., 2005; Roark et al., 2007), and these models can handle both non-local and overlapping information. However DLMs in previous studies have been restricted to speciﬁc applications. Therefore the model cannot be used for  
This paper describes a method for automatically learning effective dialogue strategies, generated from a library of dialogue content, using reinforcement learning from user feedback. This library includes greetings, social dialogue, chit-chat, jokes and relationship building, as well as the more usual clariﬁcation and veriﬁcation components of dialogue. We tested the method through a motivational dialogue system that encourages take-up of exercise and show that it can be used to construct good dialogue strategies with little effort. 
We examine the effect of contextual and acoustic cues in the disambiguation of three discourse-pragmatic functions of the word okay. Results of a perception study show that contextual cues are stronger predictors of discourse function than acoustic cues. However, acoustic features capturing the pitch excursion at the right edge of okay feature prominently in disambiguation, whether other contextual cues are present or not.  uses of individual cue phrases can be distinguished by variation in the prosody with which they are realized. For example, (Hirschberg and Litman, 1993) found that cue phrases in general could be disambiguated between their ‘semantic’ and their ‘discourse marker’ uses in terms of the type of pitch accent borne by the cue phrase, the position of the phrase in the intonational phrase, and the amount of additional information in the phrase. Despite the frequence of the word okay in natural dialogues, relatively little attention has been paid to the relationship between its use and its prosodic realization.  
 rent spoken dialogue systems do not yet yield long,  Task-solving in dialogue depends on the linguistic alignment of the interlocutors, which Pickering & Garrod (2004) have suggested to be based on mechanistic repetition effects. In this paper, we seek conﬁrmation of this hypothesis by looking at repetition in corpora, and whether repetition is correlated with task success. We show that the relevant repetition tendency is based on slow adaptation rather than short-term priming and demonstrate that lexical and syntac-  syntactically complex conversations. In this paper, we use syntactic and lexical features to predict task success in an environment where we assume no speaker model, no semantic information and no information typical for a human-computer dialogue system, e.g., ASR conﬁdence. The features we use are based on a psychological theory, linking alignment between dialogue participants to low-level syntactic priming. An examination of this priming reveals differences between short-term and long-term effects.  tic repetition is a reliable predictor of task success given the ﬁrst ﬁve minutes of a taskoriented dialogue.  1.1 Repetition supports dialogue In their Interactive Alignment Model (IAM), Pickering and Garrod (2004) suggest that dialogue be-  
 reference. Once a referent has been identiﬁed, the  We present an implemented system for the resolution of it, this, and that in transcribed multi-party dialog. The system handles NP-anaphoric as well as discoursedeictic anaphors, i.e. pronouns with VP antecedents. Selectional preferences for NP or VP antecedents are determined on the basis of corpus counts. Our results show that the system performs signiﬁcantly better than a recency-based baseline.  pronoun is resolved by linking it to one of its antecedents, i.e. one of the referent’s earlier mentions. For humans, identiﬁcation of a pronoun’s referent is often easy: it1, it2, and it6 are probably used to refer to the text on the web pages, while it4 is probably used to refer to reading this text. Humans also have no problem determining that it5 is not a normal pronoun at all. In other cases, resolving a pronoun is difﬁcult even for humans: this3 could be used to refer to either reading or changing the text on the web pages. The pronoun is ambiguous because evi-  
This paper presents a comparative study of five parameter estimation algorithms on four NLP tasks. Three of the five algorithms are well-known in the computational linguistics community: Maximum Entropy (ME) estimation with L2 regularization, the Averaged Perceptron (AP), and Boosting. We also investigate ME estimation with L1 regularization using a novel optimization algorithm, and BLasso, which is a version of Boosting with Lasso (L1) regularization. We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model (LM) adaptation task. Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a Conditional Markov Model (CMM) for part of speech tagging and a Conditional Random Field (CRF) for Chinese word segmentation. Our experiments show that across tasks, three of the estimators — ME estimation with L1 or L2 regularization, and AP — are in a near statistical tie for first place. 
 types” signiﬁcantly improves over a fully unsuper-  We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar.  vised PCFG induction model (their prototypes were formed by sequences of POS tags; for example, prototypical NPs were DT NN, JJ NN). In this paper, we present a new grammar formalism and a new learning method which together address the problem of learning a syntactic-semantic grammar in the presence of a representative sample of strings annotated with their semantics, along with minimal assumptions about syntax (such as syntactic categories). The semantic representation is an  
 of characters which have themselves been seen as  Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding ﬁnding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder. Closed tests on the ﬁrst and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.  words; here an automatic segmentor may split the  OOV word into individual single-character words.  Typical examples of unseen words include Chinese  names, translated foreign names and idioms.  The segmentation of known words can also be  × ambiguous. For example, “  ” should be “  (here) (ﬂour)” in the sentence “  ó ” (ﬂour and rice are expensive here) or “ (here)  (inside)” in the sentence “  ” (it’s  cold inside here). The ambiguity can be resolved  with information about the neighboring words. In  ¨ ï comparison, for the sentences “  ”,  ¨ possible segmentations include “ (the discus-  ï sion) (will) (very)  (be successful)” and  ¨ ï “  (the discussion meeting) (very) (be  successful)”. The ambiguity can only be resolved  with contextual information outside the sentence.  Human readers often use semantics, contextual in-  formation about the document and world knowledge  to resolve segmentation ambiguities.  
 applies discriminative learning methods to pairs of  We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.  mentions, using features which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002). Although such approaches have been successful, they have several liabilities. First, rich features require plentiful labeled data, which we do not have for coreference tasks in most domains and languages. Second, coreference is inherently a clustering or partitioning task. Naive pairwise methods can and do fail to produce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a ﬁnal coherent clustering. Nonetheless, all these systems  
This paper proposes a novel method for phrase-based statistical machine translation by using pivot language. To conduct translation between languages Lf and Le with a small bilingual corpus, we bring in a third language Lp, which is named the pivot language. For Lf-Lp and Lp-Le, there exist large bilingual corpora. Using only Lf-Lp and Lp-Le bilingual corpora, we can build a translation model for Lf-Le. The advantage of this method lies in that we can perform translation between Lf and Le even if there is no bilingual corpus available for this language pair. Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the translation quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 
 rithm’s lexicon. Often, a signiﬁcant proportion of  We propose a bootstrapping approach to training a memoriless stochastic transducer for the task of extracting transliterations from an English-Arabic bitext. The transducer learns its similarity metric from the data in the bitext, and thus can function directly on strings written in different writing scripts without any additional language knowledge. We show that this bootstrapped transducer performs as well or better than a model designed speciﬁcally to detect Arabic-English transliterations.  
This paper studies the problem of identifying erroneous/correct sentences. The problem has important applications, e.g., providing feedback for writers of English as a Second Language, controlling the quality of parallel bilingual sentences mined from the Web, and evaluating machine translation results. In this paper, we propose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising. 
In this paper, we describe our experiences in extending a standard cross-language information retrieval (CLIR) approach which uses parallel aligned corpora and Latent Semantic Indexing. Most, if not all, previous work which follows this approach has focused on bilingual retrieval; two examples involve the use of FrenchEnglish or English-Greek parallel corpora. Our extension to the approach is ‘massively parallel’ in two senses, one linguistic and the other computational. First, we make use of a parallel aligned corpus consisting of almost 50 parallel translations in over 30 distinct languages, each in over 30,000 documents. Given the size of this dataset, a ‘massively parallel’ approach was also necessitated in the more usual computational sense. Our results indicate that, far from adding more noise, more linguistic parallelism is better when it comes to cross-language retrieval precision, in addition to the self-evident benefit that CLIR can be performed on more languages. 
 A recent direction in the development of met-  Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences. This paper further analyzes aspects of learning that impact performance. We argue that previously proposed approaches of training a HumanLikeness classiﬁer is not as well correlated with human judgments of translation quality, but that regression-based learning produces more reliable metrics. We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and  rics for sentence-level evaluation is to apply machine learning to create an improved composite metric out of less indicative ones (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004). Under the assumption that good machine translation will produce “human-like” sentences, classiﬁers are trained to predict whether a sentence is authored by a human or by a machine based on features of that sentence, which may be the sentence’s scores from individual automatic evaluation metrics. The conﬁdence of the classiﬁer’s prediction can then be interpreted as a judgment on the translation quality of the sentence. Thus, the composite metric is encoded in the conﬁdence scores of the classiﬁcation labels.  show that they can achieve higher correla-  While the learning approach to metric design of-  tions with human judgments than standard  fers the promise of ease of combining multiple met-  automatic metrics.  rics and the potential for improved performance,  
 been argued that qualia structures and lexical seman-  tic relations in general have applications in informa-  This paper presents an approach for the au-  tion retrieval (Voorhees, 1994; Pustejovsky et al.,  tomatic acquisition of qualia structures for  1993). One major bottleneck however is that cur-  nouns from the Web and thus opens the pos-  rently qualia structures need to be created by hand,  sibility to explore the impact of qualia struc-  which is probably also the reason why there are al-  tures for natural language processing at a  most no practical NLP systems using qualia struc-  larger scale. The approach builds on ear-  tures, but a lot of systems relying on publicly avail-  lier work based on the idea of matching spe-  able resources such as WordNet (Fellbaum, 1998)  ciﬁc lexico-syntactic patterns conveying a  or FrameNet (Baker et al., 1998) as source of lex-  certain semantic relation on the World Wide  ical/world knowledge. The work described in this  Web using standard search engines. In our  paper addresses this issue and presents an approach  approach, the qualia elements are actually  to automatically learning qualia structures for nouns  ranked for each qualia role with respect to  from the Web. The approach is inspired in recent  some measure. The speciﬁc contribution of  work on using the Web to identify instances of a re-  the paper lies in the extensive analysis and  lation of interest such as in (Markert et al., 2003) and  quantitative comparison of different mea-  (Etzioni et al., 2005). These approaches rely on a  sures for ranking the qualia elements. Fur-  combination of the usage of lexico-syntactic pattens  ther, for the ﬁrst time, we present a quan-  conveying a certain relation of interest as described  titative evaluation of such an approach for  in (Hearst, 1992) with the idea of using the web as a  learning qualia structures with respect to a  big corpus (cf. (Kilgariff and Grefenstette, 2003)).  handcrafted gold standard.  Our approach directly builds on our previous work  
 Verkuyl, 1972; Dowty, 1979; Smith, 1991; Asher,  Situation entities (SEs) are the events, states, generic statements, and embedded facts and propositions introduced to a discourse by clauses of text. We report on the ﬁrst data-  1993; Carlson and Pelletier, 1995). Consider the text passage below, which introduces an event-type entity in (1), a report-type entity in (2), and a statetype entity in (3).  driven models for labeling clauses according to the type of SE they introduce. SE classiﬁ-  (1)  Sony Corp. has heavily promoted the Video Walkman  since the product’s introduction last summer ,  cation is important for discourse mode identiﬁcation and for tracking the temporal progression of a discourse. We show that (a)  (2)  but Bob Gerson , video editor of This Week in Con-  sumer Electronics , says  linguistically-motivated cooccurrence features and grammatical relation information from deep syntactic analysis improve clas-  (3)  Sony conceives of 8mm as a “family of products ,  camcorders and VCR decks , ”  siﬁcation accuracy and (b) using a sequencing model provides improvements over assigning labels based on the utterance alone. We report on genre effects which support the analysis of discourse modes having characteristic distributions and sequences of SEs.  SE classiﬁcation is a fundamental component in determining the discourse mode of texts (Smith, 2003) and, along with aspectual classiﬁcation, for temporal interpretation (Moens and Steedman, 1988). It may be useful for discourse relation projection and discourse parsing.  
 different degrees in the data (Baayen, 1992). Con-  Frequency distribution models tuned to words and other linguistic events can predict the number of distinct types and their frequency distribution in samples of arbitrary sizes. We conduct, for the ﬁrst time, a rigorous evaluation of these models based on cross-validation and separation of training and test data. Our experiments reveal that the prediction accuracy of the models is marred by serious overﬁtting problems, due to violations of the random sampling assumption in corpus data. We then propose  sider for example a very common preﬁx such as reand a rather rare preﬁx such as meta-. With LNRE models we can answer questions such as: If we could obtain as many tokens of meta- as we have of re-, would we also see as many distinct types? In other words, is the preﬁx meta- as productive as the preﬁx re-? Practical NLP applications, on the other hand, include estimating how many out-ofvocabulary words we will encounter given a lexicon of a certain size, or making informed guesses about type counts in very large data sets (e.g., how many typos are there on the Internet?)  a simple pre-processing method to allevi-  In this paper, after introducing LNRE models  ate such non-randomness problems. Further  (Section 2), we present an evaluation of their per-  evaluation conﬁrms the effectiveness of the  formance based on separate training and test data  method, which compares favourably to more  as well as cross-validation (Section 3). As far as  complex correction techniques.  we know, this is the ﬁrst time that such a rigorous  
This paper describes the ﬁrst system for large-scale acquisition of subcategorization frames (SCFs) from English corpus data which can be used to acquire comprehensive lexicons for verbs, nouns and adjectives. The system incorporates an extensive rulebased classiﬁer which identiﬁes 168 verbal, 37 adjectival and 31 nominal frames from grammatical relations (GRs) output by a robust parser. The system achieves state-ofthe-art performance on all three sets. 
 words are decomposed morphologically. There ex-  Morphological segmentation has been shown to be beneﬁcial to a range of NLP tasks such as machine translation, speech recognition, speech synthesis and information retrieval. Recently, a number of approaches to unsupervised morphological segmentation have been proposed. This paper describes an algorithm that draws from previous approaches and combines them into a simple model for morphological segmentation that outperforms other approaches on English and German, and also yields good results on agglutinative languages such as Finnish and Turkish. We also propose a method for detecting variation within stems in an unsupervised fashion. The segmentation quality reached with the new algorithm is good enough to improve grapheme-to-phoneme conversion.  ist a number of rule-based morphological segmentation systems for a range of languages. However, expert knowledge and labour are expensive, and the analyzers must be updated on a regular basis in order to cope with language change (the emergence of new words and their inﬂections). One might argue that unsupervised algorithms are not an interesting option from the engineering point of view, because rule-based systems usually lead to better results. However, segmentations from an unsupervised algorithm that is language-independent are “cheap”, because the only resource needed is unannotated text. If such an unsupervised system reaches a performance level that is good enough to help another task, it can constitute an attractive additional component. Recently, a number of approaches to unsupervised morphological segmentation have been proposed. These algorithms autonomously discover morpheme segmentations in unannotated text corpora. Here we describe a modiﬁcation of one such unsupervised al-  
Partition-based morphology is an approach of ﬁnite-state morphology where a grammar describes a special kind of regular relations, which split all the strings of a given tuple into the same number of substrings. They are compiled in ﬁnite-state machines. In this paper, we address the question of merging grammars using different partitionings into a single ﬁnite-state machine. A morphological description may then be obtained by parallel or sequential application of constraints expressed on different partition notions (e.g. morpheme, phoneme, grapheme). The theory of Mazurkiewicz Trace Languages, a well known semantics of parallel systems, provides a way of representing and compiling such a description. 
A number of Russian verbs lack 1sg nonpast forms. These paradigmatic gaps are puzzling because they seemingly contradict the highly productive nature of inflectional systems. We model the persistence and spread of Russian gaps via a multi-agent model with Bayesian learning. We ran three simulations: no grammar learning, learning with arbitrary analogical pressure, and morphophonologically conditioned learning. We compare the results to the attested historical development of the gaps. Contradicting previous accounts, we propose that the persistence of gaps can be explained in the absence of synchronic competition between forms. 
 The information loss inherent in the process of  Transliteration is the task of converting a word from one alphabetic script to another. We present a novel, substring-based approach to transliteration, inspired by phrasebased models of machine translation. We investigate two implementations of substringbased transliteration: a dynamic programming algorithm, and a ﬁnite-state transducer. We show that our substring-based transducer not only outperforms a state-of-the-art letterbased approach by a signiﬁcant margin, but is also orders of magnitude faster. 
 tion is not possible in most languages of the world  Speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary (OOV) ratio. Earlier work has shown that vocabulary decomposition  without ﬁrst developing the tools needed for open vocabulary speech recognition. This is due to a fundamental obstacle in current ASR called the out-ofvocabulary (OOV) problem.  methods can practically solve this problem for a subset of these languages. This paper compares various vocabulary decomposition approaches to open vocabulary speech recognition, using Estonian speech recognition as a benchmark. Comparisons are performed utilizing large models of 60000 lexical items and smaller vocabularies of 5000 items. A large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate, while the unsupervised morphology discovery method Morfessor Baseline gives marginally weaker results. Only the Morfessor-based approach is shown to adequately scale to smaller vocabulary sizes.  The OOV problem refers to the existence of words encountered that a speech recognizer is unable to recognize, as they are not covered in the vocabulary. The OOV problem is caused by three intertwined issues. Firstly, the language model training data and the test data always come from different samplings of the language, and the mismatch between test and training data introduces some OOV words, the amount depending on the difference between the data sets. Secondly, ASR systems always use ﬁnite and preferably small sized vocabularies, since the speed of decoding rapidly slows down as the vocabulary size is increased. Vocabulary sizes depend on the application domain, sizes larger than 60000 being very rare. As some of the words encountered in the training data are left out of the vo-  
This paper presents pipeline iteration, an approach that uses output from later stages of a pipeline to constrain earlier stages of the same pipeline. We demonstrate signiﬁcant improvements in a state-of-the-art PCFG parsing pipeline using base-phrase constraints, derived either from later stages of the parsing pipeline or from a ﬁnitestate shallow parser. The best performance is achieved by reranking the union of unconstrained parses and relatively heavilyconstrained parses.  later stages of a pipeline is used to constrain earlier stages of the same pipeline. To our knowledge, this is the ﬁrst time such a pipeline architecture has been proposed. It may seem surprising that later stages of a pipeline, already constrained to be consistent with the output of earlier stages, can proﬁtably inform the earlier stages in a second pass. However, the richer models used in later stages of a pipeline provide a better distribution over the subset of possible solutions produced by the early stages, effectively resolving some of the ambiguities that account for much of the original variation. If an earlier stage is  
 which is the construction of a complete, formal  This paper presents the ﬁrst empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with λoperators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain.  meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good performance in various domains. More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL  
 gained more interest (Nivre and Nilsson, 2005; Hall  Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for speciﬁc parsers and data sets. We investigate to what extent this can be generalized across languages/treebanks and parsers, focusing on pseudo-projective parsing, as a way of capturing non-projective dependencies, and transformations used to facilitate parsing of coordinate structures and verb groups. The results indicate that the beneﬁcial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank speciﬁc properties. By contrast, the construction speciﬁc transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages.  and Nova´k, 2005; McDonald and Pereira, 2006; Nilsson et al., 2006) but are still less studied, partly because constituency-based parsing has dominated the ﬁeld for a long time, and partly because dependency structures have less structure to manipulate than constituent structures. Most of the studies in this tradition focus on a particular parsing model and a particular data set, which means that it is difﬁcult to say whether the effect of a given transformation is dependent on a particular parsing strategy or on properties of a particular language or treebank, or both. The aim of this study is to further investigate some tree transformation techniques previously proposed for data-driven dependency parsing, with the speciﬁc aim of trying to generalize results across languages/treebanks and parsers. More precisely, we want to establish, ﬁrst of all, whether the transformation as such makes speciﬁc assumptions about the language, treebank  
This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English. Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language. 
 going back to the Nobel Sveriges–Riksbank Laure-  Text is not unadulterated fact. A text can make you laugh or cry but can it also make you short sell your stocks in company A and buy up options in company B? Research in the domain of ﬁnance strongly suggests that it can. Studies have shown that both the informational and affective aspects of news text affect the markets in profound ways, impacting on volumes of trades, stock prices, volatility and even future ﬁrm earnings. This paper aims to explore a computable metric of positive or negative polarity in ﬁnancial news text which is consistent with human judgments and can be used in a quantitative analysis of news sentiment impact on ﬁnancial markets. Results from a preliminary evaluation are presented and discussed.  ates Herbert Simon (1978 Prize) and Daniel Kahneman (2002 Prize), that shows that investors and traders in such markets can behave irrationally and that this bounded rationality is inspired by what the traders and investors hear from others about the conditions that may or may not prevail in the markets. Robert Engle (2003 Prize) has given a mathematical description of the asymmetric and affective impact of news on prices: positive news is typically related to large changes in prices but only for a short time; conversely the effect of negative news on prices and volumes of trading is longer lasting. The emergent domain of sociology of ﬁnance examines ﬁnancial markets as social constructs and how communications, such as e-mails and news reports, may be loaded with sentiment which could distort market trading (MacKenzie, 2003). It would appear that news affects the markets  
 and justiﬁes extraction of inhibit(XfK89→Felin-9),  We investigate automatic classiﬁcation of speculative language (‘hedging’), in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented. We show that hedge classiﬁcation is feasible using weakly supervised ML, and point toward avenues for future research. 
 interest. One of the possible applications is to help  We present a novel approach to automatically annotate images using associated text. We detect and classify all entities (persons and objects) in the text after which we determine the salience (the importance of an entity in a text) and visualness (the extent to which an entity can be perceived visually) of these entities. We combine these measures to compute the probability that an entity is present in the image. The suitability of our approach was successfully tested on 100 image-text pairs of Yahoo! News. 
We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer. We grounded our work on the empirical analysis of elicited user queries. We found that the majority of elicited queries (around 60%) pertain to argumentative processes and outcomes. Our analysis also suggests that standard keyword-based Information Retrieval can only deal successfully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference. 
 reasons: First, empirical analysis has shown that an-  Automatic segmentation is important for making multimedia archives comprehensible, and for developing downstream information retrieval and extraction modules. In this study, we explore approaches that can segment multiparty conversational speech by integrating various knowledge sources (e.g., words, audio and video recordings, speaker intention and context). In particular, we evaluate the performance of a Maximum Entropy approach, and examine the effectiveness of multimodal features on the task of dialogue segmentation. We also provide a quantitative account of the effect of using ASR transcription as opposed to human transcripts. 
Psychiatric document retrieval attempts to help people to efficiently and effectively locate the consultation documents relevant to their depressive problems. Individuals can understand how to alleviate their symptoms according to recommendations in the relevant documents. This work proposes the use of high-level topic information extracted from consultation documents to improve the precision of retrieval results. The topic information adopted herein includes negative life events, depressive symptoms and semantic relations between symptoms, which are beneficial for better understanding of users' queries. Experimental results show that the proposed approach achieves higher precision than the word-based retrieval models, namely the vector space model (VSM) and Okapi model, adopting word-level information alone. 
 The problems of word phonemization, syllabiﬁca-  Grapheme-to-phoneme conversion (g2p) is a core component of any text-to-speech system. We show that adding simple syllabiﬁcation and stress assignment constraints, namely ‘one nucleus per syllable’ and ‘one main stress per word’, to a joint n-gram model for g2p conversion leads to a dramatic improvement in conversion accuracy.  tion and word stress assignment are inter-dependent. Information about the position of a syllable boundary helps grapheme-to-phoneme conversion. (Marchand and Damper, 2005) report a word error rate (WER) reduction of approx. 5 percentage points for English when the letter string is augmented with syllabiﬁcation information. The same holds vice-versa: we found that WER was reduced by 50% when running our syllabiﬁer on phonemes instead of letters  Secondly, we assessed morphological preprocessing for g2p conversion. While morphological information has been incorporated in some past systems, its contribution has never been quantitatively assessed for German. We compare the relevance of morphological preprocessing with respect to the morphological segmentation method, train-  (see Table 4). Finally, word stress is usually deﬁned on syllables; in languages where word stress is assumed1 to partly depend on syllable weight (such as German or Dutch), it is important to know where exactly the syllable boundaries are in order to correctly calculate syllable weight. For German, (Mu¨ller, 2001) show that information about stress assignment and the position of a syllable within a word improve  ing set size, the g2p conversion algorithm,  g2p conversion.  and two languages, English and German.  1.2 Morphological Preprocessing  
 as policeman, nurse, etc. Many other professions,  We present a study aimed at investigating the use of semantic information in a novel NLP application, Electronic Career Guidance (ECG), in German. ECG is formulated as an information retrieval (IR) task, whereby textual descriptions of professions (documents) are ranked for their relevance to natural language descriptions of a person’s professional interests (the topic). We compare the performance of two semantic IR models: (IR-1) utilizing semantic relatedness (SR) measures based on either wordnet or Wikipedia and a set of heuristics, and (IR-2) measuring the similarity between the topic and documents based on Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). We evaluate the performance of SR measures intrinsically on the tasks of (T-1) computing SR, and (T-2) solving Reader’s Digest Word Power (RDWP) questions.  which can possibly match the interests of the person very well, are not chosen, as their titles are unknown and people seeking career advice do not know about their existence, e.g. electronics installer, or chemical laboratory worker. However, people are very good at describing their professional interests in natural language. That is why they are even asked to write a short essay prior to an appointment with a career guidance expert. Electronic career guidance is, thus, a supplement to career guidance by human experts, helping young people to decide which profession to choose. The goal is to automatically compute a ranked list of professions according to the user’s interests. A current system employed by the German Federal Labour Ofﬁce (GFLO) in their automatic career guidance front-end1 is based on vocational trainings, manually annotated using a tagset of 41 keywords. The user must select appropriate keywords according to her interests. In reply, the system consults a knowledge base with professions manually annotated with the keywords by career guidance experts. There-  
We present a general framework for automatically extracting social networks and biographical facts from conversational speech. Our approach relies on fusing the output produced by multiple information extraction modules, including entity recognition and detection, relation detection, and event detection modules. We describe the speciﬁc features and algorithmic reﬁnements effective for conversational speech. These cumulatively increase the performance of social network extraction from 0.06 to 0.30 for the development set, and from 0.06 to 0.28 for the test set, as measured by f-measure on the ties within a network. The same framework can be applied to other genres of text — we have built an automatic biography generation system for general domain text using the same approach. 
 (more speciﬁcally the consonant) inventories of  In this paper, we put forward an information theoretic deﬁnition of the redundancy that is observed across the sound inventories of the world’s languages. Through rigorous statistical analysis, we ﬁnd that this redundancy is an invariant property of the consonant inventories. The statistical analysis further unfolds that the vowel inventories do not exhibit any such property, which in turn points to the fact that the organizing principles of the vowel and the consonant inventories are quite different in nature.  the world’s languages. For this purpose, we present an information theoretic deﬁnition of redundancy, which is calculated based on the set of features1 (Trubetzkoy, 1931) that are used to express the consonants. An interesting observation is that this quantitative feature-based measure of redundancy is almost an invariance over the consonant inventories of the world’s languages. The observation is important since it can shed enough light on the organization of the consonant inventories, which unlike the vowel inventories, lack a complete and holistic explanation. The invariance of our measure implies that every inventory tries to be similar in  
In this paper we investigate named entity transliteration based on a phonetic scoring method. The phonetic method is computed using phonetic features and carefully designed pseudo features. The proposed method is tested with four languages – Arabic, Chinese, Hindi and Korean – and one source language – English, using comparable corpora. The proposed method is developed from the phonetic method originally proposed in Tao et al. (2006). In contrast to the phonetic method in Tao et al. (2006) constructed on the basis of pure linguistic knowledge, the method in this study is trained using the Winnow machine learning algorithm. There is salient improvement in Hindi and Arabic compared to the previous study. Moreover, we demonstrate that the method can also achieve comparable results, when it is trained on language data different from the target language. The method can be applied both with minimal data, and without target language data for various languages. 
Words of foreign origin are referred to as borrowed words or loanwords. A loanword is usually imported to Chinese by phonetic transliteration if a translation is not easily available. Semantic transliteration is seen as a good tradition in introducing foreign words to Chinese. Not only does it preserve how a word sounds in the source language, it also carries forward the word’s original semantic attributes. This paper attempts to automate the semantic transliteration process for the first time. We conduct an inquiry into the feasibility of semantic transliteration and propose a probabilistic model for transliterating personal names in Latin script into Chinese. The results show that semantic transliteration substantially and consistently improves accuracy over phonetic transliteration in all the experiments. 
 In this paper, we explore an approach in which  We present a novel method for predicting inﬂected word forms for generating morphologically rich languages in machine translation. We utilize a rich set of syntactic and morphological knowledge sources from both source and target sentences in a probabilistic model, and evaluate their contribution in generating Russian and Arabic sentences. Our results show that the proposed model substantially outperforms the commonly used baseline of a trigram target language model; in particular, the use of morphological and syntactic features leads to large gains in prediction accuracy. We also show that the proposed method is effective with a relatively small amount of data. 
We present the design and evaluation of a translator’s amenuensis that uses comparable corpora to propose and rank nonliteral solutions to the translation of expressions from the general lexicon. Using distributional similarity and bilingual dictionaries, the method outperforms established techniques for extracting translation equivalents from parallel corpora. The interface to the system is available at: http://corpus.leeds.ac.uk/assist/v05/ 
 programming (Wu, 1996; Och and Ney, 2004). In  Efﬁcient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve signiﬁcant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy.  practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we ﬁrst decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapt-  
Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translation relies on (local) associations of target words/phrases with source words/phrases for lexical selection. In contrast, in this paper, we present a novel approach to lexical selection where the target words are associated with the entire source sentence (global) without the need to compute local associations. Further, we present a technique for reconstructing the target language sentence from the selected words. We compare the results of this approach against those obtained from a ﬁnite-state based statistical machine translation system which relies on local lexical associations. 
 as Czech (Veselá et al., 2004). Unfortunately, most  Dependency-based representations of natural language syntax require a ﬁne balance between structural ﬂexibility and computational complexity. In previous work, several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense; the best-known but also most restrictive of these is projectivity. Most constraints are formulated on fully speciﬁed structures, which makes them hard to integrate into models where structures are composed from lexical information. In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages.  formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efﬁcient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999), parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural ﬂexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on  
 step, the Unfold-Fold transformation, to transform a  This paper shows how to use the UnfoldFold transformation to transform Projective Bilexical Dependency Grammars (PBDGs) into ambiguity-preserving weakly equivalent Context-Free Grammars (CFGs). These CFGs can be parsed in O(n3) time using a CKY algorithm with appropriate indexing, rather than the O(n5) time required by a naive encoding. Informally, using the CKY algorithm with such a CFG mimics the steps of the Eisner-Satta O(n3) PBDG parsing algorithm. This transformation makes all of the techniques developed for CFGs available to PBDGs. We demonstrate this by describing a maximum posterior parse decoder for PBDGs. 
We show that the problems of parsing and surface realization for grammar formalisms with “context-free” derivations, coupled with Mon-  S → NP VP VP → V NP V → V Conj V NP → Det N NP → John  V → found V → caught Conj → and Det → a N → unicorn  tague semantics (under a certain restriction) can be reduced in a uniform way to Datalog query evaluation. As well as giving a polynomialtime algorithm for computing all derivation trees (in the form of a shared forest) from an input string or input logical form, this reduction has the following complexity-theoretic consequences for all such formalisms: (i) the decision problem of recognizing grammaticality (surface realizability) of an input string (logical form) is in LOGCFL; and (ii) the search problem of ﬁnding one logical form (surface string) from an input string (logical form) is in functional LOGCFL. Moreover, the generalized supplementary magic-sets rewriting of the Datalog program resulting from the reduction yields efﬁcient Earley-style algorithms for both parsing and generation.  Figure 1: A CFG.  S(i, j) :− NP(i, k), VP(k, j). VP(i, j) :− V(i, k), NP(k, j). V(i, j) :− V(i, k), Conj(k, l), V(l, j). NP(i, j) :− Det(i, k), N(k, j). NP(i, j) :− John(i, j).  V(i, j) :− found(i, j). V(i, j) :− caught(i, j). Conj(i, j) :− and(i, j). Det(i, j) :− a(i, j). N(i, j) :− unicorn(i, j).  Figure 2: The Datalog representation of a CFG.  By naive (or seminaive) bottom-up evaluation (see, e.g., Ullman, 1988), the answer to such a query can be computed in polynomial time in the size of the database for any Datalog program. By recording rule instances rather than derived facts, a packed representation of the complete set of Datalog derivation  trees for a given query can also be obtained in poly-  
 and Smith (2005)) and thus learn to favor parses with  We examine the problem of choosing word order for a set of dependency trees so as to minimize total dependency length. We present an algorithm for computing the optimal layout of a single tree as well as a numerical method for optimizing a grammar of orderings over a set of dependency types. A grammar generated by minimizing dependency length in unordered trees from the Penn Treebank is found to agree surprisingly well with English word order, suggesting that dependency length minimization has inﬂuenced the evolution of English. 
Large corpora of parsed sentences with semantic role labels (e.g. PropBank) provide training data for use in the creation of high-performance automatic semantic role labeling systems. Despite the size of these corpora, individual verbs (or rolesets) often have only a handful of instances in these corpora, and only a fraction of English verbs have even a single annotation. In this paper, we describe an approach for dealing with this sparse data problem, enabling accurate semantic role labeling for novel verbs (rolesets) with only a single training example. Our approach involves the identification of syntactically similar verbs found in PropBank, the alignment of arguments in their corresponding rolesets, and the use of their corresponding annotations in PropBank as surrogate training data. 
 towards processing the semantic content of natural  This paper presents a novel application of Alternating Structure Optimization (ASO) to the task of Semantic Role Labeling (SRL) of noun predicates in NomBank. ASO is a recently proposed linear multi-task learning algorithm, which extracts the common  language texts. Although verbs are probably the most obvious predicates in a sentence, many nouns are also capable of having complex argument structures, often with much more ﬂexibility than its verb counterpart. For example, compare affect and effect:  structures of multiple tasks to improve accuracy, via the use of auxiliary problems. In this paper, we explore a number of different auxiliary problems, and we are able to signiﬁcantly improve the accuracy of the Nom-  [subj Auto prices] [arg−ext greatly] [pred affect] [obj the PPI]. [subj Auto prices] have a [arg−ext big] [pred effect] [obj on the PPI].  Bank SRL task using this approach. To our knowledge, our proposed approach achieves  The [pred effect] [subj of auto prices] [obj on the PPI] is [arg−ext big].  the highest accuracy published to date on the English NomBank SRL task.  [subj The auto prices’] [pred effect] [obj on the PPI] is [arg−ext big].  
 lexical resources. In addition, the corpus for  We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik’s WordNet-based model and the EM-based clustering model, but has coverage problems.  computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a ﬁxed lexical resource. We focus on one application of selectional preferences: semantic role labeling. The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet (Baker et al., 1998) paradigm, and the predicates we consider will be semantic classes of words rather than individual words  
 to Hebrew, but reported that SVM based chunking  We study the issue of porting a known NLP method to a language with little existing NLP resources, speciﬁcally Hebrew SVM-based chunking. We introduce two SVM-based methods – Model Tampering and Anchored Learning. These allow ﬁne grained analysis of the learned SVM models, which provides guidance to identify errors in the training corpus, distinguish the role and interaction of lexical features and eventually construct a model with ∼10% error reduction. The resulting chunker is shown to be robust in the presence of noise in the training corpus, relies on less lexical features than was previously understood and achieves an F-measure performance of 92.2 on automatically PoS-tagged text. The SVM analysis methods also provide general insight on SVM-based chunking.  (Kudo and Matsumoto, 2000) performs well. We extend that work and study the problem from 3 angles: (1) how to deal with a corpus that is smaller and with a higher level of noise than is available in English; we propose techniques that help identify ‘suspicious’ data points in the corpus, and identify how robust the model is in the presence of noise; (2) we compare the task deﬁnition in English and in Hebrew through quantitative evaluation of the differences between the two languages by analyzing the relative importance of features in the learned SVM models; and (3) we analyze the structure of learned SVM models to better understand the characteristics of the chunking problem in Hebrew. While most work on chunking with machine learning techniques tend to treat the classiﬁcation engine as a black-box, we try to investigate the resulting classiﬁcation model in order to understand its inner working, strengths and weaknesses. We in-  
 erate more lexical patterns. The larger and more reli-  We present a web mining method for discovering and enhancing relationships in which a speciﬁed concept (word class) participates. We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most previous work. Our method is based on clustering patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and ﬁnd that in each case the method generates a broad variety of relationships with good precision.  able sets of patterns thus generated resulted in larger and more precise sets of hyponyms and vice versa. The initial step of the resulting alternating bootstrap process – the user-provided input – could just as well consist of examples of hyponyms as of lexical patterns. A second objective was to extend the information that could be learned from the process beyond hyponyms of a given word. Thus, the approach was extended to ﬁnding lexical patterns that could produce synonyms and other standard lexical relations. These relations comprise all those words that stand in some known binary relation with a speciﬁed word. In this paper, we introduce a novel extension of  
The Penn Treebank does not annotate within base noun phrases (NPs), committing only to ﬂat structures that ignore the complexity of English NPs. This means that tools trained on Treebank data cannot learn the correct internal structure of NPs. This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank. We then examine the consistency and reliability of our annotations. Finally, we use this resource to determine NP structure using several statistical approaches, thus demonstrating the utility of the corpus. This adds detail to the Penn Treebank that is necessary for many NLP applications. 
 ample phrase structure trees (Collins, 2003), depen-  A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. In this paper we evaluate a CCG parser on DepBank, and demonstrate the difﬁculties in converting the parser output into DepBank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types.  dency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspeciﬁc dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The  
 supervised case. The fully supervised case mod-  We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough “target” data to do slightly better than just using only “source” data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms stateof-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multidomain adaptation problem, where one has data from a variety of different domains.  els the following scenario. We have access to a large, annotated corpus of data from a source domain. In addition, we spend a little money to annotate a small corpus in the target domain. We want to leverage both annotated datasets to obtain a model that performs well on the target domain. The semisupervised case is similar, but instead of having a small annotated target corpus, we have a large but unannotated target corpus. In this paper, we focus exclusively on the fully supervised case. One particularly nice property of our approach is that it is incredibly easy to implement: the Ap-  
Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classiﬁcation functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective. 
 Teh et al. (2006) proposed the hierarchical Dirich-  let process (HDP) as a way of applying the Dirichlet  Historically, unsupervised learning techniques have lacked a principled technique for selecting the number of unseen components. Research into non-parametric priors, such as the Dirichlet process, has enabled instead the use of inﬁnite models, in which the number of hidden categories is not ﬁxed, but can grow with the amount of training data. Here we develop the inﬁnite tree, a new inﬁnite model capable of representing recursive branching structure over an arbitrarily large set of hidden categories. Speciﬁcally, we develop three inﬁnite tree models, each of which enforces different independence assumptions, and for each model we deﬁne a simple direct assignment sampling inference procedure. We demonstrate the utility of our models by doing unsupervised learning of part-of-speech tags from treebank dependency skeleton structure, achieving an accuracy of 75.34%, and by doing unsupervised splitting of part-of-speech tags, which increases the accuracy of a generative dependency parser from 85.11% to 87.35%.  process (DP) to more complex model forms, so as to allow multiple, group-speciﬁc, inﬁnite mixture models to share their mixture components. The closely related inﬁnite hidden Markov model is an HMM in which the transitions are modeled using an HDP, enabling unsupervised learning of sequence models when the number of hidden states is unknown (Beal et al., 2002; Teh et al., 2006). We extend this work by introducing the inﬁnite tree model, which represents recursive branching structure over a potentially inﬁnite set of hidden states. Such models are appropriate for the syntactic dependency structure of natural language. The hidden states represent word categories (“tags”), the observations they generate represent the words themselves, and the tree structure represents syntactic dependencies between pairs of tags. To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufﬁcient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996). Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical  
 knowledge about the structure of the problem, will  Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classiﬁers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework uniﬁes and can exploit several kinds of task speciﬁc constraints. The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. 
 Tillmann & Xia, 2003). However, unlike in rule- and  Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate. In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield signiﬁcantly better PBSMT systems. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Despite the differences between these two approaches, the supertaggers give similar improvements. In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.  example-based MT, it has proven difﬁcult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi & Schabes, 1992; Bangalore & Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000). In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase cat-  
 pare against reference translations. Under a learn-  Many automatic evaluation metrics for machine translation (MT) rely on making comparisons to human translations, a resource that may not always be available. We present a method for developing sentence-level MT evaluation metrics that do not directly rely on human reference translations. Our metrics are developed using regression learning and are based on a set of weaker indicators of ﬂuency and adequacy (pseudo references). Experimental results suggest that they rival standard reference-based metrics in terms of correlations with human judgments on new test instances. 
 segmentation considered to be “good” from a mono-  We introduce a simple method to pack words for statistical word alignment. Our goal is to simplify the task of automatic word alignment by packing several consecutive words together when we believe they correspond to a single word in the opposite language. This is done using the word aligner itself, i.e. by bootstrapping on its output. We evaluate the performance of our approach on a Chinese-to-English machine translation task, and report a 12.2% relative increase in BLEU score over a state-of-the art phrasebased SMT system. 
 be based on the same basic modeling techniques  Recently, confusion network decoding has been applied in machine translation system combination. Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs. This paper describes an improved confusion network based method to combine outputs from multiple MT systems. In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show signiﬁcant improvements in BLEU scores compared to earlier confusion network decoding based methods.  but differ by, for example, alternative feature representations. Combination of speech recognition outputs is an example of this approach (Fiscus, 1997). In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In  
We investigate the factors which determine constituent order in German clauses and propose an algorithm which performs the task in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The ﬁrst task is more difﬁcult than the second one because of properties of the German sentence-initial position. Experiments show a signiﬁcant improvement over competing approaches. Our algorithm is also more efﬁcient than these. 
 Theory (MTT, (Mel’cuk, 1988)). In these theories, a  Surface realisers divide into those used in generation (NLG geared realisers) and those mirroring the parsing process (Reversible realisers). While the ﬁrst rely on grammars not easily usable for parsing, it is unclear how the second type of realisers could be parameterised to yield from among the set of possible paraphrases, the paraphrase appropriate to a given generation context. In this paper, we present a surface realiser which combines a reversible grammar (used for parsing and doing semantic construction) with a symbolic means of selecting paraphrases. 
 of each individual word (and the TAG tree it an-  We translate sentence generation from TAG grammars with semantic and pragmatic information into a planning problem by encoding the contribution of each word declaratively and explicitly. This allows us to exploit the performance of off-the-shelf planners. It also opens up new perspectives on referring expression generation and the relationship between language and action.  chors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavior, as earlier systems did; this allows us to get by with a much simpler logic. The problem we solve encompasses the generation of referring expressions (REs) as a special case. Unlike some approaches (Dale and Reiter, 1995;  
In evaluating the output of language technology applications—MT, natural language generation, summarisation—automatic evaluation techniques generally conﬂate measurement of faithfulness to source content with ﬂuency of the resulting text. In this paper we develop an automatic evaluation metric to estimate ﬂuency alone, by examining the use of parser outputs as metrics, and show that they correlate with human judgements of generated text ﬂuency. We then develop a machine learner based on these, and show that this performs better than the individual parser metrics, approaching a lower bound on human performance. We ﬁnally look at different language models for generating sentences, and show that while individual parser metrics can be ‘fooled’ depending on generation method, the machine learner provides a consistent estimator of ﬂuency. 
 The references in this passage are difﬁcult to  Non-verbal modalities such as gesture can improve processing of spontaneous spoken language. For example, similar hand gestures tend to predict semantic similarity, so features that quantify gestural similarity can improve semantic tasks such as coreference resolution. However, not all hand movements are informative gestures; psychological research has shown that speakers are more likely to gesture meaningfully when their speech is ambiguous. Ideally, one would attend to gesture only in such circumstances, and ignore other hand movements. We present conditional modality fusion, which formalizes this intuition by treating the informativeness of gesture as a hidden variable to be learned jointly with the class label. Applied to coreference resolution, conditional modality fusion signiﬁcantly outperforms both early and late modality fusion, which are current techniques for modality combination.  disambiguate, but the gestures shown in Figure 1 make the meaning more clear. However, non-verbal modalities are often noisy, and their interactions with speech are complex (McNeill, 1992). Gesture, for example, is sometimes communicative, but other times merely distracting. While people have little difﬁculty distinguishing between meaningful gestures and irrelevant hand motions (e.g., selftouching, adjusting glasses) (Goodwin and Goodwin, 1986), NLP systems may be confused by such seemingly random movements. Our goal is to include non-verbal features only in the speciﬁc cases when they are helpful and necessary. We present a model that learns in an unsupervised fashion when non-verbal features are useful, allowing it to gate the contribution of those features. The relevance of the non-verbal features is treated as a hidden variable, which is learned jointly with the class label in a conditional model. We demonstrate that this improves performance on binary coreference resolution, the task of determining whether a noun phrases refers to a single semantic entity. Conditional modality fusion yields a relative increase of  
In this paper we explore the utility of the Navigation Map (NM), a graphical representation of the discourse structure. We run a user study to investigate if users perceive the NM as helpful in a tutoring spoken dialogue system. From the users’ perspective, our results show that the NM presence allows them to better identify and follow the tutoring plan and to better integrate the instruction. It was also easier for users to concentrate and to learn from the system if the NM was present. Our preliminary analysis on objective metrics further strengthens these findings. 
 neering (e.g., manually created lexicons) or super-  Motivated by psycholinguistic ﬁndings that eye gaze is tightly linked to human language production, we developed an unsupervised approach based on translation models to automatically learn the mappings between words and objects on a graphic display during human machine conversation. The experimental results indicate that user eye gaze can provide useful information to establish such mappings, which have important implications in automatically acquiring and interpreting user vocabularies for conversational systems.  vised learning from annotated data. In this paper, we describe an unsupervised approach that relies on naturally co-occurred eye gaze and spoken utterances during human machine conversation to automatically acquire and interpret vocabularies. Motivated by psycholinguistic studies (Just and Carpenter, 1976; Grifﬁn and Bock, 2000; Tenenhaus et al., 1995) and recent investigations on computational models for language acquisition and grounding (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004), we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g., both user  
In order to effectively access the rapidly increasing range of media content available in the home, new kinds of more natural interfaces are needed. In this paper, we explore the application of multimodal interface technologies to searching and browsing a database of movies. The resulting system allows users to access movies using speech, pen, remote control, and dynamic combinations of these modalities. An experimental evaluation, with more than 40 users, is presented contrasting two variants of the system: one combining speech with traditional remote control input and a second where the user has a tablet display supporting speech and pen input.  graphical user interface using a remote control. In order to find content, users generally have to either navigate a complex, pre-defined, and often deeply embedded menu structure or type in titles or other key phrases using an onscreen keyboard or triple tap input on a remote control keypad. These interfaces are cumbersome and do not scale well as the range of content available increases (Berglund, 2004; Mitchell, 1999).  
 parsers (Klein and Manning, 2002; Klein and Man-  This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text. The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing. In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization. The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text.  ning, 2004; Bod, 2006a; Bod, 2006b) and, for the ﬁrst time, unsupervised parsers have been able to improve on the right-branching heuristic for parsing English. All these parsers learn and parse from sequences of part-of-speech tags and select, for each sentence, the binary parse tree which maximizes some objective function. Learning is based on global maximization of this objective function over the whole corpus. In this paper I present an unsupervised parser from plain text which does not use parts-of-speech. Learning is local and parsing is (locally) greedy. As a result, both learning and parsing are fast. The parser is incremental, using a new link representation for syntactic structure. Incremental parsing was chosen because it considerably restricts the search  
 these are known as edge-factored models. These  This paper introduces a Maximum Entropy dependency parser based on an efﬁcient kbest Maximum Spanning Tree (MST) algorithm. Although recent work suggests that the edge-factored constraints of the MST algorithm signiﬁcantly inhibit parsing accuracy, we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers. This motivates our parsing approach, which is based on reranking the kbest parses generated by an edge-factored model. Oracle parse accuracy results are presented for the edge-factored model and 1-best results for the reranker on eight languages (seven from CoNLL-X and English). 
How far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted? We present a new algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees. We train both on Penn’s WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set. While U-DOP* performs worse than state-of-the-art supervised parsers on handannotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl. We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight. 
 data are taken from different domains (the parser  While the average performance of statistical parsers gradually improves, they still attach to many sentences annotations of rather low quality. The number of such sentences grows when the training and test data are taken from different domains, which is the case for major web applications such as information retrieval and question answering.  adaptation scenario) the ratio of such low quality parses becomes even higher. Figure 1 demonstrates these phenomena for two leading models, Collins (1999) model 2, a generative model, and Charniak and Johnson (2005), a reranking model. The parser adaptation scenario is the rule rather than the exception for QA and IE systems, because these usually operate over the highly variable Web, making it very difﬁcult to create a representative corpus for manual  In this paper we present a Sample Ensemble Parse Assessment (SEPA) algorithm for  annotation. Medium quality parses may seriously harm the performance of such systems.  detecting parse quality. We use a function of the agreement among several copies of a parser, each of which trained on a different sample from the training data, to assess parse quality. We experimented with both generative and reranking parsers (Collins, Charniak and Johnson respectively). We show superior results over several baselines, both when the training and test data are from the same domain and when they are from different domains. For a test setting used by previous work, we show an error reduction of 31% as opposed to their 20%.  In this paper we address the problem of assessing parse quality, using a Sample Ensemble Parse Assessment (SEPA) algorithm. We use the level of agreement among several copies of a parser, each of which trained on a different sample from the training data, to predict the quality of a parse. The algorithm does not assume uniformity of training and test data, and is thus suitable to web-based applications such as QA and IE. Generative statistical parsers compute a probability p(a, s) for each sentence annotation, so the immediate technique that comes to mind for assessing parse quality is to simply use p(a, s). Another  
 on Amazom.com post reviews about products they  Deriving the polarity and strength of opinions is an important research topic, attracting signiﬁcant attention over the last few years. In this work, to measure the strength and polarity of an opinion, we consider the economic context in which the opinion is evaluated, instead of using human annotators or linguistic resources. We rely on the fact that text in on-line systems inﬂuences the behavior of humans and this effect can be observed  bought and users on eBay.com post feedback describing their experiences with sellers. The goal of opinion mining systems is to identify such pieces of the text that express opinions (Breck et al., 2007; Ko¨nig and Brill, 2006) and then measure the polarity and strength of the expressed opinions. While intuitively the task seems straightforward, there are multiple challenges involved. • What makes an opinion positive or negative? Is there an objective measure for this task?  using some easy-to-measure economic variables, such as revenues or product prices. By reversing the logic, we infer the semantic orientation and strength of an opinion by tracing the changes in the associated economic variable. In effect, we use econometrics to identify the “economic value of text” and assign a  • How can we rank opinions according to their strength? Can we deﬁne an objective measure for ranking opinions? • How does the context change the polarity and strength of an opinion and how can we take the context into consideration?  “dollar value” to each opinion phrase, measuring sentiment effectively and without the need for manual labeling. We argue that by interpreting opinions using econometrics, we have the ﬁrst objective, quantiﬁable, and contextsensitive evaluation of opinions. We make the discussion concrete by presenting results on the reputation system of Amazon.com. We show that user feedback affects the pricing power of merchants and by measuring their pricing power we can infer the polarity and strength of the underlying feedback postings.  To evaluate the polarity and strength of opinions, most of the existing approaches rely either on training from human-annotated data (Hatzivassiloglou and McKeown, 1997), or use linguistic resources (Hu and Liu, 2004; Kim and Hovy, 2004) like WordNet, or rely on co-occurrence statistics (Turney, 2002) between words that are unambiguously positive (e.g., “excellent”) and unambiguously negative (e.g., “horrible”). Finally, other approaches rely on reviews with numeric ratings from websites (Pang and Lee, 2002; Dave et al., 2003; Pang and Lee, 2004; Cui et al., 2006) and train (semi-)supervised learning algorithms  
 cipline that deals with the quantitative and qualita-  This paper presents an application of PageRank, a random-walk model originally devised for ranking Web search results, to ranking WordNet synsets in terms of how strongly they possess a given semantic property. The semantic properties we use for exemplifying the approach are positivity and negativity, two properties of central importance in sentiment analysis. The idea derives from the observation that WordNet may be seen as a graph in which synsets are connected through the binary relation “a term belonging to synset sk occurs in the gloss of synset si”, and on the hypothesis that this relation may be viewed as a transmitter of such semantic properties. The data for this relation can be obtained from eXtended WordNet, a publicly available sensedisambiguated version of WordNet. We argue that this relation is structurally akin to the relation between hyperlinked Web pages, and thus lends itself to PageRank analysis. We report experimental results supporting our intuitions.  tive analysis of text for the purpose of determining its opinion-related properties (ORPs). An important part of this research has been the work on the automatic determination of the ORPs of terms, as e.g., in determining whether an adjective tends to give a positive, a negative, or a neutral nature to the noun phrase it appears in. While many works (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al., 2004; Takamura et al., 2005; Turney and Littman, 2003) view the properties of positivity and negativity as categorical (i.e., a term is either positive or it is not), others (Andreevskaia and Bergler, 2006b; Grefenstette et al., 2006; Kim and Hovy, 2004; Subasic and Huettner, 2001) view them as graded (i.e., a term may be positive to a certain degree), with the underlying interpretation varying from fuzzy to probabilistic. Some authors go a step further and attach these properties not to terms but to term senses (typically: WordNet synsets), on the assumption that different senses of the same term may have different opinion-related properties (Andreevskaia and Bergler, 2006a; Esuli and Sebastiani, 2006b; Ide, 2006; Wiebe and Mihalcea, 2006). In this paper we contribute to this latter literature  
 reviews might require polarity classiﬁcation at the  In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Inference in the model is based on standard sequence classiﬁcation techniques using constrained Viterbi to ensure consistent solutions. The primary advantage of such a model is that it allows classiﬁcation decisions from one level in the text to inﬂuence decisions at another. Experiments show that this method can signiﬁcantly reduce classiﬁcation error relative to models trained in iso-  sentence or phrase level; a question answering system would most likely require the sentiment of paragraphs; and a system that determines which articles from an online news source are editorial in nature would require a document level analysis. This work focuses on models that jointly classify sentiment on multiple levels of granularity. Consider the following example, This is the ﬁrst Mp3 player that I have used ... I thought it sounded great ... After only a few weeks, it started having trouble with the earphone connection ... I won’t be buying another. Mp3 player review from Amazon.com  lation.  This excerpt expresses an overall negative opinion of  
 deployed industrially in systems that gauge market  Automatic sentiment classiﬁcation has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classiﬁers, focusing on online reviews for different types of products. First, we extend to sentiment classiﬁcation the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classiﬁer from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classiﬁers would transfer well to many other domains.  reaction and summarize opinion from Web pages, discussion boards, and blogs. With such widely-varying domains, researchers and engineers who build sentiment classiﬁcation systems need to collect and curate data for each new domain they encounter. Even in the case of market analysis, if automatic sentiment classiﬁcation were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features change over time. We envision a scenario in which developers annotate corpora for a small number of domains, train classiﬁers on those corpora, and then apply them to other similar corpora. However, this approach raises two important questions. First, it is well known that trained classiﬁers lose accuracy when the test data distribution is signiﬁcantly different from the training data distribution 1. Second, it is not clear which notion of domain similarity should be used to select domains to annotate that would be good proxies for many other domains. We propose solutions to these two questions and evaluate them on a corpus of reviews for four differ-  
 involving whole clauses or sentences. Dagan  Recently, there has been a rise of interest in unsupervised detection of highlevel semantic relations involving complex units, such as phrases and whole sentences. Typically such approaches are faced with two main obstacles: data sparseness and correctly generalizing from the examples. In this work, we describe the Clustered Clause representation, which utilizes information-based clustering and inter-sentence dependencies to create a simpliﬁed and generalized representation of the grammatical clause. We implement an algorithm which uses this representation to detect a predeﬁned set of high-level relations, and demonstrate our model’s eﬀectiveness in overcoming both the problems mentioned. 
 phrases and entailment rules. We deﬁne an entail-  Obtaining large volumes of inference knowledge, such as entailment rules, has become a major factor in achieving robust semantic processing. While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research. We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels. The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress.  ment rule to be a directional relation between two templates, text patterns with variables, e.g. ‘X prevent Y → X lower the risk of Y ’. The left-handside template is assumed to entail the right-handside template in certain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences and are used as building blocks for more complex entailment inference. For example, given the above rule, the answer “Aspirin” can be identiﬁed in the example above. The need for large-scale inference knowledgebases triggered extensive research on automatic acquisition of paraphrase and entailment rules. Yet the current precision of acquisition algorithms is typ-  
We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers. SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs. We evaluate these global, context-aware query expansion techniques on tﬁdf retrieval from 10 million question-answer pairs extracted from FAQ pages. Experimental results show that SMTbased expansion improves retrieval performance over local expansion and over retrieval without expansion.  and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to ﬁnd synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in  
We propose a computational model of text reuse tailored for ancient literary texts, available to us often only in small and noisy samples. The model takes into account source alternation patterns, so as to be able to align even sentences with low surface similarity. We demonstrate its ability to characterize text reuse in the Greek New Testament. 
 pressed by several means (synonyms, etc.) and dis-  Topic segmentation and identiﬁcation are often tackled as separate problems whereas they are both part of topic analysis. In this article, we study how topic identiﬁcation can help to improve a topic segmenter based on word reiteration. We ﬁrst present an unsupervised method for discovering the topics of a text. Then, we detail how these topics are used by segmentation for ﬁnding topical similarities between text segments. Finally, we show through the results of an evaluation done both for French and English the interest of the method we propose.  course cues are often rare and corpus-speciﬁc. To overcome these difﬁculties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in (Kozima, 1993); a thesaurus in (Morris and Hirst, 1991); a large set of lexical cooccurrences collected from a corpus in (Choi et al., 2001). To a certain extent, these lexical networks enable topic segmenters to exploit a sort of concept reiteration. However, their lack of any explicit topical structure makes this kind of knowledge difﬁcult to use when lexical ambiguity is high. The most simple solution to this problem is to exploit knowledge about the topics that may occur in  
We investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach, including features derived from either ﬁnite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely ﬁnite-state approaches can perform competitively. 
Over the last ﬁfty years, the “Big Five” model of personality traits has become a standard in psychology, and research has systematically documented correlations between a wide range of linguistic variables and the Big Five traits. A distinct line of research has explored methods for automatically generating language that varies along personality dimensions. We present PERSONAGE (PERSONAlity GEnerator), the ﬁrst highly parametrizable language generator for extraversion, an important aspect of personality. We evaluate two personality generation methods: (1) direct generation with particular parameter settings suggested by the psychology literature; and (2) overgeneration and selection using statistical models trained from judge’s ratings. Results show that both methods reliably generate utterances that vary along the extraversion dimension, according to human judges. 
 past (Beeferman et al., 1999; Galley et al., 2003;  We address the task of unsupervised topic segmentation of speech data operating over raw acoustic information. In contrast to existing algorithms for topic segmentation of speech, our approach does not require input transcripts. Our method predicts topic changes by analyzing the distribution of reoccurring acoustic patterns in the speech signal corresponding to a single speaker. The algorithm robustly handles noise inherent in acoustic matching by intelligently aggregating information about the similarity proﬁle from multiple local comparisons. Our experiments show that audio-based segmentation compares favorably with transcriptbased segmentation computed over noisy transcripts. These results demonstrate the desirability of our method for applications where a speech recognizer is not available, or its output has a high word error rate.  Dielmann and Renals, 2005). These methods typically assume that a segmentation algorithm has access not only to acoustic input, but also to its transcript. This assumption is natural for applications where the transcript has to be computed as part of the system output, or it is readily available from other system components. However, for some domains and languages, the transcripts may not be available, or the recognition performance may not be adequate to achieve reliable segmentation. In order to process such data, we need a method for topic segmentation that does not require transcribed input. In this paper, we explore a method for topic segmentation that operates directly on a raw acoustic speech signal, without using any input transcripts. This method predicts topic changes by analyzing the distribution of reoccurring acoustic patterns in the speech signal corresponding to a single speaker. In the same way that unsupervised segmentation algorithms predict boundaries based on changes in lexical distribution, our algorithm is driven by changes  
 lenges in deploying large LMs are not trivial. In-  A Bloom ﬁlter (BF) is a randomised data structure for set membership queries. Its space requirements are signiﬁcantly below lossless information-theoretic lower bounds but it produces false positives with some quantiﬁable probability. Here we explore the use of BFs for language modelling in statistical machine translation.  creasing the order of an n-gram model can result in an exponential increase in the number of parameters; for corpora such as the English Gigaword corpus, for instance, there are 300 million distinct trigrams and over 1.2 billion 5-grams. Since a LM may be queried millions of times per sentence, it should ideally reside locally in memory to avoid time-consuming remote or disk-based look-ups. Against this background, we consider a radically  We show how a BF containing n-grams can enable us to use much larger corpora and  different approach to language modelling: instead of explicitly storing all distinct n-grams, we store a  higher-order models complementing a conventional n-gram LM within an SMT sys-  randomised representation. In particular, we show that the Bloom ﬁlter (Bloom (1970); BF), a sim-  tem. We also consider (i) how to include ap-  ple space-efﬁcient randomised data structure for rep-  proximate frequency information efﬁciently within a BF and (ii) how to reduce the er-  resenting sets, may be used to represent statistics from larger corpora and for higher-order n-grams to  ror rate of these models by ﬁrst checking for lower-order sub-sequences in candidate n-  complement a conventional smoothed trigram model within an SMT decoder. 1  grams. Our solutions in both cases retain the  The space requirements of a Bloom ﬁlter are quite  one-sided error guarantees of the BF while  spectacular, falling signiﬁcantly below information-  taking advantage of the Zipf-like distribution  theoretic error-free lower bounds while query times  of word frequencies to reduce the space re-  are constant. This efﬁciency, however, comes at the  quirements.  price of false positives: the ﬁlter may erroneously  
 to employ Latent Semantic Analysis (LSA) to cap-  We propose a novel approach to crosslingual language model (LM) adaptation based on bilingual Latent Semantic Analysis (bLSA). A bLSA model is introduced which enables latent topic distributions to be efﬁciently transferred across languages by enforcing a one-to-one topic correspondence during training. Using the proposed bLSA framework crosslingual LM adaptation can be performed by, ﬁrst, inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language N-gram LM via marginal adaptation. The proposed framework also enables rapid bootstrapping of LSA models for new languages based on a source LSA model from another language. On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27% for a unigram LM and up to 13.6% for a 4-gram LM. Furthermore, the proposed approach consistently improved machine translation quality on both speech and text based adaptation.  ture in-domain word unigram distributions which are then integrated into the background N-gram LM. This approach has been successfully applied in automatic speech recognition (ASR) (Tam and Schultz, 2006) using the Latent Dirichlet Allocation (LDA) (Blei et al., 2003). The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution. For LM adaptation, the topic mixture weights are estimated based on in-domain adaptation text (e.g. ASR hypotheses). The adapted mixture weights are then used to interpolate a topicdependent unigram LM, which is ﬁnally integrated into the background N-gram LM using marginal adaptation (Kneser et al., 1997) In this paper, we propose a framework to perform LM adaptation across languages, enabling the adaptation of a LM from one language based on the adaptation text of another language. In statistical machine translation (SMT), one approach is to apply LM adaptation on the target language based on an initial translation of input references (Kim and Khudanpur, 2003; Paulik et al., 2005). This scheme is limited by the coverage of the translation model, and overall by the quality of translation. Since this approach only allows to apply LM adaptation af-  
 been seen in mining semantic relations from large  Semantic relatedness is a very important factor for the coreference resolution task. To obtain this semantic information, corpusbased approaches commonly leverage patterns that can express a speciﬁc semantic relation. The patterns, however, are designed manually and thus are not necessarily the most effective ones in terms of accuracy and breadth. To deal with this problem, in this paper we propose an approach that can automatically ﬁnd the effective patterns for coreference resolution. We explore how to automatically discover and evaluate patterns, and how to exploit the patterns to obtain the semantic relatedness information.  text corpora. One common solution is to utilize a pattern that can represent a speciﬁc semantic relation (e.g., “X such as Y” for is-a relation, and “X and other Y” for other-relation). Instantiated with two given noun phrases, the pattern is searched in a large corpus and the occurrence number is used as a measure of their semantic relatedness (Markert et al., 2003; Modjeska et al., 2003; Poesio et al., 2004). However, in the previous pattern based approaches, the selection of the patterns to represent a speciﬁc semantic relation is done in an ad hoc way, usually by linguistic intuition. The manually selected patterns, nevertheless, are not necessarily the most effective ones for coreference resolution from the following two concerns:  The evaluation on ACE data set shows that the pattern based semantic information is helpful for coreference resolution.  • Accuracy. Can the patterns (e.g., “X such as Y”) ﬁnd as many NP pairs of the speciﬁc semantic relation (e.g. is-a) as possible, with a  
 and negotiations) and to eliminate George W. Bush  This paper examines whether a learningbased coreference resolver can be improved using semantic class knowledge that is automatically acquired from a version of the Penn Treebank in which the noun phrases are labeled with their semantic classes. Experiments on the ACE test data show that a resolver that employs such induced semantic class knowledge yields a statistically significant improvement of 2% in F-measure over one that exploits heuristically computed semantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%.  from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have  
 mary particularly suited for accessing information in  This paper presents a method for the automatic generation of a table-of-contents. This type of summary could serve as an effective navigation tool for accessing information in long texts, such as books. To generate a coherent table-of-contents, we need to capture both global dependencies across different titles in the table and local constraints within sections. Our algorithm effectively handles these complex dependencies by factoring the model into local and global components, and incrementally constructing the model’s output. The results of automatic evaluation and manual assessment conﬁrm the beneﬁts of this design: our system is consistently ranked higher than nonhierarchical baselines. 
Though both document summarization and keyword extraction aim to extract concise representations from documents, these two tasks have usually been investigated independently. This paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single document under the assumption that the summary and keywords of a document can be mutually boosted. The approach can naturally make full use of the reinforcement between sentences and keywords by fusing three kinds of relationships between sentences and words, either homogeneous or heterogeneous. Experimental results show the effectiveness of the proposed approach for both tasks. The corpus-based approach is validated to work almost as well as the knowledge-based approach for computing word semantics. 
We describe a novel neural network archi-  [The company]ARG0 [bought]REL [sugar]ARG1 [on the world market]ARGM-LOC [to meet export commitments]ARGM-PNC  tecture for the problem of semantic role labeling. Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use  Figure 1: Example of Semantic Role Labeling from the PropBank dataset (Palmer et al., 2005). ARG0 is typically an actor, REL an action, ARG1 an object, and ARGM describe various modiﬁers such as location (LOC) and purpose (PNC).  of a syntactic parser (Pradhan et al., 2004;  Gildea and Jurafsky, 2002). Our method in-  problem. Researchers tackle several layers of pro-  stead learns a direct mapping from source  cessing tasks ranging from the syntactic, such as  sentence to semantic tags for a given pred-  part-of-speech labeling and parsing, to the semantic:  icate without the aid of a parser or a chun-  word-sense disambiguation, semantic role-labeling,  ker. Our resulting system obtains accuracies  named entity extraction, co-reference resolution and  comparable to the current state-of-the-art at  entailment. None of these tasks are end goals in  a fraction of the computational cost.  themselves but can be seen as layers of feature ex-  
 most of the time ambiguous or implicit. Interpreting  This paper addresses the automatic classiﬁcation of semantic relations in noun phrases based on cross-linguistic evidence from a set of ﬁve Romance languages. A set of novel semantic and contextual English– Romance NP features is derived based on empirical observations on the distribution of the syntax and meaning of noun phrases on two corpora of different genre (Europarl and CLUVI). The features were employed in a Support Vector Machines algorithm which achieved an accuracy of 77.9% (Europarl) and 74.31% (CLUVI), an improvement compared with two state-of-the-art models reported in the literature.  NPs correctly requires various types of information from world knowledge to complex context features. Moreover, the extension of this task to other natural languages brings forward new issues and problems. For instance, beer glass translates into tarro de cerveza in Spanish, bicchiere da birra in Italian, verre a` bie`re in French, and pahar de bere in Romanian. Thus, an important research question is how do the syntactic constructions in the target language contribute to the preservation of meaning in context. In this paper we investigate noun phrases based on cross-linguistic evidence and present a domain independent model for their semantic interpretation. We aim at uncovering the general aspects that govern the semantics of NPs in English based on a set of ﬁve Romance languages: Spanish, Italian, French,  
 Although not all of the sentences for positive pairs  We present a new approach to relation extraction that requires only a handful of training examples. Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web. We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents.  will state the desired relationship, many of them will. Presumably, none of the sentences for negative pairs state the targeted relation. Multiple instance learning (MIL) is a machine learning framework that exploits this sort of weak supervision, in which a positive bag is a set of instances which is guaranteed to contain at least one positive example, and a negative bag is a set of instances all of which are negative. MIL was originally introduced to solve a problem in biochemistry (Dietterich et al., 1997); however, it has since been applied to problems in other areas such as classifying image  
A minimally supervised machine learning framework is described for extracting relations of various complexity. Bootstrapping starts from a small set of n-ary relation instances as “seeds”, in order to automatically learn pattern rules from parsed data, which then can extract new instances of the relation and its projections. We propose a novel rule representation enabling the composition of n-ary relation rules on top of the rules for projections of the relation. The compositional approach to rule construction is supported by a bottom-up pattern extraction method. In comparison to other automatic approaches, our rules cannot only localize relation arguments but also assign their exact target argument roles. The method is evaluated in two tasks: the extraction of Nobel Prize awards and management succession events. Performance for the new Nobel Prize task is strong. For the management succession task the results compare favorably with those of existing pattern acquisition approaches.  tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered representation models (subject-verbobject, chain model, linked chain model and subtree model) are verb-centered. Relations embedded in non-verb constructions such as a compound noun cannot be discovered: (1) the 2005 Nobel Peace Prize  
Extraction of relations between entities is an important part of Information Extraction on free text. Previous methods are mostly based on statistical correlation and dependency relations between entities. This paper re-examines the problem at the multiresolution layers of phrase, clause and sentence using dependency and discourse relations. Our multi-resolution framework ARE (Anchor and Relation) uses clausal relations in 2 ways: 1) to filter noisy dependency paths; and 2) to increase reliability of dependency path extraction. The resulting system outperforms the previous approaches by 3%, 7%, 4% on MUC4, MUC6 and ACE RDC domains respectively. 
Many errors produced by unsupervised and semi-supervised relation extraction (RE) systems occur because of wrong recognition of entities that participate in the relations. This is especially true for systems that do not use separate named-entity recognition components, instead relying on general-purpose shallow parsing. Such systems have greater applicability, because they are able to extract relations that contain attributes of unknown types. However, this generality comes with the cost in accuracy. In this paper we show how to use corpus statistics to validate and correct the arguments of extracted relation instances, improving the overall RE performance. We test the methods on SRES – a self-supervised Web relation extraction system. We also compare the performance of corpus-based methods to the performance of validation and correction methods based on supervised NER components. 
 by trying to delimit permissible dependency struc-  Dependency analysis of natural language has gained importance for its applicability to NLP tasks. Non-projective structures are common in dependency analysis, therefore we need ﬁne-grained means of describing them, especially for the purposes of machine-learning oriented approaches like parsing. We present an evaluation on twelve languages which explores several constraints and measures on non-projective structures. We pursue an edge-based approach concentrating on properties of individual edges as opposed to properties of whole trees. In our evaluation, we include previously unreported measures taking into account levels of nodes in dependency trees. Our empirical results corroborate theoretical results and show that an edge-based approach using levels of nodes provides an accurate and at the same time expressive means for capturing non-projective structures in natural language. 
 from the same domain (the in-domain scenario) and  Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains. In this paper we use selftraining in order to improve the quality of a parser and to adapt it to a different domain, using only small amounts of manually annotated seed data. We report signiﬁcant improvement both when the seed and test data are in the same domain and in the outof-domain adaptation scenario. In particular, we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case. This is the ﬁrst time that self-training with small labeled datasets is applied successfully to these tasks. We were also able to formulate a characterization of when selftraining is valuable. 
 and Sag, 1994) and Combinatory Categorial Gram-  We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing to increase deep parsing accuracy, speciﬁcally by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can beneﬁt from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing.  mar (CCG) (Steedman, 2000), which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing. We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, speciﬁcally by combining dependency and HPSG parsing. We show that, by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can beneﬁt from a number of parsing techniques designed for high-accuracy dependency parsing, while actually performing deep syntactic analysis. From the  
 which we peopose, called Incremental Sigmoid Be-  We introduce a framework for syntactic parsing with latent variables based on a form of dynamic Sigmoid Belief Networks called Incremental Sigmoid Belief Networks. We demonstrate that a previous feed-forward neural network parsing model can be viewed as a coarse approximation to inference with this class of graphical model. By constructing a more accurate but still tractable approximation, we signiﬁcantly improve parsing accuracy, suggesting that ISBNs provide a good idealization for parsing. This generative model of parsing achieves state-of-theart results on WSJ text and 8% error reduction over the baseline neural network parser.  lief Networks (ISBNs) have large numbers of latent variables, which makes exact inference intractable. However, they can be approximated sufﬁciently well to build fast and accurate statistical parsers which induce features during training. We use SBNs in a generative history-based model of constituent structure parsing. The probability of an unbounded structure is decomposed into a sequence of probabilities for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-deﬁned set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all  
 Several transliteration methods are reported in the  Most current machine transliteration systems employ a corpus of known sourcetarget word pairs to train their system, and typically evaluate their systems on a similar corpus. In this paper we explore the performance of transliteration systems on corpora that are varied in a controlled way. In particular, we control the number, and prior language knowledge of human transliterators used to construct the corpora, and the origin of the source words that make up the corpora. We ﬁnd that the word accuracy of automated transliteration systems can vary by up to 30% (in absolute terms) depending on the corpus on which they are run. We conclude that at least four human transliterators should be used to construct corpora for evaluating automated transliteration systems; and that although absolute word accuracy metrics may not translate across corpora, the relative rankings of system performance remains stable across differing corpora.  literature for a variety of languages, with their performance being evaluated on multilingual corpora. Source-target pairs are either extracted from bilingual documents or dictionaries (AbdulJaleel and Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi, 2006; Zelenko and Aone, 2006), or gathered explicitly from human transliterators (Al-Onaizan and Knight, 2002; Zelenko and Aone, 2006). Some evaluations of transliteration methods depend on a single unique transliteration for each source word, while others take multiple target words for a single source word into account. In their work on transliterating English to Persian, Karimi et al. (2006) observed that the content of the corpus used for evaluating systems could have dramatic affects on the reported accuracy of methods. The effects of corpus composition on the evaluation of transliteration systems has not been specifically studied, with only implicit experiments or claims made in the literature such as introducing the effects of different transliteration models (AbdulJaleel and Larkey, 2003), language families (Linde´n, 2005) or application based (CLIR)  
 et al., 2004), phoneme-based (Knight and Graehl,  We propose a novel algorithm for English to Persian transliteration. Previous methods proposed for this language pair apply a word alignment tool for training. By contrast, we introduce an alignment algorithm particularly designed for transliteration. Our new model improves the English to Persian transliteration accuracy by 14% over an n-gram baseline. We also propose a novel back-transliteration method for this language pair, a previously unstudied problem. Experimental results demonstrate that our algorithm leads to an absolute improvement of 25% over standard transliteration approaches.  1998; Jung et al., 2000), and combined (Bilac and Tanaka, 2005) approaches. Grapheme-based methods perform a direct orthographical mapping between source and target words, while phonemebased approaches use an intermediate phonetic representation. Both grapheme- or phoneme-based methods usually begin by breaking the source word into segments, and then use a source segment to target segment mapping to generate the target word. The rules of this mapping are obtained by aligning already available transliterated word pairs (training data); alternatively, such rules can be handcrafted. From this perspective, past work is roughly divided into those methods which apply a word alignment tool such as GIZA++ (Och and Ney, 2003), and approaches that combine the alignment step into their  
 words having the same meaning.  A character-based measure of similarity is an important component of many natural language processing systems, including approaches to transliteration, coreference, word alignment, spelling correction, and the identiﬁcation of cognates in related vocabularies. We propose an alignment-based discriminative framework for string similarity. We gather features from substring pairs consistent with a character-based alignment of the two strings. This approach achieves exceptional performance; on nine separate cognate identiﬁcation experiments using six language pairs, we more than double the precision of traditional orthographic measures like Longest Common Subsequence Ratio and Dice’s Coefﬁcient. We also show strong improvements over other recent discriminative and heuristic similarity functions.  Across natural languages, these recurrent substring correspondences are found in word pairs known as cognates: words with a common form and meaning across languages. Cognates arise either from words in a common ancestor language (e.g. light/Licht, night/Nacht in English/German) or from foreign word borrowings (e.g. trampoline/toranporin in English/Japanese). Knowledge of cognates is useful for a number of applications, including sentence alignment (Melamed, 1999) and learning translation lexicons (Mann and Yarowsky, 2001; Koehn and Knight, 2002). We propose an alignment-based, discriminative approach to string similarity and evaluate this approach on cognate identiﬁcation. Section 2 describes previous approaches and their limitations. In Section 3, we explain our technique for automatically creating a cognate-identiﬁcation training set. A novel aspect of this set is the inclusion of competitive counter-examples for learning. Section 4 shows how  
 For lexical alignment from comparable corpora,  good results on single words can be obtained from  Current research in text mining favours the quantity of texts over their quality. But for bilingual terminology mining, and for many  large corpora — several millions words — the accuracy of proposed translation is about 80% for the top 10-20 candidates (Fung, 1998; Rapp, 1999; Chiao  language pairs, large comparable corpora  and Zweigenbaum, 2002). (Cao and Li, 2002) have  are not available. More importantly, as terms are deﬁned vis-à-vis a speciﬁc domain with a restricted register, it is expected that the  achieved 91% accuracy for the top three candidates using the Web as a comparable corpus. But for speciﬁc domains, and many pairs of languages, such  quality rather than the quantity of the corpus matters more in terminology mining. Our hypothesis, therefore, is that the quality of the corpus is more important than the quantity and ensures the quality of the acquired terminological resources. We show how im-  huge corpora are not available. More importantly, as terms are deﬁned vis-à-vis a speciﬁc domain with a restricted register, it is expected that the quality rather than the quantity of the corpus matters more in terminology mining. For terminology mining, therefore, our hypothesis is that the quality of the corpora  portant the type of discourse is as a characteristic of the comparable corpus.  is more important than the quantity and that this ensures the quality of the acquired terminological resources.  
Language model (LM) adaptation is important for both speech and language processing. It is often achieved by combining a generic LM with a topic-specific model that is more relevant to the target document. Unlike previous work on unsupervised LM adaptation, this paper investigates how effectively using named entity (NE) information, instead of considering all the words, helps LM adaptation. We evaluate two latent topic analysis approaches in this paper, namely, clustering and Latent Dirichlet Allocation (LDA). In addition, a new dynamically adapted weighting scheme for topic mixture models is proposed based on LDA topic analysis. Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline generic LM. The best result is obtained using the LDA-based approach by expanding the named entities with syntactically filtered words, together with using a large number of topics, which yields a perplexity reduction of 14.23% compared to the baseline generic LM. 
 As with PP attachment, most previous attempts  In this paper we present methods for improving the disambiguation of noun phrase (NP) coordination within the framework of a lexicalised history-based parsing model. As well as reducing noise in the data, we look at modelling two main sources of information for disambiguation: symmetry in conjunct structure, and the dependency between conjunct lexical heads. Our changes to the baseline model result in an increase in NP coordination dependency f-score from 69.9% to 73.8%, which represents a relative reduction in f-score error of 13%. 
This paper addresses the issue of text normalization, an important yet often overlooked problem in natural language processing. By text normalization, we mean converting ‘informally inputted’ text into the canonical form, by eliminating ‘noises’ in the text and detecting paragraph and sentence boundaries in the text. Previously, text normalization issues were often undertaken in an ad-hoc fashion or studied separately. This paper first gives a formalization of the entire problem. It then proposes a unified tagging approach to perform the task using Conditional Random Fields (CRF). The paper shows that with the introduction of a small set of tags, most of the text normalization tasks can be performed within the approach. The accuracy of the proposed method is high, because the subtasks of normalization are interdependent and should be performed together. Experimental results on email data cleaning show that the proposed method significantly outperforms the approach of using cascaded models and that of employing independent models. 
 the extraction and words or phrases indicative of  Even in a massive corpus such as the Web, a substantial fraction of extractions appear infrequently. This paper shows how to assess the correctness of sparse extractions by utilizing unsupervised language models. The REALM system, which combines HMMbased and n-gram-based language models, ranks candidate extractions by the likelihood that they are correct. Our experiments show that REALM reduces extraction error by 39%, on average, when compared with previous work. Because REALM pre-computes language models based on its corpus and does not require any hand-tagged seeds, it is far more scalable than approaches that learn models for each individual relation from handtagged data. Thus, REALM is ideally suited for open information extraction where the relations of interest are not speciﬁed in advance and their number is potentially vast.  class membership (e.g., “cities such as”). However, Zipf’s Law governs the distribution of extractions. Thus, even the Web has limited redundancy for less prominent instances of relations. Indeed, 50% of the extractions in the data sets employed by (Downey et al., 2005) appeared only once. As a result, Downey et al.’s model, and related methods, had no way of assessing which extraction is more likely to be correct for fully half of the extractions. This problem is particularly acute when moving beyond unary relations. We refer to this challenge as the task of assessing sparse extractions. This paper introduces the idea that language modeling techniques such as n-gram statistics (Manning and Schu¨tze, 1999) and HMMs (Rabiner, 1989) can be used to effectively assess sparse extractions. The paper introduces the REALM system, and highlights its unique properties. Notably, REALM does not require any hand-tagged seeds, which enables it to scale to Open IE—extraction where the relations of interest are not speciﬁed in advance, and their num-  
 One major problem with linguistically syntax-  In this paper, we propose forest-to-string rules to enhance the expressive power of tree-to-string translation models. A forestto-string rule is capable of capturing nonsyntactic phrase pairs by describing the correspondence between multiple parse trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only.  based models, however, is that tree-to-string rules fail to syntactify non-syntactic phrase pairs because they require a syntax tree fragment over the phrase to be syntactiﬁed. Here, we distinguish between syntactic and non-syntactic phrase pairs. By “syntactic” we mean that the phrase pair is subsumed by some syntax tree fragment. The phrase pairs without trees over them are non-syntactic. Marcu et al. (2006) report that approximately 28% of bilingual phrases are non-syntactic on their English-Chinese corpus. We believe that it is important to make available to syntax-based models all the bilingual phrases that are typically available to phrase-based models. On one hand, phrases have been proven to be a simple and powerful mechanism for machine translation. They excel at capturing translations of short idioms, providing local re-ordering decisions, and incorpo-  
 lexically correct translations often end up reordered  This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS ap-  incorrectly. Thus, we are interested in modeling the structural divergence encoded by such function words. A key ﬁnding of our work is that modeling the ordering of the dependent arguments of function words results in better translation quality. Most current systems use statistical knowledge obtained from corpora in favor of rich natural language knowledge. Instead of using syntactic knowledge to determine function words, we approximate this by equating the most frequent words as function words. By explicitly modeling phrase ordering around these frequent words, we aim to capture the most important and prevalent ordering productions. 2 Related Work  proach consistently outperforms the baseline system in ordering function words’ arguments and improving translation quality in both perfect and noisy word alignment scenarios.  A good translation should be both faithful with adequate lexical choice to the source language and ﬂuent in its word ordering to the target language. In pursuit of better translation, phrase-based models (Och and Ney, 2004) have signiﬁcantly improved the  
Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT. Given a source sentence and its parse tree, our method generates, by tree operations, an n-best list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation. Experiments show that, for the NIST MT-05 task of Chinese-toEnglish translation, the proposal leads to BLEU improvement of 1.56%. 
 In this paper we provide a means for obtaining  Current phrase-based SMT systems perform poorly when using small training sets. This is a consequence of unreliable translation estimates and low coverage over source and target phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. 
We consider the problem of predictive inference for probabilistic binary sequence labeling models under F-score as utility. For a simple class of models, we show that the number of hypotheses whose expected Fscore needs to be evaluated is linear in the sequence length and present a framework for efﬁciently evaluating the expectation of many common loss/utility functions, including the F-score. This framework includes both exact and faster inexact calculation methods. 
 Eisner, 2005). Nearly all of these approaches have  Unsupervised learning of linguistic structure is a difﬁcult problem. A common approach is to deﬁne a generative model and maximize the probability of the hidden structure given the observed data. Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters. We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance. Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values. This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We ﬁnd improvements both when training from data alone, and using a tagging dictionary. 
 ﬁned to locally-factored indicators on label bigrams  We describe a new loss function, due to Jeon and Lin (2006), for estimating structured log-linear models on arbitrary features. The loss function can be seen as a (generative) alternative to maximum likelihood estimation with an interesting information-theoretic interpretation, and it is statistically consistent. It is substantially faster than maximum (conditional) likelihood estimation of conditional random ﬁelds (Lafferty et al., 2001; an order of magnitude or more). We compare its performance and training time to an HMM, a CRF, an MEMM, and pseudolikelihood on a shallow parsing task. These experiments help tease apart the contributions of rich features and discriminative training, which are shown to be more than additive.  and label unigrams (with any of the observation). Even in cases where inference in log-linear mod- els is tractable, it requires the computation of a partition function. More formally, a log-linear model for random variables X and Y over X, Y deﬁnes:  pw(x, y) =  ew f (x,y)  ew f (x,y)  =  x ,y ∈X×Y ew f (x ,y )  Z (w)  (1)  where f : X×Y → Rm is the feature vector-function  and w ∈ Rm is a weight vector that parameterizes  the model. In NLP, we rarely train this model by  maximizing likelihood, because the partition func-  tion Z(w) is expensive to compute exactly. Z(w)  can be approximated (e.g., using Gibbs sampling;  Rosenfeld, 1997).  In this paper, we propose the use of a new loss  function that is computationally efﬁcient and statis-  tically consistent (§2). Notably, repeated inference  
 beling from incorrect labellings. However, the com-  In this paper, we propose guided learning, a new learning framework for bidirectional sequence classiﬁcation. The tasks of learning the order of inference and training the local classiﬁer are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.  plexity of quadratic programming for the large margin approach prevented it from being used in large scale NLP tasks. Collins (2002) proposed a Perceptron like learning algorithm to solve sequence classiﬁcation in the traditional left-to-right order. This solution does not suffer from the label bias problem. Compared to the undirected methods, the Perceptron like algorithm is faster in training. In this paper, we will improve upon Collins’ algorithm by introducing a bidirectional searching strategy, so as to effectively utilize more context information at little extra cost. When a bidirectional strategy is used, the main  
 on the notion of “information nuggets” to assess an-  The idea of “nugget pyramids” has recently been introduced as a reﬁnement to the nugget-based methodology used to evaluate answers to complex questions in the TREC QA tracks. This paper examines data from the 2006 evaluation, the ﬁrst large-scale deployment of the nugget pyramids scheme. We show that this method of combining judgments of nugget importance from multiple assessors increases the stability and discriminative power of the evaluation while introducing only a small additional burden in terms of manual assessment. We also consider an alternative method for combining assessor opinions, which yields a distinction similar to micro- and macro-averaging in the context of classiﬁcation tasks. While the two approaches differ in terms of underlying assumptions, their results are nevertheless highly correlated.  swers to complex questions. As it has become the de facto standard for evaluating such systems, the research community stands to beneﬁt from a better understanding of the characteristics of this evaluation methodology. This paper explores recent reﬁnements to the nugget-based evaluation methodology developed by NIST. In particular, we examine the recent so-called “pyramid extension” that incorporates relevance judgments from multiple assessors to improve evaluation stability (Lin and Demner-Fushman, 2006). We organize our discussion as follows: The next section begins by providing a brief overview of nugget-based evaluations and the pyramid extension. Section 3 presents results from the ﬁrst largescale implementation of nugget pyramids for QA evaluation in TREC 2006. Analysis shows that this extension improves both stability and discriminative power. In Section 4, we discuss an alternative for combining multiple judgments that parallels the distinction between micro- and macro-averaging often  
 from them. A further answer re-ranking phase is op-  We study the impact of syntactic and shallow semantic information in automatic classiﬁcation of questions and answers and answer re-ranking. We deﬁne (a) new tree structures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines. Our experiments suggest that syntactic information helps tasks such as question/answer classiﬁcation and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers.  tionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classiﬁcation (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic in-  
This paper presents a language-independent probabilistic answer ranking framework for question answering. The framework estimates the probability of an individual answer candidate given the degree of answer relevance and the amount of supporting evidence provided in the set of answer candidates for the question. Our approach was evaluated by comparing the candidate answer sets generated by Chinese and Japanese answer extractors with the re-ranked answer sets produced by the answer ranking framework. Empirical results from testing on NTCIR factoid questions show a 40% performance improvement in Chinese answer selection and a 45% improvement in Japanese answer selection.  process which pinpoints correct answer(s) from the extracted candidate answers. Since the ﬁrst three steps in the QA pipeline may produce erroneous outputs, the ﬁnal answer selection step often entails identifying correct answer(s) amongst many incorrect ones. For example, given the question “Which Chinese city has the largest number of foreign ﬁnancial companies?”, the answer extraction component produces a ranked list of ﬁve answer candidates: Beijing (AP880603-0268)1, Hong Kong (WSJ920110-0013), Shanghai (FBIS358), Taiwan (FT942-2016) and Shanghai (FBIS345320). Due to imprecision in answer extraction, an incorrect answer (“Beijing”) can be ranked in the ﬁrst position, and the correct answer (“Shanghai”) was extracted from two different documents and ranked in the third and the ﬁfth positions. In order to rank “Shanghai” in the top position, we have to address two interesting challenges:  
A sizable corpus of Taiwanese text in Latin script has been accumulated over the past two hundred or so years. However, due to the special status of Taiwan, few people can read these materials at present. It is regrettable that the utilization of these plentiful materials is very low. This paper addresses problems raised in the Taiwanese Southern-Min tone sandhi system by describing a set of computational rules to approximate this system, as well as the results obtained from its implementation. Using the romanized Taiwanese Southern-Min text as source, we take the sentence as the unit, translate every word into Chinese via an online Taiwanese-Chinese dictionary (OTCD), and obtain the part-of-speech (POS) information from the Chinese Electronic Dictionary (CED) made by the Chinese Knowledge and Information Processing (CKIP) group of Academia Sinica. By using the POS data and tone sandhi rules based on linguistics, we then tag each syllable with its post-sandhi tone marker. Finally, we implement a Taiwanese Southern-Min tone sandhi processing system which takes a romanized sentence as an input and then outputs the tone markers. Our system achieves 97.39% and 88.98% accuracy rates with training and test data, respectively. Finally, we analyze the factors influencing error for the purpose of future improvement.  * Department of Computer Science and Information Engineering, National Taiwan University E-mail: {d93001, d93005, cykao}@csie.ntu.edu.tw + Phahng Taiwanese Workshop, http://www.phahng.idv.tw E-mail: kiatgak@gmail.com # Independent scholar E-mail: chenchen@umdnj.edu [Received October 31, 2006; Revised July 6, 2007; Accepted July 11, 2007]  350  Un-Gian Iunn et al.  Keywords: Taiwanese Southern-Min, Written Taiwanese, Tone Sandhi System, Taiwanese Romanization 1. Introduction 1.1 Background and Motivation Taiwanese is often used in daily life in Taiwan, but written Taiwanese is less common by far. Even so, the history of written Taiwanese stands at well over a century [Tiunn 2001]. At present, there are several dozen if not more than a hundred proposed phonetic and writing systems for Taiwanese [Iunn and Tiunn 1999]. The orthography adopted by this article is Peh-oe-ji (POJ, 白話字, also known as Latinized Taiwanese or Missionary Romanization System for Taiwanese). Under the auspices of the National Museum of Taiwanese Literature, the Department of Taiwanese Literature of Cheng Kung University carried out a project titled “The Collection and Cataloging of Taiwanese Peh-oe-ji Literature Data” (CCTPLD). Although many texts have already been lost due to the alternation of political status, this project nevertheless revealed nearly 2,000 POJ books and periodicals, with publication sites spread over Taiwan, Xiamen (Amoy), Shanghai, Guangzhou (Canton), Hong Kong, Singapore, the Philippines, London, Japan, and beyond. The amount of publishing peaked in the 1950’s and 60’s [Iunn and Tan-Tenn, unpublished]. The scope covers both formally published books and periodicals as well as non-published items such as personal letters and medical charts. Later on, the government, citing supposedly detrimental effects of POJ on Mandarin promotion, banned its use and thus caused the rapid decline of this practice. We hope that the extant materials collected by the above-mentioned CCTPLD project can be accessed by more people, as well as contribute to both basic and applied Taiwanese research. As most people nowadays are not familiar with Latinized Taiwanese, use of state-of-the-art text-to-speech technology would enhance the value of these materials to the general public. Tone sandhi represents a challenging problem to be solved before one can successfully transform the written Taiwanese text to its natural speech-like tonal contour. This is because the written form of Latinized Taiwanese represents the tones as "basic tones", the tones of syllables when they are pronounced in isolation. At the level of the word, all syllables except the last one are usually pronounced differently (that is, they manifest tone sandhi). At the level of a whole sentence, in most situations only the last syllables next to the boundary of the phrases or structural markers are read as basic tones, the others being read as sandhi tones. In fact, besides the "regular tone sandhi" mentioned above, there are still several other kinds of tone sandhi phenomena which will be discussed in detail later.  Modeling Taiwanese Southern-Min Tone Sandhi Using Rule-Based Methods  351  We will first formulate the sandhi rules, which are the key to correct pronunciation and the core issue of this paper. The input of our experiment mainly consists of the data collected by the CCTPLD project; these data are processed by our sandhi system to produce sandhi-marked final outputs. Due to the lack of tagged data, we adopt the rule-based model, not the statistical model in this experiment. Figure 1 describes the skeleton of our system, and the webpage http://iug.csie.dahan.edu.tw/nmtl/dadwt/ demonstrates the results. Three of the authors who are native Taiwanese speakers evaluated the outputs for their accuracy. Romanized Taiwanese Southern-Min text (eg : Sî-kàu ē h lí ) [I'll give it to you when it's ready]  Written form with syntactic tagging atta Sî-kàu_D ē_D;V h_D;P;V) lí_R ched (eg : ) Written form with tone sandhi markers (eg : Sî#-kàu# ē h# lí@ ) The corresponding sound file  OTMD & CED Sandhi marking algorithms Sound generator program  Figure 1. Taiwanese Southern-Min tone sandhi system diagram  1.2 The Tone Sandhi Problem Tones in Taiwanese are traditionally analyzed as consisting of piâⁿ [平], siáng [上], khì [去], ji̍p [入], each having im [陰, “yin”] and iâng [陽, “yang”] except siáng. So there are a total of seven tones. Following the sequence of im-piâⁿ[陰平], siáng[上], im-khì[陰去], im-ji̍p[陰入], iâng-piâⁿ[陽平], iâng-khì[陽去], iâng-ji̍p[陽入], they are numbered 1 (high flat), 2 (high to  352  Un-Gian Iunn et al.  low), 3 (low), 4 (middle short), 5 (low rising), 7 (middle flat), and 8 (high short). The tone pitch is described within the parentheses. Please refer to the following examples for tone diacritics. In this paper, all examples are written in Taiwanese. For the sake of apprehensibility, we also add the Mandarin and English translations. Tone sandhi is a very important characteristic of Taiwanese. At the word level, the last syllable is usually pronounced as basic tone and the others as sandhi tones. In example (1), the underlined syllables are pronounced as basic tones, the others as sandhi tones: (1) tâi [台, “platform”] Tâi-gí [台語, “Taiwanese language”] Tâi-gí-bûn [台語文, “written Taiwanese”] Tâi-gí bûn-ha̍k [台語文學, “Taiwanese literature”] Tâi-gí bûn-ha̍k-sú [台語文學史, “history of Taiwanese literature”] At the level of the syllable or the word, tone sandhi may manifest itself in at least the following several ways: (a) Normal sandhi: using reduplicated syllables as examples (the numbers within parentheses are reading tones). (2) (i) tone 1 → tone 7: “chheng-chheng” (7,1) [清清, “clear”] (ii) tone 7 → tone 3: “chēng-chēng” (3,7) [靜靜, “quiet”] (iii) tone 3 → tone 2: “chhiò-chhiò” (2,3) [笑笑, “smiley”] (iv) tone 2 → tone 1: “léng-léng” (1,2) [冷冷, “cold”] (v) tone 5 → tone 7 or 3 (northern Taiwan): “âng-âng” (7/3,5) [紅紅, “red”] (vi) tone 4 → tone 8 (-p/t/k) or 2 (-h): like “sip-sip” (8,4); [濕濕, “moist”] “khoeh-khoeh” (2,4) [擁擠, “crowd”] (vii) tone 8 → tone 4 (-p/t/k) or 3 (-h): like “ti̍t-ti̍t” (4,8) [直直, “straight”]; “jo̍ah-jo̍ah” (3,8) [熱熱, “hot”] (b) Following sandhi: this pattern generally occurs with pronouns or the suffix of names. The tone pitch depends on that of the preceding syllable and is either tone 1 (high), 3 (middle), or 7 (low).  Modeling Taiwanese Southern-Min Tone Sandhi Using Rule-Based Methods  353  (3) (i) “A-eng--a” (7,1,1) [阿英, a personal name] (the second “a” is a suffix) (ii) “góa lâi khòaⁿ -- i” (1,7/3,3,3) [我來看他, “I come to see him/her”] (the basic tone of “i”[他, “(s)he”] is tone 1) (iii) “h --lí” (7,7) [ 給你, “give you” ] (the basic tone of “lí”[你, “you”] is tone 2) (c) Neutral sandhi: the syllable immediately preceding the neutral sandhi (marked orthographically with double hyphens same as (b)) is read as basic tone, and the tones of the neutral sandhi are pronounced softly as if they were tone 3 or tone 4. (4) (i) “Tân--sian-siⁿ” (5,3,3) [陳先生, “Mr. Tân”] (the original tones of “sian-siⁿ ”[先生, “Mr.”] are tone 7 and tone 1) (ii) “kiâⁿ--chhut-lâi” (5,4,3) [走出來, “walk out”] (the original tones of “chhut-lâi” [出來, “out”] are tone 8 and tone 5) (d) Double sandhi: this pattern mostly appears in syllables ending in the glottal stop (-h) and having tone 4. The normal sandhi rules are applied twice in sequence (i.e. tone 4 → tone 2 → tone 1): (5) (i) “beh tha̍k-chu” (1,4,1) [要讀書, “want to read books”] (“beh” [要, “want”] is tone 4, but rather than becoming tone 2, it becomes tone 1) (ii) “khì gōa-kháu” (1,3,2) [去外面, “go outside”] (“khì”[去, “go”] is tone 3, but rather than becoming tone 2, it becomes tone 1) (e) Pre-á sandhi: the syllables before á do not follow normal sandhi rules unless they are tone 1 or 2. (6) (i) tone 1 → tone 7: “sun-á” (7,2) [姪子, “nephew”] (ii) tone 2 → tone 1: “chháu-á” (1,2) [小草, “grass”] (iii) tone 3 → tone 1: “tàⁿ-á” (1,2) [攤位, “stall”] (iv) tone 4 → tone 8 (-p/t/k) or tone 1 (-h): “tek-á” (8,2) [竹子, “bamboo”] “thih-á” (1,2) [鐵,“iron”] (v) tone 5 → tone 7: “l-á ” (7,2) [爐子, “oven”] (vi) tone 7 does not change: “ph-á” (7,2) [簿子, “tablet”] (vii) tone 8 → tone 4 (-p/t/k) or tone 7 (-h): “chha̍t-á” (4,2) [賊, “thief”] “hio̍h-á ” (7,2) [葉, “leaf”]  354  Un-Gian Iunn et al.  (f) Triplicate sandhi: the first syllable of triplicated words does not follow normal sandhi rules unless it is of tone 2, 3, or 4:  (7) (i) tone 1 → tone 5: like “chheng-chheng-chheng” (5,7,1) [清清清, “very clear”] (ii) tone 2 → tone 1: like “ún-ún-ún” (1,1,2) [穩穩穩, “very stable”] (iii) tone 3 → tone 2: like “hèng-hèng-hèng” (2,2,3) [興興興, “very interesting”] (iv) tone 4 → tone 8 (-p/t/k) or tone 2 (-h): like “sip-sip-sip” (8,8,4) [濕濕濕, “very humid”] “bah-bah-bah” (2,2,4) [肉肉肉, “very fat”] (v) tone 5 → (similar to) tone 5: like “kôaⁿ-kôaⁿ-kôaⁿ” (5,7/3,5) [冷冷冷, “very cold”] (vi) tone 7 → (similar to) tone 5: like “chēng-chēng-chēng” (5,3,7) [靜靜靜, “very quiet”] (vii) tone 8 → (similar to) tone 5: like “ti̍t-ti̍t-ti̍t” (5,4,8) [直直直, “very straight”] “pe̍h-pe̍h-pe̍h” (5,3,8) [白白白, “very white”]  (g) Rising sandhi: this pattern usually occurs on loanwords from Japanese; the sandhi tone is similar to tone 5.  (8) “ŏai-siak-chù” (5,8,3) [白襯衫, “white shirt” ] “khăn-páng” (5,2) [看板, “signboard”] “hăn-t-lù ”(5,1,3) [方向盤, “steering wheel”]  We collate the above sandhi phenomena in Table 1.  Table 1. Taiwanese Southern-Min tone sandhi phenomena  Normal Basic tone of syllable sandhi Sandhi tone  
In this paper, a framework for integrated synthesis of Mandarin, Min-nan, and Hakka speech is proposed. To show its feasibility, an initial integrated system has been built as well. Through integration, a model only trained with Min-nan sentences is used to generate pitch-contours for all three languages, same rules are used to generate syllable duration and amplitude values, and the same program module implementing the method, TIPW, is used to synthesize the three languages’ speech waveforms. Also, in this system, each syllable of a language has just one recorded signal waveform, i.e. no chance of unit selection. Under such a restricted situation, the synthetic speech signals still have noticeable naturalness level and signal clarity. Keywords: Speech Synthesis, Pitch Contour Model, TIPW, Time Axis Warping. 1. Introduction There are many languages in Taiwan, including Mandarin, Min-nan, Hakka, and others spoken by smaller population groups. Mandarin has been more extensively studied than the other languages because it is the official language. However, the successful construction of a synthesis model or system for Mandarin does not imply that the same modeling method can be directly applied to another language. Developing speech synthesis systems for other languages is strongly desired because Mandarin is not the mother tongue of most people in Taiwan, and all languages except Mandarin face the crisis of disappearance. If systems developed or speech data collected previously can only be used for Mandarin, then further resources (effort and money) are inevitably needed to study other languages. Such a situation will become more severe if a corpus-based approach [Chou 1999; Chu et al. 2003] is adopted. In addition, there will be inconsistency in prosody and timbre among ∗ Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, 43 Keelung Rd., Sec. 4, Taipei, Taiwan E-mail: {guhy, M9315058, M9215001}@mail.ntust.edu.tw [Received October 31, 2006; Revised April 17, 2007; Accepted April 25, 2007]  372  Hung-Yan Gu et al.  independently developed speech synthesis systems for different languages. Therefore, a better approach is to construct a more generalized system that can synthesize not only Mandarin but also Min-nan and Hakka speech. Such an approach, if successfully realized, can not only save resources but also obtain much higher consistency among the synthesized speech for different languages. Another advantage is that an improvement, when made to a system component, can immediately benefit all the languages supported. Mandarin, Min-nan, and Hakka are all syllable prominent languages, and are all tonal languages. Hakka has many accents found in users in Taiwan, of which “four-country” and “sea-land” are the primary ones. If not specified, the sea-land accent is the default accent representing Hakka in this paper. This is because it has more lexical tone and more unique syllables than the four-country accent has, and the authors believe the speech signal of the sea-land accent is more difficult to synthesize. As to the number of different syllables (not distinguishing lexical tones), Mandarin has 405, Min-nan has 833, and Hakka has 783 [Yu 1999]. The languages also vary in the numbers of different lexical tones, being 5, 7, and 7, respectively, for Mandarin, Min-nan, and Hakka. Since the numbers, 405, 833, and 783, are not large, syllable is commonly chosen as the speech unit for synthesis processing. Actually, in this system, each syllable (tone not distinguished) of a language has only one recorded utterance. That is, no extra units are available to do unit selection, and each syllable’s waveform must be manipulated to synthesize speech signals with different required prosodic characteristics. Note that the focus of this study is in the system framework of an integrated speech synthesis system for the three languages. To show the feasibility of the proposed framework, a workable integrated synthesis system is built. This system is just in its initial phase. Therefore, there will be many unsolved problems in the details. Most of these problems belong to text analysis since signal synthesis is the major concern in this research while text analysis is just a minor concern. In general, a speech synthesis system can be divided into three subsystems, i.e., (a) text analysis, (b) prosodic parameter generation, and (c) speech waveform synthesis [Shih et al. 1996; Wang 1998]. The framework for the integrated synthesis system is also divided into such subsystems. The main processing flow of this framework is shown in Figure 1. The first two processing blocks are for text analysis, the middle two blocks are for prosodic parameter generation, and the last two blocks are for signal waveform synthesis. The synthesis system built is an integrated system, not a bundle of three independent systems for the three languages. This is because the program modules in the three subsystems are all shared in synthesizing the three languages’ speech. For example, the model for pitch-contour parameter generation is shared (or adapted) between Mandarin and Hakka, although it is originally trained with Min-nan sentences. The explanations for why the program modules can be shared are given in the following sections.  A System Framework for Integrated Synthesis of  373  Mandarin, Min-Nan, and Hakka Speech  text input  text-analysis A text-analysis B syllable pitch-contour generation (ann, hmm) amp. & dur. value generation (rule-based)  pronunciation dictionary ANN & HMM model paramtr  time-warping mapping (phone boundary mark) signal waveform synthesis (TIPW)  speech-units: waveform, pitch peaks, phone boundaries.  speech signal  Figure 1. Main processing flow.  In the subsystem of text analysis, the first block in Figure 1, “Text Analysis A”, parses  the input text to recognize tags and slice the text string into a sequence of Chinese-character or  alphanumeric-syllable tokens. Then, each Chinese-character token is tried in the second block  (Text Analysis B), to check if it can be looked up in a pronunciation dictionary in order to  determine the comprising character’s pronunciation syllable. For the subsystem of prosodic  parameter generation, the pitch-contour parameters of a syllable are determined by a mixed  model of ANN (artificial neural network) [Chen et al. 1998; Lee et al. 1991] and SPC-HMM  (syllable pitch-contour hidden Markov model) [Gu et al. 2000] in the third block of Figure 1.  As to the parameters, amplitude and duration, their values are determined with a rule-based  method [Chiou et al. 1991; Shiu 1996] in the fourth block of Figure 1. For the subsystem of  signal waveform synthesis, a piece-wise linear time-warping function is first constructed in  the fifth block of Figure 1. Then, the method of TIPW (time-proportioned interpolation of  pitch waveform) [Gu et al. 1998] is used in the sixth block to synthesize speech waveforms.  TIPW is an improved variant of PSOLA [Modulines 1990]. To show that the integrated  system has noticeable performance in naturalness and signal clarity, the authors have set up a  web page, http://guhy.csie.ntust.edu.tw/hmtts/, to demonstrate synthetic speech for the three  languages.  However,  for  the  purpose  of  online  testing,  http://guhy.csie.ntust.edu.tw/hmtts/speak.html is preferable.  374  Hung-Yan Gu et al.  2. Text Analysis  In Min-nan and Hakka, there are still many spoken words whose corresponding ideographic words are not known. Therefore, the authors made a decision that, in the input text, Chinese characters may be interleaved with syllables spelled in alphanumeric symbols. For example, “cit-4 tou-5 人” is a Min-nan word whose first two syllables are spelled in alphanumeric symbols. This decision implies that the input text must first be parsed into a sequence of Chinese-character and alphanumeric-syllable tokens. For example, “今天 mai-3 ki-3” is parsed into “今天”, “mai-3”, and “ki-3”. The number at the end of a syllable indicates the lexical tone of the syllable. This parsing processing is executed in the first block of Figure 1.  In addition, the authors have defined several kinds of tags to help carry some necessary controlling information. For example, the tag, “@>2”, may be placed between two sentences to command that the sentences behind the tag will be synthesized to Hakka speech until another language-selection tag is encountered. Such a language-selection tag is needed because it is intended that the sentences of an article may be alternatively synthesized to different languages’ speech. Another kind of tag is “@>dxxx”. This tag may also be placed between two sentences to change the speaking rate of the sentences behind it. The part, “xxx”, in the tag represents three decimal digits to specify how many milliseconds on average a syllable will be synthesized to. In addition to the two tags explained, several other kinds of tags are also defined. The details are listed in Table 1. The parsing of such tags is also executed in the first block of Figure 1.  Table 1. Tags and their meanings.  Tag symbol  Explanation  @>x @>dxxx @>txxx @>vxxx <, > *  language selection, x may be 0, 1, or 2. 0: Mandarin, 1: Min-nan, 2: Hakka speaking rate, syllable average duration in xxx milliseconds. average tone height, in xxx Hz. vocal track extended (or shrunken) to xxx percents of original length. word-constructing tag, e.g., <cit-4 tou-5> breath-break tag  After an input sentence is parsed into a sequence of tokens, the pronunciation syllables for each Chinese character token are determined in the second block of Figure 1. According to the language-selection tag, the corresponding pronunciation dictionaries are consulted to check if the prefix part of a token can be found in the dictionaries. A dictionary consisting of longer words is tried before a dictionary consisting of shorter words. Currently, the authors have collected 55,000, 12,000, and 19,000 multi-syllabic words, respectively, for Mandarin,  A System Framework for Integrated Synthesis of  375  Mandarin, Min-Nan, and Hakka Speech  Min-nan, and Hakka. Note that input text is usually composed in Mandarin written words. Therefore, use of the dictionary plays a role of word translation. For example, “今天” (today) in Mandarin is translated as “今仔日”, in Min-nan, which is pronounced as, “gin-1 a-2 rit-8”. Another example, “筷子” (chopstick) in Mandarin is translated to “箸” which is pronounced as, “di-7”. These examples also show that the words obtained after translation may have longer or shorter lengths. After a word is found in a dictionary or a block of syllables bounded with the tags, “<” and “>”, is parsed out, one knows the boundaries of a word and its syllabic composition. Then, tone-sandhi rules for the currently selected language can be applied to the compositional syllables of the word. This is executed in the second block of Figure 1. Note that different languages have very different tone-sandhi rules. For example, in a word of Mandarin, if two adjacent syllables are both of the third tone, then the former one must have its tone changed to the second tone. As another example, consider the tone-sandhi rule applied to a word of Min-nan that every syllable of a word except the final one must have its tone changed to its inflected tone. 3. Prosodic Parameter Generation The prosodic parameters of a syllable include pitch-contour, duration, amplitude, and leading pause. The generation of prosodic parameter values plays a very important role because it determines the level of naturalness of synthesized speech. Therefore, much effort has been devoted to investigate models (or methods) for generating prosodic parameter values [Chen et al. 1998; Gu et al. 2000; Lee et al. 1993; Wu et al. 2001; Yu et al. 2002]. Among these prosodic parameters, pitch-contour is the most important one for obtaining a higher naturalness level. Therefore, the authors have spent considerable effort in investigating different kinds of models, HMM [Gu et al. 2000], ANN, and a mixed model of both [Gu et al. 2005b]. In the third block of Figure 1, a mixed model of HMM and ANN is used to generate pitch-contours. Here, model mixing means taking a weighted sum of two pitch-contours generated respectively by HMM and ANN. Note that, in this study, pitch-contour models, HHM and ANN, are both trained with Min-nan spoken sentences. Then, through tone mapping, the Min-nan trained and mixed model is adapted to generate pitch-contours for Hakka and Mandarin. By such a sharing of pitch-contour model, the effort in training other languages’ pitch-contour models can be saved. In contrast to pitch-contour, duration and amplitude are thought to be minor factors for naturalness. Hence, only a rule-based method is used in the fourth block of Figure 1 to generate their values [Chiou et al. 1991; Shiu 1996]. The authors program three sets of rules for the three layers, syllable layer, word layer, and breath-group layer. In the syllable layer, a syllable containing different vowel phonemes, /a/, /i/, /u/, /e/, or /o/, is assigned different  376  Hung-Yan Gu et al.  amplitude values, 0dB, -4dB, -3dB, -2dB, or -1dB. In the word layer, the first syllable of a word is emphasized 0.5 dB in amplitude. Finally, in the breath-group layer, the first two syllables of a group are emphasized 1dB and 0.5dB respectively. In addition, the last two syllables of the last breath-group of a sentence are deemphasized 0.5dB and 1dB respectively. By interaction of these rules in the three layers, the generated amplitude and duration values appear to have some randomness, and can present a certain level of naturalness.  3.1 Syllable Pitch Contour HMM A syllable at the beginning of a sentence is usually uttered with higher pitch than one at the end, i.e., the phenomenon of declining. With respect to this phenomenon, the authors imagine that there are three prosodic states corresponding to sentence-initial, sentence-middle, and sentence-final. However, how to assign a sentence’s syllables to these states is not explicitly known. Therefore, the authors imagine these prosodic states are hidden and will simulate them by the hidden states of a left-to-right hidden Markov model [Rabiner et al. 1993]. Besides the influence of prosodic states, the lexical tones of a syllable and its adjacent syllables also have strong influences. Therefore, the authors take into account the lexical-tones of a syllable and its adjacent syllables, and call such an HMM as syllable pitch-contour HMM (SPC-HMM).  The height and shape of a syllable’s pitch-contour are mainly influenced by the lexical tones of the syllable and its immediately adjacent syllables. Therefore, the authors decide to combine the t-th syllable’s lexical tone and pitch-contour VQ (vector quantization) code with its left and right adjacent syllables’ lexical tones to define the t-th observation symbol, Ot, as  Ot = 392 ⋅ Xt−1 + 56 ⋅ Xt + 8 ⋅ Xt+1 +Vt ,  (1)  0 ≤ Xt ≤ 6, 0 ≤ Vt ≤ 7.  where Xt is the lexical-tone number of the t-th syllable, and Vt is the pitch-contour VQ code of the t-th syllable in a training sentence. Actually, the number, Xt, is indirectly obtained, i.e. lexical-tone number eight is mapped to six beforehand, and then the lexical-tone number is decreased by one. In Equation (1), the number, eight, is multiplied because there are eight codewords in each tone’s pitch-contour VQ codebook. The numbers, 56 and 392, may be viewed as 7×8 and 7×7×8, respectively, and 7 is the number of different lexical tones in Min-nan and 8 is the number of code-words in a VQ codebook. When t=1, i.e. staying at the first syllable of a training sentence, Xt-1 is undefined. In this case, the definition of Ot is modified to 7×7×7×8 + 56 Xt + 8 Xt+1 + Vt. Similarly, the definition of Ot for the last syllable of a sentence must also be modified [Gu et al. 2000]. Before VQ encoding, the pitch-contour of each syllable from a training sentences is first time normalized and then pitch-height normalized [Gu et al. 2000]. Time normalization means  A System Framework for Integrated Synthesis of  377  Mandarin, Min-Nan, and Hakka Speech  placing 16 measuring points equally spaced in time. Then, a pitch-contour is represented as a vector of 16 frequency values (in log Hz scale), called a frequency vector. After time normalization, these frequency vectors must be normalized in pitch-height to eliminate the influence of the speaker’s mood at the time of recording. Totally, the authors have recorded 643 Min-nan training sentences that are comprised of 3,696 syllables. Next, consider the generating of pitch-contours by using SPC-HMM. When a sentence is input, it will be analyzed first by the textual analysis components. Hence, its pronunciation-syllable sequence is available. For example, for the short sentence, “我來啊” (I have come), of Min-nan, its corresponding syllable sequence is “qua-1 lai-5 a-7”. Then, one can encode the three adjacent syllables’ lexical tones partially (because VQ code, Vt, is not known yet) according to Equation (1). Since each lexical tone has 8 codewords in its pitch-contour VQ codebook, each syllable of the sentence has 8 possible encoded observation symbols corresponding to it. For example, for the second syllable “lai-5”, its possible encoded observation symbols are 392(1-1)+56(5-1)+8(7-1)+Vt, i.e. the value range from 264 to 271 since the value of Vt is not determined yet. Therefore, in the synthesis phase (or testing phase), besides the time (syllable index within a sentence) and state (prosodic-state index) axes, a third axis to index the 8 possible observation-symbol candidates, must be added. Then, the conventional two-dimensional (time and state) DP (dynamic programming) algorithm for speech recognition is extended to a three-dimensional DP algorithm and used to search the most probable path [Gu et al. 2000]. The main part of the extended algorithm is shown in Equation (2),  δt (n, k) =  ⎡ ⎢ max ⎣n−1≤i≤n  max 0≤ j≤7  δ t −1 (i,  j) ⋅ ai,n  ⎤ ⎥ ⎦  ⋅ bn (Otk ),  0 ≤ n ≤ 2, 0 ≤ k ≤ 7,  (2)  where Otk represents the k-th possibly encoded observation symbol at time t, n and i are state indices, ai,n is state-transition probability, bn (•) is symbol-observing probability at state n, and δt (n, k) is the largest obtainable probability of a best path that stays at state n and selects the k-th observation symbol at time t. According to the best path found, the state value and k value of Otk at each time point, t, can then be determined. Accordingly, the pitch-contour VQ code, Vt, for the t-th syllable of the sentence is set to the value of k determined at time t.  3.2 Syllable Pitch Contour ANN The architecture of the artificial neural network used here is shown in Figure 2. It is designed to be a recurrent type ANN in order to have the prosodic state kept internally. The input layer of the ANN has 8 ports to receive contextual parameters. For the hidden and recurrent hidden layers, the numbers of nodes are both set to be 30, according to experiment results. After a syllable’s contextual parameters are input and processed, a pitch contour represented as a 16  378  Hung-Yan Gu et al.  dimensional frequency vector is output in the output layer. This frequency vector can be interpreted as a sequence of 16 frequency values along a pitch-contour. 8 contextual param eters  16 dim . pitch-contour  Figure 2. The architecture of the ANN studied here.  Here, the contextual parameters, i.e. the inputs to the ANN, are appropriately selected to provide essential contextual information and to lower the quantity of required training sentences. In detail, the contextual parameters are as listed in Table 2. As there are seven lexical tones in Min-nan, 3 bits are enough to represent them. The numbers of different syllable initials and finals are 18 and 61, respectively. Hence, 5 and 6 bits, respectively, are used to represent current syllable’s initial and final types. As to the parameter of the previous syllable’s final, the authors first group the 61 possible finals into 12 classes, using only 4 bits to represent the 12 final classes. Similarly, the authors first group the 18 possible initials into 6 classes, and use only 3 bits to represent the 6 initial classes for the next syllable’s initial. Grouping is made here because the quantity of recorded training sentences is not large enough to let the ANN learn the influences of the detailed combinations of current syllable and previous (or next) syllable. Syllable initial and final classes grouped here are detailed in Table 3 and 4, respectively. The last item in Table 2, the time-progress index, is intended to carry timing information. If the current syllable is the k-th syllable of a sentence of N syllables in length, then the value of time-progress index is set to the floating-point number k/N.  Table 2. Contextual parameters.  Items Bits  Tone of previous syllable 3  Final class of previous syllable 4  Tone of current syllable 3  Initial of current syllable 5  Final of current syllable 6  Tone of next syllable 3  Initial class of next syllable 3  Time progress index void  A System Framework for Integrated Synthesis of  379  Mandarin, Min-Nan, and Hakka Speech  Table 3. Syllable-initial classes. (General Phonetic Symbol System)  Classes  
In this paper, three studies of Min-Nan speech processing are presented. The first study concerns the implementation of a high-performance Min-Nan TTS system. On the basis of the waveform templates of 877 base-syllables used as basic synthesis units and through the application of the RNN-based prosody generation method and the PSOLA algorithm for prosody modification, this Min-Nan TTS system can convert texts, represented in both Han-Luo (漢 羅 ) and Chinese logographic writing systems, into natural Min-Nan speech. An informal, subjective listening test confirms that the system performs well and the synthetic speech sounds natural for well-tokenized Min-Nan texts and for automatically tokenized Chinese logographic texts. The second investigation concerns the realization of a Min-Nan speech recognizer. It adopts the initial-final-based HMM approach with a simple base-syllable bigram language model. A base-syllable recognition rate of 65.1% has been achieved. Finally, a model-based tone labeling method is presented. This method adopts a statistical model to eliminate the affections of all factors other than tone on the syllable pitch contour for automatic tone labeling. Experimental results confirm that this method outperforms the conventional VQ-based approach. Keywords: Min-Nan Text-to-Speech System, Speech Recognition, Model-Based Tone Labeling 1. Introduction Min-Nan is one of the subcategories of the Min dialect, which is one of the seven Chinese dialect families [Yuan et al. 1989]. Aside from some pockets of speakers scattered over ∗ Department of Communication Engineering, Chiao Tung University Tel: +886-3-5731844, Fax: +886-3-5710116 E-mail: yrwang@mail.nctu.edu.tw + Department of Foreign Languages and Literature, Chung Hua University [Received November 8, 2006; Revised June 29, 2007; Accepted July 5, 2007]  392  Wei-Chih Kuo et al.  Southeast Asia, varieties of Min-Nan are spoken in southern Fujian, eastern and southeastern Guangdong, and are spread over much of the islands of Hainan and Taiwan, where it is spoken by approximately 73.3 percent of the inhabitants [Huang 1995]; hence, it is often called Taiwanese. In recent years, even though Min-Nan has captured much attention in Taiwan’s academic community, research related to its speech processing still remains small due to (1) non-unified writing standards, (2) the various accents of Min-Nan used in Taiwan, and (3) lack of non-public Min-Nan speech and text corpora. These multiple factors may lead to hindering progress in Min-Nan speech processing technology. However, in spite of the aforementioned deficiencies, which add a degree of difficulty to the automatic processing of this language, three achievements in the technology of Min-Nan speech processing have been made in our study, including the implementation of a high-performance Min-Nan TTS system, the realization of a Min-Nan speech recognizer, and a model-based tone labeling method. The paper is organized as follows. Section 2 gives a brief introduction to the background of Min-Nan. Section 3 presents the proposed Min-Nan TTS system. Section 4 discusses the realization of a Min-Nan speech recognizer. Section 5 describes a new model-based tone labeling method for Min-Nan speech. Some conclusions are given in the last section. 2. A Brief Description of Min-Nan Like Mandarin and most other Chinese dialects, Min-Nan is monosyllabic in nature, which means that, basically, every syllable is a free morpheme with a meaning value, and that syllable is the unit for pronunciation and every character in text reading is assigned one, but not the only, syllabic sound. The syllabic structures of both Min-Nan and Mandarin can be described in terms of traditional Chinese philology, where syllable is conventionally viewed to be formed by two constituents: the “initial”, a consonantal onset, and the “final”, made up from a prenucleus onglide, the nucleus – the only obligatory syllabic element, and a coda. Compared with Mandarin, which has 21 initials, 37 finals, and 408 base-syllables, which are legitimate syllables formed by rule-governed combinations of initials and finals, Min-Nan has 18 initials, 82 finals, and 877 base-syllables. In addition to the differences in the numbers of the above-mentioned syllabic constituents and base-syllables, Min-Nan and Mandarin also show differences in the types of syllables, which are often classified by Chinese linguists into “checked” or “entering” syllables, namely syllables ending in a plosive coda (-p,t,k, and a glottal stop), and “smooth” or “slack” syllables, namely syllables ending in a non-plosive. Of the two dialects, only Min-Nan has checked/entering syllables, which leads to different prosodic features associated with syllable types from those of Mandarin. Min-Nan is a tonal language, where every syllable has an inherent tone, and tones of different pitch values function to distinguish different lexical meanings. [Yang 1999] Min-Nan  Some Studies on Min-Nan Speech Processing  393  has 8 tones, including 7 lexical tones and one degenerated tone, each of which displays a distinct pitch contour. Moreover, based on the type of syllable, tones inherent in entering/checked syllables are termed entering/checked tones accordingly, and those in smooth/slack syllables are called non-entering/non-checked tones. If syllabic tones are under consideration, Min-Nan has approximately 2000 syllables. It is also worth a mention in passing that, despite the fact that in Min-Nan mono-syllable is held to be the basic pronunciation unit, in actual speech mono-syllabic morphemes are not uttered independently; instead, two or more mono-syllabic morphemes, to convey meaning relationship, are concatenated to form meaningful or syntactic poly-syllabic units, which generates changes in the inherent pitch contours of the concatenated syllables. This tonal variation is called “tone sandhi,” a very well-known term used to describe the tonal changes depending on the tonal environment in which poly-syllabic words occur. As for the writing system, although no consistent written forms have been standardized for Min-Nan, two sets of writing systems have been more widely accepted in Taiwan, namely “Romanization” or “Luo Ma Pin Yin” (羅馬拼音) and “Han Luo” system (漢羅系統). In the former, Roman letters are used to spell or transcribe Min-Nan speech, and numbers to specify its tones. This writing system has been widely used among churches to transcribe the Bible that has been translated into Min-Nan. With limited letters and numbers, Romanization provides an easy way to learn the pronunciation of Min-Nan. Therefore, it is not uncommon to see many functionally illiterate Min-Nan elderly churchgoers who cannot read Chinese characters but can recite in Min-Nan scriptures in the Bible written in Romanization. However, since most of the Min-Nan native speakers are literate, and possible ambiguity may be caused by homophones when Chinese characters are not shown, the other writing system, namely Han-Luo system (a hybrid from Chinese characters and Romanization) is used more often in written texts. Unfortunately, the problem still exits in the inconsistency of the Chinese characters selected to represent Min-Nan words or expressions. Except for some popular words, people often choose by preference a string of Chinese characters with similar pronunciations to Min-Nan to represent a Min-Nan word. This increases the degree of difficulty of text analysis for Min-Nan speech processing. Another linguistic phenomenon worth noting is that, for many Min-Nan syllables, two pronunciation styles co-exist. The first one is called Bai Hua (白話) – the vernacular reading – which is widely used in daily conversation. The other, referred to as Wen Yan (文言) – literary reading – is restrictedly used in reading poetry, some numbers, or in terms used for naming people, buildings, festivals, and so forth.  394  Wei-Chih Kuo et al.  3. An Implementation of Min-Nan TTS System  In this section, the implementation of a high-performance Min-Nan TTS system is presented. Figure 1 shows a block diagram of the proposed Min-Nan TTS system. It is worth noting that such an approach has been successfully applied to developing a high-performance Mandarin TTS system [Chen et al. 1998] [Chen et al. 2000] [Ho et al. 2000]. The system consists of four main functional blocks: a text analyzer, a recurrent neural network (RNN)-based prosody generator, an acoustic inventory, and a PSOLA speech synthesizer. Input text is first tokenized into word/syllable sequence by the text analyzer. The waveform sequence corresponding to the syllable sequence is then formed by the acoustic inventory. Meanwhile, some linguistic features are extracted from the syllable sequence and used in the RNN-based prosody generator to generate necessary prosodic parameters. Afterwards, the PSOLA speech synthesizer uses these prosodic parameters to modify the prosody of the waveform sequence and generate the output synthetic speech. In the following subsections, we will discuss these four main functional blocks in detail.  input Min-Nan or Chinese text  Text Analyzer  base-syllable sequence  linguistic feature  Acoustic Inventory  RNN-based Prosody Generator  waveform sequence  prosodic parameters  PSOLA Speech Synthesizer  synthetic speech  Figure 1. A schematic diagram of the proposed Min-Nan TTS system.  3.1 The Text Analyzer The function of the text analyzer is first to tokenize the input text into word sequence and then extract relevant linguistic features from the sequence. Two kinds of input texts are processed. One kind is Min-Nan text represented in the hybrid written form of Han-Luo. Another kind of text is represented in Chinese characters only. Figure 2 displays the block diagram of the text analyzer. It first converts an input text into a Unicode sequence in preprocessing. Here, a look-up table is used to find all syllables represented in Romanized form. It then uses two lexica and a long-word-first criterion to convert the Unicode sequence into a word sequence.  Some Studies on Min-Nan Speech Processing  395  The first lexicon is a Min-Nan lexicon. It contains about 120,000 entries represented in the Han-Luo system. Each entry is a word with a length in the range of 1-6 syllables. The second lexicon, with 110,000 entries, is a Chinese-to-Min-Nan lexicon. It is an extended version of our Chinese lexicon in which Chinese words are transferred to Min-Nan syllable sequence character by character. The use of the Chinese-to-Min-Nan lexicon helps us solve the out-of-vocabulary problem encountered in the text analysis. This also makes the system possess the capability of processing input Chinese text. input text in Big5 code  preprocessing  Romanization syllable table  syllable sequence in Unicode  word tokenization  Min-Nan lexicon Chinese-to-Min-Nan lexcion  bracketing rules  character-duplication rules determiner-measure rules  base-syllable sequence  tone sandhi rules  linguistic feature extraction  linguistic features Figure 2. A functional block diagram of the text analyzer. We then use two bracketing rules to construct two types of compound words which are not contained in the lexicon [Huang 2001]. One is for character-duplicated compound words and the other is for determiner-measured compound words. Here, we also decide whether to pronounce the number of a determiner-measured compound word in the style of vernacular reading or in literary reading. For instance, “1998” should be pronounced in the second style as “it kiu2 kiu2 bat”, while “兩萬一千八百” (twenty one thousand eight hundred) is pronounced in the first style as “lng7 ban7 chit chheng peh pah”. After obtaining the word sequence, a set of tone sandhi rules is then explicitly applied to change the lexical tones of all syllables into the ones to be pronounced [Huang 2001]. Basically, all syllables except the final one of a word chunk (or pronunciation group) have to change their tones. These rules [Cheng 1993] are listed below:  396  Wei-Chih Kuo et al.  1→ 7  7→ 3  3→ 2  2→ 1  5→  ⎧7  ⎨ ⎩  3  sou th n orth  (1)  4 ( p,t,k ) ↔ 8 ( p,t,k )  4h → 2  8h → 3  Here, an arrow indicates the way a tone changes, e.g., Tone 2 will change to Tone 1; “north” and “south” mean the northern and southern parts of Taiwan; and “p”, “t”, “k”, and “h” represents entering tones. Besides, four additional rules [Cheng 1993] are used for special cases where a syllable preceding the special character “仔, a function word” (/a/) has been changed to Tone 2 or 3:  7 →3 →7 8h →3→7 (2) 3 →2 →1 4h →2→1  For instances, 鋸(ki3→ki1)仔(saw) and 葉(hioh8→hioh7)仔(leaf). An advantage of the approach of using explicit tone sandhi rules is that it results in obtaining an RNN-based prosody generator with high efficiency on learning phonological rules of human’s prosody generation. Two sets of linguistic features are then extracted from the word sequence. One is the syllable sequence, which is extracted directly from the word sequence by referring to the lexicon. This will be used in the acoustic inventory to form the basic waveform template sequence. Another consists of two subsets of syllable-level and word-level linguistic features and is used in the RNN-based prosody generator to synthesize proper prosodic parameters. The subset of syllable-level linguistic features contains four parameters: the initial type, final type, and tone of the current syllable, and the position of the current syllable in the current word. The subset of word-level linguistic features includes two sequences of word length and PM.  3.2 The RNN-Based Prosody Generator The RNN-based prosody generator uses four RNNs to separately generate four types of prosodic parameters for the current syllable: 4 pitch-contour parameters [Chen et al. 1990],  Some Studies on Min-Nan Speech Processing  397  initial and final durations, log-energy level, and the following pause duration. All four RNNs have the same architecture shown in Figure 3. Each RNN is a four-layer network with outputs of the two hidden layers and the output layer being fed back to their own inputs. An RNN of this architecture has been proven in previous studies to be effective in exploring the contextual information of the input linguistic features for the generation of proper output prosodic information [Chen et al. 1998]. Table 1 shows the input linguistic features used in these four RNNs.  word-level linguistic features  syllable-level linguistic features  Hidden Layer I  Hidden Layer II  Output Layer  pitch contour/log-energy level/ initial & final durations/pause duration Figure 3. The architecture of the RNN used in the TTS system. Table 1. The input linguistic features used in the four RNNs for generating syllable pitch contour, initial and final durations, syllable energy level, and pause duration. Here “common” means features commonly used for all four RNNs.  common  1. tone of current syllable 2. position of current syllable in a word  Pitch contour  1. tone of next syllable 2. initial types of current and next syllables  syllablelevel linguistic features  Initial and final durations energy level  1. initial and final types of current syllable 2. light pronunciation of current syllable 1. initial and final types of current syllable 2. light pronunciation of current syllable  1. initial and final types of current syllable  pause duration  2. light pronunciation of current and next syllables 3. tone of next syllable  4. existence a break following a long word?  word-level 1. lengths of current and next words  linguistic 2. existence of special PM following the next word whose length equals to 1?  features 3. PM type following the current word  (common) 4. POSs of the current and next words  398  Wei-Chih Kuo et al.  These four RNNs can be trained using a large, single-speaker speech database following the back-propagation through time (BPTT) algorithm [Haykin 1994]. The BPTT algorithm is a supervised training algorithm used to learn the mapping from input linguistic features extracted from the input text to output prosodic parameters extracted from the associated utterance. For preparing those inputs and outputs, all texts of the database are manually processed to obtain the word and POS sequences, and the associated utterances are also manually segmented. A further processing of the database is also done for extracting some additional features to improve the efficiency of RNN training. The further processing includes: (1) all minor and major breaks occurring at inter-syllable locations without PMs are manually detected and labeled with special marks; (2) some special characters (referred to as “虛詞, function word”) which are consistently pronounced lightly and short are marked, e.g., “甲” in “互氣甲, be angered” and “仔” in “囝仔, child”; (3) all 5-syllable and 6-syllable words are classified respectively into {2-3, 3-2} and {2-2-2, 3-3} pronunciation patterns; and (4) pitch contours of all short syllables are manually refined. Finally, we modify the learning process of the RNN for inter-syllabic pause duration d. Instead of letting the RNN learn the real pause duration, we first classify the pause duration into four classes: short (d ≤ 75 ms ), medium ( 75 ms ≤ d ≤ 175 ms ), long (175 ms ≤ d ≤ 475 ms ), and very long ( 475 ms ≤ d ). The pause duration of the “short” class was further normalized with respect to the mean and standard deviation of the final types (2 types: with and without entering tone) of the processing syllable and the initial types (4 types) of the preceding syllable. We then let the RNN learn (1) the class of the pause duration and (2) the pause duration when it belongs to the “short” class. This change can let the RNN take care of both the detail of short pause duration and rough classification of long pause duration. 3.3 The Acoustic Inventory The function of the acoustic inventory is to generate a waveform template sequence for each base-syllable sequence given by the text analyzer. It is a look-up table containing waveforms templates of all 877 base-syllables which are the basic synthesis units used in our system. All of the waveform templates are obtained from isolated-syllable utterances pronounced clearly by a male speaker. All of the speech signals are directly recorded digitally, using a PC with a sound card. The sampling rate is 20 kHz. Each utterance is manually pre-processed to detect ending-points and to label pitch marks. 3.4 The PSOLA Speech Synthesizer The function of the PSOLA speech synthesizer is to generate the output synthetic speech by modifying the waveform template sequence of the base-syllable sequence given by the acoustic inventory using the prosodic parameters given by the RNN-based prosody generator.  Some Studies on Min-Nan Speech Processing  399  Modifications include changing the pitch contour for each syllable, adjusting initial and final durations for each syllable, scaling the energy level for each syllable, and setting the pause duration for each inter-syllable location. Finally, the output synthetic speech is generated by a 16-bit Sound Blaster card.  3.5 Experimental Results  Performance of the proposed Min-Nan TTS system was examined by simulation, using a male speaker database. The database contains 255 utterances including 130 sentential utterances with lengths in the range of 5-30 syllables and 125 paragraphic utterances with lengths in the range of 85-320 syllables. The total number of syllables is 23,633. In addition, a set of 877 isolated base-syllable utterances was recorded for constructing the acoustic inventory. Most of these 877 utterances are syllables with Tone 1. All speech signals were digitally recorded at a 20 kHz rate. All utterances and associated texts were manually pre-processed in order to extract the acoustic features and the linguistic features required to train and test the system.  We first examined the performance of the RNN-based prosody synthesizer. Table 2 lists the root mean square errors (RMSEs) of the synthesized prosodic parameters. RMSEs of 10.2 (12.4) ms, 26.2 (32.4) ms, 15.1 (21) ms, 0.79 (0.80) ms/frame and 2.28 (3.12) dB were achieved in the inside (outside) test for initial duration, final duration, pause duration, pitch contour and log-energy level, respectively. Here, in the calculation of RMSE for pause duration, we set the target pause duration of the three classes of “medium”, “long” and “very long” to be 75ms. The classification errors for the four pause duration classes were 12.1% and 13.8% for the inside and outside tests, respectively. Actually, over 80% of classification errors were associated with Class 2. Figure 4 shows a typical example of these synthesized prosodic parameters. It can be seen from the figure that the synthesized prosodic parameters of most syllables matched well with their original counterparts.  Table 2. The experimental results of RNN prosody generation.  inside outside  initial duration (ms)  10.2  12.4  final duration (ms)  26.2  32.4  pause duration (ms)  15.1  21.0  pitch contour (ms/Frame)  0.79  0.80  energy level (dB)  2.28  3.12  400 (a) 9 ms/frame 7  5 (b) 100 ms 60  20 (c) 3  100 ms  2  
Taiwanese Child Language Corpus (TAICORP) is a corpus based on spontaneous conversations between young children and their adult caretakers in Minnan (Taiwan Southern Min) speaking families in Chiayi County, Taiwan. This corpus is special in several ways: (1) It is a Minnan corpus; (2) It is a speech-based corpus; (3) It is a corpus of a language that does not yet have a conventionalized orthography; (4) It is a collection of longitudinal child language data; (5) It is one of the largest child corpora in the world with about two million syllables in 497,426 lines (utterances) based on about 330 hours of recordings. Regarding the format, TAICORP adopted the Child Language Data Exchange System (CHILDES) [MacWhinney and Snow 1985; MacWhinney 1995] for transcribing and coding the recordings into machine-readable text. The goals of this paper are to introduce the construction of this speech-based corpus and at the same time to discuss some problems and challenges encountered. The development of an automatic word segmentation program with a spell-checker is also discussed. Finally, some findings in syllable distribution are reported. Keywords: Minnan, Taiwan Southern Min, Taiwanese, Speech Corpus, Child Language, CHILDES, Automatic Word Segmentation 1. Introduction Taiwanese Child Language Corpus is a corpus based on spontaneous conversations between young children and their adult caretakers in Minnan speaking families in Chiayi County, Taiwan. This corpus is special in several ways. First, it is a Minnan corpus. Minnan is Southern Min Chinese spoken in Taiwan (also known as Taiwanese in linguistic literature). It is less studied, especially when compared with Mandarin Chinese. Second, it is a speech-based corpus. The scripts in the corpus were transcribed from recordings of ∗ Institute of Linguistics, National Chung Cheng University, 168 University Road, Min-hsiung, Chiayi County, Taiwan 62102, ROC Phone: 886-5-2720411 ext. 31502 Fax: 886-5-2721654 E-mail: Lngtsay@ccu.edu.tw [Received November 1, 2006; Revised August 14, 2007; Accepted August 16, 2007]  412  Jane S. Tsay  spontaneous speech. Third, it is a corpus of a language that does not yet have a conventionalized orthography. Fourth, it is a child corpus. It's a collection of longitudinal child language data. Fifth, it is currently one of the largest child corpora in the world. It contains about 2 million syllables/characters in 497,426 lines (utterances) based on about 330 hours of recordings. Finally, it is a corpus that uses an international platform. This platform is the Child Language Data Exchange System (CHILDES) [MacWhinney and Snow 1985; MacWhinney 1995] for transcribing and coding the recordings into machine-readable text. The goals of this paper are: (1) to introduce the construction of this speech-based child language corpus, TAICORP (Section 2); (2) to introduce the automatization process of this corpus and discuss some issues encountered during the implementation of the system (Section 3); (3) to present some research findings based on this corpus (Section 4).  2. Taiwanese Child Language Corpus  Taiwanese Child Language Corpus (TAICORP) contains scripts transcribed from about 330 hours of spontaneous speech from fourteen young children acquiring Taiwan Minnan as their first language. A brief introduction to this corpus was reported at the 5th Workshop on Asia Language Resources [Tsay 2005a]. In this extended paper, in addition to a more detailed description and more discussion about the corpus and related issues, findings in syllable type distribution and tone type distribution are also presented.  There are about 1.6 million words (over 2 million syllables/characters) in this corpus, as  shown in Table 1.  Table 1. The size of TAICORP  Lines (utterances)  Words  Total  497,426  1,646,503  Syllables  Syllables (in words) 1,558,408  Syllables (in particles) 538,992  2,097,400  Since some words do not have corresponding Chinese characters and are presented in romanization notation (Minnan Pinyin) in this corpus, the syllable might be a more precise unit than the more traditional unit zi 字 (Chinese character). Note that we divide the syllables into two categories: syllables in words (e.g., chia 車) and syllables in particles (e.g., la 啦). Among all the 2,097,400 syllables, 538,992 syllables (about 26%) are in particles. This is a very interesting fact and will be discussed in more detail in Section 4.  Construction and Automatization of a  413  Minnan Child Speech Corpus with some Research Findings  In this section, TAICORP is introduced in the following aspects: 2.1. Motivation 2.2. Data collection 2.3. Text files in CHILDES format 2.4. Transcribing sound files into text files 2.5. Annotations 2.1 Motivation From the linguistics point of view, there is an urgent need to construct a Minnan child language corpus, partly because there has not been any such corpus available and partly because it may be getting more and more difficult to find young children learning Minnan as their first language, especially in the cities. On top of that, the significance of a large collection of longitudinal child language data for linguistic studies goes beyond saying. Mandarin and Minnan are the two major Chinese languages in Taiwan. For over forty years, Mandarin was the only official language for instruction at school in spite of the fact that about 73% of the population belonged to the Minnan ethnic group [Huang 1993]. Young children in kindergartens and elementary schools were not allowed to speak Minnan even if Minnan was the language spoken at home. This policy caused a decrease in the number of young children learning Minnan as their first language. Although the situation has changed in recent years and other local languages besides Mandarin, including Minnan, Hakka, and the aboriginal (Formosan) languages have been included in the curriculum of elementary schools, there is still a serious concern about the decrease of native Minnan speakers. This concern can be supported by a more recent survey. Tsay [2005] reports that in a survey of all 8th graders in Chiayi City in Southern Taiwan, an area where the population should be overwhelmingly Minnan, only about 26% of 14 year-olds used Minnan in their daily life, although over 80% of their grandparents and over 70% of their parents were native Minnan speakers. Under this consideration, Minnan was chosen as the target language. The project was conducted in a rural area in Chiayi County in Southern Taiwan with the hope to find young children who were raised in a Minnan-speaking environment. 2.2 Data collection Data collection took place over a period of around three years between August 1997 and July 2000 under the support of the National Science Council in Taiwan (NSC 87-2411-H-194-019, NSC 88-2411-H-194-019, NSC 88-2418-H-194-002).  414  Jane S. Tsay  Child participants  Young Children from Minnan-speaking families were recruited in Min-hsiung Village, Chiayi County, in Southern Taiwan. Nine boys and five girls from the following villages in Min-hsiung Xiang participated in this project: Fengshou（豐收村）, Sanxing（三興村）, Dongxing（東興村）, Xidibu（溪底部）, and Zhenbei（鎮北村）. They aged from one year and two months (1;2) to three years and eleven months (3;11) old at the beginning of the recording. More than half of the children were recorded over more than two years. The age range at the offset of the recordings is between 2;7 and 5;3.  Recording  Regular home visits were conducted every two weeks for younger children and every three weeks for children older than three years old. The recording setup was children at play at home interacting naturally with the adult(s), usually one of their caretakers (parents, grandparents, or, in very few cases, the nanny) and/or the investigator. The activities were children's daily life at home: playing with toys or games, reading picture books, or just talking without any specific topics. Since we hoped to have the most natural environment, Mini-disc recorders and microphones were used so that it was easier for the recorder (the investigator) to follow the child wherever she/he went. Usually, each recording session lasted from 40 to 60 minutes.  Information about the child participants and the recordings is given below. Table 2. Recording Information of TAICORP  Name  Sex  YDA  M  YCX  M  LJX  M  CQM  M  LMC  F  YJK  M  CEY  F  HBL  M  LWJ  F  WZX  M  YSW  M  TWX  F  HYS  M  LYC  F  Total  M=9  F=5  Age range 3;11.02 – 4;04.26 3;10.16 – 4;00.16 3;09.20 – 4;02.24 2;09.07 – 4;06.22 2;08.07 – 5;03.21 2;06.11 – 2;0626 2;01.27 – 3;10.00 2;01.22 – 4;00.03 2;01.08 – 3;07.03 2;01.17 – 4;03.15 1;07.17 – 2;07.14 1;05.12 – 3;06.15 1;02.28 – 3;04.12 1;02.13 – 3;03.29  Sessions 9 6 8 30 50 2 37 45 36 44 21 44 51 48  length (min.) 540 285 530 1584 2045 105 1728 1889 1777 1757 1210 1829 2280 2255  431  about 330 hours  Construction and Automatization of a  415  Minnan Child Speech Corpus with some Research Findings  Sound file editing There were a total of 431 recording sessions. Each session was saved as a separate sound file. The sound files were first edited so that the empty or noisy parts could be cleared. In order to have easier searching and locating the content of the recordings, each sound file was segmented into several tracks and the tracked marks were tagged.  2.3 Text Files in CHILDES Format The sound files were transcribed into text files in CHILDES format. CHILDES (Child Language Data Exchange System) was originally set up by Elizabeth Bates, Brian MacWhinney, and Catherine Snow to transcribe and code recordings into machine-readable speech text [MacWhinney and Snow 1985; MacWhinney 1995]. CHILDES has been widely accepted as the standard system for child language data. TAICORP adopted the format of CHILDES so that it will be easy to exchange and share data with researchers around the world. CHILDES includes a transcription system, CHAT, and a set of programs, CLAN, for various analyses. In this section, we introduce a simplified version of the format of text files in CHAT. For details, please refer to MacWhinney [1995] or the official website of CHILDES at http://childes.psy.cmu.edu/. The main components of the CHILDES format are headers and tiers. Headers There are three kinds of headers: obligatory headers, constant headers, and changeable headers. Obligatory headers: Obligatory headers are necessary for every file. They mark the beginning, the end, and the participants of the file. Constant headers: They mark the name of the file and the background information of the children. Changeable headers: They contain information that may change across files, such as the recording date, duration, coders, and so on.  These headers all begin with @. Some examples are given below:  Obligatory headers:  @Begin  to mark the beginning of a file  @End  to mark the end of a file  @Participants  to list all the participants in a file  416  Jane S. Tsay  Constant headers: @Age of XXX: @Birth of XXX: @Coder: @Educ of XXX: @Filename: @Language: @Language of XXX: @Sex of XXX: @Warning:  the age of speaker the birthday of the speaker the file coder's name the highest education of the speaker filename the main language used in the file the language used by the speaker the sex of the speaker the defects of the file  Changeable headers (optional):  @Activities:  Activities involved in the situation  @Bck:  background information of the utterance  @Comment:  the comment of the investigator  @Date:  the date of the interaction  @G:  gems  @Location:  the location of the interaction  @New Episode:  the new episode of the recording starts  @Room Layout:  room configuration and positioning of furniture  @Situation:  the situation of the interaction  @Tape Location:  the specific ID, side and footage  @Time Duration:  the length of recording time  @Time Start:  the starting time of recording  Tiers The content of a file is presented in tiers in CHILDES. There is a main tier and several dependent tiers for each line (utterance). The main tier, marked with *, is the speech of the speaker. Three capital letters indicate the status of the speaker, e.g., *CHI is the child, *MOT the mother, and *INV the investigator. Minnan Pinyin is used in the Main tier. Words are separated by a space. Therefore, an utterance "I want to water the vegetables" from a child would be:  Construction and Automatization of a  417  Minnan Child Speech Corpus with some Research Findings  *CHI:  gua2 beh4 ak4 chai3. I want water vegetable  The additional information is given in dependent tiers that are marked with % at the beginning of a new line. The seven dependent tiers used in TAICORP are given below.  %ort: %pro: %syl: %cod: %pho: %syc: %ton:  the utterance in logographic orthography (i.e., Chinese characters) the actual target pronunciation of the utterance (dialectal variation) syllable type coded with C and V (e.g. CVV for /gua/) part-of-speech coding phonetic transcription in Unicode IPA (for child speech only) syllable type of the child's pronunciation tone value in 5-digit scale  For the adult speech, there are only four dependent tiers: %ort, %pro, %syl, and %cod because no phonetic transcription was done on the adult speech. For the child speech, there are up to seven dependent tiers as shown in the following example.  (main tier) (depnt tier) (depnt tier) (depnt tier) (depnt tier) (depnt tier) (depnt tier) (depnt tier)  *CHI: %ort: %pro: %syl : %cod: %pho: %syc: %ton:  gua2 beh4^bueh4 ak4 chai3.  我 欲  沃 菜.  gua2 beh4 ak4 chai3.  CVV CVK VK CVV  Nh D  VA  guaŋ be ak t'ai  CVVN CV VK CVV  55 55 5 21  2.4 From Sound Files to Text Files All sound files were transcribed into text files. Transcriptions included (1) orthographic transcription; and (2) phonetic transcription (in IPA, International Phonetic Alphabet).  418  Jane S. Tsay  There were two kinds of systems used in orthographic transcription. One was the logographic orthography (i.e., traditional Chinese writing system Hanzi 漢字), and the other was a spelling-based romanization system for Minnan (called Minnan Pinyin). Thus, each sound file was transcribed into a separate text file in both Chinese characters and Minnan Pinyin. 2.4.1 Orthographic Transcription in Chinese Characters The reason that the sound files were first transcribed into Chinese characters was because this written form is closest to most native speakers' intuition. Therefore, by transcribing [tsetɕʰia] into "坐車", it makes it much easier for the user to read. Although romanization notation (Minnan Pinyin) in the Main tier (e.g., *CHI tier in the above example) makes it easier to run the analyzing programs in CHILDES and might also be easier for non-Chinese users of the corpus, having a tier with Chinese characters would be more convenient for those who know Chinese. Therefore, a dependent tier %ort was added to present the utterances in Chinese characters. This is a reasonable method because most Minnan words are cognates of Mandarin words. Still, there are quite a few words that either do not have their corresponding Chinese characters or their corresponding Chinese characters are so obsolete that they cannot be found in the software for typing Chinese characters. Since Minnan does not have as conventionalized orthography as Mandarin, quite a few words in Minnan do not have a consistent way of writing them. In order to help build consensus in Minnan cognates（閩南語本字）, Minnan dictionaries were consulted. At least seven dictionaries were used as listed after the references. There are several possibilities regarding Chinese characters used in Minnan: First, they are exactly the same as those used in Mandarin, for example, 色筆/sik4pit4/ "color pens". Second, they are synonyms of Mandarin words, but use different characters, for example, 挽 /ban2/ "pluck; pick up" is a synonym of Mandarin 摘 /zhai1/ or 採 /cai3/; 鼻芳 /phinn7phang1/ "smelling the fragrance" is a synonym of 聞香 /wen2xiang2/. Third, although the Chinese characters in Minnan can be found in the dictionary, they might be so obsolete that one has to use special software to make the character forms, as in the first character of the following word meaning "good morning". 敖早 /gau5ca2/ 刀  Construction and Automatization of a  419  Minnan Child Speech Corpus with some Research Findings  This is very inconvenient for users and is very hard to process, too. In such cases, Minnan Pinyin is used and the above word would be presented as gau5 早. Fourth, when Chinese characters cannot be found at all for Minnan words, Minnan Pinyin is used, as in the first morpheme is the word chua7 路 /chua7loo7/ "leading the way" or chit4tho5 /chit4tho5/ "playing around". For homonyms that share the same Chinese character, a number is added to the character to indicate different lemmas. For example:  蓋 1 /kah4/ "to cover with a blanket" 蓋 2 /kham3/ "to cover" 蓋 3 /kua3/ "a cover/lid"  2.4.2 Orthographic Transcription in Minnan Pinyin The reason for transcribing the sound files into Minnan Pinyin was twofold: (1) to encode the sounds in a spelling system, and (2) to make it easier for the machine (computer program) to read and to do analyses such as syllable frequency counts. The Minnan Pinyin system used in TAICORP is the Taiwan Southern Min Phonetic Alphabetic officially announced by the Ministry of Education in Taiwan in 1998.1 Like most romanization systems, the Minnan Pinyin system labels sounds at the phonemic level. The Minnan Pinyin notation system with examples is given in Table 3 (consonants) and Table 4 (vowels) below. Note that '-' before a symbol indicates the coda position, as in a checked (Rusheng) syllable. It is necessary to make such a distinction because of the asymmetry in the distribution of consonants. For example, [b] cannot occur in the coda position, although it can occur in the onset position. Following the IPA convention, a dot under a symbol is used to denote a syllabic consonant. Nasal vowels are denoted with "nn". Therefore, the word [tĩ] "sweet" is transcribed as /tinn/ in this system.  
This paper presents the algorithms used in a prototypical software system for automatic pronunciation assessment of Mandarin Chinese. The system uses forced alignment of HMM (Hidden Markov Models) to identify each syllable and the corresponding log probability for phoneme assessment, through a ranking-based confidence measure. The pitch vector of each syllable is then sent to a GMM (Gaussian Mixture Model) for tone recognition and assessment. We also compute the similarity of scores for intensity and rhythm between the target and test utterances. All four scores for phoneme, tone, intensity, and rhythm are parametric functions with certain free parameters. The overall scoring function was then formulated as a linear combination of these four scoring functions of phoneme, tone, intensity, and rhythm. Since there are both linear and nonlinear parameters involved in the overall scoring function, we employ the downhill Simplex search to fine-tune these parameters in order to approximate the scoring results obtained from a human expert. The experimental results demonstrate that the system can give consistent scores that are close to those of a human’s subjective evaluation. Keywords: CAPT, CALL, Speech Recognition, Tone Recognition, Speech Assessment, GMM, Mandarin Chinese, Downhill Simplex Method, Phoneme, Intensity, Rhythm, Forced Alignment. 1. Introduction With the fast-growing power of personal computers and the advances in speech and language processing technologies, software systems for CALL (Computer Assisted Language Learning) now allow a person to learn a language by interacting solely with computers, especially for second language (L2) learning. In general, a CALL system involves testing procedures for both the receptive and productive skills of a given subject. To evaluate receptive skills such as ∗ Department of Computer Science, National Tsing Hua University, Taiwan E-mail: {jtchen; jang}@cs.nthu.edu.tw + Innovative DigiTech-Enabled Applications & Services Institute, Institute for Information Industry [Received December 7, 2006; Revised October 16, 2007; Accepted October 17, 2007]  444  Jiang-Chun Chen, and Jyh-Shing Roger Jang  reading and listening, the procedure is relative simple, since the evaluation is usually based on exams containing questions of single or multiple choices. On the other hand, to evaluate the productive skills of speaking or writing, the procedure is relatively difficult and time-consuming, since a human expert is usually required to evaluate the speech or writing in a subjective and time-consuming manner. With advances in automatic speech recognition, a Computer-Assisted Pronunciation Training (CAPT) system can evaluate the pronunciation quality using various speech features, and provides high-level feedback (hints for further improvement, etc.) to the user. Successful applications of CAPT have been reported in the literature [Neumeyer et al. 2000; Kim and Sung 2002; Neri et al. 2003]. In this paper, we propose several algorithms for constructing a CAPT system that can assess a test utterance in Mandarin Chinese with respect to a target utterance by a native speaker. Basically, there are four evaluation criteria based on different acoustic features, as explained in the following. 1. Phoneme: This is based on the log probabilities of the test utterance with respect to the acoustic models derived from a large speech corpus for speaker-independent speech recognition. Note that the target utterance is not required for this evaluation. 2. Tone: Each syllable is associated with a tone in Mandarin Chinese. The pronounced tone of a syllable can be identified by a tone classifier, and the result is then compared with the correct tone for evaluation. Note that we can obtain the correct tones from the text of the utterance; hence, the target utterance is not used directly for this evaluation. 3. Intensity: Each syllable has an intensity vector, which is compared to that of the corresponding syllable in the target utterance to ensure it has a similar score. 4. Rhythm: The duration of each syllable and the silence in between are compared to those of the target utterance to ensure they have a similar score. Each of the scoring functions for the above four criteria involves several nonlinear parameters. These four scoring functions are then linearly combined to give a score between 0 and 100. We then employ a search method that can find optimum values of these parameters such that the computed scores can approximate those of a human expert. The experimental results demonstrate the feasibility of the proposed approach, which can give consistent results when compared with human evaluation. The rest of this paper is organized as follows. Section 2 gives a quick overview of related work on automatic pronunciation assessment. Section 3 explains the speech-related techniques used in our approach, including Viterbi decoding and tone recognition. The method of combining weighted scores based on derivative-free optimization is also explained. Section 4  Automatic Pronunciation Assessment for Mandarin Chinese:  445  Approaches and System Overview  describes the GUI of our system. Section 5 demonstrates the experimental results and Section 6 gives concluding remarks. 2. Related Work In general, a CAPT system can evaluate pronunciation quality using various speech features. Moreover, the system is expected to have optimum performance by minimizing the discrepancies between the scores from computers and those from a human expert. However, most of the reported systems do not take the characteristics of tonal languages into consideration. In particular, Mandarin Chinese is a tonal language, and each character is associated with one out of five possible tones. The tone of a given character is also context-dependent according to tone sandhi [Lee 1997]. Hence, the correct pronunciation of the tone of each character in a sentence is the most challenging problem for a Mandarin-learning non-native speaker. The proposed system takes this specific problem into consideration and tries to create a comprehensive Mandarin Chinese pronunciation assessment system. 3. The Proposed Approach The proposed pronunciation assessment system uses four criteria to evaluate a test utterance with respect to a target utterance. The algorithms of these four criteria are explained in this section.  3.1 Syllable/Phone Segmentation using HMM-based Forced Alignment HMM (Hidden Markov Models) has been used for speech recognition with satisfactory performance over the past few decades [Rabiner and Juang 1993; Huang et al. 2001]. Our system employs a speaker-independent HMM-based recognition engine, which was trained on a balanced corpus of Mandarin Chinese recorded by 70 subjects in Taiwan. Each speech feature vector contains 39 dimensions, including 12 MFCC (Mel-Frequency Cepstral Coefficients) and 1 log energy, along with their delta and double delta values. 174 right-context-dependent (RCD) biphone models are derived from the speech corpus. In other words, two phones are regarded as different models if their right phones are different. For example, the right phone of “a” in the syllable “dan” is “n”, while the right phone of “a” in the syllable ‘jiang’ is “ng”. As a result, the phone “a” in the syllable ‘dan’ is defined as “a+n” in the RCD context, to distinguish it from, say, “a+ng” in the syllable “jiang”. The number of RCD biphone models is much larger than that of the context-independent monophone models, thus a large corpus is required for the reliable training of RCD biphone models. Furthermore, we have also designed an efficient pruning method for our speech recognizer [Jang and Lin 2002].  446  Jiang-Chun Chen, and Jyh-Shing Roger Jang  For pronunciation assessment, we need to build a lexicon net consisting of the models of the uttered text. Then, Viterbi decoding is used to do forced alignment between the speech signals and the models in the lexicon net. The final results include frame indices of isolated syllables/phones and the corresponding log probability. The log probability is an absolute measure of how closely the utterance matches the acoustic models identified from the speech corpus. Consequently, the log probability varies considerably among different models due to their different phonetic characteristics, and thus cannot be used directly for phoneme assessment. Instead, we use a ranking-based confidence measure to be explained later. To deal with characters having multiple pronunciations in Mandarin, we use a sausage-like lexicon net. Take the sentence “朝辭白帝彩雲間” in Tang Poetry for example, where the third character can be pronounced as “bai” or “bo”. Both pronunciations are commonly used. Therefore, our lexicon net has two branches for this character to accommodate both pronunciations, as shown in Figure 1 where Hanyu Pinyin is used for phonetic transcription.  bai  zhao  ci  di  cai  yun  jian  bo Figure 1. The lexicon net for “朝辭白帝彩雲間”.  Figure 2 shows a typical result of forced alignment for the sentence “但使龍城飛將在” in Tang Poetry. The solid lines in the waveform plot indicate the boundaries of phones. The score for each syllable is labeled under each Chinese character, while the score for each phone is labeled under the phone name. The scores depend on the quality of the pronunciation as well as the correctness of forced alignment. In particular, we can see that the fricative phone “sh” in the second character is correctly segmented with a score of 100, while the phone “ch” in the forth character is badly segmented (due to its mispronunciation as a non-retroflexed consonant) with a score of 29. The details of phoneme assessment will be described in the next section. Figure 2 also shows the pitch vector of this utterance. The dotted lines represent the pitches of the voiced parts, while the union of the dotted and the solid lines represents the pitch curve of the whole utterance derived by dynamic programming. Details of the pitch tracking method can be found in Chen and Jang [2007].  Automatic Pronunciation Assessment for Mandarin Chinese:  447  Approaches and System Overview  Figure 2. An example of forced alignment for the test utterance “但使龍城飛將在”. The upper panel shows the phone segmentation results and the corresponding phoneme scores. The bottom panel plots the pitch vector where the dotted lines correspond to the voiced parts in the utterance. 3.2 Ranking-Based Confidence Measure for Phoneme Assessment The log probability represents an absolute measure of how closely a pronunciation approximates a given phone model, which does not take into consideration the effect of other competing models. As a result, the log probability varies considerably among different phone models due to their different phonetic characteristics. To deal with this problem, we used a relative measure based on the ranking among all competing biphone models. This is an improved version of our previous approach to confidence measure, based on the ranking among 411 syllables in Mandarin [Chen et al. 2004]. By using phone-based ranking, our  448  Jiang-Chun Chen, and Jyh-Shing Roger Jang  system is able to track down phone-level pronunciation errors for detailed and better assessment.  The phone-based phoneme assessment proceeds as follows.  1. For a given biphone model of “x+y”, we define the set of competing models as “*+y” where * is a wildcard representing all the possible phones that form a legal biphone with y.  2. After forced alignment, we can obtain the speech signals corresponding to the biphone “x+y”. We then send the speech signals to the competing models for a log probability evaluation and find the rank (zero-based) of “x+y” in the competing models.  3. Since each biphone has a different set of competing models, we divide the rank of “x+y” by the size of its competing models to obtain a rank ratio between 0 and 1. Once the rank ratio is obtained, the phoneme score of the i-th phone in an utterance is then determined by the following formula:  sphoneme,i  =  100 1+ rri  b  ,  ( 1 )  a  where rri is the rank ratio of the i th biphone, and a and b are the tunable parameters of this scoring function. In particular, when rri is equal to 0, a perfect score of 100 is obtained. On the other hand, if rri is larger, the score is lower. The values of parameters a and b are empirically set to 0.1 and 2 respectively. A typical plot of function sphoneme,i is shown in Figure 3. (An on-going research focus is to make the parameters a and b model-dependent and to determine their values automatically, which will be covered in our future publication.)  phoneme score  100  90  80  70  60  50  40  30  20  0  0.2  0.4  0.6  0.8  
Coreference resolution is the process of determining the entity that noun phrases refer to. A great deal of research has been done on this task in English, using approaches ranging from those based on linguistics to those based on machine learning. In Chinese, however, much less work has been done in this area. One reason for this is the lack of resources for Chinese natural language processing. This paper presents a knowledge-based, unsupervised clustering algorithm for Chinese coreference resolution that maximizes performance using freely and easily available resources. Experiments to demonstrate the efficacy of such an approach are performed on two data sets: TDT3 and ACE05, and the ACE value coreference resolution results achieved through our approach are 52.5% and 55.2% respectively. An oracle experiment using gold standard noun phrases achieved even more impressive results of 77.0% and 76.4%. To analyze the causes of errors, this paper also looks into false alarms and misses in documents. Keywords: Coreference Resolution, Modified K-means Clustering, Stacked Transformation-based Learning, Unsupervised Learning 1. Introduction Noun phrase (NP) coreference resolution is an important subtask in natural language processing (NLP) applications such as text summarization, information extraction, data mining, and question answering. The subject has attracted much attention in recent years, although much more in regards to the English language than to the Chinese language, and has been included as a subtask in the MUC (Message Understanding Conferences) and ACE (Automatic Content Extraction) programs. NP coreference resolution is the process of detecting noun phrases in a document and determining whether these noun phrases refer to the same entity. As defined in ACE [2005], an entity is “an object or set of objects in the world.” ∗ Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong Tel: +852-27667279 Fax: +852-22154652 E-mail: {csgngai; cscswang} @comp.polyu.edu.hk [Received April 11, 2007; Revised August 31, 2007; Accepted September 10, 2007]  460  Grace Ngai and Chi-Shing Wang  Phrases that refer to an entity are known as mentions, which may be either anaphors or antecedents. An anaphor is an expression that refers back to something mentioned previously in a discourse, and the something that the anaphor refers back to is its antecedent. Thus, in the passage in Figure 1, the term 克林頓總統 (President Clinton) in the second line of the passage is an anaphoric reference to its antecedent 克林頓 (Clinton), which begins the passage. This anaphor 克林頓總統 (President Clinton) is in turn the antecedent of the second 他 (he). All three of these terms, 克 林 頓 (Clinton), 克 林 頓 總 統 (President Clinton), and the second 他 (he), are mentions of the same entity and refer, of course, to former U.S. president Bill Clinton. Generally speaking, it is a simple matter for human beings to quickly and accurately identify such coreferences. However, the cues that are used by humans for noun phrase coreference resolution are not easily transferred to the computer. Even in English, the most heavily studied language, the accuracy of automated NP coreference resolution is currently unsatisfactory. In Chinese, which has its own particular characteristics and difficulties, NP coreference resolution is a topic where even more work remains to be done. [克林頓 1]說，華盛頓將逐步落實對[韓國 2]的經濟援助。[金大中 3]對[克林頓 1]的講 話報以掌聲。[他 3]說：「[克林頓總統 1]在會談中重申，[他 1]堅定地支持[韓國 2]擺脫 經濟危機。」 [Clinton1] said that Washington would progressively follow through on economic aid to [Korea2]. [Kim Dae-Jung3] applauded. Figure 1. An excerpt from the text, with coreferring noun phrases annotated. English translation in italics. Central to the development of efficient and reliable approaches to automatic NP coreference resolution is the issue of what features should be used to identify the coreference. Ng and Cardie [2002b] listed 53 features, including gender agreement, number agreement, head noun matches, semantic class agreement, positional information, contextual information, apposition, abbreviation, and others. At one extreme, efficiency alone forbids the use of all of these features; at the other, no single linguistic feature is completely reliable. With the careful selection of combinations of suitable features, there may be a tradeoff to be made between the efficiency of using fewer features and the accuracy to be obtained from using more. Before such an approach can be tested, there are a number of difficulties that need to be addressed, not the least of which being the limitations of currently available NLP applications and ontologies used in coreference resolution. For example, applications, such as named entity  A Knowledge-Based Approach for  461  Unsupervised Chinese Coreference Resolution  recognition, and ontologies, such as WordNet and HowNet, are currently used to identify features such as semantic class. However, these identifications are not always accurate, especially where new terms, domains or languages are concerned. Domain adaptation then becomes an issue, or ontology coverage becomes less than ideal. As already mentioned, Chinese NP coreference resolution involves certain difficulties which are not found in the English language. First, from the point of view of NLP, Chinese suffers from a lack of usable morphological and orthographic features. For example, in English, morphological features such as number agreement can indicate coreference, and this contributes to the accuracy of automatic part-of-speech (POS) tagging. Chinese, however, does not use morphological changes to indicate number agreement. As for orthography, Chinese does not, for example, use capitalization whereas English can make use of capitalization to mark elements such as proper names, place names, and abbreviations. Perhaps the greatest difficulty of written Chinese is that, unlike English, it does not mark word boundaries. Word segmentation is thus required, yet various segmentations of even a simple Chinese sentence may produce a variety of meanings, making a range of NLP tasks, for example, POS tagging, highly problematic. A second important problem faced in Chinese NP coreference resolution is a lack of Chinese corpora (specifically coreference data sets) that are either free of charge, freely available, or sufficiently free of error for use as benchmarking data sets for training and for measuring performance. The principal reason for this is that building a reasonably large coreference corpus is a labor-intensive task, especially with regard to annotation, which cannot be undertaken by any but the largest institutions. For example, the ACE corpus from the ACE program is large and is annotated for a very comprehensive number of grammatical, semantic, and discourse features. It is available, at a cost, for use in problems involving coreference resolution. In this paper, we propose an approach to Chinese NP coreference resolution that, with small amounts of training and time investment, can accurately identify chains of coreference in unannotated texts. The approach first uses an automatic, Penn Treebank trained parser [Zhang et al. 2003] to identify mentions and then filters out those that are not likely to refer to an entity using heuristic rules based on POS information. The resulting mentions are then linked into possible chains using a clustering algorithm and specific linguistic features. The advantages of this approach are, first, that the proposed algorithm is unsupervised and therefore requires no training set, relying instead on word lists, dictionaries, and gazetteers that are freely available and easily compliable; and second, that features may be easily added or deleted. This makes our method suitable for scenarios where such a system needs to be quickly compiled for a new genre or language, where pre-existing resources are not adequate. After describing the proposed system, we will demonstrate the efficacy of our algorithm by  462  Grace Ngai and Chi-Shing Wang  achieving satisfactory performance on two different corpora. The rest of the paper is laid out as follows: Section 2 gives an overview of the previous work in this area. Section 3 describes our algorithm, Section 4 introduces the experimental setup, and Section 5 gives details of our evaluation. Section 6 contains the analysis of our results, and is followed by our conclusions. 2. Previous Work In this section, we will start with a description of the most common approaches to coreference resolution and contrast them with the approach that we will be taking. Since we will be concentrating on the problem in Chinese, we will also include an introduction to the work conducted to date on Chinese NP coreference resolution. 2.1 Supervised Machine Learning Approaches Much of the previous work in NP coreference resolution has used statistical, machine learning approaches, and one of the most frequently used approaches is that of binary classification. These algorithms link up mentions into coreference chains by first identifying an anaphoric noun phrase, and then using a predetermined number of features in an effort to identify the best antecedent for each mention. Soon et al. [2001] proposed a 12-feature classifier based on a decision tree, which returns a number between 0 and 1 to indicate the likelihood that two noun phrases corefer. Their training data came from and was applied to the MUC corpora. Positive examples were generated from each anaphoric NPj and its immediately adjacent antecedent NPi. Negative examples were generated by taking all noun phrases between each antecedent-anaphor pair, NPi+1, NPi+2 … NPj-1, and pairing them with the anaphor, NPj. They found that the alias, appositive, and string match features contributed the most to performance. Ng and Cardie [2002b] extended this approach with three extra-linguistic changes: the clustering approach, the creation of training instances, and the definition of string match features. They also made use of additional features. Their system achieved good results on the MUC-6 and MUC-7 data sets, with F-Measure scores of 70.4 and 63.4, respectively. Ultimately, however, binary classification is flawed in that, at any given time, it takes into account only the relationships between two NPs rather than a longer chain. For example, given three NPs: NPa, NPb, and NPc, it is possible that the model might think that NPa and NPb are coreferential, and also that NPb and NPc are coreferential, yet at the same time think that NPa and NPc are not. This creates a problem when the system tries to create coreference chains where all of the phrases in the chain refer to the same entity. Second, a phrase by itself usually lacks sufficient descriptive information to allow a completely confident decision to be made. Where the reference is to a human, it can be quite difficult to decide if two pronouns are anaphor-antecedent pairs simply by looking at the pronoun alone.  A Knowledge-Based Approach for  463  Unsupervised Chinese Coreference Resolution  Several approaches have been proposed to compensate for these failings of the NP-NP approach. Yang et al. [2004b] adopted an NP-cluster framework, which considers the relationships between phrases and coreferential clusters. To describe the cluster properties, they introduced six additional features: cluster gender, number, semantic agreement, cluster length, cluster string similarity, and longest phrase similarity. Experiments have shown that this approach outperforms the NP-NP based approach. McCallum and Wellner [2004] introduced three conditional undirected graphical models of identity uncertainty based on conditional random fields. Their model avoids the problem of pair-wise coreference decisions being made independently of the relationships of each element of a pair. Rather than making a decision based on a single measurement to one other node, measurements are made to all nodes. This method improves upon the NP-NP based algorithm, but its supervised approach requires access to a large amount of data in order for meaningful statistics to be gathered.  2.2 Unsupervised Machine Learning Approaches Supervised methods to coreference resolution have been successful at achieving good performance; however, they require annotated corpora as training data. This is not a problem with well-studied languages such as English, where language resources such as corpora and linguistics tools are plentiful, but it does create problems for other languages or even for less well-studied genres and domains. Cardie and Wagstaff [1999] proposed an unsupervised approach that casts the problem of coreference resolution as a clustering task that applies a set of incompatibility functions and weights in the distance metric. Their algorithm starts by forming each entity into a singleton cluster, and then iteratively compares pairs of clusters. If the distance between two phrases in two clusters that are being compared is less than some threshold, the clusters are merged, provided that all their phrases are compatible. This mechanism can easily incorporate new constraints and preferences, but the merging algorithm is greedy in that it will take the first match rather than the best match.  2.3 Knowledge-Based Approaches In addition to machine learning, knowledge-based approaches have also been widely used to provide rules for filtering features for NP resolution. Zhou and Su [2004] presented a constraint-based multi-agent strategy. This strategy first uses general heuristics such as morphological and semantic consistency to filter out invalid antecedent candidates, and then an antecedent for the anaphor is chosen based on the principle of proximity. This strategy offers two different types of agents: a set for filtering out less informative antecedent candidates and another set for matching coreference types. This strategy has been shown to be efficient and accurate. In addition, Bean and Riloff [2004] pioneered an approach to identify  464  Grace Ngai and Chi-Shing Wang  NP coreferences by using information extraction patterns to identify contextual role knowledge. This approach first identifies definite, non-anaphoric noun phrases, and then uses case resolution to identify the most easily resolved phrases. Remaining non-resolved phrases are then evaluated against eleven sources of knowledge that include four contextual caseframes, that is, normalized extraction patterns. The final resolution is made using a Dempster-Shafer probabilistic model [Bean and Riloff 2004]. Knowledge-based approaches have the advantage in that, usually, little or no annotated corpora are required. However, they do rely heavily on hand-crafted heuristics or rules, which also require large investments of time and effort to create. 2.4 Feature Selection The most desirable features for use in coreference resolution are robust and inexpensive, perform well over various domains, and can be obtained automatically. Features may be lexical, grammatical, semantic, syntactic, contextual, or heuristic. Given the broad range of features that may be chosen, there is currently no definitive classification of their relative merits or effects on system performance. 2.5 Coreference in Chinese Texts To our knowledge, the previous work that has been conducted on the subject describes only two approaches to Chinese noun phrase coreference resolution, with both of them being supervised methods. Florian et al. [2004] used a language-independent framework to process Chinese data on the Entity Detection and Tracking (EDT) task, which is very similar to coreference resolution. EDT contains two subtasks, detection and tracking. The Entity Detection subtask finds all possibly coreferring phrases. The Entity Tracking subtask combines the detected phrases into groups referring to the same object. The authors formulate the detection subtask as a classification problem using a Robust Risk Minimization classifier combined with a Maximum Entropy classifier. Much like base noun phrase chunking, it labels each word token, indicating whether it starts a phrase, is inside a phrase, or is not within any phrase. They tackle the mention tracking subtask with a novel statistical approach that processes each phrase in turn, starting with the leftmost phrase in a document. For the current phrase, they make a decision to either link it with one of the existing clusters, or to make it start a new cluster. The authors reported achieving good results with English, Chinese, and Arabic. They obtained 58.8 on the ACE03 evaluation data on Chinese, but they noted that their algorithm was trained on only 90k characters for Chinese, in contrast to 340k words in English, which they believe to be insufficient for purposes of generalization. Zhou et al. [2005] proposed a unified transformation-based learning (TBL) framework and tested it on Chinese EDT. They considered five types of entities: person, geographic or  A Knowledge-Based Approach for  465  Unsupervised Chinese Coreference Resolution  political entity, organization, location, and facility. They use the MSRSeg word segmentation algorithm and integrate it with an adapter to chunk Chinese characters into words. The mention detection model then tags each segmented word with a semantic type. The TBL tracking model then looks at every pair of words and classifies them as being coreferent or not, based on the values of six features (string match, edit distance, token distance, mention type, entity type, and lexical string). They report a performance of 63.3 on the ACE03 data set. One of the biggest obstacles in Chinese noun phrase coreference resolution is that the amount of available data and resources lags far behind what is available in English. As a comparison, the ACE03 training corpus for Chinese was 90k characters, compared with over 340k words for English. In addition, there are many gazetteers and lexicons available in English but not many for other languages. These factors combine to make it difficult to get good performance in supervised efforts at noun phrase coreference in languages other than English.  2.6 Evaluation Metrics  Noun phrase coreference resolution is unlike other NLP tasks in that it does not decompose readily into either a task of bracketing or classification. As a result, it is not easy to extend current evaluation metrics to noun phrase coreference resolution. In this section, we will look at two of the most common evaluation metrics and explain how they work.  Traditionally, performance of noun phrase coreference resolution has been measured  using precision and recall, as measured by Vilain et al.’s scoring algorithm [Vilain et al. 1995].  The algorithm defines recall as follows:  R = ∑ (| Ci | − | p(Ci ) |) .  (1)  ∑ (| Ci | −1)  Each Ci is a gold standard entity (i.e., a set of mentions that we know refer to the same entity), and p(Ci) is the partitioning of Ci by the automatically identified entities. For example, suppose that the gold standard annotation identifies two entities, C1 and C2, where C1 contains the mentions {1,2,3,4,5} and C2 contains the mentions {6,7,8,9,A,B,C}. Now, assume that the automatically identified entities are partitioned as {1,2,3,4,5} {6,7} {7,8,A,B,C}. |C1| would therefore be 5, and p(C1) would be 1. Likewise, |C2| would be 7 and p(C2) would be 2. The recall for this scenario would then be calculated to be 90%. For precision, the roles of the automatically identified and gold standard entities are reversed.  Vilain et al’s evaluation metric was used for the MUC program, but as Baldwin et al. [1998] pointed out, it does have the weakness of yielding unintuitive results for some scenarios. For example, the baseline method of assuming that all identified mentions refer to the same entity actually yields a fairly good result by Vilain’s metric. There are several  466  Grace Ngai and Chi-Shing Wang  reasons for this counterintuitive result: first, the metric does not distinguish between different kinds of errors; second, it inherently favors outputs with fewer entities; and third, it ignores single-mention entities. The ACE program introduced a different evaluation metric, the ACE value [ACE 2005], which has often been referred to as a cost-based metric. The idea is to evaluate system output by application value. A system with a completely correct output would get an ACE value of 100%, while a system producing no output would get an ACE value of 0%. Negative ACE values can also be given to systems with outputs that are drastically incorrect. The overall value is calculated by looking at each of the system-generated entities and calculating its value based on a product of two factors: Valuesys _ entity = Entity _ Value(sys _ entity) ⋅ Mentions _Value({sys _ mentions}) Entity_Value is a function calculated over each gold standard entity. It takes into account how well the gold standard and system outputs match each other on the entity level (e.g. whether the mentions in the entity were detected and resolved correctly by the system). Mentions_Value is a function measuring how well the mentions detected by the system match those of the gold standard (e.g. they may match, the system may identify extra mentions, or may miss some altogether). Errors that are penalized are misses (mentions that are in the gold standard but not in the system output), false alarms (mentions that appear in the system output but not in the gold standard), and mistakes (inexact overlaps between system output and gold standard). The heaviest penalties come from misses and false alarms, with misses penalized at a heavier rate than false alarms. Even though the ACE value was developed partly to correct some of the drawbacks of the MUC metric, it does have a number of problems of its own. One of the biggest complaints is that ACE values are difficult to interpret. For example, if a system achieves an ACE score of 90%, this does not mean that the system correctly identified 90% of the entities and mentions in the corpus, but rather, that the cost of the system is 10% of one that does not give any output [Luo 2005]. Other criticisms are that it tends to be inconsistent in how it penalizes the systems for various mistakes [Zelenko 2005]. Despite all of the problems associated with its use, the ACE score remains the most widely used and accepted metric for evaluating noun phrase coreference system performance. Therefore, we will use this metric for our own evaluations. 3. Our Algorithm Coreference resolution, although often referred to as a single task, can actually be divided into two subtasks. The first is entity or mention detection, which identifies anaphors and antecedents in a document, followed by noun phrase coreference resolution, or mention  A Knowledge-Based Approach for  467  Unsupervised Chinese Coreference Resolution  tracking, whereupon we decide upon the entities referred to by the identified phrases. Since trying to tackle both subtasks at once would necessitate the drawing up of an extremely complex model, almost all approaches in previous work have handled the two phases separately. Our algorithm will follow its predecessors and do the same.  3.1 Mention Detection To start off the mention detection phase, we had our corpus parsed by a probabilistic Chinese parser [Zhang et al. 2003], which was trained on the Chinese Penn Treebank. As a precursor to doing a full parsing, the parser also performs word segmentation and POS tagging. The parser generates a full parse tree as its output. Since mentions usually correspond to noun phrases, we could simply have extracted all noun phrase chunks identified by the parser; however the boundaries of the parsed noun phrases do not usually correspond exactly with mention boundaries. In addition, since we followed the ACE conventions of only considering mentions that correspond to certain semantic types [ACE 2005], it is not too likely that all of the noun phrases are going to correspond to useful mentions. For example, the word 世界 (world), although a noun phrase, is not tagged as a mention when it is not being used in the sense of a geographical location. We, therefore, used a filtering approach to identify and remove these spurious noun phrases. Filtering approaches have been successfully used by Bean and Riloff [1999], who used an unsupervised filter to construct a list of non-anaphoric phrases and NP patterns from an unannotated training corpus to identify mentions in definite noun phases. For their part, Ng and Cardie [2002a] employed a decision tree to filter out non-anaphoric phrases. Their approach achieved a large improvement in precision, but at a significant cost to recall. The objective of filtering identified noun phrases is to identify only the noun phrases that are likely to correspond to mentions, while discarding the rest. Since the following phase, mention resolution, will work on top of these identified mentions, it is reasonable to aim for as accurate a performance on this phase as possible. The problem, however, is that precision and recall are usually inversely proportional to each other: having good precision usually means bad recall and vice-versa, and a balanced precision/recall performance usually means mediocre figures for both. Our principle was this: the mention resolution phase will not identify additional mentions, and the ACE metric penalizes misses more heavily than false alarms. Therefore, we would go for high recall during the detection phase to minimize misses in the system output. To achieve this, we used a few simple heuristics to filter out noun phrases that are extremely unlikely to correspond to mentions. These heuristics are mostly based on the POS tags of the words, were previously developed for unrelated work in English named-entity resolution, and were not written with foreknowledge of the gold standard entities. A list of the heuristics can be found  468  Grace Ngai and Chi-Shing Wang  in Appendix 1. In addition, in order to filter out spurious phrases, a stoplist was used to discard frequently occurring noun phrases such as 前提 (the aforementioned), 什麼 (what), 特色 (feature), and 同時 (at the same time). In addition, we also used a large gazetteer compiled from web sources to correct segmentation errors in proper names: e.g. to correct nr(埃斯特) v(拉) v(達) to (埃斯特拉達, T. Estrada, former Cuban president).  3.2 Mention Resolution  Once mention detection has been completed, the next step in the pipeline is that of mention tracking or resolution. In this step, the task of the system is to determine which noun phrases refer to the same entity, or are coreferent.  As defined by Trouilleux et al. [2000], “referential chains” are sets of expressions, or mentions, that denote the same referent. That is, given a text T, for each referential chain RC there exists a unique discourse referent DR, such that:  RC = {x | x is an expression denoting DR in T}.  (2)  While most referential chains contain multiple elements, a referential chain may also consist of a single expression. For example, in the sentence “彼得愛加菲貓” (Peter likes Garfield), the set {彼得 (Peter)} is a referential chain. The task of coreference resolution consists of identifying these sets, which are also called “coreference chains.” Our algorithm relies on an unsupervised clustering approach for this task, which is a natural choice as it partitions the data into groups. For mention tracking, we expect the clustering algorithm to gather coreferent phrases into the same cluster, where each cluster will hopefully correspond to one coreference chain.  3.3 Modified K-Means Clustering Most of the previous work in clustering-based noun phrase coreference resolution has centered around the use of bottom-up clustering methods [Cardie and Wagstaff 1999; Angheluta et al. 2004], where each noun phrase is initially assigned to a singleton cluster by itself, and clusters that are “close enough” to each other are merged. In our system, we use a method called modified k-means clustering [Wilpon and Rabiner 1985], which takes the opposite approach and uses a top-down approach to split clusters, interleaved with a k-means iterative phase. Modified k-means clustering has been successfully applied to speech recognition. Compared with k-means clustering, modified k-means has the advantages of neither requiring a pre-set number of clusters nor being dependent upon an arbitrary starting state [Fung et al. 2003].  A Knowledge-Based Approach for  469  Unsupervised Chinese Coreference Resolution  Modified k-means starts off with all of the instances in one big cluster. The system then iteratively performs the following steps: 1. For each cluster, find its centroid, defined as the instance that is the closest to all other instances in the same cluster. 2. For each instance: a. Calculate its distance to all of the centroids. b. Find the centroid with the minimum distance, and join its cluster. 3. Iterate 1-2 until instances stop moving between clusters. 4. Find the cluster with the largest intra-cluster distance, defined as the mean of the distances of all the instances in the cluster to the centroid instance. (Let this cluster be called Clustermax and its centroid, Centroidmax.) a. If the intra-cluster distance of Clustermax is smaller than some pre-set threshold r, stop. 5. Calculate the distances between all pairs of instances inside Clustermax and find the pair of instances that are the furthest apart. a. Add the pair of instances to the list of centroids and remove Centroidmax from the list. 6. Repeat from Step 2. The algorithm thus alternates traditional k-means clustering with a step that adds new clusters to the pool of existing ones. Used for coreference resolution, it splits up the instances into clusters in which the instances are more similar to each other than to instances in other clusters. The next step is to determine a suitable threshold and a distance function with suitable parameters. As functions that check for compatibility return negative values while positive distances indicate incompatibility, a threshold of 0 would separate compatible and incompatible elements. However, since the feature extraction will not be totally accurate, we chose to be more lenient with deciding whether two phrases should be clustered together (i.e., to go for recall over precision) and used a threshold of r = 1 to allow for possible errors.  3.4 Feature Selection One of the advantages of using a clustering algorithm is that most clustering methods can easily incorporate both context-dependent and independent constraints into their features. This is attractive for us since we use a variety of features, which are designed both to capture the content of the phrase and its role within the sentence and document. Most of our features give us information on a single phrase:  470  Grace Ngai and Chi-Shing Wang  • String Content – The string of words in the phrase. • Head Noun – The head noun in a phrase is the noun that is not a modifier for another noun. • Sentence Position – The position of the sentence that contains the phrase, relative to the document. The first sentence is in position 1, the second in position 2, and so on. • Gender – For each phrase, we use a gazetteer to assign it a gender. The possible values are male (e.g., 先生, mister), female (e.g., 小姐, miss), either (e.g., 團長, leader), and neither (e.g., 工廠, factory). • Number – A phrase can be either singular (e.g., 一隻貓, one cat), plural (e.g., 兩隻 狗, two dogs), either (e.g., 產品, product) or neither (e.g., 安全, safety). • Semantic Class – To give the system more information on each phrase, we compiled our own gazetteer from web sources. Our gazetteer consists of 12,000 entries, each of which is labeled with the following semantic classes: person, organization, location, facility, GPE, date, money, vehicle, and weapon. Phrases in the corpus that are found in the gazetteer are given the same semantic class label; phrases not in the gazetteer are marked as unknown. • HowNet Definition – The semantic class gazetteer covers about 80% of the phrases that are extracted. To increase the coverage of the phrases, we turned to HowNet [Dong and Dong 2000], an ontological knowledge base that encodes inter-conceptual relations and inter-attribute relations for the Chinese language. HowNet contains 120,496 entries for about 65,000 Chinese words defined with a set of 1503 sememes, which are considered atomic semantic units that cannot be reduced further. Examples of such sememes are “human,” or “aValue” (attribute-value). Higher-level concepts, or definitions, are composed of subsets of these sememes, sometimes with pointers that denote certain kinds of relationships, such as “agent” or “target.” For example, the word “疤” is associated with the definition “trace|疤, #disease|疾病, #wounded|受傷.” As an additional feature, we labeled phrases that appeared as HowNet concepts with their sememe definitions. Phrases that do not exist in HowNet are marked as unknown. Overall, we found that about 66% of the extracted mentions in our corpus were covered under HowNet. • Proper Noun – The part-of-speech tags “nr” (person name), “ns” (country name), “nt” (organization name), “nz” (other proper name), and a list of common proper names compiled from the Internet were used to label each noun phrase, indicating whether or not it is a proper noun. • Pronoun – The part-of-speech tag “r” (pronoun) is used to determine whether the phrase is indeed a pronoun.  A Knowledge-Based Approach for  471  Unsupervised Chinese Coreference Resolution  • Demonstrative Noun Phrase – A demonstrative noun phrase is a phrase that consists of a noun phrases preceded by one of the characters [此這那該] (this/that/some). The following features give us information on how two phrases relate to each other: • Appositive – Two noun phrases are in apposition when the first phrase is headed by a common noun, while the second one is a proper name and there no space or punctuation between the two phrases; e.g., [美國總統][克林頓]上星期到朝鮮訪問 ([US president] [Clinton] visited Pyongyang last week). This differs from English, where two nouns are considered to be in apposition when one of them is an anaphor and separated by a comma from the other phrase, which is the most immediate proper name; e.g., “Bill Gates, the chairman of Microsoft Corp”. • Abbreviative – A noun phrase is an abbreviation when it is formed using part of another noun phrase; e.g., 朝鮮中央通訊社 (Pyongyang Central Communications Office) is commonly abbreviated as 朝中社. Since name abbreviations in Chinese are often given in an ad-hoc manner, it would be infeasible to generate a list of names and abbreviations in advance. We, therefore, use the following heuristic: given two phrases, we test if one is an abbreviation of another by extracting each successive character from the shorter phrase and testing to see if it is included in the corresponding word from the longer phrase. Intuitively, we know that this is a common way of abbreviating terms; empirically, we found it to be a highly precise test: a positive result was very rarely wrong. • Edit Distance – Abbreviations and nicknames are very commonly used in Chinese and even though the previous feature will work on most of them, there are some common exceptions. For example, some name-abbreviation pairs that would not get picked up are 北大西洋公約組織 (North Atlantic Treaty Organization) and 北約, or 奧運會 (Olympics) and 奧運. To make sure that those are caught as well, we introduced a Chinese-specific feature as a further test. Since abbreviations and nicknames are not usually substrings of the original strings but will still share some common characters, we measure the Levenshtein distance, defined as the number of character insertions, deletions, and substitutions, between every potential antecedent-anaphor pair. To calculate the distance between two noun phrases, a set of functions is defined over the features. For features that give information on a single mention, functions compare the value of the same feature over a pair of phrases. For features defined relative to two mentions such as edit distance and appositive, the function simply returns the value of the feature itself. The idea behind the functions is this: some features are indicators of whether two phrases are compatible with each other, with respect to coreferentiality. These features are string  472  Grace Ngai and Chi-Shing Wang  content, head noun, demonstrative, appositive, abbreviation, and edit distance. If two phrases match on this particular feature (for example, if the head noun feature for NPi and NPj are identical), then this is a strong indicator that these two phrases are coreferential. However, if they do not match, this does not necessarily mean that the two phrases are non-coreferential. Hence, these functions return negative values (decreasing the distance) when the two phrases match, but 0 (neutral) when they do not.  Table 1. Features and Functions Used for Clustering.  Feature f String Match Head Noun Match Sentence Distance  Function (Incompatibilityf(NPi, NPj)) -1 if the string of NPi matches the string of NPj; else 0 -1 if the head noun of NPi matches the head noun of NPj; else 0 0 if NPi and NPj are in the same sentence; For non-pronouns: 1/10 if they are one sentence apart; and so on with a maximum value of 1; For pronouns: if more than two sentences apart, then 1  Gender Agreement  
In this paper, we present a non-parametric speaker identification method using Earth Mover’s Distance (EMD) designed for text-indepedent speaker identification and its evaluation results for CCC Speaker Recognition Evaluation 2006, organized by the Chinese Corpus Consortium (CCC) for the th International Symposium on Chinese Spoken Language Processing (ISCSLP 2006). EMD based speaker identification (EMD-IR) was originally designed to be applied to a distributed speaker identification system, in which the feature vectors are compressed by vector quantization at a terminal and sent to a server that executes a pattern matching process. In this structure, we had to train speaker models using quantized data, then we utilized a non-parametric speaker model and EMD. From the experimental results on a Japanese speech corpus, EMD-IR showed higher robustness to the quantized data than the conventional GMM technique. Moreover, it achieved higher accuracy than GMM even if the data was not quantized. Hence, we have taken the challenge of CCC Speaker Recognition Evaluation 2006 using EMD-IR. Since the identification tasks defined in the evaluation were on an open-set basis, we introduce a new speaker verification module. Evaluation results show that EMD-IR achieves 99.3 % Identification Correctness Rate in a closed-channel speaker identification task. Keywords: Speaker Identification, Earth Mover’s Distance, Non-Parametric, Vector Quantization, Chinese Speech Corpus  ∗ Institute of Technology and Science, The University of Tokushima, 2-1 Minami-Josanjima, Tokushima-shi 770-8506, Japan Tel: +81 886569689 Fax: +81 886560575 E-mail: kuroiwa@is.tokushima-u.ac.jp + School of Information Engineering, Beijing University of Posts and Telecommunications Beijing 100876, China [Received April 3, 2007; Revised August 17, 2007; Accepted August 20, 2007]  240  Shingo Kuroiwa et al.  1. Introduction In recent years, the use of portable terminals, such as mobile phones and PDAs (Personal Digital Assistants), has become increasingly popular. Additionally, it is expected that almost all appliances will connect to the Internet in the future. As a result, it will become increasingly popular to control these appliances using mobile and hand-held devices. We believe that a speaker recognition system will be used as a convenient personal identification system in this case. In order to meet this demand, we have proposed some speaker recognition techniques [Fattah 2006A; Kuroiwa 2006; Fattah 2006B] that have focused on Distributed Speech/Speaker Recognition (DSR) systems [Pearce 2000; Broun 2001; Grassi 2002; Sit 2004; Fukuda 2004; ETSI 2000; ITU 2004]. DSR separates the structural and computational components of recognition into two components - the front-end processing on the terminal and the matching block of the speech/speaker recognition on the server. One advantage of DSR is that it can avoid the negative effects of a speech codec, because the terminal sends the server quantized feature parameters instead of a compressed speech signal. Therefore, DSR can lead to an improvement in recognition performance. DSR is widely deployed in Japanese cellular telephone networks for speech recognition services [KDDI 2006]. On the other hand, in speaker recognition, since a speaker model has to be trained with a small amount of voice registration samples, quantization poses a big problem, especially in the case of using a continuous probability density function, e.g. GMM [Sit 2004; Fukuda 2004]. To solve this problem, we proposed a non-parametric speaker recognition method that does not require previous assumption of any probability distribution function and estimation of statistical parameters such as mean and variance for the speaker model [Kuroiwa 2006]. We represented a speaker model using a histogram of speaker-dependent VQ codebooks (VQ histogram). To calculate the distance between the speaker model and the feature vectors for recognition, we applied the Earth Mover’s Distance (EMD) algorithm. The EMD algorithm has been applied to calculate the distance between two images represented by histograms1 of multidimensional features [Rubner 1997]. In Kuroiwa [2006], we conducted text-independent speaker identification experiments using the Japanese de facto standard speaker recognition corpus and obtained better performance than GMM for quantized data. After that, we extended the algorithm to calculate the distance between a VQ histogram and a data set. From the results, we observed it achieved higher accuracy than the GMM and VQ distortion methods even if the data was not quantized. We believe that the better results were obtained by the proposed method because it considers not only the centroid location, but also the weight. 1In Rubner [1997], EMD is defined as the distance between two signatures. The signatures are histograms that have different bins, to that effect we use the term “histogram” in this paper.  Speaker Identification Method Using Earth Mover’s Distance for  241  CCC Speaker Recognition Evaluation 2006  EMD can compare the distribution of the speaker model with the distribution of the testing feature vectors as is. To evaluate the proposed method using a larger database, we have taken the challenge of CCC Speaker Recognition Evaluation 2006 [Zheng 2006] organized by the Chinese Corpus Consortium (CCC) for the 5th International Symposium on Chinese Spoken Language Processing (ISCSLP 2006). In view of the characteristics of the proposed method, we have chosen the text-independent speaker recognition task from the five tasks in CCC Speaker Recognition Evaluation 2006. The method was originally designed for the classic speaker identification problem that does not require a function to reject out-of-set speaker voices. However, since the evaluation data includes out-of-set speaker voices, we introduce a new speaker verification module in this paper. We also introduce a voice activity detector that classifies each frame as either a valid speech frame or a nonvalid frame (background noise or unreliable speech) on a frame-by-frame basis, in order to avoid miss-identification caused by non-speech frame information. This paper will continue as follows. Section 2 explains the Earth Mover’s Distance and the originally proposed speaker identification method. Some modifications for CCC Speaker Recognition Evaluation 2006 and its evaluation results for the Japanese de facto standard speaker recognition corpus are also described. Section 3 presents speaker identification experiments using CCC Speaker Recognition Evaluation corpus. Finally, we summarize this paper in Section 4. 2. Non-Parametric Speaker Recognition Method Using EMD In this section, we first provide a brief overview of Earth Mover’s Distance. Next, we describe the distributed speaker recognition method using a non-parametric speaker model and EMD measurement. Finally, we propose EMD speaker identification for non-quantized data and a speaker verification module for identifying out-of-set speaker voices.  2.1 Earth Mover’s Distance EMD was proposed by Rubner [1997] as an efficient image retrieval method. In this section, we describe the EMD algorithm. EMD is defined as the minimum amount of work needed to transport goods from several suppliers to several consumers. The EMD computation has been formalized by the following linear programming problem: Let P = {( p1, wp1),…, ( pm , wpm )} be the discrete distribution, such as a histogram, where pi is the centroid of each cluster and wpi is the corresponding weight ( = frequency) of the cluster; let Q = {(q1, wq1),…, (qn , wqn )} be the histogram of test feature vectors; and D = [ dij ] be the ground distance matrix where dij is the ground  242  Shingo Kuroiwa et al.  distance between centroids pi and q j .  We want to find a flow F = [ fij ], where fij is the flow between pi and q j (i.e.  the number of goods sent from pi to q j ), that minimizes the overall cost:  mn  WORK (P,Q, F ) = ∑ ∑ dij fij ,  (1)  i=1 j=1  subject to the following constraints  fij ≥ 0 (1 ≤ i ≤ m,1 ≤ j ≤ n) ,  (2)  n  ∑ j =1  fij  ≤  w pi  (1 ≤ i ≤ m) ,  (3)  m  ∑ i=1  fij  ≤  wq j  (1 ≤ j ≤ n) ,  (4)  mn  ⎛m  n⎞  ∑ i=1  ∑ j =1  fij  =  min  ⎜⎜⎝  ∑ i=1  w  pi  ,  ∑ j =1  wq  j  ⎟⎟⎠  .  (5)  Constraint (2) allows moving goods from P to Q and not vice-versa. Constraint (3) limits  the amount of goods that can be sent by the cluster in P to their weights. Constraint (4)  limits the amount of goods that can be received by the clusters in Q to their weights.  Constraint (5) forces movement of the maximum amount of goods possible. They call this  amount the total flow. Once the transportation problem is solved, and we have found the  optimal flow F , the EMD is defined as the work normalized by the total flow:  EMD(P, Q) = ∑ im=1∑nj=1dij fij  (6)  ∑  m i=1  ∑  n j =1  fij  The normalization factor is the total weight of a smaller distribution, due to of constraint (5). This factor is needed when the two distributions of suppliers have different total weight, in order to avoid favoring a smaller distribution. In order to find the optimal flow, we used “EMD.c”, which has been made by available by Rubner [1999], in the following experiments. This program uses the transportation-simplex method and its computational complexity increases exponentially with the number of histogram bins [Rubner 1997].  2.2 Recognition Flow of the Proposed Method In the previous section, we described the concept that EMD is calculated as the least amount of work which fills the requests of consumers with the goods of suppliers. If we define the speaker model as the suppliers and the testing feature vectors as the consumers, the EMD can be applied to speaker recognition. Hence, we propose a distributed speaker recognition method using a non-parametric speaker model and EMD measurement.  Speaker Identification Method Using Earth Mover’s Distance for  243  CCC Speaker Recognition Evaluation 2006  The proposed method represents the speaker model and testing feature vectors as histograms. The details of the proposed method are described as follows.  Figure 1. A block diagram of the feature extraction process and the proposed speaker recognition method [Kuroiwa 2006] Figure 1 illustrates the outline of the feature extraction process using the ETSI DSR standard [ETSI 2000] and the proposed method. In the figure, dotted ( ) elements indicate data quantized once and double dotted ( ) elements indicate data quantized twice. As shown in the upper part of the figure, both registered utterances and testing utterances are converted to quantized feature vector sequences, VA,VB ,… , andVX , using the ETSI DSR front-end and back-end ( N A , NB , and N X are the number of frames in each sequence). In this block, ct is a feature vector of time frame t that consists of MFCC and logarithmic energy; xt is a code vector that is sent to the back-end (server); ct is a decompressed feature vector; and vt is a feature vector for use in the subsequent speaker recognition process. Using VA,VB ,… , and VX , the proposed method is executed as follows.  244  Shingo Kuroiwa et al.  (a) Speaker Model Generation  Using the registered feature vectors, the system generates each speaker’s VQ codebook, { p1sp ,…, pmsp} , using the LBG algorithm with Euclidean distance where sp is the speaker name and m is the codebook size. In order to make a histogram of VQ centroids, the number of registered vectors whose nearest centroid is pisp is counted and the frequency is set to wsppi .2 As a result, we get a histogram of the speaker, sp , that is used as the speaker model in  the proposed method:  P sp = {( p1sp , wspp1 )…, ( pmsp , wsppm )} .  (7)  This histogram is used as the suppliers’ discrete distribution, P , described in the previous section.  (b) Testing data  A histogram of the testing data is directly calculated fromV X , which was quantized by the  ETSI DSR standard. The quantized feature vectors consist of static cepstrum vectors that have  646 possible combinations and their delta cepstrum vectors, creating a set of vectors,  {q1X ,…, qmXx } , where mx is the number of individual vectors. from the set of vectors, the occurrence frequency of the vector  In order to create a histogram qiX is set to wqXi . As a result,  we get a histogram of the testing data:  Q X = {(q1X , wqX1 )…, (qmXx , wqXmx )} .  (8)  This histogram is used as the consumers’ discrete distribution, Q , described in the previous section.  (c) Identification  Using the speaker models, P sp , and the testing data, Q X , speaker recognition is executed as in the following equation: 
1. Introduction In essence, the speaker verification task is a hypothesis testing problem. Given an input utterance U, the goal is to determine whether U was spoken by the hypothesized speaker or not. The log-likelihood ratio (LLR)-based detector [Reynolds 1995] is one of the state-of-the-art approaches for speaker verification. Consider the following hypotheses: H0: U is from the hypothesized speaker, H1: U is not from the hypothesized speaker. The LLR test is expressed as: ∗ Institute of Information Science, Academia Sinica, Taipei, Taiwan + Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan E-mail: {yschao,whm}@iis.sinica.edu.tw; rc@cc.nctu.edu.tw [Received April 3, 2007; Revised August 9, 2007; Accepted August 12, 2007]  256  Yi-Hsiang Chao et al.  L(U ) = log p(U | H0 ) p(U | H1)  ⎧ ≥θ ⎨⎩< θ  accept H0 accept H1 ( i.e., reject H0 ) ,  (1)  where p(U | Hi ), i = 0, 1, is the likelihood of hypothesis Hi given the utterance U, and θ is the threshold. H0 and H1 are, respectively, called the null hypothesis and the alternative hypothesis. Mathematically, H0 and H1 can be represented by parametric models denoted as λ and λ , respectively; λ is often called an anti-model. Though H0 can be modeled straightforwardly using speech utterances from the hypothesized speaker, H1 does not involve any specific speaker, thus lacks explicit data for modeling. Many approaches have been  proposed to characterize H1, and various LLR measures have been developed. We can formulate these measures in the following general form [Reynolds 2000]:  L(U ) = log p(U | λ) = log  p(U | λ)  ,  (2)  p(U | λ )  Ψ( p(U | λ1), p(U | λ2 ),..., p(U | λN ))  where Ψ(⋅) is some function of the likelihood values from a set of so-called background models {λ1,λ2,...,λN}. For example, the background model set can be obtained from N representative speakers, called a cohort [Rosenberg 1992], which simulates potential impostors. If Ψ(⋅) is an average function [Reynolds 1995], the LLR can be written as:  L1(U )  =  log  p(U  |  λ)  −  log  ⎧ ⎨ ⎩  
This paper describes a speaker identification system that uses complementary acoustic features derived from the vocal source excitation and the vocal tract system. Conventional speaker recognition systems typically adopt the cepstral coefficients, e.g., Mel-frequency cepstral coefficients (MFCC) and linear predictive cepstral coefficients (LPCC), as the representative features. The cepstral features aim at characterizing the formant structure of the vocal tract system. This study proposes a new feature set, named the wavelet octave coefficients of residues (WOCOR), to characterize the vocal source excitation signal. WOCOR is derived by wavelet transformation of the linear predictive (LP) residual signal and is capable of capturing the spectro-temporal properties of vocal source excitation. WOCOR and MFCC contain complementary information for speaker recognition since they characterize two physiologically distinct components of speech production. The complementary contributions of MFCC and WOCOR in speaker identification are investigated. A confidence measure based score-level fusion technique is proposed to take full advantage of these two complementary features for speaker identification. Experiments show that an identification system using both MFCC and WOCOR significantly outperforms one using MFCC only. In comparison with the identification error rate of 6.8% obtained with MFCC-based system, an error rate of 4.1% is obtained with the proposed confidence measure based integrating system. Keywords: Speaker Identification, Vocal Source Feature, Vocal Tract Feature, Information Fusion, Confidence Measure 1. Introduction Speaker recognition is the process of determining a person's identity based on the intrinsic characteristics of his/her voice. In the source-filter model of human speech production, the ∗ Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong. E-mail: nhzheng@ee.cuhk.edu.hk [Received May 10, 2007; Revised August 9, 2007; Accepted August 12, 2007]  274  Nengheng Zheng et al.  speech signal is modeled as the convolutional output of a vocal source excitation signal and the impulse response of a vocal tract filter system [Rabiner and Schafer 1978]. The most representative vocal tract related acoustic features are the cepstral coefficients, e.g., Mel-frequency cepstral coefficients (MFCC) [Davis and Mermelstein 1980] and linear predictive cepstral coefficients (LPCC) [Furui 1981], which aim at modeling the spectral envelope, or the formant structure of the vocal tract. With the primary goal being identifying different speech sounds, these features are believed to provide pertinent cues for phonetic classification and have been successfully applied to automatic speech recognition [Rabiner and Juang 1993]. At the same time, these features are also implemented in most existing speaker recognition systems [Campbell 1997; Reynolds 2002]. This indicates that MFCC and LPCC features do contain important speaker-specific information, in addition to the intended phonetic information. Ideally, if a large amount of phonetically balanced speech data is available for speaker modeling, the phonetic variability tends to be smoothed out so that speaker-specific aspects can be captured. The vocal source related features, e.g., pitch and harmonics, on the other hand, characterize the vocal folds’ vibration style in speech production and are closely related to the speaker-specific laryngeal system. The spoken contents have less effect on the variation of the vocal source excitation than on that of the vocal tract system [Miller 1963; Childers 1991]. This makes the vocal source derived acoustic features useful for speaker recognition, especially for text-independent cases. However, the usefulness of vocal source information for speaker recognition, although having been investigated in some literature, has not been thoroughly studied, let alone the efficient information retrieving techniques. In this paper, a novel vocal source feature is presented and implemented to supplement the vocal tract features in speaker recognition. For voiced speech, the source excitation signal is a quasi-periodic glottal waveform, which is generated with quasi-periodic vocal fold vibration. The vibration frequency determines the pitch of voice. It has been shown that temporal pitch variation is useful for speaker recognition [Atal 1972; Sonmez 1998]. The amplitude of pitch harmonics has also been demonstrated to be an effective feature for speaker identification [Imperl et al. 1997]. To exploit detailed vocal source information, we need a method of automatically estimating the glottal waveform from the speech signal. This can be done by inverse filtering the speech signal with the vocal tract filter parameters estimated during the glottal closing phase (GCI). In Brookes and Chan [1994], a separately recorded laryngograph signal was used to detect the GCI. In Plumpe et al. [1999], a method of automatic GCI detection was proposed and the estimated glottal waveform was represented using the Liljencrants-Fant (LF) model. The model parameters were shown to be useful in speaker identification. However, this method worked well only for the typical voices in which the GCI clearly exists and the estimated  Integrating Complementary Features from Vocal Source and  275  Vocal Tract for Speaker Identification  glottal waveform can be well explained by the LF model [Plumpe et al. 1999]. In linear predictive (LP) modeling of speech signals, the vocal tract system is represented by an all-pole filter. The prediction error, which is named the LP residual signal, contains useful information about the source excitation [Rabiner and Schafer 1978]. In Thevenaz and Hugli [1995], it is shown that the cepstrum of LP residual signal could be used to improve the performance of a text-independent speaker verification system. In He et al. [1995] and Chen and Wang [2004], the standard procedures for extracting MFCC and LPCC features were applied to LP residual signals, resulting in a set of residual features for speaker recognition. In Yegnanarayana et al. [2005], the speaker information present in LP residual signals was captured using an auto-associative neural network model. Murty and Yegnanarayana [2006] proposed to extract residual phase information by applying Hilbert transform on LP residual signals. The phase features were used to supplement MFCC in speaker recognition.  
In this study, discriminative HMM training and its performance are investigated in both clean and noisy environments. Recognition error is defined at string, word, phone, and acoustic levels and treated in a unified framework in discriminative training. With an acoustic level, high-resolution error measurement, a discriminative criterion of minimum divergence (MD) is proposed. Using speaker-independent, continuous digit databases, Aurora2, the recognition performance of recognizers, which are trained in terms of different error measures and different training modes, is evaluated under various noise and SNR conditions. Experimental results show that discriminatively trained models perform better than the maximum likelihood baseline systems. Specifically, in MWE and MD training, relative error reductions of 13.71% and 17.62% are obtained with multi-training on Aurora2, respectively. Moreover, compared with ML training, MD training becomes more effective as the SNR increases. Keywords: Noise Robustness, Minimum Divergence, Minimum Word Error, Discriminative Training 1. Introduction With the progress of Automatic Speech Recognition (ASR), noise robustness of speech recognizers attracts more and more attention for practical recognition systems. Various noise robust technologies can be grouped into three classes: 1. Feature domain approaches, which aim at noise resistant features, e.g., speech enhancement, feature compensation or transformation methods [Gong 1995]; 2. Model domain approaches, e.g., Hidden Markov  ∗ University of Science and Technology of China, Hefei, P. R. China, 230027  Tel: +86-551-3601363-806  Fax: +86-551-3601363-807  E-mail: unuedjwj@ustc.edu; rhw@ustc.edu.cn + Microsoft Research Asia, Beijing, P. R. China, 100080  E-mail: {pengliu, frankkps, jlzhou }@microsoft.com  [Received March 31, 2007; Revised August 14, 2007; Accepted August 17, 2007]  292  Jun Du et al.  Model (HMM) decompensation [Varga et al. 1990], Parallel Model Combination (PMC) [Gales et al. 1994], which aim at modeling the distortion of features in noisy environments directly; 3. Hybrid approaches. In the past decade, discriminative training has been shown quite effective in reducing word error rates of HMM based ASR systems in a clean environment. In the first stage, sentence level discriminative training criteria, including Maximum Mutual Information (MMI) [Schluter 2000; Valtchev et al. 1997] and Minimum Classification Error (MCE) [Juang et al. 1997], were proposed and proven effective. Recently, new criteria such as Minimum Word Error (MWE) and Minimum Phone Error (MPE) [Povey 2004], which are based on fine error analysis at word or phone level, have achieved further improvement in recognition performance. In [Ohkura et al. 1993; Meyer et al. 2001; Laurila et al. 1998], noise robustness investigation on sentence level discriminative criteria such as MCE, Corrective Training (CT) is reported. Hence, we give a more complete investigation of noise robustness for general minimum error training. From a unified view of error minimization, the major difference between MCE, MWE and MPE is the error definition. String based MCE is based upon minimizing sentence error rate, while MWE is based on word error rate, which is more consistent with the popular metric used in evaluating ASR systems. Hence, the latter yields a better word error rate, at least on the training set [Povey 2004]. However, MPE performs slightly but universally better than MWE on the testing set [Povey 2004]. The success of MPE might be explained as follows: when refining acoustic models in discriminative training, it makes more sense to define errors in a more granular form of acoustic similarity. However, error definition at phone label level is only a rough approximation of acoustic similarity. Based on the analysis above, we have proposed using acoustic dissimilarity to measure errors [Du et al. 2006]. As acoustic behavior of speech units is characterized by HMMs, by measuring Kullback-Leibler Divergence (KLD) [Kullback et al. 1951] between two given HMMs, we can obtain a physically more meaningful assessment of their acoustic similarity. Adopting KLD for defining dissimilarity, the corresponding training criterion is referred as Minimum Divergence (MD) [Du et al. 2006; Du et al. 2007]. The criterion possesses the following potential advantages: 1) It employs acoustic similarity for high-resolution error definition, which is directly related to acoustic model refinement; 2) Label comparison is no longer used, which alleviates the influence of the chosen language model and phone set and the resultant hard binary decisions caused by label matching. Due to these advantages, MD is expected to be more flexible and robust. In our work, MWE, which matches the evaluation metric, and MD, which focuses on  Performance of Discriminative HMM Training in Noise  293  refining acoustic dissimilarity, are compared. Other issues related to robust discriminative training, including how to design the maximum likelihood baseline and how to treat with the silence model is also discussed. Experiments were performed on Aurora2 [Hirsch et al. 2000], which is a widely adopted database for research on noise robustness. For completeness, we tested the effectiveness of discriminative training on different ML baselines and different noise environments. The rest of paper is organized as follows. In Section 2, issues on noise robustness of minimum error training will be discussed. In Section 3, MD training will be introduced. Experimental results are shown and discussed in Section 4. Finally, in Section 5, we give our conclusions. 2. Noise Robustness Analysis of Minimum Error Training In this section, we will give a brief discussion of the major issues we are facing in robust discriminative training.  2.1 Error Resolution of Minimum Error Training In [Povey 2004] and [Du et al. 2006], various discriminative training approaches are unified under the framework of minimum error training, where the objective function is an average of the recognition accuracies A (W ,Wr ) of all hypotheses weighted by the posterior probabilities. For conciseness, we consider the single training utterance case:  F (θ ) = ∑ Pθ (W | O)A (W ,Wr )  (1)  W ∈M  where θ represents the set of the model parameters; O is a sequence of acoustic observation vectors; Wr is the reference word sequence; M is the hypotheses space; Pθ (W | O) is the posterior probability of the hypothesis W given O, which can be formulated as:  Pθ  (W  |  O)  =  ∑W  Pθκ (O | W )P(W ) '∈M Pθκ (O | W ')P(W  ')  (2)  where κ is the acoustic scaling factor.  The gain function A (W ,Wr ) is an accuracy measure of W given its reference Wr . In Table 1, comparison of several minimum error criteria are listed. In MWE training, A (W ,Wr ) is word accuracy, which matches the commonly used evaluation metric of speech recognition. However, MPE has been shown to be more effective in reducing recognition  errors because it provides a more precise measurement of word errors at the phone level. We  can argue this point by advocating the final goal of discriminative training. In refining  acoustic models to obtain better performance, it makes more sense to measure acoustic  294  Jun Du et al.  similarity between hypotheses instead of word accuracy. The symbol matching does not relate acoustic similarity with recognition. The measured errors can also be strongly affected by the phone set definition and language model selection. Therefore, acoustic similarity is proposed as a finer and more direct error definition in MD training.  Table 1. Comparison of criteria of minimum error training. ( PW : Phone sequence corresponding to word sequence W; LEV(,): Levenshtein distance between two symbol strings;| ⋅ |: Number of symbols in a string.)  Criterion  A (W ,Wr )  Objective  String based MCE MWE  δ (W = Wr ) Wr − LEV(W ,Wr )  Sentence accuracy Word accuracy  MPE MD  PWr − LEV(PW ,PWr ) −D(Wr || W )  Phone accuracy Acoustic similarity  Here, we aim at seeking how criteria with different error resolution performs in noisy environments. In our experiments, the whole-word model, which is commonly used in digit tasks, is adopted. For the noisy robustness analysis, MWE, which matches with the evaluation metric of speech recognition, will compared with MD, which possesses the highest error resolution as shown in Table 1.  2.2 Training Modes In noisy environments, various ML trained baselines can be designed. So, the effectiveness of minimum error training with different training modes will be explored. In [Hirsch et al. 2000], two different sets of training, clean-training and multi-training, are used. In clean-training mode, only clean speech is used for training. Hence, there will be a mismatch when the model is tested in noisy environments. To alleviate the mismatch, multi-training, in which training set is composed of noisy speech with different SNRs, can be applied. Actually, multi-training can only achieve a “global SNR” match. To achieve a “local SNR” match, we adopt a SNR-based training mode. In the training phase, we train a series of models at different SNR levels, while in testing, all these models are paralleled as multi pronunciations of a HMM. Ideally, the model that matched the local SNR best will be automatically selected in decoding. SNR-based training can be considered as a high resolution acoustic modeling of multi-training. An illustration of the three training modes is shown in Figure 1. An important issue in discriminative training is how to update silence or background models, which is even more critical in a noisy environment. In our research, we pay special attention to this issue for appropriate guidelines.  Performance of Discriminative HMM Training in Noise  295  Figure 1. Illustration of three training modes 3. Word Graph based Minimum Divergence Training  3.1 Defining Errors by Acoustic Similarity  A word sequence is acoustically characterized by a sequence of HMMs. For automatically measuring acoustic similarity between W and Wr , we adopt KLD between the corresponding HMMs:  A (W ,Wr ) = −D(Wr || W )  (3)  The HMMs, when they are reasonably well trained in ML sense, can serve as succinct descriptions of data.  3.2 KLD between Two Word Sequences Our goal is to measure the KLD for word sequences in Eq. 3. Given two word sequences Wr and W without their state segmentations, we should use a state matching algorithm to measure the KLD between the corresponding HMMs [Liu et al. 2005]. With state segmentations, the calculation can be further decomposed down to the state level:  D(Wr & W ) = D(s1r:T & s1:T )  =∫  p(o1:T  |  s1r:T ) log  p(o1:T p(o1:T  |s1r:T |s1:T  ) )  do1:T  (4)  296  Jun Du et al.  where T is the number of frames; o1:T and s1r:T are the observation sequence and hidden state sequence, respectively.  By assuming all observations are independent, we obtain:  D(s1r:T  & s1:T )  =  T ∑  D(srt  t =1  &  st )  =  T ∑  ∫  t =1  p(ot  | srt ) log  p(ot p(ot  | srt ) dot | st )  (5)  which means we can calculate KLD state by state, and sum them up.  Now, our problem is how to measure the KLD between two states. Conventionally, each  state s is characterized by a Gaussian Mixture Model (GMM):  p(o | s) =  ∑  Ms m=1  wsm  N  (o; µsm , ∑ sm ) ,  so  the  comparison  is  reduced  to  measuring  KLD  between two GMMs. Since there is no closed-form solution, we need to resort to the  computationally intensive Monte-Carlo simulations. The unscented transform mechanism  [Goldberger et al. 2003] has been proposed to approximate the KLD measurement of the two  GMMs.  Let N (o; µ, ∑ ) be a N -dimensional Gaussian distribution and h be an arbitrary IR N → IR function, the unscented transform mechanism suggests approximating the expectation of h by:  ∫N  (o; µ, ∑ )h(o)do ≈  
Multilingual spoken language corpora are indispensable for research on areas of spoken language communication, such as speech-to-speech translation. The speech and natural language processing essential to multilingual spoken language research requires unified structure and annotation, such as tagging. In this study, we describe an experience with multilingual spoken language corpus development at our research institution, focusing in particular on speech recognition and natural language processing for speech translation of travel conversations. An integrated speech and language database, Spoken Language DataBase (SLDB) was planned and constructed. Basic Travel Expression Corpus (BTEC) was planned and constructed to cover a variety of situations and expressions. BTEC and SLDB are designed to be complementary. BTEC is a collection of Japanese sentences and their translations, and SLDB is a collection of transcriptions of bilingual spoken dialogs. Whereas BTEC covers a wide variety of travel domains, SLDB covers a limited domain, i.e., hotel situations. BTEC contains approximately 588k utterance-style expressions, while SLDB contains about 16k utterances. Machine-aided Dialogs (MAD) was developed as a development corpus, and both BTEC and SLDB can be used to handle MAD-type tasks. Field Experiment Data (FED) was developed as the evaluation corpus. We conducted an experiment, and based on analysis of our follow-up questionnaire, roughly half the subjects of the  ∗ ATR Spoken Language Communication Research Laboratories, 2-2-2 Hikaridai, Keihanna Science  City, Kyoto 619-0288, Japan  Telephone: +81 774 95 1301 Fax: +81 774 95 1308  E-mail: toshiyuki.takezawa@atr.jp + currently with NTT Cyberspace Laboratories, Japan  E-mail: {kikui.genichiro, mizushima.masahide}@lab.ntt.co.jp # National Institute of Information and Communications Technology, Japan  E-mail: eiichiro.sumita@{nict.go.jp; atr.jp}  [Received April 6, 2007; Revised August 3, 2007; Accepted August 6, 2007]  304  Toshiyuki Takezawa et al.  experiment felt they could understand and make themselves understood by their partners. Keywords: Multilingual Corpus, Spoken Language, Speech Translation, Dialog, Communication. 1. Introduction Various kinds of corpora developed for analysis of linguistic phenomena and statistical information gathering are now accessible via electronic media and can be utilized for the study of natural language processing. Since these include written-language and monolingual corpora, however, they are not necessarily useful for research and development of multilingual spoken language processing. A multilingual spoken language corpus is indispensable for research on areas of spoken language communication such as speech-to-speech translation. Research on speech translation began in the 1980s. NEC demonstrated a prototype speech translation system at the Telecom ’83 exhibition. ATR Interpreting Telephony Research Laboratories was established in 1986 for the research of basic speech translation technologies and produced ASURA [Morimoto et al. 1993]. This system can recognize well-formed Japanese utterances in a limited domain, translate them into both English and German, and output synthesized speech. The ASURA system was used for the International Joint Experiment of Interpreting Telephony with participants from Kyoto, Japan (ATR), Pittsburgh, USA (Carnegie Mellon University [Lavie et al. 1997]) and Munich, Germany (Siemens and University of Karlsruhe) in January 1993 [Morimoto et al. 1993]. Many projects on speech-to-speech translation began at that time [Rayner et al. 1993; Roe et al. 1992; Wahlster et al. 2000]. SRI International and Swedish Telecom developed a prototype speech translation system that could translate queries from spoken English to spoken Swedish in the domain of air travel information systems [Rayner et al. 1993]. AT&T Bell Laboratories and Telefónica Investigación y Desarrollo developed a restricted domain spoken language translation system called Voice English/Spanish Translator (VEST) [Roe et al. 1992]. In Germany, Verbmobil [Wahlster 2000], was created as a major speech-to-speech translation research project. The Verbmobil scenario assumes native speakers of German and of Japanese who both possess at least a basic knowledge of English. The Verbmobil system supports them by translating from their mother tongue, i.e. Japanese or German, into English. In the 1990s, speech recognition and synthesis research shifted from a rule-based to a corpus-based approach such as HMM and N -gram. However, machine translation research still depended mainly on a rule-based or knowledge-based approach. In the 2000s, wholly corpus-based projects such as European TC-STAR [Höge 2002; Lazzari 2006] and DARPA GALE [Roukos 2006] began to deal with monologue speeches such as broadcast news and  Multilingual Spoken Language Corpus Development for  305  Communication Research  European Parliament plenary speeches. In this paper, we report corpus construction activities for translation of spoken dialogs of travel conversations. There are a variety of requirements for every component technology, such as speech recognition and language processing. A variety of speakers and pronunciations may be important for speech recognition, and a variety of expressions and information on parts of speech may be important for natural language processing. The speech and natural language processing essential to multilingual spoken language research requires unified structure and annotation, such as tagging. In this paper, we introduce an interpreter-aided spoken dialog corpus and discuss corpus configuration. Next, we introduce the basic travel expression corpus developed to train machine translation of spoken language among Japanese, English, and Chinese speakers. Finally, we discuss the Japanese, English, and Chinese multilingual spoken dialog corpus that we created using speech-to-speech translation systems. 2. Overview of Approach We first planned and constructed an integrated speech and language database called Spoken Language DataBase (SLDB) [Morimoto et al. 1994; Takezawa et al. 1998]. The task involved travel conversations between a foreign tourist and a front desk clerk at a hotel; this task was selected because people are familiar with it and because we expect it to be included in future speech translation systems. All of the conversations for this database take place in English and Japanese through interpreters because the research at that time concentrated on Japanese and English. The interpreters serve as the speech translation system. One remarkable characteristic of the database is its integration of speech and linguistic data. Each conversation includes data on recorded speech, transcribed utterances, and their correspondences. This kind of data is very useful because it contains transcriptions of spoken dialogs between speakers who speak different mother tongues. However, the cost of collecting spoken languages is too high to expand the size. There are three important points to consider in designing and constructing a corpus for dialog-style speech communication such as speech-to-speech translation. The first is to have a variety of speech samples with a wide range of pronunciations, speaking styles, and speakers. The second point is to have data for a variety of situations. A “situation” means one of various limited circumstances in which the system’s user finds him- or herself, such as an airport, a hotel, a restaurant, a shop, or in transit during travel; it also involves various speakers’ roles, such as communication with a middle-aged stranger, a stranger wearing jeans, a waiter or waitress, or a hotel clerk. The third point is to have a variety of expressions.  306  Toshiyuki Takezawa et al.  According to our previous study [Takezawa et al. 2000], human-to-machine conversational speech data shared characteristics with human-to-human indirect communication speech data such as spoken dialogs between Japanese and English speakers through human interpreters. Moreover, human-to-human indirect communication data had an intermediate characteristic, i.e., it was positioned somewhere between direct communication data, that is, Japanese monolingual conversations, and speech data from conversational text. If we assume that a speaker would accept a machine-friendly speaking style, we could take a great step forward: a clear separation of speech data collection and multilingual data collection. In the following, we focus on multilingual data collection. In order, Basic Travel Expression Corpus (BTEC) [Takezawa et al. 2002; Kikui et al. 2003] was planned to cover the varieties of situations and expressions.  Machine-aided Dialogs (MAD) was planned as a development corpus to handle the differences between the target utterance with which speech translation systems must deal and the following two corpora.  SLDB contains no recognition/translation errors because the translations between people speaking different languages are done by professional human interpreters. However, even a state-of-the-art speech translation system cannot avoid recognition/translation errors.  BTEC contains edited colloquial travel expressions, which are not transcriptions, so some people might not express things in the same way, and the frequency distribution of expressions might be different from actual dialogs.  Field Experiment Data (FED) was planned as the evaluation corpus. Table 1 shows an overview of the corpora. In the table, S2ST stands for speech-to-speech translation, MT stands for machine translation, J, E, and C stand for Japanese, English, and Chinese, respectively.  Table 1. Overview of corpora  SLDB  BTEC  MAD  FED  Name  Spoken Language DataBase  Basic Travel Expression Corpus  MachineAided Dialogs  Field Experiment Data  Purpose Domain Languages  Developing S2ST Training MT Developing S2ST Evaluation of S2ST  Hotel  Travel  Travel  Travel  J E (C)  J E C  J E (C)  J E C  Speaker Participants  71 (+23 Interpreters)  Not spoken  45  84  Size  16k  588k  13k  2k  Multilingual Spoken Language Corpus Development for  307  Communication Research  3. Interpreter-Aided Spoken Dialog Corpus (SLDB)  SLDB contains data from dialog spoken between English and Japanese speakers through human interpreters [Morimoto et al. 1994; Takezawa et al. 1998]. All utterances in SLDB have been translated into Chinese. The content is entirely travel conversations between a foreign tourist and a front desk clerk at a hotel. Human interpreters serve as the speech translation system.  Table 2 is an overview of the corpus, and Table 3 shows its basic characteristics.  Table 2. Overview of SLDB  Number of collected dialogs  618  Speaker participants  71  Interpreter participants  23  Table 3. Basic characteristics of SLDB  Japanese  Number of utterances  16,084  Number of sentences  21,769  Number of word tokens  236,066  Number of word types  5,298  Average number of words per sentence  10.84  English 16,084 22,928 181,263 4,320 7.91  One remarkable characteristic of SLDB is its integration of speech and linguistic data. Each conversation includes recorded speech data, transcribed utterances, and the correspondences between them. The transcribed Japanese and English utterances are tagged with morphological information. This kind of tagged information is crucial for natural language processing as well as for speech recognition language modeling. The recorded speech signals and transcribed utterances in the database provide both examples of various phenomena in bilingual conversations, and input data for speech recognition and machine translation evaluation purposes. Data can be classified into the following three major categories. 1. Transcribed data 2. Tagged data 3. Speech data  308  Toshiyuki Takezawa et al.  Transcribed data consists of the following. (a) Bilingual text (b) Japanese text (c) English text The recorded bilingual conversations are transcribed into a text file. The bilingual text contains descriptions of the situations in which a speech translation system is used. J: Arigatou gozaimasu. Kyoto Kankou Hotel de gozaimasu. JE: Thank you for calling Kyoto Kanko Hotel. |How may I help you? E: Good evening. |I’d like to make a reservation, please. EJ: Konbanwa. |Yoyaku wo shi tai n desu keredomo. J: Hai,[e-]go yoyaku no hou wa itsu desho u ka? JE: Yes, when do you plan to stay? E: I’d like to stay from August tenth through the twelfth, for two nights.| If possible, I’d like a single room, please. EJ: Hachigatsu no tooka kara juuni-nichi made, ni-haku shi tai n desu.| Dekire ba, single room de onegaishimasu. J: Kashikomarimashita. |Shoushou o-machi kudasai mase. JE: All right, please wait a moment. J: O-mata se itashimashita.| Osoreiri masu ga, single room wa manshitsu to nat te orimasu. JE: I am very sorry our single rooms are all booked. J: [e]Washitsu ka twin room no o-hitori sama shiyou deshi tara o-tori dekimasu ga. JE: But, Japanese style rooms and twin rooms for single use are available. E: [Oh] what are the rates on those types of rooms? EJ: Sono o-heya no ryoukin wo oshie te kudasai. Figure 1. Conversation between an American tourist and a Japanese front desk clerk.  Multilingual Spoken Language Corpus Development for  309  Communication Research  Figure 1 shows an example of transcribed conversations. The Japanese text in Figure 1 has been transcribed into Romanized Japanese for the convenience of readers who do not understand Japanese hiragana, katakana, and kanji (Chinese characters). The original text was transcribed in Japanese characters hiragana, katakana, and kanji. Interjections are bracketed. J, E, JE, or EJ at the beginning of a line denotes a Japanese speaker, an English speaker, a Japanese-to-English interpreter, or an English-to-Japanese interpreter, respectively. “ | ” denotes a sentence boundary. A blank line between utterances shows that the utterance’s right was transferred. The Japanese text is produced by extracting the utterances of a Japanese speaker and an English-to-Japanese interpreter, while the English text is produced by extracting the utterances of an English speaker and a Japanese-to-English interpreter. These two kinds of data are utilized for such monolingual investigations as morphological analysis. The tagged data consists of the following. (d) Japanese morphological data (e) English morphological data SLDB is available to outside research institutions and can be accessed at the following URL: http://www.atr.jp. 4. Basic Travel Expression Corpus (BTEC) The Basic Travel Expression Corpus (BTEC) [Takezawa et al. 2002; Kikui et al. 2003] was designed to cover utterances for possible travel conversations topic and their translations. Since it is practically impossible to collect them by transcribing actual conversations or simulated dialogs, we decided to use sentences provided by bilingual travel experts based on their experience. We started by looking at phrasebooks that contain bilingual sentence pairs (in this case Japanese/English) that the editors consider useful for tourists traveling abroad. Such sentence pairs were collected and rewritten to make translation as context-independent as possible and to comply with the speech transcription style of our research institution. Sentences that were outside of the travel domain or have very special meanings were removed. Table 4 lists the basic statistics of the BTEC collections, called BTEC1, 2, 3, 4, and 5. Each collection was created using the same procedure in a different time period or using a different translation direction from the source language to target languages. Strictly speaking, morphemes are used as the basic linguistic unit for Japanese (instead of words), since morpheme units are more stable than word units.  310  Toshiyuki Takezawa et al.  Table 4. Overview of BTEC  BTEC1 BTEC2 BTEC3 BTEC4 BTEC5  Number of utterance-style expressions 172k  Number of Japanese word tokens  1,174k  Number of Japanese word types  28k  Languages (Source:Targets)  J:EC  46k 341k 20k J:EC  198k 1,434k 43k J:EC  74k 548k 22k E:JC  98k 1,046k 28k E:JC  The aims of the BTEC corpus are for translation and language modeling for automatic speech recognition. For translation, one of the key points to cover is the translation direction from the source language to target languages. For automatic speech recognition in the travel domain, one of the key points to cover is multiple sub-domains such as airport-related dialogs, hotel-related dialogs, and so on. For translation, the BTEC collections cover both translation directions. BTEC1, BTEC2, and BTEC3 contain expressions for Japanese tourists visiting the USA, UK, or Australia. The translation direction is from Japanese to English and Chinese. BTEC4 mainly contains expressions for American tourists who visit Japan. The translation direction is from English to Japanese and Chinese. BTEC5 contains various expressions, such as those for American tourists who go to Korea. The translation direction is from English to Japanese and Chinese. For automatic speech recognition, BTEC covers multiple domains. Domain information is given for BTEC1, BTEC2, and BTEC3. Table 5 shows an overview. BTEC sentences, as described above, did not come from actual conversations but were generated by experts as reference materials. This approach enabled us to efficiently create a broad corpus; however, it may have two problems. First, this corpus may lack utterances that occur in real conversation. For example, when people ask the way to a bus stop, they often use a sentence like (1). However, in BTEC this is expressed more directly, as in (2). (1) I’d like to go downtown. Where can I catch a bus? 
The Pinyin-to-Character Conversion task is the core process of the Chinese pinyin-based input method. Statistical language model techniques, especially ngram-based models, are mostly adopted to solve that task. However, the ngram model only focuses on the constraints between characters, ignoring the pinyin constraints in the input pinyin sequence. This paper improves the performance of the Pinyin-to-Character Conversion system through exploitation of the pinyin constraints. The MEMM framework is used to describe the pinyin constraints and the character constraints. A Class-based MEMM (C-MEMM) model is proposed to address the MEMM efficiency problem in the Pinyin-to-Character Conversion task. The C-MEMM probability functions are strictly deduced and well formulized according to the Bayes rule and the Markov property. Both the cases of hard class and soft class are well discussed. In the experiments, C-MEMM outperforms the traditional ngram model significantly by exploitation of the pinyin constraints in the Pinyin-to-Character Conversion task. In addition, C-MEMM can well utilize the syntax and semantic information in word class and further improve the system performance. Keywords: Pinyin-to-Character Conversion, MEMM, Class-Based 1. Introduction The standard keyboard was initially designed for native English speakers. In Asia, such as China, Japan and Thailand, people cannot input their language through the standard keyboard directly. Asian text input becomes one of the challenges for computer users in Asia. Therefore, an Asian language input method is one of the most difficult problems in Asian language processing. * School of Computer Science and Techniques, Harbin Institute of Technology, Harbin, 150001, China E-mail: {xiaojinghui, liubq, wangxl}@insun.hit.edu.cn [Received October 12, 2006; Revised May 15, 2007; Accepted May 23, 2007]  326  Jinghui Xiao et al.  For Chinese, the input methods can be roughly divided into two types: one is the structure-based or shape-based input method, which was developed based on the structure of Chinese characters, such as the Wubi method [Wang 2005], Cangjie method, Boshiamy method, among others. These methods can reach a high input speed by a skilled user. However, a lot of effort is required to master them. The other is the pronunciation-based input method, such as the Insun input method [Wang 1993], Microsoft input method, Bopomofo, among others. These methods are easy to learn. The user can input the Chinese character with scarcely any training, on the condition that they can pronounce it correctly. Hybrid input methods have also been proposed, i.e. Renzhi and Tze-loi input method. However, they only possess a limited share of the market. The Pinyin-based input method is one of the most important pronunciation-based input methods. Pinyin is a system of Romanization for standard Mandarin. It uses Roman letters to represent the sound of Chinese characters. Hanyu Pinyin is the most common variant of pinyin in use. It was approved in 1958, and the government of the People ’s Republic of China has adopted Hanyu pinyin as the phonetic instruction in the mainland of China. In 1979, Hanyu pinyin was adopted by the International Organization for Standardization (ISO) as the standard Romanization for modern Chinese [ISO 7098: 1991]. The Pinyin-based input method dominates the market of Chinese input methods. It is said that over 97% of Chinese computer users are using pinyin to input Chinese [Chen 1997]. According to the scale of input unit, the pinyin-based input method can be divided into three types: the character-level input method, the word-level or phrase-level input method, and the sentence-level input method, respectively. The sentence-level input method usually achieves higher accuracy by exploitation of more context information than the other two. It has become the most prevalent pinyin-based input method. The Pinyin-to-Character Conversion task aims to convert a sequence of pinyin strings into one Chinese sentence. It is the core process of the sentence-level pinyin-based input method. Therefore, improving the performance of the Pinyin-to-Character Conversion system is well worth studying. In addition, the Pinyin-to-Character Conversion task can be taken as a simplified task of automatic speech recognition, both of which aim to convert phonetic information into character sequence. However, the Pinyin-to-Character Conversion task doesn ’t have to deal with acoustic ambiguity because the pinyin strings are directly input through the keyboard. Therefore, the technique is also illuminative in the task of automatic speech recognition. The linguist approach [Wang 1993; Hsu and Chen 1993; Kuo 1995] and the statistical approach [Zhang et al. 1998; Xu et al. 2000; Wu 2000; Gao et al. 2002; Gao et al. 2005; Xiao et al. 2005a] are two technical approaches to the Pinyin-to-Character Conversion task. The statistical approach is mainly based on the technique of statistical language models, especially the ngram model and its variant forms. In recent years, it has drawn great interest due to its  Exploiting Pinyin Constraints in Pinyin-to-Character Conversion Task:  327  a Class-Based Maximum Entropy Markov Model Approach  efficiency and robustness. However, several drawbacks have also been found in the traditional ngram model. First, according to Zipf ’s law [Zipf 1935], there are a lot of words which rarely or never occur in the training corpus. The data sparseness problem is severe [Brown et al. 1992] in the ngram model. Second, long distance constraints are difficult to capture since the ngram model only focuses on local lexical constraints. Third, it ’s hard to utilize the linguistic knowledge of the ngram model. Many techniques have been proposed to address the drawbacks of the traditional ngram model. To solve the data sparseness problem, various kinds of smoothing techniques have been proposed, such as additive smoothing [Jeffreys 1948], Katz smoothing [Katz 1987], linear interpolation smoothing [Jelinek and Mercer 1980], semantic based smoothing [Xiao et al. 2005b; Xiao et al. 2006]. To utilize the linguistic knowledge, a set of linguistic rules are generated automatically and they are incorporated into the traditional ngram model by a hybrid ngram model [Wang et al. 2005]. Hsu [Hsu 1995] proposes the context sensitive model (CSM) in which the semantic patterns are captured by the templates. As much as 96% accuracy, which is the best result of the traditional Chinese input methods as far as we know, is reported for CSM on the Phoneme-to-Character Conversion task. Trigger techniques have been proposed [Zhou and Lua 1998] and word-pair techniques have been proposed [Tsai and Hsu 2002; Tsai et al. 2004; Tsai 2005; Tsai 2006]. The linguist knowledge can be effectively described by triggers and pairs; meanwhile, the long distance constraints can be well captured. Compared with the commercial input system (MS-IME 2003), effective improvements have been achieved by these techniques [Tsai 2006]. Wang [Wang et al. 2004] utilizes the theory of rough set so as to discover the linguistic knowledge and incorporate it into the Pinyin-to-Character Conversion system. Compared with the traditional ngram model, Wang ’s system achieves a higher accuracy with a smaller storage requirement. Xiao [Xiao et al. 2005a] incorporates the word positional information into the Pinyin-to-Character Conversion system and achieves encouraging results in experiments. Gao [Gao et al. 2005] proposes the Minimum Sample Risk (MSR) principle to estimate the parameters of the ngram model. Success has been achieved with this principle for a Japanese input method. What’s more, some techniques have been proposed especially for Chinese text input method. A Pinyin-to-Character Conversion system with spelling-error correction was developed by Zhang [Zhang et al. 1997]. In the system, a rule-based model is designed to correct typing errors when the user inputs pinyin strings. Not only can the system accept the correct pinyin input, but it can also tolerate common typing errors. Similar work has been done by Chen [Chen and Lee 2000]. Chen constructs a statistical model to correct user typing errors. Moreover, Chen proposes a modeless input technique in which the user can input English using a Chinese input method, not requiring language mode switch. However, there is another drawback of the ngram model in the Pinyin-to-Character  328  Jinghui Xiao et al.  Conversion task, which has been ignored by most researchers. It takes no account of pinyin constraints on the input pinyin sequence while actually in the process of Pinyin-to-Character Conversion. This paper regards that the pinyin information from the pinyin sequence is helpful for selecting the correct character sequence in the Pinyin-to-Character Conversion task. First, the current input pinyin string is helpful for selecting the correct character which corresponds to that pinyin. For example, the input pinyin sequence is “ta1 shi4 di2 shi4 you3?” which should be converted into “他是敌是友?” (“Is he an enemy or friend?”). Let’s focus on the third pinyin string of “di2”. There are two homonyms which correspond to it: “敌” and “的”. (There are actually many homonyms, but let ’s only focus on “敌” and “的” for simplification). “的” is one of the most frequent Chinese characters and its frequency is usually much higher than “敌”. According to the ngram model, the above pinyin sequence should be converted into “ 他 是 的 是 友 ?” which is a wrong conversion. However, “ 的 ” is a polyphone which corresponds to both “di2” and “de5”. In Chinese, “的” is usually pronounced as “de5” instead of “di2”. (“的” is pronounced as “di2” only in the word “的确” (certainly)). The frequency of “的” mainly comes with its pronunciation “de5”. If the pinyin information is considered in the above conversion, the co-occurrences of “的” and “di2” are usually lower than that of “敌” and “di2”. Then, the above pinyin sequence is correctly converted into “他是敌是友?”. Second, the contextual information, especially the future information, can be well exploited in the pinyin constraints. For example, there are two pinyin sequences. The first one is “yi4 zhi1 ke3 ai4 de5 xiao3 hua1” which should be converted into “一枝可爱的小花” (This is a lovely flower). The second pinyin “zhi1” should be converted into “枝” which is determined by its future character “花” (flower). The second pinyin sequence is “yi4 zhi1 ke3 ai4 de5 xiao3 hua1 mao1” which should be converted into “一只可爱的小花猫” (This is a lovely cat). The second pinyin “zhi1” should be converted into “只” which is determined by its future character “猫” (cat). However, according to the ngram model, the conversion of “zhi1” is only determined by its history information which is the character “一” in the above two cases. The characters of “花” and “猫” are both the further information that the ngram model can not exploit. Therefore, the same probabilities are assigned to both the characters of “只” and “枝”. They can not be distinguished by the ngram model. In the above two conversions, at least one of them would be converted incorrectly. However, if the pinyin constraints are considered, the constraints of “hua1” and “mao1”, which correspond to the characters of “花” and “猫”, are exploited and imposed on the conversion of “zhi1”. Then, the above two cases can be distinguished and the correct conversions can be obtained. Third, the long distance constraints can be exploited from the pinyin sequence. As for the ngram model, it has to construct a high-order model to capture the long distance constraints. However, high-order ngram models suffer from the curse of dimensionality which usually leads to a severe data sparseness problem. The current model order is usually 2 or 3. In the above example, in order to exploit  Exploiting Pinyin Constraints in Pinyin-to-Character Conversion Task:  329  a Class-Based Maximum Entropy Markov Model Approach  the constraints of “花” and “猫” on the conversion of “zhi1”, it has to build up at least a 7-order ngram model which suffers from a great data sparseness problem and cannot work well in reality. However, the pinyin constraints are collected as features and exploited under the Maximum Entropy (ME) framework in this paper. The context window size can be relatively large (i.e. 5 pinyin strings or 7 pinyin strings) without the curse of dimensionality. Then the constraints of “花” and “猫” can be imposed on the conversion of “zhi1” by exploitation of their pinyin information. This paper aims to improve the performance of the Pinyin-to-Character Conversion system by exploitation of the pinyin constraints from pinyin sequence. The pinyin constraints are described under the ME framework [Berge et al. 1996], and the character constraints are modeled by the traditional ngram model. Combining these two models into a unified framework, the paper builds the Pinyin-to-Character Conversion system on a MEMM model [McCallum et al. 2000]. However, the label set on the Pinyin-to-Character Conversion task is the Chinese lexicon. The scale of Chinese lexicon is usually in the range of 104 : 106 , which is too large for the current training algorithms of MEMM. Therefore MEMM cannot be directly applied to the Pinyin-to-Character Conversion task. This paper involves the addition of the class of target label into traditional MEMM and proposes a Class-based Maximum Entropy Markov Model (C-MEMM) so as to solve the MEMM efficiency problem in the Pinyin-to-Character Conversion task. In C-MEMM, the pinyin constraints are first imposed on the class sequence instead of the target label sequence as MEMM does. The classes of target label can be obtained by some automatic algorithms [Li 1998; Chen and Huang 1999; Gao et al. 2001] or from some pre-defined thesauri [Mei et al. 1983]. The scale of class set is usually much smaller than that of target label, which makes it feasible to train C-MEMM under the Maximum Entropy principle. Then, these constraints are conveyed from the class sequence to the target label sequence. So, C-MEMM can efficiently exploit the pinyin constraints from pinyin sequence and get effective improvement in the Pinyin-to-Character Conversion task. The paper is organized as follows: the MEMM model is briefly reviewed in Section 2. In Section 3, the C-MEMM model is proposed and its probability functions are deduced according to the Bayes rule and the Markov property. Both the cases of hard class and soft class are discussed in detail. Experimental results and discussions are provided in Section 4. The related works are described in Section 5, and the conclusions are drawn in Section 6. 2. Brief Review of MEMM MEMM is a powerful tool used to perform the sequence labeling task, which is to determine a state sequence according to the observation sequence. Different from the ngram model, MEMM not only makes use of the constraints between states but also utilizes the constraints from observations. MEMM integrates these two kinds of constraints into a uniform  330  Jinghui Xiao et al.  conditional probability function. More formally, given the observation sequence of O = o1, o2...on and the state sequence of S = s1, s2...sn , MEMM estimates the conditional probability of P(S | O) . The probability function of MEMM can be deduced in the following way:  P(S | O) = p(s1, s2...sn | o1, o2...on )  Bayesian Rule = p(s1 | o1, o2...on ) p(s2 , s3...sn | o1, o2...on , s1)  Markov Property  =  p(s1 | o1) p(s2 , s3...sn | o1, o2...on , s1)  Bayesian Rule  = p(s1 | o1) p(s2 | o1, o2...on , s1) p(s3...sn | o1, o2...on , s1, s2 )  (1)  Markov Property  =  p(s1 | o1) p(s2 | s1, o2 ) p(s3...sn | o1, o2...on , s1, s2 )  KKKK  n = p(s1 | o1) Õ p(si | si-1, oi ) i=2  MEMM estimates the probability of p(si | si-1, o) under the ME principle so as to utilize the overlapping features. The ME principle assumes that the trained model should be consistent with certain constraints derived from the training data; meanwhile the model should make the fewest assumptions about the data. To predicate the current state s, the contextual information of s is extracted from the training data and represented as the feature function:  f  (h,  s)  =  ìï1 í  if  h = h* and s = s*  (2)  ïî 0 otherwise  where h is the contextual information of state s and h* (or s*) is the concrete instance of h (or  s). The following constraints are imposed so that the expectation of each feature in the learned  model should be consistent with its empirical value in the training corpus. More formally, the  constraints can be expressed as:  Ep( f ) = E~ ( f )  (3)  p  where Ep ( f ) is the model expectation and is defined as:  ~  Ep ( f ) = å p(h) p(s | h) f (h, s)  (4)  h,s  and E ( f ) is the empirical expectation and is defined as: ~ p  ~  E ~ ( f ) = å p(h, s) f (h, s)  (5)  p  h,s  Exploiting Pinyin Constraints in Pinyin-to-Character Conversion Task:  331  a Class-Based Maximum Entropy Markov Model Approach  Under these constraints, ME principle guarantees a learned model as uniform as possible, and the model can be obtained by maximizing the conditional entropy of the training data:  ~  H ( p) = - å p(h) p(s | h) logp(s | h)  (6)  h,s  It results in the probability function of exponential form:  p(s  |  s', o)  =  
This paper presents a generative model based on the language modeling approach for sentiment analysis. By characterizing the semantic orientation of documents as “favorable” (positive) or “unfavorable” (negative), this method captures the subtle information needed in text retrieval. In order to conduct this research, a language model based method is proposed to keep the dependent link between a “term” and other ordinary words in the context of a triggered language model: first, a batch of terms in a domain are identified; second, two different language models representing classifying knowledge for every term are built up from subjective sentences; last, a classifying function based on the generation of a test document is defined for the sentiment analysis. When compared with Support Vector Machine, a popular discriminative model, the language modeling approach performs better on a Chinese digital product review corpus by a 3-fold cross-validation. This result motivates one to consider finding more suitable language models for sentiment detection in future research. Keywords: Sentiment Analysis, Subjective Sentence, Language Modeling, Supervised Learning. 1. Introduction Traditional wisdom of document categorization lies in mapping a document to given topics that are usually sport, finance, politics, etc. Whereas, in recent years there has been a growing interest in non-topical analysis, in which characterizations are sought by the opinions and feelings depicted in documents, instead of just their themes. This method of analysis is defined to classify a document as favorable (positive) or unfavorable (negative), which is called sentiment classification. Labeling documents by their semantic orientation provides succinct summaries to readers and will have a great impact on the field of intelligent information retrieval.  ∗ Department of Computer Science and Engineering, Shanghai Jiao Tong University, 800 Dongchuan Rd, Shanghai, China. Tel: 86-21-3420 4591 E-mail: huyi@cs.sjtu.edu.cn [Received August 2, 2006; Revised March 7, 2007; Accepted March 8, 2007]  108  Yi Hu et al.  In this study, the set of documents is rooted in the topic of digital product review, which will be defined in the latter part of this article. Accordingly, the documents can be classified into praising the core product or criticizing it. Obviously, a praising review corresponds to “favorable” and a criticizing one is “unfavorable” (the neutral review is not considered in this study). Most research for document categorization adopts the “bag of words” representing model that treats words as independent features. On the other hand, utilizing such a representing mechanism may be imprecise for sentiment analysis. Take a simple sentence in Chinese as an example: “柯达 P712 内部处理器作了升级，处理速度应该更快了。(The processor inside Kodak P712 has been upgraded, so its processing speed ought to be faster.)” The term “柯达 (Kodak)” is very helpful for determining its theme of “digital product review”, but words “升 级(update)” and “快(fast)” corresponding to “处理器(processor)” and “处理速度(processing speed)” ought to be the important clues for semantic orientation (praise the product). Inversely, see another sentence in Chinese: “这样电池损耗就很快。(So, the battery was used up quickly.)” The words “损耗 (use up)” and “快 (fast)” become unfavorable features of the term “电池 (battery)”. That is to say, these words probably contribute less to the sentiment classification if they are dispersed into the document vector, because the direct/indirect relationships between ordinary words and the terms within the sentence are lost. Unfortunately, traditional n-gram features cannot easily deal with these long-distance dependencies. Sentiment classification is a complex semantic problem [Pang et al. 2002; Turney 2002] that needs knowledge for decision-making. The researchers, here, explore a new idea-based language model for the sentiment classification of sentences rather than full document, in which the terms such as “处理器 (processor)”, “处理速度 (processing speed)” are target objects to be evaluated in the context. They are mostly the nouns or noun phrases: “屏幕 (Screen)”, “ 分 辨 率 (Resolution)”, “ 颜 色 (Color)”, etc. If the sentiment classifying knowledge on how to comment on these terms can be obtained by the training data in advance, the goal of sentiment analysis can be achieved by matching the terms in the test documents. Thus, the classifying task for the full document is changed to recognizing the semantic orientation of all terms in accordance with their sentence-level contexts. This can also be considered a positive/negative word counting method for sentiment analysis. In this study, the authors construct two language models for each term to capture the difference of sentiment context for that term. In these language models, sentences are divided into terms and their contexts. Sentences without the defined terms are ignored since they make no contribution to the document level sentiment classification; hence, they are omitted from training and test documents. This idea of grouping a document under subjective and objective portions is similar to Pang’s work [Pang and Lee 2004].  Using a Generative Model for Sentiment Analysis  109  This work can be divided into three main parts: first, some terms are extracted from a Chinese digital product review corpus [Chen et al. 2005]; second, two language models representing positive and negative classifying knowledge for each term are determined from training a subjective sentence set; third, the two models are applied to the test set and then compared with a popular discriminative classifier, SVM. The experiments demonstrate the better performance of the language modeling approach. The rest of this paper is structured as follows. Section 2 briefly reviews the related works. Section 3 provides short introductions to SVM and language model. Section 4 describes the model in detail. Section 5 presents the method of estimating model parameters, in which a smoothing technique is utilized. Section 6 shows some experiments to exemplify the availability of the language modeling approach. In section 7, conclusions are given.  2. Related Works  A considerable amount of research has been done about document categorization other than topic-based classification in recent years. For example, Biber [Biber 1988] concentrated on sorting documents in terms of their source or source style with stylistic variation such as author, publisher, and native-language background. Sentiment classification for documents, though, has attracted tremendous attention for its broad applications in various domains such as movie reviews and customer feedback reviews [Gamon 2004; Pang et al. 2002; Pang and Lee 2004; Turney and Littman 2003]. Many research projects have used positive or negative term counting methods, which automatically determine the positive or negative orientation of a term [Turney and Littman 2002]. Other projects have focused on machine learning algorithms, such as Bayesian Classifier and SVMs, to classify entire reviews in a manner similar to a pattern recognition task.  Some related works focus on categorizing the semantic orientation of individual words or phrases by employing linguistic heuristics [Hatzivassiloglou and McKeown 1997; Hatzivassiloglou and Wiebe 2000; Turney and Littman 2002]. The word’s semantic orientation refers to a real number measure of the positive or negative sentiment expressed by a word or a phrase [Hatzivassiloglou and McKeown 1997]. In previous works, the approach taken by Turney [Turney and Littman 2002] is used to derive such values for selected phrases in the document. The semantic orientation of a phrase is determined based on the phrase’s Pointwise Mutual Information (PMI) with the words “excellent” and “poor”. PMI is defined by Church and Hanks [Church and Hanks 1989] as follows:  PMI (w1  & w2 )  =  log2  ⎛ ⎜ ⎝  p(w1 & w2 ) p(w1) p(w2 )  ⎞ ⎟, ⎠  (1)  110  Yi Hu et al.  where p(w1&w2) is the probability that w1 and w2 co-occur. The orientation for a phrase is the difference between its PMI with the word “excellent” and the PMI with the word “poor”. The final orientation is:  SO( phrase) = PMI ( phrase,"excellent ") − PMI ( phrase," poor ") .  (2)  This yields values above zero for phrases having greater PMI with the word “excellent” and below zero for greater PMI with “poor”. An SO value of zero denotes a neutral semantic orientation. This approach is simple but effective. Moreover, it is neither restricted to words of a particular part of speech (e.g. adjectives), nor restricted to a single word, but can be applied to multiple-word phrases. The semantic orientation of phrases can be used to determine the sentiment of complete sentences and reviews. In Turney’s work, 410 reviews were taken and the accuracy of classifying the documents was found when computing the polarity of phrases for different kinds of reviews. Results ranged from 84% for automobile reviews to as low as 66% for movie reviews. Another method of classifying documents into positive and negative is to use a learning algorithm to classify the documents. Several algorithms were compared in [Pang et al. 2002], where it was found that SVMs generally give better results. Unigrams, bigrams, part of speech information, and the position of the terms in the text are used as features, where using only unigrams is found to produce the best results. Pang et al. further analyzed the problem to discover how difficult sentiment analysis is. Their findings indicate that, generally, these algorithms are not able to generate accuracy in the sentiment classification problem in comparison with the standard topic-based categorization. As a method to determine the sentiment of a document, Bayesian belief networks are used to represent a Markov Blanket [Bai 2004], which is a directed acyclic graph where each vertex represents a word and the edges are dependencies between the words. Methods for extracting subjective expressions from collections are presented in [Pang and Lee 2004]. Subjectivity clues include low-frequency words, collocations, and adjectives and verbs identified using distribution similarity. In [Riloff and Wiebe 2003], a bootstrapping process learns linguistically rich extraction patterns for subjective expressions. Classifiers define unlabeled data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. A method to distinguish objective statements from subjective statements is also presented in [Pang and Lee 2004]. This method is based on the assumption that objective and subjective sentences are more possibly to appear in groups. First, each sentence is given a score indicating if the sentence is more likely to be subjective or objective using a Naive Bayes classifier trained on a subjectivity data set. The system then adjusts the subjectivity of a sentence based on how close it is to other subjective or objective sentences.  Using a Generative Model for Sentiment Analysis  111  This method obtains amazing results with up to 86% accuracy on the movie review set. A similar experiment is presented in [Yu and Hatzivassiloglou 2003]. Past works on sentiment-based categorization of entire texts also involve using cognitive linguistics [Hearst 1992; Sack 1994] or manually constructing discriminated lexicons [Das and Chen 2001; Tong 2001]. These works enlighten researchers on the research on learning sentiment models for terms in the given domain. It is worth referring to an interesting study conducted by Koji Eguchi and Victor Lavrenko [Eguchi and Lavrenko 2006]. In their contribution, they do not pay more attention to sentiment classification itself, but propose several sentiment retrieval models in the framework of generative modeling approach for ranking. Their research assumes that the polarity of sentiment interest is specified in the users’ need in some manner, where the topic dependence of the sentiment is considered. 3. SVMs and Language Model  3.1 SVMs Support Vector Machine (SVM) is highly effective on traditional document categorization [Joachims 1998], and its basic idea is to find the hyper-plane that separates two classes of training examples with the largest margin [Burges 1998]. It is expected that the larger the margin, the better the generalization of the classifier. The hyper-plane is in a higher dimensional space called feature space and is mapped from the original space. The mapping is done through kernel functions that allow one to compute inner products in the feature space. The key idea in mapping to a higher space is that, in a sufficiently high dimension, data from two categories can always be separated by a hyper-plane. In order to implement the sentiment classification task, these two categories are designated positive and negative. Accordingly, if d is the vector of a document, then the discriminant function is given by:  f (d) = w⋅φ(d) + b .  (3)  Here, w is the weight vector in feature space that is obtained by the SVM from the training examples. The “·” denotes the inner product and b is a constant. The function φ is the mapping function. The equation w·φ(d) + b = 0 represents the hyper-plane in the higher space. Its value f(d) for a document d is proportional to the perpendicular distance of the document’s augmented feature vector φ(d) from the separating hyper-plane. The SVM is trained such that f(d) ≥ 1 for positive (favorable) examples and f(x) ≤ -1 for negative (unfavorable) examples. Joachim’s SVMlight package [Joachims 1999] was used for training and testing. For more details on SVM, the reader is referred to Cristiani and Shawe-Tailor’s tutorial [Cristianini and  112  Yi Hu et al.  Shawe-Taylor 2000] and Roberto Basili’s paper [Basili 2003].  3.2 Language Models A statistical language model is a probability distribution over all possible word sequences in a language [Rosenfeld 2000]. Generally, the task of language modeling handles the problem: how likely would the ith word occur in a sequence given the history of the preceding i-1 words? In most applications of language modeling, such as speech recognition and information retrieval, the probability of a word sequence is decomposed into a product of n-gram probabilities. Let one assume that L denotes a specified sequence of k words,  L = w1w2...wk .  (4)  An n-gram language model considers the sequence L to be a Markov process with probability  p(L)  =  k ∏  p(wi  |  wii−−1n +1 )  .  (5)  i=1  When n is 1, it is a unigram language model which uses only estimates of the probabilities of individual words, and when n is equal to 2, it is the bigram model which is estimated using information about the co-occurrence of pairs of words. On the other hand, the value of n-1 is also called the order of the Markov process. To establish the n-gram language model, probability estimates are typically derived from frequencies of n-gram patterns in the training data. It is common that many possible n-gram patterns would not appear in the actual data used for estimation, even if the size of the data is huge. As a consequence, for a rare or unseen n-gram, the likelihood estimates that are directly based on counts may become problematic. This is often referred to as data sparseness. Smoothing is used to address this problem and has been an important part of various language models.  4. A Generative Model for Sentiment Classification  In this section, a language modeling approach to detect semantic orientation of document is proposed. This approach is very simple: one must observe the usage of language in contexts of terms appearing in positive and negative documents. “Favorable” and “unfavorable” language models are likely to be substantially different: they are prone to different language habits. This divergence in the language models is exploited to effectively classify a test document as positive or negative.  Using a Generative Model for Sentiment Analysis  113  4.1 Two Assumptions Models usually have their own basic assumptions as foundation of reasoning and calculating, which support their further applications. The researchers also propose two assumptions in this study, and, based on them, employ a language modeling approach to deal with the sentiment classification problem. As mentioned above, ordinary words in a sentence might have correlation with the term in the same sentence. Therefore, this method follows the idea of learning positive and negative language models for each term within sentences. After this, the sentiment classification is transferred into calculating the generation probability of all subjective sentences in a test document by these sentiment models. The following two assumptions are presented: A1. A subjective sentence contains at least one sentiment term and is assumed to have obvious semantic orientation. A2. A subjective sentence is the processing unit for sentiment analysis. The first assumption (A1) gives the definition of subjective sentence, and it means a significant sentence for training or testing should contain at least one term. In contrast, a sentence without any term is regarded as an objective sentence because of its “no contribution” to sentiment. It also assumes that a subjective sentence has complete sentiment information to characterize its own orientation. The second assumption (A2) allows one to handle the classification problem of sentence-level processing. Therefore, the authors pay more attention to construct models within the given sentence in terms of this assumption. A2 is an intuitive idea in many cases. Previous work has rarely integrated sentence-level subjectivity detection with document-level sentiment polarity. Yu and Hatzivassiloglou [Yu and Hatzivassiloglou 2003] provide methods for sentence-level analysis and for determining whether a sentence is subjective or not, but do not consider document polarity classification. The motivation behind the single sentence selection method of Beineke et al. [Beineke et al. 2004] is to reveal a document's sentiment polarity, but they do not evaluate the polarity-classification accuracy of results.  4.2 Document Representation  Based on these two assumptions, a document d is naturally reorganized into subjective sentences, and the objective sentences are omitted from d. That is to say, the original d is reduced to:  d  {s | ∃t ∈ s} .  (6)  114  Yi Hu et al.  Furthermore, a subjective sentence can be traditionally represented by a Chinese word sequence as follows,  w1 w2... wl−1 ti,l wl+1 ...wl+2 wn .  (7)  In this, “ti,l ” indicates one term ti appears in the sentence si, which is usually denoted as the serial number ‘l’ in the sequence. Moreover, the subsequence from w1 to wl-1 is the group of ordinary words on the left side of ti, and the subsequence from wl+1 to wn is the group of ordinary words on the right. In (7), ordinary words in this sentence consist of ti’s context (Cxi). So, a subjective sentence si is simplified to:  si < ti ,Cxi > .  (8)  The authors now focus on a special form, by which a document is represented. Let d be defined again,  d  {< ti ,Cxi >} .  (9)  Definition (9) means that there also exists an independent assumption between sentences and every word has certain correlation with the term within a sentence. Each sentence has semantic orientation and makes a contribution to the global polarity.  Note that it is possible for there to exist more than one term in a sentence. However, when investigating one of them, the others are to be treated as ordinary words. Each term can create a <t, Cx> structure. That is to say, one sentence may create more than one such structure.  4.3 Sentiment Models of Term  With respect to each term, each plays an important role in sentiment classification because the pivotal point of this work lies in learning and evaluating its context. This kind of classifying knowledge, derived from the contexts of terms in two subject-sentence collections labeled positive or negative in different contexts, would like to use words with polarity, such as “快 (Fast)” and “慢 (Slow)”. A formalized depiction of classifying knowledge is shown as the following 3-tuple ki:  ki < ti ,θiP ,θiN > ti ∈T .  (10)  The character “T” denotes the list of all terms obtained from collections. With respect to ti, its 
Recently many new techniques have been proposed for language modeling, such as ME, MEMM and CRF. However, the ngram model is still a staple in practical applications. It is well worthy of studying how to improve the performance of the ngram model. This paper enhances the traditional ngram model by relaxing the stationary hypothesis on the Markov chain and exploiting the word positional information. Such an assumption is made that the probability of the current word is determined not only by history words but also by the words positions in the sentence. The non-stationary ngram model (NS ngram model) is proposed. Several related issues are discussed in detail, including the definition of the NS ngram model, the representation of the word positional information and the estimation of the conditional probability. In addition, three smoothing approaches are proposed to solve the data sparseness problem of the NS ngram model. Several smoothing algorithms are presented in each approach. In the experiments, the NS ngram model is evaluated on the pinyin-to-character conversion task which is the core technique of the Chinese text input method. Experimental results show that the NS ngram model outperforms the traditional ngram model significantly by the exploitation of the word positional information. In addition, the proposed smoothing techniques solve the data sparseness problem of the NS ngram model effectively with great error rate reduction. Keywords: Ngram, Stationary Hypothesis, Pinyin-to-character Conversion, Smoothing 1. Introduction Statistical language model plays an important role in natural language processing. It has a wide range of applications in many domains, such as speech recognition [Jelinek 1997], OCR [Kolak et al. 2003], machine translation [Brown et al. 1992], and pinyin-to-character * School of Computer Science and Techniques, Harbin Institute of Technology, Harbin, 150001, China E-mail: {xiaojinghui, liubq, wangxl}@insun.hit.edu.cn [Received November 28, 2006; Revised April 19, 2007; Accepted April 23, 2007]  128  Jinghui Xiao et al.  conversion [Gao et al. 2005; Xiao et al. 2005] etc. In recent years, great efforts are devoted to the research of language modeling. Many novel techniques are proposed, such as maximum entropy model [Rosenfeld 1994], maximum entropy Markov model [McCallum et al. 2000] and conditional random field model [Lafferty et al. 2001]. However, the ngram model is still a staple in practical applications. Therefore, it is well worthy of studying how to improve the performance of the ngram model. The ngram model takes the word sequence as a Markov chain. It makes the Markov hypothesis on the sequence so as to simplify the probability inference. There are actually two hypotheses implied by the Markov hypothesis, named the limited history hypothesis and the stationary hypothesis [Manning and Schutze 1999]. The first one assumes that the probability of the current word is determined only by a few of previous words, but irrelevant to the whole history of words. The second one assumes that the word probability is irrelevant to the actual word positions in the sentence. The most obvious extension to the traditional ngram model is simply to enlarge the number of history words and build up the higher-order ngram model [Carpenter 2005]. However, the high-order ngram model suffers from the curse of dimensionality [Novak and Ritter 1998]. The bigram model and the trigram model are currently two prevalent language models. From another point of view, the paper relaxes the stationary hypothesis and enhances the traditional ngram model by exploiting the word positional information. It is based on the philosophy that most words are not only constrained by their contextual information, but also influenced by their positions in the sentence. For example, the Chinese word “首先” (first of all) is usually used to start a sentence, but rarely occurs elsewhere in the sentence. Then higher probability should be assigned to it by a language model when it is in the front of a sentence, and lower probability elsewhere. Moreover, some of punctuations, such as full stop and exclamation, always appear at the end of a sentence. So it may be mistaken for a Chinese sentence that the exclamation appears in the middle of it. Therefore, a language model can benefit from modeling the word positional information. This paper enhances the traditional ngram model by the exploitation of the word positional information. The non-stationary ngram model (NS ngram model) is proposed. Several related issues are discussed in detail, including the definition of the NS ngram model, the representation of the word positional information and the estimation of the conditional probability. In addition, three smoothing approaches are proposed to solve the data sparseness problem of the NS ngram model. The NS ngram model is evaluated on the pinyin-to-character conversion task which is the core technique of the Chinese text input method. Experimental results show that the NS ngram model outperforms the traditional ngram model significantly and the smoothing techniques proposed in this paper solve the data sparseness problem of the  An Empirical Study of Non-Stationary Ngram Model and its Smoothing Techniques 129 NS ngram model effectively with great error rate reduction. The remaining part of the paper is organized as follows. The related works are outlined in section 2. In section 3, the NS ngram model is proposed and several related issues are discussed in detail. In section 4, the data sparseness problem of the NS ngram problem is addressed and three smoothing approaches are proposed. The experimental results and discussions are presented in section 5 and the conclusion is drawn in section 6. 2. Related Works There are many ways to improve the performance of the ngram model. The most obvious way is to relax the limited history hypothesis and build up the high-order ngram model, which has been discussed in the above section. Another way is to construct the skipping ngram model [Rosenfeld 1994; Ney et al. 1994], in which the current word is constrained by the skipped words in the word history, other than the adjacent words. The skipping ngram model can exploit more information of history words and avoid the curse of dimensionality meanwhile. In the experiments, it yields limited improvements by interpolating with the traditional ngram model. The class-based ngram model [Brown et al. 1992] is constructed based on word cluster instead of word. The syntax and semantic information can be well captured in this way. Meanwhile, the parameter space is reduced greatly and the data sparseness problem is alleviated. However, the predictive capability of the class-based ngram model is much lower than the traditional ngram model due to its small parameter space. It usually achieves limited improvements by interpolating with the traditional ngram model. The cache-based ngram model [Kuhn 1988; Kuhn and Mori 1990] assumes that people tends to use words as few as possible in the article. If a word has been used, it would possibly be used again in the future. The cache-based ngram model is usually utilized to construct a self-adaptive language model. 3. Non-Stationary Ngram Model This section firstly reviews the traditional ngram model briefly. Secondly, it defines the NS ngram model formally. Thirdly, the word positional information is formalized. Finally, the estimation method is provided for the conditional probability of the NS ngram model. 3.1 Ngram Model Language model aims to determine the probability of the sequence of words. The sequence probability is usually decomposed into the conditional probabilities of words which are composed of sequences. For the sequence of s = wl1, p1 wl2 , p2 ...wlm , pm , its probability is  130  Jinghui Xiao et al.  calculated in formula (1):  m  P(s)  =  Õ i=1  p(wli  ,  pi  | wl1, p1 , wl2 , p2 ...wli-1, pi-1 ) ,  (1)  where wli , pj is the ith word in the lexicon and appears at the jth position in sequence S. The ngram model makes the Markov hypothesis on the sequence so as to simplify formula (1). The procedures are described in formula (2):  m  m  P(s)  »  Õ i=1  p(wli  ,  pi  | wli-n+1, pi-n+1 ...wli-1, pi-1 )  »  Õ i=1  p  (  wli  | wli-n+1 ...wli-1 )  .  (2)  Actually, there are two hypotheses implied by the Markov hypothesis: 1. The limited history hypothesis: the probability of current word is dependent only on the previous n-1 words, but irrelevant to the whole history of words.  2. The stationary hypothesis: the word transition probability is determined only by the words which consist of the transition probability, but irrelevant to the positions where these words possess in the sequence.  Formula (1) is firstly simplified by the limited history hypothesis, resulted in the second item of formula (2). Then, the stationary hypothesis is applied on it and the final form of the ngram model is obtained, as represented by the last item of formula (2). The paper substitutes wli for wli , pj since the conditional probability is irrelevant to word position. In literature, the limited history hypothesis is referred to frequently, but seldom is the stationary hypothesis.  The most obvious way to extend the ngram model is simply to relax the limited history hypothesis and involve more history information of words. The higher-order ngram model is built up. However, the high-order ngram model suffers from the curse of dimensionality. As the model order increases, the parameter space explodes at an exponential rate. The data sparseness problem becomes very severe which hampers its applications gravely. From another point of view, the paper relaxes the stationary hypothesis and enhances the ngram model by the exploitation of the word positional information. The NS ngram model is proposed. It is described in the following sections.  3.2 NS Ngram Model As presented in section 1, the occurrence of words is relevant to their positional information in sentence. It is beneficial for the language model to exploit the positional information to determine the word probability. However, the Markov hypothesis is too restricted to exploit the positional information due to its stationary assumption. The paper relaxes the stationary hypothesis of the traditional ngram model and proposes a non-stationary ngram model. The NS ngram model is formulized in as below:  An Empirical Study of Non-Stationary Ngram Model and its Smoothing Techniques 131  m  m  P(s)  »  Õ i=1  p(wli  ,  pi  | wli-n+1, pi-n+1 ...wli-1, pi-1 )  =  Õ i=1  p  (  wli  | wli-n+1 ...wli-1 , t) .  (3)  In the NS ngram model, formula (1) is simplified merely by the limited history hypothesis, rather than the stationary hypothesis. The conditional probability of the current word is determined not only by history words but also by the words’ positions in sentence. The paper uses a single positional variable of t to denote the word positional information in formula (3). The traditional ngram model is a special case of the NS ngram model in which t is a constant. Important things for the NS ngram model are how to calculate the value of t and how to estimate the conditional probability of word in formula (3).  3.3 Representation of t Since t denotes the word positional information in a sentence, it is a natural way to take the word position index as the concrete value of t. However, there are two serious problems with this method. Firstly, index has different meanings in sentences of different lengths. For example, there are two English sentences: “Yesterday I saw you” and “Yesterday I saw you were looking around here”. In both of the sentences, the word “you” has the same position index - 4. However, “you” appears at the end of the first sentence, while it is in the middle in the second. It possesses completely different positional information in these two sentences. Secondly, since a sentence may have arbitrary length, the t value can be any natural number. But computer can not deal with infinite value. A refined method is to use the ratio of the word position index to the sentence length, which maps t into a real number in the range of [0, 1]. But there are infinite real numbers in that range and it can not make statistics based on each real number. This paper divides the above range into several equivalent classes (bins). It assumes that the words in each bin share the same positional information. The value of t is set to the index of the according class. More formally, the above procedures are described as below: 1. Calculate the ratio of the word position index to the sentence’s length, which maps t into the range of [0, 1]. 2. Divide the range into several bins. The words in each bin share the same positional information. 3. Set the t value of current word as the index of the according bin. Figure 1 shows an example of the above procedures:  132  Jinghui Xiao et al.  Figure 1. Calculation of the t value in NS ngram model From the above procedures, the more number of bins it divides of the word sequence, the more accuracy of the positional information is extracted from the sentence.  3.4 Training Method  The section discusses how to estimate the conditional probability in formula (3), which is the training problem of the NS ngram model. Based on the representation of t in section 3.3, the sentences in the training corpus are divided into the same number of bins. The words in each bin share the same value of t. The paper builds up a specific ngram model for each value of t within each bin. All these specific ngram models constitute of the NS ngram model. Using k to denote the number of bins, there are totally k specific ngram models in the NS ngram model with k bins. The conditional probability of p(wli | wli-n+1 ...wli-1 , t) is estimated under the Maximum Likelihood Estimation (MLE) principle:  p(wli  | wli-n+1 ...wli-1 , t)  =  C(wli-n+1 ...wli , t) C(wli-n+1 ...wli-1 , t)  .  (4)  C(wli-n+1 ...wli ,t) is the occurrence times that the word sequence wli-n+1 ...wli falls in the tth bin of the sentences in the training corpus. It is similar to interpreting C(wli-n+1 ...wli-1 ,t) . In order to calculate the probability of a sentence, the t value is firstly obtained for each word. Then, the conditional probability of word is computed according to formula (4). Finally, the sentence probability is calculated by formula (3). The traditional ngram model is a special case of the NS ngram model in which there is only one bin.  4. Smoothing Techniques  As shown in section 3.4, there are totally k traditional ngram models in the NS ngram model with k bins. The space complexity of the NS ngram model is consequently k times more than  An Empirical Study of Non-Stationary Ngram Model and its Smoothing Techniques 133 the traditional ngram model. Data sparseness problem is an inherent and severe problem in the traditional ngram model [Brown et al. 1992]. Therefore, it is more severe in the NS ngram model. Figure 2 illustrates the data sparseness problem in the NS ngram model. Figure 2. Data sparseness problem in NS ngram model In Figure 2, the color of deep shade indicates that the data sparseness problem is severe in the NS ngram model, while the color of light shade means that the problem is not severe. As shown in Figure 2, there are two main factors in determining the degree of the data sparseness problem in the NS ngram model. They are the model order n and the bin number k. As n (or k) increases, the problem becomes more severe, and the estimated probability becomes more unreliable. It is necessary to start with these two factors to solve the data sparseness problem of the NS ngram model. Considering the factor of the model order which is represented as the vertical axis in Figure 2, the high-order NS ngram model can be smoothed by lower-order NS ngram model, just as the traditional smoothing techniques do. It is our first smoothing approach. Considering the factor of the bin number which is shown as the horizontal axis, there are two ways to design the smoothing methods. The first way, the NS ngram model with larger value of k can be smoothed by the NS ngram model with smaller value of k. In particular, the traditional ngram model ( k=1) can be utilized to smooth the NS ngram model (k>1). It is our second smoothing approach. The second way, the paper builds up a more compact form of the NS ngram model. It firstly constructs some statistical variables of the word positional information from the bins of the NS ngram model. Then, it calculates a weight from these variables for the traditional ngram probability. The weight is used to substitute for the concrete positional information which tends to cause the data sparseness problem in the NS ngram model. It is our third approach to smooth the NS ngram model. Until now, three smoothing approaches have been provided in sketch. They will be described in the following  134  Jinghui Xiao et al.  sections in detail.  4.1 The First Approach  Since the NS ngram model is composed of several traditional ngram models, each of these component ngram models can be smoothed separately by the traditional smoothing techniques. The traditional smoothing techniques have been well studied before. Many smoothing algorithms have been proposed, such as the additive smoothing [Jeffreys 1948], the Good-Turing smoothing [Good 1953], the back-off smoothing [Katz 1987], the linear interpolation smoothing [Jelinek and Mercer 1980], the Kneser-Ney smoothing [Kneser and Ney 1995], and so on. Generally, they smooth the unreliable probabilities in the high-order ngram model by the reliable probabilities in the low-order ngram model. The paper can not try each existent smoothing algorithm on the NS ngram model. Three popular algorithms are taken in the paper. They are the additive smoothing, the back-off smoothing and the linear interpolation smoothing. The NS bigram model is taken as an example and the formulas are listed as below.  Additive smoothing:  ~ P(wli  | wli-1 , t)  =  C(wli-1 , wli , t) +1 C(wli-1 , t) + l  (5)  ~ t is the positional variable which is defined in section 3.3; l is the lexicon size; and p is the  smoothed probability of the NS bigram model.  Back-off smoothing:  ~ P(wli  | wli-1 , t)  =  ìïPGT (wli | wli-1 , t) if  í  ~  ïî a (wli-1 , t) P(wli , t)  C(wli-1 , wli , t) > 0 otherwise  (6)  PGT is the probability of the NS bigram model which is smoothed by the Good-Turing method. It is formalized as below:  PGT (wli  | wli-1 , t)  =  CGT (wli-1 , wli , t) C(wli-1 , t)  (7)  and  CGT  (wli-1  ,  wli  ,t)  =  (C ( wli-1  ,  wli  ,  t)  + 1)  ´  E(C(wli-1 , wli , t) +1) E(C(wli-1 , wli , t))  (8)  E(C) is the expectation of the number of the bigram items which occurs C times in the corpus. In reality, N(C) is usually substituted for E(C). N(C) is the concrete number of the bigram  An Empirical Study of Non-Stationary Ngram Model and its Smoothing Techniques 135  items which actually occurs C times in the training corpus. Formula (8) is reformulated as below:  CGT  (wli-1  ,  wli  ,t)  =  (C ( wli-1  ,  wli  ,t)  +1) ´  N (C(wli-1 , wli , t) +1) N (C(wli-1 , wli , t))  (9)  However, N(C) can not be estimated reliably for some large values of C. At this time, formula (9) can not work properly and problems occur in the Good-Turing method. In particular, when C reaches its max value in the training corpus, CGT (wli-1 , wli ,t) is calculated to be zero according to formula (9) because N(C+1) is equal to zero. It is obviously wrong. In this paper, a simple strategy is adopted to address the problem. Formula (7) and formula (9) are adopted only for the small value of C (i.e. below a threshold). For the large value of C, it is regarded that the bigram probabilities can be estimated reliably according to the word frequencies and they need not to be smoothed. The MLE principle is applied on them directly.  In formula (6), α is the coefficient for normalization and it is calculated as below:  a (wli-1 , t) =  b (wli-1 , t) ~  =  b (wli-1 , t) ~  wli  :C  å ( wli-1 wli  ,t )=0  P(wli  ,  t)  
Web catalog integration has become an integral aspect of current digital content management for Internet and e-commerce environments. The Web catalog integration problem concerns integration of documents in a source catalog into a destination catalog. Many investigations have focused on flattened (one-dimensional) catalogs, but few works address hierarchical Web catalog integration. This study presents a hierarchical catalog integration (EHCI) approach based on the conceptual thesauri extracted from the source catalog and the destination catalog to improve performance. Experiments involving real-world catalog integration are performed to measure the performance of the improved hierarchical catalog integration scheme. Experimental results demonstrate that the EHCI approach consistently improves the average accuracy performance of each hierarchical category. Keywords: Hierarchical catalog integration, conceptual relationships, thesaurus, Support Vector Machines (SVMs) 1. Introduction Automatically integrating various information sources is pertinent for many real applications given the large, and still rapidly growing, amount of information available. For instance, an on-line service provider may merge various catalogs from other on-line vendors into its local catalog to provide customers with versatile content, and a Web portal may also have to integrate different Web catalogs from other portals to provide increasingly abundant information services to users [Agrawal and Srikant 2001]. In these examples, users can gain more relevant and organized information in an integrated catalog. They can also save considerable time, because they do not need to browse different Web catalogs. According to * Dept. of Computer Sci. and Eng., Yuan Ze University, 135 Yuan-Tung Rd., Chungli, 320, Taiwan. Tel.: +886-3-4638800 ext: 2361 Fax: +886-3-4638850. The author for correspondence is Cheng-Zen Yang. E-mail: czyang@syslab.cse.yzu.edu.tw [Received March 18, 2007; Revised June 23, 2007; Accepted July 2, 2007]  156  Ing-Xiang Chen et al.  previous studies [Keller 1997; Stonebraker and Hellerstein 2001; Kim et al. 2002; Marrón et al. 2003], Web catalog integration has attracted much research interest. Web catalog integration is not just a straightforward classification task [Agrawal and Srikant 2001]. Exploring implicit source information can effectively improve the integration accuracy [Agrawal and Srikant 2001]. Many methods for enhancing catalog integration performance have been proposed so far. The most important approach, called ENB, enhances the Naive Bayes classifiers with implicit source information. Other state-of-the-art approaches, including Support Vector Machines (SVMs) [Sarawagi et al. 2003; Tsay et al. 2003; Zhang and Lee 2004a; Chen et al. 2005; Chen et al. 2006; Ho et al. 2006] and the Maximum Entropy model [Wu et al. 2005], have been also presented to elevate the performance of Web catalog integration, and they further outperform the ENB approach. Past studies in text classification [MacCallum et al. 1998; Dumais and Chen 2000] have indicated that exploiting a hierarchical structure can bring strong advantages over using a flattened structure in classification. [MacCallum et al. 1998] presented a probabilistic framework, and a shrinkage approach was proposed to improve text classification in a hierarchy of classes. Experimental results indicate that hierarchical text classification with large numbers of features (feature set > 10000) can obtain better average accuracy performance than flattened text classification. However, the shrinkage approach may either have no effect or hurt slightly in some classes with a large amount of training data [MacCallum et al. 1998]. Previous hierarchical data integration studies [Doan et al. 2002; Rajan et al. 2005] examined the hierarchical structures of the destination catalog are studied to improve the accuracy of catalog integration. [Doan et al. 2002] extracted the domain constraint features obtained from the neighboring nodes to enhance the mapping of ontological data. [Rajan et al. 2005] developed a maximum likelihood-based framework that exploits the hierarchical structure of categories, and examined four mapping scenarios. Experimental results have demonstrated that hierarchical relationships in the destination catalog are effective in catalog integration. Some source class labels can further be integrated into the destination catalog as new classes to maintain a new hierarchy. However, hierarchical relationships of the categories and subcategories between the source and destination catalogs have not been investigated in the previous work. Moreover, experimental results indicate that the previously proposed approaches only integrate the data into the leaf nodes of the destination catalog. Although past methods for conventional text classification and hierarchical catalog integration can benefit from using a hierarchical structure, they only address the hierarchical structure in the destination categories and do not consider the differing hierarchical structures in the source and destination catalogs. Hence, this work performs some pilot studies for the hierarchical catalog integration problem by  Hierarchical Web Catalog Integration with  157  Conceptual Relationships in a Thesaurua  considering the implicit information embedded in the hierarchical structure of both the source and destination catalogs. The pilot experimental results reported in Chen et al. [2006] indicate that the implicit hierarchical information does indeed contribute to the hierarchical Web catalog integration problem. While extending the results of our previous pilot study, this work presents an enhanced hierarchical catalog integration (EHCI) approach with conceptual relationships extracted from the source and destination catalog thesauri to improve the integration performance. An EHCI approach based on SVM was adopted in these experiments due to its good classification performance. To demonstrate the effectiveness of EHCI, its performance is compared with that of a simple hierarchical catalog integration approach (SHCI) based on previous hierarchical classification studies [Dumais and Chen 2000; Sun and Lim 2001; Sun et al. 2003; Vural and Dy 2004]. Results of experiments with real-world catalogs reveal that the EHCI approach consistently raises the accuracy of hierarchical Web catalog integration in almost all hierarchical levels in both Yahoo!-to-Google and Google-to-Yahoo! catalog integration. These results also demonstrate that EHCI attains an average accuracy improvement of 11.1% in Yahoo!-to-Google catalog integration, and 21.6% in Google-to-Yahoo! catalog integration. The results further indicate that hierarchical catalog integration can be effectively improved by enhancing the conceptual relationships discovered from the hierarchical thesauri. The remainder of this paper is organized as follows. Section 2 reviews the related studies of catalog integration. Section 3 then describes in detail the hierarchical Web catalog integration and the enhanced hierarchical integration approach. Next, Section 4 shows the environmental settings and discusses the experimental results. Finally, conclusions are drawn in Section 5, along with recommendations for future research. 2. Related Work Most methods proposed for solving the catalog integration problem have been based on a flattened structure, implying that the categories in a catalog are isolated and lack hierarchical relationships. Agrawal and Srikant were the first to study this problem in 2001, and presented an enhanced Naive Bayes approach (ENB) to improve the integration accuracy by exploiting implicit information from the source catalog [Agrawal and Srikant 2001]. Experimental results involving real-world catalogs indicate that ENB can achieve an average accuracy improvement of more than 14%. Their promising results reveal that exploiting implicit source information indeed benefits the accuracy for automated catalog integration. Several algorithms have been proposed in the past few years to increase the accuracy of catalog integration based on a flattened structure. Since SVM has presented superior  158  Ing-Xiang Chen et al.  performance in classification problems [Dumais et al. 1998; Joachims 1998; Yang and Liu 1999; Rennie and Rifkin 2001], many related studies have also adopted the SVM classifiers with different strategies to extract the implicit information and improve the integration accuracy. These SVM-based integration approaches include a cross-training technique for SVM classifiers (SVM-CT) [Sarawagi et al. 2003], a topic restriction strategy (SVM-TR) [Tsay et al. 2003], a cluster shrinkage approach (CS-TSVM) [Zhang and Lee 2004a], and an iterative approach with pseudo-relevance feedback (SVM-IA) [Chen et al. 2005]. Most of these approaches employing the SVM classifiers were found to have higher accuracy then ENB. In addition to the SVM-based approaches, some state-of-the-art investigations have also been presented to enhance the catalog integration accuracy with a flattened structure. Zhang and Lee proposed a co-bootstrapping approach with boosting to obtain the optimal combination of heterogeneous weak hypotheses without adjusting feature weights manually [Zhang and Lee 2004b]. Wu et al. first extracted the source hierarchical information and then applied the Maximum Entropy model to increase the accuracy of catalog integration in a flattened structure. Their experimental results showed that their approach is more accurate than ENB. Most previous catalog integration studies adopted a flattened structure to simplify the catalog integration problem, thus neglecting the hierarchical relationships among the categories. Since previous studies on text classification problems have reported that a hierarchical structure can improve performance, an approach called shrinkage was presented to further improve the Bayesian classifiers in hierarchical text classification [MacCallum et al. 1998]. With the shrinkage-based approach, the parameter estimation of a node is smoothed by interpolation from the parent nodes, thus significantly reducing the number of prediction errors in hierarchical text classification. Experimental results indicate that the accuracy performance of the method of MacCallum et al. [1998] can be raised by shrinking each leaf node with linear interpolation of the parent nodes in the destination hierarchy. However, the classification is based on the same hierarchy, instead of considering both the source and the destination hierarchies, respectively. Therefore, the original algorithm may need to be modified for application to hierarchal Web catalog integration. Rajan et al. [2005] presented a two-stage mapping and integration approach, and discussed four integration scenarios. They comprehensively investigated their hierarchical catalog integration scheme using a maximum likelihood approach, and found that its integration performance is very promising, particularly in one-to-many mapping (Scenario 3). Rajan et al. further demonstrated that the hierarchical structure of the destination catalog is helpful in improving integration accuracy in different data sets. However, the implicit  Hierarchical Web Catalog Integration with  159  Conceptual Relationships in a Thesaurua  information in the source hierarchy has not been utilized in this work. The hierarchical relationships between the source catalog and the destination catalog requires further investigation when considering hierarchical Web catalog integration. Chen et al. preliminarily explored the effectiveness of a hierarchical catalog integration scheme with the consideration of both the source catalog and the destination catalog [Chen et al. 2006]. Their experimental results indicated a consistent improvement in accuracy of real-world Web catalog integration over the EHCI approach. Although the performance improvements are significant, the integration effectiveness based on a hierarchical structure has not been comprehensively studied. The following sections first define the problem, and then describe the ECHI approach in detail. 3. Hierarchical Web Catalog Integration The integration process of the hierarchical catalog integration problem involves two hierarchical catalogs. Figure 1 illustrates the integration process in which the source catalog S with a set of m categories S1, S2,…, Sm, is integrated into the destination catalog D with a set of n categories D1, D2,…, Dn. These categories may have subcategories, such as S11, D11 and D121. The integration process in Figure 1 is performed by merging each document di in S into a correspondent destination category in D. Thus, for each directory in the hierarchy, the training documents trained as directory classifiers and local classifiers are utilized to help integrate each document di into a corresponding directory. Only the documents integrated into the corresponding level categories and subcategories are regarded as correctly integrated.  Figure 1. The process of hierarchical catalog integration.  160  Ing-Xiang Chen et al.  This study adopts SVM classifiers with linear kernel functions [Yang and Liu 1999], f : X Î Rn ® R to locate a hyperplane that can separate the positive examples, f (x) ³ +1 , from the negative examples, f (x) £ -1 . The linear function is in the form f (x) = (x,b) + b = åin=1wixi + b where (w,b) Î Rn ® R . The linear SVM is trained to determine the optimal values of w and b such that ||w|| is minimized. These trained SVM classifiers are employed in the simple hierarchical catalog integration (SHCI) approach and the enhanced hierarchical catalog integration (EHCI) approach in hierarchical catalog integration. The SHCI approach and the EHCI scheme are described as follows. 3.1 The Simple Hierarchical Catalog Integration (SHCI) Approach In SHCI, the SVM classifiers are trained with the training documents coming from the destination catalog and are used to integrate the test documents from the source catalog into the destination catalog. Whether a training document is considered a positive document or a negative document depends on its subordinate relationship to each destination category. Referring to Sun and Lim [2001] and Sun et al. [2003], the destination catalog was designed with two classifiers at every category node, namely a directory classifier and a local classifier. The directory classifiers were designed to categorize the source documents into different category and subcategory trees. The directory classifiers are trained with equal numbers of positive and negative examples. The positive examples were chosen from the categories and their subcategories where the documents were located. The negative examples were selected from the remaining categories and their subcategories under the same level. The local classifiers were designed to classify the source documents further into different destination levels in each category tree. The local classifiers in each level were trained with the positive examples chosen from each destination level, and the negative examples selected from the subcategories under that level. In real-world Web catalogs, a document may be integrated into more than one category. Therefore, a “one-against-rest” strategy was adopted to extend the binary SVM classifiers and solve the multi-class catalog integration problem. This study uses the SHCI approach as a baseline for hierarchical catalog integration, and considers the performance improvement of the SVM classifiers resulting from the enhancement of conceptual relationships in thesauri. 3.2 Conceptual Relationships in Web Thesaurus Foskett utilized a thesaurus as a dictionary and a reference for classification [Foskett 1997]. A thesaurus can be defined as a set of related terms in a given domain knowledge, and these related terms are the basic semantic units for conveying concepts [Wikipedia: thesaurus]. Since a hierarchical thesaurus defines broad and narrow terms, its classification system can be considered a vocabulary hierarchy. Likewise, the child nodes in a hierarchical Web catalog  Hierarchical Web Catalog Integration with  161  Conceptual Relationships in a Thesaurua  structure generally comprise related terms to express the classified concepts of the parent nodes, and so the classified terms in a hierarchical Web catalog can be treated as a hierarchical thesaurus. Figure 2 shows an example in which the “Automotive” category in Yahoo! Web catalog is categorized like a hierarchical thesaurus with some conceptual relationships in the hierarchy.  Figure 2. The illustration of a Web thesaurus in Yahoo! catalog In Figure 2, the term “Automotive” is the thesaurus root, which expresses a broad term in the hierarchy, and has different narrow terms to define different types of “Automotive”. Narrower terms are defined down to the leaf nodes in the hierarchy. In the Web catalog hierarchy, the conceptual relationships can be extracted from the hierarchical thesaurus and can construct different semantic concepts. Therefore, different domain knowledge can be extracted from the Web catalog hierarchy, thus enhancing the performance of the SVM classifiers. 3.3 Enhanced Hierarchical Catalog Integration (EHCI) Scheme To elevate the integration performance, a weighting formula, Equation (1), is designed to exploit the conceptual relationships from the hierarchical Web thesaurus, where the terms in different category levels are extracted as label features. Equation (1) calculates the feature weight of each document, FeatureWeight(x, d), where Li denotes the relevant label weight assigned exponentially as 1/2i, fx represents the occurrence ratio of feature x in the document, and λ indicates the magnitude relation of the label weight. In Equation (1), the weight of each thesaurus is exponentially decreased and accumulated based on the increased levels, where n denotes the depth of a document in the hierarchy. If feature x appears in the label feature, then Lx is denoted as the label weight with the level where x is located. Otherwise, Lx=0. Consequently, Equation (1) is applied to both the source and destination hierarchies to represent the semantic concepts obtained from the source category labels and the destination category labels.  162  Ing-Xiang Chen et al.  FeatureWeight( x,  d)  =  l´  Lx åin=0 Li  + (1- l) ´  fx  (1)  Table 1. The label weights assigned for different hierarchical levels  Hierarchical Level Document Level (L0) One Level Upper (L1) Two Levels Upper (L2) … n Levels Upper (Ln)  Label Weight 1/20 1/21 1/22 … 1/2n  Figure 3. The process of the enhanced hierarchical catalog integration In Equation (1), Li further denotes the label weight at a depth of i. The label weight falls from the document level (i=0) to top level n. This thesaurus weighting method can be utilized to transform the conceptual relationships of the hierarchical source categories, and add them into the test documents. Table 1 lists the weights of different hierarchical labels, where L0 denotes the document level; L1 represents one level above, and so on down to Ln representing n levels above. Similarly, the EHCI scheme is used in the destination catalog to build enhanced classifiers in destination categories. With the enhancement of the features and native category label information, the classifiers can thus be trained to be more distinctive to classify the documents into the correct categories. The weights of the features and native category label information in the destination catalog are also calculated according to Equation (1). The threshold l is set with different values from 0 to 1 to find the optimized weights for the source thesauri to enhance the destination classifiers, as are the values of l set in the native destination category. Moreover, the features occurring in the upper categories are removed to avoid misleading integration in the subcategories.  Hierarchical Web Catalog Integration with  163  Conceptual Relationships in a Thesaurua  Figure 3 displays a three-level example to demonstrate the concept of the EHCI approach. In the source catalog, the hierarchical thesaurus information is added to the test documents with different label weights accumulated upward from their current categories to the top-level category according to the weighting formula. In the destination catalog, the test documents are integrated into the destination categories based on the EHCI integration scheme. Figure 3 also indicates that a document d1 may be integrated into more than one destination category.  3.4 Enhanced Catalog Integration Process  Since a Web document generally comprises HTML tags, script codes and texts, the HTML tags and scripts codes are eliminated, and only the texts obtained after retrieving the Web documents from both the source and destination catalogs are kept. In the preprocessing stage, the texts are segmented into terms by removing the stopwords and stemming the terms with the Porter Stemmer [Porter 1980]. The weight of each stemmed term x is assigned by TFx åTFi , where i denotes the number of the stemmed terms in each document. This preprocessing flow and feature weight strategy is applied to both the SHCI and EHCI schemes.  The documents in the destination catalog hierarchy  The source documents in the source catalog  Extracting the Web thesaurus from the destination catalog Removing the common features from the parent categories  Training the directory classifiers Training the local classifiers  Directory classifiers Local classifiers  Extracting the Web thesaurus from the source catalog Adding the source labels into the test documents Integrating the test documents into the destination layers  Figure 4. The process of enhanced hierarchical catalog integration  Figure 4 shows the process of hierarchical catalog integration with the EHCI scheme. In the integration process, the terms transformed from the test documents are added with the source catalog labels based on Equation (1). Similarly, the terms transformed from the documents in the destination categories are trained using the labels extracted from the destination catalog. To establish the directory classifiers and local classifiers in the destination catalog, the common features in the parent categories are removed in the training stage to avoid building ambiguous classifiers.  164  Ing-Xiang Chen et al.  In Figure 4, the directory classifiers are trained with the positive documents from their categories and subcategories to represent the classifiers of the category trees. The local classifiers are trained by the positive documents of the same levels to represent the classifiers of their local levels. The selection of negative examples in the directory classifiers and the local classifiers is similar to the SHCI approach as described in Section 3.1. The test documents are then integrated into the destination categories through both the directory classifiers and the local classifiers. The integration process is finished when all the test documents from their source categories are integrated into the designated destination categories. 4. Experiments and Discussion Experiments were performed involving real-world catalogs from both Yahoo! and Google to examine the performance of the EHCI schemes with SVM light [Joachims 2002]. The average integration performance with different l values between 0 and 1 were compared. The results with the optimal l value are listed in detail. Experimental results indicate that the EHCI approach consistently enhances the SVM classifiers in almost all levels and boosts the integration accuracy of a hierarchical structure. The following subsections describe the data sets and the experimental results.  4.1 Data Sets  Five categories were extracted from Yahoo! and Google. Table 2 shows the statistics of our experimental data including the number of hierarchical classes, the training documents and the test documents in these five categories. The experimental data were collected after neglecting the documents that could not be retrieved and removing the documents with error messages. The stopword list in Frakes and Baeza-Yates [1992] was adopted to remove the stopwords in preprocessing. Over 38,000 terms were employed for training and testing after removing the stopwords and stemming. As in [Agrawal and Srikant 2001], documents appearing in only one catalog were used as the training documents in the destination catalog D, and the common documents were adopted as the test documents in the source catalog S.  Table 2. The experimental data collected from the Google catalog  Category  Google  |G-Y| |G Class| |G Test|  Yahoo!  |Y-G| |Y Class| |Y Test|  Autos …/Autos/…  1094 312 437 …/Automotive/… 1823 148 404  Movies …/Movies/…  5174 1165 1340 …/Movies_Film/… 7776 1035 1211  Outdoors …/Outdoors/… 2308 523 224 …/Outdoors/… 1724 100 177  Photo …/Photography/… 615 158 206 …/Photography/… 1399 80 175  Software …/Software/… 5693 1185 683 …/Software/… 1940 109 646  Total  14884 3343 2890  14662 1472 2613  Hierarchical Web Catalog Integration with  165  Conceptual Relationships in a Thesaurua  A set of 1,472 classes in the Yahoo! catalog and a set of 3,343 classes in the Google catalog were organized according to the original hierarchy to a depth of six levels as shown in Table 2. The test documents were chosen by cross-referencing the documents of Yahoo! with those of Google. Table 2 indicates that the numbers of test documents in Yahoo! and Google were different, in the sense that some test documents may appear in more than one class simultaneously. The training documents of the Yahoo! catalog and the Google catalog were accumulated by subtracting the common documents in the other catalog. In this experiment, the documents were integrated both from Yahoo! into Google and from Google to Yahoo!.  Tables 3 and 4 further describe the number of the hierarchical classes, the training documents, and the test documents of six levels in the Google and Yahoo! catalogs. Since most of the sixth levels contain less than ten documents, the hierarchies were only retrieved down to the sixth level, and any documents below the sixth level were merged upward to the sixth level. Tables 3 and 4 indicate that the numbers of some Level 1 classes were zero, meaning that the destination category contained no Level 1 test documents. This experiment only considered the documents that were correctly integrated into the destination categories, thus we list the number of classes with common test documents.  Table 3. The experimental data collected from the Google catalog  Level 1 Level 2 Level3 Level 4 Level 5 Level 6 Total  Class # in Autos  0  14  98  148  46  6  312  Training doc.# in Autos  0  144  422 389 127  12 1094  Test doc. # in Autos  0  86  218 111  19  3  437  Class # in Movies  
MiniJudge is free online open-source software to help theoretical syntacticians collect and analyze native-speaker acceptability judgments in a way that combines the speed and ease of traditional introspective methods with the power and statistical validity afforded by rigorous experimental protocols. This paper shows why MiniJudge is useful, what it feels like to use it, and how it works. Keywords: Syntax, Experimental Linguistics, JavaScript, R, Generalized Linear Mixed Effect Modeling 1. Introduction Every theoretical syntactician has faced the problem of native-speaker judgments that, instead of correlating neatly with the theoretical issue at hand, vary unexpectedly across sentences or speakers. This problem is generally dealt with indiscriminately, either by fiat (“assuming these judgments are correct...”) or by dropping the data entirely, along with the potentially important theoretical issue it may provide. Perhaps forty years ago [Chomsky 1965:19-20] was right to declare that “[t]he critical problem for grammatical theory today is not a paucity of evidence but rather the inadequacy of present theories of language to account for masses of evidence that are hardly open to serious question.” However, as [Schütze 1996:27] observed (ten years ago now), “the questions linguists are now addressing rely crucially on facts that are indeed ‘open to serious question’.” Acceptability judgments reflect grammatical knowledge, but as data they are merely a form of linguistic behavior, parallel to the accuracy rates or reaction times measured by psycholinguists ([Chomsky 1965], [Penke and Rosenbach 2004]). From a cognitive science perspective, then, the ideal solution to the linguists’ data woes would be for them to adopt the rigorous experimental protocols honed over the two centuries scientists have been struggling  ∗ Graduate Institute of Linguistics, National Chung Cheng University, 168 University Road, Min-Hsiung, Chia-Yi 62102, Taiwan Phone: 886-5-242-8251 Fax: 886-5-272-1654 E-mail: lngmyers@ccu.edu.tw [Received November 26, 2006; Revised July 3, 2007; Accepted July 11, 2007]  176  James Myers  to extract information about mental structure from often messy behavioral data. When linguistic judgments are collected with such protocols, they often (though not always) reconfirm the essential validity of empirical claims made on the basis of more informal methods, but they can also go beyond simple reconfirmation (or falsification) to reveal hitherto unsuspected theoretical insights. Recent examples of the growing experimental syntax literature include [Sorace and Keller 2005], [Featherston 2005], and [Clifton et al. 2006]; [Cowart 1997] is a user-friendly handbook. Unfortunately, full-fledged experimental syntax is complex, forcing the researcher to spend considerable time on work that is not theoretically very interesting. Fortunately, the complexity of an experiment need only be proportional to the subtlety of the effect it is trying to detect. Most judgments are very clear (perhaps because a grammar must be shared by a speech community, and hence must be “obvious” enough to learn), and so are reliably detected even with traditional “trivially simple” methods. Very subtle or variable judgments, or hypotheses involving gradient degrees of acceptability or interactions between grammar and processing, may require full-fledged experimental methods. However, in the large area in between, a compromise seems appropriate, where methods are powerful enough to yield statistically valid results, yet are simple enough to apply quickly. This is where MiniJudge comes in. MiniJudge [Myers 2007a] is a family of software tools designed to help theoretical syntacticians design, run, and analyze linguistic judgment experiments quickly and painlessly. Though MiniJudge experiments are small-scale experiments, testing the minimum number of speakers and sentences in the shortest amount of time, they use statistical techniques designed to maximize interpretive power from small data sets. In this paper, I first define more precisely what makes a MiniJudge experiment small-scale. Then, I walk through a sample MiniJudge experiment on Chinese. Finally, I reveal MiniJudge’s inner workings, which involve some underused or novel statistical techniques. The most updated implementation of MiniJudge is MiniJudgeJS, which is written in JavaScript, HTML, and the statistical language R [R Development Core Team 2007]. It has been tested most extensively in Firefox for Windows XP, but also seems to work properly in Internet Explorer and Opera in Windows, Firefox for Linux (though line breaks are not handled properly in R for Linux), and Firefox, Opera, and Safari for Macintosh. There is also a Java implementation called MiniJudgeJava [Chen et al. 2007] with somewhat different internal algorithms and interface, but which otherwise works the same as the JavaScript version described in this paper.  MiniJudge: Software for Small-Scale Experimental Syntax  177  2. Small-Scale Experimental Syntax  Experimental syntax (at least the type carried out in laboratories) generally adheres rather closely to conventions developed in psycholinguistics: multiple stimuli and subjects (naive ones rather than the bias-prone experimenters themselves), factorial designs (where materials represent all possible combinations of the experimental factors, to avoid confounds and make it possible to study interactions between factors), filler items (to prevent subjects from guessing which materials are the theoretically crucial ones), counterbalancing (so no subject is presented with “minimal pairs” differing only in theoretically relevant factors), continuous response measures (e.g., open-ended judgment scales, to permit the use of standard statistical techniques like the analysis of variance, or ANOVA), and statistical analysis (to determine how unlikely the obtained results were to have occurred by chance alone). Together, these conventions can make the designing, running, and analysis of syntax experiments quite time-consuming and intimidating to the novice, especially if the experiment ends up merely reconfirming results already suspected from informally collected judgments.  In a small-scale judgment experiment, however, only the most essential of these conventions are maintained, as summarized in Table 1.  Table 1. Key characteristics of small-scale experimental syntax  Very few sentence sets (about 10)  No fillers  Very few (naive) speakers (about 10-20)  No counterbalancing of sentence lists  Maximum of two binary factors  Random sentence order  Binary yes/no judgments  Order treated as a factor in the statistics  The very small number of sentence sets and speakers (in comparison with the typical psycholinguistics experiment) means that experiments can be designed and conducted quite quickly. Statistical power need not be sacrificed, since, as explained below, the statistical analysis uses all of the raw data; hence an experiment with ten speakers judging ten sentence pairs yields 200 distinct observations. Restricting to two binary factors also speeds up experimental design, and reflects quite well the sorts of designs implicit in most actual syntactic research; an example demonstrating this is given below. Binary yes/no judgments are inherently less information-rich than judgments on a continuous scale, but they are generally easier for naive subjects to provide (see, e.g., [Snyder 2000]); unclear cases can simply be responded to with an arbitrary guess (which may feel random, but rarely is). Though binary judgments are the default when judgments are collected informally, they are often avoided when experimenters intend to analyze their results statistically, one reason being that the most familiar statistical techniques (like ANOVA) are designed for continuous data. Rather than adjusting the judgment conventions to suit the statistics, MiniJudge adjusts the statistics to suit the judgment conventions of actual practicing syntacticians, adopting a recently developed  178  James Myers  method designed specifically for binary response measures collected across both subjects and materials (see 4.2.1). The lack of fillers and counterbalancing means that subjects have more opportunities to guess the purpose of the experiment than is typically tolerated in psycholinguistics, but the effect of any biases that may result is limited due to the treatment sentence order. First, as is standard in psycholinguistic experiments, materials are presented in random order, since by far the most powerful (hence, annoying) bias in linguistic responses is memory of recently processed forms. Second, going beyond standard practice, MiniJudge is capable of ignoring in the statistics any lingering order effects (see 4.2.2). As I demonstrate below, this feature is sometimes essential to bring particularly subtle and sensitive judgment patterns up to the level of statistical significance. For further justification of the built-in restrictions of MiniJudge, see the MiniJudge homepage [Myers 2007a]. 3. Using MiniJudge To show how MiniJudge is used, I describe a recent application of it to a morphosyntactic issue in Chinese (see [Myers 2007b] for discussion of the linguistic background). MiniJudge has also been used to run syntax experiments on English and Taiwan Sign Language, as well as to run pilots for larger studies and to help teach basic concepts in experimental design. MiniJudge can also be used for judgments experiments in pragmatics, semantics, and phonology.  3.1 Goal of the Experiment  [He 2004] presents an interesting observation about the interaction of compound-internal phrase structure and affixation of the plural marker men in Chinese. Part of his paradigm is shown in Table 2, where V = verb and O = object (based on his (2) & (4), pp. 2-3).  Table 2. The VOmen paradigm of He (2004)  [+men]  [-men]  [+VO] *zhizao yaoyan zhe men make rumor person PLURAL  zhizao yaoyan zhe make rumor person  [-VO] yaoyan zhizao zhe men rumor make person PLURAL  yaoyan zhizao zhe rumor make person  He’s analysis is not relevant here; the question is simply whether or not his observation about the judgment pattern in Table 2 is empirically correct. As a non-native speaker of Chinese, I have no intuitions myself. When I have informally asked colleagues and students to double-check the judgments, I have received a mixed response. Some looking at He’s paper  MiniJudge: Software for Small-Scale Experimental Syntax  179  seem to be influenced more by the printed star pattern than the examples themselves. Others rule out men or VO entirely, but this misses the point, since He’s claim concerns the ungrammaticality of the VOmen form relative to all the others. It may also be that He’s generalization works for the few examples he cites, but fails in general. My goal, then, was to use MiniJudge to generate more examples to test systematically on native speakers.  3.2 The MiniJudgeJS Interface  MiniJudgeJS is simply a JavaScript-enabled HTML form. Input and output are handled by text areas; generated text includes code to run statistical analyses in R. Like the rest of the MiniJudge family, MiniJudgeJS divides the experimental process into the steps in Table 3.  Table 3. The steps used by MiniJudge  I. Design experiment  II. Run experiment  III. Analyze experiment  Choose experimental factors  Choose number of speakers Download and install R  Choose set of prototype sentences Write instructions for speakers Enter raw results  Choose number of sentence sets Print or email survey forms Generate data file  Segment prototype set (optional) Save schematic survey file Save data file  Replace segments (optional)  Generate R code  Save master list of test sentences  Paste R command code into R  3.3 Designing the Experiment A MiniJudge experiment is defined by its experimental factors. Thus, the paradigm in Table 2 is derived via two binary factors: [±VO] (VO vs. OV) and [±men] (with or without men suffixation). As noted above, He’s observation doesn’t relate to each factor separately, but rather to an interaction: the combination of the factor values [+VO] and [+men] is claimed to result in lower acceptability, relative to overall judgments for [+VO] and for [+men]. The next step is to enter the prototype set of sentences (a pair if one factor, a quartet if two factors). Similar to the example sets shown in syntax papers and presentations, the prototype set serves multiple purposes. Most fundamentally, it helps to make the logic of factorial experimental design intuitive for novice experimenters. Syntacticians are not always aware of the importance of contrasting sentences that differ only in theoretically relevant factors, or of the central role played by interactions in many syntactic claims (for further discussion of the relevance of factors and interactions in syntax experiments, see [Cowart 1997], as well as the MiniJudge main page [Myers 2007a]).  180  James Myers  Another purpose of the prototype set is that it can be used to help generate further sentence sets that maintain the same factorial contrasts but vary in irrelevant lexical properties. In the case of the present experiment, the claim made in [He 2004] says nothing about the particular verb, object, or head that is used. Thus the judgment pattern claimed for Table 2 above should also hold for the sets shown in Table 4 below, regardless of any additional influences from pragmatics, frequency, suffixlikeness (zhe vs. the others), or freeness (ren vs. the others); the stars here represent what He should predict (lexical content for the new sets was chosen with the help of Ko Yu-guang and Zhang Ning).  Table 4. Extending the VOmen paradigm of He [2004]  [+men]  [-men]  [+VO] *chuanbo bingdu yuan men spread virus person PLURAL  chuanbo bingdu yuan spread virus person  [-VO] bingdu chuanbo yuan men virus spread person PLURAL  bingdu chuanbo yuan virus spread person  [+VO] *sheji shipin ren men design ornaments person PLURAL  sheji shipin ren design ornaments person  [-VO] shipin sheji ren men ornaments design person PLURAL  shipin sheji ren ornaments design person  MiniJudge partly automates the process of creating new sentence sets by dividing up the prototype sentences into the largest repeating segments and replacing them with user-chosen substitutes. The prototype segments for Table 2 are shown in the first row of Table 5. The user only has to find parallel substitutes for four segments, rather than having to construct whole new sentences consistent with the factorial design (Table 5 shows the segments needed to generate the new sets in Table 4). The segmentation and set generation algorithms (see 4.1) are designed to work equally well in English-like and Chinese-like orthographies. Of course, since MiniJudge knows no human language, it sometimes makes strange errors, so users are allowed to correct its output, or even to generate new sets manually.  Table 5. Prototype segments and new segments for the VOmen experiment  Set 1 (prototype) segments:  zhizao yaoyan zhe  men  Set 2 segments:  chuanbo bingdu yuan  men  Set 3 segments:  sheji  shipin  ren  men  After the user has corrected and approved the master list of sentences, it can be saved to a file for use in reports. In the present experiment, the master list contained 48 sentences (12 sets of 4 sentences each). This is an unusually large number of sentences for a MiniJudge experiment; significant results have been found with experiments with as few as 10 sentences.  MiniJudge: Software for Small-Scale Experimental Syntax  181  3.4 Running the Experiment In order to run a MiniJudge experiment, the user must make three decisions. The first concerns the maximum number of speakers to test. It is possible to get significant results with as few as 7 speakers, but in the present experiment, I generated 30 surveys. As it turned out, only 18 surveys were returned.  The second decision concerns whether surveys will be distributed by printed form or by email. In MiniJudgeJS, printing surveys involves saving them from a text area and printing them with a word processor. MiniJudgeJS cannot send email automatically, so emailed surveys must be individually copied and pasted. In the present experiment, I emailed thirty students, former students, or faculty of my linguistics department who did not know the purpose of the experiment.  The final decision concerns the instructions, which the user may edit from a default. MiniJudgeJS requires that judgments be entered as 1 (yes) vs. 0 (no). Chinese instructions for the VOmen experiment were written with the help of Ko Yu-guang.  Surveys themselves are randomized individually to prevent order confounds, as is standard in psycholinguistics. The randomization algorithm, taken from [Cowart 1997:101], results in every sentence having an equal chance to appear at any point in the experiment (by randomization of blocks), while simultaneously distributing sentence types evenly and randomly.  Each survey starts with the instructions, followed by a speaker ID number (e.g. “##02”), and finally the survey itself, with each sentence numbered in the order seen by the speaker. Because the speakers’ surveys intentionally hide the factorial design, the experimenter must save this information separately in a schematic survey file. This file is meant to be read only by MiniJudgeJS; as an example, the first line of the schematic survey file for the present experiment is explained in Table 6.  Table 6. The structure of the schematic survey information file for the VOmen experiment  File line: 01  20  05  01  -VO  -men  Explanation: speaker ID sentence ID set ID number order in value of first value of  number number  survey factor  second factor  After completed surveys have been returned, the experimenter pastes them into a text area in any order (as long as each survey still contains its ID number), and pastes the schematic survey information back into another text window. MiniJudgeJS extracts judgments from the surveys and creates a data file in which each row represents a single observation, with IDs for speakers, sentences, and sets, presentation order of sentences, factor values (1 for [+] and -1 for [-]), and judgments. As an example, the first three lines of the data file for the  182  James Myers  VOmen experiment are shown in Table 7.  Table 7. First three lines of data file for the VOmen experiment  Speaker Sentence Set Order VO men Judgment  
There are many methods to improve performance of statistical parsers. Resolving structural ambiguities is a major task of these methods. In the proposed approach, the parser produces a set of n-best trees based on a feature-extended PCFG grammar and then selects the best tree structure based on association strengths of dependency word-pairs. However, there is no sufficiently large Treebank producing reliable statistical distributions of all word-pairs. This paper aims to provide a self-learning method to resolve the problems. The word association strengths were automatically extracted and learned by parsing a giga-word corpus. Although the automatically learned word associations were not perfect, the constructed structure evaluation model improved the bracketed f-score from 83.09% to 86.59%. We believe that the above iterative learning processes can improve parsing performances automatically by learning word-dependence information continuously from web. Keywords: Parsing, Word association, Knowledge Extraction, PCFG, PoS Tagging, Semantic. 1. Introduction How to solve structural ambiguity is an important task in building a high-performance statistical parser, particularly for Chinese. Since Chinese is an analytic language, words can play different grammatical functions without inflection. A great deal of ambiguous structures would be produced by parsers if no structure evaluation were applied. There are three main steps in our approach that aim to disambiguate the structures. The first step is to have the parser produce n-best structures. Second, we extract word-to-word associations from large corpora and build semantic information. The last step is to build a structural evaluator to find the best tree structure from the n-best candidates. There have been some approaches proposed in the past to resolve structure ambiguities. For instance: ∗ Institute of Information science, Academia Sinica, Taipei, Taiwan E-mail: {morris, ydc, kchen}@iis.sinica.edu.tw [Received February 7, 2007; Revised July 2, 2007; Accepted July 9, 2007]  196  Yu-Ming Hsieh et al.  Adding on lexical dependencies. Collins [1999] solves structural ambiguity by extracting lexical dependencies from Penn WSJ Treebank and applying dependencies to the statistic model. Lexical dependency (or Word-to-word association, WA) is one type of semantic information. It is a current trend to add on semantic related information in traditional parsers. Some incorporate word-to-word association in their parsing models, such as the Dependency Parsing in Chen et al. [2004]. They take advantage of statistical information of word dependency in the parsing process to produce dependency structures. However, word association methods suffer low coverage when lacking very large tree-annotated training corpora while checking dependency relationships between word pairs. Adding on word semantic knowledge where CiLin and HowNet information are used in the statistic model in the experiment [Xiong et al. 2005]. Their results work to solve common parsing mistakes efficiently. Using a re-annotation method in grammar rules. Johnson [1998] thinks that re-annotating each node with the category of its parent category in Treebank is able to improve parsing performance. Klein et al. [2003] proposes internal, external, and tag-splitting annotation strategies to obtain better results. Building an evaluator. Some people re-rank the structure values and find the best parse [Collins 2000; Charniak et al. 2005]. At first, the parser produces a set of candidate parses for each sentence. Later, the re-ranker finds the best tree through relevance features. The performance is better than without the re-ranker. This paper is going to show a self-learning method to produce imperfect (due to errors produced by automatic parsing) but unlimited amount of word association data to evaluate the n-best trees produced by a feature-extended PCFG grammar. The parser with this WA evaluation is considerably superior to those without the evaluation. The organization of the paper is as follows: Section 2 describes how to generate n-best trees in a simple way. In Section 3, we account for building word-to-word association and a primitive semantic class as well. As to the design of the evaluating model, our probability model, coordination of rule probability, and word association probability are presented in Section 4. In Section 5, we discuss and explain the experimental data and results. Ambiguities of PoS are to be considered in a practical system. Section 6 deals with further experiments on  Improve Parsing Performance by Self-Learning  197  automatic tagging with PoS. Finally, we offer concluding remarks in Section 7. 2. Feature Extension of PCFG Grammars for Producing the N-best Trees It is clear that Treebanks [Chen et al. 2003] provide not only instances of phrasal structures and word dependencies but also their statistical distributions. Recently, probabilistic preferences for grammar rules and feature dependencies were incorporated to resolve structure-ambiguities and had great improvement on parsing performance. However, the automatically extracted grammars and feature-dependence pairs suffer the problem of low coverage. We proposed different approaches to solve these two different types of low coverage problems. For the low coverage of extracted grammar, a linguistically-motivated grammar generalization method is proposed [Hsieh et al. 2005]. The low coverage of word association pairs is resolved by a self-learning method of automatic parsing and extracting word dependency pairs from very large corpora. The linguistically-motivated generalized grammars are derived from probabilistic context-free grammars (PCFG) by right-association binarization and feature embedding. The binarized grammars have better coverage than the original grammars directly extracted from Treebank. Features are embedded into the lexical and phrasal categories to improve the precision of generalized grammar. The important features adopted in our grammar are described in the following:  Head (Head feature): Example: Linguistic motivations:  The PoS of phrasal head will propagate all intermediate nodes within the constituent. S(NP(Head:Nh: 他 )|S’-Head:VF(Head:VF: 叫 |S’-Head:VF(NP(Head:Nb: 李四)| VP(Head:VC:撿| NP(Head:Na:皮球))))) To constrain the sub-categorization frame.  Left (Leftmost feature): Example: Linguistic motivation:  The PoS of the leftmost constitute will propagate one-level to its intermediate mother-node only. S(NP(Head:Nh: 他 )|S’-Head:VF(Head:VF: 叫 |S’-NP(NP(Head:Nb: 李 四)| VP(Head:VC:撿| NP(Head:Na:皮球))))) To constrain linear order of constituents.  Head 0/1 (Existence of phrasal head): Example: Linguistic motivation:  If phrasal head exists in intermediate node, the nodes will be marked with feature 1; otherwise 0. S(NP(Head:Nh: 他 )|S’-1(Head:VF: 叫 |S’-0(NP(Head:Nb: 李 四)|VP(Head:VC:撿| NP(Head:Na:皮球))))) To enforce unique phrasal head in each phrase.  198  Yu-Ming Hsieh et al.  There are two functions of applying the embedded features: one is to increase the precision of the grammar and the other is to produce more candidate parse structures. With features embedded in phrasal categories, PCFG parsers are forced to produce varieties of different possible structures1. In order to achieve a better n-best oracle performance (i.e. the ceiling performance achieved by picking the best structure from n bests), we designed some different feature-embedded grammars and try to find a grammar with the better n-best oracle performance. For instance, “S(NP(Head:Nh: 他 )|Head:VF: 叫 | NP(Head:Nb: 李 四 )| VP(Head:VC:撿| NP(Head:Na:皮球)))”. The explanations of feature sets are as follow. Rule type-1: Intermediate node: add on “Left and Head 1/0” features. Non-intermediate node: if there is only one member in the NP, add on “Head” feature. Example: S(NP-Head:Nh(Head:Nh:他)|S’-Head:VF-1(Head:VF:叫|S’-NP-0(NP-Head:Nb(Head:Nb:李 四)|VP(Head:VC:撿| NP-Head:Na(Head:Na:皮球))))) Rule type-2: Intermediate node: add on “Left and Head 1/0” features. Non-intermediate node: add on “Head and Left” features, if there is only one member in the NP, add on “Head” feature. Example: S-NP-Head:VF(NP-Head:Nh(Head:Nh:他)|S’-Head:VF-1(Head:VF:叫 |S’-NP-0(NP-Head:Nb(Head:Nb:李四)|VP-Head:VC(Head:VC:撿| NP-Head:Na(Head:Na:皮球))))) Rule type-3: Intermediate node: add on “Left, and Head 1/0” features. Top-Level node: add on “Head and Left” features. (see example of S-NP-Head:VF) Non-intermediate node: if there is only one member in the NP, add on “Head” feature. Example: S-NP-Head:VF(NP-Head:Nh(Head:Nh:他)|S’-Head:VF-1(Head:VF:叫 |S’-NP-0(NP-Head:Nb(Head:Nb:李四)|VP(Head:VC:撿| NP-Head:Na(Head:Na:皮球)))))  
The performance of current automatic speech recognition (ASR) systems often deteriorates radically when the input speech is corrupted by various kinds of noise sources. Quite a few techniques have been proposed to improve ASR robustness over the past several years. Histogram equalization (HEQ) is one of the most efficient techniques that have been used to reduce the mismatch between training and test acoustic conditions. This paper presents a comparative study of various HEQ approaches for robust ASR. Two representative HEQ approaches, namely, the table-based histogram equalization (THEQ) and the quantile-based histogram equalization (QHEQ), were first investigated. Then, a polynomial-fit histogram equalization (PHEQ) approach, exploring the use of the data fitting scheme to efficiently approximate the inverse of the cumulative density function of training speech for HEQ, was proposed. Moreover, the temporal average (TA) operation was also performed on the feature vector components to alleviate the influence of sharp peaks and valleys caused by non-stationary noises. All the experiments were carried out on the Aurora 2 database and task. Very encouraging results were initially demonstrated. The best recognition performance was achieved by combing PHEQ with TA. Relative word error rate reductions of 68% and 40% over the MFCC-based baseline system, respectively, for clean- and multi- condition training, were obtained. Keywords: Automatic Speech Recognition, Robustness, Histogram Equalization, Data Fitting, Temporal Average 1. INTRODUCTION With the successful development of much smaller electronic devices and the popularity of wireless communication and networking, it is widely believed that speech will play a more ∗ Department of Computer Science & Information Engineering, National Taiwan Normal University, Taipei, Taiwan E-mail: { shlin, ymyeh, berlin }@csie.ntnu.edu.tw [Received March 2, 2007; Revised July 4, 2007; Accepted July 12, 2007]  218  Shih-Hsiang Lin et al.  active role and will serve as the major human machine interface (HMI) for the interaction between people and different kinds of smart devices in the near future [Lee and Chen 2005]. Therefore, automatic speech recognition (ASR) has long been one of the major preoccupations of research in the speech and language processing community. Nevertheless, varying environmental effects, such as ambient noise, noises caused by the recording equipment and transmission channels, etc., often lead to a severe mismatch between the acoustic conditions for training and test. Such a mismatch will no doubt cause substantial degradation in the performance of an ASR system. Substantial effort has been made and a large number of techniques have been presented in the last few decades to cope with this issue for improving ASR performance [Gong 1995; Junqua et al. 1996; Huang et al. 2001]. In general, they fall into three main categories [Gong 1995]: z Speech enhancement, which removes the noise from the observed speech signal. z Robust speech features extraction, which searches for noise resistant and robust features. z Acoustic model adaptation, which transforms acoustic models from the training (clean) space to the test (noisy) space. Techniques of each of the above three categories have their own reasons for superiority and their own limitations. In practical implementation, acoustic model adaptation often yields the best recognition performance, because it directly adjusts the acoustic models parameters (e.g., the mean vectors or covariance matrices of mixture Gaussian models) to accommodate the uncertainty caused by noisy environments. Representative techniques, include, but are not limited to, the maximum a posteriori (MAP) adaptation [Gauvain and Lee 1994; Huo et al. 1995], the maximum likelihood linear regression (MLLR) [Leggeter and Woodland 1995; Gales 1998], etc. However, such techniques generally require a sufficient amount of extra adaptation data (either with or without reference transcripts) and a significant computational cost in comparison with the other two categories. Moreover, most of the speech enhancement techniques target enhancing the signal-to-noise ratio (SNR) but not necessarily at improving the speech recognition accuracy. On the other hand, robust speech feature extraction techniques can be further divided into two subcategories, i.e., model-based compensation and feature space normalization. Model-based compensation assumes the mismatch between clean and noisy acoustic conditions can be modeled by a stochastic process. The associated compensation models can be estimated in the training phase, and then exploited to restore the feature vectors in the test phase. Typical techniques of this subcategory, include, but are not limited to, the minimum mean square error log spectral amplitude estimator (MMSE-LSA) [Ephraim and Malah 1985], the vector Taylor series (VTS) [Moreno 1996], the stochastic vector mapping (SVM) [Wu and Huo 2006], the multi-environment model-based linear normalization (MEMLIN) [Buera et al. 2007], etc.  A Comparative Study of Histogram Equalization (HEQ) for  219  Robust Speech Recognition  Feature space normalization is believed to be a simpler and more effective way to compensate for the mismatch caused by noise, and it has also demonstrated the capability to prevent the degradation of ASR performance under various noisy environments. Several attractive techniques have been successfully developed and integrated into the state-of-the-art ASR systems. As an example, the cepstral mean subtraction (CMS) [Furui 1981] is a simple but effective technique for removing the time-invariant distortion introduced by the transmission channel; while a natural extension of CMS, called the cepstral mean and variance normalization (CMVN) [Vikki and Laurila 1998], attempts to normalize not only the means of speech features but also their variances. Although these two techniques have already shown their capabilities in compensating for channel distortions and some side effects resulting from additive noises, their linear properties still make them inadequate in tackling the nonlinear distortions caused by various noisy environments [Torre et al. 2005]. Accordingly, a considerable amount of work on seeking more general solutions for feature space normalization has been done over the past several years. For example, not content with using either CMN or CMVN merely to normalize the first or the first two moments of the probability distributions of speech features, some researchers have extended the principal idea of CMN and CMVN to the normalization of the third [Suk et al. 1999] or even more higher order moments of the probability distributions of speech features [Hsu and Lee 2004, 2006]. On the other hand, the histogram equalization (HEQ) techniques also have gained much attention, and have been widely investigated in recent years [Dharanipragada and Padmanabhan 2000; Molau et al. 2005; Torre et al. 2005; Hilger and Ney 2006; Lin et al. 2006]. HEQ seeks for a transformation mechanism that can map the distribution of the test speech onto a predefined (or reference) distribution utilizing the relationship between the cumulative distribution functions (CDFs) of the test speech and those of the training (or reference) speech. Therefore, HEQ not only attempts to match the means and variances of speech features but also completely match the distributions of speech features between training and test. More specifically, HEQ normalizes all moments of the probability distributions of test speech features to those of the reference ones. However, most of the current HEQ techniques still have some inherent drawbacks for practical usage. For example, they require either large storage consumption or considerable online computational overhead, which might make them infeasible when being applied to the ASR systems built on devices with limited resources, such as personal digital assistants (PDAs), smart phones and embedded systems, etc. With these observations in mind, in this paper we present a comparative study of various HEQ approaches for robust speech recognition. Two representative HEQ approaches, namely, the table-based histogram equalization (THEQ) and the quantile-based histogram equalization (QHEQ), were first investigated. Then, a polynomial-fit histogram equalization (PHEQ)  220  Shih-Hsiang Lin et al.  approach, exploring the use of the data fitting scheme to efficiently approximate the inverse of the cumulative density function of training speech for HEQ, was proposed. Moreover, the temporal average (TA) operation was also performed on the feature vector components to alleviate the influence of sharp peaks and valleys that were caused by non-stationary noises. The remainder of this paper is organized as follows. Section 2 describes the basic concept of HEQ and reviews two representative HEQ approaches, namely, THEQ and QHEQ. Section 3 elucidates our proposed HEQ approach, namely, PHEQ, and also briefly introduces several standard temporal average operations. Section 4 gives an overview of the Aurora 2 database as well as a description of the experimental setup, while the corresponding experimental results and discussions are also presented in this section. Finally, conclusions are drawn in Section 5. 2. HISTOGRAM EQUALIZATION (HEQ) 2.1 Theoretical Foundation of HEQ Histogram equalization is a popular feature compensation technique that has been well studied and practiced in the field of image processing for normalizing the visual features of digital images, such as the brightness, grey-level scale, contrast, and so forth. It has also been introduced to the field of speech processing for normalizing the speech features for robust ASR, and many good approaches have been continuously proposed and reported in the literature [Dharanipragada and Padmanabhan 2000; Molau et al. 2003; Torre et al. 2005; Hilger and Ney 2006; Lin et al. 2006]. Meanwhile, HEQ has shown its superiority over the conventional linear normalization techniques, such as CMN and CMVN, for robust ASR. One additional advantage of HEQ is that it can be easily incorporated with most feature representations and other robustness techniques without the need of any prior knowledge of the actual distortions caused by different kinds of noises. Theoretically, HEQ has its roots in the assumptions that the transformed speech feature distributions of the test (or noisy) data should be identical to that of the training (or reference) data and each feature vector dimension can be normalized independently of each other. The speech feature vectors can be estimated either from the Mel-frequency filter bank outputs [Molau 2003; Hilger and Ney 2006] or from the cepstral coefficients [Segura et al. 2004; Torre et al. 2005; Lin et al. 2006]. Since each feature vector dimension is considered independently, from now on, the dimension index of each feature vector component will be omitted from the discussion for the simplicity of notation unless otherwise stated. Under the above two assumptions, the aim of HEQ is to find a transformation that can convert the distribution of each feature vector component of the input (or test) speech into a predefined target distribution which corresponds to that of the training (or reference) speech. The basic  A Comparative Study of Histogram Equalization (HEQ) for  221  Robust Speech Recognition  idea of HEQ is illustrated in Figure 1.  Transformation Function  CDF of Test Speech CDF of Reference Speech  y  ( ) 1.0 CTrain y  x  CDF  ( ) CTest x  1.0  Figure 1. The basic idea of HEQ.  Accordingly, HEQ attempts not only to match the means and variances of the speech  features, but also to completely match the speech feature distributions of training and test data.  Phrased another way, HEQ normalizes all the moments of the probability distributions of the  speech features. The formulation of HEQ is described as follows [Torre et al. 2005]. For each  feature space dimension, let x be the feature vector component that follows the distribution pTest ( x) . A transformation function F ( x) converts x to y and follows a reference distribution pTrain ( y) , according to the following expression:  ( ) pTrain ( y) =  pTest  ( x) dx dy  =  pTest  F −1 ( y)  dF −1 ( y) , dy  (1)  where F−1 ( y) is the inverse function of F ( x) . Moreover, the relationship between the cumulative probability density functions (CDFs) associated with the test and training speech, respectively, is governed by:  CT est ( x ) = ∫−x∞ pT est ( x ′) d x ′  =  ∫−F∞( x )  pT est  (F  −1 (  y ′))  dF  −1 ( dy′  y ′)  d  y′  (2)  = ∫−y∞ pT rain ( y ′) d y ′ y = F ( x )  = CTrain ( y) ,  222  Shih-Hsiang Lin et al.  where CTest ( x) and CTrain ( y) are the CDFs for the test and training speech, respectively; y′ is the corresponding output of the transformation function F ( x′) ; and the transformation function F ( x) has the following property:  ( ) F ( x) = CT−r1ain CTest ( x) ,  (3)  where C−1 is the inverse function of CTrain . Train It is worth noting that the reliability of CDF estimation will have a significant influence on the performance of HEQ. Due to the finite number of speech features being considered, the CDFs of speech features are usually approximated by the cumulative histograms of speech features for practical implementation. The CDFs of speech features can be accurately and reliably approximated when there is a large amount of data available. On the contrary, such approximation will probably not be accurate enough when the (test) speech utterance becomes much shorter. Several studies have shown that the order-statistics based method tends to be more accurate than the cumulative-histogram based when the amount of speech data is insufficient for reliable approximation of CDFs [Segura et al. 2004; Torre et al. 2005].  2.2 Table-Based Histogram Equalization (THEQ) The table-based histogram equalization (THEQ) was first proposed by Dharanipragada and Padmanabhan [Dharanipragada and Padmanabhan 2000] and is a non-parametric method to let the distributions of the test speech match those of the training speech. THEQ uses a cumulative histogram to estimate the corresponding CDF value of each feature vector component y . During the training phase, the cumulative histogram of each feature vector component y of the training data is constructed as follows. The range of values of each feature vector dimension over the entire training data is first determined by finding the feature vector components ymax and ymin that have the maximum and minimum values, respectively. Let K be the total number of histogram bins and the range ⎡⎣ymin , ymax ⎤⎦ is then divided into K non-overlapped bins of equal size, {B0 , B1," BK −1} . Next, the entire training data is scanned once and each individual feature vector component falls exactly into one bin. Thus, if we let N be the total number of training feature vector components of one specific dimension and ni be the number of feature vector components of that dimension belonging to Bi , the probability of feature vector components of that dimension being in Bi is approximated by:  pTrain  ( Bi )  =  ni N  .  (4)  A Comparative Study of Histogram Equalization (HEQ) for  223  Robust Speech Recognition  The mean yBi of each bin i is taken as one of the representative outputs of the transformation function F ( x) and the approximate CDF value CTrain ( y) of the feature vector component y that belongs to Bi is calculated by:  ( ) CTrain  (  y)  =  ∑  i j  =0  pTrain  Bj .  (5)  ( ) Finally, a look-up table consisting of all possible distinct reference pairs CTrain ( y), yBi is constructed, where CTrain ( y) is taken as the key and yBi is the corresponding restored value. During the test phase, the CDF estimation of the test utterance can be done in the same way by using the cumulative histograms of itself. The restored value of each feature vector component x of the test utterance is obtained by taken its approximate CDF value CTest ( x) as the key to finding the corresponding transformed (restored) value in the look-up table. However, the normalization of the test data alone results in only a moderate gain of performance improvement. It has been suggested that one should normalize the training data in the same way to achieve good performance [Molau et al. 2003]. On the other hand, because a set of cumulative histograms of all speech feature vector dimensions of the training data has to be kept in memory for the table-lookup of restored feature values, THEQ needs large disk storage consumption and its associated table-lookup procedure is also time-consuming, which might make THEQ not very feasible for ASR systems that are built into devices with limited resources, such as PDAs, smart phones and embedded systems, etc.  2.3 Quantile-Based Histogram Equalization (QHEQ) The quantile-based histogram equalization (QHEQ) is a parametric type of histogram equalization. QHEQ attempts to calibrate the CDF of each feature vector component of the test speech to that of the training speech in a quantile-corrective manner instead of a full-match of the cumulative histogram as done by THEQ, described earlier in Section 2.2. Normally, QHEQ only needs a small number of quantiles (usually the number is set to 4) for reliable estimation [Hilger and Ney 2001, 2006]. A transformation function H ( x) is calculated by minimizing the mismatch between the quantiles of the test utterance and those of the training data. The transformation function H ( x) is a power function applied to each feature vector component x , which attempts to make the CDF of the equalized feature vector component match that observed in training. Before the actual application of the transformation function H ( x) , each feature vector component x is first scaled down into the interval [0　, 1　] by being divided by the maximum value QK over the entire utterance. Then, the transformation function H ( x) is applied to x and the transformed (or restored) value of x is scaled back to the original value range [Hilger and Ney 2006]:  224  Shih-Hsiang Lin et al.  H  (x)  =  QK  ⎛ ⎜ ⎜ ⎝  α  ⎛ ⎜ ⎝  x QK  ⎞γ ⎟ ⎠  + (1−α ) x QK  ⎞  ⎟ ⎟  ,  ⎠  (6)  where K is the total number of quantiles; QK is the maximum value over the entire utterance; and α and γ are the transformation parameters. For each feature vector dimension, α and γ are chosen to minimize the squared distance between the quantiles H (Qk ) of the test utterance and the quantiles QkTrain of the training data by using the following equation:  ( ) {α  ,  γ  }  =  arg  min {α ,γ }  ⎛ ⎜ ⎝  K −1 ∑ k =1  H (Qk ) − QkTrain  2⎞ ⎟. ⎠  (7)  In summary, QHEQ allows the estimation of the transformation function H ( x) to merely rely on a single test utterance (or extremely, a very short utterance), without the need of an additional set of adaptation data [Hilger and Ney 2006]. However, in order to find the optimum transformation parameters for each feature vector dimension, an exhaustive online grid search is required, which, in fact, is very time-consuming.  3. IMPROVED APPROACHES  3.1 Polynomial-Fit Histogram Equalization (PHEQ) In contrast to the above table-lookup or quantile based approaches, we propose a polynomial-fit histogram equalization (PHEQ) approach which explores the use of the data fitting scheme to efficiently approximate the inverse functions of the CDFs of the training speech for HEQ [Lin et al. 2006]. Data fitting is a mathematical optimization method which, when given a series of data points (ui ,vi ) with i = 1,", N , attempts to find a function G(ui ) whose output vi closely approximates vi . That is, it minimizes the sum of the squares error (or the squares of the ordinate differences) between the points (ui ,vi ) and their corresponding points (ui ,vi ) in the data. The function G(ui ) to be estimated can be either linear or nonlinear in its coefficients. For example, if G(ui ) is a linear M -order polynomial function:  G (ui ) = vi = a0 + a1ui + a2ui2 + " + aM uiM ,  (8)  where a0, a1,", aM are the coefficients, then its corresponding squares error can be defined by  E2  =  N ∑  ( vi  −  v i  )2  =  N⎛ ∑ ⎜ vi −  M ∑  amuim  ⎞2 ⎟  .  (9)  i =1  i=1⎝ m=0  ⎠  A Comparative Study of Histogram Equalization (HEQ) for  225  Robust Speech Recognition  PHEQ makes use of such data fitting (or so-called least squares regression) scheme to estimate the inverse functions of the CDFs of the training speech. For each speech feature vector dimension of the training data, given the pair of the CDF value CTrain ( yi ) of the ( ) vector component yi and yi itself, the linear polynomial function G CTrain ( yi ) with output yi can be expressed as:  ( ) ( ) G  CTrain ( yi )  = yi =  M ∑  am  CTrain  ( yi  )  m ,  (10)  m=0  where the coefficients am can be estimated by minimizing the squares error expressed in the following equation:  ( ) E '2  =  N ∑  (  yi  −  yi )2  =  N ∑  ⎛ ⎜  yi  −  M ∑  am  CTrain  ( yi )  m  ⎞2 ⎟,  (11)  i =1  i=1⎝ m=0  ⎠  where N is the total number of training speech feature vectors. In implementation, we used the order-statistics based method instead of the cumulative-histogram based method to obtain the approximate CDF values. For the feature vector component sequence Y = ⎡⎣y1,", yi ,", yN ⎤⎦ of a specific dimension of a speech utterance, the corresponding CDF value of each feature component yi can be approximated by the following two steps:  Step1: The sequence Y = ⎡⎣y1,", yi ,", yN ⎤⎦ is first sorted according to the values of the feature vector components in ascending order.  Step2: The order-statistics based approximation of the CDF value of a feature vector  component yi is then given as:  C  (  yi  )  ≈  S pos  (  yi N  )  −  0.5  (12)  where Spos ( yi ) is a function that returns the rank of yi in ascending order of the values of the feature vector components of the sequence Y = ⎡⎣y1,", yi ,", yN ⎤⎦ . Therefore, for each utterance, Equation (12) can be used to approximate the CDF values of the feature vector components of all dimensions. During the training phase, the polynomial functions of all dimensions are obtained by minimizing the squares error expressed in Equation (11). During the test phase, for each feature vector dimension, the feature vector components of the test utterance are simply sorted in ascending order of their values to obtain the approximate CDF values, which can be then taken as the inputs to the inverse function to obtain the corresponding restored component values.  The reason we choose the polynomial function here as the inverse function is mainly because it has a simple form, without the need of a complicated computational procedure, and  226  Shih-Hsiang Lin et al.  has moderate flexibility in controlling the shape of the function. Though the polynomial function is efficient in delineating the transformation function, it is worth mentioning that the polynomial function to some extent has its inherent limitations. For example, high order polynomial functions might lead to over-fitting of the training data. Moreover, the polynomial function provides good fits for input data points that are located within the range of values of the training data, but would also probably have rapid deterioration when the input data points are located outside the range of values of the training data. 3.2 Temporal Average (TA)  Figure 2. The 2th cepstral feature component sequence of an utterance Though the above HEQ approaches are very effective in matching the global feature statistics of the test (or noisy) speech to that of the training (or reference) set, we found that some undesired sharp peaks or valleys of the feature vector component sequence caused by the non-stationary noises often occurring during the equalization process. This phenomenon is illustrated in the upper and middle parts of Figure 2. Therefore, we believe that a rigorous smoothing operation further performed on the time trajectory of the HEQ restored feature vector component sequence will be helpful for suppressing the extraordinary changes of component values. From the other perspective, temporal average can be treated as a low-pass filter. The basic idea of TA is quite similar to RelAtive SpecTrA (RASTA) [Hermansky and Morgan 1994] which aims to filter out the slow-varying or fast-varying artifacts (or noises) based on the evidence of human auditory perception. The main differences between TA and RASTA are the target (or feature domain) where the smoothing operation is performed and the  A Comparative Study of Histogram Equalization (HEQ) for  227  Robust Speech Recognition  design of the temporal filters. The smoothing (or temporal average) operation can be defined as one of the following forms [Chen and Bilmes 2007]: z Non-Causal Moving Average  yˆ t  =  ⎧ ⎪ ⎨  ∑iL=−L yt +i 2L +1  if L < t ≤ T − L,  (13)  ⎪⎩ yt  otherwise  z Causal Moving Average  yˆ t  =  ⎧ ⎪ ⎨  ∑iL=0 yt−i L +1  if L < t ≤ T ,  (14)  ⎪⎩ yt  otherwise  z Non-Causal Auto Regression Moving Average  yˆ t  =  ⎧ ⎪  ∑  L i=1  yˆt −i  +  ∑  L j=0  ⎨  2L +1  yt+ j  if L < t ≤ T − L,  (15)  ⎪ ⎩  y t  otherwise  z Causal Auto Regression Moving Average  yˆ t  =  ⎧ ⎪ ⎨  ∑ iL=1  yˆ t  −i + 2L  ∑  L j=0  +1  yt  −  j  if L < t ≤ T ,  (16)  ⎪ ⎩  y t  otherwise  where yt denotes the HEQ restored feature vector component at speech frame t ; L is the span order of temporal average operation; and yˆt is the corresponding one after the temporal average operation. The feature vector component sequence obtained by Equation (13) is also shown in the lower part of Figure 2.  4. EXPERIEMENTAL RESULTS  4.1 Experimental Setup The speech recognition experiments were conducted under various noise conditions using the Aurora-2 database and task [Hirsch and Pearce 2002]. The Aurora-2 database is a subset of the TI-DIGITS, which contains a set of connected digit utterances spoken in English; while the task consists of the recognition of the connected digit utterances interfered with various noise sources at different signal-to-noise ratios (SNRs), in which Test Sets A and B are artificially contaminated with eight different types of real-world noises (e.g., subway noise, street noise,  228  Shih-Hsiang Lin et al.  babble noise, etc.) in a wide range of SNRs (-5 dB, 0 dB, 5 dB, 10 dB, 15 dB, 20 dB and Clean) and Test Set C additionally includes channel distortions. For the baseline system, the training and recognition tests used the HTK recognition toolkit [Young et al. 2005], following the original setup defined for the ETSI AURORA evaluations [Hirsch and Pearce 2002]. More specifically, each digit was modeled as a left-to-right continuous density hidden Markov model (CDHMM) with 16 states and three diagonal Gaussian mixtures per state. Two additional CDHMMs were defined for the silence. The first one had three states with six diagonal Gaussian mixtures per state for modeling the silence at the beginning and at the end of each utterance. The other one had one state with 6 diagonal Gaussian mixtures for modeling the inter-word short pause. In the front-end speech analysis, the frame length is 25 ms and the corresponding frame shift is 10 ms. Speech frames are pre-emphasized using a factor of 0.97, and the Hamming window is then applied. From a set of 23 Mel-scaled log filter banks outputs a 39-dimensional feature vector, consisting of 12 Mel-frequency cepstral coefficients (MFCCs), the 0-th cepstral coefficient, and the corresponding delta and acceleration coefficients, is extracted at each speech frame. The average word error rate (WER) results obtained by the MFCC-based baseline system are 45.44% and 14.65%, respectively, for cleanand multi-condition training, each of which is an average of the WER results of the test utterances respectively contaminated with eight types of noises under different SNR levels (0 dB to 20 dB) for the three sets (Sets A, B and C).  4.2 Experiments on HEQ Approached  Table 1. Average WER results (%) of THEQ for clean-condition training, with respect to different numbers of histogram bins and different sizes of table.  Table Size  10  50  100 500 1000 5000 10000 50000  100  41.32 45.65 46.39 44.59 44.55 44.65 44.67 44.65  500  33.21 28.60 25.44 22.42 22.42 22.41 22.45 22.41  1000  29.63 24.19 22.12 19.19 19.04 19.46 19.88 19.87  5000  28.13 23.72 20.68 18.22 18.02 18.18 18.19 18.10  10000  27.64 23.50 20.50 18.33 18.10 18.13 18.30 18.32  50000  27.46 23.30 20.29 18.58 18.41 18.46 18.47 18.45  Order-Statistics 27.26 23.30 20.65 18.62 18.32 18.51 18.53 18.58  Histogram Bin Number  A Comparative Study of Histogram Equalization (HEQ) for  229  Robust Speech Recognition  Histogram Bin Number  Table 2. Average WER results (%) of THEQ for multi-condition training, with respect to different numbers of histogram bins and different sizes of table.  100 500 1000 5000 10000 50000 Order-Statistics  10 19.46 18.54 18.94 19.24 19.27 19.42 19.43  50 22.27 20.71 19.46 18.98 18.79 18.79 18.91  100 23.81 19.06 17.04 15.91 15.75 15.69 15.73  Table Size 500 1000 23.85 23.96 14.94 14.58 13.63 13.30 12.52 12.30 12.26 12.26 12.76 12.14 12.79 12.18  5000 24.05 14.57 13.36 12.31 12.23 12.16 12.17  10000 24.06 14.52 13.35 12.29 12.22 12.15 12.17  50000 24.07 14.59 13.33 12.27 12.23 12.16 12.17  In the first set of experiments, we compare the recognition performance when different numbers of the histogram bins and different sizes of the look-up table are applied for THEQ. Notice that the equalization was conducted on all dimensions of the feature vectors for the training and test data, and the approximation of the CDFs of the test speech was conducted in an utterance-by-utterance manner. The results are summarized in Tables 1 and 2 for clean- and multi-condition training, respectively. As can been seen, the recognition performance is very sensitive to the number of the histogram bins and the size of the look-up table. The WER is improved when either the number of the histogram bins or the size of the look-up table is increased. As compared to the MFCC-based baseline system, the best results of HEQ yield about 60% and 16% relative WER improvements for clean- and multi-condition training, respectively. These results suggest that a larger histogram bin number or table size can improve the recognition performance, however, at the cost of huge consumption of the memory storage. Moreover, THEQ is also time-consuming, because a huge set of cumulative histograms of all speech feature vector dimensions of the training data have to be kept in memory for the table-lookup of restored feature values. Furthermore, the CDF value of a feature vector component approximated by the cumulative-histogram based method is equivalent to that done by the order-statistics based method when the number of histogram bins is taken to be infinite. In the next set of experiments, we investigate the use of different quantile numbers for QHEQ to see if the quantile number has any apparent effect on the recognition performance. The corresponding average WER results are shown in Table 3. As indicated by the results, it can be found the recognition performance is closely dependent on the quantile number. The transformation function H ( x) would tend to be too coarse to model the relationship between the test utterance and the training data when only few quantiles are being considered. On the contrary, the use of too many quantiles for the estimation of the transformation function  230  Shih-Hsiang Lin et al.  Table 3. Average WER results (%) of QHEQ, with respect to different quantile numbers.  2  Clean-Condition Training  24.02  Multi-Condition Training 11.63  3 23.67 11.25  Quantile Number  4  5  8  22.86 23.00 24.93  10.23 10.24 12.36  16 24.83 12.32  32 24.95 12.36  Table 4. Average WER results (%) of PHEQ, with respect to different orders of the polynomial transformation functions.  Polynomial Order  1-th 3-th 5-th 7-th 9-th 11-th 13-th  Clean-Condition Training  18.54 17.1 16.05 15.71 15.72 15.72 16.68  Multi-Condition Training 12.17 9.44 9.26 9.50 9.45 9.46 11.45  H ( x) might instead degrade the recognition performance [Hilger and Ney 2001]. However, the optimum number of quantiles is found to be four for the Aurora 2 task studied here, and the corresponding relative WER improvements over the MFCC-based baseline system are 50% and 30% for clean- and multi-condition training, respectively. In the third set of experiments, we evaluate the performance of PHEQ with respect to different polynomial orders and the associated results are presented in Table 4. Due to the end behavior property of polynomial functions, even order polynomials are either “up” on both ends or “down” on both ends which is not appropriate to characterize the behavior of a cumulative distribution [Lial et al. 2006]. Therefore, only odd-order polynomials are utilized in this paper for PHEQ. As evidenced by the results shown in Table 4, the average WER results of PHEQ are slightly improved when the order of the polynomial function becomes higher. However, as the order increases, the polynomial function might sometimes tend to over-fit of the training data. The improvement of PHEQ seems to saturate when the order is set to seven. As is indicated, PHEQ yields about a relative WER improvement of 65% for clean-condition training, and 35% for multi-conditions training, as compared to the MFCC-based baseline system. To go a step further, the average WER results under different SNR levels for the MFCC baseline, THEQ, QHEQ and PHEQ are shown in Tables 5 and 6, for clean- and multi-condition training, respectively. In the case of clean-condition training, these three HEQ approaches all yield significant improvement over the MFCC-based baseline, especially when the SNR level becomes much lower (e.g., 10 dB, 5 dB or 0 dB). The average WERs for  A Comparative Study of Histogram Equalization (HEQ) for  231  Robust Speech Recognition  Table 5. Average WER results (%) of the MFCC-based baseline system, THEQ, QHEQ and PHEQ for clean-condition training, with respect to different SNR levels.  MFCC THEQ QHEQ PHEQ  Clean 0.89 1.73 0.82 0.92  20 dB 7.55 3.61 2.05 1.83  SNR Level  15 dB 10 dB  20.41 43.17  5.69 10.22  4.14 10.84  3.45  7.52  5 dB 70.80 21.66 30.90 18.84  0 dB 90.21 47.41 66.11 45.78  -5 dB 96.37 77.91 86.72 76.77  Table 6. Average WER results (%) of the MFCC-based baseline system, THEQ, QHEQ and PHEQ for multi-condition training, with respect to different SNR levels.  MFCC THEQ QHEQ PHEQ  Clean 1.15 1.10 2.15 1.34  20 dB 2.16 2.24 2.02 1.65  SNR Level  15 dB 10 dB  3.22  5.97  3.53  6.52  2.74  5.10  2.43  4.19  5 dB 15.45 15.63 10.32 10.14  0 dB 44.06 40.60 29.46 27.96  -5 dB 79.24 73.39 57.96 62.13  clean-condition training are 18.02%, 15.71% and 22.86% for THEQ, PHEQ and QHEQ, respectively. In the case of multi-condition training, the average WER results for these three HEQ approaches are slightly better than that of the MFCC-based baseline system (average WERs of 12.30%, 9.5% and 10.23% for THEQ, PHEQ and QHEQ, respectively) which might mainly be due to the fact that with multi-condition training, the mismatch between the training and test conditions can be reduced to a great extent. On the other hand, Table 7 shows the average WER results obtained by combining PHEQ with different temporal average (TA) operations of different span orders. When the span order is set to 0, it denotes that only PHEQ was applied to the feature vector components. The results in Table 7 demonstrate that combining PHEQ with anyone of the TA operations can further provide an additional relative WER reduction of about 5% to 8%. In a word, the TA operations conducted after HEQ indeed provide a good compensation for non-stationary noises. Nevertheless, TA operations with much higher span orders may instead result in the degradation of the recognition performance.  232  Shih-Hsiang Lin et al.  Table 7. Average WER results (%) obtained by combining PHEQ with different TA operations of different span orders.  Span Order  0  
This paper, based on a selected one hour of expressive speech, is a pilot study on how to use breath segments to get more natural and expressive speech. It mainly deals with the status of when the breath segments occur and how the acoustic features are affected by the speaker ’s emotional states in terms of valence and activation. Statistical analysis is made to investigate the relationship between the length and intensity of the breath segments and the two state parameters. Finally, a perceptual experiment is conducted by employing the analysis results to synthesized speech, the results of which demonstrate that breath segment insertion can help improve the expressiveness and naturalness of the synthesized speech. Keywords: Breath Segment, Expressive Speech, Emotion, Valence, Activation 1. Introduction In the current speech synthesis and recognition systems, some characteristics of spontaneous speech are treated as noise, such as disfluent utterances, repeated sounds, filled pauses, salient breaths and coughs. In corpus collection for speech synthesis and recognition systems, the speaking style of the speakers is always strictly controlled and the speaker is usually required to give a “canonical pronunciation” to decrease the speaking noise as much as possible. However, in recent study, researchers have begun to pay more attention to the non-verbal information in natural speech, especially the paralinguistic and physiological information. They have focused on how to use these types of information to improve the naturalness and expressiveness of emotion and attitude in synthesized speech, so that the speaker ’s intention can be better understood during verbal communication. In 1989, Cahn compiled a simple feeling editor based on the phonetic characteristics of emotion [Cahn 1990]. Vroomen, Collier and Mozziconacci examined the duration and intonation of emotional speech and proposed that emotions can be expressed accurately by manipulating pitch and duration based on rules. This conclusion showed that, in emotional * Institute of Linguistics, Chinese Academy of Social Sciences, No. 5 Jianguomennei Dajie, Beijing, 100732 China E-mail: Yuanchu8341@gmail.com; liaj@cass.org.cn [Received August 12, 2006; Revised October 22, 2006; Accepted November 1, 2006]  18  Chu Yuan and Aijun Li  speech, duration and intonation can be employed to observe the speakers ’ attitude [Vroomen et al. 1993]. In 1998, Campbell found that if one compares the same content in different forms, for example, a news item in its read form, its formal spoken or broadcast form, and its informal conversational form, differences are obvious not only in lexis, word-order, chunking, and prominence relations, but also in the mood of the speaker and in the tone of the voice [Campbell 1998]. In 2000, the International Workshop on Speech and Emotion of ISCA (held in Ireland) invited, for the first time, researchers who were devoted to the study of emotion and speech. Before this conference, many researchers had begun to investigate the voice quality, prosodic features, and acoustic features of emotional speech. Alku and Vilkman designed an experiment to illustrate that the phonation types could be separated from each other effectively when the quantification was based on the parameters extracted from the instant of the maximal glottal opening and the minimal peak of the flow derivative [Alku et al. 1996]. Heuft, Portele, and Rauth carried out a more sophisticated test in order to determine the influence of the prosodic parameters in the perception of a speaker's emotional state in three different testing procedures. Their studies proved that the recognition rates were lower than those in the preliminary test, although the differences between the recognition rates of natural vs. synthetic speech were comparable in both tests. The outcome of the saw tooth test showed that the amount of information about the speaker's emotional state transported by F0, energy, and overall duration was rather small. However, the relations between the acoustic, prosodic parameters, and the emotional content of speech could be determined [Heuft et al. 1996]. Iida recorded a corpus of one speaker which included three kinds of emotion: anger, happiness and sadness. When synthesizing emotional speech, they picked up the corresponding emotional segments from the emotion corpus. The emotion speech, synthesized in this way, achieved a correct recognition rate 50% ~80% higher than through previous means [Iida et al. 2000]. Campbell focused on how to express a modal word in spontaneous speech with various emotions and attitudes [Campbell 2004]. Some researchers have also studied the non-verbal information in emotional speech. Trouvain attempted to analyze the terminological variety from a phonetic perspective. He proposed that the overview of various types of laughter indicated that further concepts of description were needed. In a pilot study on a small corpus of spontaneous laughter, the usefulness of the concepts and terms in practice was examined [Trouvain 2003]. In the light of the above overview of emotion speech research, this paper mainly discusses the function of the non-verbal information in natural speech, specifically the common non-verbal information which includes breath, laugh, filled pause, long silence, and cry. The breath segment is taken as an example to observe how the acoustic characteristics are related to prosodic structure, expressive valence, and activation through statistic analysis of  The Breath Segment in Expressive Speech  19  reading and spontaneous speech. The concluded rules are then applied to a perceptual experiment to see how it works. 2. Materials 2.1 Breath Segments This paper studies breath segments which appear in both read and spontaneous speech, as shown in Figures 1 and 2, annotated between two dotted lines in the read and spontaneous speech, respectively.  Figure 1. Breath segment in reading speech  Figure 2. Breath segment in spontaneous speech  The breath shown here is not the normal unconscious physiological exhalation or inspiration process but the deliberate breath for expressing a kind of emotion. Therefore, the following breath segment carries the emotional or attitudinal information of the utterance. Moreover, the acoustic features, such as the length and intensity of the breath segment, may be correlated to the emotional state in terms of valence and activation. Further, the small blanks preceding and following the breath segment which are caused by the physiological need of a breath segment may be inserted when the synthesis of emotional speech is conducted. The breath has two functions: fulfilling the physiological requirement of the intake of air and the expression of emotion or attitude. The authors determine the activation and valence degrees for each recitation of each phrase and use the information to label the breath segment before this phrase.  2.2 The Corpus and Annotation The corpus used in this paper is called CASS-EXP which includes read and spontaneous speech. The first part contains some stories read by actors and actresses in emotional and neutral states while the second part includes TV and radio programs along with spontaneous speech: monologues and dialogues.  20  Chu Yuan and Aijun Li  SAMPA-C [Li 2002] and C-ToBI [Chen et al. 2000] are adopted to label segmental and prosodic information. Furthermore, the starting and ending points of breath segments in terms of valence and activation degrees are labeled as well. The authors labeled the emotion characteristics of the breath segments based on two factors: valence and activation. The theoretical foundation of valence is the concept of a separation of positive or negative emotion. The function of activation is the enabled degree of energy which is in contact with the emotion condition. The activation and valence of one breath segment here refer to the activation and valence of the following intonational phrase. Emotional valence is categorized into three levels: positive (1), neutral (0) and negative (-1). The activation has three categories as well: excited (1), steady (0) and low (0). When both the emotional valence and activation of a certain breath segment are marked as 0, the breath segment is considered to be a neutral physiological segment without carrying any expressive information. Three boundary levels (break index) 1, 2, 3 are annotated which stand for prosodic word, minor prosodic phrase, and major prosodic phrase (intonational phrase), respectively. The authors intend to examine whether the breath segment occurs in a normal stop or in an unexpected position. The normal stop refers to the breath at a prosodic phrase boundary, and the unexpected or abnormal position is the breath at a prosodic word boundary or within a prosodic word. 3. Breath Segments in Read Speech From CASS-EXP, the authors select fifteen fragments from a read story which have different emotional states and attitudes. The valence and activity of nine fragments were labeled. 3.1 Occurring Number and Position of the Breath Segments Based on what has been labeled, the number of breath segments is calculated for neutral and expressive speech. It was found that the number of breath segments in expressive speech is 50% higher than in that of neutral read speech in the same text. In these nine fragments, the number of breath segments in expressive speech is 334, and only one of them appears in an abnormal stop; the number in neutral speech is 225, of which all appear in normal boundaries, as shown in figure 3.  The Breath Segment in Expressive Speech  21  Number of breath segments  400 350 300 250 200 150 100 50 0 Expressive  Neutral  Figure 3. Number of breath segments in expressive and neutral read speech. In fragments of read form, most of the breath segments occur at boundary 3 (intonational phrase boundary). The number of the breath segments at boundary 1 (prosodic word boundary) is the smallest, as shown in Figure 4. Table 1 demonstrates that the boundary distribution of breath segments appearing in expressive speech and neutral speech exhibits no difference. In expressive and neutral speech, the number of breath segments at boundary 1 is the smallest, and the number of breath segments at boundary 3 is the largest.  300 250 200 150 100 50 0 Expressive  Neutral  boundary 3 boundary 2 boundary 1  Figure 4. The number of breath segments at the different boundaries.  Table 1. Number and percent of breath segments of emotion and neutral read speech at the different boundaries.  Boundary 3 2 1  Number of breath segments in expressive speech 284 43 7  Percent 85.2% 12.8% 2%  Number of breath segments in neutral speech 210 13 2  Percent 93.3% 5.8% 0.9%  22  Chu Yuan and Aijun Li  In general, breath segments in read speech, either expressive or neutral, usually appear between two prosodic phrases, especially between two intonational phrases. From the perspective of syntactic analysis, most of the breath segments appear between two intonational phrases or two intonational phrase groups.  We measured the duration of the silence which was between the breath segment and the prosodic phrase following this breath segment. The mean duration of the silence in different valence and activity is shown in Table 2.  Table 2 .The mean duration of the silence in different valence and activity  Emotional Neutral  -1 64ms  Valence 0 54ms 48ms  
The speech fundamental frequency (henceforth F0) contour plays an important role in expressing the affective information of an utterance. The most popular F0 modeling approaches mainly use the concept of separating the F0 contour into a global trend and local variation. For Mandarin, the global trend of the F0 contour is caused by the speaker’s mood and emotion. In this paper, the authors address the problem of affective intonation. For modeling affective intonation, an affective corpus has been designed and established, and all intonations are extracted with an iterative algorithm. Then, the concept of eigen-intonation is proposed based on the technique of Principal Component Analysis on the affective corpus and all the intonations are transformed to the lower-dimensional eigen sub-space spanned by eigen-intonations. A model of affective intonations is established in the sub-space. As a result, the corresponding emotion (maybe a mixed emotion) can be expressed by speech whose intonation is modified according to the above model. The experiments are performed with the affective Mandarin corpus, and the experimental results show that the intonation modeling approach proposed in this paper is efficient for both intonation representation and speech synthesis. Keywords: Eigen-Intonation, Affective Speech, Mixed Emotion, F0 Contour, Speech Synthesis 1. Introduction Speech can convey not only literal meanings, but also the mood and emotion of a speaker. Some researchers have proven that the contour of the speech fundamental frequency (henceforth F0 contour) plays an important role in expressing the affective information of an utterance. It is concluded that some statistical characteristics of F0 play the most important roles in emotion perception [Tao and Kang 2005]. Especially, F0 contours differ from each ∗ Department of Automation, University of Science and Technology of China, Hefei 230027, China E-mail: zfwang@ustc.edu.cn The author for correspondence is Zengfu Wang. [Received September 16, 2006; Revised October 10, 2006; Accepted November 1, 2006]  34  Zhuangluan Su and Zengfu Wang  other because of the speaker’s different emotion in Mandarin [Yuan et al. 2002]. Due to significance of F0, the F0 contour modeling is one of the key issues that should be addressed. The most popular F0 modeling approaches mainly use the concept of separating the F0 contour into a global trend and local variation [Abe and Sato 1992; Bellegarda et al, 2001]. Mandarin is a tonal language including four basic tone types and a so-called ‘light’ tone. The F0 contour is composed of three elements [Zhao 1980]: the tone of the syllable, the variety of tone in continuous utterance, and the movement influenced by mood. How to extract tones and intonations from speech is a difficult problem. Tian and Nurminen have proposed a data-driven tone modeling approach to describe the tonal element [Tian and Nurminen 2004]. In previous work [Su and Wang 2005], the authors of this paper also proposed an affective-tone modeling approach for Mandarin to separate F0 contour into two elements: variational tones based on syllables and intonations for prosody phrases. In this paper, the authors propose a data-driven intonation modeling approach based on Principal Component Analysis (henceforth, PCA [Fukunaga 2000]). For modeling affective intonations, an affective corpus of Mandarin has been designed and the corresponding intonations are extracted with an iterative algorithm from the original speech. The eigen-intonation concept is proposed based on the principal components of the above intonations obtained from the affective corpus, and all the intonations are then transformed into the sub-space spanned by the eigen-intonations. The distribution of affective intonations corresponding to an emotion in the above sub-space is a help to establish the corresponding affective intonation model. As a result, speech whose intonation is modified according to the model can express the corresponding emotion, even mixed emotions. In addition, the authors will also show emotion perception results using the proposed modeling approach. The remainder of the paper is organized as follows. The speech corpus and some statistic results of F0 based on the database are described first. Then, the algorithm of eigen-intonation extraction is described, and some of the basic properties of the eigen-intonation representation are concluded. Next, how to model the affective intonation is discussed. Last, the performance of the proposed modeling approach is given by experimental results. 2. Speech Corpus and Statistic Results of F0 Carrying on the affective speech research, a reasonable classification of the emotion is needed first, and then the speech features with different emotions can be analyzed effectively. In emotional psychology, Robert Plutchik proposed a four pair emotional ring constructed of eight pure emotions, including anger, joy, acceptance, surprise, fear, sadness, hatred and expectation. In the affective speech research for Mandarin, four emotions are generally selected, either including anger, joy, fear, sadness [Yuan et al. 2002; Tao and Kang 2005], or including anger, joy, surprise and sadness [Zhao et al. 2004]. In contrast, five emotions are selected for this paper, and  Affective Intonation-Modeling for Mandarin Based on PCA  35  they are anger, joy, surprise, fear and sadness. What is discussed in this paper is the global variety of the F0 contour, so a reasonable duration of the target needs to be considered. Due to the multi-level structure of prosody [Abney 1995; Li et al. 2000], a complicated sentence with many syllables can be divided into several simple prosody units with fewer syllables at prosody boundaries. So, studying intonation based on prosody units can transform this complicated problem into several simple ones. Moreover, it is known that prosodic phrases can keep a relatively stable intonation pattern. Therefore, the authors model intonation based on prosodic phrases in the paper. It is known that F0 contour is influenced by several factors, including syntax, stress, speaker’s emotion and his or her individual character. This paper focuses on the movement of intonation caused by emotion, and the influence of other factors such as syntax, stress, and the individual characters will not be considered. Currently, there are no effective methods that can eliminate the influence of these factors from the original speech signals directly, so the corpus used in the paper are obtained in such a way as to avoid these interferential factors’ influence. To avoid unwanted factors’ influence and to simplify the following processing, the corpus is designed with some limitations. The authors have designed 40 sentences with different literal contents for the following test, and each sentence only consists of three components: subject, verb, and object. Furthermore, the subject, verb, and the object are all designed to be disyllabic words. So, each sentence only has 6 syllables in this case, and all of these sentences have the same syntax. As the length of a prosodic phrase is approximately six syllables [Zhao et al. 2002], each sentence consists of only one prosodic phrase. An example of such a sentence is given by “北京召开奥运”. This design can be advantageous to the following experiments, and the model will be established directly based on one sentence. Each sentence is then performed by a female actor with all six emotions, including fear, sadness, neutral, anger, joy and surprise. In the end, the corpus used for analysis contains 240 total sentences, consisting of 1,440 syllables from a single speaker, with same syntax and the same individual characters. The speech signals are digitized at 16 kHz with 16-bit precision. To evaluate the representational ability of the corpus, some experiments about the distributions of F0 are performed. Here, the F0 of a speech is extracted by using a modified autocorrelation algorithm. The results are demonstrated in Figure 1. Figure 1 shows that “surprise”, “happy” and “angry” make a very high F0, while “sad” generates lower value than the neutral state. It can also be found that the varying range of “sad” is smaller than the others. F0 parameters of “fear” make quite similar behaviors as “sad”. “Angry”, “happy”, and “surprise” also behave similarly. All of the results accord with the conclusions given by other researches [Yuan et al. 2002; Zhao et al. 2004; Tao and Kang 2005]. So the speech corpus is representational and effective for the following analysis.  36  Zhuangluan Su and Zengfu Wang  Figure 1. Statistic results for F0 with different emotional states 3. Concept of Eigen-Intonation The affective intonation will be modeled with a concept called “eigen-intonation”. The concept of eigen-intonation is derived through the use of the PCA technique. PCA [Fukunaga 2000] is a multivariate analysis method that carries out a compact description of a data set. In a PCA process, a set of correlated variables is transformed into a set of uncorrelated variables that are ordered by reducing variability, and these new uncorrelated variables are linear combinations of the original variables. It can be concluded that the first new variable contains the greatest amount of variation; the second contains the next greatest residual variance and orthogonal to the first, and so on. Thus, the last of these variables can be removed with a minimal loss of real data. With the affective corpus in the paper, the speech intonations for sentences should be very similar in all configurations, and they should be able to be described by some “basic intonations”. From the previous description, one knows that one of the main functions of PCA is that it can be used to extract new uncorrelated features from original data. According to these ideas, one can find the “basic intonations” that best account for distribution of speech intonations within the entire intonation space using the principal components analysis. The “basic intonations” are called “eigen-intonations”. With eigen-intonation, original intonations can be transformed to corresponding representations with lower dimensions. Some rules can also be possibly given out in the low-dimensional space. Moreover, the resultant rules with low dimensions have simpler expression, and it is advantageous to control the rules for the goal of this study.  Affective Intonation-Modeling for Mandarin Based on PCA  37  4. Analysis for Eigen-Intonation The concept of eigen-intonation is proposed based on PCA technique. Mathematically, the principal component analysis involves an eigen analysis on a covariance matrix. A good low-dimensional representation in the space of possible speech intonations can be achieved by considering only a few principal components or eigenvectors, corresponding to the first largest eigenvalues. 4.1 Extraction of Intonation In order to obtain the intonation of a speech, the F0 contour of the speech should be extracted first. After that, the F0 contour will be separated into a global variety, which is regarded as intonation, and rapidly-varying components corresponding to local changes based on syllables. The details of intonation extraction are described in the following. The entire intonation extracting algorithm can be divided into five main steps: 1) Estimating initial F0 values based the modified normalized autocorrelation from voiced regions of the original speech. 2) Cubic Hermite interpolating for unvoiced regions and obtaining a continuous F0 curve. 3) Filtering the continuous F0 contour with two serial modified smoothing processes. 4) Applying piecewise three-order polynomial iterative fitting to the entire F0 contour, the n-th iterative processing step is as: (a) Fitting the entire F0 contour with n pieces of cubic polynomial. (b) Calculating the fitting error En. (c) If En < Et, ending the iterative algorithm and taking n pieces of cubic polynomial fitting as final resultant F0 contour. Else, n = n + 1, go to (a). Where Et is a given threshold of maximal fitting error. 5) The ln(F0) contour is passed through a high-pass filter with a stop frequency at 0.5Hz, and the residual low frequency contour after filtering is denoted as LF contour. From the authors’ previous work [Su and Wang 2005], The LF contour can be regarded as the F0 global variety of a speech. As all sentences have the same syntax and each sentence consists of only one prosodic phrase in this corpus, the model can be established directly based on one sentence. It is to say that the resultant LF contour of the algorithm for each sentence in the corpus is the modeling target, intonation based prosodic phrase (henceforth intonation). Finally, each intonation is normalized into an N-dimensional vector (N = 100 in the paper).  38  Zhuangluan Su and Zengfu Wang  4.2 PCA for Intonation  Let the data set of intonations be I1, I2, … IM, where Ii is an N-dimensional intonation sample, and M is the number of intonations (M = 240 in the paper). Then the intonation covariance matrix CN × N is computed by (1).  C  =  
The paper presents an emotional speech recognition system with the analysis of manifolds of speech. Working with large volumes of high-dimensional acoustic features, the researchers confront the problem of dimensionality reduction. Unlike classical techniques, such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), a new approach, named Enhanced Lipschitz Embedding (ELE) is proposed in the paper to discover the nonlinear degrees of freedom that underlie the emotional speech corpus. ELE adopts geodesic distance to preserve the intrinsic geometry at all scales of speech corpus. Based on geodesic distance estimation, ELE embeds the 64-dimensional acoustic features into a six-dimensional space in which speech data with the same emotional state are generally clustered around one plane and the data distribution feature is beneficial to emotion classification. The compressed testing data is classified into six emotional states (neutral, anger, fear, happiness, sadness and surprise) by a trained linear Support Vector Machine (SVM) system. Considering the perception constancy of humans, ELE is also investigated in terms of its ability to detect the intrinsic geometry of emotional speech corrupted by noise. The performance of the new approach is compared with the methods of feature selection by Sequential Forward Selection (SFS), PCA, LDA, Isomap and Locally Linear Embedding (LLE). Experimental results demonstrate that, compared with other methods, the proposed system gives 9%-26% relative improvement in speaker-independent emotion recognition and 5%-20% improvement in speaker-dependent recognition. Meanwhile, the proposed system shows robustness and an improvement of approximately 10% in emotion recognition accuracy when speech is corrupted by increasing noise.  * College of Computer Science, YuQuan Campus, ZheJiang University, Hangzhou 310027, CHINA E-mail: {roseyoumy, chenc, bjj, liujia}@zju.edu.cn The author for correspondence is Jiajun Bu. + National Laboratory of Pattern Recognition, Chinese Academy of Sciences, Beijing 100080, CHINA E-mail: jhtao@nlpr.ia.ac.cn [Received August 10, 2006; Revised September 11, 2006; Accepted September 30, 2006]  50  Mingyu You et al.  Keywords: Enhanced Lipschitz Embedding (ELE), Dimensionality Reduction, Emotional Speech Analysis, Emotion Recognition 1. Introduction Human-machine interaction technology has been investigated for several decades. Recent research has put more emphasis on enabling computers to recognize human emotions. As the most effective method in human-human and human-machine communication, speech conveys vast emotional information. Accurate emotion recognition from speech signals will benefit the human-machine interaction and will be applied to areas of entertainment, learning, social development, preventive medicine, consumer relations, etc. [Picard 1997]. The general process of emotion recognition from speech signals can be formulated as below: extracting acoustic features such as Mel-Frequency Cepstral Coefficient (MFCC), Linear Predictive Cepstral Coefficient (LPCC) or low-level features [Ververidis et al. 2004], reducing feature dimensionality to an appropriate range for less computational complexity and recognizing emotions with trained SVM, Hidden Markov Model (HMM), Neural Network (NN) or other classifiers. Dimensionality reduction methods can be grouped into two categories: Feature Selection (FS) and Feature Extraction (FE). An FS method chooses a subset from the original features, preserving most characteristics of the raw data. Ververidis [Ververidis et al. 2004] used the Sequential Forward Selection (SFS) method to select the five best features for the classification of five emotional states. However, feature selection needs complex computation to evaluate all the features. How to acquire the best feature set is another tough task. An FE method projects the original features to a completely new space with lower dimensionality through linear or nonlinear affine transformation. PCA, LDA and Multidimensional Scaling (MDS) are popular feature extraction techniques. PCA finds a set of the most representative projection vectors such that the projected samples retain the most information about the original samples. Lee [Lee et al. 2002] used PCA to analyze the feature set in classifying two emotions in spoken dialogs. Chuang [Chuang et al. 2004] adopted PCA to select 14 principle components from 33 acoustic features in the analysis of emotional speech. LDA uses the class information and finds a set of vectors that maximize the between-class scatter while minimizing the within-class scatter. MDS computes the low dimensional representation of a high dimensional data set that most faithfully preserves the inner products between different input patterns. LDA and MDS have also been employed to reduce the feature dimensionality for emotion recognition [Go et al. 2003]. Though widely used for their simplicity, PCA, LDA and MDS are limited by their underlying assumption that data lies in a linear subspace. For nonlinear structures, these methods fail to detect the true freedom degrees of the data.  Manifolds Based Emotion Recognition in Speech  51  Recently, a number of research efforts have shown that the speech points possibly reside on a nonlinear submanifold [Jain et al. 2004; Togneri et al. 1992]. The classical ways of projecting speech into low dimensional space by linear methods are not suitable. Some nonlinear techniques have been proposed to discover the nonlinear structure of the manifold, e.g. Isomap [Tenenbaum et al. 2000] and LLE [Roweis et al. 2000]. Isomap is based on computing the low dimensional representation of a high dimensional data set that most faithfully preserves the pairwise distances between input patterns as measured along the submanifold from which they are sampled. The LLE method captures the local geometry of complex embedding manifold by a set of linear coefficients that best approximate each data point from its neighbors in the input space. These nonlinear methods do yield impressive results in some statistical pattern recognition applications [Jain et al. 2004]. However, they yield maps that are defined only on the training data points, so how to evaluate the maps on novel testing data points remains unclear. Lipschitz embedding [Bourgain 1985; Johnson et al. 1984] is another nonlinear dimensionality reduction method which works well when there are multiple clusters in the input data [Chang et al. 2004]. It is suitable for emotion classification whose input data can be grouped into several emotions. Most previous work on detecting emotional states investigated speech data recorded in a quiet environment [Song et al. 2004; Zeng et al. 2005], but humans are able to perceive emotions even in a noisy background. The nonlinear manifold learning algorithms mentioned above [Tenenbaum et al. 2000; Roweis et al. 2000; Bourgain 1985] try to discover the underlying reason of how humans perceive constancy even though the raw sensory inputs are in flux. Facial images with different poses and lighting directions were also observed to make a smooth manifold [Tenenbaum et al. 2000]. Similarly, speech with different emotions, even corrupted by noise, could also be embedded into a low dimensional nonlinear manifold, although none of the previous work has paid attention to this area. In this paper, an enhanced Lipschitz embedding system is developed to analyze the intrinsic manifold of both emotional speech recorded in quiet environment and those corrupted by noise. Geodesic distance is expected to reflect the true geometry of the emotional speech manifold. With geodesic distance estimation, ELE is developed to embed the extracted acoustic features into a low dimensional space. Then, a linear SVM is trained to recognize the emotional states of the embedded results. In addition, other dimensionality reduction methods such as PCA, LDA, feature selection by SFS with SVM, Isomap, and LLE are implemented for comparison. The rest of the paper is organized as follows. Section 2 gives a brief description of the emotional speech recognition system. Section 3 presents the ELE algorithm. Experimental results are provided and discussed in Section 4. Section 5 concludes the paper and discusses future work.  52 2. System Overview  Mingyu You et al.  Figure 1. The Framework of Emotion Recognition from Speech Figure 1 displays the overall structure of this system. Clean speech from the database and speech corrupted by generated noise are both investigated in the system. The emotional speech analysis is done in two phases in this system: training and testing. In the training phase, 64-dimensional acoustic features for each training utterance are obtained after feature extraction. Using ELE, a six-dimensional submanifold is then gained to embody the intrinsic geometry of the emotional training data. Finally, a linear SVM is trained by the embedded training data. In the testing phase, the feature extraction method also extracts 64-dimensional acoustic features for the testing data. The high-dimensional features are then projected into the six-dimensional manifold obtained in the training phase. The emotional state of the testing data is recognized by the trained SVM system. There are two feature spaces mentioned in the workflow: the original acoustic feature space, which is a high-dimensional space found before feature embedding and the embedded space, which is a low-dimensional space found after feature projection.  3. Enhanced Lipschitz Embedding (ELE)  A Lipschitz embedding is defined in terms of a set R(R = {A1, A2 ,L, Ak }) , where Ai Ì S  and  U  k i =1  Ai  =  S  .  The  subset  A i  is termed the reference set of the embedding. Let  d (o, A)  be an extension of the distance function d from object o to a subset A Ì S , such that  d (o, A) = min xÎA d (o, x) . An embedding with respect to R is defined as a mapping F such that F (o) = (d (o, A1), d (o, A2 ),L, d (o, Ak )) . In other words, Lipschitz embedding defines a coordinate space where each axis corresponds to a subset Ai Ì S and the coordinate values of object o are the distances from o to the closest element in each Ai .  Manifolds Based Emotion Recognition in Speech  53  The distance function d in Lipschitz embedding reflects the essential structure of data set. Due to the nonlinear geometry of the speech manifold, Euclidean distance fails to find the real freedom degrees of the manifold. Tenenbaum et al. [Tenenbaum et al. 2000] tried to preserve the intrinsic geometry of the data by capturing the geodesic distances between all pairs of data points, which is followed by the algorithm found in this research.  In this new approach, the speech corpus is divided into six subsets {A1, A2 ,L, A6} according to six emotional states (neutral, anger, fear, happiness, sadness and surprise). Object o of speech corpus is embedded into a six-dimensional space where the coordinate values of o are obtained from the process below.  (1) Construct a graph G connecting neighbor data points. The edge length is determined by the Euclidean distance between neighbor points. The detailed operation can be formulated as Equation (1). Initiate element mij in matrix M:  mij = {C:åel6¶s4=e1 ( x¶ - y¶ )2 :"i, jÎKNN  (1)  Here, mij stands for the geodesic distance from point i to j . i, j Î KNN means that j is among the k nearest neighbors of i . Specifically, k is set to 10 in this method, which will be discussed further in the following section. i and j are data points in the 64-dimensional feature space, i = [x1, x2 ,L, x64 ] and j = [ y1, y2 ,L, y64 ] . C is a very large constant which guarantees that i and j are unconnected in the graph G consisting of speech data points. Matrix M actually corresponds to the neighborhood graph G whose edge only connects neighbor data points. (2) Reconstruct matrix M. Replace element mij with the length of the shortest path between data point i and j in graph G. The shortest path between i and j can be found by tracing through the edges in graph G.  mij = min{mij , mik + mkj }  (2)  Matrix M contains the shortest path distances between all pairs of points in graph G  constructed in Equation (1).  (3) Get the coordinate values of o({o1 , o2 ,L, o6 }) from matrix M. The coordinate value of object o to axis Ai is the distance from o to the closest element in Ai .  or  =  min m Î Ar  mom  (3)  54  Mingyu You et al.  where mom is an element of matrix M. In this work, object o is projected into a space with six axes {A1, A2 ,L, A6} in accordance with the six emotional states.  Figure 2. Training data in the embedded space. Different colors correspond to different emotions. Figure 2 shows the six-dimensional embeddings of 64-dimensional training speech corpus in the six emotional states. Figure 2(a) reveals the first three dimensions of the embedded space and (b) displays the other three dimensions. Emotions neutral, anger and fear, denoted by points in red, green and blue, are easy to be separated in the first three dimensions. Happiness, sadness and surprise, denoted by light blue, yellow and pink are separable in the last three dimensions, though they are mixed in Figure 2(a). Actually, points of the same emotional state are highly clustered around one plane in the embedded space. The distribution property of data points in the six-dimensional space indicates that they can be easily classified into six clusters. In the proposed ELE technique, the distance matrix M is constructed on training data. The training data projection easily depends on the minimal distance to each emotional speech class. Similar to Isomap and LLE, how to evaluate new testing data is still unclear. It is impossible to reconstruct matrix M combining the testing data because it is time consuming. Based on the constructed matrix M, the authors propose an approach to compute the coordinate values of testing data t in the embedded space. (1) Based on Euclidean distance, the k nearest neighbors ({n1, n2 ,L, nk }) , with distances {d1, d2 ,L, dk } , of testing data t are found in the training data set.  Manifolds Based Emotion Recognition in Speech  55  (2) Get the coordinate values ({v1n , vn2 ,L, vn6}kn=1) of the k neighbors from matrix M. The k nearest neighbors come from the training data, so their coordinates can be found with the processes mentioned in the training phase.  (3) Compute the coordinate values of testing data t ({t1,t2 ,L,t6}) . In this approach, the testing data t makes the shortest paths to subsets through its neighbors. Therefore, the geodesic distances of t to subsets can be approximated by averaging the sum of “short hops” to its neighboring points and the geodesic distances of its neighbors.  ti  =  
This paper presents an approach to feature compensation for emotion recognition from speech signals. In this approach, the intonation groups (IGs) of the input speech signals are extracted first. The speech features in each selected intonation group are then extracted. With the assumption of linear mapping between feature spaces in different emotional states, a feature compensation approach is proposed to characterize feature space with better discriminability among emotional states. The compensation vector with respect to each emotional state is estimated using the Minimum Classification Error (MCE) algorithm. For the final emotional state decision, the compensated IG-based feature vectors are used to train the Gaussian Mixture Models (GMMs) and Continuous Support Vector Machine (CSVMs) for each emotional state. For GMMs, the emotional state with the GMM having the maximal likelihood ratio is determined as the final output. For CSVMs, the emotional state is determined according to the probability outputs from the CSVMs. The kernel function in CSVM is experimentally decided as a Radial basis function. A comparison in the experiments shows that the proposed IG-based feature compensation can obtain encouraging performance for emotion recognition. Keywords: Emotional Speech, Emotion Recognition, Intonation Group, Feature Compensation 1. Introduction Human-machine interface technology has been investigated for several decades. Recent research has put more emphasis on the recognition of nonverbal information, especially on the topic of emotion reaction. Scientists have found that emotional skills can be an important component of intelligence, especially for human-human communication. Although human-computer interaction is different from human-human communication, some theories ∗ Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan, ROC E-mail: {chwu, bala}@csie.ncku.edu.tw [Received September 8, 2006; Revised September 20, 2006; Accepted October 10, 2006]  66  Chung-Hsien Wu and Ze-Jing Chuang  have shown that human-computer interaction essentially follows the basics of human-human interaction [Reeves et al. 1996; Picard 1997; Cowie et al. 2001]. Scientists have found that emotion technology can be an important component in artificial intelligence, especially for human-human communication [Salovey et al. 1990]. Although the human-computer interaction is different from human-human communication, some theories show that human-computer interaction basically follows the fundamental forms of human-human interaction [Reeves et al. 1996]. In this study, an emotion recognition approach from speech signals is proposed. This method consists of the definition and extraction of intonation groups (IGs), IG-based feature extraction, and feature compensation. In past years, many researchers have paid attention to emotion recognition via speech signals. Several important recognition models have been applied to the emotion recognition task, such as Neural Network (NN) [Bhatti et al. 2004], Hidden Markov Model (HMM) [Inanoglu et al. 2005], Support Vector Machine (SVM) [Kwon et al. 2003; Chuang et al. 2004], and others [Subasic et al. 2001; Wu et al. 2006; Silva et al. 2000]. Besides the generally used prosodic and acoustic features, some special features are also applied for this task, such as TEO-based features [Rahurkar et al. 2003]. Although lots of features and recognition models have been tested in these works, large overlaps between the feature spaces for different emotional states is rarely considered. Besides, the pre-trained emotion recognition model is highly speaker-dependent [Chuang et al. 2004; Wu et al. 2004]. To solve the above questions, this paper proposes an approach to emotion recognition based on feature compensation. The block diagram of the approach is shown in Figure 1. The feature extraction process is shared by the training and testing phase and is divided into two steps: intonation group (IG) identification and IG-based feature extraction. In order to identify the most significant segment, the intonation groups (IGs) of the input speech signals are first extracted. Following the feature extraction process [Deng et al. 2003], the prosodic feature sets are estimated for the IG segments. Then, in training phase, the extracted feature vectors are applied for compensation vector estimation. All the feature vectors compensated by compensation vectors are modeled by a Gaussian Mixture Model (GMM). Finally, the minimum classification error (MCE) training method [Wu et al. 2002] iteratively estimates all the model parameters. As a comparison, the compensated vectors are also used to train the Continuous Support Vector Machine (CSVM) model. In the testing phase, the extracted feature vectors are directly compensated using the compensation vectors. Then, the final emotional state is decided using the CSVM model. The rest of the paper is organized as follows. Section 2 describes the definition of Intonation Group and the extraction of the prosodic features. Then the feature compensation technique and MCE training is provided in Section 3. The model description of CSVM is shown in Section 4. Finally, experimental results and conclusions are drawn in Section 5 and 6,  Emotion Recognition from Speech Using IG-Based Feature Compensation  67  respectively.  Figure 1. Block diagram of the proposed emotion recognition approach 2. IG-Based Feature Extraction 2.1 Intonation Group Extraction The intonation group, also known as breath-groups, tone-groups, or intonation phrases, is usually defined as the segment of an utterance between two pauses.  Frequency  • Type 2 • Selected  • Type 1 • Selected Time  • Type 1 • Nonselected  • Type 3 • Selected  Figure 2. An illustration of the definition and extraction of Intonation Groups. Four IGs are extracted from the smoothed pitch contour (the gray-thick line), but only three IGs (the first, second, and forth IGs) are selected for feature extraction  68  Chung-Hsien Wu and Ze-Jing Chuang  As shown in Figure 2, the intonation group is identified by analyzing the smoothed pitch contour (the gray-thick line in Figure 2). Three types of smoothed pitch contour patterns are defined as the intonation group: • Type 1:a complete pitch segment that starts from the point of a pitch rise to the point of the next pitch rise, • Type 2: a monotonically decreasing pitch segment, • Type 3: a monotonically increasing pitch segment. For all identified IG segments, only those IGs that match the following criterion are selected for feature extraction: • the complete IGs with the largest pitch range or duration, • the monotonically decreasing or increasing IGs with the largest pitch range or duration, • the monotonically decreasing or increasing IGs at the start or end of a sentence. In Figure 2, the numbers before the slash symbol indicate the type of IG, and the symbol S and NS indicate “Selected” and “Not-Selected” IGs, respectively. Although there are four IGs extracted, only three IGs are selected for feature extraction. 2.2 IG-Based Feature Extraction Emotional state can be characterized by many speech features, such as pitch, energy, or duration [Ververidis et al. 2005]. In this paper, the authors use the following 64 prosodic features as the input features for emotion recognition: • Speaking rate and relative duration (2 values). The relative duration is normalized with respect to the length of the input sentence. • Pause number and relative pause duration (2 values). The relative duration is normalized with respect to the length of intonation group. The same definition of relative position and relative duration are made in the following features. • Mean and standard deviation of pitch, energy, zero-crossing-rate, and F1 values (8 values). • Mean and standard deviation of jitter (for pitch) and shimmer (for energy) (4 values). • Maximum and minimum of pitch, energy, zero-crossing-rate, and F1 values (8 values). • Relative positions at which the maximal pitch, energy, zero-crossing-rate, and F1 value occur (4 values). • Relative positions at which the minimal pitch, energy, zero-crossing-rate, and F1 value occur (4 values). • Fourth-order Legendre parameters of pitch, energy, zero-crossing-rate, and F1 contours of the whole sentence (16 values).  Emotion Recognition from Speech Using IG-Based Feature Compensation  69  • Fourth-order Legendre parameters of pitch, energy, zero-crossing-rate, and F1 contours inside the “significant segment,” which is the segment during the positions of maximum and minimum values (16 values).  The definitions of jitter and shimmer are given in [Levity et al. 2001]. Jitter is a variation of individual cycle lengths in pitch-period measurement, while shimmer is the measure for energy values. The calculation of the jitter contour is shown as:  m  Ji  =  ∑ u=0  fi − ugu fi  ,  g  =  
An effective method based on GMM is proposed in this paper for speech emotional recognition; a compensation transformation is introduced in the recognition stage to reduce the influence of variations in speech characteristics and noise. The extraction of emotional features includes the globe feature, time series structure feature, LPCC, MFCC and PLP. Five human emotions (happiness, angry, surprise, sadness and neutral) are investigated. The result shows that it can increase the recognition ratio more than normal GMM; the method in this paper is effective and robust. Key words: Speech Emotional Recognition (SER), GMM, Emotion Recognition, Compensation Transformation 1. Introduction One of the natural goals for research on speech signals is recognizing emotions of humans [Chen 1987; Oppenheim 1976; Cowie 2001]; it has gained growing amounts of interest over the last 20 years. A study conducted by Shirasawa et al. showed that SER could be made by ICA and attain an 87% average recognition ratio [Shirasawa 1997; Shirasawa 1999] Many studies have been conducted to investigate neural networks for SER. Chang-Hyun Park tried to recognize sequentially inputted data using DRNN in 2003[Park et al. 2003], Muhammad, W. B. obtained about 79% recognition rate using GRNN [Bhatti et al. 2004]. Aishah Abdul Razak achieved an average recognition rate of 62.35% using combination MLP [Razak et al. 2005]. Fuzzy rules are also introduced into SER such that an 84% rate has been achieved in recognizing anger and sadness [Austermann et al. 2005]. A number of studies in SER have * Foshan University, Foshan, 528000, Guangdong, China + Research Center of Learning Science, Southeast University, Nanjing, 210096, China E-mail: zhaoli@seu.edu.cn [Received August 11, 2006; Revised September 30, 2006; Accepted October 22, 2006]  80  Cairong Zou et al.  also been done with the development of GMM/HMM [Rabiner 1989; Jiang et al. 2004; Lin et al. 2005]. However, in SER, the variations in speech characteristics, noise and individual differences always influence the recognition results. In addition, the methods above have always handled such problems in the preprocessing stage and have not been able to eliminate the influence effectively. Therefore, a valid solution has still not been proposed. In this paper a compensation transformation is introduced into an algorithm for GMM which operates in the recognition module. The experiments with five emotions (happiness, angry, neutral, surprise and sadness) show that the method in this paper is effective in emotional recognition.  2. Descriptions of Emotion and Selection of Emotion Speech Materials  Usually, emotions are classified into two main categories: basic emotions and derived emotions. Basic emotions, generally, can be found in all mammals. Derived emotions mean derivations from basic emotions. One viewpoint is that the basic emotions are composed by the basic mood. Due to different research backgrounds, different researchers have expressed different definitions of basic emotions. Some of the major definitions [Ortony et al. 1990] of the basic emotions are shown in Table 1.  Table 1. Researches about basic emotions definition  Researchers  definitions  Plutchik  Acceptance, joy, anger, anticipation, disgust, fear, sadness, surprise  Ekman/Friesen/ Ellsworth  Anger, disgust, fear, joy, sadness, surprise  James  Fear, grief, love, rage  Izard  Anger, contempt, disgust, distress, fear, guilt, interest, joy, shame, surprise  Oatley/Johnson -Laird  Anger, disgust, anxiety, happiness, sadness  Panksepp Weiner/Graham  Expectancy, fear, rage, panic Happiness, sadness  The common emotion classification which was proposed by Plutchik is shown in Figure 1e. In this paper, the authors only recognize five kinds of emotion.  Joy  Acceptance  Anticipation  Fear  Anger  Neutral  Surprise  Disgust  Sadness  Figure 1. Emotion wheel  Emotional Recognition Using a Compensation Transformation in Speech Signal  81  This is a relatively conservative view of what emotion is so special attention has been paid to emotional dimension space theory. Three major dimensions (valence, arousal, and control) [Cowie 2001] are used to describe emotions. a. Valence: The clearest common element of emotional states is that the person is materially influenced by feelings that are valenced, i.e., they are centrally concerned with positive or negative evaluations of people or things or events. b. Arousal: It has been proven that emotional states involve dispositions to act in certain ways. A basic way of reflecting that theme turns out to be surprisingly useful. States are simply rated in terms of the associated activation level, i.e., the strength of the person’s disposition to take some action rather than none. c. Control: Embodying in the initiative and the degree of control. For instance, contempt and fear are in different ends of the control dimension. In this paper, two aspects have to be taken into consideration in the selection of emotional materials: 1. the sentence materials can ’t have any emotional tendency; 2. the materials should relate to five kinds of emotions (happiness, angry, surprise, sadness, and neutral). All recordings were carried out in a large, soundproof room with no echo interference using a high quality microphone, a SONY DAT recorder and a PC164 audio card at a sampling rate of l2KHZ with 16-bit resolution. Six speakers (three male and three female) who are good at acting spoke the sentences with happiness, anger, surprise and sadness, expressing each emotion three times. At the same time, the researchers made the speakers speak each sentence three times in a neutral way. In this way, 2430 sentences for experiments were compiled. 3. Feature Extraction The emotional features of speech signals are always represented as the change of speech rhythm [Shigenaga 1999; Muraka 1998]. For example, when a man is in a rage, his speech rate, volume and tone will all get higher. Some characteristics of phonemes can also reflect the change of emotions such as formant and the cross section of the vocal tract [Muraka 1998; Zhao et al. 2001]. As the emotional information of speech signals is more or less related to the meaning of the sentences, the distributing rules and construction characteristics should be attained by analyzing the relationship between emotional speech and neutral speech to avoid the effect caused by the meaning of the sentences. The global features used in this paper are duration, mean pitch, maximum pitch, average different rate of pitch, average amplitude power, amplitude power dynamic range, average frequency of formant, average different rate of formant, mean slope of the regression line of the peak value of the formant and the average peak value of formant [Zhao et al. 2001; Zhao  82  Cairong Zou et al.  et al. 2000; Zhao et al. 2000]. The duration is the continuous time from start to end in each emotional sentence. It includes the silence, because these parts contribute to the emotion. Duration ratio of emotional speech and neutral speech was used as the characteristic parameters for recognition. The frequency of pitch was obtained by calculating cepstrum. Then the pitch-track was gained, and maximum pitch ( F 0max ), average fundamental frequency ( F0 ), average different rate of pitch ( F 0rate ) of the envelopes of different emotional speech signals can all be extracted from it. F 0rate mentioned here, refers to the mean absolute value of the difference between each frame of speech signal ’s fundamental frequencies. The authors used the differences in value of the mean pitch, the maximum pitch and the ratio of F 0rate between the emotional and neutral speech as the characteristic parameters. In this paper, the average amplitude power ( A ) and the dynamic range ( Arange ) are to be taken into account. To avoid the influence of the silent and noisy parts of the speech, the authors only took the mean absolute value of the amplitude into account and all the absolute values must above a threshold. The difference of average amplitude power and the dynamic range between the emotional and neutral speech was used for parameters of recognition. Formant is an important parameter that reflects the characteristics of vocal track. Formant was attained as follows [Zhao et al. 2001]. At first, LPC method was applied to calculate 14-order coefficients of linear prediction. Then, the coefficients were used to estimate the track’s frequency of the formant by analyzing the frequency average ( F1 ), frequency-changing rate ( F1rate ) of the first formant, the average and the average slope of recursive lines of the first four formants. The authors use the difference of F1 , the last two parameters and the ratio of F1rate between the emotional and neutral speech as the characters in each frame. The structural features of time series for the emotional sentences used in this paper is maximum value of the pitch in each vowel segment, amplitude power of the corresponding frame, maximum value of the amplitude energy in each vowel segment, pitch of the corresponding frame, duration of each vowel segment and mean value and rate of change of the first three formants. For these parameters, the ratio between the emotional and neutral speech was used as the recognition characters. In addition to the above features, LPCC, PLP, MFCC are also taken into consideration for precise decision. Figure 2 is the module for feature extraction.  Emotional Recognition Using a Compensation Transformation in Speech Signal  83  Speech Signal Endpoint Detection Preprocessing Adding window, Frame  Amplitude, duration, etc.  LPC  MFCC  PLP  pitch,  LPCC  format, etc.  Figure. 2 the module for feature extraction  4. Speech Emotion Recognition based on GMM  GMM can be described as follow:  li = {ai , mi , Si} ,  (1)  r p(x  |  l)  =  M å  aibi  r (x)  ,  M å  ai  =1  ,  (2)  i=1  i=1  bi  ( xr )  =  (2p  )D  /  
This paper investigates the influences of three different reading styles ( Lyric, Critical and Explanatory) to the distribution tendency of sentential accents (classified as rhythmic accent and semantic accent). The comparison among multiple styles is performed in three research domains: high-level constructions, low-level phrases and disyllabic prosodic words. One finds that the assignment of semantic accents shows some differences across reading styles, while the assignment of rhythmic accents does not. Furthermore, the larger the speech unit studied, the stronger the influence is observed, i.e. most differences in the assignment of semantic accents are shown in high-level constructions, some are shown in low-level phrases, and none are shown in prosodic words across the three reading styles. Compared with previous studies, the allocation scheme of semantic accents in the Explanatory style is close to that in the neutral style, i.e. in high-level constructions, it has a final-accented tendency in theme + rheme (TR), predicate + object(PO) and subject + predicate(SP) constructions, and uniform distribution in adjunct + head constructions. In low-level phrases, the Explanatory style exhibits an initial-accented tendency in adjunct + head phrases, but a final-accented tendency in subject + predicate (SP) phrases and predicate + object (PO) phrase. The Critical style is adopted to make comments, where semantic focal points are normally on the core subjects and their actions. As a result, more accents are allocated to the subject part in the AS constructions and to the predicate part in the PO constructions. Accordingly, in low-level phrases, more accents go to the heads 
In this paper, syllable-based prosody modelings of pitch contour and syllable duration for  isolated Mandarin words are proposed. In the syllable pitch contour model, three main affecting factors of tone, syllable position in word, and inter-syllable coarticulation are considered. These three affecting factors are assumed to be independent and additive. Similarly, in the syllable duration model, four affecting factors of tone, syllable position in word, base-syllable, and inter-syllable coarticulation are considered. We also assume that these affecting factors are independent and additive. A large single female-speaker speech database containing 107,936 words was used to evaluate the performance of the proposed methods. After well-training, the decision tree method was used to analyze the 411 affecting factors of base-syllable and to explore the relationship between inter-syllable pause duration and the nearby linguistic features. Experimental results showed that all these affecting factors conformed to our knowledge about Mandarin prosody. 關鍵詞：韻律模式，基頻軌跡，影響因素，連音 Keywords: Prosody modeling, Pitch contour, Affecting factor, Coarticulation  一、緒論 文句轉語音系統要能合成出自然流利的語音，關鍵在於韻律的變化是否自然順暢。韻律 的變化包括音調的高低起伏、音量的強弱、發音的長短及停頓的時機、長度等。目前韻 律的合成方法大致分為規則法 [1,2]、類神經網路 [3,4]和統計法。規則法是以語言學的 方法，歸納出一些發音的規則，利用這些規則來產生合成語音的韻律。但是人類說話的 方式變化複雜，不容易掌握。類神經網路是利用一組複雜的網路來模擬人腦的記憶與學 習功能，其學習方法是採用漸進式的修正錯誤與更新記憶的方式，需經由長時間的學習 訓練，雖有不錯的效果，但無法分析影響韻律的因素。本文以統計法的方式，可從大量 的語音資料中統計出韻律變化，利用所考慮影響韻律的因素加總後，控制韻律變化，並 分析各個影響因素對韻律訊息的影響程度。 本文著重於以中文單詞語料庫為基礎之韻律模式的研究，探討音節的基頻軌跡及長 度模式，考慮幾個主要的影響因數，希望藉此了解中文單詞的音節基頻軌跡及長度如何 變化，以作為未來中文語音合成系統產生韻律信息之用，期望合成出自然流暢的中文單 詞聲音。本論文在接下來的第二部份會介紹我們所提出的韻律模型，第三部份介紹模型 的訓練方法，實驗結果在第四部份討論，最後於第五部份對於本研究給予一個結論。  二、韻律模式 韻律模式以音節為單位，在給予特定的語言資訊後，使之預測音節的基頻軌跡及長度， 做為韻律訊息，並分析音節基頻軌跡及長度在各個因素的影響程度。考慮主要影響因素 分別為：聲調(tone)、音節在詞的位置(word-position)、基本音節(base syllable)、音節間 的連音狀態(inter-syllable coarticulation state)。 （一）、基頻軌跡之韻律模型 假設所有影響因素可用累加的方式來表示音節的基頻軌跡，如式子(1)：  spn  =  sp  r n  + βtn  + βwn  +βf cn−1 ,tpn−1  + βb cn ,tpn  +μp  (1)  其中 spn 、 sprn 、 βtn 、 βwn 分別為 pitch 模型中第 n 個音節的基頻軌跡參數向量、基頻軌 跡參數向量殘餘值(residual)、聲調及詞中位置影響因素； spn 為由一段音節基頻軌跡轉  化為四個正交參數表示的向量，轉換方法參見 [5]； wn ∈{(2,1), (2, 2),..( j, k),..(8,8)}代表  音節在詞的位置，其中 ( j, k ) 代表 j 字詞中的第 k 個音節；cn 、tpn =(tn ,tn+1) 分別為在第 n  個音節與第 n+1 個音節間的連音狀態、聲調組合(tone pair)，在這裡音節間的連音狀態  cn ∈{c1, c2, c3} 代表音節間的連音程度， c1 、 c2 、 c3 分別為強連音(tight)、正常連音  (normal)、弱連音(loose)；  βb cn  ,tpn  為第  n  個音節受第  n+1  個音節的後向影響因素(backward  affecting  factor)；  βf cn-1 ,tpn-1  為第  n  個音節受第  n-1  個音節的前向影響因素(forward  affecting  factor)； μ p 為基頻軌跡參數的整體平均(global mean)。  （二）、音節長度模型 假設所有影響因素可用累加的方式來表示音節的長度，如式子(2)：  γ γ γ γ γ μ sd = sd + + + + + + r  f  b  d  n  n  tn  wn  syn  cn−1 , fi_inn−1  cn , fi_inn  (2)  其中  sdn  、  sd  r n  、γ  tn  、γ  wn  、γ  syn  分別為  duration  模型中第  n  個音節的長度、長度殘餘值、  聲調、詞中位置及基本音節影響因素； cn 、 fi _ inn 分別為在第 n 個音節與第 n+1 個音  節間的連音狀態及第 n 個音節韻母類別與第 n+1 個音節聲母類別之組合(final-initial class  pair)；  γ  b cn ,  fi_inn  為第  n  個音節長度受第  n+1  個音節的後向影響因素(backward  affecting  factor)；  γ  f cn−1 ,  fi_inn−1  為第  n  個音節長度受第  n-1  個音節的前向影響因素(forward  affecting  factor)； μ d 為音節長度的整體平均(global mean)。音節韻律與影響因素的關係示意圖以  pitch 模型為例，如圖一：  βt1  βtn−1  βtn  βtn+1  βtN  βf cn−2 ,tpn−2  βf cn−1,tpn−1  βcfn ,tpn  βf cn+1,tpn+1  sp1  spn−1  spn  spn+1  spN  βw1  βb cn−2 ,tpn−2  βb cn−1,tpn−1  βwn−1  βwn  βbcn ,tpn  β b cn+1,tpn+1  βwn+1  βwN  圖一、音節基頻軌跡參數向量與影響因素關係圖  我們分別假設  sprn  及  sd  r n  呈  N  (sp  r n  ;  0,  R  p  )  及  N  (  sd  r n  ;  0,  R  d  )  的 高 斯 分 佈 (Gaussian  distribution)，因此 spn 與 sdn 可表示成數學式如式(3)及(4)：  P(spn  | tn , wn , cn−1, cn , tpn−1, tpn )  =  N (spn;βtn  + βwn  +βf cn−1 ,tpn−1  + βb cn ,tpn  +μp,Rp)  (3)  P(sdn  |  tn, wn, syn, cn−1, cn,  fi_inn−1,  fi_inn)  =  N(sdn;γtn  +γwn  +γsyn  +γ f cn−1, fi_inn−1  +γ b cn, fi_inn  + μd , Rd )  (4)  三、模型的訓練  為了求取韻律模式的各個參數，我們採用 sequential optimization 的方法以及最大相似度  法則(Maximum likelihood criterion)的條件來訓練模型，我們首先定義相似度函數  (likelihood function)如下式：  N  ∑ LP =  log  N (spn;βtn  + βwn  +βf cn−1 ,tpn−1  + βb cn ,tpn  +μp,Rp)  (5)  n=1  N  ∑ Ld =  log N (sdn;γ tn  + γ wn  + γ syn  +γ f cn−1 , fi_inn−1  +γb cn , fi_inn  + μd , Rd )  (6)  n=1  pitch 以及 duration 模型獨立訓練各自的參數，且訓練方法類似，訓練的過程可分為兩大  部分，第一部分為參數的初始化，第二部份為以疊代法的 sequential optimization。以下  以訓練 pitch 模型為例：  （一）、參數的初始化(Initialization)  (a) 直接平均所有音節的 spn ，求出整體 pitch 平均(global pitch mean) μ p  (b) 以下式求取聲調影響因素的初始值：  ( ) ∑ (spn - μ p )δ (tn = t)  ∑ βt = n  δ (tn = t)  ,for t = 1, 2..5  (7)  n  (c) 以下式求取詞位置影響因素的初始值：  ∑ ( ) (spn - βtn -μ p )δ (wn = w)  βw= n  ∑δ (wn = w)  ,for w = (2,1), (2, 2)..(8,8)  (8)  n  (d) 以下列的條件，標記音節間的連音狀態 cn  I. 若兩音節間基頻軌跡相連接，表示兩連音互相影響程度強，連音狀態標記  為強連音 c1。  II. 兩音節間的基頻軌跡不相連接，但音節間的間隔區間內最低能量較大(大  於一個臨界值)，連音狀態標記為正常連音 c2 。  III. 不滿足以上條件者，則連音狀態標記為弱連音 c3。  (e) 求取前後音節影響因素的初始值，如下式：  N  N  ∑ ∑ spnδ (cn−1 = c)δ (tpn−1 = tp) spnδ (cn−1 = c)δ (tn = j )  β = f  n=1  c,tp  N  − n=1 N  (9)  ∑δ (cn−1 = c)δ (tpn−1 = tp)  ∑δ (cn−1 = c)δ (tn = j)  n=1  n=1  N  N  ∑ spnδ (cn = c)δ (tpn = tp) ∑spnδ (cn = c)δ (tn = i)  β = b  n=1  c,tp  N  − n=1 N  (10)  ∑δ (cn = c)δ (tpn = tp) ∑δ (cn = c)δ (tn = i)  n=1  n=1  For c = 1~3 and tp=( i,j )  （二）、以疊代法的 sequential optimization  各個影響因素初始化後依序將聲調( βt )、word-position( βt )、受前後音節等影響因素  (  βf c,tp  ,  βb c,tp  )及  covariance  matrix(  R  p  )的參數值更新，然後使用更新後的參數值，算出整個  訓練語料的目標函數值，一直重覆更新參數值及目標函數值，直到含數值收斂，如圖二  之流程圖：  βt  βw  βf c,tp  βb c,tp  R p  圖二、訓練流程圖  而 duration model 各個參數更新方法與 pitch model 類似，而參數更新的順序為聲調  (  γ  t  ) 、 word-position(  γ  w  )、基本音節(  γ  sy  )、受前後音節等影響因素(  γf c,  , fi_in  γ  b c,  fi_in  )及  covariance matrix( Rd )。  四、實驗結果與分析  實驗語料庫之單詞來自於『NCTU 文句分析器』的詞典選擇而來，以聲調平衡為主要的 選擇條件，總共有 107936 個詞，277218 個字，其中詞長最短為二字詞、最長八字詞， 詞長統計如表一，聲調統計如表二，語料庫是由專業的女性廣播人員以流利的方式唸出 錄製，錄音場所為一般安靜房間。  表一、詞長數量之統計  詞長 二字詞 三字詞 四字詞 五字詞 六字詞 七字詞 八字詞  數量 64872 26026 16062 797  124  49  6  表二、聲調數量之統計 聲調 一聲 二聲 三聲 四聲 數量 62349 69278 48904 94786  五聲 1901  接下來我們依序分析基頻軌跡、音節長度韻律模型以及預測 pause 長度。  （一）、基頻軌跡韻律模型  1、聲調影響因素(Tone affecting factor) βt  聲調影響因素的基頻軌跡如圖三所示，觀察得知，由模型所訓練出來的聲調影響因素符 合我們所認知的聲調基頻軌跡。  0.3  0.2  0.1  Log-F0(Hz)  0  -0.1  -0.2  -0.3  -0.4  tone 1  tone 2  tone 3  tone 4  tone 5  圖三、聲調影響因素基頻軌跡  2、音節在詞的位置影響因素(Word position affecting factor) βw  Word position affecting factor 的基頻軌跡影響如下圖所示，分別有二字詞至五字詞位置 影響因素(圖四~七)，同時將詞首與詞末的基頻軌跡整合比較(圖八、九)，可觀察得，音 節在詞的位置愈接近詞首，基頻愈高，愈接近詞末愈低，且詞首有上仰趨勢(圖八)，在 詞尾均有微幅的上揚(圖九)，同時我們也發現到詞長越長，則基頻軌跡變化的動態範圍 愈大，如二字詞的動態範圍在 0.06 ~ -0.08，而五字詞的動態範圍在 0.15 ~ -0.17 之間。  Log-F0(HZ)  0.08  0.06  0.04  0.02  0  -0.02  -0.04  -0.06  -0.08  -0.1  (2,1)  (2,2)  圖四、二字詞影響因素基頻軌跡  Log-F0(HZ)  0.15  0.1  0.05  0  -0.05  -0.1  -0.15  -0.2  (3,1)  (3,2)  (3,3)  圖五、三字詞影響因素基頻軌跡  Log-F0(HZ)  0.2  0.15  0.1  0.05  0  -0.05  -0.1  -0.15  (4,1)  (4,2)  (4,3)  (4,4)  圖六、四字詞影響因素基頻軌跡  Log-F0(HZ)  0.2  0.15  0.1  0.05  0  -0.05  -0.1  -0.15  -0.2  (5,1)  (5,2)  (5,3)  (5,4)  (5,5)  圖七、五字詞影響因素基頻軌跡  Log-F0(HZ) Log-F0(HZ)  0.25  0.2  0.15  0.1  0.05  (2,1)  (3,1)  (4,1)  (5,1)  (6,1)  圖八、整合詞首基頻軌跡  0  -0.05  -0.1  -0.15  -0.2  -0.25  (2,2)  (3,3)  (4,4)  (5,5)  (6,6)  圖九、整合詞末基頻軌跡  3、受前後音節影響因素(forward and backward affecting factor)  圖十為五個聲調受前音節中各個聲調在各個連音狀態影響的基頻軌跡。可觀察得 c1 明 顯比 c2 及 c3易受影響。以 c1為觀察對象，目前音節的基頻軌跡前端高度，受前一個音 節基頻軌跡後端高度影響，若前音節基頻軌跡後端比受影響的前端聲調基頻軌跡還高， 則受影響的前端基頻軌跡會往上改變，反之往下改變。圖十一為受後音節的影響，其影 響原理類似於受前音節的影響因素，可觀察得受後音節較受前音節的基頻軌跡影響相對 較小。  Log-F0(Hz) 0.5 j=1 0  -0.5  0.5 20  -0.5  0.5  30  -0.5  0.5 40  -0.5 0.5 50  -0.5  i = 1  2  3  4  5  圖十、Forward affecting factor  βf c,tp  ，  c  =  {c1,c2,c3}  ，  tp=(i, j) ，其中點線(…)為 c1、點  虛線(--‧)為 c2 、虛線(--)為 c3  Log-F0(Hz) 0.5 j=1 0  -0.5  0.5 20  -0.5 0.5  30  -0.5  0.5 40  -0.5  0.5 50  -0.5  i =1  2  3  4  5  圖十一、Backward affecting factor  βb c,tp  ，  c  =  {c1,c2,c3}  ，  tp=(  j,  i)  ，其中點線(…)為  c1  、  點虛線(--‧)為 c2 、虛線(--)為 c3  4、二字詞基頻軌跡預測 圖十二為各種二字詞聲調組合(tone pair)的預測結果，假設二字詞音節基頻軌跡相連且 相互影響，可觀察得 c1 明顯易受影響，而且 c1 在受前後音節影響下，兩音節基頻軌跡 相連續且平滑。而三聲接三聲變二聲接三聲的語言特性也可由下圖證實。  Log-F0(Hz) j =1 4555....82465  2  55..46 45..825  5.6 3 455...8245  5.6 4 55..24 4.85  5.6 5 455...8245  i = 1  2  3  4  5  圖十二、二字詞基頻軌跡變化圖，其中點線(…)為 c1、點虛線(--‧)為 c2、虛線(--)為 c3， 其 j 為第一音節，i 為第二個音節  5、預估之基頻軌跡範例 訓練模型後，pitch 模型預測音節基頻軌跡結果如下圖所示，其中黑色線(實線)為原始音  節的基頻軌跡，紅色線(點線)為 pitch 模型所預測的基頻軌跡，可觀察得，不僅個別音 節的基頻軌跡走勢相似，且強連音音節基頻軌跡也預測的不差。  圖十三、Pitch 模型預測基頻軌跡  6、Covariance matrix 比較  Covariance matrices 在訓練前(covariance matrices of the original syllable F0)與訓練後  (covariance matrices of the normalized syllable F0)分別為  RP original  及  R p 。可觀察得訓練  模型後 covariance 明顯有意義下降。  ⎡0.040124  R P original  =  ⎢⎢0.0053695 ⎢-0.0020751  ⎢  ⎢⎣-0.00075677  0.0053695 0.018669 0.0020441 -0.0016243  -0.0020751 0.0020441 0.0037581 8.3657 ×10-5  -0.00075677⎤  -0.0016243  ⎥ ⎥  8.3657 ×10-5 ⎥ ⎥  0.0011703 ⎥⎦  ⎡0.011843  ⎢  R p  =  ⎢0.0021328 ⎢⎢9.2796 ×10-5  ⎢⎣-0.00035788  0.0021328 0.0052484 0.0011326 -0.00030867  9.2796 ×10-5 0.0011326 0.0022113 0.00033545  -0.00035788⎤  -0.00030867⎥⎥  0.00033545  ⎥ ⎥  0.00090293 ⎥⎦  （二）、Duration 模型 1、聲調影響因素(Tone affecting factor) 聲調對音節長度影響程度如表三，觀察得五聲音節最短，二聲音節最長。 表三、聲調對音節長度影響 聲調 一聲 二聲 三聲 四聲 五聲 長度 6.9ms 35.0ms -22.5ms -17.2ms -82.2ms 2、詞的位置影響因素(Word-position affecting factor)  音節在詞的位置對長度影響程度如圖十四，可觀察在詞的字末位置音節長度較長，觀察 五字詞、六字詞可得知詞愈長愈容易產生較短的音節。  (ms) 100 80 60 40  2 character word 3 character word 4 character word 5 character word 6 character word  20  duration  0  -20  -40  -60  -80  -100 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 syllable position in word 圖十四、詞的位置影響因素  3、音節影響因素(syllable affecting factor) 我們使用決策樹觀察音節影響因素對音節長度影響程度，如圖十五。由決策樹可觀察出 聲母為空聲母(Q1)、{b、d、g}(Q2)或韻母類別為單母音(Q8)這類音節長度較短，而鼻 音結束(Q10)、{f,s,sh,x,h}(Q3)及{c,ch,q}(Q5)這類摩擦音的音節長度會較長。  圖十五、以決策樹分析音節影響因素分類結果  4、音節長度預估之範例  Duration 模型預測音節長度與實際長度比較，如圖十六、十七分別為二字詞及三、四字 詞的比較圖。其中 observed 為音節實際長度，reconstructed 為音節預測長度，其每線段 為一單詞，大致有不錯的預測結果。 (ms) 500  450  400  duration  350  300  250  200  original  reconstruction  150  圖十六、二字詞音節實際長度與模型預測長度之比較圖  (ms) 450 400 350  observed reconstructed  duration  300 250  200 150  100 圖十七、三字詞及四字詞音節實際長度與模型預測長度之比較圖  5、Variance 比較  訓練模型後，以訓練前 variance 為 9304.5 與訓練後 variance 為 2494.7。可觀察得使用模 型後變異量明顯有意義下降。  （三）、以決策樹預測 pause 長度 在下面小節中，利用決策樹預測音節間的連音狀態及 pause 長度，其應用於 question set 中的聲母類別和韻母類別分別為兩音節的間隔區間相鄰的聲母類別及韻母類別。  1、預測音節間之連音狀態 以連音狀態做為決策樹分類的目標，由 c1設值為 1， c2 設值為 2， c3設值為 3。由分類  結果觀察得聲母為 NULL (Q1)及{m,n,l,r}(Q4)分別為 mean=1.1173 及 mean = 1.047，意 指分佈較集中於 c1；而聲母為{f,s,sh,x,h}(Q3)，此類 mean=2.1341，是指分佈較集中於 c2 ，其他類別假設為 c3。如圖十八所示。  圖十八、以決策樹分析語料庫連音狀態的分類結果 2、預測音節間之 pause 長度 由語料庫中 pause 的長度視為目標值，其單位為毫秒(ms)。 由圖十九觀察得 Pause 長度 分類明顯與 pause 相鄰的聲母類別有關。將 Pause 長度與聲母類別關係整理如下： 爆破音_不送氣 > 爆破音_送氣 > 塞擦音_不送氣 > 塞擦音_送氣 >摩擦音_清音 >m,n,l,r 及空聲母。 假設預測 pause 長度可直接利用相鄰的聲母類別判斷，由圖十九可觀察得 pause 長 度分類的 mean 設為預測的 pause 長度，整理於表四，其中聲母類別為 NULL 及{m,n,l,r} 的 mean 為 0.31028，可假設 pause 長度為 0ms，滿足由圖十八預測此種類別為連音狀態 最明顯的 c1。  類別 1 2 3 4 5 6  表四、pause 長度選取表 聲母 ㄇ、ㄋ、ㄌ、ㄖ、空聲母 (鼻音_濁音) ㄏ、ㄒ、ㄕ、ㄈ、ㄙ (摩擦音_清音) ㄅ、ㄉ、ㄍ (爆破音_不送氣) ㄐ、ㄓ、ㄗ (塞擦音_不送氣) ㄆ、ㄊ、ㄎ (爆破音_送氣) ㄑ、ㄔ、ㄘ (塞擦音_送氣)  Pause 長度 0ms 7ms 46ms 28ms 37ms 24ms  圖十九、以決策樹分析語料庫 pause 長度的分類結果  再假設預測 pause 長度與 pause 在詞的位置有相關性，利用 pause 在詞的位置以決 策樹分析整理於表五及表六：  表五、二字詞第一個 pause 位置的 pause 長度選取表  類別 1  2  3  4  5  6  (2,1) 0ms 8ms 56ms 34ms 44ms 29ms  表六、三字詞至四字詞 pause 在詞位置的 pause 長度選取表其中(詞長 , 詞中第幾 個 pause)。  類別  位置  
Voice Onset Time (VOT) is considered as one of the best methods for examining the timing of voicing in stop consonants and has been applied in the study of many languages. The present study is designed to examine VOT production for phonetically voiceless stops in Mandarin and English by native Chinese speakers. Thirty-six Taiwanese Chinese speakers recruited from National Cheng Kung University participated in this study. The results indicate the following. 1) Based on the three universal categories proposed by Lisker and Abramson (1964), for phonetically voiceless stops, Mandarin and English occupy the same place along the VOT continuum. 2) The mean VOT value for the apical stop /t/ is slightly lower than the mean value for the labial stop /p/. This does not conform to the general consensus, which states that the further back the place of articulation the longer the VOT. Very similar findings were also observed in previous studies. 3) The difference between the mean VOT values of the English /p/ and /t/ produced by Chinese speakers was subtle, while it reached significance for native English speakers. This suggests that a first language could be a crucial factor in L2 production. Future studies might examine variations in L2 production both for the same persons over time and for different speakers. Keywords: voice onset time (VOT), voiceless stops, place of articulation 1. Introduction Voicing contrast in stops has been discussed in phonetics and phonology for the past few decades. Beginning with Lisker and Abramson (1964), in their well-known cross-language study, voice onset time (VOT) has been widely used to differentiate stop categories across languages. Since then, VOT has come to be regarded as one of the best acoustic cues for discriminating three general stop categories, especially in word-initial position. In contrast with the considerable number of studies investigating stop voicing contrast in a variety of  languages, only a few have examined Mandarin word-initial stops, not to mention comparing VOT patterns in Mandarin and English. Therefore, the purpose of this present study is threefold. First, it is intended to provide information for a general VOT pattern of Mandarin word-initial stops. By analyzing VOTs in stop consonants, linguists have concluded that for most languages, VOT values get longer as the place of articulation moves backward (Lisker & Abramson, 1964; Cho & Ladefoged, 1999; Gósy, 2001). However, there are some exceptions, such as Mandarin, which does not follow the general rule (Lisker & Abramson, 1964; Cho & Ladefoged, 1999; Chao, Khattab & Chen, 2006). The second purpose is to explore the possible effects of this phenomenon. Vowel context is also examined to determine whether there is a correlation between VOT and subsequent vowels. Moreover, to date no study has focused on comparing the in-depth differences between Mandarin and English, except for Chao et al. (2006) who pinpoints the existence of subtle differences between the two languages. Thus, the third aim is to compare VOT patterns of the two languages and observe L2 production (i.e. English production) by native Chinese speakers. 2. Literature review 2.1 Voice onset time Lisker and Abramson (1964) conducted a cross-language investigation of word-initial stops in 11 languages and define voice onset time as the temporal interval from the release burst of the stop consonant to the onset of the first formant (F1) frequency that reflects glottal vibration. Following their study, VOT has been widely used to examine voicing contrast in stops in many languages (Keating, Linker, and Huffman, 1983; Rochet & Fei, 1991; Cho and Ladefoged, 1999; Gósy, 2000; Khattab, 2000; Zheng & Li, 2005; Riney, Takagi, Ota, and Uchida, 2006). In addition to investigating phonetic characteristics of voiced and voiceless stops in various languages, some researchers have studied VOT with respect to place of articulation, speaking rate, bilingual language learners, and vowel environment (Kewley-Port, Pisoni, and Studdert-Kennedy, 1983; Port and Rotunno 1979; Kessinger and Blumstein 1997; Benkí, 2001; Kehoe, Lleó, and Rakow, 2004). Thus, VOT is one of the main acoustic cues used to measure the timing of voicing in stops. Although VOT is now used across the world as a linguistic cue, some researchers, however, challenge its role and importance as a reliable measure for separating phonemic categories. In their study examining voicing contrast among French-English bilinguals, Caramazza, Yeni-Komshian, Zurif, and Carbone (1973) argue that voice onset time is ineffective at differentiating stop categories. Bohn and Flege (1993) also question its importance to the perception of stop voicing. Docherty (1992) indicates that VOT narrowly concentrates on word-initial stops. Moreover, Klatt (1975) even suggests five other acoustic cues that are equally important to voice onset time: that is, low frequency energy in subsequent vowels, burst loudness, fundamental frequency, pre-voicing, and segmental duration. Even if VOT does have limitations, it is still regarded as one of the most important acoustic parameters for distinguishing voicing contrast, especially for word-initial stops.  2.2 VOT category In Lisker and Abramson’s 1964 study, all stops are classified into three groups depending on the number of stop categories in each language. VOT ranges for the three stop categories are -125 to -75ms, 0 to +25ms, and +60 to +100ms. Cho and Ladefoged (1999) also provide VOT ranges for occlusives, particularly in voiceless aspirated and unaspirated stops. Rather than three categories, they distinguish four: unaspirated, slightly aspirated, aspirated, and highly aspirated. The approximate mean VOT values for each category are 30 ms, 50 ms, 90 ms, and over 90 ms, respectively. In agreement with Lisker and Abramson’s (1964) categorization, on the basis of Cho and Ladefoged’s (1999) categorization, stops in Mandarin and English are found to occupy the same place along the VOT continuum, whereas stops in the two languages do not belong to the same range along the continuum, especially for voiceless aspirated occlusives. Chao, Khattab, and Chen’s (2006) findings confirm Cho and Ladefoge’s classification and reveal that for voiceless aspirated stops, Mandarin falls into the ‘highly aspirated’ region while English belongs to ‘highly aspirated’ category. A comparison of the different stop categories in Mandarin and English is given in section 2.4, below. 2.3 Effect on VOT 2.3.1 Place of articulation Some researchers have reported a significant link between place of articulation and voice onset time. Cho & Ladefoged (1999) propose some possible relations including 1) the further back the closure, the longer the VOT; 2) the more extended the contact area, the longer the VOT; and 3) the faster the movement of the articulator, the shorter the VOT. Of these three suggested links, the present study focuses on the first in connection with Mandarin. In addition to this first principle, it may be stated that the velar stop /k/ has the longest VOT duration and bilabial stop /p/ the shortest, with the alveolar stop /t/ in between the two (Lisker & Abramson, 1964). Factors used to explain why VOT is longer when articulation takes place nearer the back of the mouth include aerodynamics, articulatory movement velocity, and differences in the mass of the articulators (Cho & Ladefoged, 1999). The size of the supraglottal cavity behind the constricted points should be taken into consideration when considering the impact of aerodynamics. The cavity behind the velar stop has a smaller volume than that behind the alveolar and bilabial stops. In other words, the velar stop is under greater pressure when airflow is released; therefore, it might take longer to produce a velar stop, and the VOT value for the velar stop might be longer than either the alveolar or the bilabial stop. As for articulatory movement velocity, Cho and Ladefoged (1999) claim that the tip of the tongue and the lips move faster than the back of the tongue; moreover, the tongue tip moves faster than the lower lip. This may explain why in many languages velar stops have longer VOT than labial and alveolar stops. However, articulatory movement velocity does not affect alveolar and bilabial stops in this way in all languages, which implies that other factors are involved. In reference to the extent of articulatory contact area, Cho and Ladefoged (1999: 211) claim that, “In general, stops with a  more extended articulatory contact have a longer VOT.” In summary, it is indubitable that velar stops have longer VOT than the two other stops. However, no final conclusion may be reached in the case of labial and alveolar stops. Although there is general agreement that the further back the place of articulation, the longer the VOT, there are still some exceptions. Lisker and Abramson’s (1964) study reports that unaspirated stops in Tamil and aspirated stops in Cantonese and Eastern Armenian do not follow this rule. It is found that the VOT of alveolar /t/ is shorter than bilabial stop /p/, but the velar stop /k/ still has the longest VOT. Studies by Rochet and Fei (1991) and Chao et al. (2006) arrive at similar results. Investigating Mandarin Chinese, they conclude that the VOT duration for /t/ does not confirm the predictions; on the contrary, it is shorter than the VOT for /p/. The cause of this phenomenon is still unknown. 2.3.2 Vowel context How vowels influence the VOT of preceding stops is still an open question. Lisker and Abramson (1967) propose that following vowels have no significant influence on VOTs, while other researchers apply similar research methods, but more systematically, and find that VOTs are longer when followed by tense high vowels (Klatt, 1975; Weismer, 1979). Similar results are obtained in Port’s (1979) study, which analyzes VOT for English word-initial stops, and in Gósy’s research, which examines Hungarian voiceless plosives. Rochet & Fei (1991) also reach similar findings with respect to Mandarin stops, claiming that “the nature of the vowel had a significant effect on the VOT values of the preceding consonants” (p. 105). In other words, word-initial stops have longer VOT values when followed by either of the high vowels /i/ or /u/ than when followed by the low vowel /a/. This accords with the results presented in Chao et al’s (2006) study which examines the Mandarin Chinese of Taiwanese speakers. By contrast, however, Fant (1973) finds that for Swedish aspirated stops, VOTs are longer when stops are followed by /a/ than /i/ or /u/. Although the finer points of the issue are still undecided, a general conclusion that may be made is that vowel context does have some effects on voice onset time. 2.4 Mandarin and English stops and VOT patterns In Lisker and Abramson’s (1964) study, VOT measurements occurring before the release burst are said to have negative values, called ‘voicing lead’, whereas ‘voicing lag’ refers to measurements occurring after the release burst and are assigned positive values. Following these definitions, Keating (1984) subdivides the voicing lag dimension into ‘short lag’ (20–35ms) and ‘long lag’ (over 35ms). On the basis of this classification, stops are divided into three phonetic categories: voiced, voiceless unaspirated, and voiceless aspirated. Mandarin and English are said to contain two stop categories; detailed descriptions of the stops in these two languages are elaborated in the following sections. 2.4.1 English stops Although, as Keating (1984) mentions, English has a great deal of positional variation, in the  present study only syllable initial stops are discussed. English is known to contrast voiced  and voiceless phonemes in word-initial position, while voiced stops are said to have two  possible phonetic realizations, voiced or voiceless unaspirated (Keating, Linker, & Huffman,  1983; Keating, 1984; Docherty, 1992). Lisker and Abramson (1964) provide two sets of  VOT values for English voiced stops (/b, d, g/), one with a positive short lag, and the other  with a negative voicing lead. They further suggest that only a single type of phonetic  representation is produced by each native speaker. Klatt (1975) measures VOT values for  English stops and reports positive values for both voiced /b, d, g/ and voiceless unaspirated  stops /p, t, k/. Keating (1984) also points out that English voiced stops are sometimes  pronounced with some lead values but mainly with short lag and long lag. Table 1 shows  mean VOTs for English stops, as reported by Lisker and Abramson (1964), Klatt (1975), and  Docherty (1992).  Table 1. Mean VOTs for English stops  Lisker & Abramson, 1964 (AE)  Mean  /p’/  58  /t’/  70  /k’/  80  /p/  /t/  /k/  /b/  1/-101  /d/  5/-102  /g/  21/-88  Klatt, 1975 Mean 47 65 70 12 23 30 11 17 27  Docherty, 1992 (BE) Mean 42 64 62 15 21 27  (AE=American English; BE=British English. All measurements are in milliseconds (ms).  Note: /p’, t’, k’/ represents voiceless aspirated stops, while /p, t, k/ refers to voiceless  unaspirated stops.  2.4.2 Mandarin stops It is known that all Mandarin stops are phonetically voiceless and that aspiration is the only distinctive phonetic feature, differentiating two phonemic categories: voiceless unaspirated /p, t, k/ and voiceless aspirated /p’, t’, k’/. Unlike in English, stops in Mandarin occur only in word-initial position. Moreover, Mandarin stops fall into short lag versus long lag patterns. Table 2 juxtaposes mean Mandarin VOTs, as measured by different researchers. As well as Rochet and Fei’s (1991) study of Mandarin Chinese, Liao (2005) and Chao et al. (2006) focus on Taiwanese Chinese accents. Two points are of note. First, as the table shows, VOT values for Mandarin /p’, t’, k’/ are obviously higher than their equivalents in English. This may imply that for voiceless aspirated stops, especially for the velar /k’/, Mandarin and English may occupy different areas along the VOT continuum. Secondly, all values for /t’/ production are close to, but slightly lower than, the values for /p’/. It is interesting to note the possible effect of not conforming to the general pattern with respect to place of articulation.  Table 2. Mean VOTs in Mandarin  Rochet & Fei, 1991 (MC)  Mean  /p’/  99.6  /t’/  98.7  /k’/  110.3  /p/  /t/  /k/  Liao, 2005 (TC) Mean 75.4 71.4 98.8 17.9 18.6 28  Chao et al., 2006 (TC) Mean 82 81 92 14 16 27  (MC=Mandarin Chinese; TC=Taiwanese Chinese accent. All measurements are in  milliseconds (ms). Note: Rochet & Fei only provide the mean VOT for voiceless aspirated /p’,  t’, k’/.  3. Methodology 3.1 Aims of the experiment As mentioned above, some studies have examined VOT in Mandarin Chinese, but few have attempted to compare Mandarin and English VOT patterns, particularly with respect to voiceless aspirated stops. To the best of our knowledge, so far only Chao et al. (2006) have compared VOT patterns in these two langauges, and they found that there are indeed subtle differences in VOT production between Mandarin and English. Therefore, the aim of the present experiment is to compare Mandarin and English VOT patterns.  3.2 Stimuli It is known that, in Mandarin, stops occur only in the word-initial position; moreover, all stops are phonetically voiceless and they are only distinguished by aspiration. The present experiment examines only voiceless stops in the initial position. Klatt (1975) finds that the differences in VOT values relate to the environment of the following vowel. Therefore, in this experiment each of the stops is augmented by three peripheral vowels; that is, two high vowels, /i/ and /u/, and one low vowel, /a/. The Mandarin word list consists of 16 words (excluding /k’i/ and /ki/, as no meaningful lexical items for /k’i/ and /ki/ exist in Chinese). Note that compound words (two or three characters side by side forming a ‘word’) are used rather than single characters because they are more complete and more sense to the subjects. Two procedures are used to create an English word list. First, only voiceless aspirated stops /p’, t’, k’/ in the word-initial position are examined here due to the debatable implementation of English voiced stops; moreover, a CVCV sequence is used to ensure the target stop is stressed. Velar /k’/ followed by the high vowel /i/ is not included, as no corresponding words are found in Mandarin. Secondly, analogous to the Mandarin stimuli, disyllabic and not monosyllabic words are used to design the English word list.  3.3 Subjects Thirty-six native speakers of Taiwanese Chinese were recruited from various departments at National Cheng Kung University in southern Taiwan. Subjects include 21 staff (mean age= 40 years) and fifteen students (mean age= 22 years), aged from 20 to 50 (mean age for all  subjects= 32 years). All of the subjects were born and raised in Taiwan, have no marked regional accent, and reported no sophisticated knowledge of linguistics at the time of testing. 3.4 Procedures Each subject was scheduled to record the word lists in a soundproof booth, using a high-quality microphone (AKG C1000S) and a professional 2-channel mobile digital recorder (MicroTrack 24/96). The target words for both languages were randomised in order not to be predictable. The recording was made when the subjects indicated they were ready. The subjects were first asked to read each word on the Mandarin and English word lists at a normal speed and repeat the whole lists twice in a row. All speakers were allowed to ask questions and practice words with which they were unfamiliar, but they were not informed of the purpose of the experiment. After the recording, they were asked to fill in a short questionnaire relating to their linguistic background. 3.5 Measurements and analyses Wavesurfer software was used to make acoustic measurements of the speech material. Spectrograms and waveforms are displayed on screen and a manually controlled cursor is used for durational measurements, as shown in figure 1. VOT values were obtained by measuring the interval between the beginning of the release burst and the onset of the first formant visible in the frequency region. Target sounds that were obviously mispronounced are not included in the final analysis. Mean VOT values, standard deviations (SD), and graphical representation were made using EXCEL and SPSS. ANOVA tests were used for all statistical analyses, including the comparison of results and calculation of significance. Figure 1: Spectrogram and waveform for the Mandarin word, “ti qiu” 4. Results Mandarin VOT patterns for voiceless stops are discussed in section 4.1 below. Owing to the debatable phonetic implementations for English voiced stops, only voiceless aspirated stops (/p’, t’, k’/) in Mandarin and English are compared. Vowel quality is also taken into consideration in section 4.1.2, below.  4.1 Mandarin VOT 4.1.1 VOT means and distribution The mean VOT values for six Mandarin stops are shown in figure 2, and detailed measurements including standard deviation (SD) are presented in table 3. Compared with the data reported by other researchers (Rochet & Fei, 1991; Liao, 2005; Chao et al., 2006), the VOT means for Mandarin stops presented in this study are relatively low, especially for the voiceless aspirated /k’/. Overall, VOT values for velar stops /k’/ and /k/ are significantly higher than those for bilabial and alveolar stops [F (2, 835) = 15.917, p= .000< .05]. Regarding the relation between place of articulation and VOT value, it is interesting to note that among voiceless aspirated stops, /t’/ has a higher value than /p’/, which does not conform to the general rule that VOT values rise as the place of articulation moves further back. The AONOVA test shows that the difference between /p’/ and /t’/ does not reach significance [F (1,627) = 1.885, p= .170 > .05]. However, this finding is only relevant to the voiceless aspirated /p’, t’/, and not to the voiceless unaspirated /p, t/. In addition, as table 3 indicates, , contrary to English VOT patterns, the mean VOTs for Chinese bilabial and alveolar stops are much closer to each other, both for aspirated and unaspirated stops. The two main results of the present study are in accordance with studies by three other researchers (Rochet & Fei, 1991; Liao, 2005; Chao et al., 2006). The only difference is that for aspirated stops, Liao (2005) reports /p’/ with a significantly higher value than /t’/ [F (1, 19) = 7.464, p= .013< .05], while the two other studies showed no significant difference.  VOT in msec  Mandarin mean VOT  90 80 70  60  50 
This paper is to compare two most common features representing a speech word for speech recognition on the basis of accuracy, computation time, complexity and cost. The two features to represent a speech word are the linear predict coding cepstra (LPCC) and the Mel-frequency cepstrum coefficient (MFCC). The MFCC was shown to be more accurate than the LPCC in speech recognition using the dynamic time warping method. In this paper, the LPCC gives a recognition rate about 10% higher than the MFCC using the Bayes decision rule for classification and needs much less computational time to be extracted from speech signal waveform, i.e., the MFCC needs computational time 5.5 time as much as the LPCC does. The algorithm to compute a LPCC from a speech signal much simpler than a MFCC, which has many parameters to be adjusted to smooth the spectrum, performing a processing that is similar to be adjusted to smooth the spectrum, performing a processing that is similar to that executed by the human ear, but the LPCC is easily obtained by the least squares method using a set of recursive formula. Key words: Bayes decision rule, linear predict coding, Mel-frequency cepstrum coefficient, signal processing, speech recognition. 1. Introduction A speech recognition system basically contains extraction of features and classification of an utterance of an acoustical word. The measurements made on the speech waveform include energy, zero crossings, extrema count, formants, LPC cepstrum (LPCC) [1-4] and the Mel frequency cepstrum coefficient (MFCC) [5-8]. The LPC method provides a robust, reliable and accurate method for estimating the parameters that characterize the linear, time-varying system which is recently used to approximate the nonlinear, time-varying system of the speech waveform. The MFCC method uses the bank of filters scaled according to the Mel scale to smooth the spectrum, performing a processing that is similar to that  executed by the human ear. The filters with Mel scales spaced linearly at low frequencies and logarithmically at high frequencies are used to capture phonetically the characteristics of speech [8]. For recognition, Davis and Mermelstein [5] used the dynamic time warping algorithm to show that the performance of the MFCC was better than the LPCC.  In this paper, we use a simple technique [9] for speech data compression of the sequence of MFCC vectors and the sequence of LPCC vectors to obtain a matrix of feature values respectively. For speech recognition, we simply use a simplified Bayes decision rule with weighted variance, where each step is a simple calculation and which has the minimum probability of misclassification. In our study, there are two speech recognition experiments. In the first experiment, since both LPCC and MFCC are said to be robust and reliable to noise and estimation errors, our speech experiment is implemented in a noisy environment to test which feature is better on speech recognition. Pick up 9 female and 10 male students and each pronounces 10 digits once using a common (not high-quality) microphone. Some students pronounce mandarin syllables not very clearly, since we have several types of accents to pronounce the same mandarin syllables. In the second experiment, there are 87 students to pronounce the mandarin syllables in a quiet classroom, which are most commonly used in usual conversations. Our speech experiment is done like natural talking. Hence our speech system can be commonly used for all peoples and in all environments. The recognition rate using LPCC is significantly better than the rate using MFCC and the LPCC needs much less computational time to be extracted from speech signal waveform.  2. Bayes Decision Rules  Let X = ( X1, ..., X k ) be the input feature vector of a speech data, which belongs to  one of m categories (syllables) ci , i = 1, ..., m . Consider the decision problem consisting  of determining whether X belongs to ci . Let f (x | ci ) be the conditional density  function of X given category ci . Let θi be the prior probability of ci such that  Σ  m i =1  θ  i  = 1,  i.e., the θ i  is the probability for the category  c i  to occur.  Let d be a  decision rule. A simple loss function L(ci , d (x)), i = 1, ..., m, is used such that the loss  L(ci , d (x)) = 1 when d (x) ≠ ci makes a wrong decision and the loss L(ci , d (x)) = 0 when  d (x) = ci makes a right decision. Let τ = (θ1, ...,θm ) and let R(τ , d ) denote the risk  function (the probability of misclassification) of d . Let Γi , i = 1, ..., m , be m regions  separated by d in the k -dimensional domain of X , i.e., d decides ci when X ∈ Γi .  Then  m ∑ ∫ R(τ , d ) = θi L(ci , d (x)) f (x | ci )dx i =1 m ∑ ∫ = θi Γic f (x | ci )dx i =1  (2.1)  where Γic is the complement of Γi . Let D be the family of all decision rules which  separate m categories. Let the minimum probability of misclassification be denoted by  R(τ ) = inf R(τ , d ) d∈D  (2.2)  A decision rule dτ which satisfies (2.2) is called the Bayes decision rule with respect to the prior distribution τ and is given in (2.3) [10]. We state the Bayes decision rule in the following theorem. Theorem 2.1. [10] The Bayes decision rule with respect to τ is defined by  dτ (x) = ci if θi f (x | ci ) > θ j f (x | c j )  (2.3)  for all j ≠ i ,i.e., Γi = {x | θi f (x | ci ) > θ j f (x | c j )} for all j ≠ i .  Note that if θi = 1/ m , i = 1, ..., m , the Bayes decision rule (2.3) become a ML classifier.  3. Feature Extraction  3.1 Preprocessing Speech Signal  Since our speech recognition experiment is implemented in a noisy environment, the speech data must contain noise. We propose two simple methods to eliminate noise. One way is to use the sample variance of a fixed number of sequential samples to detect the real speech signal, i.e., the samples with small variance does not contain speech signal. Another way is to compute the sum of the absolute values of difference of two consecutive samples in a fixed number of sequential speech samples, i.e., the speech data with small absolute value do not contain real speech signal. In our speech recognition experiment, the latter provides slightly faster and more accurate speech recognition.  3.2 Mel-Frequency Cepstrum Coefficient (MFCC)  The MFCC is a representation defined as the real cepstrum of a windowed short-time signal derived from the fast Fourier transform of the speech signal. In the MFCC, a nonlinear frequency scale is used, which approximates the behavior of the auditory system. The discrete cosine transform of the real logarithm of the short-time energy spectrum expressed on this nonlinear frequency scale is called the MFCC. Davis and Mermelstein [5] showed the MFCC representation to be beneficial for speech recognition. We detail the MFCC as follows [8]:  Let s[n] denote the N samples of a speech waveform. The discrete Fourier transform (DFT) X[k] of the speech signal is defined by  N −1 ∑ X [k ] = s[n]e− j2πnk / N , n=0  0≤k < N  (3.1)  We define a filterbank with M filters (m = 1, ..., M ) , where filter m is a triangular filter given  H[m, k] = 0  if k < f [m −1]  H[m, k] = (k − f [m −1]) /( f [m] − f [m −1])  if f [m −1] ≤ k ≤ f [m]  H[m, k] = ( f [m + 1] − k) /( f [m + 1] − f [m])  if f [m] ≤ k ≤ f [m + 1]  H[m, k] = 0  if k > f [m + 1]  (3.2)  which satisfies  Σ  M m=1  H[m, k] = 1,  k  =  0,1, ..., N  −1 .  Let fl and fh be the lowest and highest frequencies of the filterbank in H z and let Fs be the sampling frequency in H z . The boundary points f [m] are uniformly spaced in the mel-scale:  f  [m]  =  (  N Fs  )B  −1  (B(  fl  )  +  m  B(  f  h ) − B( M +1  f  l  )  )  (3.3)  where B( f ) = 1125ln(1 + f / 700) and B −1 (b) = 700(e(b /1125) −1) . The log-energy is computed by  N −1 S[m] = ln{∑| X [k] |2 H[m, k]}, k =0  0 < m ≤ M.  (3.4)  The MFCC is then the discrete cosine transform of the M filters outputs:  M −1  c(n) = ∑ S[m]cos(πn(m − 0.5) / M ) 0 ≤ n < M  (3.5)  m=0  For speech recognition, normally, the number M of filters is from 10 to 20 and the MFCC produced from the first few filters are the most effective in recognition. In our experiment, we use M = 12  3.3 Linear Predict Coding Cepstrum (LPCC)  The MFCC was proved to be better than the LPC cepstrum for recognition by using the dynamic time warping (DTW) method [5], but the computational complexity for the MFCC is much heavier than that of the LPC cepstrum. The LPC coefficients can be easily obtained by Durbin's recursive procedure [11-13] and their cepstra can be quickly  found by another recursive equations [11-13] without computing the discrete Fourier transform (DFT) and the inverse DFT, which are computationally complex and time consuming. The LPC method can also provide a robust, reliable and accurate method for estimating the parameters that characterize the linear and time-varying system [3, 11-13]. The following is a brief discussion of LPC method. It is assumed [13] that the sampled speech waveform sˆ(n) can be linearly predicted from the past p samples of s(n) . Let  p sˆ(n) = ∑ ak s(n − k) k =1  (3.6)  where p is the number of the past samples and let E be the squared difference between s(n) and sˆ(n) over N samples of s(n) , i.e.,  N −1 E = ∑[s(n) − sˆ(n)]2. n=0  (3.7)  The unknown ak , k = 1, ..., p , are called the LPC coefficients and can be solved by the least square method. The most efficient method known for obtaining the LPC coefficients is Durbin's recursive procedure [3, 11-13]. Here in our experiments, p = 12 , because the cepstra in the last few elements are almost zeros.  Both LPCC and MFCC are the method to compress or simplify the huge speech data s(n) of a syllable into a simple data without loss of speech information. The LPCC is more or less like the sufficient statistics of a random samples in statistics [14]. The LPC coefficients ak , k = 1, ..., p , are actually the least squares estimators of the regression coefficients, i.e., the minimum variance linear estimators ak of the regression coefficients [14]. The huge data of a frame are well-represented by the LPC coefficients unless LPC coefficients are too small, i.e., the estimates ak of the regression coefficients are not significant as compared with noise. On the other hand, to produce a MFCC, one has to obtain the DFT of a frame of the huge data and after the Mel filter banks smooth the spectrum, performs the inverse DFT on the logarithm of the magnitude of filter bank output. It seems to us that the formula in (3.1)-(3.5) to produce a MFCC are a little arbitrarily or artificially or experimentally adjusted for human ears. There is no theoretical theory to support the MFCC to well represent a syllable without loss of information. Hence in this paper, we create a huge database from common mandarin sentences to obtain the recognition rates using the LPCC and MFCC respectively.  3.4 Feature Extraction [9]  Our method to extract the feature from LPCC (MFCC) is quite simple. Let x(k) = (x(k)1, ..., x(k) p ), k = 1, ..., n , be the LPCC (MFCC) vector of size p = 12 for the k-th frame of a speech waveform, where n is the length of the LPCC (MFCC) sequence and p is the number of LPC coefficients in each frame. Normally, if a speaker does not intentionally elongate pronunciation, a mandarin syllable has 30-70 vectors of LPCC (MFCC).  Since an utterance of a syllable is composed of two basic parts: stable part and feature part. In the feature parts, the vectors have a dramatic change between two consecutive vectors, representing the unique characteristics of the syllable utterance and in the stable parts, the vectors stay about the same. Even if the same speaker utters the same syllable, the duration of stable parts of the sequence of LPCC (MFCC) vectors changes every time with nonlinear expansion and contraction and hence the duration of the portion of feature vectors and duration of stable parts are different every time. Therefore, the duration of stable parts is contracted such that the compressed speech waveform has about the same length of the sequence of the vectors. Li [9] proposed several simple compression techniques to contract the stable parts of the sequence of vectors. We state a simple one with good recognition rate as follows:  Let x(k) = (x(k)1, ..., x(k) p ), k = 1, ..., n , be the k-th vector of a LPCC (MFCC) sequence with n vectors, which represents a mandarin syllable. Let the difference of two consecutive vectors be denoted by  p ∑ D(k) = | x(k)i − x(k −1)i |, k = 2, ..., n. i =1  (3.8)  In order to accurately identify the syllable utterance, a compression process must first be performed to remove the stable and flat portion in the sequence of vectors. A LPCC (MFCC) vector is removed if its absolute difference D(k) from the previous vector x(k −1) is too small. In this study, a squared difference criterion is also used to remove the stable and flat portion of the sequence. The criterion is expressed as follows:  p ∑ D(k) = [x(k)i − x(k −1)i ]2 , k = 2, ..., n. i =1  (3.9)  Let x′(k), k = 1, ..., m(< n) , be the new sequence of LPCC (MFCC) vectors after deletion. We think that the first part (about first 40 vectors) of an utterance of a mandarin syllable contains main features which can most represent the syllable and the rest of the sequence contains the "tail" sound, which has a variable length. If a speaker intentionally elongates pronunciation of a syllable, the speaker only increases the tail part of the sequence. The length of the feature part stays about the same. As in [9], we partition the feature part (first 40 vectors of the new sequence) into 8 equal segments and partition the tail part with variable length into two equal segments. If the length of the new sequence of vectors representing a syllable is less than 40, we neglect the tail sound and partition the new sequence into 10 equal segments. The average value of the LPCC (MFCC) in each segment is used as a feature value. Note that the average values of samples tend to have a normal distribution. This compression produces 12×10 feature values for each mandarin syllable.  4. Experimental Results  There are two speech recognitions implemented in our study. One is the digit recognition in a noisy environment and the other is the speech recognition on the mandarin monosyllables which are most commonly used in general conversations.  The following is a flow chart to show the speech recognition on a syllable. Figure 1. Flowchart of a syllable recognition  In put sample of unknown syllable  speech waveform  Receiver  A/D Converter  Pre-Processing delete noise Database containing means and variances of LPCC and MFCC of all known syllables  Compute LPCC and MFCC of unknown syllable  Compare LPCC and MFCC of unknown syllable with all known syllables  Syllable identification by Bayes rule  4.1 The Digit Recognition The digit recognition is implemented in a noisy environment, a classroom with windows open, which has noise from students inside classroom and from students and autos on the street outside classroom. The database of 10 mandarin digits is created by 19 persons (9 female and 10 male students) who pronounce 10 digits (0-9) once. The speech signal of a mandarin monosyllable is sampled at 10 kHz . A Hamming window with a width of 25.6 ms is applied every 12.8 ms for our study. A 256 point Hamming window is used to select the data points to be analyzed. In our experiments, we use this database to produce the LPCC (MFCC) and obtain a 12×10 matrix for each digit sample. On the average, the time to produce a MFCC using DFT and formula in Section 3.2 is 5.5 times as much as to produce a LPCC. Among 19 samples (pronounced by 19 students) of each mandarin digit, pick up one sample (from one student) for recognition and the rest of 18 samples (from the other 18 students) of the digit is used for training, i.e., the rest of 18 samples of this digit is used to estimate the parameters which represent the digit. Hence each of 19 students has to be tested, i.e., there are 19 testing samples for each digit. Since the average value of samples tends to be normally distributed. In order to reduce computation for classification, we assume that all elements in the 12×10 matrix of feature values are stochastically independent. It was proved [15] that using weighted variance in the Bayes decision rule for each class may increase the recognition rate. Hence, the conditional normal density given syllable ci with weighted variance c can be represented as  ⎡k  ∏ f  (x1, ...,  xk  |  ci )  =  ⎢ ⎢⎣  l =1  
Abstract This paper explores the possibilities of using independent component analysis (ICA) for features extraction that could be applied to word sense induction. Two different methods for using the features derived by ICA are introduced and results evaluated. Our goal in this paper is to observe whether ICA based feature vectors can be efﬁciently used for word context encoding and subsequently for clustering. We show that it is possible, further research is, however, necessary to ascertain more reliable results. 
In studies of automatic text processing, it is popular to apply the probabilistic topic model to infer word correlation through latent topic variables. Probabilistic latent semantic analysis (PLSA) is corresponding to such model that each word in a document is seen as a sample from a mixture model where mixture components are modeled by multinomial distribution. Although PLSA model deals with the issue of multiple topics, each topic model is quite simple and the word burstiness phenomenon is not taken into account. In this study, we present a new Bayesian topic mixture model (BTMM) to overcome the burstiness problem inherent in multinomial distribution. Accordingly, we use the Dirichlet distribution for representation of topic information beyond document level. Conceptually, the documents in the same class are generated by the associated multinomial distribution. In the experiments on TREC text corpus, we show the results of average precision and model perplexity to demonstrate the superiority of using proposed BTMM method. 
This paper describes our Korean-Chinese cross-language information retrieval system. Our system uses a bi-lingual dictionary to perform query translation. We expand our bilingual dictionary by extracting words and their translations from the Wikipedia site, an online encyclopedia. To resolve the problem of translating Western people’s names into Chinese, we propose a transliteration mapping method. We translate queries form Korean query to Chinese by using a co-occurrence method. When evaluating on the NTCIR-6 test set, the performance of our system achieves a mean average precision (MAP) of 0.1392 (relax score) for title query type and 0.1274 (relax score) for description query type. 摘要 本文描述我們所提出之韓中雙語跨語言檢索系統。我們採用韓中雙語辭典進行問題之翻譯，並 利用線上維基百科以及韓國 Naver 網站來擴增我們雙語辭典的覆蓋率。此外，針對韓文中西 方人名的翻譯，我們提出一音譯對應的搜尋方法。對於韓中翻譯時的歧義性問題，我們採用 Mutual Information 方法來解決。我們使用 NTCIR-6 之 test set 測試我們韓中跨語言檢索系統 之效率，其使用標題部分進行查詢時之 Mean average precision (MAP) 之結果為 0.1392；使 用敘述部分進行查詢時之 MAP 為 0.1274。 Keywords: Korean-Chinese cross-language information retrieval, query translation 關鍵詞：韓中跨語言資訊檢索，問題翻譯 
摘要 在自動語音辨識的研究上，如何有效地降低背景雜訊的影響，以增加語音辨識系統的強健 性，一直是一大研究重點，其中語音特徵參數正規化法是廣為人用的強健技術之一。然而，對 於多變的語音訊號該如何準確的估算出其不同時段的統計值，是影響語音特徵參數正規化法效 果的一個重要因素。本論文主要是針對在加成性雜訊環境下，對不同的特徵參數提出更有效率 且準確的統計值補償法，以降低加成性雜訊對語音特徵參數的影響。我們提出了運用虛擬雙通 道碼簿為基礎之特徵參數補償技術，其中包含三種方法：倒頻譜統計補償法、線性最小平方回 歸法與二次最小平方回歸法。我們將這些方法運用在四種不同的語音特徵參數的補償上，發現 都能有效降低加成性雜訊對語音特徵的影響，進而大幅提升辨識率，同時，在與傳統以段落為 基礎的特徵參數正規化技術比較下，我們所提出的方法可達到更佳的強健效果。 Abstract In this paper, we propose several compensation approaches to alleviate the effect of additive noise on speech features for speech recognition. These approaches are simple yet efficient noise reduction techniques that use online constructed pseudo stereo codebooks to evaluate the statistics in both clean and noisy environments. The process yields transforms for noise-corrupted speech features to make them closer to their clean counterparts. We apply these compensation approaches on various well- known speech features, including mel-frequency cepstral coefficients (MFCC), autocorrelation mel-frequency cepstral coefficients (AMFCC), linear prediction cepstral coefficients (LPCC) and perceptual linear prediction cepstral coefficients (PLPCC). Experimental results conducted on the Aurora-2 database show that the proposed approaches provide all types of the features with a significant performance gain when compared to the baseline results and those obtained by using the conventional utterance-based cepstral mean and variance normalization (CMVN). 關鍵詞：自動語音辨識、虛擬雙通道碼簿、倒頻譜統計補償法、線性最小平方回歸法、二次最 小平方回歸法 Keywords: automatic speech recognition、pseudo stereo codebooks、cepstral statistics compensation,  linear least squares regression, quadratic least squares regression 一、緒論 本論文主要重點是在加成性雜訊環境下，對語音特徵參數補償法的探討，目的是使測試語音的 統計特性在經過補償後能更接近訓練語音的統計特性。 我們運用四種語音特徵參數擷取技術結合兩大類特徵參數補償法，並觀察兩類特徵參數補 償法之間的差異與優缺點。本論文中所討論的兩大類特徵參數補償法分別為： (1)以段落為基礎之特徵參數正規化法 即 傳 統 的 整 段 式 倒 頻 譜 平 均 與 變 異 數 正 規 化 法 [1](utterance-based cepstral mean and variance normalization，U-CMVN)與分段式倒頻譜平均與變異數正規化法[2](segmental cepstral mean and variance normalization，S-CMVN)。前者是以一整段語句為基準去估算該維特徵參數的 統計值，並執行特徵參數正規化法；後者則是將每段語句以一小段的片段為基準，去估算該片 段的統計值，然後執行特徵參數正規化。 (2)以碼簿為基礎之特徵參數補償法 為了更精確地估測語音特徵參數統計值，以執行特徵參數補償與正規化法進而消除雜訊影 響，我們提出透過虛擬雙通道碼簿，來幫助我們更準確地估算出代表訓練語音與測試語音的統 計值，並藉由較準確的統計值來執行特徵參數補償，以提升辨識效率。其中包含三種方法：倒 頻譜統計補償法(cepstral statistics compensation, CSC)、線性最小平方回歸法(linear least squares regression, LLS)與二次最小平方回歸法(quadratic least squares regression, QLS)。 在之後的第二章裡，我們簡單介紹本論文所使用的四種語音特徵參數抽取流程。第三章介 紹傳統之以段落為基礎特徵參數正規化法，第四章之討論即為本論文之重點：包括虛擬雙通道 碼簿的建立方法，及三種以碼簿為基礎之特徵參數補償法。第五章與第六章分別為實驗環境介 紹與實驗結果及討論。最後，第六章包含了簡要的結論。 二、各種語音訊號特徵參數抽取流程的介紹 本章節介紹在語音訊號處理中四種常用的語音特徵參數及其抽取流程，分別為梅爾倒頻譜 係數(mel-frequency cepstral coefficients，MFCC)、自相關梅爾倒頻譜係數[3](autocorrelation mel-frequency cepstral coefficients，AMFCC )、線性預測倒頻譜係數[4][5](linear prediction cepstral coefficients ， LPCC) 以 及 感 知 線 性 預 測 倒 頻 譜 係 數 [6](perceptual linear prediction cepstral coefficients，PLPCC)。 我們將使用這四種語音特徵參數來驗證本論文所提出的強健性語音特 徵參數技術，並且與其他強健性方法運用在這四種特徵參數上做比較。 (一) 梅爾倒頻譜係數 (mel-frequency cepstral coefficients，MFCC) 圖一為梅爾倒頻譜係數擷取流程圖，梅爾倒頻譜係數結合了人在發音上與聽覺上的諸多性質， 是目前語音研究上，最常被使用的特徵參數。  語音訊號  預強調  音框化  漢明視窗  能量值  離散傅立葉轉換 梅爾濾波器組 對數轉換  em cm = ⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩cΔΔm2cc,memm,,ΔΔe2mem ⎫⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎭  離散餘弦轉換 c m  圖一、梅爾倒頻譜特徵擷取流程圖  (二) 自相關梅爾倒頻譜係數 (autocorrelation mel-frequency cepstral coefficients，AMFCC) 自相關梅爾倒頻譜係數是利用自相關序列結合梅爾倒頻譜係數求取步驟來做特徵參數抽 取[1]，其流程即在一音框內的語音訊號經過漢明視窗處理後，取其偏移式自相關係數(biased autocorrelation coefficients)，並捨棄其前端約 2ms 係數後，再經過一凱瑟視窗以降低高頻效應。 除了上述步驟外，其餘取頻譜、對數轉換及離散餘弦轉換等流程，皆與梅爾倒頻譜係數抽取流 程相同。圖二為自相關梅爾倒頻譜係數之抽取流程。  語音訊號  預強調 音框化 漢明視窗 能量值  偏移式自相關序列 捨棄前2ms 凱瑟視窗  em c m = ⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩cΔΔm2cc,memm,,ΔΔe2mem ⎫⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎭  離散傅立葉轉換 梅爾濾波器組 對數轉換 離散餘弦轉換 c m  圖二、自相關梅爾倒頻譜特徵擷取流程圖 (三) 線性預測倒頻譜係數(linear prediction cepstral coefficients，LPCC) 線性預測(linear prediction)的基本原理是假設目前的聲音取樣值可由在前面的 p 個取樣 值，以線性組合來預測。圖三即為線性預測倒頻譜係數之擷取流程圖。如同前兩種特徵參數擷 取技術，我們將語音訊號經過預強調後，切割成許多一小段的音框與漢明視窗的處理後取其自 相關係數，透過 Levinson Durbin 演算法求得線性預測係數，最後將線性預測係數轉換成倒頻 譜，便得到線性預測倒頻譜係數(linear prediction cepstral coefficients，LPCC)。  語音訊號  預強調  音框化  漢明視窗  自相關分析  Levinson Durbin 演算法  倒頻譜轉換  ci  ci  =  ⎧ ⎪⎪ ⎨  ci Δ  {c  i  }  ⎫ ⎪⎪ ⎬  ⎪ ⎪⎩  Δ  2  {c  i  }⎪⎪⎭  圖三、線性預測倒頻譜特徵擷取流程圖  (四) 感知線性預測倒頻譜係數(perceptual linear prediction cepstral coefficients，PLPCC) 感知線性預測倒頻譜係數的擷取流程圖如圖四，與線性預測係數倒頻譜擷取流程不同之處 在於：(1)它經過模擬人耳的梅爾濾波器組，對於頻譜作頻率校準(frequency warping)的處理。(2) 它利用等響度曲線(equal loudness curve)對強度頻譜做預強調。(3)對於經過預強調後的強度頻譜 取三次方根(cubic root)，相當於對強度與響度之間做校準(intensity-loudness warping)的動作。這 些改變都是針對人的聽覺特性而做的。  預強調 音框化 漢明視窗 離散傅立葉轉換  梅爾濾波器組  語音訊號  等響度曲線預強  強度-響度冪次法則  離散餘弦轉換  ci  =  ⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩  ci Δ Δ  {c i 2 {c  }i }⎫⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎭  ci  圖四、感知線性預測倒頻譜係數的擷取流程圖  三、整段式與分段式之強健性語音特徵等化技術 倒頻譜平均值與變異數正規化法(CMVN)是常被使用來強健語音特徵參數的方法之一，其 作法是將一連串語音資料中的每一維倒頻譜特徵參數做統計量的調整，以便消除雜訊對語音的 影響；在作法上，整段式的倒頻譜平均值與變異數正規化法[1](U-CMVN)是利用一整段語音特 徵求取平均值與變異數，因此所用的音框長度隨特徵係數序列長短而異，而分段式倒頻譜平均 值與變異數正規化法[2](S-CMVN)的作法，是將每一維語音特徵參數，以當時的音框為中心， 對其前後數十個音框做分段統計量的計算，然後對當下的音框作正規化處理，以下將分別對這 兩種方法詳加介紹。  ( 一 ) 整 段 式 倒 頻 譜 平 均 值 與 變 異 數 正 規 化 法 (utterance-based cepstral mean and variance normalization，U-CMVN) 乾淨的語音訊號在經過加成性雜訊干擾後，其倒頻譜之平均值會和原本的乾淨語音倒頻譜 的平均值之間會存在一個偏移量，而其變異數相對於乾淨語音特徵參數而言則會有壓縮性，因 此會造成訓練與測試特徵的不匹配而降低辨識效果。而使用倒頻譜平均值與變異數等化法 [1](CMVN)可將每一維倒頻譜特徵參數之平均值化為零，並將其變異數正規化為 1，這樣就能 降低上述所謂的偏移量與壓縮性，進而提升倒頻譜參數的強健性。 整段式倒頻譜平均值與變異數等化法的作法如(式 3-1)，假設 {Y [n ],n = 1, 2,…,N } 為一由語 音資料擷取所得到的某一維倒頻譜特徵參數序列，而經過整段式倒頻譜平均值與變異數等化法 處理後，得到新的特徵參數 {YU−CMVN [n ],n = 1, 2,…, N } ，其中的 {YU−CMVN [n ],n = 1, 2,…, N } 平均值 與標準差是經由整段語音的音框求取而得，如式(3-2)與式(3-3)。  其中  YU −CMVN [n ]  =  Y[n] − σ  μ Y  ,  Y  n = 1, 2,...., N  ∑ μ Y  =  
The performance of a speech recognition system is often degraded due to the mismatch between the environments of development and application. One of the major sources that give rises to this mismatch is additive noise. The approaches for handling the problem of additive noise can be divided into three classes: speech enhancement, robust speech feature extraction, and compensation of speech models. In this thesis, we are focused on the second class, robust speech feature extraction. The approaches of speech robust feature extraction are often together with the voice activity detection in order to estimate the noise characteristics. A voice activity detector (VAD) is used to discriminate the speech and noise-only portions within an utterance. This thesis  primarily investigates the effectiveness of various features for the VAD. These features include low-frequency spectral magnitude (LFSM), full-band spectral magnitude (FBSM), cumulative quantized spectrum (CQS) and high-pass log-energy (HPLE). The resulting VAD offers the noise information to two noise-robustness techniques, spectral subtraction (SS) and silence log-energy normalization (SLEN), in order to reduce the influence of additive noise in speech recognition. The recognition experiments are conducted on Aurora-2 database. Experimental results show that the proposed VAD is capable of providing accurate noise information, with which the following processes, SS and SLEN, significantly improve the speech recognition performance in various noise-corrupted environments. As a result, we confirm that an appropriate selection of features for VAD implicitly improves the noise robustness of a speech recognition system. Keywords ： voice activity detection, spectral magnitude, spectral subtraction, speech recognition 一、緒論 在我們的生活環境中，影響語音辨識結果的因素很多。其中一重要的因素為語音辨 識系統訓練與應用環境上的不匹配(environmental mismatch)，此不匹配的相關因素 又包含了加成性雜訊(additive noise)、摺積性雜訊(convolutional noise)以及頻寬限 制(bandwidth limitation) 等因素。其中加成性雜訊也可說是背景雜訊，這是因為語音 辨識系統所處在的環境，並非都像實驗室毫無其他干擾雜訊。也許系統是處於地鐵站 中、餐廳、機場等這些具有其他干擾源的環境。甚至旁人呼吸喘息聲音都會混進語音裡 面，造成辨識率的降低。摺積性雜訊也稱為通道雜訊或是通道失真，主要是因為麥克風 的不同、傳輸線材的遮蔽效應不好而受外在電磁波影響所造成的。頻寬限制也是因為收 音通道的差異所帶來的影響。後面兩項的因素在電話語音辨識系統就非常的明顯。在有 限頻寬的電話線，把通話者頻寬做限制以便利傳輸，這往往會造成語者的聲音變調，甚 至通道失真的影響還會造成使用者兩端會發生吱吱的雜音，造成語音辨識很大的困擾。 為了改善以上所述之環境上的不匹配，有眾多學者提出各種改進的方法，其中一類 的方法為強健性語音特徵參數技術，強健性語音特徵參數技術的主要目的是在抽取出不 容易受到外在環境干擾而失真的語音特徵參數，進而突顯出語音變化的部分。在許多針 對加成性雜訊所發展的強健性語音特徵參數技術裡，如何取得雜訊部分的資訊是很重要 的，亦即在一段語音訊號中，我們通常必須偵測純雜訊所在的位置，以利於雜訊資訊的 取得，其相關的技術則統稱語音活動偵測法(voice activity detection, VAD)，或簡稱為端 點偵測法(endpoint detection)。本論文的重點即在發展一系列使用端點偵測法的信號特 徵，藉由這些特徵，使端點偵測的結果更精確，進而使之後的強健性語音特徵參數技術 能達到更好的效果。在論文中。我們將介紹幾個用以語音偵測的信號特徵，分別為低頻 帶頻譜強度(low-frequency spectral magnitude, LFSM)、全頻帶頻譜強度(full-band spectral magnitude, FBSM)、累積量化頻譜(cumulative quantized spectrum, CQS)、 以及高通對數能量(high-pass log-energy, HPLE)等。我們嘗試把這些端點偵測的方 法與兩種強健式語音特徵參數擷取法結合，即頻譜消去法(spectral subtraction)與靜 音對數能量正規化法(silence log-energy normalization, SLEN)等，發現皆有相當程 度的提高辨識效率。  圖一、雜訊干擾語音之示意圖 本 論 文 其 餘 部 分 共 分 為 五 章，其 中 第 二 章 詳 細 介 紹 所 提 出 之 端 點 偵 測 的 各 種 信 號 特 徵，第 三 章 介 紹 本 論 文 所 用 的 兩 種 強 健 語 音 特 徵 技 術，第 四 章 為 實 驗 環 境 的 設 定，第五章為實驗結果與討論，最後，第六章則包含了簡要的結論。  二、端點偵測所使用之信號特徵  端點偵測法(endpoint detection)或稱為語音活動偵測法(voice activity detection, VAD)是指  可以將一段語音中雜訊與語音的位置偵測出來的演算法。藉由一個有效的端點偵測法，  我們可以利用所求得的純雜訊音框，準確的估測雜訊的資訊，例如其頻譜能量等。進而  促成各種強健技術的使用，以達到雜訊的抑制，降低雜訊對語音訊號的影響。在以下幾  節，我們將介紹用以端點偵測的各種信號特徵。  （一）低頻帶頻譜強度(low-frequency spectral magnitude, LFSM)  我 們 觀 察 到 無 論 任 何 種 類 的 雜 訊 ， 在 頻 帶 [0,50Hz]之 間 都 有 相 當 比 例 的 能  量。同時，語音在此低頻帶的能量也具有一定程度的比例。因此我們根據此頻譜  上的特性，去計算每個音框在此低頻帶的頻譜強度。根據此強度值，判斷此語音  音框是否為純雜訊音框，或是包含語音的音框。 首 先 ， 我 們 假 設 {xm [n ],1 ≤ n ≤ N } 是 語 音 訊 號 的 第 m 個 音 框 ， 將 其 取 K (K ≥ N ) 點的離散傅立葉轉換，我們將可得到此音框所對應之頻譜如下式(1)：  ∑ ( ) X f (m) k  =  X (m) [k ]  =  N −1 xm  −j 2πnk [n ]e K ,  0 ≤k ≤K −1  (1)  n=0  其中 fk 為頻率，其值如下式：  fk  =  Fs 2K  k  (2)  其中 Fs 為取樣頻率，因此我們定義出頻帶 ⎡⎣FL,FU ⎤⎦ 之頻譜強度計算方式為：  ∑ ( ) Y = (m) [FL,FU ]  Xm fk  (3)  FL ≤fk ≤FU  根據式(3)，我們可以計算每一音框之低頻帶頻譜強度，即 0 至 50Hz 以內的低頻帶頻譜  強度如下：  ∑ ( ) Y = Y = (m) Low  (m ) [0,50]  Xm fk  (4)  0≤fk ≤50  接著我們以一段語音前 P 個音框之低頻帶頻譜強度的平均為參考值，其計算如下：  ∑ θ  =  λ  ⎛⎜⎜⎜⎝  
機 器 學 習 方 法 主 要 可分 為 監 督 式 (supervised learning) 及 非 監 督 式 (unsupervised learning)。兩者的差別在於前者的訓練語料有標記答案的而後者沒有，我們所採用的方 法是監督式的方法。無論是哪一種機器學習的詞義辨識演算法都需要利用語境的訊息。 例如 Purandare and Pedersen (2004) 採用非監督式的方法，從沒有標示詞義純文字語料 抽出語境並將機讀辭典 Wordnet 裡面不同詞義的定義去除功能詞後建立共現矩陣 (co-occurrence matrix)，利用 Singular Value Decomposition (SVD)將維數降到 100，最後 用 Latent Semantic Indexing (LSI)找出某一句中的目標詞最有可能的詞義。Jurafsky and Martin (2000)將常用的語境特徵分成兩類。一類是搭配語特徵(collocational features)，另 一類是 bag of words information。兩者的最大差別在於後者只考慮某些詞在目標詞左右 一定範圍的詞有沒有出現，不考慮這些詞彼此或跟目標詞前後的關係，而前者則納入與 目標詞前後相對位置的訊息，甚至用語法剖析器得到語法依存關係。 詞義辨識方法除了可以利用 Semantic Concordancer 或 Senseval 這些有標示詞義的 語料之外，還可以利用 pseudoword 或雙語語料。pseudoword 是 Gale et al. (1992)和 Schutze(1992)為了省去標示詞義所需的大量人力與時間所創造出來的方法。透過人造的 岐義詞如 banana-door，將語料中所有出現 banana 或 door 都代換成 banana-door，這樣 就可以得到類似人工標記詞義的訓練語料。此外，某一個有岐義的詞在另一個語言通常 沒有岐義，例如英文的 duty 有兩個意義，但在中文裡則由海關和責任兩個詞來表達。 Brown et al. (1991) 及 Gale et al. (1992)利用這個特性，以英法雙語語料庫作為訓練語 料，採取目標詞左右若干詞（例如 50 個詞）構成一個語境向量(context vector),再利用 Bayesian classification 來選擇在某一個語境當中哪一個詞義的機率最大。我們也採用 Bayesian classification 但搭配不同的特徵。Bayesian classification 的概念是目標詞周圍的  詞會反映出目標詞的意義，因此將周圍的詞以及目標詞做統計再利用機率選擇詞義，在 第三節中會有詳細的介紹。 Yarowsky (1995)注意到在某一篇文章中一個目標詞的詞義通常是固定某一個詞義 (One sense per discourse)。且目標詞的搭配語提示了這個目標詞的詞義(One sense per collocation) 。本文所採用搭配語作為機器學習演算的法特徵受到 Yarowsky (1995)的啟 發。Lin (1997)有鑑於以機器學習分類器(classifier)來辨識詞義需為不同的詞分別訓練出 不同的分類器，頗不方便，因此提出一種使用同一種知識來源(knowledge source)的方 法。他利用自己所發展的 MINIPAR 英文剖析器得到的語法依存關係(dependency relations)，如動詞與受詞的關係作為機器學習演算法的特徵。比較特別的地方在於他的 方法不需要標示詞義的語料，而是利用相同語意的詞會出現在具有相同的依存關係所組 成的局部語境(local context)。Lin (1997) 的正確率達到與其它機器學習演算法相同水 準。本文採用語法依存關係作為詞義辨識的特徵源自於 Lin (1997)的想法。有關於特徵 的選取，Le and Shimazu (2004)針對英文詞義辨識提出數個特徵並以 Forward Sequential Selection Algorithm 來得到最佳的特徵組合，本文採用 Le and Shimazu (2004)所提出的 5 個特徵另外加上 4 個特徵，並仿照 Le and Shimazu (2004)所使用的 Forward Sequential Selection Algorithm 得到最佳特徵的組合。 除了上面介紹的方法，還有許多詞義辨識的方法，例如利用 mutual information 的 Flip-Flop algorithm (Brown et al. (1991)),使用 decision list (Yarowsky (1994))等，限於篇幅 無法一一介紹。近幾年詞義辨識的演算法除了 Naïve Bayes 之外，越來越多人使用 Maximum Entropy，Support Vector Machine，及 Conditional Random Field 等較新的機器 學習演算法。本文所選取特徵和組合的方法也可以與這些方法一起使用。 三、我們採取的方法  （一）、Bayesian Classification  在我們的實驗中，我們採用 Bayesian Classification 搭配多種特徵的方法，下面簡述  Bayesian Classification。  假設我們現在要對一個目標詞做詞義辨認，該目標詞的詞義有 k 個，依序是  ，  則目標就是要找出一個 ，使得  為最大，c 是目標詞所含有的某種特徵。根據貝  式定理，可以得到如下的等式：  因此  我們所有的實驗都是使用這個方法來作詞義辨認，差別是在於選取的特徵的不同。 （二）、Forward Sequential Selection Algorithm 在特徵的選取方面，由於我們嘗試了很多種特徵，假如特徵有 7 種那麼特徵組合的種類 就有 127 種，數量非常的可觀，一個一個將所有的組合做實驗非常沒有效率，因此使用 Le and Shimazu (2004)所提出的 Forward Sequential Selection 演算法來挑選特徵。這個方 法大致上是先令一個特徵的集合 S 為空集合，首先挑一個最好的特徵放進 S 中，接著 將每一個特徵都放進 S 中看哪個得到的正確率最高來決定第二個要放入 S 中的特徵， 如此反覆直到最後正確率不再增加為止，最後集合 S 中的特徵就會是一個很不錯的特徵 組合，雖然未必真的是最佳解但應用在英文詞義辨認的特徵選取上與真正最佳解的差異 非常小。 （三）、特徵 我們一共嘗試了 9 種特徵，分別以 F1 到 F9 命名之，前五個主要是針對目標詞周 圍的詞以及其詞性，這 5 個特徵都是 Le and Shimazu (2004)所使用的特徵，第六個以及 第七個則著重在詞的依存關係，例如主詞與動詞，動詞與受詞的關係等等，而最後兩個 則是利用 HowNet 的取目標詞的前後兩個詞以及與目標詞有依存關係的 HowNet 義元 (語義特徵)當作特徵，在此一一介紹。 F1 是直接把目標詞周圍的詞做為特徵，但是會排除ㄧ些如 is, a 之類的功能詞(stop words)。 F2 也是目標詞周圍的詞，但會加上位置的資訊，例如目標詞是 art 時，〝The art of design〞 中會被取出的特徵會是{(The, -1), (of, 1), (design, 2)}。 F3 跟 F2 類似，但不同的是 F3 是取出詞性。 F4 則是目標詞與周圍詞的組合，同樣以〝The art of design〞為例，會被取出的特徵有 {The-art, art-of, The-art-of, art-of-design, The-art-of-design}。 F5 與 F4 類似但取詞性組合。  F6 則是利用 Sketch Engine，將可能與目標詞有語法搭配關係的詞列為特徵。 F7 是利用 Stanford Parser，將 Stanford Parser 所剖析出的與目標詞有依存關係的詞以及 依存關係的類別列為特徵。 F8 是取目標詞前後兩個詞的 HowNet 義元做為特徵。 F9 是先利用 Stanford Parser 找出與目標詞有依存關係的詞，再取出其 HowNet 義元做為 特徵。 我們使用 Sketch Engine (Kilgarriff et al. (2004))找出跟目標詞具有語法搭配關係的 所有詞。圖一是利用 Sketch Engine (http://www.sketchengine.co.uk/)的 word sketch 查 詢 duty 個目標詞的輸出，object_of 這一欄表示目標詞可以作為這些詞的受詞的搭配 語，subject_of 表示目標詞可以作為這些詞的主詞的搭配語, a_modifier 是可以修飾這個 目標詞的形容詞，n_modifier 是可以修飾這個目標詞的名詞, modifies 則是可以被這個目 標詞修飾的詞。我們選擇的英文語料超過 20 億，如此龐大的語料可以確保得到大部分 的搭配語。 圖一 Sketch Engine Word Sketch 的輸出結果 Stanford Parser 是史丹福大學 Klein and Manning (2003)發展出來的多國語言剖析 器，只要輸入符合 Pen Treebank 格式的語法樹庫，即可自動從語法樹庫中訓練得到該語 言的語法剖析器。下面的例子是 Stanford Parser 的輸出結果。除了標示詞性，語法結構，  最特別的是還將語法的依存關係列出來,例如,nsubj 表示動詞和主詞的關係，dobj 表示動 詞和受詞的關係，advmod 表示動詞和副詞修飾語的關係，amod 表示名詞和名詞修飾語 的關係。必須強調的是 Stanford Parser 的語法依存關係是從語法樹庫歸納出來後利用 regular expression 抽取出來的，因此即使語法剖析的結果正確，語法依存關係不一定正 確。下面是 Stanford Parser 的輸出結果。 The/DT government/NN first/RB established/VBD modern/JJ criminal/JJ investigation/NN system/NN in/IN 1946/CD ./. (ROOT (S (NP (DT The) (NN government)) (ADVP (RB first)) (VP (VBD established) (NP (NP (JJ modern) (JJ criminal) (NN investigation) (NN system)) (PP (IN in) (NP (CD 1946))))) (. .))) det(government-2, The-1) nsubj(established-4, government-2) advmod(established-4, first-3) amod(system-8, modern-5) amod(system-8, criminal-6) nn(system-8, investigation-7) dobj(established-4, system-8) prep(system-8, in-9) pobj(in-9, 1946-10) 知網 Hownet(http://www.keenage.com)是由董振東所發展出來（參考 Dong and Dong (2006)）。Hownet 架構不同於 Wordnet，Wordnet 基本上是一個詞彙網路，同樣語意的詞 屬於同一組的 synset，裡面的定義，例句都相同。Wordnet 裡面包含的詞彙語意關係包 括上位詞，下位詞等。Hownet 則利用抽象的義元作為表達所有概念的工具和單位。 Hownet 包含的訊息相當的多，是一個中英雙語的知識庫，包括義元，語意角色，上下 位關係，部件與整體關係等等語意訊息。義元類似一個語意特徵。Hownet 對於醫生 (doctor)的義元表示法為 {human| 人 :HostOf={Occupation| 職 位 },domain={medical| 醫 },{doctor| 醫  治:agent={~}}} Hownet 裡面的訊息表示醫生是一個人，具有職位，屬於醫學領域，醫生在醫治這 個事件裡扮演主事者的語義角色。在我們的實驗中，我們只使用 Hownet 表示法當中第 一個義元，例如：doctor 的第一個義元是 human。對於名詞而言，第一個義元相當於這 個詞的語意類別或本體 ontology。  四、實驗結果  在 F1~F6 中，都必須取一個 Window Size，否則會導致特徵和目標詞以及詞義的相關聯 性表現不出來，因此這六種特徵都會有 Window Size 的實驗。而我們的實驗是對各種特 徵先獨立的來做詞義辨認以得到各種特徵的最佳參數，每種特徵都最佳化以後，最後再 使用 Forward Sequential Selection Algorithm 來決定要採用哪些特徵。處理語料以及辨認 程式是以 Perl 及 C++寫成。 （一）、F1 F1 是最簡單的直接把目標詞周圍的詞做為特徵，但是會排除ㄧ些如 is, a 之類的 stop words，原因是 stop words 通常對於辨認一個詞的詞義沒有什麼幫助。表一顯示 F1 的最 佳 Window Size 為 3。  Window Size 
In order to train machines to ‘understand’ natural language, we proposed a universal concept representational mechanism called E-HowNet to encode lexical semantics. In this paper, we take interrogative constructions as examples, i.e. concepts or sentences for asking questions or making inquiries, to demonstrate the mechanisms of semantic representation and composition under the framework of E-HowNet. We classify the interrogative words into five types according to their semantic distinctions, and represent each type with fine-grained features and operators. The process of semantic composition and the difficulties of the representation, such as word sense disambiguation, will be addressed. Finally, we’ll show how machine discriminates two synonymous sentences with different syntactic structures and surface strings to prove that machine understanding is achievable. Keywords: semantic representation, interrogatives, E-HowNet 1. Introduction To understand natural language by machines, lexical semantic representation and composition are the most important techniques. In this paper, we will take the interrogatives as examples to demonstrate the mechanism of lexical semantic representation and composition in E-HowNet (Chen et.al., 2004). E-HowNet uses the word sense definition mechanism of HowNet (Dong, 1988) and the vocabulary of WordNet (Fellbaum,1998) synsets to describe concepts.1 Its goal is to achieve near canonical semantic representation, 
Question answering systems provide an elegant way for people to access an underlying knowledge base. Humans are not only interested in factual questions but also interested in opinions. This paper deals with question analysis and answer passage retrieval in opinion QA systems. For question analysis, six opinion question types are defined. A two-layered framework utilizing two question type classifiers is proposed. Algorithms for these two classifiers are described. The performance achieves 87.8% in general question classification and 92.5% in opinion question classification. The question focus is detected to form a query for the information retrieval system and the question polarity is detected to retain relevant sentences which have the same polarity as the question. For answer passage retrieval, three components are introduced. Relevant sentences retrieved are further identified whether the focus (Focus Detection) is in a scope of opinion (Opinion Scope Identification) or not, and if yes, whether the polarity of the scope matches with the polarity of the question (Polarity Detection). The best model achieves an F-measure of 40.59% using partial match at the level of meaningful unit. With relevance issues removed, the F-measure of the best model boosts up to 84.96%. 
We take the four following steps to extract collocations made of combinations of 2, 3, 4 words and/or part of speech, respectively. First, we use “Smadja’s Xtract” to extract the co-occurrence combinations of words and/or part of speech of varying distance by computing means and variances. Second, we evaluate the significances of collocation candidates by 2 metrics: mutual information and t-test value. At last, we compare the head words of tagged word sense corpus made by Academic Sinica with the collocation candidates. If in the same distance, the head words of collocation candidates match the ones made by Academic Sinica, we say they are collocations. In addition, we apply the collocation information produced from this research to word sense disambiguation. It reaches application rate of 20.07% and precision rate of 90.83%. ‫׮؟‬ҮƝǌǯӗϤҮƙͪǐԂϛΌƙɬғՏˁъжƙтͽǰ̙ƙT ‫˯ז‬ΌƙҮӮֽ‫ؙ‬ Keywords: Chinese collocation, mutual information, natural language processing, statistical method, t-test, word sense disambiguation. ưƚ‫׻‬ǖ ǋɅȪФ̡֯ȖǯẠ̈̄‫ؙ‬Ͷ҄ȁǡƵΏ̡͋ɨ‫ֻه‬չəǋɅƙ͈ͫɅշ̡Ƶˌ̟ƚɅշ ̡ɯͤЖԝԑҙƙɏՏˁ̡ѽ΀ƽƾӛɟ̀ǋɅƛʵΐɎϽ̡Տˁѝɟ˘шԧ˾̡ȯ 191  ̙ƙɩֹ̡̀ӗϤҮ (collocation) ժӮɩˁƙѵ͗͏˖ΐ˿ɐΐҮˏӣՏˁшԧ˾Ҡ ɊɏưϞ̫ȵ͙ԻσξӑЮ̡ҮӏзҰƛӗϤҮɏǋɅ̡ͮʻ՞ЄƽɇɟǋɅ̡Ӹ‫ق‬ƙ ˲Ȧɟưͷ˾̡˯ӮƛͮʻӗϤҮҨɉ̡֢̦ Smadja [1] ˯ӮӗϤҮɟȁƼșΐσկƝ 1ƚӗϤҮ͗ȻӑҮ̡хɊƜ2ƚӗϤҮ˥՞Єͪ‫؟‬Ɯ3ƚӗϤҮ͗΂֋Ȇз̡Ɯ4ƚӗϤ Ү˗ɟҮӏ̡ǐͪʑǪ˾ƛȩՏˑȯ̦Ԣ̈ӗϤҮ̡ʃ˯ƾяͪӦΟ̊ƙɹԢ̈ȚЃƵ ̡Տˁ֢шƙЏӛՑȯӗϤՏҮƛȁ˹Ԣ̈ӗϤҮɬϼ‫״‬ˠ̡ͮʻƙǂɐ͗ϦԢͺՏʼ Տΰϲъжƛɭ̈‫״‬ˠǌǯӗϤҮ̡ͪ‫؟‬ǯ‫إ‬ǔғ͗ͪӦҚǦ̡ƙɌɡȥͮʻʄȯтͽ ̡ǰəԢǂюչ̡ǌǯԂΰҸɯǜ̓ȁ‫״‬ˠȆǌǯӗϤҮƛ˘кȆ̡Ҡ̐ЎȐȁ‫ב‬ȯɏ ɬғՏˁͪ‫؟‬ъжƽƙ˓ɓƝҮӮɬϼոȵƚԂϛ‫ז‬ύƚ֮֡‫خ׽‬ȁǡ‫˙؛‬ֆ‫ة‬ƛ ȥͮʻѾȆЎ˧‫؜‬ҮӏǡҮ˾ͧɺͤ‫״‬ˠӗϤҮ̡΂ͼσկƙНȯ Smadja’s Xtract [1] Їтͽƽ̡ȡʕմǡ‫ن‬мմǎǰ̙ƙ̢К‫״‬ˠȆ˗ɟ‫ن‬ϼҶ؅չə̀ȿɅȆз̡ Үӏ˿Ү˾ǎхɊƙɀˑȯӗϤҮ‫و‬Ҩ˾ֵ̡ҹǰ̙ƝͪǐԂϛΌ (Mutual Information) ˥ T ‫˯ז‬Όƛѕԑȁƽ‫̡ىז‬Α‫־‬ӗϤҮƙɏѧ͊ʃ˯ӗϤҮ̡Іӡƙ͗Їʵΐӗ ϤҮӇưΐҮӮ̡ΖȰ [2]ƙʦΏНˠӫȰԢӣǌțͮʻϧҮӮոȵՏΰΦ SSTC (Sinica Sense-Tagged Corpus) [3]ƙɏͪɅȲոҮӏ˥˧‫؜‬ҮӏԂϛ̡ԅҶƼƙȲոҮɏ ՏΰΦϜօԂΰ̡̀ɟҮ˓ǌƙ͹Ӈ˗ЂưҮӮƙ̻ʦΏѵЎɡӗϤҮ‫״‬ˠͤҮӮոȵ ̣‫ؙ‬ƜҸưʴɀȁͪɅ̡ǰəҸɩ‫״‬ˠƻіҮǡșіҮǎӗϤҮƛѧ͊ʦΏЎкȆǎӗ ϤԂϛ‫ב‬ȯ̈ҮӮɬϼոȵъжƛ ȥǯх‫׼‬ɓƼƙпƴӪ͗ɟ‫؟‬ӗϤҮ‫״‬ˠʧыǎͪ‫؟‬ǯ‫إ‬ЙϚƛпƻӪՒ̌ȥͮʻ ѾȆǎ‫״‬ˠӗϤҮǰ̙ƛпșӪͤԠ‫ى‬ѐͽՊҠ̐ҭɵƛѧ͊ƙ͗ȥǯ̡‫ם‬Ҡƛ ƴƚͪ‫؟‬ǯ‫إ‬ κ֧тͽǰ̙‫״‬ˠӗϤҮ̡ͪ‫؟‬ǯ‫إ‬ǌƙSmadja’s Xtract [1] Нȯȡʕմǡ‫ن‬мմ̡ǰ ̙̈ͺǯ̡Տΰǌ‫״‬ˠ؇іҮƙˋȰ؇іҮǎҠ̐‫ײ‬ե‫״‬ˠ n іҮƙɡǰ̙ьՐͤ͗‫״‬ ˠӗϤҮ̡ӫ˙ǰ̙ƛBreidt [4] ЎͪǐԂϛΌǡ T ‫˯ז‬ҠɊˑȯ̈ծǯ̡Տΰǌ‫״‬ˠ ϼҮ-ɉҮ̡ӗϤҮƛɏǌǯ̡ӗϤҮͮʻǌƙLu [5] ҜƵ̡ CXtract ͮʻǌ‫ב‬ȯ Smadja’s Xtract ̡ǰ̙̈ǌǯՏΰǌƙɹ˘ͮʻԑҙ̀ѐӬ̡̰‫׶‬ΌӛЎưˍӜɟȐϖͤӗϤҮ ̡˧‫؜‬Үԑ‫׸‬ЛƛЎӗϤҮ‫ב‬ȯ̈˘ȝɬғՏˁъж̡ͪ‫؟‬՞ЄǎͮʻƙˇǰҦ [6] Ҝ ȯȡʕմƚ‫ن‬мմǡ T ‫̡˯ז‬ǰ̙Г˜ҮՊҮǎҼӗϤВ̸ͅմƙˋЎɡҠ̐‫ב‬ȯ̈ ӘύǪ‫ה‬ǌ‫כ‬ҍ‫ז‬ύȘǄǌ̡̔ӮͅƛȾ̋Ӊ [7] ҜʄȯӗϤҮ˙̡Փʆ‫ט‬ˠѧ‫׌‬Ի ǄƙɀȰѧ‫׌‬ԻǄɬϼ֢ш‫ײ‬ȅ͏ȵҮҿƙɟʆҮӮֽ‫ؙ‬ǎъжƛɟ‫؟‬ҮӮֽ‫؟ؙ̡ͪ‬ ͮʻǌƙ˘ǌȁՏΰͤІ‫̡׹‬ԸӨə֢ш̙͗ѧͤɚȉ̡ǰ̙ƙǾͼ͗ˏ֧ƽƼǯ̡σ կ˔Ͻʂ̔ӮҮƙɹɌƽƼǯȿɅȆз̡Үӏմҹǥɐƙ͹ȾѝϲͤϜօ̡շȥӛˑГ ؆ϛ͇ɐƙɏոȵ̔ӮҮγ̻Ο̊ոȵׂՑƛLi [8] ѾȆ‫כ‬ǅƽƼǯ̡ւѯƙˑȯӗϤ ҮɺͤσկƙˋǼЇӗϤҮ̡̔ӮҮҮӮЂư˾̡ӝ˼ƙɏոȵ̔ӮҮγƙӦƽƼǯ ‫״‬ˠ˜ӗϤҮγƙƽƼǯǌ˘ȝҮӏ̡խ‫˾ؼ‬ЎьҍǦƛЃǙϦԢ‫״‬ˠӗϤҮ̡ͪ‫ͮ؟‬ ʻƙǾͼˑȯ̡Ԃӟǜͤ˖ǂ‫آ‬ƙпưƙЎՄԆҫͤ˗ɟγҼ˾̡ՏΰΦԂӟƙChen [9] ҜƵʄȯՄԆ͠ҹͳׄ˥ Google ӘѴǪ‫ה‬ȁ‫״‬ˠӗϤҮƙTeng [10] ҜƵʄȯՄԆќӲ νِԡγҼ˾˥ӗϤҮǎҼ̡‫ע؟‬ƜпƴƙʄȯȡɯՏΰΦ [11, 12, 13]ƙκ֧Տˁ̡ σկ˥тͽǜ̡̓ǰ̙ƙˠГͺǯ̡ӗϤҮҠԭƙҸɩ‫״‬ˠ؇ՏӗϤҮƛ ɟʂ̈ԑȏ̡ͮʻӇϖ‫״‬ˠȆҮӏ̡ӗϤҮ˿͗˨˯շə̡ӗϤҮҠԭƙɓϼҮՊ 192  ɉҮƚʢΟҮՊɉҮҜƛȥͮʻѾȆɨҹҫқւѯǙ˧‫؜‬Үӏ˿˘Ү˾ǎхɊƙЇ Smadja’s Xtract ̡ԯԿ̙˥ͪǐԂϛΌƚT ‫˯ז‬Όǎтͽ‫̡ىז‬ǰ̙ƙȁǡǂюչǌǯ ҮӮոȵՏΰΦ SSTC [3] ̡Փʆƙȁ‫״‬ˠȆ؇іҮƚƻіҮƚșіҮǎӗϤҮƛ ƻƚɬϼ‫״‬ˠӗϤԂϛǰ̙ ȥͮʻ̀ѾȆǎɬϼ‫״‬ˠӗϤԂϛъжǰ̙ɓԜừȵƙΉȽНȯ Smadja’s Xtract ̡ԯ Կ̙ [1, 5]ƙ‫״‬ˠȆҮՏҼҼԓ˘ȝҮӏ̀ȿɅȆз̡Α‫־‬ӗϤҮƙКƼ˔НȯͪǐԂ ϛΌǡ T ‫˯ז‬Ό̡ǰə‫״̀ىז‬ˠȆ̡Α‫־‬ӗϤҮɏՏΰΦǌȿɅȆз̡‫و‬Ҩҙͅƙ ѧ͊ͤӗϤҮҠ̡̐ʃ˯ƙʦΏԢӣǌțͮʻϧ SSTC ҮӮոȵՏΰΦƙ͹ȲոҮҮӮ ˗ɟЂư˾̦ƙ̻Ր˯˘ͤӗϤҮƛ ǰâ Smadja’s Xtract ÐĤǹĒÖďöȒťƢȁǙóĚêìÖźĦƟƩ ŃǊĿÆƺŞň(ČƯňĦŢþůă) ŃǊ T Ǳĕň(ǱȔƫŠĦȓƝĘ) ćĕƫŠƟ(ČǦäǘƟÃǘåƟƴĦŧºĘ) Ԝưƚɬϼ‫״‬ˠӗϤԂϛъжǰ̙ǎ͠ҙԜ  Ơươ‫״‬ˠ˗‫ن‬ϼҶ؅չəǎȿзҮӏ  ΉȽѐ˯ȲոҮƙѐӬȁȘǄͤѮɲƙֆֻȲոҮǎ˧‫؜‬ҮӏԅҶͤ ƍ ̡ҫқǙ˧‫؜‬Ԃ ϛƛɏȲոҮ̡ų ±dų ԅҶǙ̡˧‫؜‬ҮԼɺ wi (1  i  n , n ͤ̀ɟ˧‫؜‬Ү̡ΐմŵƜѐ˯ wi  ɏп j ΐɲӬų ŴՊȲոҮ̡Ҷ؅ŵų Ȇз̡ɠմ˯Ӯͤ fi, j Ɯ˧‫؜‬Ү wi ɏȲոҮų Ɨƍų ԅҶ  " " d  d  Ǚ‫ם‬ȿȆз̡ɠմ˯Ӯͤų fi  fi, j Ɯ fi ̡ȡʕɠմͤ fi  fi, j / 2d ƜϦԢʵưΐȲ  d  d  " " ոҮƙȡʕɠմ  f    
1. Introduction My motivation to explore the nasal merger of Mandarin spoken in Taiwan originates from an incident in my life. My brother’s first baby was born on August 31st, 1999. He gave his son a name called Geng-ren /kəŋ.ʐən/ 1 ( 耕 仁 , meaning “to cultivate benevolence”). Interestingly, I found, as a native speaker of Mandarin in Taiwan, that I would easily mispronounce his name as Gen-ren [kən.ʐən] (跟人, meaning “to follow people”), rather than its standard pronunciation. Since the name was subject to mispronunciation and misunderstanding, I suggested to my brother that he should change the name; hence, he later selected another name Jia-he (家和, meaning “harmony in the family”). Because of this interesting incident, I came to realize that many native speakers of Mandarin in Taiwan seem to merge the syllable-final velar nasal /ŋ/ with the dental nasal /n/, hence neutralizing such minimal pairs as geng /kəŋ/ (耕, “to cultivate”) and gen /kən/ (跟, “to follow”). To explore this possible sound change, I later conducted a speech production experiment, which is discussed in the subsequent section. 2. Speech Production Experiment To investigate the possible nasal merger observed above, I addressed three research questions: 1) Is the syllable-final nasal modification a free variation or a conditioned alteration? 2) Does it occur in Mandarin spoken in Taiwan, China, or both? 3) Is it an ongoing or complete sound change? To address these questions, I invited 30 native Mandarin speakers to participate in the speech production experiment, including 11 males and 19 females, who were students of the University of Hawaii at Manoa. Fifteen of them were from Taiwan, and another fifteen were from China. They were all young adults with the average age of 27, the eldest subject being 36 years old and the youngest one being 21. For this experiment, I designed a questionnaire to understand the subjects’ basic sociolinguistic backgrounds. In addition, I also created sixty easy and interesting riddles to elicit spontaneous speech data. The answers to the riddles included the test words needed for this study--that is, words which end with three types of rhymes: –ing, -eng, and –ang. Three samples of the riddles are displayed below: 
4. Language Models for Special Retrieval Tasks (a) Cross-language IR (b) Distributed IR 3 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL pages 3–4, Rochester, April 2007. c 2007 Association for Computational Linguistics  (c) Structured document retrieval (d) Personalized/context-sensitive retrieval (e) Expert retrieval (f) Modeling redundancy (g) Predicting query difﬁculty (h) Subtopic retrieval 5. A General Framework for Applying SLMs to IR (a) Risk minimization framework (b) Special cases (c) Generative relevance hypothesis 6. Summary (a) SLMs vs. traditional methods: Pros & Cons (b) What we have achieved so far (c) Challenges and future directions ChengXiang Zhai is an Assistant Professor of Computer Science at the University of Illinois at UrbanaChampaign, where he also holds a joint appointment at the Institute for Genomic Biology and the Graduate School of Library and Information Science. He received a Ph.D. in Computer Science from Nanjing University in 1990, and a Ph.D. in Language and Information Technologies from Carnegie Mellon University in 2002. He worked at Clairvoyance Corp. as a Research Scientist and, later, a Senior Research Scientist from 1997 to 2000. His research interests include information retrieval, text mining, natural language processing, machine learning, and bioinformatics. He serves on the editorial board of ACM Transactions on Information Systems, and is the program co-chair of ACM CIKM 2004 and NAACL HLT 2007. He is an invited participant of the National Academy of Engineering’s 2006 US Frontiers of Engineering Symposium. He received an NSF CAREER Award in 2004, the ACM SIGIR 2004 Best Paper Award, and the 2004 Presidential Early Career Award for Scientists and Engineers (PECASE). 4 
5 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL pages 5–6, Rochester, April 2007. c 2007 Association for Computational Linguistics  Socially, it is common to distinguish three sub-dialects within each dialect region: city dwellers, peasants/farmers and Bedouins. The three degrees are often associated with a class hierarchy in which rich settled city dwellers are on top and Bedouins are on bottom. Different social associations exist as common in many other languages around the world. For example, the city dialect is considered less marked, better and smarter; whereas the Bedouin dialect is considered lower class, rough, yet pure to the origin of the language. The relationship between MSA and the dialect in a specific region is rather complex. Arabs do not think of these two as separate languages. This particular perception leads to a special kind of coexistence between two forms of language that serve different purposes. This kind of situation is what linguists term diglossia. Although the two variants have clear domains of prevalence: formal written (MSA) versus informal spoken (dialect), there is a large gray area in between and it is often filled with mixing of the two forms. For Natural Language Processing (NLP), the existence of dialects for any language constitutes a challenge in general since it adds another set of variation dimensions from a known standard. The problem is particularly interesting and challenging in Arabic and its different dialects, where the diversion from the standard could, in some linguistic theories, warrant a classification as a different language. This problem would not be as pronounced if standard Arabic were to be a living language, however it is not. Any realistic and practical approach to processing Arabic will have to account for dialectal usage since it is so pervasive. In this tutorial, we highlight different dialectal phenomena. We discuss how dialects migrate from the standard and why they pose challenges to NLP. Our tutorial will have four different parts: First, we describe a background layout of issues for standard Arabic NLP. Then we discuss a high level generic view of dialects and their different aspects that are of interest for the NLP community. We address both text and speech issues in addition to standardization issues. We focus in depth on two aspects of dialect processing in the third and fourth parts of the tutorial, namely, dialectal morphology and dialectal syntactic parsing. Throughout the presentation we will make references to the different resources available and draw contrastive links with standard Arabic and English. Moreover, we will discuss annotation standards as exemplified in the Linguistic Data Consortium Arabic Treebank. We will provide links to recent publications and available toolkits/resources for all four sections. This tutorial is designed for computer scientists and linguists alike. No knowledge of Arabic is required. However, we recommend taking a look at Nizar Habash’s Arabic NLP tutorial1 which will be reviewed in the first quarter of the tutorial. 
2. Speciﬁc techniques (a) Perceptrons (b) Naive Bayes (c) Logistic regression / maximum entropy (d) Support vector machines (e) Comparison and trade-oﬀs 3. Kernel methods (a) Why kernels (and why not)? (b) Kernelized linear classiﬁers (c) Kernelized perceptrons (d) Kernelizing SVMs and logistic regression (e) Advanced kernels and structure 7 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL page 7, Rochester, April 2007. c 2007 Association for Computational Linguistics   
 complex environments. See Jung et al (2006) and Chambers et al (2006) for more details on the PLOW system. The PLOW System PLOW learns tasks executable on the web involving actions such as navigation, information extraction and form ﬁlling, and can learn iterative steps that operate over lists of objects on pages. Figure 1 shows the system during learning a task to ﬁnd publications for a speciﬁed author. Upper left is the Mozilla browser, in which the user can demonstrate action and the system can execute actions in a mixed-initiative fashion. The user may speak or type to the system (SR output is lower right), and PLOW combines knowledge from the language and the demonstrated actions to produce a parameterized procedure (described in generated natural language in the upper right corner). Figure 2 shows a complete training dialogue in which PLOW learns how to ﬁnd article titles. To save space, simple acknowledgments by the system are not shown. Figure 1: PLOW learning a task  
NLP-based text adaptation capabilities in the tool  are described in this section (also see Figure 1.) These adaptation features were selected for implementation since they resemble teacherbased adaptation methods. 2.1 English and Spanish Marginal Notes Pedagogically, marginal notes are a kind of text summary. The Rhext automatic summarization tool (Marcu, 2000) is used to produce marginal notes in English. The amount of marginal notes generated can be increased or decreased based on students’ needs. Using Language Weaver’s1 English-to-Spanish machine translation system, English marginal notes can be translated into Spanish. 2.2 Vocabulary Support Synonyms for lower frequency (more difficult) words are output using a statistically-generated word similarity matrix (Lin, 1998). ATA v.1.0 generates antonyms for vocabulary in the text using WordNet®.2 Cognates are words which have the same spelling and meaning in two languages (e.g., animal in English and Spanish). The tool generates these using an ETS English/Spanish cognate lexicon. 2.3 English and Spanish Text-to-Speech The tool offers English and Spanish text-tospeech (TTS)3. English TTS may be useful for pronunciation support, while Spanish TTS provides access to the Spanish texts for Spanishspeaking ELLs who are not literate in Spanish. 
We present tutorial dialogue systems in two different domains that demonstrate the use of dialogue management and deep natural language processing techniques. Generation techniques are used to produce natural sounding feedback adapted to student performance and the dialogue history, and context is used to interpret tentative answers phrased as questions. 
The POSSLT 1 is a Korean to English spoken language translation (SLT) system. Like most other SLT systems, automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) are coupled in a cascading manner in our POSSLT. However, several novel techniques are applied to improve overall translation quality and speed. Models used in POSSLT are trained on a travel domain conversational corpus.  niques can be applied (Lee et. al., 2006-b). The POSSLT applies most of these techniques using a preprocessor. 2 System Description The POSSLT was developed by integrating ASR, SMT, and TTS. The system has a pipelined architecture as shown in Fig. 1. LM loader, preprocessor and re-ranking module are newly developed to improve the translation quality and speed for POSSLT.  
We present a demonstration of an annotation tool designed to annotate texts into a semantic network formalism called MultiNet. The tool is based on a Java Swing GUI and allows the annotators to edit nodes and relations in the network, as well as links between the nodes in the network and the nodes from the previous layer of annotation. The data processed by the tool in this presentation are from the English version of the Wall Street Journal. 
This demonstration will illustrate interactive computer games intended to help a native speaker of English learn Mandarin. These systems provide users with humanlike conversational exercises with contextualized help mechanisms. Two distinctly different activities, a translation game and a dialogue game are illustrated. The level of difﬁculty can be manipulated, and the sentence variations covered by the systems familiarize users with different expressions of the same meaning. The systems preserve the qualities of a typical computer system, being inﬁnitely patient and available any time of day. Students will be able to repeatedly practice conversation with no embarrassment. 
The CALO Meeting Assistant is an integrated, multimodal meeting assistant technology that captures speech, gestures, and multimodal data from multiparty interactions during meetings, and uses machine learning and robust discourse processing to provide a rich, browsable record of a meeting. 
We introduce a simple opinion mining system for analyzing Japanese Weblog reviews called OMS-J. OMS-J is designed to provide an intuitive visual GUI of opinion mining graphs for a comparison of different products of the same type to help a user make a quick purchase decision. We ﬁrst use an opinion mining method using a combination of supervised (a Naive Bayes Classiﬁer) and unsupervised (an improved SO-PMI: Semantic Orientation Using Pointwise Mutual Information) learning. 
Many texts containing proper names (e.g., “The cities of Mesopotamia prospered under Parthian and Sassanian rule.”) are submitted to machine translation services on the Web every day, and there are also service on the Web specifically target transliteration of proper names, including CHINET (Kwok et al. 2005) ad Livetrans (Lu, Chien, and Lee 2004). 
In this demonstration we present a conversational dialog system for automobile drivers. The system provides a voicebased interface to playing music, ﬁnding restaurants, and navigating while driving. The design of the system as well as the new technologies developed will be presented. Our evaluation showed that the system is promising, achieving high task completion rate and good user satisfation. 
Matthew Broadhead  Michael Cafarella  Oren Etzioni  Stephen Soderland  University of Washington  Computer Science and Engineering  Box 352350  Seattle, WA 98195-2350  {ayates,banko,hastur,mjc,etzioni,soderlan}@cs.washington.edu  
The Hidden Information State (HIS) Dialogue System is the ﬁrst trainable and scalable implementation of a spoken dialog system based on the PartiallyObservable Markov-Decision-Process (POMDP) model of dialogue. The system responds to n-best output from the speech recogniser, maintains multiple concurrent dialogue state hypotheses, and provides a visual display showing how competing hypotheses are ranked. The demo is a prototype application for the Tourist Information Domain and achieved a task completion rate of over 90% in a recent user study. 
This paper describes a novel text comparison environment that facilities text comparison administered through assessing and aggregating information nuggets automatically created and extracted from the texts in question. Our goal in designing such a tool is to enable and improve automatic nugget creation and present its application for evaluations of various natural language processing tasks. During our demonstration at HLT, new users will able to experience first hand text analysis can be fun, enjoyable, and interesting using system-created nuggets. 
Voice-Rate is an automated dialog system which provides access to over one million ratings of products and businesses. By calling a toll-free number, consumers can access ratings for products, national businesses such as airlines, and local businesses such as restaurants. Voice-Rate also has a facility for recording and analyzing ratings that are given over the phone. The service has been primed with ratings taken from a variety of web sources, and we are augmenting these with user ratings. Voice-Rate can be accessed by dialing 1-877-456-DATA. 
This paper describes a query expansion strategy for domain speciﬁc information retrieval. Components of compounds are used selectively. Only parts belonging to the same domain as the compound itself will be used in expanded queries. 
My research is focused on developing machine learning algorithms for inferring dependency parsers from language data. By investigating several approaches I have developed a unifying perspective that allows me to share advances between both probabilistic and non-probabilistic methods. First, I describe a generative technique that uses a strictly lexicalised parsing model, where all the parameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efﬁcient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 
We present our work on using Wikipedia as a knowledge source for Natural Language Processing. We ﬁrst describe our previous work on computing semantic relatedness from Wikipedia, and its application to a machine learning based coreference resolution system. Our results suggest that Wikipedia represents a semantic resource to be treasured for NLP applications, and accordingly present the work directions to be explored in the future. 
 An increasing number of NLP tasks require semantic labels to be assigned, not only to entities that appear in textual elements, but to the relationships between those entities. Interest is growing in shallow semantic role labeling as well as in deep semantic distance metrics grounded in ontologies, as each of these contributes to better understanding and organization of text. In this work I apply knowledgebased techniques to identify and explore deep semantic relationships in several styles of English text: nominal compounds, full sentences in the domain of knowledge acquisition, and phrase-level labels for images in a collection. I also present work on a graphical tool for exploring the relationship between domain text and deep domain knowledge. 
The goals of my dissertation are: 1) to propose a French terminology for the presentation of evaluation results of automatic summaries, 2) to identify and describe experimental variables in evaluations of automatic summaries, 3) to highlight the most common tendencies, inconsistencies and methodological problems in summarization evaluation experiments, and 4) to make recommendations for the presentation of evaluation results of automatic summaries. In this paper, I focus on the second objective, i.e. identifying and describing variables in summarization evaluation experiments. 
Previous work on discourse parsing has mostly relied on surface syntactic and lexical features; the use of semantics is limited to shallow semantics. The goal of this thesis is to exploit event semantics in order to build discourse parse trees (DPT) based on informational rhetorical relations. Our work employs an Inductive Logic Programming (ILP) based rhetorical relation classiﬁer, a Neural Network based discourse segmenter, a bottom-up sentence level discourse parser and a shift-reduce document level discourse parser. 
Most dialogue systems are built with a single task in mind. This makes the extension of an existing system one of the major problems in the ﬁeld as large parts of the system have to be modiﬁed. Some recent work has shown that ontologies have a role on the domain knowledge representation as the knowledge collected in an ontology can be used in all the modules. This work aims to follow the footsteps of the use of ontologies in dialogue systems and take it further as the current state of the art only uses taxonomical knowledge. 
Interests to realize semantic frames databases as a stable starting point in developing semantic knowledge based systems exists in countries such as Germany (the Salsa project), England (the PropBank project), United States (the FrameNet project), Spain, Japan, etc. I thus propose to create a semantic frame database for Romanian, similar to the FrameNet database. Since creating language resources demands many temporal, financial and human resources, a possible solution could be the import of standardized annotation of a resource developed for a specific language to other languages. This paper presents such a method for the importing of the FrameNet annotation from English to Romanian.  closely debated in the last years (Irimia, 1997; Dobrovie-Sorin, 1994; Monachesi, 1998; Barbu, 1999), creating a proper frame for the introduction of semantic roles. This paper presents the steps considered for the achievement of this project. Thus, Section 2 gives a very brief description of the frame semantics, and Section 3 presents the realization of a semantic structures database for the Romanian language, similar to those existing for English, German, or Spanish, containing detailed information about the relations between the semantic meaning and the syntax of the words. In the last section, some possible applications of the detection of semantic roles to written and spoken texts are mentioned (question answering systems, summarization systems, prosody prediction systems), before drawing some final conclusions. 2 Frame Semantics  
The goal of my dissertation research is to investigate the combination of new evidence sources for improving information retrieval on speech collections. The utility of these evidence sources is expected to vary depending on how well they are matched to a collection’s domain. I outline several new evidence sources for speech retrieval, situate them in the context of this domain dependency, and detail several methods for their combination with speech recognition output. Secondly, I highlight completed and proposed work for the production of this evidence. 
In the past, NLP has always been based on the explicit or implicit use of linguistic knowledge. In classical computer linguistic applications explicit rule based approaches prevail, while machine learning algorithms use implicit knowledge for generating linguistic knowledge. The question behind this work is: how far can we go in NLP without assuming explicit or implicit linguistic knowledge? How much efforts in annotation and resource building are needed for what level of sophistication in text processing? This work tries to answer the question by experimenting with algorithms that do not presume any linguistic knowledge in the system. The claim is that the knowledge needed can largely be acquired by knowledge-free and unsupervised methods. Here, graph models are employed for representing language data. A new graph clustering method finds related lexical units, which form word sets on various levels of homogeneity. This is exemplified and evaluated on language separation and unsupervised part-of-speech tagging, further applications are discussed. 
This paper explores what kind of user simulation model is suitable for developing a training corpus for using Markov Decision Processes (MDPs) to automatically learn dialog strategies. Our results suggest that with sparse training data, a model that aims to randomly explore more dialog state spaces with certain constraints actually performs at the same or better than a more complex model that simulates realistic user behaviors in a statistical way. 
The work1 we present here is concerned with the acquisition of deep grammatical information for nouns in Spanish. The aim is to build a learner that can handle noise, but, more interestingly, that is able to overcome the problem of sparse data, especially important in the case of nouns. We have based our work on two main points. Firstly, we have used distributional evidences as features. Secondly, we made the learner deal with all occurrences of a word as a single complex unit. The obtained results show that grammatical features of nouns is a level of generalization that can be successfully approached with a Decision Tree learner. 
We describe ConQuest, an open-source, reusable spoken dialog system that provides technical program information during conferences. The system uses a transparent, modular and open infrastructure, and aims to enable applied research in spoken language interfaces. The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base. In this paper, we describe the system’s functionality, overall architecture, and we discuss two initial deployments.  ployments and evaluations and therefore can provide a natural common evaluation task for the spoken language interfaces community. The ConQuest system is constructed on top of the open, transparent and modular Olympus dialog system framework (2007), and can be easily reused across different conferences. To date, the system has been deployed in two conferences: InterSpeech 2006 and IJCAI 2007. Together with corpora collected from these deployments, the system is freely available for download (Conquest, 2007). We begin by describing the ConQuest functionality in the next section. Then, in section 3 we provide an overview of the system architecture and discuss the development process. In section 4 we briefly discuss the two deployment efforts. Finally, in section 5 we discuss related work and draw a number of conclusions.  
We compare the effect of joint modeling of phonological features to independent feature detectors in a Conditional Random Fields framework. Joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes. We ﬁnd that joint modeling provides superior performance to the independent models on the TIMIT phone recognition task. We explore the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent. 
Suppose we have a large dictionary of strings. Each entry starts with a figure of merit (popularity). We wish to find the kbest matches for a substring, s, in a dictinoary, dict. That is, grep s dict | sort –n | head –k, but we would like to do this in sublinear time. Example applications: (1) web queries with popularities, (2) products with prices and (3) ads with click through rates. This paper proposes a novel index, k-best suffix arrays, based on ideas borrowed from suffix arrays and kdtrees. A standard suffix array sorts the suffixes by a single order (lexicographic) whereas k-best suffix arrays are sorted by two orders (lexicographic and popularity). Lookup time is between log N and sqrt N. 
We describe a new pruning approach to remove phrase pairs from translation models of statistical machine translation systems. The approach applies the original translation system to a large amount of text and calculates usage statistics for the phrase pairs. Using these statistics the relevance of each phrase pair can be estimated. The approach is tested against a strong baseline based on previous work and shows significant improvements. 
 2 Related Work  We present an approach to using multiple preprocessing schemes to improve statistical word alignments. We show a relative reduction of alignment error rate of about 38%. 
We present a fast method to identify homogeneous parallel documents. The method is based on collecting counts of identical low-frequency words between possibly parallel documents. The candidate with the most shared low-frequency words is selected as the parallel document. The method achieved 99.96% accuracy when tested on the EUROPARL corpus of parliamentary proceedings, failing only in anomalous cases of truncated or otherwise distorted documents. While other work has shown similar performance on this type of dataset, our approach presented here is faster and does not require training. Apart from proposing an efﬁcient method for parallel document identiﬁcation in a restricted domain, this paper furnishes evidence that parliamentary proceedings may be inappropriate for testing parallel document identiﬁcation systems in general. 
We introduce a novel framework for the expression, rapid-prototyping, and evaluation of statistical machine-translation (MT) systems using graphical models. The framework extends dynamic Bayesian networks with multiple connected different-length streams, switching variable existence and dependence mechanisms, and constraint factors. We have implemented a new general-purpose MT training/decoding system in this framework, and have tested this on a variety of existing MT models (including the 4 IBM models), and some novel ones as well, all using Europarl as a test corpus. We describe the semantics of our representation, and present preliminary evaluations, showing that it is possible to prototype novel MT ideas in a short amount of time. 
Situated models of meaning ground words in the non-linguistic context, or situation, to which they refer. Applying such models to sports video retrieval requires learning appropriate representations for complex events. We propose a method that uses data mining to discover temporal patterns in video, and pair these patterns with associated closed captioning text. This paired corpus is used to train a situated model of meaning that significantly improves video retrieval performance. 
 We  use  2 χ  to  investigate  the  context  de-  pendency of student affect in our com-  puter tutoring dialogues, targeting uncer-  tainty in student answers in 3 automati-  cally monitorable contexts. Our results  show signiﬁcant dependencies between  uncertain answers and speciﬁc contexts.  Identiﬁcation and analysis of these depen-  dencies is our ﬁrst step in developing an  adaptive version of our dialogue system.  
We propose an efﬁcient method to detect end-of-utterances from prosodic information in conversational speech. Our method is based on the application of a large set of binary and ramp ﬁlters to the energy and fundamental frequency signals obtained from the speech signal. These ﬁlter responses, which can be computed very efﬁciently, are used as input to a learning algorithm that generates the ﬁnal detector. Preliminary experiments using data obtained from conversations show that an accurate classiﬁer can be trained efﬁciently and that good results can be obtained without requiring a speech recognition system. 
 speakers that cannot be attributed to any demographic factor other than the language ability itself.  The ability to distinguish statistically different populations of speakers or writers can be an important asset in many NLP applications. In this paper, we describe a method of using document similarity measures to describe differences in behavior between native and non-native speakers of English in a writing task.1 
 We present a diacritization system for written Arabic which is based on a lexical resource. It combines a tagger and a lexeme language model. It improves on the best results reported in the literature.  
This paper describes an efﬁcient method to extract large n-best lists from a word graph produced by a statistical machine translation system. The extraction is based on the k shortest paths algorithm which is efﬁcient even for very large k. We show that, although we can generate large amounts of distinct translation hypotheses, these numerous candidates are not able to signiﬁcantly improve overall system performance. We conclude that large n-best lists would beneﬁt from better discriminating models. 
Dependency analysis of natural language gives rise to non-projective structures. The constraint of well-nestedness on dependency trees has been recently shown to give a good ﬁt with empirical linguistic data. We present a reformulation of this constraint using properties of nonprojective edges and show its formal relationship to level types of non-projective edges; we also derive a simple O(n2) algorithm for checking well-nestedness. 
We present an improved system combination technique, ı˙ROVER. Our approach obtains signiﬁcant improvements over ROVER, and is consistently better across varying numbers of component systems. A classiﬁer is trained on features from the system lattices, and selects the ﬁnal word hypothesis by learning cues to choose the system that is most likely to be correct at each word location. This approach achieves the best result published to date on the TC-STAR 2006 English speech recognition evaluation set. 
This paper presents an alternative algorithm based on the singular value decomposition (SVD) that creates vector representation for linguistic units with reduced dimensionality. The work was motivated by an application aimed to represent text segments for further processing in a multi-document summarization system. The algorithm tries to compensate for SVD’s bias towards dominant-topic documents. Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the baseline algorithms - the SVD and the vector space model. 
We describe the use of meeting metadata, acquired using a computerized meeting organization and note-taking system, to improve automatic transcription of meetings. By applying a two-step language model adaptation process based on notes and agenda items, we were able to reduce perplexity by 9% and word error rate by 4% relative on a set of ten meetings recorded in-house. This approach can be used to leverage other types of metadata. 
We present results from a new Interagency Language Roundtable (ILR) based comprehension test. This new test design presents questions at multiple ILR difficulty levels within each document. We incorporated Arabic machine translation (MT) output from three independent research sites, arbitrarily merging these materials into one MT condition. We contrast the MT condition, for both text and audio data types, with high quality human reference Gold Standard (GS) translations. Overall, subjects achieved 95% comprehension for GS and 74% for MT, across 4 genres and 3 difficulty levels. Surprisingly, comprehension rates do not correlate highly with translation error rates, suggesting that we are measuring an additional dimension of MT quality. We observed that it takes 15% more time overall to read MT than GS. 
We present a method for utilizing unannotated sentences to improve a semantic parser which maps natural language (NL) sentences into their formal meaning representations (MRs). Given NL sentences annotated with their MRs, the initial supervised semantic parser learns the mapping by training Support Vector Machine (SVM) classiﬁers for every production in the MR grammar. Our new method applies the learned semantic parser to the unannotated sentences and collects unlabeled examples which are then used to retrain the classiﬁers using a variant of transductive SVMs. Experimental results show the improvements obtained over the purely supervised parser, particularly when the annotated training set is small. 
In present Statistical Machine Translation (SMT) systems, alignment is trained in a previous stage as the translation model. Consequently, alignment model parameters are not tuned in function of the translation task, but only indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. In this approach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed. 
Vocal activity detection is an important technology for both automatic speech recognition and automatic speech understanding. In meetings, standard vocal activity detection algorithms have been shown to be ineﬀective, because participants typically vocalize for only a fraction of the recorded time and because, while they are not vocalizing, their channels are frequently dominated by crosstalk from other participants. In the present work, we review a particular type of normalization of maximum cross-channel correlation, a feature recently introduced to address the crosstalk problem. We derive a plausible geometric interpretation and show how the frame size aﬀects performance. 
Training statistical models to detect nonnative sentences requires a large corpus of non-native writing samples, which is often not readily available. This paper examines the extent to which machinetranslated (MT) sentences can substitute as training data. Two tasks are examined. For the native vs non-native classiﬁcation task, nonnative training data yields better performance; for the ranking task, however, models trained with a large, publicly available set of MT data perform as well as those trained with non-native data. 
 Gram+  Gram- periplasm  This paper proposes a ternary relation extraction method primarily based on rich syntactic information. We identify PROTEIN-ORGANISM-LOCATION relations in the text of biomedical articles. Different kernel functions are used with an SVM learner to integrate two sources of information from syntactic parse trees: (i) a large number of syntactic features that have been shown useful for Semantic Role Labeling (SRL) and applied here to the relation extraction task, and (ii) features from the entire parse tree using a tree kernel. Our experiments show that the use of rich syntactic features signiﬁcantly outperforms shallow word-based features. The best accuracy is obtained by combining SRL features with tree kernels. 
Speaker name recognition plays an important role in many spoken language applications, such as rich transcription, information extraction, question answering, and opinion mining. In this paper, we developed an SVM-based classification framework to determine the speaker names for those included speech segments in broadcast news speech, called soundbites. We evaluated a variety of features with different feature selection strategies. Experiments on Mandarin broadcast news speech show that using our proposed approach, the soundbite speaker name recognition (SSNR) accuracy is 68.9% on our blind test set, an absolute 10% improvement compared to a baseline system, which chooses the person name closest to the soundbite. 
We describe our linguistic rule-based tagger IceTagger, and compare its tagging accuracy to the TnT tagger, a state-of-theart statistical tagger, when tagging Icelandic, a morphologically complex language. Evaluation shows that the average tagging accuracy is 91.54% and 90.44%, obtained by IceTagger and TnT, respectively. When tag proﬁle gaps in the lexicon, used by the TnT tagger, are ﬁlled with tags produced by our morphological analyser IceMorphy, TnT’s tagging accuracy increases to 91.18%. 
Entropy regularization is a straightforward and successful method of semi-supervised learning that augments the traditional conditional likelihood objective function with an additional term that aims to minimize the predicted label entropy on unlabeled data. It has previously been demonstrated to provide positive results in linear-chain CRFs, but the published method for calculating the entropy gradient requires signiﬁcantly more computation than supervised CRF training. This paper presents a new derivation and dynamic program for calculating the entropy gradient that is signiﬁcantly more efﬁcient—having the same asymptotic time complexity as supervised CRF training. We also present efﬁcient generalizations of this method for calculating the label entropy of all sub-sequences, which is useful for active learning, among other applications. 
Document representation has a large impact on the performance of document retrieval and clustering algorithms. We propose a hybrid document indexing scheme that combines the traditional bagof-words representation with spectral embedding. This method accounts for the speciﬁcs of the document collection and also uses semantic similarity information based on a large scale statistical analysis. Clustering experiments showed improvements over the traditional tf-idf representation and over the spectral methods based solely on the document collection. 
This paper presents a way to perform speaker adaptation for automatic speech recognition using the stream weights in a multi-stream setup, which included acoustic models for “Articulatory Features” such as ROUNDED or VOICED. We present supervised speaker adaptation experiments on a spontaneous speech task and compare the above stream-based approach to conventional approaches, in which the models, and not stream combination weights, are being adapted. In the approach we present, stream weights model the importance of features such as VOICED for word discrimination, which offers a descriptive interpretation of the adaptation parameters. 
 Section 5 examines some closely-related work, and Section 6 discusses some implications.  Contemporary parser research is, to a large extent, focused on statistical parsers and deep-unification-based parsers. This paper describes an alternative, hybrid architecture in which an ATN-like parser, augmented by many preference tests, builds on the results of a fast chunker. The combination is as efficient as most stochastic parsers, and accuracy is close and continues to improve. These results raise questions about the practicality of deep unification for symbolic parsing. 
In this study, we address the problem of extracting relations between entities from Wikipedia’s English articles. Our proposed method ﬁrst anchors the appearance of entities in Wikipedia’s articles using neither Named Entity Recognizer (NER) nor coreference resolution tool. It then classiﬁes the relationships between entity pairs using SVM with features extracted from the web structure and subtrees mined from the syntactic structure of text. We evaluate our method on manually annotated data from actual Wikipedia articles. 
The aim of this work is to show the ability of ﬁnite-state transducers to simultaneously translate speech into multiple languages. Our proposal deals with an extension of stochastic ﬁnite-state transducers that can produce more than one output at the same time. These kind of devices offer great versatility for the integration with other ﬁnite-state devices such as acoustic models in order to produce a speech translation system. This proposal has been evaluated in a practical situation, and its results have been compared with those obtained using a standard monotarget speech transducer. 
In the framework of the Tc-Star project, we analyze and propose a combination of two Statistical Machine Translation systems: a phrase-based and an N -gram-based one. The exhaustive analysis includes a comparison of the translation models in terms of eﬃciency (number of translation units used in the search and computational time) and an examination of the errors in each system’s output. Additionally, we combine both systems, showing accuracy improvements. 
Texts exhibit subtle yet identifiable modality about writers’ estimation of how true each statement is (e.g., definitely true or somewhat true). This study is an analysis of such explicit certainty and doubt markers in epistemically modalized statements for a written news discourse. The study systematically accounts for five levels of writer’s certainty (ABSOLUTE, HIGH, MODERATE, LOW CERTAINTY and UNCERTAINTY) in three news pragmatic contexts: perspective, focus, and time. The study concludes that independent coders’ perceptions of the boundaries between shades of certainty in epistemically modalized statements are highly subjective and present difficulties for manual annotation and consequent automation for opinion extraction and sentiment analysis. While stricter annotation instructions and longer coder training can improve intercoder agreement results, it is not entirely clear that a five-level distinction of certainty is preferable to a simplistic distinction between statements with certainty and statements with doubt. 
We present a joint morphological-lexical language model (JMLLM) for use in statistical machine translation (SMT) of language pairs where one or both of the languages are morphologically rich. The proposed JMLLM takes advantage of the rich morphology to reduce the Out-Of-Vocabulary (OOV) rate, while keeping the predictive power of the whole words. It also allows incorporation of additional available semantic, syntactic and linguistic information about the morphemes and words into the language model. Preliminary experiments with an English to Dialectal-Arabic SMT system demonstrate improved translation performance over trigram based baseline language model.  
This paper investigates the problem of bootstrapping a statistical dialogue manager without access to training data and proposes a new probabilistic agenda-based method for simulating user behaviour. In experiments with a statistical POMDP dialogue system, the simulator was realistic enough to successfully test the prototype system and train a dialogue policy. An extensive study with human subjects showed that the learned policy was highly competitive, with task completion rates above 90%. 
This paper describes a new grapheme-tophoneme framework, based on a combination of formal linguistic and statistical methods. A context-free grammar is used to parse words into their underlying syllable structure, and a set of subword “spellneme” units encoding both phonemic and graphemic information can be automatically derived from the parsed words. A statistical -gram model can ¡ then be trained on a large lexicon of words represented in terms of these linguistically motivated subword units. The framework has potential applications in modeling unknown words and in linking spoken spellings with spoken pronunciations for fully automatic new-word acquisition via dialogue interaction. Results are reported on sound-to-letter experiments for the nouns in the Phonebook corpus. 
This study investigates whether some speech recognition (SR) errors are easier to detect and what patterns can be identiﬁed from those errors. Speciﬁcally, SR errors were examined from both nonlinguistic and linguistic perspectives. The analyses of non-linguistic properties revealed that high error ratios and consecutive errors lowered the ease of error detection. The analyses of linguistic properties showed that ease of error detection was associated with changing parts-of-speech of reference words in SR errors. Additionally, syntactic relations themselves and the change of syntactic relations had impact on the ease of error detection. 
In this paper we propose a statistical parsing technique that simultaneously identiﬁes biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles. We build a parser that derives both syntactic and domain-dependent semantic information and achieves an F-score of 48.4% for the relation extraction task. We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to 83.2%. Our key contributions are: learning from noisy data, and building an annotated corpus that can beneﬁt relation extraction research. 
Collecting supervised training data for automatic speech recognition (ASR) systems is both time consuming and expensive. In this paper we use the notion of virtual evidence in a graphical-model based system to reduce the amount of supervisory training data required for sequence learning tasks. We apply this approach to a TIMIT phone recognition system, and show that our VE-based training scheme can, relative to a baseline trained with the full segmentation, yield similar results with only 15.3% of the frames labeled (keeping the number of utterances ﬁxed). 
Customization to specific domains of discourse and/or user requirements is one of the greatest challenges for today’s Information Extraction (IE) systems. While demonstrably effective, both rule-based and supervised machine learning approaches to IE customization pose too high a burden on the user. Semisupervised learning approaches may in principle offer a more resource effective solution but are still insufficiently accurate to grant realistic application. We demonstrate that this limitation can be overcome by integrating fully-supervised learning techniques within a semisupervised IE approach, without increasing resource requirements. 
Individuals using an Augmentative and Alternative Communication (AAC) device communicate at less than 10% of the speed of “traditional” speech, creating a large communication gap. In this user study, we compare the communication rate of pseudo-impaired individuals using two different word prediction algorithms and a system without word prediction. Our results show that word prediction can increase AAC communication rate and that more accurate predictions signiﬁcantly improve communication rate. 
We present a method for automatic determiner selection, based on an existing language model. We train on the Penn Treebank and also use additional data from the North American News Text Corpus. Our results are a signiﬁcant improvement over previous best. 
This paper presents empirical results that contradict the prevailing opinion that entity extraction is a boring solved problem. In particular, we consider data sets that resemble familiar MUC/ACE data, and report surprisingly poor performance for both commercial and research systems. We then give an error analysis that suggests research challenges for entity extraction that are neither boring nor solved. 
We present a novel machine translation framework based on kernel regression techniques. In our model, the translation task is viewed as a string-to-string mapping, for which a regression type learning is employed with both the source and the target sentences embedded into their kernel induced feature spaces. We report the experiments on a French-English translation task showing encouraging results.  
We propose a variation of the SO-PMI algorithm for Japanese, for use in Weblog Opinion Mining. SO-PMI is an unsupervised approach proposed by Turney that has been shown to work well for English. We ﬁrst used the SO-PMI algorithm on Japanese in a way very similar to Turney’s original idea. The result of this trial leaned heavily toward positive opinions. We then expanded the reference words to be sets of words, tried to introduce a balancing factor and to detect neutral expressions. After these modiﬁcations, we achieved a wellbalanced result: both positive and negative accuracy exceeded 70%. This shows that our proposed approach not only adapted the SO-PMI for Japanese, but also modiﬁed it to analyze Japanese opinions more effectively. 
This paper investigates the combined use of pause duration and pitch reset for automatic story segmentation in Mandarin broadcast news. Analysis shows that story boundaries cannot be clearly discriminated from utterance boundaries by speaker-normalized pitch reset due to its large variations across different syllable tone pairs. Instead, speaker- and tonenormalized pitch reset can provide a clear separation between utterance and story boundaries. Experiments using decision trees for story boundary detection reinforce that raw and speaker-normalized pitch resets are not effective for Mandarin Chinese story segmentation. Speaker- and tone-normalized pitch reset is a good story boundary indicator. When it is combined with pause duration, a high F-measure of 86.7% is achieved. Analysis of the decision tree uncovered four major heuristics that show how speakers jointly utilize pause duration and pitch reset to separate speech into stories. 
We propose a high-performance cascaded hybrid model for Chinese NER. Firstly, we use Boosting, a standard and theoretically wellfounded machine learning method to combine a set of weak classiﬁers together into a base system. Secondly, we introduce various types of heuristic human knowledge into Markov Logic Networks (MLNs), an effective combination of ﬁrst-order logic and probabilistic graphical models to validate Boosting NER hypotheses. Experimental results show that the cascaded hybrid model signiﬁcantly outperforms the state-of-the-art Boosting model. 
This paper presents a three-step dependency parser to parse Chinese deterministically. By dividing a sentence into several parts and parsing them separately, it aims to reduce the error propagation coming from the greedy characteristic of deterministic parsing. Experimental results showed that compared with the deterministic parser which parsed a sentence in sequence, the proposed parser achieved extremely significant improvement on dependency accuracy.  great help for stopping the error propagation between different parsing stages.  
We evaluate semantic relatedness measures on different German datasets showing that their performance depends on: (i) the deﬁnition of relatedness that was underlying the construction of the evaluation dataset, and (ii) the knowledge source used for computing semantic relatedness. We analyze how the underlying knowledge source inﬂuences the performance of a measure. Finally, we investigate the combination of wordnets and Wikipedia to improve the performance of semantic relatedness measures. 
Phrase-based statistical machine translation systems depend heavily on the knowledge represented in their phrase translation tables. However, the phrase pairs included in these tables are typically selected using simple heuristics that potentially leave much room for improvement. In this paper, we present a technique for selecting the phrase pairs to include in phrase translation tables based on their estimated quality according to a translation model. This method not only reduces the size of the phrase translation table, but also improves translation quality as measured by the BLEU metric. 
We present the ﬁrst known empirical study on speech summarization without lexical features for Mandarin broadcast news. We evaluate acoustic, lexical and structural features as predictors of summary sentences. We ﬁnd that the summarizer yields good performance at the average Fmeasure of 0.5646 even by using the combination of acoustic and structural features alone, which are independent of lexical features. In addition, we show that structural features are superior to lexical features and our summarizer performs surprisingly well at the average F-measure of 0.3914 by using only acoustic features. These ﬁndings enable us to summarize speech without placing a stringent demand on speech recognition accuracy.  et al., 2005), which are dependent on lexical features. Considering the difﬁculty in obtaining high quality transcriptions, some researchers proposed speech summarization systems with non-lexical features (Inoue et al., 2004; Koumpis and Renals, 2005; Maskey and Hirschberg, 2003; Maskey and Hirschberg, 2006). However, there does not exist any empirical study on speech summarization without lexical features for Mandarin Chinese sources. In this paper, we construct our summarizer with acoustic and structural features, which are independent of lexical features, and compare acoustic and structural features against lexical features as predictors of summary sentences. In Section 2 we review previous work on broadcast news summarization. We describe the Mandarin broadcast news corpus on which our system operates in Section 3. In Section 4 we describe our summarizer and these features used in experiments. We set up our experiments and evaluate the results in Section 5, followed by our conclusion in Section 6.  
In this paper we describe automatic information nuggetization and its application to text comparison. More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material. A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement. 
In this paper we describe an automatic prosody labeling framework that exploits both language and speech information. We model the syntactic-prosodic information with a maximum entropy model that achieves an accuracy of 85.2% and 91.5% for pitch accent and boundary tone labeling on the Boston University Radio News corpus. We model the acousticprosodic stream with two diﬀerent models, one a maximum entropy model and the other a traditional HMM. We ﬁnally couple the syntactic-prosodic and acousticprosodic components to achieve signiﬁcantly improved pitch accent and boundary tone classiﬁcation accuracies of 86.0% and 93.1% respectively. Similar experimental results are also reported on Boston Directions corpus. 
The immense prosodic variation of natural conversational speech makes it challenging to predict which words are prosodically prominent in this genre. In this paper, we examine a new feature, accent ratio, which captures how likely it is that a word will be realized as prominent or not. We compare this feature with traditional accentprediction features (based on part of speech and N -grams) as well as with several linguistically motivated and manually labeled information structure features, such as whether a word is given, new, or contrastive. Our results show that the linguistic features do not lead to signiﬁcant improvements, while accent ratio alone can yield prediction performance almost as good as the combination of any other subset of features. Moreover, this feature is useful even across genres; an accent-ratio classiﬁer trained only on conversational speech predicts prominence with high accuracy in broadcast news. Our results suggest that carefully chosen lexicalized features can outperform less ﬁne-grained features. 
In this paper, we report on an empirical study on initiative conﬂicts in human-human conversation. We examined these conﬂicts in two corpora of task-oriented dialogues. The results show that conversants try to avoid initiative conﬂicts, but when these conﬂicts occur, they are efﬁciently resolved by linguistic devices, such as volume. 
This study addresses the problem of automatically detecting decisions in conversational speech. We formulate the problem as classifying decision-making units at two levels of granularity: dialogue acts and topic segments. We conduct an empirical analysis to determine the characteristic features of decision-making dialogue acts, and train MaxEnt models using these features for the classiﬁcation tasks. We ﬁnd that models that combine lexical, prosodic, contextual and topical features yield the best results on both tasks, achieving 72% and 86% precision, respectively. The study also provides a quantitative analysis of the relative importance of the feature types.  is difﬁcult to locate the decision points by the browsing and playback utilities alone. Banerjee and Rudnicky (2005) have shown that it is easier for users to retrieve the information they seek if the meeting record includes information about topic segmentation, speaker role, and meeting state (e.g., discussion, presentation, brieﬁng). To assist users in identifying or revisiting decisions in meeting archives, our goal is to automatically identify the dialogue acts and segments where decisions are made. Because reviewing decisions is indispensable in collaborative work, automatic decision detection is expected to lend support to computerassisted meeting tracking and understanding (e.g., assisting in the fulﬁlment of the decisions made in the meetings) and the development of group information management applications (e.g., constructing group memory). 2 Related Work  
The quality of a sentence translated by a machine translation (MT) system is difﬁcult to evaluate. We propose a method for automatically evaluating the quality of each translation. In general, when translating a given sentence, one or more conditions should be satisﬁed to maintain a high translation quality. In EnglishJapanese translation, for example, prepositions and inﬁnitives must be appropriately translated. We show several procedures that enable evaluating the quality of a translated sentence more appropriately than using conventional methods. The ﬁrst procedure is constructing a test set where the conditions are assigned to each test-set sentence in the form of yes/no questions. The second procedure is developing a system that determines an answer to each question. The third procedure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 
We propose three new features for MT evaluation: source-sentence constrained n-gram precision, source-sentence reordering metrics, and discriminative unigram precision, as well as a method of learning linear feature weights to directly maximize correlation with human judgments. By aligning both the hypothesis and the reference with the sourcelanguage sentence, we achieve better correlation with human judgments than previously proposed metrics. We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classiﬁcation-based framework. 
We study the use of rich syntax-based statistical models for generating grammatical case for the purpose of machine translation from a language which does not indicate case explicitly (English) to a language with a rich system of surface case markers (Japanese). We propose an extension of n-best re-ranking as a method of integrating such models into a statistical MT system and show that this method substantially outperforms standard n-best re-ranking. Our best performing model achieves a statistically significant improvement over the baseline MT system according to the BLEU metric. Human evaluation also confirms the results. 
This paper presents a maximum entropy machine translation system using a minimal set of translation blocks (phrase-pairs). While recent phrase-based statistical machine translation (SMT) systems achieve signiﬁcant improvement over the original source-channel statistical translation models, they 1) use a large inventory of blocks which have signiﬁcant overlap and 2) limit the use of training to just a few parameters (on the order of ten). In contrast, we show that our proposed minimalist system (DTM2) achieves equal or better performance by 1) recasting the translation problem in the traditional statistical modeling approach using blocks with no overlap and 2) relying on training most system parameters (on the order of millions or larger). The new model is a direct translation model (DTM) formulation which allows easy integration of additional/alternative views of both source and target sentences such as segmentation for a source language such as Arabic, part-of-speech of both source and target, etc. We show improvements over a state-of-the-art phrase-based decoder in Arabic-English translation. 
Conditional Random Fields (CRFs) have shown great success for problems involving structured output variables. However, for many real-world NLP applications, exact maximum-likelihood training is intractable because computing the global normalization factor even approximately can be extremely hard. In addition, optimizing likelihood often does not correlate with maximizing task-speciﬁc evaluation measures. In this paper, we present a novel training procedure, structured local training, that maximizes likelihood while exploiting the beneﬁts of global inference during training: hidden variables are used to capture interactions between local inference and global inference. Furthermore, we introduce biased potential functions that empirically drive CRFs towards performance improvements w.r.t. the preferred evaluation measure for the learning task. We report promising experimental results on two coreference data sets using two task-speciﬁc evaluation measures. 
A twin-model is proposed for coreference resolution: a link component, modeling the coreferential relationship between an anaphor and a candidate antecedent, and a creation component modeling the possibility that a phrase is not coreferential with any candidate antecedent. The creation model depends on all candidate antecedents and is often expensive to compute; Therefore constraints are imposed on feature forms so that features in the creation model can be efﬁciently computed from feature values in the link model. The proposed twin-model is tested on the data from the 2005 Automatic Content Extraction (ACE) task and the proposed model performs better than a thresholding baseline without tuning free parameter. 
Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a ﬁrst-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a ﬁrstorder logic representation can be incorporated into a probabilistic model and scaled efﬁciently. 
We explore the problem of retrieving semi-structured documents from a realworld collection using a structured query. We formally develop Structured Relevance Models (SRM), a retrieval model that is based on the idea that plausible values for a given ﬁeld could be inferred from the context provided by the other ﬁelds in the record. We then carry out a set of experiments using a snapshot of the National Science Digital Library (NSDL) repository, and queries that only mention ﬁelds missing from the test data. For such queries, typical ﬁeld matching would retrieve no documents at all. In contrast, the SRM approach achieves a mean average precision of over twenty percent. 
We introduce a novel ranking algorithm called GRASSHOPPER, which ranks items with an emphasis on diversity. That is, the top items should be different from each other in order to have a broad coverage of the whole item set. Many natural language processing tasks can beneﬁt from such diversity ranking. Our algorithm is based on random walks in an absorbing Markov chain. We turn ranked items into absorbing states, which effectively prevents redundant items from receiving a high rank. We demonstrate GRASSHOPPER’s effectiveness on extractive text summarization: our algorithm ranks between the 1st and 2nd systems on DUC 2004 Task 2; and on a social network analysis task that identiﬁes movie stars of the world. 
A novel random text generation model is introduced. Unlike in previous random text models, that mainly aim at producing a Zipfian distribution of word frequencies, our model also takes the properties of neighboring co-occurrence into account and introduces the notion of sentences in random text. After pointing out the deficiencies of related models, we provide a generation process that takes neither the Zipfian distribution on word frequencies nor the small-world structure of the neighboring co-occurrence graph as a constraint. Nevertheless, these distributions emerge in the process. The distributions obtained with the random generation model are compared to a sample of natural language data, showing high agreement also on word length and sentence length. This work proposes a plausible model for the emergence of large-scale characteristics of language without assuming a grammar or semantics. 
Relation extraction is the task of ﬁnding semantic relations between entities from text. The state-of-the-art methods for relation extraction are mostly based on statistical learning, and thus all have to deal with feature selection, which can signiﬁcantly affect the classiﬁcation performance. In this paper, we systematically explore a large space of features for relation extraction and evaluate the effectiveness of different feature subspaces. We present a general deﬁnition of feature spaces based on a graphic representation of relation instances, and explore three different representations of relation instances and features of different complexities within this framework. Our experiments show that using only basic unit features is generally sufﬁcient to achieve state-of-the-art performance, while overinclusion of complex features may hurt the performance. A combination of features of different levels of complexity and from different sentence representations, coupled with task-oriented feature pruning, gives the best performance. 
The task of identifying synonymous relations and objects, or Synonym Resolution (SR), is critical for high-quality information extraction. The bulk of previous SR work assumed strong domain knowledge or hand-tagged training examples. This paper investigates SR in the context of unsupervised information extraction, where neither is available. The paper presents a scalable, fully-implemented system for SR that runs in O(KN log N ) time in the number of extractions N and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. Given two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and an estimated 68% recall and resolves relations with 90% precision and 35% recall. 
In this paper, we empirically demonstrate what we call the domain restriction hypothesis, claiming that semantically related terms extracted from a corpus tend to be semantically coherent. We apply this hypothesis to deﬁne a post-processing module for the output of Espresso, a state of the art relation extraction system, showing that irrelevant and erroneous relations can be ﬁltered out by our module, increasing the precision of the ﬁnal output. Results are conﬁrmed by both quantitative and qualitative analyses, showing that very high precision can be reached. 
This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar. 
 We relate the problem of ﬁnding the best  application of a Synchronous Context-  Free Grammar (SCFG) rule during pars-  ing to a Markov Random Field. This  representation allows us to use the the-  ory of expander graphs to show that the  complexity of SCFG parsing of an input sentence of length N is Ω(N cn), for a  grammar with maximum rule length n and  some constant c. previous best result  Tohf iΩs (iNmpc√ronv)e.s  on  the  
This paper introduces an unsupervised morphological segmentation algorithm that shows robust performance for four languages with different levels of morphological complexity. In particular, our algorithm outperforms Goldsmith’s Linguistica and Creutz and Lagus’s Morphessor for English and Bengali, and achieves performance that is comparable to the best results for all three PASCAL evaluation datasets. Improvements arise from (1) the use of relative corpus frequency and suffix level similarity for detecting incorrect morpheme attachments and (2) the induction of orthographic rules and allomorphs for segmenting words where roots exhibit spelling changes during morpheme attachments. 
This paper reports experiments in which pCRU — a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space — is used to semi-automatically create several versions of a weather forecast text generator. The generators are evaluated in terms of output quality, development time and computational efﬁciency against (i) human forecasters, (ii) a traditional handcrafted pipelined NLG system, and (iii) a HALOGEN-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators receive higher scores from human judges than forecasts written by experts. 
This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation. We present results on using phrase-based SMT for learning to map meaning representations to natural language. Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations. Finally, we show that hybridizing these two approaches results in still more accurate generation systems. Automatic and human evaluation of generated sentences are presented across two domains and four languages. 
We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000). We deﬁne a headdriven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions. We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora. Finally, we evaluate different Markovized models, and ﬁnd that our selected best model is one that exploits head-modiﬁer bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work. 
This paper addresses the problem of classifying Chinese unknown words into fine-grained semantic categories defined in a Chinese thesaurus. We describe three novel knowledge-based models that capture the relationship between the semantic categories of an unknown word and those of its component characters in three different ways. We then combine two of the knowledge-based models with a corpus-based model which classifies unknown words using contextual information. Experiments show that the knowledge-based models outperform previous methods on the same task, but the use of contextual information does not further improve performance. 
This paper describes a method for generating sense-tagged data using Wikipedia as a source of sense annotations. Through word sense disambiguation experiments, we show that the Wikipedia-based sense annotations are reliable and can be used to construct accurate sense classiﬁers. 
Graph-based semi-supervised learning has recently emerged as a promising approach to data-sparse learning problems in natural language processing. All graph-based algorithms rely on a graph that jointly represents labeled and unlabeled data points. The problem of how to best construct this graph remains largely unsolved. In this paper we introduce a data-driven method that optimizes the representation of the initial feature space for graph construction by means of a supervised classiﬁer. We apply this technique in the framework of label propagation and evaluate it on two different classiﬁcation tasks, a multi-class lexicon acquisition task and a word sense disambiguation task. Signiﬁcant improvements are demonstrated over both label propagation using conventional graph construction and state-of-the-art supervised classiﬁers. 
This paper introduces a novel evaluation framework for question series and employs it to explore the effectiveness of QA and IR systems at addressing users’ information needs. The framework is based on the notion of recall curves, which characterize the amount of relevant information contained within a ﬁxed-length text segment. Although it is widely assumed that QA technology provides more efﬁcient access to information than IR systems, our experiments show that a simple IR baseline is quite competitive. These results help us better understand the role of NLP technology in QA systems and suggest directions for future research. 
Information retrieval systems are frequently required to handle long queries. Simply using all terms in the query or relying on the underlying retrieval model to appropriately weight terms often leads to ineffective retrieval. We show that rewriting the query to a version that comprises a small subset of appropriate terms from the original query greatly improves effectiveness. Targeting a demonstrated potential improvement of almost 50% on some difﬁcult TREC queries and their associated collections, we develop a suite of automatic techniques to re-write queries and study their characteristics. We show that the shortcomings of automatic methods can be ameliorated by some simple user interaction, and report results that are on average 25% better than the baseline. 
Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward ﬁnding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from -best lists, ¤ system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods. 
Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identiﬁcation as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the ﬁnal assignments. This joint ILP formulation provides f score improvements of 3.7-5.3% over a base coreference classiﬁer on the ACE datasets. 
We describe Castanet, an algorithm for automatically generating hierarchical faceted metadata from textual descriptions of items, to be incorporated into browsing and navigation interfaces for large information collections. From an existing lexical database (such as WordNet), Castanet carves out a structure that reﬂects the contents of the target information collection; moderate manual modiﬁcations improve the outcome. The algorithm is simple yet effective: a study conducted with 34 information architects ﬁnds that Castanet achieves higher quality results than other automated category creation algorithms, and 85% of the study participants said they would like to use the system for their work.  manually created metadata is considered of high quality, it is costly in terms of time and effort to produce, which makes it difﬁcult to scale and keep up with the vast amounts of new content being produced. In this paper, we describe Castanet, an algorithm that makes considerable progress in automating faceted metadata creation. Castanet creates domain-speciﬁc overlays on top of a large general-purpose lexical database, producing surprisingly good results in a matter of minutes for a wide range of subject matter. In the next section we elaborate on the notion of hierarchical faceted metadata and show how it can be used in interfaces for navigation of information collections. Section 3 describes other algorithms for inducing category structure from textual descriptions. Section 4 describes the Castanet algorithm, Section 5 describes the results of an evaluation with information architects, and Section 6 draws conclusions and discusses future work.  
We propose a new framework for supervised machine learning. Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with “rationales.” When annotating an example, the human teacher will also highlight evidence supporting this annotation—thereby teaching the machine learner why the example belongs to the category. We provide some rationale-annotated data and present a learning method that exploits the rationales during training to boost performance signiﬁcantly on a sample task, namely sentiment classiﬁcation of movie reviews. We hypothesize that in some situations, providing rationales is a more fruitful use of an annotator’s time than annotating more examples. 
Reinforcement learning gives a way to learn under what circumstances to perform which actions. However, this approach lacks a formal framework for specifying hand-crafted restrictions, for specifying the effects of the system actions, or for specifying the user simulation. The information state approach, in contrast, allows system and user behavior to be speciﬁed as update rules, with preconditions and effects. This approach can be used to specify complex dialogue behavior in a systematic way. We propose combining these two approaches, thus allowing a formal speciﬁcation of the dialogue behavior, and allowing hand-crafted preconditions, with remaining ones determined via reinforcement learning so as to minimize dialogue cost. 
Past approaches for using reinforcement learning to derive dialog control policies have assumed that there was enough collected data to derive a reliable policy. In this paper we present a methodology for numerically constructing conﬁdence intervals for the expected cumulative reward for a learned policy. These intervals are used to (1) better assess the reliability of the expected cumulative reward, and (2) perform a reﬁned comparison between policies derived from different Markov Decision Processes (MDP) models. We applied this methodology to a prior experiment where the goal was to select the best features to include in the MDP statespace. Our results show that while some of the policies developed in the prior work exhibited very large conﬁdence intervals, the policy developed from the best feature set had a much smaller conﬁdence interval and thus showed very high reliability. 
Motivated by psycholinguistic ﬁndings, we are currently investigating the role of eye gaze in spoken language understanding for multimodal conversational systems. Our assumption is that, during human machine conversation, a user’s eye gaze on the graphical display indicates salient entities on which the user’s attention is focused. The speciﬁc domain information about the salient entities is likely to be the content of communication and therefore can be used to constrain speech hypotheses and help language understanding. Based on this assumption, this paper describes an exploratory study that incorporates eye gaze in salience modeling for spoken language processing. Our empirical results show that eye gaze has a potential in improving automated language processing. Eye gaze is subconscious and involuntary during human machine conversation. Our work motivates more in-depth investigation on eye gaze in attention prediction and its implication in automated language processing. 
We propose a method for extracting semantic orientations of phrases (pairs of an adjective and a noun): positive, negative, or neutral. Given an adjective, the semantic orientation classiﬁcation of phrases can be reduced to the classiﬁcation of words. We construct a lexical network by connecting similar/related words. In the network, each node has one of the three orientation values and the neighboring nodes tend to have the same value. We adopt the Potts model for the probability model of the lexical network. For each adjective, we estimate the states of the nodes, which indicate the semantic orientations of the adjective-noun pairs. Unlike existing methods for phrase classiﬁcation, the proposed method can classify phrases consisting of unseen words. We also propose to use unlabeled data for a seed set of probability computation. Empirical evaluation shows the effectiveness of the proposed method. 
We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further conﬁrm the strength of the model: the algorithm provides signiﬁcant improvement over both individual rankers and a state-of-the-art joint ranking model. 
Sentiment analysis seeks to characterize opinionated or evaluative aspects of natural language text. We suggest here that appraisal expression extraction should be viewed as a fundamental task in sentiment analysis. An appraisal expression is a textual unit expressing an evaluative stance towards some target. The task is to ﬁnd and characterize the evaluative attributes of such elements. This paper describes a system for effectively extracting and disambiguating adjectival appraisal expressions in English outputting a generic representation in terms of their evaluative function in the text. Data mining on appraisal expressions gives meaningful and non-obvious insights. 
Scientiﬁc papers revolve around citations, and for many discourse level tasks one needs to know whose work is being talked about at any point in the discourse. In this paper, we introduce the scientiﬁc attribution task, which links diﬀerent linguistic expressions to citations. We discuss the suitability of diﬀerent evaluation metrics and evaluate our classiﬁcation approach to deciding attribution both intrinsically and in an extrinsic evaluation where information about scientiﬁc attribution is shown to improve performance on Argumentative Zoning, a rhetorical classiﬁcation task. 
This paper studies methods that automatically detect action-items in e-mail, an important category for assisting users in identifying new tasks, tracking ongoing ones, and searching for completed ones. Since action-items consist of a short span of text, classiﬁers that detect action-items can be built from a document-level or a sentence-level view. Rather than commit to either view, we adapt a contextsensitive metaclassiﬁcation framework to this problem to combine the rankings produced by different algorithms as well as different views. While this framework is known to work well for standard classiﬁcation, its suitability for fusing rankers has not been studied. In an empirical evaluation, the resulting approach yields improved rankings that are less sensitive to training set variation, and furthermore, the theoretically-motivated reliability indicators we introduce enable the metaclassiﬁer to now be applicable in any problem where the base classiﬁers are used. 
Previous multi-document relationship extraction and fusion research has focused on single relationships. Shifting the focus to multiple relationships allows for the use of mutual constraints to aid extraction. This paper presents a fusion method which uses a probabilistic database model to pick relationships which violate few constraints. This model allows improved performance on constructing corporate succession timelines from multiple documents with respect to a multi-document fusion baseline. 
Measuring semantic similarity between words is vital for various applications in natural language processing, such as language modeling, information retrieval, and document clustering. We propose a method that utilizes the information available on the Web to measure semantic similarity between a pair of words or entities. We integrate page counts for each word in the pair and lexico-syntactic patterns that occur among the top ranking snippets for the AND query using support vector machines. Experimental results on MillerCharles’ benchmark data set show that the proposed measure outperforms all the existing web based semantic similarity measures by a wide margin, achieving a correlation coefﬁcient of 0.834. Moreover, the proposed semantic similarity measure signiﬁcantly improves the accuracy (F measure of 0.78) in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content. 
In word sense disambiguation, choosing the most frequent sense for an ambiguous word is a powerful heuristic. However, its usefulness is restricted by the availability of sense-annotated data. In this paper, we propose an information retrieval-based method for sense ranking that does not require annotated data. The method queries an information retrieval engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers’ intuitions. 
An intelligent thesaurus assists a writer with alternative choices of words and orders them by their suitability in the writing context. In this paper we focus on methods for automatically choosing nearsynonyms by their semantic coherence with the context. Our statistical method uses the Web as a corpus to compute mutual information scores. Evaluation experiments show that this method performs better than a previous method on the same task. We also propose and evaluate two more methods, one that uses anticollocations, and one that uses supervised learning. To asses the difﬁculty of the task, we present results obtained by human judges. 
We propose a novel HMM-based framework to accurately transliterate unseen named entities. The framework leverages features in letteralignment and letter n-gram pairs learned from available bilingual dictionaries. Letter-classes, such as vowels/non-vowels, are integrated to further improve transliteration accuracy. The proposed transliteration system is applied to out-of-vocabulary named-entities in statistical machine translation (SMT), and a signiﬁcant improvement over traditional transliteration approach is obtained. Furthermore, by incorporating an automatic spell-checker based on statistics collected from web search engines, transliteration accuracy is further improved. The proposed system is implemented within our SMT system and applied to a real translation scenario from Arabic to English. 
Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes. Typically, the alignments are limited to one-to-one alignments. We present a novel technique of training with many-to-many alignments. A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with ﬁxed lists. We also apply an HMM method in conjunction with a local classiﬁcation model to predict a global phoneme sequence given a word. The many-to-many alignments result in signiﬁcant improvements over the traditional one-to-one approach. Our system achieves state-of-the-art performance on several languages and data sets. 
We analyze subword-based language models (LMs) in large-vocabulary continuous speech recognition across four “morphologically rich” languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. By estimating n-gram LMs over sequences of morphs instead of words, better vocabulary coverage and reduced data sparsity is obtained. Standard word LMs suffer from high out-of-vocabulary (OOV) rates, whereas the morph LMs can recognize previously unseen word forms by concatenating morphs. We show that the morph LMs generally outperform the word LMs and that they perform fairly well on OOVs without compromising the accuracy obtained for in-vocabulary words. 
We present a revision learning model for improving the accuracy of a dependency parser. The revision stage corrects the output of the base parser by means of revision rules learned from the mistakes of the base parser itself. Revision learning is performed with a discriminative classiﬁer. The revision stage has linear complexity and preserves the efﬁciency of the base parser. We present empirical evaluations on the treebanks of two languages, which show effectiveness in relative error reduction and state of the art accuracy. 
An open issue in data-driven dependency parsing is how to handle non-projective dependencies, which seem to be required by linguistically adequate representations, but which pose problems in parsing with respect to both accuracy and efﬁciency. Using data from ﬁve different languages, we evaluate an incremental deterministic parser that derives non-projective dependency structures in O(n2) time, supported by SVM classiﬁers for predicting the next parser action. The experiments show that unrestricted non-projective parsing gives a signiﬁcant improvement in accuracy, compared to a strictly projective baseline, with up to 35% error reduction, leading to state-of-the-art results for the given data sets. Moreover, by restricting the class of permissible structures to limited degrees of non-projectivity, the parsing time can be reduced by up to 50% without a signiﬁcant decrease in accuracy. 
We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-ﬁne method in which a grammar’s own hierarchical projections are used for incremental pruning, including a method for efﬁciently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-speciﬁc tuning. 
We present a novel method for creating A∗ estimates for structured search problems. In our approach, we project a complex model onto multiple simpler models for which exact inference is efﬁcient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and lexicalized parsing, demonstrating its effectiveness in these domains. 
A new architecture for identifying and interpreting temporal expressions is introduced, in which the large set of complex hand-crafted rules standard in systems for this task is replaced by a series of machine learned classiﬁers and a much smaller set of context-independent semantic composition rules. Experiments with the TERN 2004 data set demonstrate that overall system performance is comparable to the state-of-the-art, and that normalization performance is particularly good. 
We report results of experiments which build and reﬁne models of rhetoricalsemantic relations such as Cause and Contrast. We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by reﬁning the training and classiﬁcation process using parameter optimization, topic segmentation and syntactic parsing. Using human-annotated and automatically-extracted test sets, we ﬁnd that each of these techniques results in improved relation classiﬁcation accuracy. 
We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and signiﬁcantly better than either of the models it is based on. 
The task of selecting and ordering information appears in multiple contexts in text generation and summarization. For instance, methods for title generation construct a headline by selecting and ordering words from the input text. In this paper, we investigate decoding methods that simultaneously optimize selection and ordering preferences. We formalize decoding as a task of ﬁnding an acyclic path in a directed weighted graph. Since the problem is NP-hard, ﬁnding an exact solution is challenging. We describe a novel decoding method based on a randomized color-coding algorithm. We prove bounds on the number of color-coding iterations necessary to guarantee any desired likelihood of ﬁnding the correct solution. Our experiments show that the randomized decoder is an appealing alternative to a range of decoding algorithms for selection-andordering problems, including beam search and Integer Linear Programming. 
This paper explores the potential for annotating and enriching data for low-density languages via the alignment and projection of syntactic structure from parsed data for resource-rich languages such as English. We seek to develop enriched resources for a large number of the world’s languages, most of which have no signiﬁcant digital presence. We do this by tapping the body of Web-based linguistic data, most of which exists in small, analyzed chunks embedded in scholarly papers, journal articles, Web pages, and other online documents. By harvesting and enriching these data, we can provide the means for knowledge discovery across the resulting corpus that can lead to building computational resources such as grammars and transfer rules, which, in turn, can be used as bootstraps for building additional tools and resources for the languages represented.1 
This work evaluates a system that uses interpolated predictions of reading difficulty that are based on both vocabulary and grammatical features. The combined approach is compared to individual grammar- and language modeling-based approaches. While the vocabulary-based language modeling approach outperformed the grammar-based approach, grammar-based predictions can be combined using confidence scores with the vocabulary-based predictions to produce more accurate predictions of reading difficulty for both first and second language texts. The results also indicate that grammatical features may play a more important role in second language readability than in first language readability. 
This paper introduces the use of speech translation technology for a new type of voice-interactive Computer Aided Language Learning (CALL) application. We describe a computer game we have developed, in which the system presents sentences in a student’s native language to elicit spoken translations in the target new language. A critical technology is an algorithm to automatically verify the appropriateness of the student’s translation using linguistic analysis. Evaluation results are presented on the system’s ability to match human judgment of the correctness of a student’s translation, for a set of 1115 utterances collected from 9 learners of Mandarin Chinese translating ﬂight domain sentences. We also demonstrate the effective use of context information to improve both recognition performance on non-native speech as well as the system’s accuracy in judging the translation quality. 
 tice of language learning. For example, intelligent  Assessing learning progress is a critical step in language learning applications and experiments. In word learning, for example, one important type of assessment is a deﬁnition production test, in which subjects are asked to produce a short deﬁnition of the word being learned. In current practice, each free response is manually scored according to how well its meaning matches the target deﬁnition. Manual scoring is not only time-consuming, but also limited in its ﬂexibility and ability to detect partial learning effects. This study describes an effective automatic method for scoring free responses to deﬁnition production tests. The algorithm compares the text of the free response to the text of a reference deﬁnition using a statistical model of text semantic similarity that uses Markov chains on a graph of individual word relations. The model can take advantage of both corpusand knowledge-based resources. Evaluated on a new corpus of human-judged free responses, our method achieved signiﬁcant improvements over random and cosine baselines in both rank correlation and label error.  Computer Assisted Language Learning (CALL) systems are being developed that can automatically tailor lessons and questions to the needs of individual students (Heilman et al., 2006). One critical task that language tutors, word learning experiments, and related applications have in common is assessing the learning progress of the student or experiment subject during the course of the session. When the task is learning new vocabulary, a variety of tests have been developed to measure word learning progress. Some tests, such as multiplechoice selection of a correct synonym or cloze completion, are relatively passive. In production tests, on the other hand, students are asked to write or say a short phrase or sentence that uses the word being learned, called the target word, in a speciﬁed way. In one important type of production test, called a deﬁnition production test, the subject is asked to describe the meaning of the target word, as they understand it at that point in the session. The use of such tests has typically required a teacher or researcher to manually score each response by judging its similarity in meaning to the reference deﬁnition of the target word. The resulting scores can then be used to analyze how a person’s learning of the word responded to different stimuli, such as seeing the word used in context. A sample target word and its reference deﬁnition, along with examples of humanjudged responses, are given in Sections 3.3 and 4.1.  
We compare two pivot strategies for phrase-based statistical machine translation (SMT), namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a phrase translation table (phrase-table) of the source and target language pair from two phrase-tables; one constructed from the source language and English and one constructed from English and the target language. We then use that phrase-table in a phrase-based SMT system. The sentence translation strategy means that we ﬁrst translate a source language sentence into n English sentences and then translate these n sentences into target language sentences separately. Then, we select the highest scoring sentence from these target sentences. We conducted controlled experiments using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy signiﬁcantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems. 
In phrase-based statistical machine translation, the phrase-table requires a large amount of memory. We will present an efﬁcient representation with two key properties: on-demand loading and a preﬁx tree structure for the source phrases. We will show that this representation scales well to large data tasks and that we are able to store hundreds of millions of phrase pairs in the phrase-table. For the large Chinese– English NIST task, the memory requirements of the phrase-table are reduced to less than 20 MB using the new representation with no loss in translation quality and speed. Additionally, the new representation is not limited to a speciﬁc test set, which is important for online or real-time machine translation. One problem in speech translation is the matching of phrases in the input word graph and the phrase-table. We will describe a novel algorithm that effectively solves this combinatorial problem exploiting the preﬁx tree data structure of the phrase-table. This algorithm enables the use of signiﬁcantly larger input word graphs in a more efﬁcient way resulting in improved translation quality. 
We present an efﬁcient, novel two-pass approach to mitigate the computational impact resulting from online intersection of an n-gram language model (LM) and a probabilistic synchronous context-free grammar (PSCFG) for statistical machine translation. In ﬁrst pass CYK-style decoding, we consider ﬁrst-best chart item approximations, generating a hypergraph of sentence spanning target language derivations. In the second stage, we instantiate speciﬁc alternative derivations from this hypergraph, using the LM to drive this search process, recovering from search errors made in the ﬁrst pass. Model search errors in our approach are comparable to those made by the state-of-the-art “Cube Pruning” approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars. 
We propose to use a statistical phrasebased machine translation system in a post-editing task: the system takes as input raw machine translation output (from a commercial rule-based MT system), and produces post-edited target-language text. We report on experiments that were performed on data collected in precisely such a setting: pairs of raw MT output and their manually post-edited versions. In our evaluation, the output of our automatic post-editing (APE) system is not only better quality than the rule-based MT (both in terms of the BLEU and TER metrics), it is also better than the output of a stateof-the-art phrase-based MT system used in standalone translation mode. These results indicate that automatic post-editing constitutes a simple and efﬁcient way of combining rule-based and statistical MT technologies. 
We introduce an answer typing strategy speciﬁc to quantiﬁable how questions. Using the web as a data source, we automatically collect answer units appropriate to a given how-question type. Experimental results show answer typing with these units outperforms traditional ﬁxedcategory answer typing and other strategies based on the occurrences of numerical entities in text. 
This paper describes a probabilistic answer selection framework for question answering. In contrast with previous work using individual resources such as ontologies and the Web to validate answer candidates, our work focuses on developing a uniﬁed framework that not only uses multiple resources for validating answer candidates, but also considers evidence of similarity among answer candidates in order to boost the ranking of the correct answer. This framework has been used to select answers from candidates generated by four different answer extraction methods. An extensive set of empirical results based on TREC factoid questions demonstrates the effectiveness of the uniﬁed framework. 
This paper addresses the task of providing extended responses to questions regarding specialized topics. This task is an amalgam of information retrieval, topical summarization, and Information Extraction (IE). We present an approach which draws on methods from each of these areas, and compare the effectiveness of this approach with a query-focused summarization approach. The two systems are evaluated in the context of the prosecution queries like those in the DARPA GALE distillation evaluation. 
In this paper, we present a new string pattern matching-based passage ranking algorithm for extending traditional textbased QA toward videoQA. Users interact with our videoQA system through natural language questions, while our system returns passage fragments with corresponding video clips as answers. We collect 75.6 hours videos and 253 Chinese questions for evaluation. The experimental results showed that our method outperformed six top-performed ranking models. It is 10.16% better than the second best method (language model) in relatively MRR score and 6.12% in precision rate. Besides, we also show that the use of a trained Chinese word segmentation tool did decrease the overall videoQA performance where most ranking algorithms dropped at least 10% in relatively MRR, precision, and answer pattern recall rates. 
PropBank has been widely used as training data for Semantic Role Labeling. However, because this training data is taken from the WSJ, the resulting machine learning models tend to overﬁt on idiosyncrasies of that text’s style, and do not port well to other genres. In addition, since PropBank was designed on a verb-by-verb basis, the argument labels Arg2 - Arg5 get used for very diverse argument roles with inconsistent training instances. For example, the verb “make” uses Arg2 for the “Material” argument; but the verb “multiply” uses Arg2 for the “Extent” argument. As a result, it can be difﬁcult for automatic classiﬁers to learn to distinguish arguments Arg2-Arg5. We have created a mapping between PropBank and VerbNet that provides a VerbNet thematic role label for each verb-speciﬁc PropBank label. Since VerbNet uses argument labels that are more consistent across verbs, we are able to demonstrate that these new labels are easier to learn. 
Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. 
This paper explores the problem of computing text similarity between verb phrases describing skilled human behavior for the purpose of finding approximate matches. Four parsers are evaluated on a large corpus of skill statements extracted from an enterprise-wide expertise taxonomy. A similarity measure utilizing common semantic role features extracted from parse trees was found superior to an information-theoretic measure of similarity and comparable to the level of human agreement. 
© 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 4  (1) a. Bill has a cari.  Iti/The cari is black.  b. Bill doesn’t have a cari. *Iti/*The cari is black. 1  A year later in 1970, I gave my ﬁrst ACL presentation at the 8th annual meeting in Columbus, Ohio. The title of the invited talk was The Logic of English Predicate Complement Constructions. It started off with the following declaration (Karttunen 1971b):  It is evident that logical relations between main sentences and their complements are of great signiﬁcance in any system of automatic data processing that depends on natural language. For this reason a systematic study of such relations, of which this paper is an example, will certainly have a great practical value, in addition to what it may contribute to the theory of the semantics of natural languages.  The paper presented a classiﬁcation of verbs and constructions that take sentential complements, that-clauses and inﬁnitival complements, based on whether the sentence commits the author to the truth or falsity of the complement clause. For example, all the sentences in (2) imply, for different reasons, that the complement is true while all the sentences in (3) imply that the complement is false.2  (2) a. John forgot that Mary was sick. b. Bill managed to solve the problem. c. Harry forced Ed to leave.  Mary was sick. Bill solved the problem. Ed left.  (3) a. John pretended that Mary was sick. b. Bill failed to solve the problem. c. Harry prevented Ed from leaving.  Mary was not sick. Bill did not solve the problem. Ed did not leave.  Neither one of these two papers would have been accepted at this 2007 ACL conference. There was no implementation, no evaluation, and very little discussion of related work. In the happy childhood of computational linguistics even the most junior person in the ﬁeld, like myself, was allowed—even invited—to give a talk at the main COLING/ACL session about uncharted linguistic phenomena. It was a small ﬁeld then. A future historian of the ﬁeld might be puzzled by the 1969 and 1970 papers. They were written by a postdoctoral research associate at the University of Texas at Austin, who had arrived from Finland in 1964 by way of the University of Indiana at Bloomington where he had just received a Ph.D. in Linguistics. Where did the young man acquire, and why was he spouting, that kind of computational rhetoric, when the record shows that for the next ten years he never laid his hands on a computer? The fact is that I did have a brush with computational linguistics before settling down to do pure semantics in the 1970s. I wanted to do linguistics because of Syntactic Structures (Chomsky 1957) and when the Uralic and Altaic Studies Department in Bloomington offered me a job in 1964 as a “native informant” in Finnish I accepted and managed to get into the Linguistics department as a graduate student. My job title turned out not to be accurate. During my ﬁrst two years in Bloomington I was teaching Finnish on my own for nine hours per week. Luckily, I signed up for a course on computational linguistics taught by an excellent teacher and mentor, Robert E. Wall. Bob Wall had participated in an early MT project at Harvard and in a project on automatic  
∗ Institute for Natural Language Processing, University of Stuttgart, Azenbergstr. 12, 70174 Stuttgart, Germany. E-mail: atterer@ims.uni-stuttgart.de. ∗∗ Institute for Natural Language Processing, University of Stuttgart, Azenbergstr. 12, 70174 Stuttgart, Germany. E-mail: hinrich@hotmail.com. 
∗∗ Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912, USA. E-mail: Mark Johnson@brown.edu. Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted for publication: 30 March 2007. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 4  treebank, maximum likelihood estimation can be applied to learn the probability values in the model. More recently, new machine learning methods have been developed or extended to handle models of grammatical structure. Notably, conditional estimation (Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised contrastive estimation (Smith and Eisner 2005) have been applied to structured models. Weighted grammars learned in this way differ in two important ways from traditional, generative models. First, the weights can be any positive value; they need not sum to one. Second, features can “overlap,” and it can be difﬁcult to design a generative model that uses such features. The beneﬁts of new features and discriminative training methods are widely documented and recognized. This article focuses speciﬁcally on the ﬁrst of these differences. It compares the expressive power of weighted context-free grammars (WCFGs), where each rule is associated with a positive weight, to that of the corresponding PCFGs, that is, with the same rules but where the weights of the rules expanding a nonterminal must sum to one. One might expect that because normalization removes one or more degrees of freedom, unnormalized models should be more expressive than normalized, probabilistic models. Perhaps counterintuitively, previous work has shown that the classes of probability distributions deﬁned by WCFGs and PCFGs are the same (Abney, McAllester, and Pereira 1999; Chi 1999). However, this result does not completely settle the question about the expressive power of WCFGs and PCFGs. As we show herein, a WCFG can deﬁne a conditional distribution from strings to trees even if it does not deﬁne a probability distribution over trees. Because these conditional distributions are what are used in classiﬁcation tasks and related tasks such as parsing, we need to know the relationship between the classes of conditional distributions deﬁned by WCFGs and PCFGs. In fact we extend the results of Chi and of Abney et al., and show that WCFGs and PCFGs both deﬁne the same class of conditional distribution. Moreover, we present an algorithm for converting an arbitrary WCFG that deﬁnes a conditional distribution over trees given strings but possibly without a ﬁnite partition function into a PCFG with the same rules as the WCFG and that deﬁnes the same conditional distribution over trees given strings. This means that maximum conditional likelihood WCFGs are non-identiﬁable, because there are an inﬁnite number of rule weights all of which maximize the conditional likelihood.  2. Weighted CFGs A CFG G is a tuple N, S, Σ, R where N is a ﬁnite set of nonterminal symbols, S ∈ N is the start symbol, Σ is a ﬁnite set of terminal symbols (disjoint from N), and R is a set of production rules of the form X → α where X ∈ N and α ∈ (N ∪ Σ) . A WCFG associates a positive number called the weight with each rule in R.1 We denote by θX→α the weight attached to the rule X → α, and the vector of rule weights by Θ = {θA→α : A → α ∈ R}. A weighted grammar is the pair GΘ = G, Θ . 
Submission received: 27 April 2006; revised submission received: 30 November 2006; accepted for publication: 16 March 2007. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 4  1. Introduction Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999). Log-linear models are also referred to as maximum entropy models and random ﬁelds in the NLP literature. They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks. Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated. For manually constructed grammars, this assumption is usually sufﬁcient for efﬁcient estimation and decoding. However, for wide-coverage grammars extracted from a treebank, enumerating all parses is infeasible. In this article we apply the dynamic programming method of Miyao and Tsujii (2002) to a packed chart; however, because the grammar is automatically extracted, the packed charts require a considerable amount of memory: up to 25 GB. We solve this massive estimation problem by developing a parallelized version of the estimation algorithm which runs on a Beowulf cluster. The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000). A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b). In this article we extend existing parsing techniques by developing log-linear models for CCG, as well as a new model and efﬁcient parsing algorithm which exploits all CCG’s derivations, including the nonstandard ones. Estimating a log-linear model involves computing expectations of feature values. For the conditional log-linear models used in this article, computing expectations requires a sum over all derivations for each sentence in the training data. Because there can be a massive number of derivations for some sentences, enumerating all derivations is infeasible. To solve this problem, we have adapted the dynamic programming method of Miyao and Tsujii (2002) to packed CCG charts. A packed chart efﬁciently represents all derivations for a sentence. The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside–outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990). Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar ﬁnding for globally optimized log-linear models for sequences. As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997). Despite the use of a packed representation, the complete set of derivations for the sentences in the training data requires up to 25 GB of RAM for some of the models in this article. There are a number of ways to solve this problem. Possibilities include using a subset of the training data; repeatedly parsing the training data for each iteration of the estimation algorithm; or reading the packed charts from disk for each iteration. 494  Clark and Curran  Wide-Coverage Efﬁcient Statistical Parsing  These methods are either too slow or sacriﬁce parsing performance, and so we use a parallelized version of BFGS running on an 18-node Beowulf cluster to perform the estimation. Even given the large number of derivations and the large feature sets in our models, the estimation time for the best-performing model is less than three hours. This gives us a practical framework for developing a statistical parser. A corollary of CCG’s base-generative treatment of long-range dependencies in relative clauses and coordinate constructions is that the standard predicate–argument relations can be derived via nonstandard surface derivations. The addition of “spurious” derivations in CCG complicates the modeling and parsing problems. In this article we consider two solutions. The ﬁrst, following Hockenmaier (2003a), is to deﬁne a model in terms of normal-form derivations (Eisner 1996). In this approach we recover only one derivation leading to a given set of predicate–argument dependencies and ignore the rest. The second approach is to deﬁne a model over the predicate–argument dependencies themselves, by summing the probabilities of all derivations leading to a given set of dependencies. We also deﬁne a new efﬁcient parsing algorithm for such a model, based on Goodman (1996), which maximizes the expected recall of dependencies. The development of this model allows us to test, for the purpose of selecting the correct predicate–argument dependencies, whether there is useful information in the additional derivations. We also compare the performance of our best log-linear model against existing CCG parsers, obtaining the highest results to date for the recovery of predicate– argument dependencies from CCGbank. A key component of the parsing system is a Maximum Entropy CCG supertagger (Ratnaparkhi 1996; Curran and Clark 2003) which assigns lexical categories to words in a sentence. The role of the supertagger is twofold. First, it makes discriminative estimation feasible by limiting the number of incorrect derivations for each training sentence; the supertagger can be thought of as supplying a number of incorrect but plausible lexical categories for each word in the sentence. Second, it greatly increases the efﬁciency of the parser, which was the original motivation for supertagging (Bangalore and Joshi 1999). One possible criticism of CCG has been that highly efﬁcient parsing is not possible because of the additional “spurious” derivations. In fact, we show that a novel method which tightly integrates the supertagger and parser leads to parse times signiﬁcantly faster than those reported for comparable parsers in the literature. The parser is evaluated on CCGbank (available through the Linguistic Data Consortium). In order to facilitate comparisons with parsers using different formalisms, we also evaluate on the publicly available DepBank (King et al. 2003), using the Briscoe and Carroll annotation consistent with the RASP parser (Briscoe, Carroll, and Watson 2006). The dependency annotation is designed to be as theory-neutral as possible to allow easy comparison. However, there are still considerable difﬁculties associated with a cross-formalism comparison, which we describe. Even though the CCG dependencies are being mapped into another representation, the accuracy of the CCG parser is over 81% F-score on labeled dependencies, against an upper bound of 84.8%. The CCG parser also outperforms RASP overall and on the majority of dependency types. The contributions of this article are as follows. First, we explain how to estimate a full log-linear parsing model for an automatically extracted grammar, on a scale as large as that reported anywhere in the NLP literature. Second, the article provides a comprehensive blueprint for building a wide-coverage CCG parser, including theoretical and practical aspects of the grammar, the estimation process, and decoding. Third, we investigate the difﬁculties associated with cross-formalism parser comparison, evaluating the parser on DepBank. And ﬁnally, we develop new models and decoding algorithms for 495  Computational Linguistics  Volume 33, Number 4  CCG, and give a convincing demonstration that, through use of a supertagger, highly efﬁcient parsing is possible with CCG.  2. Related Work  The ﬁrst application of log-linear models to parsing is the work of Ratnaparkhi and colleagues (Ratnaparkhi, Roukos, and Ward 1994; Ratnaparkhi 1996, 1999). Similar to Della Pietra, Della Pietra, and Lafferty (1997), Ratnaparkhi motivates log-linear models from the perspective of maximizing entropy, subject to certain constraints. Ratnaparkhi models the various decisions made by a shift-reduce parser, using log-linear distributions deﬁned over features of the local context in which a decision is made. The probabilities of each decision are multiplied together to give a score for the complete sequence of decisions, and beam search is used to ﬁnd the most probable sequence, which corresponds to the most probable derivation. A different approach is proposed by Abney (1997), who develops log-linear models for attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG). Rather than deﬁne a model in terms of parser moves, Abney deﬁnes a model directly over the syntactic structures licensed by the grammar. Another difference is that Abney uses a global model, in which a single log-linear model is deﬁned over the complete space of attribute–value structures. Abney’s motivation for using log-linear models is to overcome various problems in applying models based on PCFGs directly to attributevalue grammars. A further motivation for using global models is that these do not suffer from the label bias problem (Lafferty, McCallum, and Pereira 2001), which is a potential problem for Ratnaparkhi’s approach. Abney deﬁnes the following model for a syntactic analysis ω:  P(ω) =  i βifi(ω) Z  (1)  where fi(ω) is a feature, or feature function, and βi is its corresponding weight; Z is a normalizing constant, also known as the partition function. In much work using loglinear models in NLP, including Ratnaparkhi’s, the features of a model are indicator functions which take the value 0 or 1. However, in Abney’s models, and in the models used in this article, the feature functions are integer valued and count the number of times some feature appears in a syntactic analysis.1 Abney calls the feature functions frequency functions and, like Abney, we will not always distinguish between a feature and its corresponding frequency function. There are practical difﬁculties with Abney’s proposal, in that ﬁnding the maximumlikelihood solution during estimation involves calculating expectations of feature values, which are sums over the complete space of possible analyses. Abney suggests a Metropolis-Hastings sampling procedure for calculating the expectations, but does not experiment with an implementation. Johnson et al. (1999) propose an alternative solution, which is to maximize the conditional likelihood function. In this case the likelihood function is the product of the conditional probabilities of the syntactic analyses in the data, each probability conditioned on the respective sentence. The advantage of this method is that calculating the conditional feature expectations only requires a sum over the syntactic analyses for the  
∗ Department of Informatics, Brighton BN1 9QH, UK. E-mail: {dianam,robk,juliewe,johnca}@sussex.ac.uk. Submission received: 16 November 2005; revised submission received: 12 July 2006; accepted for publication 16 February 2007. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 4  1. Introduction In word sense disambiguation, the “ﬁrst sense” heuristic (choosing the ﬁrst, or predominant sense of a word) is used by most state-of-the-art systems as a back-off method when information from the context is not sufﬁcient to make a more informed choice. In this article, we present an in-depth study of a method for automatically acquiring predominant senses for words from raw text (McCarthy et al. 2004a). The method uses distributionally similar words listed as “nearest neighbors” in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the observation that the more prevalent a sense of a word, the more neighbors will relate to that sense, and the higher their distributional similarity scores will be. The senses of a word are deﬁned in a sense inventory. We use WordNet (Fellbaum 1998) because this is widely used, is publicly available, and has plenty of gold-standard evaluation data available (Miller et al. 1993; Cotton et al. 2001; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004). The distributional strength of the neighbors is associated with the senses of a word using a measure of semantic similarity which relies on the relationships between word senses, such as hyponyms (available in an inventory such as WordNet) or overlap in the deﬁnitions of word senses (available in most dictionaries), or both. In this article we provide a detailed discussion and quantitative analysis of the motivation behind the ﬁrst sense heuristic, and a full description of our method. We extend previously reported work in a number of different directions: r We evaluate the method on all parts of speech (PoS) on SemCor (Miller et al. 1993). Previous experiments (McCarthy et al. 2004c) evaluated only nouns on SemCor, or all PoS but only on the Senseval-2 (Cotton et al. 2001) and Senseval-3 (Mihalcea and Edmonds 2004) data. The evaluation on all PoS is much more extensive because the SemCor corpus is composed of 220,000 words in contrast to the 6 documents in the Senseval-2 and -3 English all words data (10,000 words). r We compare two WordNet similarity measures in our evaluation on nouns, and also contrast performance using two publicly available thesauruses, both produced from the same NEWSWIRE corpus, but one derived using a proximity-based approach and the other using dependency relations from a parser. It turns out that the results from the proximity-based thesaurus are comparable to those from the dependencybased thesaurus; this is encouraging for applying the method to languages without sophisticated analysis tools. r We manually analyze a sample of errors from the SemCor evaluation. A small number of errors can be traced back to inherent shortcomings of our method, but the main source of error is due to noise from related senses. This is a common problem for all WSD systems (Ide and Wilks 2006) but one which is only recently starting to be addressed by the WSD community (Navigli, Litkowski, and Hargraves 2007). r One motivation for an automatic method for acquiring predominant senses is that there will always be words for which there are insufﬁcient data available in manually sense-tagged resources. We compare the performance of our automatic method with the ﬁrst sense heuristic derived from SemCor on nouns in the Senseval-2 data. We ﬁnd that the 554  McCarthy, Koeling, Weeds, and Carroll  Acquisition of Predominant Word Senses  automatic method outperforms the one obtained from manual annotations in SemCor for nouns with fewer than ﬁve occurrences in SemCor. r Aside from the lack of coverage of manually annotated data, there is a need for ﬁrst sense heuristics to be speciﬁc to domain. We explore the potential for applying the method with domain-speciﬁc text for all PoS in an experiment using a gold-standard domain-speciﬁc resource (Magnini and Cavaglia` 2000) which we have used previously only with nouns. We show that although there is a little mileage to be had from domain-speciﬁc ﬁrst sense heuristics for verbs, nouns beneﬁt greatly from domain-speciﬁc training. r In previous work (Koeling, McCarthy, and Carroll 2005) we produced manually sense-annotated domain-speciﬁc test corpora for a lexical sample, and demonstrated that predominant senses acquired (from hand-classiﬁed corpora) in the same domain as the test data outperformed the SemCor ﬁrst sense. We further this exploration by contrasting with results from training on automatically categorized text from the English Gigaword Corpus and show that the results are comparable to those using hand-classiﬁed domain data. The article is organized as follows. In the next section we motivate the use of predominant sense information in WSD systems and the need for acquiring this information automatically. In Section 3 we give an overview of related work in WSD, focusing on the acquisition of prior sense distributions and domain-speciﬁc sense information. Section 4 describes our acquisition method. Section 5 describes the experimental setup for the work reported in this article. Section 6 describes four experiments. The ﬁrst evaluates the ﬁrst sense heuristic using predominant sense information acquired for all PoS on SemCor; for nouns we compare two semantic similarity methods and three different types of distributional thesaurus. We also report an error analysis for all PoS of our method. The second experiment compares the performance of the automatic method to the manually produced data in SemCor, on nouns in the Senseval-2 data, looking particularly at nouns which have a low frequency in SemCor. The third uses corpora in restricted domains and the subject ﬁeld code gold standard of Magnini and Cavaglia` (2000) to investigate the potential for domain-speciﬁc rankings for different PoS. The fourth compares results when we train and test on domain-speciﬁc corpora, where the training data is (1) manually categorized for domain and from the same corpus as the test data, and (2) where the training data is harvested automatically from another corpus which is categorized automatically. Finally, we conclude (Section 7) and discuss directions for future work (Section 8). 2. Motivation The problem of disambiguating the meanings of words in text has received much attention recently, particularly since the inception of the Senseval evaluation exercises (Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004). One of the standard Senseval tasks (the “all words” task) is to tag each open class word with one of its senses, as listed in a dictionary or thesaurus such as WordNet (Fellbaum 1998). The most accurate word sense disambiguation (WSD) systems use supervised machine learning approaches (Stevenson and Wilks 2001), trained on text which has been sense tagged by hand. However, the performance of these systems is strongly 555  Computational Linguistics  Volume 33, Number 4  dependent on the quantity of training data available (Yarowsky and Florian 2002), and manually sense-annotated text is extremely costly to produce (Kilgarriff 1998). The largest all words sense tagged corpus is SemCor, which is 220,000 words taken from 103 passages, each of about 2,000 words, from the Brown corpus (Francis and Kucˇera 1979) and the complete text of a 19th-century American novel, The Red Badge of Courage, which totals 45,600 words (Landes, Leacock, and Tengi 1998). Approximately half of the words in this corpus are open-class words (nouns, verbs, adjectives, and adverbs) and these have been linked to WordNet senses by human taggers using a software interface. The shortage of training data due to the high costs of tagging texts has motivated research into unsupervised methods for WSD. But in the English all-words tasks in Senseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not make use of hand-tagged data (in some form or other) performed substantially worse than those that did. Table 1 summarizes the situation. It gives the precision and recall of the best1 two supervised (S) and unsupervised (U)2 systems for the English all words and English lexical sample for Senseval-23 and -3, along with the ﬁrst sense baseline (FS) reported by the task organizers.4 This is a simple application of the “ﬁrst sense” heuristic—that is, using the most common sense of a word for every instance of it in the test corpus, regardless of context. Although contextual WSD is of course preferable, the baseline is a very powerful one and unsupervised systems ﬁnd it surprisingly hard to beat (indeed, some of the systems that report themselves as unsupervised actually make some use of a manually obtained ﬁrst-sense heuristic). Considering both precision and recall, only 5 of 26 systems in the Senseval-3 English all-words task beat the ﬁrst sense heuristic as derived from SemCor (61.5%5), and then by only a few percentage points (the top system scoring 65% precision and recall) despite using hand-tagged training data available from SemCor and previous Senseval data sets, large sets of contextual features, and sophisticated machine learning algorithms. The performance of WSD systems, at least for all-words tasks, seems to have plateaued at a level just above the ﬁrst sense heuristic (Snyder and Palmer 2004). This is due to the shortage of training data and the often ﬁne granularity of sense distinctions. Ide and Wilks (2006) argue that it is best to concentrate effort on distinctions which are useful for applications and where systems can be conﬁdent of high precision. In cases where systems are less conﬁdent, but word senses, rather than words, are needed, the ﬁrst sense heuristic is a powerful back-off strategy. This strategy is dependent on information provided in dictionaries. Two dictionaries that have been used by English WSD systems are the Longman Dictionary of Contemporary English (LDOCE) (Procter  
 Computational Linguistics  Volume 33, Number 4  Figure 1 Example of a tree-based model of text structure. Figure 2 Example of a graph-based model of text structure. Figure 3 Example of Wolf and Gibson’s model of text structure. units created by regular discourse relations. An example of a Wolf-and-Gibson-style text structure is given in Figure 3. Wolf and Gibson’s arguments for their model of text structure derive mainly from an exercise in manual annotation. Two annotators hand-analyzed a corpus of 135 texts in the newspaper/newswire genre. In the ﬁrst part of Chapter 2, Wolf and Gibson describe their model of text structure in detail, and give an admirably detailed protocol for manual analysis of texts, with copious examples. The remainder of the book outlines a number of separate arguments for their conception of text structure. I will ﬁrst summarize these arguments, and then assess them. Wolf and Gibson’s ﬁrst argument derives directly from an analysis of the text structures which make up the corpus of hand-annotated texts. In the second part of Chapter 2, they argue that there are many phenomena captured in these analyses that would have been missed by a model imposing a strict tree structure. In particular, their analyses reveal a large number of crossed dependencies, and a large number of nodes participating in more than one relation, neither of which can be accounted for with a tree-based model. Two further arguments concern the inﬂuence of relations on the pattern of pronouns in a text. These arguments address the issue of the “psychological reality” of the relations hypothesized in the model: the authors reason that if discourse relations are 592  Book Reviews shown to inﬂuence the way pronouns are resolved or generated, this is evidence that they reﬂect real psychological representations in the minds of readers and writers. The ﬁrst part of Chapter 3 reports on a psycholinguistic study, in which subjects read a series of two-clause sentences. The second clause of each sentence contained a pronoun whose antecedent was unambiguously found in the ﬁrst clause (e.g., Fiona defeated Craig, and so James congratulated him). The connective in between the clauses was manipulated; it was found that this had an inﬂuence on the time taken by subjects to read the pronoun. The second part of Chapter 3 reports on a study of the pronouns in the annotated newspaper corpus. Again it was found that pronominalization preferences are different for different discourse relations. A ﬁnal argument, given in Chapter 4, concerns the utility of Wolf and Gibson’s model of text structure in a practical text-processing application: text summarization. Wolf and Gibson develop a number of algorithms for text summarization that use their graph-based text structures. They then compare the quality of the summaries generated by these algorithms with summaries produced using other techniques, including some techniques using hand-annotated tree structures (from Carlson, Marcu, and Okurowski 2003). The graph-based summarization techniques outperform the other techniques, which they take as another piece of evidence in favor of a graph-based conception of text structure. It is worth noting that the arguments relating to pronouns presented in Chapter 3 have a rather different aim from those presented in Chapters 2 and 4. The arguments given in Chapter 3 are only tangentially relevant to the issue of the structure of relations in coherent text. They bear on the psychological reality of individual relations, rather than on the general question of how relations organize text into coherent structures. The experiments in Chapter 3 certainly provide good evidence about the relevance of individual relations to pronoun generation and interpretation. In fact there is already quite a lot of experimental work showing the effect of coherence relations on pronoun interpretation; see, for example, the references cited by Stevenson et al. (2000) for a review. Much of this work uses a sentence completion paradigm, in which subjects are asked to complete a sentence like Ken impressed Geoff because he. . . ; the connective used in a sentence has a strong inﬂuence on subjects’ interpretation of the pronoun. Wolf and Gibson’s experiments come to a similar conclusion about the inﬂuence of coherence relations on pronominal reference, using different experimental paradigms (self-paced reading and corpus analysis), and therefore extend the earlier ﬁndings. However, they do not bear on the authors’ central hypothesis about text structure. Wolf and Gibson’s other two arguments bear directly on the question of whether texts should be analyzed using tree or graph structures. The argument in Chapter 2 is the most direct: the authors analyze a large number of texts, and identify many phenomena that a tree-based account would overlook. Although there have been several other analyses in this vein in the past, these have typically involved discussion of a small set of problematic example texts. Wolf and Gibson are the ﬁrst to provide a largescale quantitative evaluation of the coverage of a tree-based theory; they enumerate the frequency of crossed dependencies and of nodes with multiple parents in their corpus, to emphasize that these phenomena are widespread. Are these analyses convincing evidence against a tree-based model of text structure? I do not believe so. My main concern is that the analyses assume the truth of the very theory they are being used to test. Wolf and Gibson’s analysts follow a detailed protocol when analyzing the texts in the corpus. This protocol allows analysts to create crossing dependencies and re-entrant structures. It is unsurprising that the resulting analyses display these phenomena. To take a fanciful analogy: imagine proposing a 593  Computational Linguistics  Volume 33, Number 4  theory that holds that each discourse segment in a text is related to each other segment. We can certainly write a protocol to tell analysts how to annotate texts in line with this theory. No doubt analysts following this protocol will also achieve excellent interannotator agreement. However, we obviously can’t use the set of analyses they produce as empirical evidence for the theory. Rather, the theory must be assessed in relation to its predictions about independently observable phenomena in discourse.1 It might be argued that although Wolf and Gibson allow re-entrancy and crossing dependencies, they do not oblige an analyst to ﬁnd such phenomena in a text, and therefore that the presence of these phenomena still constitutes an empirical result. However, there are several somewhat nonstandard relations in Wolf and Gibson’s model that introduce relations between discontinuous spans of text, and therefore heavily bias analyses towards crossing dependencies. For instance, one relation, called SAME-SEGMENT, is used to link two portions of a single sentence separated by a sentential modiﬁer; for example, [The economy,] according to some analysts, [is expected to improve]. This relation is in fact deﬁned to hold across discontinuous text segments. Another relation is used to analyze cases of explicit attribution of an utterance to a speaker; for example, [“Sure I’ll be polite,”][promised one driver]. Such attribution statements frequently appear within an extended speaker utterance, which again makes relations between discontinuous segments almost inevitable. Both SAME-SEGMENT and ATTRIBUTION are contentious as discourse relations because they apply between portions of propositions rather than whole propositions. And by creating relations between discontinuous text segments, both bias analyses towards crossing dependencies. Wolf and Gibson do in fact consider the possibility that divergences from tree structure are due to a speciﬁc subset of relations. Indeed, they perform an analysis which apparently rules out this possibility. Surprisingly, this analysis seems to indicate that SAME-SEGMENT and ATTRIBUTION appear considerably less frequently in crossed dependencies than in analyses generally. This is very hard to believe, given that nearly all of their own examples of crossed dependencies involve one or other of these relations, and often involve both. In conclusion, Wolf and Gibson’s collection of text analyses do not provide sufﬁcient evidence to prefer the graph-based structural theory over the treebased theory. These analyses are perhaps better thought of as embodying a statement of their theory than as an empirical test of it. The argument presented in Chapter 4 is considerably more compelling as empirical support for a graph-based theory of text structure. To recap, the theory is used as the basis for an automatic text summarization algorithm, the results of which are compared to those of other algorithms, including algorithms based on the alternative tree-structure model of text. In this case, the theory (supplemented by its associated summarization algorithm) makes predictions about a quite distinct empirical phenomenon: the intuitions of naive readers about the relative importance of the segments of a text. The alternative tree-structure theory (supplemented by its own summarization algorithm) also makes such predictions. We can evaluate these two sets of predictions against data gathered from actual readers. And it turns out that the graph-based model of text structure outperforms the tree-based model. It is not completely true to say that Wolf and Gibson’s analysis protocol makes no reference to the concept of segment importance. In fact their deﬁnition of directed relations is similar to the deﬁnition of “nuclearity” in Mann and Thompson’s (1988)  
© 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 3  Galliers (1996). Perhaps this derived from her being a long associate of Wittgenstein’s student Margaret Masterman (a connection Karen acknowledged in her acceptance speech for the ACL Lifetime Achievement Award in 2005) and consequently from the focus of Wittgenstein’s philosophy on language use. Karen was born in Huddersﬁeld, Yorkshire, England in 1935 of English and Norwegian parents and was very much a child of Second World War Britain. She initially read History at Cambridge, but graduated in Philosophy (Moral Sciences). Following a brief spell as a schoolteacher she joined the Cambridge Language Research Unit in 1957 under Masterman and so began a very lengthy period on soft money. The University of Cambridge ﬁnally recognized her standing by awarding her a Readership in 1994 and a personal chair in 1999. Karen received very many awards during her career. These include Fellowships of the American and European Artiﬁcial Intelligence societies, Fellowship of the British Academy, the ACL Lifetime Achievement Award, the Lovelace Medal of the British Computer Society, the ACM SIGIR Salton Award, the American Society for Information Science and Technology’s Award of Merit, and the ACM-AAAI Allen Newell Award. Finally she was awarded the ACM Women’s Group Athena Award, which sadly she did not live to receive formally, although she bravely recorded an acceptance lecture a few weeks prior to her death. Although Karen felt she was never overtly discriminated against as a woman, it must have taken considerable courage and tenacity to develop a leading academic career at a time when society’s expectations of women went little beyond their roles as wives and mothers. She acknowledged the support of her parents and then her late husband (and long-time academic collaborator) Roger Needham in her obtaining a good education and developing a career. She noted that in professional circles she was very frequently the only woman, and made very active efforts, especially in recent years, to interest young women and girls in careers in computing, believing that “computing was too important to be left to men.” Karen remained as active as ever after her formal and undesired retirement (see The Guardian’s Letters Page, 5 October 20021). These academic achievements should not be allowed to eclipse Karen’s wellroundedness as a person. She and her late husband Roger were keen sailors, owning, and maintaining themselves for many years, an 1872 Itchen Ferry Cutter. They also built their ﬁrst house with their own hands. Karen could sketch well, made works of art from everyday objects, and had an enormous knowledge of natural history and church architecture. She was candid and passionate and could be enormously generous in her support. She will be greatly missed.  References Masterman, Margaret, Roger M. Needham, and Karen Spa¨rck Jones. 1958. The analogy between mechanical translation and library retrieval. In Proceedings of the International Conference on Scientiﬁc Information (1958), volume 2, pages 917–935, Washington, D.C. Spa¨rck Jones, Karen. 1964. Synonymy and Semantic Classiﬁcation. Ph.D. thesis, University of Cambridge.  Spa¨rck Jones, Karen. 1972. A statistical interpretation of term speciﬁcity and its application in retrieval. Journal of Documentation, 28:11–21. Reprinted in Journal of Documentation, 60:493–502, 2004. Spa¨rck Jones, Karen. 2005. “Some points in a time.” Acceptance speech for ACL Lifetime Achievement Award. Computational Linguistics, 31(1):1–14.  
∗ USC/ISI - Natural Language Group, 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292-6601. E-mail: fraser@isi.edu, marcu@isi.edu. 
∗ Department of Statistical Science, Cornell University, Ithaca, NY 14853. E-mail: pl332@cornell.edu. ∗∗ Microsoft Research, Microsoft Corp., Redmond, WA 98052. E-mail: church@microsoft.com. 
∗ Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street, Suite 400A, Philadelphia, PA 19104-6228, USA. E-mail: juliahr@cis.upenn.edu. ∗∗ School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: steedman@inf.ed.ac.uk. Submission received: 16 July 2005; revised submission received: 24 January 2007; accepted for publication: 21 February 2007. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 3  subcorpus of Wall Street Journal text that has become the de facto standard training and test data for statistical parsers. Its annotation, which is based on generic phrasestructure grammar (with coindexed traces and other null elements indicating non-local dependencies) and function tags on nonterminal categories providing (a limited degree of) syntactic role information, is designed to facilitate the extraction of the underlying predicate–argument structure. Statistical parsing on the Penn Treebank has made great progress by focusing on the machine-learning or algorithmic aspects (Magerman 1994; Ratnaparkhi 1998; Collins 1999; Charniak 2000; Henderson 2004; McDonald, Crammer, and Pereira 2005). However, this has often resulted in parsing models and evaluation measures that are both based on reduced representations which simplify or ignore the linguistic information represented by function tags and null elements in the original Treebank. (One exception is Collins 1999, whose Model 2 includes a distinction between arguments and adjuncts, and whose Model 3 additionally captures wh-movement in relative clauses with a GPSG-like “slash-feature-passing” mechanism.) The reasons for this shift away from linguistic adequacy are easy to trace. The very healthy turn towards quantitative evaluation interacts with the fact that just about every dimension of linguistic variation exhibits a Zipﬁan distribution, where a very small proportion of the available alternatives accounts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes 1992), Minimalist Program–related Grammars (Stabler 2004), or Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000). However, until very recently, only handwritten grammars, which lack the wide coverage and robustness of Treebank parsers, were available for these formalisms (Butt et al. 1999; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill et al. (2004) uses  
(1) a. A: Who wants Beethoven music? B: Richard and James. [BNC: KB8 1024–1025]1 b. A: It’s Ruth’s birthday. B: When? [BNC: KBW 13116–13117] ∗ Karl-Liebknecht Strasse 24-25, 14476 Golm, Germany. E-mail: raquel@ling.uni-potsdam.de. ∗∗ The Strand, London WC2R 2LS, UK. E-mail: jonathan.ginzburg@kcl.ac.uk. † The Strand, London WC2R 2LS, UK. E-mail: shalom.lappin@kcl.ac.uk. 1 This notation indicates the name of the ﬁle and the sentence numbers in the BNC. Submission received: 24 September 2004; revised submission received: 10 November 2006; accepted for publication: 9 March 2007. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 3  Arguably the most important issue in the processing of NSUs concerns their resolution, that is, the recovery of a full clausal meaning from a form which is standardly considered non-clausal. In the ﬁrst of the examples, the NSU in bold face is a typical “short answer,” which despite having the form of a simple NP would most likely be understood as conveying the proposition Richard and James want Beethoven music. The NSU in (1b) is an example of what has been called a “sluice.” Again, despite being realized by a bare wh-phrase, the meaning conveyed by the NSU could be paraphrased as the question When is Ruth’s birthday? Although short answers and short queries like those in (1) are perhaps two of the most prototypical NSU classes, recent corpus studies (Ferna´ndez and Ginzburg 2002; Schlangen 2003) show that other less well-known types of NSUs—each with its own resolution constraints—are also pervasive in real conversations. This variety of NSU classes, together with their inherent concise form and their highly context-dependent meaning, often make NSUs ambiguous. Consider, for instance, example (2): (2) a. A: I left it on the table. B: On the table. b. A: Where did you leave it? B: On the table. c. A: I think I put it er. . . B: On the table. d. A: Should I put it back on the shelf? B: On the table. An NSU like B’s response in (2a) can be understood either as a clariﬁcation question or as an acknowledgment, depending on whether it is uttered with raising intonation or not. In (2b), on the other hand, the NSU is readily understood as a short answer, whereas in (2c) it ﬁlls a gap left by the previous utterance. Yet in the context of (2d) it will most probably be understood as a sort of correction or a “helpful rejection,” as we shall call this kind of NSU later on in this article. As different NSU classes are typically related to different resolution constraints, in order to resolve NSUs appropriately systems need to be equipped in the ﬁrst place with the ability of identifying the intended kind of NSU. How this ability can be developed is precisely the issue we address in this article. We concentrate on the task of automatically classifying NSUs, which we approach using machine learning (ML) techniques. Our aim in doing so is to develop a classiﬁcation model whose output can be fed into a dialogue processing system—be it a full dialogue system or, for instance, an automatic dialogue summarization system—to boost its NSU resolution capability. As we shall see, to run the ML experiments we report in this article, we annotate our data with small sets of meaningful features, instead of using large sets of arbitrary features as is common in some stochastic approaches. We do this with the aim of obtaining a better understanding of the different classes of NSUs, their distribution, and their properties. For training, we use four machine learning systems: the rule induction learner SLIPPER (Cohen and Singer 1999), the memory-based learner TiMBL (Daelemans et al. 2003), the maximum entropy algorithm MaxEnt (Le 2003), and the Weka toolkit (Witten and Frank 2000). From the Weka toolkit we use the J4.8 decision tree learner, as well as a majority class predictor and a one-rule classiﬁer to derive baseline systems that help us to evaluate  398  Ferna´ndez, Ginzburg, and Lappin  Classifying NSUs in Dialogue  the difﬁculty of the classiﬁcation task and the ML results obtained. The main advantage of using several systems that implement different learning techniques is that this allows us to factor out any algorithm-dependent effects that may inﬂuence our results. The article is structured as follows. In Section 2, we introduce the taxonomy of NSU classes we adopt, present a corpus study done using the BNC, and give an overview of the theoretical approach to NSU resolution we assume. After these introductory sections, in Section 3 we present a pilot study that focuses on bare wh-phrases or sluices. This includes a small corpus study and a preliminary ML experiment that concentrates on disambiguating between the different interpretations that sluices can convey. We obtain very encouraging results: around 80% weighted F-score (an 8% improvement over a simple one-rule baseline). After this, in Section 4, we move on to the full range of NSUs. We present our main experiments, whereby the ML approach is extended to the task of classifying the full range of NSU classes in our taxonomy. The results we achieve on this task are decidedly positive: around an 87% weighted F-score (a 25% improvement over a four-rule baseline where only four features are used). Finally, in Section 5, we offer conclusions and some pointers for future work. 2. A Taxonomy of NSUs We propose a taxonomy that offers a comprehensive inventory of the kinds of NSUs that can be found in conversation. The taxonomy includes 15 NSU classes. With a few modiﬁcations, these follow the corpus-based taxonomy proposed by Ferna´ndez and Ginzburg (2002). In what follows we exemplify each of the categories we use in our work and characterize them informally. Clariﬁcation Ellipsis (CE). We use this category to classify reprise fragments used to clarify an utterance that has not been fully comprehended. (3) a. A: There’s only two people in the class B: Two people? [BNC: KPP 352–354] b. A: [. . . ] You lift your crane out, so this part would come up. B: The end? [BNC: H5H 27–28] Check Question. This NSU class refers to short queries, usually realized by conventionalized forms like alright? and okay?, that are requests for explicit feedback. (4) A: So <pause> I’m allowed to record you. Okay? B: Yes. [BNC: KSR 5–7] Sluice. We consider as sluices all wh-question NSUs, thereby conﬂating under this formbased NSU class reprise and direct sluices like those in (5a) and (5b), respectively.2 In the taxonomy of Ferna´ndez and Ginzburg (2002) reprise sluices are classiﬁed as CE. In the taxonomy used in the experiments we report in this article, however, CE only includes clariﬁcation fragments that are not bare wh-phrases.  2 This distinction is due to Ginzburg and Sag (2001). More on it will be discussed in Section 2.2. 399  Computational Linguistics  Volume 33, Number 3  (5) a. A: Only wanted a couple weeks. B: What? [BNC: KB1 3311–3312] b. A: I know someone who’s a good kisser. B: Who? [BNC: KP4 511–512] Short Answer. This NSU class refers to typical responses to (possibly embedded) whquestions (6a)/(6b). Sometimes, however, wh-questions are not explicit, as in the context of a short answer to a CE question, for instance (6c). (6) a. A: Who’s that? B: My Aunty Peggy. [BNC: G58 33–35] b. A: Can you tell me where you got that information from? B: From our wages and salary department. [BNC: K6Y 94–95] c. A: Vague and? B: Vague ideas and people. [BNC: JJH 65–66] Plain Afﬁrmative Answer and Plain Rejection. The typical context of these two classes of NSUs is a polar question (7a), which can be implicit as in CE questions like (7b). As shown in (7c), rejections can also be used to respond to assertions. (7) a. A: Did you bring the book I told you? B: Yes./ No. b. A: That one? B: Yeah. [BNC: G4K 106–107] c. A: I think I left it too long. B: No no. [BNC: G43 26–27] Both plain afﬁrmative answers and rejections are strongly indicated by lexical material, characterized by the presence of a ‘yes’ word ( yeah, aye, yep. . . ) or the negative interjection no. Repeated Afﬁrmative Answer. We distinguish plain afﬁrmative answers like the ones in (7) from repeated afﬁrmative answers like the one in (8), which respond afﬁrmatively to a polar question by verbatim repetition or reformulation of (a fragment of) the query. (8) A: Did you shout very loud? B: Very loud, yes. [BNC: JJW 571-572] Helpful Rejection. The context of helpful rejections can be either a polar question or an assertion. In the ﬁrst case, they are negative answers that provide an appropriate alternative (9a). As responses to assertions, they correct some piece of information in the previous utterance (9b). (9) a. A: Is that Mrs. John <last or full name>? B: No, Mrs. Billy. [BNC: K6K 67-68] 400  Ferna´ndez, Ginzburg, and Lappin  Classifying NSUs in Dialogue  b. A: Well I felt sure it was two hundred pounds a, a week. B: No ﬁfty pounds ten pence per person. [BNC: K6Y 112–113] Plain Acknowledgment. The class plain acknowledgment refers to utterances (like yeah, mhm, ok) that signal that a previous declarative utterance was understood and/or accepted. (10) A: I know that they enjoy debating these issues. B: Mhm. [BNC: KRW 146–147] Repeated Acknowledgment. This class is used for acknowledgments that, as repeated afﬁrmative answers, also repeat a part of the antecedent utterance, which in this case is a declarative. (11) A: I’m at a little place called Ellenthorpe. B: Ellenthorpe. [BNC: HV0 383–384] Propositional and Factual Modiﬁers. These two NSU classes are used to classify propositional adverbs like (12a) and factual adjectives like (12b), respectively, in stand-alone uses. (12) a. A: I wonder if that would be worth getting? B: Probably not. [BNC: H61 81–82] b. A: There’s your keys. B: Oh great! [BNC: KSR 137–138] Bare Modiﬁer Phrase. This class refers to NSUs that behave like adjuncts modifying a contextual utterance. They are typically PPs or AdvPs. (13) A: [. . . ] they got men and women in the same dormitory! B: With the same showers! [BNC: KST 992–996] Conjunct. This NSU class is used to classify fragments introduced by conjunctions. (14) A: Alistair erm he’s, he’s made himself coordinator. B: And section engineer. [BNC: H48 141–142] Filler. Fillers are NSUs that ﬁll a gap left by a previous unﬁnished utterance. (15) A: [. . . ] twenty two percent is er <pause> B: Maxwell. [BNC: G3U 292–293] 2.1 The Corpus Study The taxonomy of NSUs presented herein has been tested in a corpus study carried out using the dialogue transcripts of the BNC. The study, which we describe here brieﬂy, supplies the data sets used in the ML experiments we will present in Section 4. The present corpus of NSUs includes and extends the subcorpus used in Ferna´ndez and Ginzburg (2002). It was created by manual annotation of a randomly selected section of 200-speaker-turns from 54 BNC ﬁles. Of these ﬁles, 29 are transcripts of 401  Computational Linguistics  Volume 33, Number 3  conversations between two dialogue participants, and 25 ﬁles are multi-party transcripts. The total of transcripts used covers a wide variety of domains, from free conversation to meetings, tutorials and training sessions, as well as interviews and transcripts of medical consultations. The examined subcorpus contains 14,315 sentences. Sentences in the BNC are identiﬁed by the CLAWS segmentation scheme (Garside 1987) and each unit is assigned an identiﬁer number. We found a total of 1,299 NSUs, which make up 9% of the total of sentences in the subcorpus. These results are in line with the rates reported in other recent corpus studies of NSUs: 11.15% in (Ferna´ndez and Ginzburg 2002), 10.2% in (Schlangen and Lascarides 2003), 8.2% in (Schlangen 2005).3 The NSUs found were labeled according to the taxonomy presented previously together with an additional class Other introduced to catch all NSUs that did not fall in any of the classes in the taxonomy. All NSUs that could be classiﬁed with the taxonomy classes were additionally tagged with the sentence number of their antecedent utterance. The NSUs not covered by the classiﬁcation only make up 1.2% (16 instances) of the total of NSUs found. Thus, with a rate of 98.8% coverage, the present taxonomy offers a satisfactory coverage of the data. The labeling of the entire corpus of NSUs was done by one expert annotator. To assess the reliability of the annotation, a small study with two additional, non-expert annotators was conducted. These annotated a total of 50 randomly selected instances (containing a minimum of two instances of each NSU class as labeled by the expert annotator) with the classes in the taxonomy. The agreement obtained by the three annotators is reasonably good, yielding a κ score of 0.76. The non-expert annotators were also asked to identify the antecedent sentence of each NSU. Using the expert annotation as a gold standard, they achieved 96% and 92% accuracy in this task. The distribution of NSU classes that emerged after the annotation of the subcorpus is shown in detail in Table 1. By far the most common class can be seen to be Plain Acknowledgment, which accounts for almost half of all NSUs found. This is followed in frequency by Short Answer (14.5%) and Plain Afﬁrmative Answer (8%). CE is the most common class among the NSUs that denote questions (i.e., CE, Sluice, and Check Question), making up 6.3% of all NSUs found. 2.2 Resolving NSUs: Theoretical Background and Implementation The theoretical background we assume with respect to the resolution of NSUs derives from the proposal presented in Ginzburg and Sag (2001), which in turn is based on the theory of context developed by Ginzburg (1996, 1999). Ginzburg and Sag (2001) provide a detailed analysis of a number of classes of NSUs—including Short Answer, Sluice, and CE—couched in the framework of Headdriven Phrase Structure Grammar (HPSG). They take NSUs to be ﬁrst-class grammatical constructions whose resolution is achieved by combining the contribution of the NSU phrase with contextual information—concretely, with the current question under discussion, or QUD, which roughly corresponds to the current conversational topic.4  3 For a comparison of our NSU taxonomy and the one proposed by Schlangen (2003), see Ferna´ndez (2006). 4 An anonymous reviewer asked about the distinction between NSUs that are meaning complete and those which are not. In fact we take all NSUs to be interpreted as full propositions or questions. 402  Ferna´ndez, Ginzburg, and Lappin  Classifying NSUs in Dialogue  Table 1 Distribution of NSU classes. NSU class Plain Acknowledgment Short Answer Plain Afﬁrmative Answer Repeated Acknowledgment Clariﬁcation Ellipsis Plain Rejection Factual Modiﬁer Repeated Afﬁrmative Answer Helpful Rejection Check Question Filler Bare Modiﬁer Phrase Propositional Modiﬁer Sluice Conjunct Other Total  Total 599 188 105 86 82 49 27 26 24 22 18 15 11 21 10 16 1,299  % 46.1 14.5 8.0 6.6 6.3 3.7 2.0 2.0 1.8 1.7 1.4 1.1 0.8 1.6 0.7 1.2 100  The simplest way of exemplifying this strategy is perhaps to consider a direct short answer to an explicit wh-question, like the one shown in (16a). (16) a. A: Who’s making the decisions? B: The fund manager. (= The fund manager is making the decisions.) [BNC: JK7 119–120] b. QUD: λ(x).Make decision(x, t) Resolution: Make decision( fm,t) In this dialogue, the current QUD corresponds to the content of the previous utterance— the wh-question Who’s making the decisions? Assuming a representation of questions as lambda abstracts, the resolution of the short answer amounts to applying this question to the phrasal content of the NSU, as shown in (16b) in an intuitive notation.5 Ginzburg and Sag (2001) distinguish between direct and reprise sluices. For direct sluicing, the current QUD is a polar question p?, where p is required to be a quantiﬁed proposition.6 The resolution of the direct sluice consists in constructing a whquestion by a process that replaces the quantiﬁcation with a simple abstraction. For instance: (17) a. A: A student phoned. B: Who? (= Which student phoned?)  5 To simplify matters, throughout the examples in this section we use lambda abstraction for wh-questions and a simple question mark operator for polar questions. For a far more accurate representation of questions in HPSG and Type Theory with Records, see Ginzburg and Sag (2001) and Ginzburg (2005), respectively. 6 In Ginzburg’s theory of context an assertion of a proposition p raises the polar question p? for discussion. 403  Computational Linguistics  Volume 33, Number 3  b. QUD: ?∃xPhone(x, t) Resolution: λ(x).Phone(x, t) In the case of reprise sluices and CE, the current QUD arises in a somewhat less direct way, via a process of utterance coercion or accommodation (Larsson 2002; Ginzburg and Cooper 2004), triggered by the inability to ground the previous utterance (Traum 1994; Clark 1996). The output of the coercion process is a question about the content of a (sub)utterance which the addressee cannot resolve. For instance, if the original utterance is the question Did Bo leave? in (18a), with Bo as the unresolvable sub-utterance, one possible output from the coercion operations deﬁned by Ginzburg and Cooper (2004) is the question in (18b), which constitutes the current QUD, as well as the resolved content of the reprise sluice in (18a). (18) a. A: Did Bo leave? B: Who? (= Who are you asking if s/he left?) b. QUD: λ(b).Ask(A, ?Leave(b, t)) Resolution: λ(b).Ask(A, ?Leave(b, t)) The interested reader will ﬁnd further details of this approach to NSU resolution and its extension to other NSU classes in Ginzburg (forthcoming) and Ferna´ndez (2006). The approach sketched here has been implemented as part of the SHARDS system (Ginzburg, Gregory, and Lappin 2001; Ferna´ndez et al., in press), which provides a procedure for computing the interpretation of some NSU classes in dialogue. The system currently handles short answers, direct and reprise sluices, as well as plain afﬁrmative answers to polar questions. SHARDS has been extended to cover several types of clariﬁcation requests and used as a part of the information-state-based dialogue system CLARIE (Purver 2004b). The dialogue system GoDiS (Larsson et al. 2000; Larsson 2002) also uses a QUD-based approach to handle short answers.  3. Pilot Study: Sluice Reading Classiﬁcation The ﬁrst study we present focuses on the different interpretations or readings that sluices can convey. We ﬁrst describe a corpus study that aims at providing empirical evidence about the distribution of sluice readings and establishing possible correlations between these readings and particular sluice types. After this, we report the results of a pilot machine learning experiment that investigates the automatic disambiguation of sluice interpretations. 3.1 The Sluicing Corpus Study We start by introducing the corpus of sluices. The next subsections describe the annotation scheme, the reliability of the annotation, and the corpus results obtained. Because sluices have a well-deﬁned surface form—they are bare wh-words—we were able to use an automatic mechanism to reliably construct our subcorpus of sluices. This was created using SCoRE (Purver 2001), a tool that allows one to search the BNC using regular expressions. 404  Ferna´ndez, Ginzburg, and Lappin  Classifying NSUs in Dialogue  Table 2 Total of sluices in the BNC.  what why who where which N when how which Total  3,045 1,125 491 350  160  107 50 15 5,343  The dialogue transcripts of the BNC contain 5,183 bare sluices (i.e., 5,183 sentences consisting of just a wh-word). We distinguish between the following classes of bare sluices: what, who, when, where, why, how, and which. Given that only 15 bare which were found, we also considered sluices of the form which N. Including which N, the corpus contains a total of 5,343 sluices, whose distribution is shown in Table 2. For our corpus study, we selected a sample of sluices extracted from the total found in the dialogue transcripts of the BNC. The sample was created by selecting all instances of bare how (50) and bare which (15), and arbitrarily selecting 100 instances of each of the remaining sluice classes, making up a total of 665 sluices. Note that the sample does not reﬂect the frequency of sluice types found in the full corpus. The inclusion of sufﬁcient instances of the lesser frequent sluice types would have involved selecting a much larger sample. Consequently it was decided to abstract over the true frequencies to create a balanced sample whose size was manageable enough to make the manual annotation feasible. We will return to the issue of the true frequencies in Section 3.1.3. 3.1.1 Sluice Readings. The sample of sluices was classiﬁed according to a set of four semantic categories—drawn from the theoretical distinctions introduced by Ginzburg and Sag (2001)—corresponding to different sluice interpretations. The typology reﬂects the basic direct/reprise divide and incorporates other categories that cover additional readings, including an Unclear class intended for those cases that cannot easily be classiﬁed by any of the other categories. The typology of sluice readings used was the following: Direct. Sluices conveying a direct reading query for additional information that was explicitely or implicitly quantiﬁed away in the antecedent, which is understood without difﬁculty. The sluice in (19) is an example of a sluice with direct reading: It asks for additional temporal information that is implicitly quantiﬁed away in the antecedent utterance. (19) A: I’m leaving this school. B: When? [BNC: KP3 537–538] Reprise. Sluices conveying a reprise reading emerge as a result of an understanding problem. They are used to clarify a particular aspect of the antecedent utterance corresponding to one of its constituents, which was not correctly comprehended. In (20) the reprise sluice has as antecedent constituent the pronoun he, whose reference could not be adequately grounded. (20) A: What a useless fairy he was. B: Who? [BNC: KCT 1752–1753] 405  Computational Linguistics  Volume 33, Number 3  Clariﬁcation. As reprise, this category also corresponds to a sluice reading that deals with understanding problems. In this case the sluice is used to request clariﬁcation of the entire antecedent utterance, indicating a general breakdown in communication. The following is an example of a sluice with a clariﬁcation interpretation: (21) A: Aye and what money did you get on it? B: What? A: What money does the government pay you? [BNC: KDJ 1077–1079] Wh-anaphor. This category is used for the reading conveyed by sluices like (22), which are resolved to a (possibly embedded) wh-question present in the antecedent utterance. (22) A: We’re gonna ﬁnd poison apple and I know where that one is. B: Where? [BNC: KD1 2370–2371] Unclear. We use this category to classify those sluices whose interpretation is difﬁcult to grasp, possibly because the input is too poor to make a decision as to its resolution, as in the following example: (23) A: <unclear> <pause> B: Why? [BNC: KCN 5007]  3.1.2 Reliability. The coding of sluice readings was done independently by three different annotators. Agreement was moderate (κ = 0.59). There were important differences among sluice classes: The lowest agreement was on the annotation of how (0.32) and what (0.36), whereas the agreement on classifying who was substantially higher (0.74). Although the three coders may be considered “experts,” their training and familiarity with the data were not equal. This resulted in systematic differences in their annotations. Two of the coders had worked more extensively with the BNC dialogue transcripts and, crucially, with the deﬁnition of the categories to be applied. Leaving the third annotator out of the coder pool increases agreement very signiﬁcantly (κ = 0.71). The agreement reached by the more expert pair of coders was acceptable and, we believe, provides a solid foundation for the current classiﬁcation.7  3.1.3 Distribution Patterns. The sluicing corpus study shows that the distribution of readings is signiﬁcantly different for each class of sluice. The distribution of interpretations is shown in Table 3, presented as row counts and percentages of those instances where  7 Besides the difﬁculty of annotating ﬁne-grained semantic distinctions, we think that one of the reasons why the κ score we obtain is not too high is that, as shall become clear in the next section, the present annotation is strongly affected by the prevalence problem, which occurs when the distributions for categories are skewed (highly unequal instantiation across categories). In order to control for differences in prevalence, Di Eugenio and Glass (2004) propose an additional measure called PABAK (prevalence-adjusted bias-adjusted kappa). In our case, we obtain a PABAK score of 0.60 for agreement amongst the three coders, and a PABAK score of 0.80 for agreement between the pair of more expert coders. A more detailed discussion of these issues can be found in Ferna´ndez (2006). 406  Ferna´ndez, Ginzburg, and Lappin  Classifying NSUs in Dialogue  Table 3 Distribution patterns.  Sluice Direct n (%) Reprise n (%) Clariﬁcation n (%) Wh-anaphor n (%)  what why who where when which whichN how  7 (9.60) 55 (68.7) 10 (13.0) 31 (34.4) 50 (63.3) 
Submission received: 20 December 2004; revised submission received: 26 September 2006; accepted for publication: 23 November 2006. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 2  can then be quantiﬁed directly using a distance measure such as cosine or Euclidean distance. Contexts are deﬁned as a small number of words surrounding the target word (Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs—even documents (Salton, Wang, and Yang 1975; Landauer and Dumais 1997). Latent Semantic Analysis (LSA; Landauer and Dumais 1997) is an example of a document-based vector space model that is commonly used in information retrieval and cognitive science. Each target word t is represented by a k element vector of paragraphs p1...k and the value of each vector element is a function of the number of times t occurs in pi. In contrast, the Hyperspace Analogue to Language model (HAL; Lund and Burgess 1996) creates a word-based semantic space: each target word t is represented by a k element vector, whose dimensions correspond to context words c1...k. The value of each vector element is a function of the number of times each ci occurs within a window of size n before or after t in a large corpus. In their simplest incarnation, semantic space models treat context as a set of unordered words, without even taking parts of speech into account (e.g., to drink and a drink are represented by a single vector). In fact, with the exception of function words (e.g., the, down), which are often removed, it is often assumed that all context words within a certain distance from the target word are semantically relevant. Because no linguistic knowledge is taken into account, the construction of semantic space models is straightforward and language-independent—all that is needed is a segmented corpus of written or spoken text. However, the assumption that contextual information contributes indiscriminately to a word’s meaning is clearly a simpliﬁcation. There is ample evidence demonstrating that syntactic relations across and within sentences are crucial for sentence and discourse processing (West and Stanovich 1986; Neville et al. 1991; Fodor 1995; Miltsakaki 2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994). Furthermore, much research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995). It is therefore not surprising that there have been efforts to enrich vector-based models with morpho-syntactic information. Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a). In these semantic space models, contexts are deﬁned over words bearing a syntactic relationship to the target words of interest. This makes semantic spaces more ﬂexible; different types of contexts can be selected; words do not have to co-occur within a small, ﬁxed word window; and word order or argument structure differences can be naturally mirrored in the semantic space. This article proposes a general framework for semantic space models which conceptualizes context in terms of syntactic relations. We introduce an algorithm for constructing semantic space models from texts annotated with syntactic information (speciﬁcally dependency relations) and illustrate how different model classes can be derived from this linguistically rich representation. Our guiding hypothesis is that syntactic structure in general and argument structure in particular is a close reﬂection of lexical meaning (Levin 1993). We thus model meaning by quantifying the degree to which words are attested in similar syntactic environments. The expressive power of our framework stems from three novel parameters which guide model construction. The ﬁrst parameter determines which types of syntactic structures contribute towards the 162  Padó and Lapata  Dependency-Based Semantic Spaces  representation of lexical meaning. The second parameter allows us to weigh the relative importance of different syntactic relations. Finally, the third parameter determines how the semantic space is actually represented, for instance as co-occurrences of words with other words, words with parts of speech, or words with argument relations (e.g., subject, object). We evaluate our framework on tasks relevant for cognitive science and NLP. We start by simulating semantic priming, a phenomenon that has received much attention in computational psycholinguistics and is typically modeled using word-based semantic spaces (Landauer and Dumais 1997; McDonald and Brew 2004). We next consider the problem of recognizing synonyms by selecting an appropriate synonym for a target word from a set of semantically related candidate words. Speciﬁcally, we evaluate the performance of our model on synonym questions from the Test of English as a Foreign Language (TOEFL). These are routinely used as a testbed for assessing how well vector-based models capture lexical knowledge (Landauer and Dumais 1997; Turney 2001; Sahlgren 2006). Our ﬁnal experiment concentrates on unsupervised word sense disambiguation (WSD), thereby exploring the potential of the proposed framework for NLP applications requiring large scale semantic processing. We automatically infer predominant senses in untagged text by incorporating our syntax-based semantic spaces into the modeling paradigm proposed by McCarthy et al. (2004). In all cases, we show that our framework consistently outperforms word-based models yielding results that are comparable or superior to state of the art. Our contributions are threefold: a novel framework for semantic spaces that incorporates syntactic information in the form of dependency relations and generalizes previous syntax-based vector-based models; an application of this framework to a wide range of tasks relevant to cognitive modeling and NLP; and an empirical comparison of our dependency-based models against state-of-the-art word-based models. In Section 2, we give a brief overview of existing word-based and syntax-based models. In Section 3, we present our modeling framework and relate it to previous work. Section 4 discusses the parameter settings for our experiments. Section 5 details our priming experiment, Section 6 presents our study on the TOEFL synonymy task, and Section 7 describes our sense-ranking experiment. Discussion of our results and future work concludes the article (Section 8).  2. Overview of Semantic Space Models 2.1 Word-Based and Syntax-Based Models To facilitate comparisons with our framework, we begin with a brief overview of existing semantic space models. We describe traditional word-based co-occurrence models as exempliﬁed in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a). Lowe (2001) deﬁnes a semantic space model as a quadruple B, A, S, V . B is the set b1...D of basis elements, the dimensions of the space. B can be a set of words (Lund and Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows 2003) or words with a syntactic relation such as subject or object (Lin 1998a). Usually, the dimensionality of the matrix is restricted to a relatively small number. A popular choice is the k most frequent words (minus the stop words) in a corpus, typically 100– 2,000 (McDonald 2000; Levy and Bullinaria 2001). A is a lexical association function 163  Computational Linguistics  Volume 33, Number 2  applied to the co-occurrence frequency of target word t with basis element b so that each word is represented by a vector v = A( f (t, b1)), A( f (t, b2)), . . . , A( f (t, bn)) . If A is the identity function, the raw frequencies are used. Functions such as mutual information or the log-likelihood ratio are often applied to factor out co-occurrences due to chance. S is a similarity measure that maps pairs of vectors onto a continuous-valued scale of contextual similarity. V is an optional transformation that reduces the dimensionality of the semantic space. Singular value decomposition (SVD; Berry, Dumais, and O’Brien 1994; Golub and Loan 1989) is commonly used for this purpose. SVD can be thought of as a means of inferring latent structure in distributional data, while making sparse matrices more informative. For the rest of this article, we will ignore V and other statistical transformations and concentrate primarily on ways of inducing structure from grammatical and syntactic information. To illustrate this deﬁnition, we construct a word-based semantic space for the target words T = {lorry, carry, sweet, fruit}, using as our corpus the following sentence: A lorry might carry sweet apples. For a word-based space, we might use the basis elements B = {lorry, might, carry, sweet, apples}, a symmetric window of size 2, and identity as the association function A. Each target word ti ∈ T will then be represented by a ﬁvedimensional row vector, and the value of each vector element will record the number of times each basis element bi ∈ B occurs within a window of two words to the left and two words to the right of the target word ti. The co-occurrence matrix that we obtain according to these speciﬁcations is shown in Figure 1. A variety of distance measures can be used to compute the similarity S between two target words (see Lee [1999] for an overview), the cosine being the most popular:  n  xiyi  simcos(x, y ) =  i=1  n  n  (1)  x2i  y2i  i=1  i=1  Syntax-based semantic space models (Grefenstette 1994; Lin 1998a) go beyond mere co-occurrence by capturing syntactic relationships between words such as subject–verb or modiﬁer–noun, irrespectively of whether they are physically adjacent or not. The basis elements are generally assumed to be tuples (r, w) where w is a word occurring in relation type r with a target word t. The relations typically reﬂect argument structure (e.g., subject, object, indirect object) or modiﬁcation (e.g., adjective–noun, noun–noun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999; Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran 2004). The basis elements (r, w) are treated as a single unit and are often called attributes (Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a).  Figure 1 Word-based semantic space (symmetric window size 2). 164  Padó and Lapata  Dependency-Based Semantic Spaces  Figure 2 shows a syntax-based semantic space in the manner of Grefenstette (1994), using the basis elements (subj,lorry), (aux,might ), (mod,sweet ), and (obj,apples). The binary association function A records whether the target word possesses the feature (denoted by x in Figure 2) or not. Because the cells of the matrix do not contain numerical values, a similarity measure that is appropriate for categorical values must be chosen. Grefenstette (1994) uses a weighted version of Jaccard’s coefﬁcient, a measure of association commonly employed in information retrieval (Salton and McGill 1983). Assuming Attr(t) is the set of basis elements co-occurring with t, Jaccard’s coefﬁcient is deﬁned as:  simJacc(t1, t2 )  =  Attr(t1 ) Attr(t1 )  ∩ ∪  Attr(t2 ) Attr(t2 )  (2)  Lin (1998a) constructs a semantic space similar to Grefenstette (1994) except that the matrix cells represent the number of times a target word t co-occurs with basis element (r, w), as shown in Figure 3. He proposes an information theoretic similarity measure based on the distribution of target words and basis elements:  I(t1, r, w) + I(t2, r, w)  simlin(t1, t2) =  (r,w)∈T(t1 )∩T(t2 ) I(t1, r, w) +  I(t2, r, w)  (3)  (r,w)∈T(t1 )  (r,w)∈T(t2 )  where I(t, r, w) is the mutual information between t and r, w and T(t) is the set of basis elements (r, w) such that I(t, r, w) is positive and  I(t, r, w)  =  log  P(t, r, w)P(r) P(w, r)P(t, r)  =  log  P(w|r, t) P(w|r)  (4)  Figure 2 Grefenstette’s (1994) semantic space. Figure 3 Lin’s (1988a) semantic space. 165  Computational Linguistics  Volume 33, Number 2  2.2 Discussion Because syntax-based models capture more linguistic structure than word-based models, they should at least in theory provide more informative representations of word meaning. Unfortunately, comparisons between the two types of models have been few and far between in the literature. Furthermore, the potential of syntax-based models has not been fully realized since most previous approaches limit themselves to a speciﬁc model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002). This section discusses these issues in more detail and sketches how we plan to address them. Modeling of syntactic context. All existing syntax-based semantic space models we are aware of incorporate syntactic information in a rather limited fashion. For example, the construction of the space is either based on all relations (Grefenstette 1994; Lin 1998a) or a ﬁxed subset (Lee 1999), but there is no qualitative distinction between different relations. Even in cases where many relations are used (Lin 1998a; Lin and Pantel 2001), only direct relations are taken into account, ignoring potentially important co-occurrence patterns between, for instance, the subject and the object of a verb, or between a verb and its non-local argument (e.g., in control structures). Comparison between model classes. Syntax-based vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identiﬁcation (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003). Comparisons between word-based and syntax-based models on the same task are rare, and the effect of syntactic knowledge has not been rigorously investigated or quantiﬁed. The few studies on this topic reveal an inconclusive picture. On the one hand, Grefenstette compared the performance of the two classes of models on the task of automatic thesaurus extraction and found that a syntactically enhanced model gave signiﬁcantly better results over a simple word co-occurrence model. A replication of Grefenstette’s study with a more sophisticated parser (Curran and Moens 2002) revealed that additional syntactic information yields further improvements. On the other hand, attempts to generate more meaningful indexing terms for information retrieval (IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et al. 2002) have been largely unsuccessful. Experimental results show minimal differences in retrieval effectiveness at a substantially greater processing cost (see Voorhees [1999] for details). Impact on cognitive modeling. Despite their widespread use in NLP, syntax-based semantic spaces have attracted little attention in cognitive science and computational psycholinguistics. Wiemer-Hastings and Zipitria (2001) construct a semantic space similar to LSA, but enhanced with part-of-speech tags with the aim of modeling human raters in an intelligent tutoring context. Their results, however, show that the tagged LSA space yields worse performance than a word-based model. Kanejiya, Kumar, and Prasad (2003) attempt to capture syntactic context in a shallow manner by enhancing target words with the parts-of-speech of their immediately preceding words. They argue that this representation can provide useful information for the upcoming target words, as is often the case in language modeling and left-to-right parsing. They employ a documentbased semantic space, which they submit to SVD and subsequently compare against an LSA model that contains no syntactic information, again in the context of an intelligent 166  Padó and Lapata  Dependency-Based Semantic Spaces  tutoring system. Their results indicate that the syntactically enhanced model has better coverage than the LSA model (i.e., it is able to evaluate more student answers), although it displays a lower correlation with human raters than raw LSA. In this article, we argue the case for investigating dependency-based semantic space models in more depth. We provide a general deﬁnition for these models, which incorporates a wider range of syntactic relations than previously considered and subsumes existing syntax-based and word-based models. In order to demonstrate the scope of our framework, we evaluate our models on tasks popular in both cognitive science and NLP. Furthermore, in all cases we report comparisons against state of the art wordbased models and show that the additional processing cost incurred by syntax-based models is worthwhile. 3. A General Framework for Semantic Space Models Once we move away from words as the basic context unit, the issue of representation of syntactic information becomes pertinent. An ideal syntactic formalism should abstract over surface word order, mirror semantic relationships as closely as possible, and incorporate word-based information in addition to syntactic analysis. It should be also applicable to different languages. These requirements point towards dependency grammar, which can be considered as an intermediate layer between surface syntax and semantics. More formally, dependency relations are asymmetric binary relationships between a head and a modiﬁer (Tesnière 1959). The structure of a sentence is analyzed as a directed graph whose nodes correspond to words. The graph’s edges correspond to dependency relationships and each edge is labeled with a speciﬁc relationship type (e.g., subject, object). The dependency analysis for the sentence A lorry might carry sweet apples is given in Figure 4. On the left, the sentence is represented as a graph. The sentence head is the main verb carry which is modiﬁed by its subject lorry, its object apples and the auxiliary might. The subject and object are modiﬁed respectively by a determiner (a) and an adjective (sweet). On the right side of Figure 4, an adjacency matrix notation is used. Edges in the graph are represented as triples of a dependent word (e.g., lorry), a dependency label (e.g., [N, subj, V]), and a head word (e.g., carry). The dependency label consists of the part of speech of the modiﬁer (capitalized, e.g., N) , the dependency relation itself (in lower case, e.g., subj), and the part of speech of the head (also capitalized, e.g., V). It is combinations of dependencies like the ones in Figure 4 that will form the context over which the semantic space will be constructed. We base our discussion  Figure 4 A dependency analysis of the sentence A lorry might carry sweet apples as parse tree (left) and set of head-relation-modiﬁer triples (right). 167  Computational Linguistics  Volume 33, Number 2  and experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin 1998a, 2001). However, there is nothing inherent in our formalization that restricts us to this particular parser. Any other parser with broadly similar dependency output (e.g., Briscoe and Carroll 2002) could serve our purposes. In the remainder of this section, we ﬁrst give a non-technical description of our algorithm for the construction of semantic spaces. Then, we proceed to discuss each construction step (context selection, basis mapping, and quantiﬁcation of co-occurrences) in more detail. Finally, we show how our framework subsumes existing models. Table 1 lists the notation we use in the rest of the article.  3.1 The Construction Algorithm Our algorithm for creating semantic space models is summarized in Figure 5. Central to the construction process is the notion of paths, namely sequences of dependency edges extracted from the dependency parse of a sentence (we deﬁne paths formally in Section 3.2). Consider again the graph in Figure 4. Besides individual edges (i.e., paths of length 1), it contains several longer paths, such as the path between lorry and sweet ( lorry, carry, apples, sweet ), the path between a and carry ( a, lorry, carry ), the path between lorry and carry ( lorry, carry ), and so forth. The usage of paths allows us to represent direct and indirect relationships between words and gives rise to three novel parameters: 1. The context selection function cont(t) determines which paths in the graph contribute towards the representation of target word t. For example, we may choose to consider only paths of length 1, or paths with length ≥ 3. The function is effectively a syntax-based generalization of the traditional “window size” parameter. 2. The path value function v assigns weights to paths, thus allowing linguistic knowledge to inﬂuence the construction of the space. For  Table 1 Summary of notation. b∈B t∈T W(t) M[t][b] ∈ R π Π Πs Πt start(π), end(π) Cat R l : Π → (Cat × R × Cat)∗ cont : T → 2Πs µ:Π→B v:Π→R A : R4 → R  Basis element Target word type Set of tokens of target type t Cell of semantic space matrix for target word t and basis element b Dependency path (in a given dependency tree) Set of all undirected paths Set of all undirected paths in sentence s Set of all undirected paths in a sentence anchored at word t First and last node of an undirected path Set of POS categories (for given parser) Set of dependency relations (for given parser) Edge (sequence) labeling function Local context selection function (subset of paths in sentence s) Basis element mapping function Path value function Lexical association function  168  Padó and Lapata  Dependency-Based Semantic Spaces  Figure 5 Algorithm for construction of semantic space. instance, it can be used to discount longer paths, or give more weight to paths containing subjects and objects as opposed to determiners or modiﬁers. 3. The basis mapping function µ creates the dimensions of the semantic space. Although paths themselves could serve as dimensions, the resulting co-occurrence matrix would be overly sparse (this is especially true for lexicalized paths whose number can become unwieldy when parsing a large corpus). For this reason, the basis elements forming the dimensions of the space are deﬁned independently from the path construction. The basis mapping function maps paths onto basis elements by collapsing paths deemed functionally equivalent. For instance, we may consider paths carrying the same dependency relations, or paths ending in the same word, as equivalent. We thus disassociate the deﬁnition of context entities (paths) from the dimensions of the ﬁnal space (basis elements). As discussed in Section 2, the main difference among variants of semantic space models lies in the speciﬁcation of basis elements B. By treating the dependency paths as distinct from the basis elements, we obtain a general framework for vector-based models that can be parametrized for different tasks and allows for the construction of spaces with basis elements consisting of words, syntactic entities, or combinations of both. This ﬂexibility, in conjunction with the context selection and path value functions, allows our model to subsume both traditional word-based and syntax-based models (see Section 3.6 for more details). 3.2 Step 1: Building the Context The ﬁrst step in constructing a semantic space from a large collection of dependency relations is to deﬁne an appropriate syntactic context for the target words of interest. We deﬁne contexts as anchored paths, that is, paths in a dependency graph that start at a particular target word t. Our assumption is that the set of paths anchored at t is a superset of the paths that can contribute relevant distributional information about t. 169  Computational Linguistics  Volume 33, Number 2  Deﬁnition 1. The dependency parse p of a sentence s is a directed graph ps = (Vs, Es), where Es ⊆ Vs × Vs. The nodes v ∈ Vs are labeled with individual words wi. For simplicity, we use nodes and their labels interchangeably, and the set of nodes corresponds to the words of the sentence: Vs = {w1, . . . , wn}. Each edge e ∈ Es bears a label l : Es → Cat × R × Cat where Cat belongs to a set of POS tags and R to a set of dependency relations. We assume that this set is ﬁnite and parser-speciﬁc.1 We write edge labels in square brackets. [Det,det,N] and [N,subj,V] are examples for labels provided by MINIPAR (see Figure 4, right-hand side). We are now ready to deﬁne paths in our dependency graph, save one important issue: Should we conﬁne ourselves to directed paths or perhaps disregard the direction of the edges? In a dependency graph, directed paths can only capture the relationship between a head and its (potentially transitive) dependents (e.g., carry and sweet in Figure 4). This excludes informative contexts representing, for instance, the relationship between the subject and the object of a predicate (e.g., lorry and apples in Figure 4). Our intuition is therefore that directed paths would limit the context too severely. In the following, we assume undirected paths.  Deﬁnition 2. An (undirected) path π is an ordered tuple of nodes v0, . . . , vn ∈ Vs∗ for some sentence s that meets the following two constraints:  ∀ i : (vi−1, vi) ∈ Es ∨ (vi, vi−1) ∈ Es (connectedness)  ∀ i ∀ j : i = j ⇒ vi = vj  (cycle-freeness)  In the rest of the article, we use the term path as a shorthand for undirected path.  Deﬁnition 3. A path π is anchored at a word t iff start(π) = t. We write Πt ⊆ Πs for the set of all paths anchored at t in sentence s. As an example, the set of paths anchored at lorry in Figure 4 is: { lorry, carry , lorry, a , (two paths of length 1) lorry, carry, apples , lorry, carry, might , (two paths of length 2) lorry, carry, apples, sweet } (one path of length 3) Deﬁnition 4. The context selection function cont : W → 2Πt assigns to a word t a subset of the paths anchored at t. We call this subset the syntactic context of t. The context selection function allows direct control over the type of linguistic information represented in the semantic space. In traditional vector-based models, the context  
 arg max P(e | f ) = arg max P(e, f )  (1)  e  e  = arg max(P(e) × P( f | e))  (2)  e  The phrase-based translation model P( f | e) “encodes” e into f by the following steps:  1. segment e into phrases e¯1 · · · e¯I, typically with a uniform distribution over segmentations;  ∗ 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292, USA. E-mail: chiang@isi.edu. Much of the research presented here was carried out while the author was at the University of Maryland Institute for Advanced Computer Studies. Submission received: 1 May 2006; accepted for publication: 3 October 2006. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 2  2. reorder the e¯i according to some distortion model; 3. translate each of the e¯i into French phrases according to a model P( f¯ | e¯) estimated from the training data.  Other phrase-based models model the joint distribution P(e, f ) (Marcu and Wong 2002) or make P(e) and P( f | e) into features of a log-linear model (Och and Ney 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn, Och, and Marcu (2003) ﬁnd that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity. But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation:  ¾ »Æ  ©  Ô ò Ã¢  Aozhou shi yu Beihan  you bangjiao de shaoshu guojia zhiyi .  Australia is with North Korea have dipl. rels. that few countries one of .  Australia is one of the few countries that have diplomatic relations with North Korea.  If we count zhiyi (literally, ‘of-one’) as a single token, then translating this sentence correctly into English requires identifying a sequence of ﬁve word groups that need to be reversed. When we run a phrase-based system, ATS, on this sentence (using the experimental setup described herein), we get the following phrases with translations:  [Aozhou] [shi]1 [yu Beihan]2 [you] [bangjiao] [de shaoshu guojia zhiyi] [.] [Australia] [has] [dipl. rels.] [with North Korea]2 [is]1 [one of the few countries] [.] where we have used subscripts to indicate the reordering of phrases. The phrase-based model is able to order “has diplomatic relations with North Korea” correctly (using phrase reordering) and “is one of the few countries” correctly (using a combination of phrase translation and phrase reordering), but does not invert these two groups as it should. We propose a solution to these problems that does not interfere with the strengths of the phrase-based approach, but rather capitalizes on them: Because phrases are good for learning reorderings of words, we can use them to learn reorderings of phrases as well. In order to do this we need hierarchical phrases that can contain other phrases. For example, a hierarchical phrase pair that might help with the above example is  yu 1 you 2 , have 2 with 1  (3)  where 1 and 2 are placeholders for subphrases (Chiang 2005). This would capture the fact that Chinese prepositional phrases almost always modify verb phrases on the  202  Chiang  Hierarchical Phrase-Based Translation  left, whereas English prepositional phrases usually modify verb phrases on the right. Because it generalizes over possible prepositional objects and direct objects, it acts both as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably more powerful than a conventional phrase pair. Similarly, the hierarchical phrase pair  
∗ Av.Arlindo Bettio, 1000 - 03828-000, Sa˜o Paulo, Brazil. E-mail: ivandre@usp.br. ∗∗ King’s College, Meston building, Aberdeen AB24 3UE, Scotland, UK. E-mail: kvdeemte@csd.abdn.ac.uk. † King’s College, Meston building, Aberdeen AB24 3UE, Scotland, UK. E-mail: jmasthoff@csd.abdn.ac.uk. Submission received: 17 February 2004; revised submission received: 27 July 2006; accepted for publication: 7 December 2006. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 2  referring expression; we take resolution to be the identiﬁcation of the referent of the expression once its meaning has been determined. It is resolution that will take center stage in our investigation. Difﬁculty of resolution and interpretation do not always go hand in hand. Consider sentences (1a)–(1c), uttered somewhere in Brighton but not on Lewes Road. The description in (1a) is longer (and might take more time to read and interpret) than (1b), but the additional material in (1a) makes resolution easier once interpretation is successfully completed. (1a) 968 Lewes Road, Moulsecoomb area (1b) 968 Lewes Road (1c) number 968 The ﬁrst two of these descriptions refer uniquely. As for the third: Lewes Road is a long street. Supposing that other streets in Brighton do not have numbers above 900, then even (1c) is a unique description—but a pretty useless one, because it does not help you to ﬁnd the house unless your knowledge of Brighton is exceptional. We will explore how a natural-language-generation (NLG) program should make use of logically redundant properties so as to simplify resolution (i.e., the identiﬁcation of the referent). When we write about identifying or “ﬁnding” the referent of a referring expression, we mean this in the sense of determining which object is the intended referent. This conceptual goal may or may not require the hearer to make a physical effort, for example by turning the pages of a book, or more dramatically by walking and waiting for trafﬁc lights. The fact that referring expressions tend to contain logically redundant information has been observed in many empirical studies. Levelt (1989), for example, mentions the need for redundancy in situations of “degraded communication” (e.g., background noise); and even in normal situations, redundant nondiscriminating information can help the addressee identify the referent (Deutsch 1976; Mangold 1986; Sonnenschein 1982, 1984; Arts 2004). In Levelt’s words, psycholinguistic experiments show that [l]isteners apparently create a ‘gestalt’ of the object for which they have to search. It is harder to search for ‘something red’ than for ‘a big red bird’, even if the color would be sufﬁciently discriminating. Information about the kind of object to be looked for (e.g., a bird) is especially helpful for constructing such a gestalt. (Levelt 1989, page 131) Although early GRE algorithms have often followed the Gricean maxim, “be brief” (Grice 1975), by minimizing the number of properties in a generated description, Dale and Reiter (1995) proposed an algorithm that allows certain redundancies, for example, by guaranteeing that each generated description expresses the ontological “type” of the referent, in the form of a noun, a move that addresses Levelt’s claim to some extent.1 In corpus-based studies, it has been shown that logically redundant properties tend to be included when their inclusion fulﬁls one of a number of pragmatic functions, such as to indicate that a property is of particular importance to the speaker (i.e., it constitutes one of her reasons for being interested in the referent) or to highlight the 
Computational Linguistics  Volume 33, Number 2  starting from a coarser-grained level that humans and systems can more reliably discriminate. Once we are closer to 100% accuracy, we can then see if this improves a given NLP application, rather than the somewhat futile situation we have at present where system performance is too poor to be sure if WSD is beneﬁcial to an end application. The issue remains of which word senses to use. Ide and Wilks discuss using evidence for distinctions from cross-linguistic and psycholinguistic studies or the etymology distinctions captured by lexicographers as homonyms. Chapter 4, by Palmer, Ng, and Dang, is essential reading for those needing to get to grips with the standard evaluation data sets, particularly those created for the SENSEVAL exercises. They provide thorough descriptions of the procedures involved in producing the data sets for SENSEVAL-1 and -2 and a brief overview of SENSEVAL-3 (the book was written around the time of the latter). There are details on the range of tasks in these evaluation exercises, and the core methodology is described speciﬁcally with respect to English. There is a bias towards description of the English all-words and verbal lexical sample tasks due to the role of the ﬁrst and third authors in the construction of these resources. The methods for producing the coarse-grained groups that were used for scoring the SENSEVAL-2 verbs are clearly and succinctly described. The involvement of Palmer and Dang in the construction of these resources (Palmer, Dang, and Fellbaum 2007) makes them well qualiﬁed to provide a very useful overview of this valuable work on verb classes. The methods for producing coarse senses for other parts of speech (PoS) are unfortunately not available in the SENSEVAL-2 literature. The bulk of the book, Chapters 5–10, is devoted to WSD techniques. As the editors point out, there is inevitably some overlap. Classiﬁcation of approaches and division of the material is not straightforward because topics are very much interleaved and the ﬁeld abounds with hybrid systems. Some duplication could perhaps have been avoided, for example the repetition of factual material on data sets and evaluations, but the overlap is not signiﬁcant and some is beneﬁcial because it makes each chapter self-contained, reinforces important issues, and provides different perspectives on the same material. Chapters 5–7 describe WSD methods according to three categories: knowledgebased, unsupervised, and supervised. In Chapter 5, Mihalcea gives a clear exposition of methods that use “knowledge” for disambiguation. This knowledge is typically information coded manually in a given inventory (often WordNet), though the term “knowledge-based” is also used here and elsewhere for hybrid approaches that use the predeﬁned information to structure knowledge acquired from corpus data. Heuristics are included in this chapter, some of which rely on sense-tagged data, so they might well have been placed with the supervised methods in Chapter 7. The division between unsupervised and supervised systems in Chapters 6 and 7 captures the difference between systems that distinguish senses according to evidence from raw data and those that make distinctions according to a predeﬁned inventory. This is a very important distinction considering the issues with predeﬁned inventories raised by Kilgarriff. In the WSD community, however, there is ambiguity in the term unsupervised, which is widely used in the WSD literature, and particularly the SENSEVAL proceedings, for systems that do not use hand-tagged data (even though they use predeﬁned senses). The reader is warned of the ambiguity in many places but because the ambiguity is present in the book from the Introduction, it might have been worth making this ambiguity explicit from the start. In Chapter 6, Pedersen reserves the terminology for systems that discover senses automatically from data. So far such work has focused on sense discovery, with a few exceptions, such as Schu¨ tze (1998), who applied induced senses to the task of disambiguation and information retrieval. The 256  Book Reviews full potential of these unsupervised systems has yet to be realized because evaluation on standard data sets necessitates mapping from the induced classes to whichever predeﬁned inventory was used in the creation of the data set (Agirre et al. 2006). When evaluation can be performed on a task that doesn’t prescribe a particular set of labels, these methods should beneﬁt from renewed interest. Chapter 7 is the longest chapter in the book, which reﬂects the predominance of work on supervised methods that use both predeﬁned senses and hand-tagged data. The availability of standard data sets has made systematic comparison possible; however, such comparison is difﬁcult because so many factors are involved and typically only a few parameters are considered by any one study. In this respect, Chapters 7 and 8 contain some very useful analysis. In Chapter 7, Ma´rquez, Escudero, Mart´ınez, and Rigau provide an experimental comparison of some of the supervised learning algorithms on the DSO corpus. The results are dependent on the combination of algorithm, features, and evaluation data set and because the differences are small it is hard to determine deﬁnitive winners. The comparison is followed by an excellent section explaining why systems are hitting ceilings on performance and expanding on some possibilities that have been advocated for tackling the problems. In Chapter 8, Agirre and Stevenson categorize the various knowledge sources that feed into WSD systems, identify these sources in existing WSD systems, and collate ﬁndings on the beneﬁts of the various categories from a number of comparative evaluations in the literature. They provide a useful set of observations on the contribution of the knowledge sources described in the chapter. Again it is apparent that although there are general trends to be found—for example that verbs may do better with speciﬁc knowledge sources, such as subcategorization, and discriminative approaches—there is no “one size ﬁts all” even across a given PoS and the interaction between features and algorithms makes the exploration difﬁcult. There is plenty of motivation for combining knowledge sources to get optimum results, as no one knowledge source or algorithm is a panacea; however, work is clearly needed to isolate components and to determine what works well when. The discussion in Chapter 8 is a useful step in that direction. In Chapter 9, Gonzalo and Verdejo examine how knowledge sources needed for WSD can be acquired automatically. They look at automatic acquisition of topical knowledge about word senses and also pick up on the thread from the end of Chapter 7 on trying to provide supervised systems with sufﬁcient sense-tagged examples using cross-lingual resources and information gleaned from the Web. They highlight research (Agirre and Mart´ınez 2004) demonstrating the importance of determining the right sampling bias when using Web data. Another approach has been to exploit the Web community for voluntary labor in annotation tasks. Web directories also show potential for ﬁnding valuable domain-speciﬁc information, which is championed in Chapter 10 as a crucial input for WSD. Chapter 10 is a practical chapter on the importance of domains, subjects, and topics. Buitelaar, Magnini, Strapparava, and Vossen ask whether domains are necessary for WSD and whether they are the whole story. The evidence suggests that the answer is somewhere between these two viewpoints. There is no doubt that for many words, domain information is important, though the high percentage of “factotum” (i.e., domain-independent) words implies it cannot be the whole answer. The importance of the domain issue depends on the purpose of WSD. If one were using WSD output for semantic analysis of generic text, then domain issues would not be as signiﬁcant compared to a cross-lingual task operating on domain-speciﬁc text. The chapter ends 257 
 Volume 33, Number 2  In Part IV, Feldman explains how systems can be described at different levels of abstraction. A description at the computational level involves role-ﬁller (or feature-value) structures and rules for manipulating these. The reader is regularly reminded that this is just a higher-level description of a connectionist, neural structure. Nevertheless, it remains unclear how exactly such a computational system can be implemented in a neural network, especially as the models’ complexity increases in the later chapters of the book. Part IV also introduces the notion of conceptual schemas. These are cognitive structures that emerge from the organization of our bodies and of the world, and are therefore universal across cultures. Examples are basic concepts such as “grasp” and “support.” Although the conceptual schemas themselves are universal, the mapping between words and schemas differs across languages. For instance, there is no one-toone translation between the spatial prepositions of different languages. The acquisition of words denoting spatial relations in different languages is simulated by Regier’s (1996) connectionist model, which is the ﬁrst speciﬁc model discussed in the book. Part V discusses the acquisition of words for actions. Knowledge of particular actions is encoded as execution schemas for controlling motor behavior. People are not aware of complete actions, but only of particular parameters that can be set in the execution schema—for instance, the action’s speed, direction, and duration. Learning the meaning of a verb comes down to associating the word with the corresponding parameters of the execution schema. These ideas are implemented in Bailey’s (1997) model of verb learning, presented at the end of this part of the book. Part VI explains how abstract expressions are understood through metaphor. When a metaphor is used, words in the abstract domain are understood by using words and knowledge from more-concrete domains. Primary metaphors are grounded directly in perception and action. For instance, the expression a warm greeting makes use of primary metaphor by talking about affection in terms of temperature. Complex metaphors are conceptual combinations of primary metaphors. One of the most important complex metaphors is used for describing the structure of events, and combines mappings such as “causes are physical forces,” “states are locations,” and “changes are movements.” Understanding a sentence, so it is claimed in Part VI, is not akin to logical inference, but comes down to mentally simulating the event or situation described by the sentence (after metaphorically mapping to a more-concrete domain, if necessary). In support of this view, the book gives evidence from brain-imaging experiments but, oddly, ignores compelling behavioral ﬁndings by, for instance, Glenberg and Kaschak (2003), Stanﬁeld and Zwaan (2001), and Zwaan, Stanﬁeld, and Yaxley (2002). In Part VII, this idea of mental simulation is put into computational form by presenting Narayanan’s (1997) model of story comprehension. In this model, background knowledge is implemented in temporal belief (Bayes) networks. These perform probabilistic inference by means of a process of parallel multiple constraint satisfaction, which is quite plausible from a neural perspective. Again, metaphorical or abstract language is understood by mapping to a more-concrete domain. As an example, it is shown how the model would process an excerpt from a newspaper article about economics. It isn’t until Part VIII that grammar begins to play a role, reﬂecting that the neural theory of language relies less on the syntactic structure of sentences than do more traditional theories of language comprehension. According to the proposed theory of grammar (based on Bergen and Chang’s [2005] Embodied Construction Grammar), the basic element of linguistic knowledge is the construction: a pairing of linguistic form and meaning. By applying linguistic and conceptual knowledge, an utterance (in the context of a situation) is transformed into a network of conceptual schemas that  260  Book Reviews  speciﬁes the meaning of the sentence. “The job of grammar is to specify which semantic schemas are being evoked, how they are parameterized, and how they are linked together in the semantic speciﬁcation. The complete embodied meaning comes from enacting or imagining the content . . . and always depends on the context in addition to the utterance” (p. 288). This is clariﬁed by a complete and detailed analysis of the simple sentence Harry strolled into Berkeley. The book’s ﬁnal part presents a model (Chang 2006) that simulates how constructions are learned by associating an utterance (a linguistic form) to its meaning as inferred from the situational context. To conclude, From Molecule to Metaphor is a good introduction to embodied theories of language and computational modeling in cognitive linguistics. It discusses a wide range of issues, many of which were not included in this review. The view that language should not be described as a mathematical or logical system but is foremost a part of human behavior and a function of the brain could well be new and thought-provoking to readers of this journal. However, the same readers are also likely to desire a high level of computational detail which the book does not provide, as it does not present any formal speciﬁcation of the discussed models. 
 Volume 33, Number 2  models, for they make a closed-world assumption in which anything that is not forced to happen does not happen. All of this leads naturally to what might be called a “dynamic” account of meaning. But the authors distance themselves both from DRT (Kamp and Reyle 1993) (because of its reliance on Davidson-style events with predicates corresponding to thematic roles) and from Amsterdam-style dynamic semantics (Groenendijk and Stokhof 1991) (which they view as treating computation implicitly rather than explicitly). Although isolated elements of the approach just sketched may have the ring of familiarity, the details are all-important. For it is the painstaking combination of these choices that gives their approach its individuality, and the details are always chosen with one end in mind: cashing out their fundamental conviction that planning and causality are crucial to the cognitive construction of time. Part III of the book (which, at around 160 pages, is by far the longest section) puts this apparatus to work to construct a theory of tense and aspect. Every VP is associated with a default scenario (that is, a microtheory) that determines the Aktionsart of the verb. The word “default” is important: temporal and aspectual operators, and many other linguistic items, may coerce the verb to assume a different Aktionsart. And the underlying machinery accommodates this easily. In Chapter 10, for example, Lambalgen and Hamm are able to give a simple treatment of the imperfective paradox, and in Chapter 11 they treat a wide range of coercive phenomena in more detail. Once again, this review only hints at the ground covered. For example, there is a chapter devoted to the passe´ simple and imparfait in French, and another that treats nominalization. This is an interesting book. It is not particularly easy to read, and not merely because some parts (notably Part II) are technically dense (indeed, readers with some logical background are likely to ﬁnd Part II the most straightforward). The authors are faced with a difﬁcult presentational task: They have lots of ideas, and all are intended to serve one intellectual end—and at times the strain of merging the various themes shows. Chapter 1, which sets out the underlying idea that time is cognitively constructed, will probably irritate some readers: It sketches the leading idea in a rather rudimentary fashion, yet subsequent chapters act as if all the points it mentions have been ﬁrmly established; they have, at best, been indicated. Many chapters show signs of hasty writing (accompanied by all-too-frequent asides that limitations of space make more detailed exposition impossible). Moreover, although there are signs that the authors wanted to make the book pedagogically attractive (some chapters contain exercises, and there is an appendix discussing the basic ideas of constraint logic programming), important ideas are sometimes used before they have been explained. I would certainly think twice (and prepare very carefully) before unleashing this book on a class. And yet the book works, and indeed in a curious way these (relatively minor) faults highlight its good points. For it is impossible to read this book without being impressed by the determination of the authors to get their message across; their occasional lapses enhance this sense of urgency. Taken as a whole, the book is an impressive achievement. It will certainly prove of interest to theoretical linguists, philosophers, and cognitive scientists. And, yes, it should prove of interest to computational linguists and other NLP practitioners too. As should be clear from the review, this book treats temporal and aspectual phenomena from a perspective very different from that of current corpusbased work. But it does so systematically and with great precision. Although it is unlikely that every detail will survive the test of time, the general themes it so rigorously explores may well help provide structure and coherence to more empirically based approaches. Interested in temporal semantics? Then this is essential reading.  264  Book Reviews  References Groenendijk, Jeroen and Martin Stokhof. 1991. Dynamic predicate logic. Linguistics and Philosophy, 14(1):39–100. Kamp, Hans and Uwe Reyle. 1993. From Discourse to Logic. Kluwer, Dordrecht. Moschovakis, Yiannis. 1993. Sense and denotation as algorithm and value. In Juha  
Volume 33, Number 2  the process of obtaining such a license and provide a detailed map of it on some easily accessible Web site. As it stands, the barriers to building the full spoken dialog systems detailed in the book are higher than they should be. The second half of the book goes under the hood to examine, more generally, the issues involved in making the authors’ approach work. The chapter on compilation of feature grammars into context-free grammars looks in depth at alternatives to exhaustive expansion, including efﬁcient ﬁltering techniques and grammar transformation. There is also a chapter on developing an English feature grammar speciﬁcally for integration with a speech recognition system. A third chapter presents the authors’ approach for adapting the general English feature grammar to a particular domain, which they term grammar specialization. In effect, they induce a new feature grammar by transforming—ﬂattening, to a greater or lesser extent—an automatically produced treebank of domain-speciﬁc text and extracting new rules from the treebank, as well as estimating production probabilities from the corpus. A subsequent chapter investigates the impact of various parameterizations on this grammar specialization approach— for example, the degree of ﬂattening—thus establishing, at least for the applications presented, some best practices. A ﬁnal chapter presents a comparison of their grammarbased approach with a class-based n-gram language model. This half of the book will be more enjoyable for readers of this journal, who are presumably interested in more general questions of computation and language than the step-by-step tutorial format of the ﬁrst half of the book. The details of the approach are interesting, particularly the insights about how to build linguistically rich grammars that can be effectively compiled into high-utility context-free grammars for speech recognition. The primary shortcoming of this presentation lies in perpetuating the false dichotomy between “grammar-based” and “data-driven” approaches to language modeling for speech recognition, which motivates the ﬁnal chapter of the book. In fact, the authors’ approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang, Stolcke, and Harper 2004, among others), and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the ﬁeld over the past decade, and some understanding of the impact of ﬂattening trees can be had in Johnson (1998), where the beneﬁcial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models. Perhaps this is worth demonstrating, but the chapter couches the results within the context of a clash between paradigms, which simply does not ring true. This one misstep, however, does not detract from the quality of the authors’ system, nor from the interesting presentation of too-often-ignored aspects of spoken language systems engineering. The book and the toolkit it describes can serve a very useful pedagogical purpose by providing a foundation upon which students and researchers  272  Book Reviews  can build spoken dialog applications. For those interested in constrained spoken dialog systems, there is much of interest in this book.  References Charniak, Eugene. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 116–123, Toulouse. Chelba, Ciprian and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332. Johnson, Mark. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):617–636.  Roark, Brian. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276. Wang, Wen,Andreas Stolcke, and Mary P. Harper. 2004. The use of a linguistically motivated language model in conversational speech recognition. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 1, pages 261–264, Montreal.  Brian Roark is an Assistant Professor in the Department of Computer Science & Electrical Engineering and the Center for Spoken Language Understanding at the OGI School of Science & Engineering of Oregon Health & Science University (OHSU). Before joining OHSU in 2004, he spent 3 years at AT&T Labs–Research. His research focuses on syntactic parsing of speech and text, and language modeling for speech recognition. With Richard Sproat, he is the author of Computational Approaches to Syntax and Morphology (Oxford University Press, forthcoming). 
 Volume 33, Number 2  to the proposed hypothesis or experimental results. Inkpen, Feiguina, and Hirst explore the possibility of changing affect in language generation, presenting the same content in positive or negative terms. The Applications section of the book starts with two investigations of the use of Systemic Functional Grammar, one for assessing interpersonal distance in text and the other for identifying genre distinctions. The next two articles deal with the argumentative structure of text. Argumentative structure is used here for critiquing scientiﬁc abstracts by novice writers. DiMarco et al. examine the role of hedging in citation contexts in scientiﬁc papers, work that is closely related to Teufel’s paper in the previous section. Two articles in this section examine machine-learning techniques: for feature selection and for feature generalization. The latter is performed in the joint extraction of topics and their associated opinions, using WordNet as a resource to identify synonymy and hypernymy. Tong and Yager’s work addresses the summarization of temporal trends in opinions using Internet data sources such as blogs and newsgroups. The remaining papers deal with the summarization of viewpoints, and with creating labeled data for good versus bad news from the reaction of ﬁnancial markets to company news. It should be clear from this quick overview that there is no shortage of perspectives on the topic of affect and attitude in text in this collection. Although this breadth may seem at ﬁrst glance to pull in too many diverse directions, the reader should keep in mind that it is exactly this diversity of perspectives that is the hallmark of a successful ﬁrst meeting dedicated to the topic. No one will come away from a reading of this volume with a narrow and simplistic view of the problem, and it would be surprising if the papers did not spark additional ideas in the interested reader. The volume could have beneﬁtted, however, from some more thorough editorial organization to bring more structure into the collection. The categorization into three main topics is not indicated in the table of contents; it seems to be more of a hasty afterthought. For many papers, it is not clear at all why they are part of one section rather than another. A more thorough introduction could have helped to categorize the papers along various possible dimensions. These dimensions could include features and resources used (words, part-of-speech tags, deeper linguistic analysis, semantic resources), type of attitude/affect computed (simple polarity, trending, interpersonal distance, argumentative structure, certainty, multiple viewpoints, hedging, etc.), and type of data (product review texts, news, blogs/newsgroups, scientiﬁc writing, transcribed dialogues). In summary, this volume should become an indispensable resource for anyone interested in this area. Whether the reader is more interested in the computational or the linguistic aspects of the problem—or even just the range of possible applications—this collection will broaden the perspective on the issue. For readers with no background in sentiment detection the volume can serve as an initial overview of the ﬁeld, with the caveat that they would have to do some additional reading of both earlier seminal work such as Hatzivassiloglou and McKeown (1997), Wiebe (2000), Turney (2002), and Pang, Lee, and Vaithyanathan (2002), and some more recent developments (as can be found in recent proceedings of COLING, ACL, and EMNLP).  References Hatzivassiloglou, Vasileios and Kathleen McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational  Linguistics and 8th conference of the European Chapter of the Association for Computational Linguistics, pages 174–181, Madrid. Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?  276  Book Reviews  Sentiment classiﬁcation using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 79–86, Philadelphia. Turney, Peter. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classiﬁcation of reviews. In Proceedings of the 40th  Annual Meeting of the Association for Computational Linguistics, pages 417–424, Philadelphia. Wiebe, Janyce. 2000. Learning subjective adjectives from corpora. In Proceedings of the 17th National Conference on Artiﬁcial Intelligence and 12th Conference on Innovative Application of Artiﬁcial Intelligence, pages 735–740, Austin.  Michael Gamon has been a member of the Natural Language Processing group of Microsoft Research since 1996. His publications include research on sentiment detection and mining for sentiment terminology. He organized the Workshop on Sentiment and Subjectivity in Text at COLING/ACL 2006 in Sydney. Gamon’s e-mail address is: mgamon@microsoft.com; URL: 
© 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 2  Table 1 Percentage of citations to journal papers in different areas, from papers in Computational Linguistics (raw number of cites in parentheses). Recent cites means papers published within the past ten years.  All cites  Recent cites  Area  (2005) (1995) (2005) (1995) % (n) % (n) % (n) % (n)  Computational Linguistics other NLP and speech other artiﬁcial intelligence human-computer interaction other computer science linguistics psychology and psycholinguistics mathematics and statistics other  37 (56) 16 (25) 12 (18) 0 (0) 11 (17) 8 (13) 1 (2) 10 (15) 5 (7)  26 (50) 9 (17) 
Many annotation projects have shown that the quality of manual annotations often is not as good as would be desirable for reliable data analysis. Identifying the main sources responsible for poor annotation quality must thus be a major concern. Generalizability theory is a valuable tool for this purpose, because it allows for the differentiation and detailed analysis of factors that inﬂuence annotation quality. In this article we will present basic concepts of Generalizability Theory and give an example for its application based on published data. 1. Introduction Manual annotations are still a major source of information in many small- and largescale projects in diverse areas of corpus and computational linguistics. Often, however, manual annotations are not reliable enough for a given application. Measures must be taken to increase and secure the consistency of linguistic annotations, if analyses and applications are not to suffer from low data quality. Because a multitude of factors may be responsible for inadequate reliability, a method is needed that is able to simultaneously consider a variety of probable factors and indicate those that are mainly responsible for low reliability in a given case. Generalizability Theory, or G-Theory (Cronbach et al. 1972), is a methodological framework speciﬁcally designed for this purpose. Because it is not restricted to any type of data or study design, it can be of great use in any kind of manual annotation project that needs to systematically identify sources of annotator disagreement. In this article we provide an outline of the approach and its basic assumptions and demonstrate its application based on an annotation study done by Shriberg and Lof (1991).1  ∗ Department of Psychology, Tulsa Graduate College, 4502 East 41st Street, Tulsa, OK 74135, USA. † Chair of Psychology, especially Organizational and Social Psychology (Prof. Dr. Moser), Lange Gasse 20, 90403 Nuremberg, Germany. 1 A more comprehensive introduction to G-Theory is provided in a longer version of this article, which is available from the authors. The work for this article was done at the Department for Applied and Computational Linguistics, Justus-Liebig-University Giessen, Germany. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 1  2. The G-Theory Approach Reliability in G-Theory is deﬁned by the amount of variation or variance observed in annotations; the lower the total variance in the data, the higher is its reliability. G-Theory further assumes that data reliability is inﬂuenced by several independent factors or facets, which are, individually as well as in interaction, responsible for the observed variation.2 Sources of variation might be idiosyncratic behaviors of individual annotators or external inﬂuences like alterations in the tools used for annotations, increasing time pressure, removal or adding of rewards, or changes in the annotation scheme. Each of these inﬂuences can lead to systematic changes in an annotator’s behavior and so to higher disagreement among annotators. According to G-Theory each possible facet, annotator, tools, rewards, and so on, will have its own independent impact on the quality, that is, reliability, of annotations. The task of a G-study is to isolate the inﬂuence of single facets and determine the degree of their impact. 2.1 Basic G-Study Designs The main distinction with respect to G-study designs is the choice between a crossed and a (partially) nested design. In crossed designs measurements are obtained for each possible combination of facet values. Given two facets, items and coders, each individual item (phrase, phone, gesture, etc.) is annotated by all possible coders, so that each value of the item facet is measured on every value of the coders facet. Nested designs, in contrast, only measure a subset of possible combinations of facet values, for instance, when limited resources determine that only some of the coders annotate the same objects on more than one occasion. In general, fully crossed designs require a higher number of observations, but also provide more information. To obtain a full picture of possible inﬂuences crossed designs should therefore be preferred. For a detailed discussion of G-study designs, including unbalanced designs or missing data, and random and ﬁxed facets, see, for example, Brennan (2001). 2.2 Estimating Variance Components In fully crossed designs the total variance in the data is a result of individual facets as well as their interactions. Because G-Theory assumes independence of facets, effects of components are additive. Given three facets a, b, c, the total variance σ2(Xabc) therefore is calculated as  σ2(Xabc ) = σ2a + σ2b + σ2c + σ2ab + σ2ac + σ2bc + σ2abc,e  (1)  where σ2 refers to variance and the subscripts to the name of one or more facets. The subscript e in the last variance component denotes error variance. In nested designs some facets cannot be determined as independent terms due to their confounding with other facets. For instance, in a nested design with three factors a, b, c, different  2 This deﬁnition of reliability differs from the traditional true-score-model of classical reliability theory (Spearman 1904) and can be considered a modern approach to the question of consistency or ’dependability’ of data measurement. A discussion of the conceptual differences is beyond the scope of this article. Information on this topic may be found in Thompson (2002) or Matt (2001). 4  Bayerl and Paul  Identifying Sources of Disagreement  values of c may be associated with different values of b. Here the effect for c will be confounded both with bc and the residual term abc,e so that no independent term for the c facet can be obtained. Instead of the seven variance components in the crossed design only ﬁve variance components can be calculated, again stressing the fact that nested designs provide less information than fully crossed designs  σ2(Xabc ) = σ2a + σ2b + σ2ab + σ2c,cb + σ2ac,abc,e  (2)  For information on the mathematical foundation of G-Theory and the derivation of estimates see Cronbach et al. (1972) and Brennan (2001). 2.3 Interpreting Variance Components Based on the assumption that the total variance is a sum of single variance components, the total variance is 100%. The relative magnitude of each component with respect to the total variance is an indicator of the individual contribution of this component with respect to overall (un)reliability. A facet explaining 60% of the total variance would thus be considered a major source of variation in contrast to a minor facet explaining only 5% of the variance. For instance, given that the coder facet is the largest facet, variation can be explained through systematic differences in the annotation behavior of individual coders—for example, annotators differ in their tendency to set prosodic boundaries in utterances leading to systematic differences in the number of boundaries placed. In this case retraining of annotators to reach a more comparable behavior would be advisable. A high schema component indicates that there is systematic variation in the use of categories, whereas a high coder–schema interaction indicates systematic differences in annotators’ use of these categories; for example, coders annotating rhetorical (RST) relations could differ in the frequency with which they use individual relations such as ’background’, ’concession’, ’evidence’, and so forth, pointing to possible problems with the interpretation of rhetorical relations and their application. Variation mainly due to the item facet indicates that certain materials are harder to annotate than others. Such a result would imply retraining or elimination of overly difﬁcult material. In consequence, the identiﬁcation of distinct sources of variation should lead to speciﬁcally designed steps for improvement.  3. A Re-Analysis of Shriberg and Lof (1991) As an illustration for the application of G-Theory we reanalyzed data provided by Shriberg and Lof (1991), who studied the accuracy of broad and narrow phonetic transcriptions. In Set A of their study they investigated four facets: annotation scheme (type of consonant, C), granularity (broad vs. narrow transcription G), material (continuous speech vs. articulation test, M), and annotation team (T). Data in Set A were given as agreement percentages. Our G-study results are shown in Table 1. Traditionally, reliability concerns focus on disagreements among individual annotators assuming that variation is due to incommensurable annotator behavior. In our case, however, the team facet explains only a very small percentage of variance both as an individual factor and in interaction with other factors. This suggests that the four annotation teams are comparable in their annotation quality. The major factors responsible for the observed variance are granularity and type of consonants. Material 5  Computational Linguistics  Volume 33, Number 1  Table 1 G-Study results for Shriberg and Lof (1991), Table 8, Set A.  Effect  df  Variance  Percentage of  components estimates total variance  Consonant (C) 23 234.86877  Granularity (G) 1 312.80278  Team (T)  3 3.70906  Material (M)  
1. Introduction The work presented in this article deals with conﬁdence estimation for machine translation (MT). Because sentences generated by a machine translation system are often incorrect but may contain correct substrings, a method for identifying these correct substrings and ﬁnding possible errors is desirable. For this purpose, each word in the generated target sentence is assigned a value expressing the conﬁdence that it is correct. Conﬁdence measures have been extensively studied for speech recognition. Only recently have researchers started to investigate conﬁdence measures for machine translation (Gandrabur and Foster 2003; Uefﬁng, Macherey, and Ney 2003; Blatz et al. 2004; Quirk 2004). In this article, we will develop a sound theoretical framework for  ∗ Now at National Research Council Canada, Interactive Language Technologies Group, Gatineau, Que´bec J8P 3G5, Canada. E-mail: nicola.uefﬁng@nrc.gc.ca. † Lehrstuhl fu¨ r Informatik VI, Computer Science Department, D-52056 Aachen, Germany. E-mail: ney@cs.rwth-aachen.de. Submission received: 7 March 2006; revised submission received: 30 September 2006; accepted for publication: 3 October 2006. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 1  calculating and evaluating word conﬁdence measures. Possible applications of conﬁdence measures include: r marking words with low conﬁdence as potential errors for post-editing r improving translation prediction accuracy in TransType-style interactive machine translation (Gandrabur and Foster 2003; Uefﬁng and Ney 2005a) r combining output from different machine translation systems: Hypotheses with low conﬁdence can be discarded before selecting one of the system translations (Akiba et al. 2004), or the word conﬁdence scores can be used in the generation of new hypotheses from the output of different systems (Jayaraman and Lavie 2005), or the sentence conﬁdence value can be employed for reranking (Blatz et al. 2003). The article is organized as follows: In Section 2, we brieﬂy review the statistical approach to machine translation. The phrase-based translation system, which serves as the basis for one of the direct conﬁdence measures, will be presented. Section 3 gives an overview of related work on conﬁdence estimation for machine translation. Moreover, word posterior probabilities will be introduced, and we will explain how they can be used as word-level conﬁdence measures. In Section 4, we describe so-called systembased methods for conﬁdence estimation, which make use of the output of a statistical machine translation system, such as word graphs or N-best lists. In Section 5, we present conﬁdence measures based on direct models. The combination of several conﬁdence measures into one is described in Section 6. Experimental evaluation and comparison of the different conﬁdence measures is provided in Section 7. Section 8 deals with the rescoring of translation hypotheses using conﬁdence measures. The article concludes in Section 9.  2. Statistical Machine Translation  2.1 General  In statistical machine translation (SMT), the translation is modeled as a decision process: Given a source string f1J = f1 . . . fj . . . fJ, we seek the target string eI1 = e1 . . . ei . . . eI with maximal posterior probability:  eˆI1ˆ = argmax Pr(eI1 | f1J ) = argmax Pr( f1J | eI1) · Pr(eI1)  (1)  I,eI1  I,eI1  Through this decomposition of the probability, we obtain two knowledge sources: the translation model Pr( f1J | eI1) and the language model Pr(eI1). Both can be modeled independently of each other. The translation model is responsible for linking the source string f1J and the target string eI1. It captures the semantics of the sentence. The target language model captures the well-formedness of the syntax in the target language. Nowadays, most state-of-the-art SMT systems are based on bilingual phrases (Och, Tillmann, and Ney 1999; Koehn, Och, and Marcu 2003; Tillmann 2003; Bertoldi et al. 2004; Vogel et al. 2004; Zens and Ney 2004; Chiang 2005). A more detailed description of  10  Uefﬁng and Ney  Word-Level Conﬁdence Estimation for MT  a phrase-based approach to statistical machine translation will be given in the following section.  2.2 Review of the Phrase-Based Translation System For the conﬁdence measures which will be introduced in Section 5.1, we use a stateof-the-art phrase-based translation approach as described in Zens and Ney (2004). The key elements of this translation approach are bilingual phrases. Note that these phrases are sequences of words in the two languages and not necessarily phrases in the linguistic sense. The bilingual phrases are extracted from a word-aligned bilingual training corpus. In this translation approach, the posterior probability Pr(eI1 | f1J ) is modeled directly using a weighted log-linear combination of a language model, a phrase translation model, and a word-based lexicon model. The translation models are used for both directions: p( f | e) and p(e | f ). Additionally, a word penalty and a phrase penalty are applied. With the exception of the language model, all models can be considered as within-phrase models as they depend only on a single phrase pair, but not on the context outside the phrase. In the following, we will present the generation criterion for the phrase-based translation approach. This will be done for a monotone search in order to keep the equations simple. The extension to the non-monotone case is straightforward. Let ( jK0 , iK0 ) be a segmentation of the source sentence into phrases, where jk−1 < jk and ik−1 < ik for k = 1, . . . , K. The corresponding (bilingual) phrase pairs are denoted as  ( f˜k, e˜k ) = ( fjjkk−1+1, eiikk−1+1 ), k = 1, . . . , K Assume a trigram language model. The phrase-based approach to SMT is then expressed by the following equation:  I  K  eˆI1ˆ = argmax  c1 · p(ei | eii−−12 )λ1 ·  c2 · p( f˜k | e˜k )λ2 · p(e˜k | f˜k )λ3 ·  (2)  jK0 ,iK0 ,I,eI1  i=1  k=1  jk  ik  ·  p( fj | e˜k )λ4 ·  p(ei | f˜k )λ5  j=jk−1 +1  i=ik−1 +1  where p( f˜k | e˜k) and p(e˜k | f˜k) are the phrase lexicon models in both translation directions. The phrase translation probabilities are computed as a log-linear interpolation of the relative frequencies and the IBM model 1 probability. The single word–based lexicon models are denoted as p( fj | e˜k) and p(ei | f˜k), respectively. p( fj | e˜k) is deﬁned as the IBM model 1 probability of fj over the whole phrase e˜k, and p(ei | f˜k) is the inverse model, respectively. c1 is the so-called word penalty, and c2 is the phrase penalty, assigning constant costs to each target language word/phrase. The language model is a trigram model with modiﬁed Kneser–Ney discounting and interpolation (Stolcke 2002). The search determines the target sentence and segmentation that maximize the objective function.  11  Computational Linguistics  Volume 33, Number 1  As Equation (2) shows, the sub-models are combined via weighted log-linear in- terpolation. The model scaling factors λ1, . . . , λ5 and the word and phrase penalties are optimized with respect to some evaluation criterion (Och 2003) such as BLEU score. The phrase-based translation model will be needed later to deﬁne the different conﬁdence measures. We therefore introduce the following notation: Let QPM( f˜k, e˜k) be the score of the phrase pair, which consists of the phrase penalty c2, the phrase lexicon scores, and the two word lexicon model scores (see Equation (2)):  jk  ik  QPM( f˜k, e˜k ) := c2 · p( f˜k | e˜k )λ2 · p(e˜k | f˜k )λ3 ·  p( fj | e˜k )λ4 ·  p(ei | f˜k )λ5  (3)  j=jk−1 +1  i=ik−1 +1  3. Conﬁdence Measures for MT 3.1 Related Work In many areas of natural language processing, conﬁdence measures have scarcely been investigated. The exception is automatic speech recognition, where an extensive amount of research on the topic exists. Conﬁdence measures are widely used in this area—for example, in dialogue systems and in unsupervised training. Recently, researchers have started to investigate conﬁdence measures for machine translation (Blatz et al. 2003, 2004; Gandrabur and Foster 2003; Uefﬁng, Macherey, and Ney 2003; Quirk 2004; Sanchis 2004). This section gives an overview of conﬁdence estimation for machine translation on the word level as well as the sentence level and discusses its applications. The ﬁrst work that studied conﬁdence estimation for statistical machine translation was Gandrabur and Foster (2003). Their conﬁdence measures consist of a combination of different features in a neural network. The conﬁdence is estimated for a sequence of up to four words in an interactive machine translation environment. The probability of being a correct extension of a given sentence preﬁx is computed for this word sequence. The authors report signiﬁcant improvement in quality of the predicted translations. In 2003, a team at the yearly summer workshop at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University, Baltimore, MD, developed conﬁdence measures for machine translation. The combination of several conﬁdence features using neural networks and a naive Bayes classiﬁer was investigated. The workshop team studied conﬁdence estimation on the word level as well as on the sentence level, though the focus was on the sentence level. The features applied included new features as well as those that had previously been developed by team members (Gandrabur and Foster 2003; Uefﬁng, Macherey, and Ney 2003). Among them were also some of the word posterior probabilities, which will be presented here. Additionally, heuristic and semantic features were studied. For a description of the features and results, see Blatz et al. (2003, 2004). Following the work of the summer workshop team, Quirk (2004) presented an investigation of different approaches to sentence-level conﬁdence estimation. A set of features is computed for each sentence generated by an MT system, and these features are combined using several different methods: modiﬁed linear regression, neural nets, support vector machines, and decision trees. Many of the sentence features are similar to  12  Uefﬁng and Ney  Word-Level Conﬁdence Estimation for MT  those presented in Blatz et al. (2003); the others are speciﬁc to the underlying MT system that generated the translations. Quirk (2004) also investigated the use of manually tagged data for training the conﬁdence measures. The author found that using a small amount of manually labeled training data yields better performance than using large quantities of automatically labeled data. Akiba et al. (2004) reported the application of conﬁdence measures to the selection of output on N-best lists produced by different MT systems. Word-level conﬁdence measures, namely the rank-weighted sum as described in Section 4.1 (and ﬁrst introduced in Uefﬁng, Macherey and Ney [2003]), are used to discard low-quality system output before selecting a translation from the various MT systems. Zens and Ney (2006) presented an extension of the word posterior probabilities presented in this article: Posterior probabilities are calculated not only on the word level, but also for n-grams, and are successfully applied to the rescoring of MT hypotheses. 3.2 Word Posterior Probabilities The conﬁdence of a target word can be expressed by its posterior probability, that is, the probability of the word occurring in the target sentence, given the source sentence. Word posterior probabilities are the basis of all approaches to conﬁdence estimation presented here. The following explains how they can be determined. The different methods can be classiﬁed into two categories: system-based methods, which make use of system output such as word graphs or N-best lists; and direct methods, which use external knowledge sources such as statistical word or phrase lexica. The system-based approaches derive the word posterior probability from the sentence posterior. The posterior probability of a sentence eI1 can be approximated by the joint probability p( f1J, eI1), which the statistical machine translation system assigns to a generated translation. The sentence probabilities employed in the search (see Equation (1)) are not normalized, which does not affect the result of the search. But for use in conﬁdence estimation, they need to be normalized in order to obtain a probability distribution over all target sentences (see Equation (6)). From the sentence posterior probabilities, the word posterior probabilities can be calculated by summing up the probabilities of all sentences containing the target word. For an exact quantiﬁcation of word posterior probabilities, we need to consider the following problem: How can we deﬁne a criterion for the occurrence of a word in a sentence? The answer to this question is not at all trivial. Due to ambiguities, the word position in the sentence is not ﬁxed. Sentences can have different numbers of words because of deletions and insertions. Additionally, the words can be reordered in different ways during the translation process. The posterior probability of a target word e can depend on its occurrence in position i of the target sentence, for example, or on the number of times the word is contained in the sentence. Thus, several different deﬁnitions of posterior probabilities will be introduced and investigated in the following discussion. The basic concept of calculating the posterior probability will be explained for the target word e occurring in a ﬁxed position i of the sentence. This is a rather strict and simple criterion; it will be used here mainly to illustrate the idea. Section 4 will describe several different concepts of word posterior probabilities that relax this condition. Let p( f1J, eI1) be the joint probability of source sentence f1J and target sentence eI1. Here, this is approximated by the probability that an SMT system assigns to the sentence pair (see Section 2). The word posterior probability of e occurring in position 13  Computational Linguistics  Volume 33, Number 1  i is calculated as the normalized sum of probabilities of all sentences containing e in exactly this position:  pi(e | f1J ) =  pi(e, f1J ) pi(e , f1J )  (4)  e  where pi(e, f1J ) = δ(ei, e) · p( f1J, eI1)  (5)  I,eI1  Here δ(·, ·) is the Kronecker function. The normalization term in Equation (4) is  pi(e , f1J ) = p( f1J, eI1) = p( f1J )  (6)  e  I,eI1  This deﬁnition of word posterior probabilities raises the question of how to calculate the sums over the target sentences in Equations (5) and (6). This problem can be solved by approximating the summation space via a word graph or an N-best list. The summation is then performed explicitly over all sentences given in this restricted space. In the case of an N-best list, this is straightforward because the sentences are already listed. On a word graph, the forward–backward algorithm can be applied to carry out the summation efﬁciently. In these system-based approaches, the calculation depends on the output of the SMT system that generated the translations. The sentence probabilities summed in Equations (5) and (6) are the scores assigned by the underlying SMT system. The summation space is restricted to those hypotheses that are assigned a high probability by the SMT system, and the others are not considered. The second approach to the calculation of word posterior probabilities is summation using direct models such as IBM model 1 or a phrase-based translation model. These methods do not consider the whole target sentence. The summation of probabilities is carried out over single words or phrases without context. These modelbased word posterior probabilities are independent of the system generating the translations. They do not require the MT system to assign a probability to the translation hypothesis. Thus, they can also be used for conﬁdence estimation on hypotheses from a non-statistical MT system or if only the single best translations without any scores are given.  3.3 Word Conﬁdence Measures The idea behind word-level conﬁdence estimation is to be able to detect possible errors in the output of a machine translation system. Using conﬁdence measures, individual words can be labeled as either correct or incorrect. This additional information can be used in, for example, interactive TransType-style machine translation systems (Gandrabur and Foster 2003; Uefﬁng and Ney 2005a). Two problems have to be solved in order to compute conﬁdence measures. First, suitable conﬁdence features have to be computed. Second, a binary classiﬁer has to be deﬁned, which decides whether a word is correct or not. The word posterior probabilities introduced in Section 3.2 can be interpreted as the probability of a word being correct. That is, the probability can directly be used as a conﬁdence measure. For this  14  Uefﬁng and Ney  Word-Level Conﬁdence Estimation for MT  purpose, it is compared to a threshold t. All words that have conﬁdence above this threshold are tagged as correct, and all others are tagged as incorrect, translations. Thus, the binary classiﬁer is deﬁned as     correct if p(e | f1J ) ≥ t  class(e) =  incorrect otherwise  (7)  The threshold t is optimized on a distinct development set beforehand. The question of how the correctness of a word in MT output is determined is not at all an easy one. We will address this issue in Section 7.2.  4. System-Based Conﬁdence Measures In this section, we will present conﬁdence measures that are calculated over N-best lists or word graphs generated by an SMT system. Several different models for the occurrence of a target word in a sentence will be deﬁned and experimentally evaluated. These are the models that proved most promising from a theoretical viewpoint and in the experimental evaluation: r Target word e occurs in position i of the target sentence (see Section 4.1). The calculation of word posterior probabilities over word graphs and N-best lists is explained in detail for this concept. r The word is considered if it occurs in a window around the position: i ± t, t ∈ N, for some position i (see Section 4.2). r The Levenshtein alignments between the hypothesis under consideration and all other possible translations are determined. The target word e (in some position i) is taken into account if it is Levenshtein-aligned to itself (see Section 4.3). r e is contained in the sentence at least n times, n ∈ N (see Section 4.4). Section 4.5 will treat the issue of scaling the probabilities that the SMT system assigns to the translation hypothesis. 4.1 Approach Based on the Fixed Target Position In this approach, the word posterior probability is determined for word e occurring in target position i as shown in Equation (4). This variant requires the word to occur exactly in the given position i. Hence, a probability distribution over the pairs (e, i) of target words e and positions i in the target sentence is obtained. This type of word posterior probability was ﬁrst introduced in Uefﬁng, Macherey, and Ney (2003). The concept of word posterior probabilities based on the ﬁxed target position allows for easy calculation over word graphs and N-best lists. However, this concept is rather restrictive. In practice, the target position of a word varies between different translation alternatives. The method presented here is a starting point for more ﬂexible approaches that perform summation over a window of target positions. In the following, we will show how the word posterior probabilities based on ﬁxed target positions are calculated over word graphs and over N-best lists.  15  Computational Linguistics  Volume 33, Number 1  Calculation using word graphs. A word graph represents the most promising hypotheses generated by the translation system (Uefﬁng, Och, and Ney 2002; Zens and Ney 2005). It has the advantage of being a compact representation of the translation hypothesis space, which allows for efﬁcient calculation of word posterior probabilities. A word graph is a directed acyclic graph G = (V, E) with vertex set V and edge set E. It has one designated root node n0 ∈ V, representing the beginning of the sentence. Each path through the word graph represents a translation candidate. The nodes of the graph contain information such as the set of covered source positions and the language model history. Two hypotheses can be recombined if their information is identical. Recombination is carried out during decoding to accelerate the search process. If two hypotheses represent the same information with respect to translation and language models, they will be assigned the same probabilities by these models in the future. Therefore, the outcome of the search is not altered, but the processing time can be signiﬁcantly decreased if only the more promising of the two hypotheses is considered for further expansion. If no recombination were carried out, the word graph would have the structure of a tree. The edges in the word graph are annotated with target words. Additionally, they contain weights representing the part of the probability that is assigned to each particular word as part of the target hypothesis. When multiplying the scores along a path, the probability of the corresponding hypothesis is obtained. The sentence position of a word refers to the path length in the word graph: Consider an edge (n, n ) that is annotated with word e. If a path leading from source node n0 into n has i edges, then e will be the (i + 1)-th word in the corresponding sentence. Note that due to recombination this position is not unambiguous. If two hypotheses of different lengths i and i are recombined in node n, then e will be in position i + 1 in the one resulting sentence, and in i + 1 in the other sentence. For an example of a word graph, see Figure 1. The source sentence is Wir ko¨nnen das machen!, and the reference translation is We can do that!. The leftmost node is the root node n0. The other nodes represent different states with respect to the set of covered source positions and language model history. In this example, a trigram  Figure 1 Example of forward–backward calculation on a word graph. The posterior probability of can in the second position is obtained by multiplying the total probability of all incoming paths (dashed lines) and outgoing paths (dotted), separately for the two edges, and summing the products. 16  Uefﬁng and Ney  Word-Level Conﬁdence Estimation for MT  language model is applied, that is, all paths leading into a node share the last two words.  The translation alternatives contained in this word graph represent different reorderings  of the words in the sentence: The monotone translation that do as well as the correctly  reordered sequence do that occur. Note that in order to limit the size of the graph and  keep the presentation simple, an example was chosen where all target sentences have  the same length.  The posterior probabilities of word e in position i can be computed by summing up  the probabilities of all paths in the graph that contain an edge annotated with e in posi-  tion i of the target sentence. This summation is performed efﬁciently using the forward–  backward algorithm (Jurafsky and Martin 2000). This algorithm also determines the  total probability mass that is needed for normalization, as shown in Equation (6). In  the following, we will present the exact equations for a word graph generated by the  phrase-based translation system described in Section 2.2. In such a word graph, the  ﬁrst word of a target phrase is assigned the score for the whole phrase. That is, when  translating a source phrase f˜k by a target phrase e˜k = eik−1+1 . . . eik , the full contribution  of all sub-models for this phrase is included for the ﬁrst word eik−1+1. All following  words eik−1+2 . . . eik are assigned probability 1. The forward–backward algorithm works  as  follows:  Let  QPM (  f˜k,  e˜k )  be  the  phrase  model score of a phrase pair as deﬁned in Equation (3) in Section 2.2. In order to keep  the notation simple, we assume a bigram language model. The extension to higher-order language models is straightforward. The forward probability Φi(ei ; e˜k, f˜k) of word ei is the probability of reaching word ei from the sentence start, where ei occurs in position i of the sentence. It depends on the phrase pair ( f˜k, e˜k) for which ei ∈ e˜k. Because the full score of this phrase pair is included at the ﬁrst word eik−1+1, two cases have to be distinguished in the calculation: Either ei is the ﬁrst word in the phrase, that is, i = ik−1 + 1, or ik−1 + 1 < i ≤ ik. The forward probability can be determined by summing the probabilities of all partial hypotheses of length i − 1. This allows for recursive  calculation in ascending order of i. We obtain the following formula:  Φi(ei ; e˜k, f˜k ) =    QPM( f˜k, e˜k) ·  ik  c1 · p(ei | ei −1 )λ1 ·  p(ei | eik−1 )λ1  =    Φi−1 (ei−1  ;  i e˜k,  =i+1 · f˜k−1 f˜k )  Φi−1 (eik−1  ;  e˜k−1 e˜k−1, f˜k−1 ) if  if ik−1  i= +1  ik−1 + 1 < i ≤ ik  The backward probability Ψi(ei ; e˜k, f˜k) expresses the probability of completing a sentence from the current word on. It can be determined recursively in descending order of i. Again, we distinguish two cases:  Ψi(ei ; e˜k, f˜k ) =    ik+1 c1 · p(ei | ei −1 )λ1 ·  QPM( f˜k+1, e˜k+1 ) · Ψi+1(ei+1 ; e˜k+1, f˜k+1 )  =  e˜k+1 i =i  Ψi+1(ei+1  ;  e˜k, f˜k)  f˜k+1  if i = ik if ik−1 < i < ik  17  Computational Linguistics  Volume 33, Number 1  Using the forward–backward algorithm, the word posterior probability of word e in position i is determined by combining the forward and backward probabilities of all hypotheses containing e in this position. We carry out a summation over all corresponding phrase pairs ( f˜k, e˜k). This yields  pi(e, f1J ) =  Φi(e ; e˜k, f˜k) · Ψi(e ; e˜k, f˜k)  (8)  e˜k f˜k  To obtain a posterior probability, a normalization (as shown in Equation (4)) has to be performed. The normalization term p( f1J ) := pi(e , f1J ) corresponds to the probability e mass contained in the word graph and can be calculated by summing the backward probabilities of all words that occur in the ﬁrst sentence position:  p( f1J ) =  Ψ1(e1; e˜1, f˜1)  e˜1=e1...ei1 f˜1  Figure 1 illustrates the forward–backward algorithm. Assume the word posterior probability of the word can appearing in the second position of the target sentence is to be calculated. There are two edges in the graph that contain this word in the desired target position. Thus, the probabilities of the paths leading through these edges have to be summed. The forward probabilities are the probabilities of the incoming edges, shown by dashed lines. The backward probabilities are those of the paths marked by dotted lines. They are combined (separately for each edge) and then summed to obtain the word posterior probability of can in position 2.  Calculation using N-best lists. An N-best list contains the n most promising translation hypotheses generated by the statistical machine translation system. The N-best list is extracted from a word graph. The hypotheses are sorted by their probability in descending order. This representation allows for easy computation of the sum given in Equation (5). Furthermore, the calculation of more complex variants of word posterior probabilities, such as the approach based on Levenshtein alignment (see Section 4.3), is feasible. Let enn,,I1n , n = 1, . . . , N, be the target hypotheses in the N-best list. The word posterior probabilities presented in Equation (4) are calculated by summing the sentence probabilities of all sentences containing target word e in target position i. The sentence probability p( f1J, enn,,I1n ) is given in the N-best list. The word posterior probability is then determined as  N δ(en,i, e) · p( f1J, enn,,I1n )  pi(e | f1J ) =  n=1 N δ(en,i, e ) · p( f1J, enn,,I1n )  e n=1  The normalization term in the denominator equals the probability mass contained in the N-best list.  18  Uefﬁng and Ney  Word-Level Conﬁdence Estimation for MT  Instead of the sum of probabilities, one can also determine the relative frequency or the rank-weighted frequency of a word as follows: The relative frequency of e occurring in target position i in the N-best list is computed as  N  hi(e | f1J )  :=  
Automated question answering has been a topic of research and development since the earliest AI applications. Computing power has increased since the ﬁrst such systems were developed, and the general methodology has changed from the use of hand-encoded knowledge bases about simple domains to the use of text collections as the main knowledge source over more complex domains. Still, many research issues remain. The focus of this article is on the use of restricted domains for automated question answering. The article contains a historical perspective on question answering over restricted domains and an overview of the current methods and applications used in restricted domains. A main characteristic of question answering in restricted domains is the integration of domain-speciﬁc information that is either developed for question answering or that has been developed for other purposes. We explore the main methods developed to leverage this domain-speciﬁc information. 1. Introduction There has been an interest in representing knowledge and automatically processing it from the time of the ﬁrst generation of computers. This interest has increased from the end of the 1980s to become an urgent necessity. Decisive factors in this increase of interest are an unprecedented growth in the amount of digital information available, an explosion of growth in the use of computers for communications, and the increasing number of users that have access to all this information. These circumstances have fostered research into information systems that can facilitate the localization, retrieval, and manipulation of these enormous quantities of data. Question Answering (QA) is one of these research ﬁelds. In this article, QA is deﬁned as the task whereby an automated machine (such as a computer) answers arbitrary questions formulated in natural language. QA systems are especially useful in situations in which a user needs to know a very speciﬁc piece of information and does not have the time—or just does not want—to read all the available documentation related to the search topic in order to solve the problem at hand. ∗ Division of Information and Communication Sciences, Macquarie University, New South Wales 2109, Australia. E-mail: diego@ics.mq.edu.au. † Departamento de Lenguajes y Sistemas Informa´ticos, Universidad de Alicante, Campus de San Vicente del Raspeig, Apdo. 99. Alicante, Spain. E-mail: vicedo@dlsi.ua.es. Submission received: 2 June 2006; revised submission received: 15 October 2006; accepted for publication: 23 October 2006. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 1  Research in QA has been developed from two different scientiﬁc perspectives, artiﬁcial intelligence (AI) and information retrieval (IR). Work in QA since the early stages of AI has led to systems that respond to questions using the knowledge encoded in databases as an information source. Obviously, these systems can only provide answers concerning the information previously encoded in the database. The beneﬁt of this approach is that having a conceptual model of the application domain represented in the database structure allows the use of advanced techniques such as theorem proving and deep reasoning in order to address complex information needs. Currently we are witnessing a surge of activity in the area from the perspective of IR, initiated by the Question Answering track of TREC1 in 1999 (Voorhees 2001). Since then, increasingly powerful systems have participated in TREC and other evaluation fora such as CLEF2 (Vallin et al. 2005) and NTCIR3 (Kando 2005). From this perspective, question answering focuses on ﬁnding text excerpts that contain the answer within large collections of documents. The tasks set in these conferences have molded a speciﬁc kind of question answering that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain. In other words, current research focuses on text-based, open-domain question answering. Both trends have developed in parallel and represent the opposite ends of a spectrum connecting what we might label as structured knowledge-based and free textbased question answering. Whereas structured knowledge-based QA systems are well adapted to applications managing complex queries in a very structured information environment, the kind of research developed in TREC, CLEF, and NTCIR is probably better suited to broad-purpose generic applications dealing with simple factual questions such as World Wide Web–based question answering. However, both approaches have serious disadvantages when they attempt to tackle important real applications that handle complex questions by combining domainspeciﬁc information typically expressed in different sources (structured, semistructured, unstructured, etc.) using reasoning techniques. Examples of such applications are: Interfaces to machine-readable technical manuals: Many software applications are very complex and they are accompanied by extensive documentation. A QA system that ﬁnds speciﬁc answers to a user’s question based on such documentation would be very useful. Front-ends to knowledge sources: Many disciplines and areas of human activity have their own speciﬁc knowledge sources. An example is the medical domain, which, as we shall see in this article, contains a wealth of technical information and resources that can be used for a QA system targeting this kind of information. Help desk systems in large organizations: Help desk staff in large organizations need to quickly satisfy the customer’s need for information. Although many such requests for information will be found in FAQs available to the help desk staff, there will always be requests that are unique and that require staff to have access to fast methods to ﬁnd the relevant information. End systems tailored to such staff (who can be trained) are different from QA systems designed for the end user, but they still need to leverage the organization domain.  
Recently, the focus of question-answering research has shifted away from simple factbased questions that can be answered with relatively little linguistic knowledge to “harder” questions that require ﬁne-grained text analysis, reasoning capabilities, and the ability to synthesize information from multiple sources. General purpose reasoning on anything other than superﬁcial lexical relations is exceedingly difﬁcult because there is a vast amount of world knowledge that must be encoded, either manually or automatically, to overcome the brittleness often associated with long chains of evidence. This situation poses a serious bottleneck to “advanced” question-answering systems. However, the availability of existing knowledge sources and ontologies in certain domains provides exciting opportunities to experiment with knowledge-rich approaches. How might one go about leveraging these resources effectively? How might one integrate  ∗ Department of Computer Science and Institute for Advanced Computer Studies. E-mail: demner@umd.edu. † College of Information Studies, Department of Computer Science, and Institute for Advanced Computer Studies. E-mail: jimmylin@umd.edu. Submission received: 4 July 2005; revised submission received: 7 January 2006; accepted for publication: 12 April 2006. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 1  statistical techniques to overcome the brittleness often associated with knowledgebased approaches? We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codiﬁed in the Uniﬁed Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identiﬁes concepts in free text, and SemRep (Rindﬂesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process. The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The conﬂuence of these many factors makes clinical question answering a very exciting area of research. Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985; Gorman, Ash, and Wykoff 1994; Ely et al. 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians’ questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufﬁcient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts. Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology—better information systems to provide decision support for physicians have the potential to improve the quality of health care. Our question-answering system supports the practice of evidence-based medicine (EBM), a widely accepted paradigm for medical practice that stresses the importance of evidence from patient-centered clinical research in the health care process. EBM prescribes an approach to structuring clinical information needs and identiﬁes elements (for example, the problem at hand and the interventions under consideration) that factor into the assessment of clinically relevant studies for medical practice. The foundation of our question-answering strategy is built on knowledge extractors that automatically identify these elements in MEDLINE abstracts. Using these knowledge extractors, we have developed algorithms for scoring the relevance  64  Demner-Fushman and Lin  Answering Clinical Questions  of MEDLINE citations in accordance with the principles of EBM. Our scorer is employed to rerank citations retrieved by the PubMed search engine, with the goal of bringing as many topically relevant abstracts to higher ranking positions as possible. From this reranked list of citations, our system is then able to generate textual responses that directly address physicians’ information needs. We evaluated our system with a collection of real-world clinical questions and demonstrate that our combined knowledge-based and statistical approach delivers signiﬁcantly better document retrieval and question-answering performance, compared to systems used by physicians today. This article is organized in the following manner: We start in the next section with an overview of evidence-based medicine and its basic principles. Section 3 provides an overview of MEDLINE, the bibliographic database used by our system, and PubMed, the public gateway for accessing this database. Section 4 describes our system architecture and outlines our conception of clinical question answering as “semantic uniﬁcation” between query frames and knowledge frames derived from MEDLINE citations. The knowledge extractors that underlie our approach are described in Section 5, along with intrinsic evaluations of each component. In Section 6, we detail an algorithm for scoring the relevance of MEDLINE citations with respect to structured query representations. This scoring algorithm captures the principles of EBM and uses the results of the knowledge extractors as basic features. To evaluate the performance of this citation scoring algorithm, we have gathered a corpus of real-world clinical questions. Section 7 presents results from a document reranking experiment where our EBM scores were used to rerank citations retrieved by PubMed. Section 8 provides additional details on attempts to optimize the performance of our EBM citation scoring algorithm. Answer generation, based on reranked results, is described in Section 9. Answers from our system were manually assessed by two physicians; results are presented in Section 10. Related work is discussed in Section 11, followed by future work in Section 12. Finally, we conclude in Section 13.  2. The Framework of Evidence-Based Medicine Evidence-based medicine (EBM) is a widely accepted paradigm for medical practice that involves the explicit use of current best evidence, that is, high-quality patientcentered clinical research such as reports from randomized controlled trials, in making decisions about patient care. Naturally, such evidence, as reported in the primary medical literature, must be suitably integrated with the physician’s own expertise and patient-speciﬁc factors. It is argued by many that practicing medicine in this manner leads to better patient outcomes and higher quality health care. The goal of our work is to develop question-answering techniques that complement this paradigm of medical practice. EBM offers three orthogonal facets that, when taken together, provide a framework for codifying the knowledge involved in answering clinical questions. These three complementary facets are outlined below. The ﬁrst facet describes the four main clinical tasks that physicians engage in (arranged roughly in order of prevalence): Therapy: Selecting treatments to offer a patient, taking into account effectiveness, risk, cost, and other relevant factors (includes Prevention—selecting actions to reduce the chance of a disease by identifying and modifying risk factors). 65  Computational Linguistics  Volume 33, Number 1  Diagnosis: This encompasses two primary types: Differential diagnosis: Identifying and ranking by likelihood potential diseases based on ﬁndings observed in a patient. Diagnostic test: Selecting and interpreting diagnostic tests for a patient, considering their precision, accuracy, acceptability, cost, and safety. Etiology/Harm: Identifying factors that cause a disease or condition in a patient. Prognosis: Estimating a patient’s likely course over time and anticipating likely complications. These activities represent what Ingwersen (1999) calls “work tasks.” It is important to note that they exist independently of information needs, namely, searching is not necessarily implicated in any of these activities. We are, however, interested in situations where questions arise during one of these clinical tasks—only then does the physician engage in information-seeking behavior. These activities translate into natural “search tasks.” For therapy, the search task is usually therapy selection (for example, determining which course of action is the best treatment for a disease) or prevention (for example, selecting preemptive measures with respect to a particular disease). For diagnosis, there are two different possibilities: in differential diagnosis, a physician is considering multiple hypotheses regarding what disease a patient has; in diagnostic methods selection, the clinician is attempting to ascertain the relative utility of different tests. For etiology, cause determination is the search task, and for prognosis, patient outcome prediction. Terms and the types of studies relevant to each of the four tasks have been extensively studied by the Hedges Project at the McMaster University (Haynes et al. 1994; Wilczynski, McKibbon, and Haynes 2001). The results of this research are implemented in the PubMed Clinical Queries tools, which can be used to retrieve task-speciﬁc citations (more about this in the next section). The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question. The following four components have been identiﬁed as the key elements of a question related to patient care (Richardson et al. 1995): r What is the primary problem or disease? What are the characteristics of the patient (e.g., age, gender, or co-existing conditions)? r What is the main intervention (e.g., a diagnostic test, medication, or therapeutic procedure)? r What is the main intervention compared to (e.g., no intervention, another drug, another therapeutic procedure, or a placebo)? r What is the desired effect of the intervention (e.g., cure a disease, relieve or eliminate symptoms, reduce side effects, or lower cost)? These four elements are often referenced with the mnemonic PICO, which stands for Patient/Problem, Intervention, Comparison, and Outcome. Finally, the third facet serves as a tool for appraising the strength of evidence presented in the study, that is, how much conﬁdence should a physician have in the results? Several taxonomies for appraising the strength of evidence based on the type and quality of the study have been developed. We chose the Strength of Recommendations Taxonomy (SORT) as the basis for determining the potential upper bound on the 66  Demner-Fushman and Lin  Answering Clinical Questions  quality of evidence, due to its emphasis on the use of patient-oriented outcomes and its attempt to unify other existing taxonomies (Ebell et al. 2004). There are three levels of recommendations according to SORT: r A-level evidence is based on consistent, good-quality patient outcome-oriented evidence presented in systematic reviews, randomized controlled clinical trials, cohort studies, and meta-analyses. r B-level evidence is inconsistent, limited-quality, patient-oriented evidence in the same types of studies. r C-level evidence is based on disease-oriented evidence or studies less rigorous than randomized controlled clinical trials, cohort studies, systematic reviews, and meta-analyses. A question-answering system designed to support the practice of evidence-based medicine must be sensitive to the multifaceted considerations that go into evaluating an abstract’s relevance to a clinical information need. It is exactly these three complementary facets that we attempt to encode in a question-answering system for clinical decision support. 3. MEDLINE and PubMed MEDLINE is a large bibliographic database maintained by the U.S. National Library of Medicine (NLM). This database is viewed by medical professionals, biomedical researchers, and many other users as the authoritative source of clinical evidence, and hence we have adopted it as the target corpus for our clinical question-answering system. MEDLINE contains over 15 million references to articles from approximately 4,800 journals in 30 languages, dating back to the 1960s. In 2004, over 571,000 new citations were added to the database, and it continues to grow at a steady pace. The subject scope of MEDLINE is biomedicine and health, broadly deﬁned to encompass those areas of the life sciences, behavioral sciences, chemical sciences, and bioengineering needed by health professionals and others engaged in basic research and clinical care, public health, health policy development, or related educational activities. MEDLINE also covers life sciences vital to biomedical practitioners, researchers, and educators, including aspects of biology, environmental science, marine biology, plant and animal science, as well as biophysics and chemistry.1 Each MEDLINE citation includes basic information such as the title of the article, name of the authors, name of the publication, publication type, date of publication, language, and so on. Of the entries added over the last decade or so, approximately 76% have English abstracts written by the authors of the articles—these texts provide the source for answers extracted by our system. Additional metadata are associated with each MEDLINE citation. The most important of these is the controlled vocabulary terms assigned by human indexers. NLM’s controlled vocabulary thesaurus, Medical Subject Headings (MeSH),2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a  
This article describes a method for composing ﬂuent and complex natural language questions, while avoiding the standard pitfalls of free text queries. The method, based on Conceptual Authoring, is targeted at question-answering systems where reliability and transparency are critical, and where users cannot be expected to undergo extensive training in question composition. This scenario is found in most corporate domains, especially in applications that are risk-averse. We present a proof-of-concept system we have developed: a question-answering interface to a large repository of medical histories in the area of cancer. We show that the method allows users to successfully and reliably compose complex queries with minimal training. 1. Introduction Where early attempts to build natural language question-answering systems focused on accessing and presenting information held in (closed domain) databases (e.g., Hendrix et al. 1978; Templeton and Burger 1983; Kaplan 1984; Hafner and Godden 1985), the advent of the World Wide Web has led to a shift towards (open domain) collections of texts. However, despite signiﬁcant advances in open domain question answering since the simple pattern-matching systems of the ﬁrst TREC competition in 1999, current systems are still largely restricted to simple questions. They can, for example, successfully ﬁnd answers to questions like Which is the highest peak in Africa? or Who ﬁrst climbed Kilimanjaro? but they cannot correctly answer more complex questions like: What is the median height of the top twelve highest peaks in Africa? Which explorer who climbed Kilimanjaro but not Everest between 1960 and 1995 died in the last three years before the age of 55? How many of the explorers who climbed Kilimanjaro but not Everest between 1960 and 1995 did so more than three times during that period? ∗ Department of Computing, The Open University, Walton Hall, Milton Keynes, Buckinghamshire, MK7 6AA, UK. E-mail: D.Scott@open.ac.uk; C.Hallett@open.ac.uk; R.Power@open.ac.uk. The research presented here was supported in part by Medical Research Council grant G0100852 under the e-Science GRID Initiative. Special thanks are due our colleagues on CLEF (Alan Rector, Jeremy Rogers, and James McKay) and to the CLEF clinical collaborators at the Royal Marsden and Royal Free hospitals—see www.clinical-escience.org. Submission received: 17 July 2005; revised submission received: 3 May 2006; accepted for publication: 28 July 2006. © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 1  There are many reasons why such queries are unlikely to be successful. For example, although the ﬁrst question is very simple to interpret, a correct answer is unlikely to be available (in a retrievable form) in any individual document in the target collection. A question-answering system would thus have to ﬁrst retrieve the heights of each of the top twelve highest peaks, probably from different documents, and apply some calculations to obtain their median height, and then generate a response that aggregates answers from multiple documents. The answer to the second question, on the other hand, is very simple and likely to be found in a small number of documents, but the question itself is not trivial to interpret and would require (among other things) resolving the temporal information, correctly assuming that 55 refers to age at the time of death, and interpreting the negation but not as referring to the climbing of Everest only within the speciﬁed time span. For the third question, the difﬁculty comes from a combination of complex question and complex answer. Retrieving aggregated results from the World Wide Web also introduces issues of reliability because the sources may not all be trusted, and there is no guarantee that a different selection of sources would not yield a contrary result. For many applications of question answering, the need for complex questions and trusted answers is paramount—for example, in the medical, legal, and ﬁnancial domains, or indeed in any research area—and it is to this scenario that the work we present here applies. Our goal is to develop a general and intuitive method by which users can pose complex queries to data repositories; we are particularly concerned with scenarios where the users are domain experts (i.e., clinicians, lawyers, ﬁnanciers, etc.) rather than database experts, where reliability of the answer is critical, where the method of posing questions should be easy to learn, and where the questions themselves should be transparent (i.e., clear and unambiguous) to both user and system. Current methods for querying databases typically make use of formal query languages such as SQL. These languages are highly technical and require a great deal of training to achieve the level of proﬁciency required to pose the kinds of complex queries shown in the previous example. Successful query composition requires the user to be proﬁcient in the query language and have detailed knowledge of the structure of the database to which the queries are being addressed. Users also need to be ﬂuent in any formal codes employed to refer to entities in the domain (e.g., disease classiﬁcations, laws, bank codes). For example, in the medical domain alone there are a large number of clinical terminologies and classiﬁcations, used for different purposes: Some classiﬁcations, such as ICD-9, ICD-10, and OPCS-4, are employed in summarizing the incidence of diseases and operations on a national or worldwide level; others, such as CPT4 or ICD-9CM, manage the process of billing patients. Each covers a large number of terms and associated codes: SNOMED-CT alone, to name the most widely used medical terminology, currently contains some 365,000 individual concepts, and is being updated continuously (College of American Pathologists 2004). Finally, because database languages are not transparent, mistakes in query formulation can be difﬁcult to spot; so even where the system itself may be highly reliable, there is a reasonable chance that—except for very highly experienced database programmers—the returned answer may not be an accurate response to the intended question. A well-known alternative to formal database languages is available in visual query systems, which make use of graphical devices such as forms, diagrams, menus, and pointers to communicate the content of a database to the user. They are also widely used in commercial applications, and research shows that they are much preferred over textual query languages like SQL, especially by casual and non-expert users (Capindale 106  Hallett, Scott, and Power  Composing Questions through Conceptual Authoring  and Crawford 1990; Bell and Rowe 1992; Catarci and Santucci 1995). However, visual interfaces are also problematic: empirical studies report high error rates by domain experts using visual object-oriented modeling tools (Kim 1990), and a clear advantage of text over graphics for understanding nested conditional structures (Petre 1995). Natural language clearly provides a more intuitive means for users to pose their questions, but this is also highly problematic because queries expressed in free natural language are obviously very sensitive to errors of composition (e.g., misspellings, ungrammaticalities) or processing (at the lexical, syntactic, or semantic level). 2. Natural Language Interfaces In a typical natural language interface to a database (henceforth NLIDB), the user requests database records through a query expressed in natural language. The question is ﬁrst parsed and analyzed semantically by a linguistic front-end, which translates it into an intermediate meaning representation language (typically, some form of logic). The intermediate language expression is then translated into a database language (usually SQL) that is supported by the underlying database management system. A large number of NLIDBs have been developed in the past 30 years, featuring a wide range of techniques. The general drawback of these systems1 is that they normally understand only a subset of natural language. Casual users cannot always discern which constructions are valid or whether the lack of response from the system is due to the unavailability of an answer or to an unaccepted input construction. On the positive side, natural language is far more expressive than formal database query languages such as SQL, so it is generally easier to ask complex questions using natural language (NL) than a database language (a single natural language query will have to be translated into multiple SQL statements). Natural language queries are not only more userfriendly for the non-expert user, but they also allow easier manipulation of temporal constructions. Broadly, research in NLIDBs has addressed the following issues:2 r domain knowledge acquisition (Frank et al. 2005) r interpretation of the NL input query, including parsing and semantic disambiguation, semantic interpretation, and transformation of the query to an intermediate logical form (Hendrix et al. 1978; Zhang et al. 1999; Tang and Mooney 2001; Popescu, Etzioni, and Kautz 2003; Kate, Wong, and Mooney 2005) r translation to a database query language (Lowden et al. 1991; Androutsopoulos 1992) r portability (Templeton and Burger 1983; Kaplan 1984; Hafner and Godden 1985; Androutsopoulos, Ritchie, and Thanitsch 1993; Popescu, Etzioni, and Kautz 2003)  
 Computational Linguistics  Volume 33, Number 1  reference architecture for text mining that combines these components with each other and with domain resources is presented and compared with the organization of two extant systems, GeneWays (Rzhetsky et al. 2004) and PASTA (Gaizauskas et al. 2003). The comparison leads to the conclusion that the reference architecture represents a somewhat “idealized view of system building . . . which has to be supplemented by many heuristic solutions” (p. 37), which might explain why this architecture is not referred to extensively in subsequent chapters. The archetypal text-mining system should ideally strive to produce “some form of proposition” (p. 33) as its output, which will be subject to subsequent processing, for example, to discover new knowledge. However, the authors acknowledge that they are not aware of any system with such an advanced reasoning functionality (p. 34). I did not spot such a system being reviewed anywhere else in the book so I may rather safely claim that the book is mainly about conducting NLP in the biomedical domain rather than discussing the text mining process as a whole. This chapter is meant to serve as “an introduction to the general techniques of NLP . . . necessary to fully appreciate discussions in following chapters” (p. 7). The authors cite and discuss the seminal literature for each NLP component in their hypothetical architecture quite comprehensively, although they do not include any references to other introductory readings to NLP. These could have been helpful, because one often comes across terminology that might be unknown to the non-specialist (e.g., Section 2.3.1 on part-of-speech tagging contains terms such as seed tagging, second order n-gram Markov models, probabilistic sufﬁx analysis, and smoothing by linear interpolation without explanation). Quite a bit of jargon is used in other chapters as well, so my feeling is that the book will be more accessible to readers with some familiarity with NLP than to the non-initiated. Chapter 3, “Lexical, terminological, and ontological resources for biological text mining” by Olivier Bodenreider, discusses how the major publicly available knowledge sources may support biomedical NER and IE, with particular emphasis on the three components of the Uniﬁed Medical Language System (UMLS), namely, the Specialist Lexicon, the Metathesaurus, and the Semantic Network (Bodenreider 2004). The chapter exempliﬁes the utility that these resources may provide, although Bodenreider also points out that they might often need to be extended or re-engineered to better serve NER and IE. This is one of the main insights of the book that will be repeated in subsequent chapters. This chapter provides a clear discussion of the commonalities, differences, and complementarity of the existing resources, which are classiﬁed into three types: lexical, terminological, and ontological. Not distinguishing between the three types of resources has been claimed earlier in the book to be likely to “lead to confusion and hamper attempts at exploitation for text mining” (p. 7) although, in Bodenreider’s own words, this distinction often ends up being “somewhat arbitrary” (p. 55). Hence he appears to concentrate more on some of the limitations of the resources, such as the restricted coverage of the genomic and the molecular biology subdomain. An overview of suggested solutions to this problem is provided, although these seem to be more applicable to domain-speciﬁc resources (e.g., model organism databases) than to the more general UMLS resources that the chapter focuses on. It has also been argued earlier that the lack of an explicit link between a lemma in the Specialist Lexicon and the corresponding concept in the Metathesaurus might also limit their utility for NER and IE (p. 27), but this issue remains unaddressed in Chapter 3. Chapter 4, “Automatic terminology management in biomedicine” by Sophia Ananiadou and Goran Nenadic, focuses on automatic term recognition (ATR) and  136  Book Reviews automatic term structuring (ATS). ATR identiﬁes lexical units that correspond to domain concepts, and ATS organizes the recognized terms into knowledge structures (terminologies). A brief introduction to terminology construction is followed by a presentation of terminological resources in biomedicine (somewhat overlapping with material in the previous chapter). A detailed review of the main approaches to ATM and ATS constitutes the core of the chapter. Equally interesting is the discussion of the challenges posed to ATR by the pervasive phenomena of term variation and ambiguity. The chapter concludes with an overview of the ATRACT system (Mima, Ananiadou, and Nenadic 2001), a terminology management workbench incorporating modules for ATR and ATS. There is much valuable material in this chapter, although I felt that it would have been more appropriate to discuss the difference between ATR and NER here rather than having to wait until Chapter 6 (or go back to page 8). Some discussion of the differences between ATS and IE (overviewed in Chapter 7) would have been useful as well. Chapter 5, “Abbreviations in biomedical text” by Jeff Chang and Hinrich Schu¨ tze, deals with the problem of linking an abbreviation to its expanded form(s). This is important because of the very frequent use of abbreviations in the biomedical genre and the continuous introduction of many new abbreviations. An introductory discussion of the problems of deﬁning and identifying abbreviations is followed by a detailed review of the methods used to construct and evaluate the Stanford Biomedical Abbreviation Database (Chang, Schu¨ tze, and Altman 2002). Different types of abbreviation variations (already introduced in the previous chapter, although no cross-reference is provided) and the methods used for their normalization are also overviewed. The chapter also touches upon the problem of identifying long forms that do not appear in the same document as the abbreviation. Several directions for future work are proposed, the most interesting of which, in my view, are the need for a comprehensive study to compare the coverage and accuracy of different abbreviation databases and the more extended investigation of algorithms that can automatically generate abbreviations from long forms. Chapter 6, “Named entity recognition” by Jong Park and Jung-jae Kim, concentrates on applying NER to biomedical text. The nature of candidate named entities (NEs) and issues related to their ambiguity, variation, and growth rate (also mentioned in previous chapters) are discussed in detail to exemplify how biomedical NER differs from traditional NER in the newswire domain. The main approaches to biomedical NER are reviewed in depth with particular emphasis on the reported evaluation results (although the authors also point out that these cannot always be used to directly compare the approaches because of important methodological differences between the evaluation studies). Grounding the recognized NEs in an ontology and dealing with NEs other than gene and protein names are identiﬁed as the main challenges to address in forthcoming research. Chapters 4 through 6 clearly complement one another, although identifying overlapping or related sections often requires some effort on behalf of the reader. In particular, I often found it hard to keep track of literature reviewed in different chapters. Each chapter contains its own list of references (enumerated in the order in which they appear in the text, which seems to be the norm in biomedical publications). The number in the list is typically used to point to a reference in the text. For instance, the ABGENE system (Tanabe and Wilbur 2002) is reviewed both in Chapter 4 (p. 77) and in Chapter 6 (p. 134). In Chapter 4 the system is mentioned by name followed by its citation number (i.e., “ABGENE [36]”), whereas in Chapter 6 the authors’ names appear together with the citation number (i.e., “Tanabe and Wilbur [26]”). (Note that the term ABGENE is not 137  Computational Linguistics  Volume 33, Number 1  included in the book’s index.) Had the book come with a single reference section and a citation index, the reader’s attempt to identify and extract related information would have been facilitated greatly. Chapter 7, “Information extraction” by John McNaught and William Black, is devoted to rule-based methods for the extraction of simple facts and more complex events. An overview of IE as shaped by the MUC evaluation efforts is followed by a comprehensive critical assessment of the various approaches adopted for IE in the biomedical domain. Sublanguage-driven systems that simultaneously consider syntax and semantics, such as GENIES (Friedman et al. 2001), and systems that take advantage of ontological information (Gaizauskas et al. 2003; Cimiano, Saric, and Reyle 2005) are proclaimed to be the most successful. As in Chapter 2, being able to deliver abstract representations of facts and events that can be subjected to subsequent data mining or integrated in a knowledge base to enable reasoning (instead of simply returning textual strings or their transforms) is regarded as a bonus for a system. Given that these representations are likely to be heavily reliant on the requirements of the mining or reasoning process, I would welcome more discussion on how systems developed to deliver material suitable for different knowledge bases may be compared with each other. Chapter 7 concludes with a call for further efforts to produce resources that can be used to train and evaluate more advanced IE systems, echoing other similar statements throughout the book (most notably in Chapters 2 and 3). Given that the preparation of such resources is not a trivial task (as discussed in the following chapter), it is somehow surprising that semi-supervised or unsupervised machine learning methods, for example those discussed by McCallum (2005), are not mentioned as an alternative research avenue. The problem of resolving anaphoric references is mentioned in several chapters as another essential NLP task that awaits in-depth investigation in the biomedical domain. Chapter 7 is meant to provide an overview of existing approaches to anaphora resolution in biomedical text (p. 148), but this takes place only in passing. Devoting some more space to this issue would have been worthwhile as well. Chapter 8, “Corpora and their annotation” by Jin-Dong Kim and Jun’ichi Tsujii, discusses issues related to the collection and annotation of corpora. From their own experience in the development of the GENIA corpus (Kim et al. 2003), the authors provide practical advice on how to compile a representative corpus, prepare annotation schemes and guidelines, perform the actual annotation and, ultimately, assess the reliability of the produced data. There is a section on annotation format that lays emphasis on XMLbased schemes but does not mention the B-I-O notation that is used in Chapters 2 and 4. An informative discussion on available annotation tools concludes the chapter, which is written very clearly, although I found some material too low level (particularly the script to retrieve MEDLINE abstracts in Figure 8.1) or even subpar (Section 8.3.3 on the comparison of corpora). Chapter 9, “Evaluation of text mining in biology” by Lynette Hirschman and Christian Blaschke, begins by explaining how the MUC and TREC evaluation challenges and similar efforts in molecular biology inspired community attempts to build shared assessment resources and agree on evaluation methodologies to appraise the state of the art in biomedical NLP. The authors address the key issues of why, how, and what to evaluate and then detail the design, organization, and main results of four recent evaluation challenges and how these should motivate additional efforts in the years to come. One of the main points in Chapter 9 is that research on biomedical NLP should be focused on resolving problems of practical relevance to biologists in order for them to become involved in the development effort and continue to participate in challenging  138  Book Reviews  evaluations. This issue is of particular interest to me because of my current involvement in a project aiming to integrate an NLP system into an existing curation workﬂow (Karamanis et al. 2007). In Section 9.3.1, the authors distinguish between the tasks performed by different types of users, namely database curators and research scientists, and then go on to explain how different evaluation tasks were designed with a different type of user in mind. Meeting users’ requirements seems to be relevant to other chapters of the book as well. For instance, it is not clear whether delivering abstract representations of facts (as suggested in Chapters 2 and 7) will assist curators more than pointing them to actual textual strings. Developing and evaluating integrated systems that address the users’ real-world needs is one of the greatest challenges in biomedical NLP (Cohen and Hersh 2005), but is not substantially covered in the book. Chapter 10, “Integrating text mining with data mining” by See-Kiong Ng, demonstrates how certain NLP techniques may be incorporated into extant algorithms for analyzing nontext biological data such as genomic sequences and expression proﬁles. Many of the overviewed techniques have been shown to improve solutions to problems that are of particular importance to research scientists, such as homology search and sequence-based functional classiﬁcation. This is a very interesting chapter, which comes closer to the Hearstian notion of text mining than previous chapters, although most of the reviewed techniques treat the text as a bag of words, thus deviating signiﬁcantly from the NLP technology discussed previously. 
Fourthly, search hits are for pages, not for instances. Working with commercial search engines makes us develop workarounds. We become experts in the syntax and constraints of Google, Yahoo, Altavista, and so on. We become ‘googleologists’. The argument that the commercial search engines provide © 2007 Association for Computational Linguistics  Computational Linguistics  Volume 33, Number 1  low-cost access to the Web fades as we realize how much of our time is devoted to working with and against the constraints that the search engines impose. But science is hard work, and there are usually many foothill problems to be mastered before we get to the mountains that are our true goal. So this is all regular science. Or so it may seem until we consider the arbitrariness of search engine counts. They depend on many speciﬁcs of the search engine’s practice, including how it handles spam and duplicates. (See the entries “Yahoo’s missing pages” [2005] and “Crazy duplicates” [2006] in Jean Ve´ronis’s blog.1) The engines will give you substantially different counts, even for repeats of the same query. In a small experiment, queries repeated the following day gave counts over 10% different 9 times in 30, and by a factor of two different 6 times in 30. The reasons are that queries are sent to different computers, at different points in the update cycle, and with different data in their caches. People wishing to use the URLs, rather than the counts, that search engines provide in their hits pages face another issue: The hits are sorted according to a complex and unknown algorithm (with full listings of all results usually not permitted) so we do not know what biases are being introduced. If we wish to investigate the biases, the area we become expert in is googleology, not linguistics. An Academic-Community Alternative An alternative is to work like the search engines, downloading and indexing substantial proportions of the World Wide Web, but to do so transparently, giving reliable ﬁgures, and supporting language researchers’ queries. In Baroni and Kilgarriff (2006) we report on a feasibility study: We prepared Web corpora for German (‘DeWaC’) and Italian (‘ItWaC’) with around 1.5 billion words each, now loaded into a sophisticated corpus query tool and available for research use.2 (Of course there are various other large Web datasets that research groups have downloaded and are using for NLP.) By sharing good practice and resources and developing expertise, the prospects of the academic research community having resources to compare with Google, Microsoft, and so forth, improves. Data Cleaning The process involves crawling, downloading, ’cleaning’, and de-duplicating the data, then linguistically annotating it and loading it into a corpus query tool. Expertise and tools are available for most of these steps, with the Internet community providing crawlers and a de-duplication algorithm (Broder et al. 1997) and the NLP community providing corpus query tools, lemmatizers, and POS-taggers for many languages. But in the middle there is a logjam. The questions r How do we detect and get rid of navigation bars, headers, footers, . . . .? r How do we identify paragraphs and other structural information? r How do we produce output in a standard form suitable for further processing?  
Sentence compression holds promise for many applications ranging from summarisation to subtitle generation and subtitle generation. The task is typically performed on isolated sentences without taking the surrounding context into account, even though most applications would operate over entire documents. In this paper we present a discourse informed model which is capable of producing document compressions that are coherent and informative. Our model is inspired by theories of local coherence and formulated within the framework of Integer Linear Programming. Experimental results show signiﬁcant improvements over a stateof-the-art discourse agnostic approach. 
Shallow semantic parsing, the automatic identiﬁcation and labeling of sentential constituents, has recently received much attention. Our work examines whether semantic role information is beneﬁcial to question answering. We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models. 
This paper presents a syntax-driven approach to question answering, speciﬁcally the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classiﬁers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimental results using the TREC dataset are shown to signiﬁcantly outperform strong state-of-the-art baselines. 
Previous machine learning techniques for answer selection in question answering (QA) have required question-answer training pairs. It has been too expensive and labor-intensive, however, to collect these training pairs. This paper presents a novel unsupervised support vector machine (USVM) classiﬁer for answer selection, which is independent of language and does not require hand-tagged training pairs. The key ideas are the following: 1. unsupervised learning of training data for the classiﬁer by clustering web search results; and 2. selecting the correct answer from the candidates by classifying the question. The comparative experiments demonstrate that the proposed approach signiﬁcantly outperforms the retrieval-based model (Retrieval-M), the supervised SVM classiﬁer (S-SVM), and the pattern-based model (Pattern-M) for answer selection. Moreover, the cross-model comparison showed that the performance ranking of these models was: U-SVM > PatternM > S-SVM > Retrieval-M. 
We describe an approach to improve Statistical Machine Translation (SMT) performance using multi-lingual, parallel, sentence-aligned corpora in several bridge languages. Our approach consists of a simple method for utilizing a bridge language to create a word alignment system and a procedure for combining word alignment systems from multiple bridge languages. The ﬁnal translation is obtained by consensus decoding that combines hypotheses obtained using all bridge language word alignments. We present experiments showing that multilingual, parallel text in Spanish, French, Russian, and Chinese can be utilized in this framework to improve translation performance on an Arabic-to-English task. 
Word alignment is the problem of annotating parallel text with translational correspondence. Previous generative word alignment models have made structural assumptions such as the 1-to-1, 1-to-N, or phrase-based consecutive word assumptions, while previous discriminative models have either made such an assumption directly or used features derived from a generative model making one of these assumptions. We present a new generative alignment model which avoids these structural limitations, and show that it is effective when trained using both unsupervised and semi-supervised training methods. 
This paper presents a tree-to-tree transduction method for text rewriting. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring signiﬁcant improvements over a state-of-the-art model. 
Many emerging applications require documents to be repeatedly updated. Such documents include newsfeeds, webpages, and shared community resources such as Wikipedia. In this paper we address the task of inserting new information into existing texts. In particular, we wish to determine the best location in a text for a given piece of new information. For this process to succeed, the insertion algorithm should be informed by the existing document structure. Lengthy real-world texts are often hierarchically organized into chapters, sections, and paragraphs. We present an online ranking model which exploits this hierarchical structure – representationally in its features and algorithmically in its learning procedure. When tested on a corpus of Wikipedia articles, our hierarchically informed model predicts the correct insertion paragraph more accurately than baseline methods. 
Approaches to plural reference generation emphasise descriptive brevity, but often lack empirical backing. This paper describes a corpus-based study of plural descriptions, and proposes a psycholinguisticallymotivated algorithm for plural reference generation. The descriptive strategy is based on partitioning and incorporates corpusderived heuristics. An exhaustive evaluation shows that the output closely matches human data. 
This paper compares a deep and a shallow processing approach to the problem of classifying a sentence as grammatically wellformed or ill-formed. The deep processing approach uses the XLE LFG parser and English grammar: two versions are presented, one which uses the XLE directly to perform the classiﬁcation, and another one which uses a decision tree trained on features consisting of the XLE’s output statistics. The shallow processing approach predicts grammaticality based on n-gram frequency statistics: we present two versions, one which uses frequency thresholds and one which uses a decision tree trained on the frequencies of the rarest n-grams in the input sentence. We ﬁnd that the use of a decision tree improves on the basic approach only for the deep parser-based approach. We also show that combining both the shallow and deep decision tree features is effective. Our evaluation is carried out using a large test set of grammatical and ungrammatical sentences. The ungrammatical test set is generated automatically by inserting grammatical errors into well-formed BNC sentences. 
 We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development. 
A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence. We exploit the Matrix Tree Theorem (Tutte, 1984) to derive an algorithm that efﬁciently sums the scores of all nonprojective trees in a sentence, permitting the deﬁnition of a conditional log-linear model over trees. While discriminative methods, such as those presented in McDonald et al. (2005b), obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization, “summing trees” permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 
This paper provides an algorithmic framework for learning statistical models involving directed spanning trees, or equivalently non-projective dependency structures. We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff’s Matrix-Tree Theorem. To demonstrate an application of the method, we perform experiments which use the algorithm in training both log-linear and max-margin dependency parsers. The new training methods give improvements in accuracy over perceptron-trained models. 
Inclusions from other languages can be a signiﬁcant source of errors for monolingual parsers. We show this for English inclusions, which are sufﬁciently frequent to present a problem when parsing German. We describe an annotation-free approach for accurately detecting such inclusions, and develop two methods for interfacing this approach with a state-of-the-art parser for German. An evaluation on the TIGER corpus shows that our inclusion entity model achieves a performance gain of 4.3 points in F-score over a baseline of no inclusion detection, and even outperforms a parser with access to gold standard part-of-speech tags. 
Semantic inference is a core component of many natural language applications. In response, several researchers have developed algorithms for automatically learning inference rules from textual corpora. However, these rules are often either imprecise or underspecified in directionality. In this paper we propose an algorithm called LEDIR that filters incorrect inference rules and identifies the directionality of correct ones. Based on an extension to Harris’s distributional hypothesis, we use selectional preferences to gather evidence of inference directionality and plausibility. Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines. 
This paper assesses the role of multi-label classiﬁcation in modelling polysemy for language acquisition tasks. We focus on the acquisition of semantic classes for Catalan adjectives, and show that polysemy acquisition naturally suits architectures used for multilabel classiﬁcation. Furthermore, we explore the performance of information drawn from different levels of linguistic description, using feature sets based on morphology, syntax, semantics, and n-gram distribution. Finally, we demonstrate that ensemble classiﬁers are a powerful and adequate way to combine different types of linguistic evidence: a simple, majority voting ensemble classiﬁer improves the accuracy from 62.5% (best single classiﬁer) to 84%. 
Traditional research on spelling correction in natural language processing and information retrieval literature mostly relies on pre-defined lexicons to detect spelling errors. But this method does not work well for web query spelling correction, because there is no lexicon that can cover the vast amount of terms occurring across the web. Recent work showed that using search query logs helps to solve this problem to some extent. However, such approaches cannot deal with rarely-used query terms well due to the data sparseness problem. In this paper, a novel method is proposed for use of web search results to improve the existing query spelling correction models solely based on query logs by leveraging the rich information on the web related to the query and its top-ranked candidate. Experiments are performed based on realworld queries randomly sampled from search engine’s daily logs, and the results show that our new method can achieve 16.9% relative F-measure improvement and 35.4% overall error rate reduction in comparison with the baseline method.  Mu Li Microsoft Research Asia 5F Sigma Center Zhichun Road, Haidian District Beijing, China, 100080 muli@microsoft.com 
The increasing use of large open-domain document sources is exacerbating the problem of ambiguity in named entities. This paper explores the use of a range of syntactic and semantic features in unsupervised clustering of documents that result from ad hoc queries containing names. From these experiments, we find that the use of robust syntactic and semantic features can significantly improve the state of the art for disambiguation performance for personal names for both Chinese and English. 
Trigram language models are compressed using a Golomb coding method inspired by the original Unix spell program. Compression methods trade off space, time and accuracy (loss). The proposed HashTBO method optimizes space at the expense of time and accuracy. Trigram language models are normally considered memory hogs, but with HashTBO, it is possible to squeeze a trigram language model into a few megabytes or less. HashTBO made it possible to ship a trigram contextual speller in Microsoft Office 2007. 
In morphologically rich languages, should morphological and syntactic disambiguation be treated sequentially or as a single problem? We describe several efﬁcient, probabilisticallyinterpretable ways to apply joint inference to morphological and syntactic disambiguation using lattice parsing. Joint inference is shown to compare favorably to pipeline parsing methods across a variety of component models. State-of-the-art performance on Hebrew Treebank parsing is demonstrated using the new method. The beneﬁts of joint inference are modest with the current component models, but appear to increase as components themselves improve. 
This paper proposes a new bootstrapping approach to unsupervised part-of-speech induction. In comparison to previous bootstrapping algorithms developed for this problem, our approach aims to improve the quality of the seed clusters by employing seed words that are both distributionally and morphologically reliable. In particular, we present a novel method for combining morphological and distributional information for seed selection. Experimental results demonstrate that our approach works well for English and Bengali, thus providing suggestive evidence that it is applicable to both morphologically impoverished languages and highly inflectional languages. 
We introduce a relation extraction method to identify the sentences in biomedical text that indicate an interaction among the protein names mentioned. Our approach is based on the analysis of the paths between two protein names in the dependency parse trees of the sentences. Given two dependency trees, we deﬁne two separate similarity functions (kernels) based on cosine similarity and edit distance among the paths between the protein names. Using these similarity functions, we investigate the performances of two classes of learning algorithms, Support Vector Machines and k-nearest-neighbor, and the semisupervised counterparts of these algorithms, transductive SVMs and harmonic functions, respectively. Signiﬁcant improvement over the previous results in the literature is reported as well as a new benchmark dataset is introduced. Semi-supervised algorithms perform better than their supervised version by a wide margin especially when the amount of labeled data is limited. 
We describe a discriminatively trained sequence alignment model based on the averaged perceptron. In common with other approaches to sequence modeling using perceptrons, and in contrast with comparable generative models, this model permits and transparently exploits arbitrary features of input strings. The simplicity of perceptron training lends more versatility than comparable approaches, allowing the model to be applied to a variety of problem types for which a learned edit model might be useful. We enumerate some of these problem types, describe a training procedure for each, and evaluate the model’s performance on several problems. We show that the proposed model performs at least as well as an approach based on statistical machine translation on two problems of name transliteration, and provide evidence that the combination of the two approaches promises further improvement. 
In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text. The approach is fully unsupervised and based on kernel methods. We demonstrate the effectiveness of our technique largely surpassing both the random and most frequent baselines and outperforming current state-of-the-art unsupervised approaches on a benchmark ontology available in the literature. 
To date, work on Non-Local Dependencies (NLDs) has focused almost exclusively on English and it is an open research question how well these approaches migrate to other languages. This paper surveys non-local dependency constructions in Chinese as represented in the Penn Chinese Treebank (CTB) and provides an approach for generating proper predicate-argument-modiﬁer structures including NLDs from surface contextfree phrase structure trees. Our approach recovers non-local dependencies at the level of Lexical-Functional Grammar f-structures, using automatically acquired subcategorisation frames and f-structure paths linking antecedents and traces in NLDs. Currently our algorithm achieves 92.2% f-score for trace insertion and 84.3% for antecedent recovery evaluating on gold-standard CTB trees, and 64.7% and 54.7%, respectively, on CTBtrained state-of-the-art parser output trees. 
We present a simple history-based model for sentence generation from LFG f-structures, which improves on the accuracy of previous models by breaking down PCFG independence assumptions so that more f-structure conditioning context is used in the prediction of grammar rule expansions. In addition, we present work on experiments with named entities and other multi-word units, showing a statistically signiﬁcant improvement of generation accuracy. Tested on section 23 of the Penn Wall Street Journal Treebank, the techniques described in this paper improve BLEU scores from 66.52 to 68.82, and coverage from 98.18% to 99.96%. 
Given multiple translations of the same source sentence, how to combine them to produce a translation that is better than any single system output? We propose a hierarchical system combination framework for machine translation. This framework integrates multiple MT systems’ output at the word-, phrase- and sentence- levels. By boosting common word and phrase translation pairs, pruning unused phrases, and exploring decoding paths adopted by other MT systems, this framework achieves better translation quality with much less redecoding time. The full sentence translation hypotheses from multiple systems are additionally selected based on N-gram language models trained on word/word-POS mixed stream, which further improves the translation quality. We consistently observed signiﬁcant improvements on several test sets in multiple languages covering different genres. 
This paper proposes a method using the existing Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Machine Translation (SMT) system. We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus. With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus. In our experiments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora. We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora. The interpolated model achieves an absolute improvement of 0.0245 BLEU score (13.1% relative) as compared with the individual model trained on the real bilingual corpus. 
This paper investigates why the HMMs estimated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech (POS) taggers. We ﬁnd that the HMMs estimated by EM generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to POS tags is highly skewed. This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution. We investigate Gibbs Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB signiﬁcantly improves 1-to-1 tagging accuracy over EM. We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced. We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought. 
This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics. We integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames. This approach simultaneously addresses two tasks of coordination disambiguation: the detection of coordinate conjunctions and the scope disambiguation of coordinate structures. Experimental results on web sentences indicate the effectiveness of our approach.  more than two conjuncts, commas, which have various usages, also function like coordinate conjunctions. Recognizing true coordinate conjunctions from such possible coordinate conjunctions is a task of coordination disambiguation (Kurohashi, 1995). The other is the task of identifying the range of coordinate phrases or clauses. Previous work on coordination disambiguation has focused on the task of addressing the scope ambiguity (e.g., (Agarwal and Boggess, 1992; Goldberg, 1999; Resnik, 1999; Chantree et al., 2005)). Kurohashi and Nagao proposed a similarity-based method to resolve both of the two tasks for Japanese (Kurohashi and Nagao, 1994). Their method, however, heuristically detects coordinate conjunctions by considering only similarities between possible conjuncts, and thus cannot disambiguate the following cases1:  
We cannot use non-local features with current major methods of sequence labeling such as CRFs due to concerns about complexity. We propose a new perceptron algorithm that can use non-local features. Our algorithm allows the use of all types of non-local features whose values are determined from the sequence and the labels. The weights of local and non-local features are learned together in the training process with guaranteed convergence. We present experimental results from the CoNLL 2003 named entity recognition (NER) task to demonstrate the performance of the proposed algorithm. 
In this paper, we address a unique problem in Chinese language processing and report on our study on extending a Chinese thesaurus with region-specific words, mostly from the financial domain, from various Chinese speech communities. With the larger goal of automatically constructing a Pan-Chinese lexical resource, this work aims at taking an existing semantic classificatory structure as leverage and incorporating new words into it. In particular, it is important to see if the classification could accommodate new words from heterogeneous data sources, and whether simple similarity measures and clustering methods could cope with such variation. We use the cosine function for similarity and test it on automatically classifying 120 target words from four regions, using different datasets for the extraction of feature vectors. The automatic classification results were evaluated against human judgement, and the performance was encouraging, with accuracy reaching over 85% in some cases. Thus while human judgement is not straightforward and it is difficult to create a PanChinese lexicon manually, it is observed that combining simple clustering methods with the appropriate data sources appears to be a promising approach toward its automatic construction. 
Product reviews posted at online shopping sites vary greatly in quality. This paper addresses the problem of detecting lowquality product reviews. Three types of biases in the existing evaluation standard of product reviews are discovered. To assess the quality of product reviews, a set of specifications for judging the quality of reviews is first defined. A classificationbased approach is proposed to detect the low-quality reviews. We apply the proposed approach to enhance opinion summarization in a two-stage framework. Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews. 
Parallel corpus is an indispensable resource for translation model training in statistical machine translation (SMT). Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora. Two kinds of methods are proposed: offline data optimization and online model optimization. The offline method adapts the training data by redistributing the weight of each training sentence pairs. The online method adapts the translation model by redistributing the weight of each predefined submodels. Information retrieval model is used for the weighting scheme in both methods. Experimental results show that without using any additional resource, both methods can improve SMT performance significantly. 
We present a domain-independent unsupervised topic segmentation approach based on hybrid document indexing. Lexical chains have been successfully employed to evaluate lexical cohesion of text segments and to predict topic boundaries. Our approach is based in the notion of semantic cohesion. It uses spectral embedding to estimate semantic association between content nouns over a span of multiple text segments. Our method signiﬁcantly outperforms the baseline on the topic segmentation task and achieves performance comparable to state-of-the-art methods that incorporate domain speciﬁc information. 
We present a method for improving word alignment for statistical syntax-based machine translation that employs a syntactically informed alignment model closer to the translation model than commonly-used word alignment models. This leads to extraction of more useful linguistic patterns and improved BLEU scores on translation experiments in Chinese and Arabic. 
In this paper we explore the use of selectional preferences for detecting noncompositional verb-object combinations. To characterise the arguments in a given grammatical relationship we experiment with three models of selectional preference. Two use WordNet and one uses the entries from a distributional thesaurus as classes for representation. In previous work on selectional preference acquisition, the classes used for representation are selected according to the coverage of argument tokens rather than being selected according to the coverage of argument types. In our distributional thesaurus models and one of the methods using WordNet we select classes for representing the preferences by virtue of the number of argument types that they cover, and then only tokens under these classes which are representative of the argument head data are used to estimate the probability distribution for the selectional preference model. We demonstrate a highly signiﬁcant correlation between measures which use these ‘typebased’ selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ-  ual features used in previous research on the same dataset. 
 Most of the text summarization research carried out to date has been concerned with the summarization of short documents (e.g., news stories, technical reports), and very little work if any has been done on the summarization of very long documents. In this paper, we try to address this gap and explore the problem of book summarization. We introduce a new data set speciﬁcally designed for the evaluation of systems for book summarization, and describe summarization techniques that explicitly account for the length of the documents.  
We demonstrate an approach for inducing a tagger for historical languages based on existing resources for their modern varieties. Tags from Present Day English source text are projected to Middle English text using alignments on parallel Biblical text. We explore the use of multiple alignment approaches and a bigram tagger to reduce the noise in the projected tags. Finally, we train a maximum entropy tagger on the output of the bigram tagger on the target Biblical text and test it on tagged Middle English text. This leads to tagging accuracy in the low 80’s on Biblical test material and in the 60’s on other Middle English material. Our results suggest that our bootstrapping methods have considerable potential, and could be used to semi-automate an approach based on incremental manual annotation. 
In this paper, we consider the computational modelling of human plausibility judgements for verb-relation-argument triples, a task equivalent to the computation of selectional preferences. Such models have applications both in psycholinguistics and in computational linguistics.  Verb shoot shoot shoot shoot  Relation agent patient agent patient  Noun hunter hunter deer deer  Plausibility 6.9 2.8 1.0 6.4  Table 1: Verb-relation-noun triples with plausibility judgements on a 7-point scale (McRae et al., 1998)  By extending a recent model, we obtain a completely corpus-driven model for this task which achieves signiﬁcant correlations with human judgements. It rivals or exceeds deeper, resource-driven models while exhibiting higher coverage. Moreover, we show that our model can be combined with deeper models to obtain better predictions than from either model alone. 
We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously deﬁned cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the “problem of matching”, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisﬁes several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering. 
In this paper, we proposed a novel probabilistic generative model to deal with explicit multiple-topic documents: Parametric Dirichlet Mixture Model(PDMM). PDMM is an expansion of an existing probabilistic generative model: Parametric Mixture Model(PMM) by hierarchical Bayes model. PMM models multiple-topic documents by mixing model parameters of each single topic with an equal mixture ratio. PDMM models multiple-topic documents by mixing model parameters of each single topic with mixture ratio following Dirichlet distribution. We evaluate PDMM and PMM by comparing F-measures using MEDLINE corpus. The evaluation showed that PDMM is more effective than PMM. 
We address the problem of smoothing translation probabilities in a bilingual N-grambased statistical machine translation system. It is proposed to project the bilingual tuples onto a continuous space and to estimate the translation probabilities in this representation. A neural network is used to perform the projection and the probability estimation. Smoothing probabilities is most important for tasks with a limited amount of training material. We consider here the BTEC task of the 2006 IWSLT evaluation. Improvements in all ofﬁcial automatic measures are reported when translating from Italian to English. Using a continuous space model for the translation model and the target language model, an improvement of 1.5 BLEU on the test data is observed. 
Morphological analysis and disambiguation are crucial stages in a variety of natural language processing applications, especially when languages with complex morphology are concerned. We present a system which disambiguates the output of a morphological analyzer for Hebrew. It consists of several simple classiﬁers and a module which combines them under linguistically motivated constraints. We investigate a number of techniques for combining the predictions of the classiﬁers. Our best result, 91.44% accuracy, reﬂects a 25% reduction in error rate compared with the previous state of the art. 
Textual records of business-oriented conversations between customers and agents need to be analyzed properly to acquire useful business insights that improve productivity. For such an analysis, it is critical to identify appropriate textual segments and expressions to focus on, especially when the textual data consists of complete transcripts, which are often lengthy and redundant. In this paper, we propose a method to identify important segments from the conversations by looking for changes in the accuracy of a categorizer designed to separate different business outcomes. We extract effective expressions from the important segments to deﬁne various viewpoints. In text mining a viewpoint deﬁnes the important associations between key entities and it is crucial that the correct viewpoints are identiﬁed. We show the effectiveness of the method by using real datasets from a car rental service center. 
A Bloom ﬁlter (BF) is a randomised data structure for set membership queries. Its space requirements fall signiﬁcantly below lossless information-theoretic lower bounds but it produces false positives with some quantiﬁable probability. Here we present a general framework for deriving smoothed language model probabilities from BFs. We investigate how a BF containing n-gram statistics can be used as a direct replacement for a conventional n-gram model. Recent work has demonstrated that corpus statistics can be stored efﬁciently within a BF, here we consider how smoothed language model probabilities can be derived efﬁciently from this randomised representation. Our proposal takes advantage of the one-sided error guarantees of the BF and simple inequalities that hold between related n-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities. We use these models as replacements for a conventional language model in machine translation experiments. 
We present results that show that incorporating lexical and structural semantic information is effective for word sense disambiguation. We evaluated the method by using precise information from a large treebank and an ontology automatically created from dictionary sentences. Exploiting rich semantic and structural information improves precision 2–3%. The most gains are seen with verbs, with an improvement of 5.7% over a model using only bag of words and n-gram features. 
We consider the impact Active Learning (AL) has on effective and efﬁcient text corpus annotation, and report on reduction rates for annotation efforts ranging up until 72%. We also address the issue whether a corpus annotated by means of AL – using a particular classiﬁer and a particular feature set – can be re-used to train classiﬁers different from the ones employed by AL, supplying alternative feature sets as well. We, ﬁnally, report on our experience with the AL paradigm under real-world conditions, i.e., the annotation of large-scale document corpora for the life sciences. 
We investigate methods to improve the recall in coreference resolution by also trying to resolve those deﬁnite descriptions where no earlier mention of the referent shares the same lexical head (coreferent bridging). The problem, which is notably harder than identifying coreference relations among mentions which have the same lexical head, has been tackled with several rather different approaches, and we attempt to provide a meaningful classiﬁcation along with a quantitative comparison. Based on the different merits of the methods, we discuss possibilities to improve them and show how they can be effectively combined. 
Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4gram baseline, and most of them to a simple cache model as well. 
We propose a domain speciﬁc model for statistical machine translation. It is wellknown that domain speciﬁc language models perform well in automatic speech recognition. We show that domain speciﬁc language and translation models also beneﬁt statistical machine translation. However, there are two problems with using domain speciﬁc models. The ﬁrst is the data sparseness problem. We employ an adaptation technique to overcome this problem. The second issue is domain prediction. In order to perform adaptation, the domain must be provided, however in many cases, the domain is not known or changes dynamically. For these cases, not only the translation target sentence but also the domain must be predicted. This paper focuses on the domain prediction problem for statistical machine translation. In the proposed method, a bilingual training corpus, is automatically clustered into sub-corpora. Each sub-corpus is deemed to be a domain. The domain of a source sentence is predicted by using its similarity to the sub-corpora. The predicted domain (sub-corpus) speciﬁc language and translation models are then used for the translation decoding. This approach gave an improvement of 2.7 in BLEU (Papineni et al., 2002) score on the IWSLT05 Japanese to English evaluation corpus (improving the score from 52.4 to 55.1). This is a substantial gain and indicates the validity of the proposed bilingual cluster based models.  
We address the problem of training the free parameters of a statistical machine translation system. We show signiﬁcant improvements over a state-of-the-art minimum error rate training baseline on a large ChineseEnglish translation task. We present novel training criteria based on maximum likelihood estimation and expected loss computation. Additionally, we compare the maximum a-posteriori decision rule and the minimum Bayes risk decision rule. We show that, not only from a theoretical point of view but also in terms of translation quality, the minimum Bayes risk decision rule is preferable. 
Reordering model is important for the statistical machine translation (SMT). Current phrase-based SMT technologies are good at capturing local reordering but not global reordering. This paper introduces syntactic knowledge to improve global reordering capability of SMT system. Syntactic knowledge such as boundary words, POS information and dependencies is used to guide phrase reordering. Not only constraints in syntax tree are proposed to avoid the reordering errors, but also the modification of syntax tree is made to strengthen the capability of capturing phrase reordering. Furthermore, the combination of parse trees can compensate for the reordering errors caused by single parse tree. Finally, experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based SMT system. 
In this paper, we present a machine learning approach to the identiﬁcation and resolution of Chinese anaphoric zero pronouns. We perform both identiﬁcation and resolution automatically, with two sets of easily computable features. Experimental results show that our proposed learning approach achieves anaphoric zero pronoun resolution accuracy comparable to a previous state-ofthe-art, heuristic rule-based approach. To our knowledge, our work is the ﬁrst to perform both identiﬁcation and resolution of Chinese anaphoric zero pronouns using a machine learning approach. 
This paper explores a parsimonious approach to Data-Oriented Parsing. While allowing, in principle, all possible subtrees of trees in the treebank to be productive elements, our approach aims at ﬁnding a manageable subset of these trees that can accurately describe empirical distributions over phrase-structure trees. The proposed algorithm leads to computationally much more tracktable parsers, as well as linguistically more informative grammars. The parser is evaluated on the OVIS and WSJ corpora, and shows improvements on efﬁciency, parse accuracy and testset likelihood. 
A lexical analogy is a pair of word-pairs that share a similar semantic relation. Lexical analogies occur frequently in text and are useful in various natural language processing tasks. In this study, we present a system that generates lexical analogies automatically from text data. Our system discovers semantically related pairs of words by using dependency relations, and applies novel machine learning algorithms to match these word-pairs to form lexical analogies. Empirical evaluation shows that our system generates valid lexical analogies with a precision of 70%, and produces quality output although not at the level of the best humangenerated lexical analogies. 
We present the idea of estimating semantic distance in one, possibly resource-poor, language using a knowledge source in another, possibly resource-rich, language. We do so by creating cross-lingual distributional proﬁles of concepts, using a bilingual lexicon and a bootstrapping algorithm, but without the use of any sense-annotated data or word-aligned corpora. The cross-lingual measures of semantic distance are evaluated on two tasks: (1) estimating semantic distance between words and ranking the word pairs according to semantic distance, and (2) solving Reader’s Digest ‘Word Power’ problems. In task (1), cross-lingual measures are superior to conventional monolingual measures based on a wordnet. In task (2), cross-lingual measures are able to solve more problems correctly, and despite scores being affected by many tied answers, their overall performance is again better than the best monolingual measures. 
Many systems for tasks such as question answering, multi-document summarization, and information retrieval need robust numerical measures of lexical relatedness. Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph. By contrast, we propose a new model of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the entire graph. Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics. We treat the graph as a Markov chain and compute a word-speciﬁc stationary distribution via a generalized PageRank algorithm. Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions. In our experiments, the resulting relatedness measure is the WordNet-based measure most highly correlated with human similarity judgments by rank ordering at ρ = .90. 
This paper proposes the use of Lexicalized Tree-Adjoining Grammar (LTAG) formalism as an important additional source of features for the Semantic Role Labeling (SRL) task. Using a set of one-vs-all Support Vector Machines (SVMs), we evaluate these LTAG-based features. Our experiments show that LTAG-based features can improve SRL accuracy signiﬁcantly. When compared with the best known set of features that are used in state of the art SRL systems we obtain an improvement in F-score from 82.34% to 85.25%. 
 We propose a novel method for Japanese dependency analysis, which is usually reduced to the construction of a dependency tree. In deterministic approaches to this task, dependency trees are constructed by series of actions of attaching a bunsetsu chunk to one of the nodes in the tree being constructed. Conventional techniques select the node based on whether the new bunsetsu chunk and each node in the trees are in a parent-child relation or not. However, tree structures include relations between two nodes other than the parent-child relation. Therefore, we use ancestor-descendant relations in addition to parent-child relations, so that the added redundancy helps errors be corrected. Experimental results show that the proposed method achieves higher accuracy. 
We propose a sequence-alignment based method for detecting and disambiguating coordinate conjunctions. In this method, averaged perceptron learning is used to adapt the substitution matrix to the training data drawn from the target language and domain. To reduce the cost of training data construction, our method accepts training examples in which complete word-by-word alignment labels are missing, but instead only the boundaries of coordinated conjuncts are marked. We report promising empirical results in detecting and disambiguating coordinated noun phrases in the GENIA corpus, despite a relatively small number of training examples and minimal features are employed. 
In this paper, we describe a new algorithm for recovering WH-trace empty nodes. Our approach combines a set of hand-written patterns together with a probabilistic model. Because the patterns heavily utilize regular expressions, the pertinent tree structures are covered using a limited number of patterns. The probabilistic model is essentially a probabilistic context-free grammar (PCFG) approach with the patterns acting as the terminals in production rules. We evaluate the algorithm’s performance on gold trees and parser output using three different metrics. Our method compares favorably with state-of-the-art algorithms that recover WH-traces. 
Recent studies focussed on the question whether less-conﬁgurational languages like German are harder to parse than English, or whether the lower parsing scores are an artefact of treebank encoding schemes and data structures, as claimed by Ku¨bler et al. (2006). This claim is based on the assumption that PARSEVAL metrics fully reﬂect parse quality across treebank encoding schemes. In this paper we present new experiments to test this claim. We use the PARSEVAL metric, the Leaf-Ancestor metric as well as a dependency-based evaluation, and present novel approaches measuring the effect of controlled error insertion on treebank trees and parser output. We also provide extensive past-parsing crosstreebank conversion. The results of the experiments show that, contrary to Ku¨bler et al. (2006), the question whether or not German is harder to parse than English remains undecided. 
In this paper, we study the problem of automatically segmenting written text into paragraphs. This is inherently a sequence labeling problem, however, previous approaches ignore this dependency. We propose a novel approach for automatic paragraph segmentation, namely training Semi-Markov models discriminatively using a Max-Margin method. This method allows us to model the sequential nature of the problem and to incorporate features of a whole paragraph, such as paragraph coherence which cannot be used in previous models. Experimental evaluation on four text corpora shows improvement over the previous state-of-the art method on this task. 
This paper presents a method for categorizing named entities in Wikipedia. In Wikipedia, an anchor text is glossed in a linked HTML text. We formalize named entity categorization as a task of categorizing anchor texts with linked HTML texts which glosses a named entity. Using this representation, we introduce a graph structure in which anchor texts are regarded as nodes. In order to incorporate HTML structure on the graph, three types of cliques are deﬁned based on the HTML tree structure. We propose a method with Conditional Random Fields (CRFs) to categorize the nodes on the graph. Since the deﬁned graph may include cycles, the exact inference of CRFs is computationally expensive. We introduce an approximate inference method using Treebased Reparameterization (TRP) to reduce computational cost. In experiments, our proposed model obtained signiﬁcant improvements compare to baseline models that use Support Vector Machines. 
We introduce a technique for identifying the most salient participants in a discussion. Our method, MavenRank is based on lexical centrality: a random walk is performed on a graph in which each node is a participant in the discussion and an edge links two participants who use similar rhetoric. As a test, we used MavenRank to identify the most inﬂuential members of the US Senate using data from the US Congressional Record and used committee ranking to evaluate the output. Our results show that MavenRank scores are largely driven by committee status in most topics, but can capture speaker centrality in topics where speeches are used to indicate ideological position instead of inﬂuence legislation. 
One may need to build a statistical parser for a new language, using only a very small labeled treebank together with raw text. We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features, as in recent models for scoring dependency parses (McDonald et al., 2005). Drawing on Abney’s (2004) analysis of the Yarowsky algorithm, we perform bootstrapping by entropy regularization: we maximize a linear combination of conditional likelihood on labeled data and conﬁdence (negative Re´nyi entropy) on unlabeled data. In initial experiments, this surpassed EM for training a simple feature-poor generative model, and also improved the performance of a feature-rich, conditionally estimated model where EM could not easily have been applied. For our models and training sets, more peaked measures of conﬁdence, measured by Re´nyi entropy, outperformed smoother ones. We discuss how our feature set could be extended with cross-lingual or cross-domain features, to incorporate knowledge from parallel or comparable corpora during bootstrapping.  tion (Weischedel, 2004)1 and question answering (Peng et al., 2005). These systems rely on edges or paths in dependency parse trees to deﬁne their extraction patterns and classiﬁcation features. Parsing is also key to the latest advances in machine translation, which translate syntactic phrases (Galley et al., 2006; Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent ﬁt for bootstrapping, because the parse is often overdetermined by many redundant features.  
We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar—for example allowing ﬂexible word order, or insertion of lexical items— with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match ﬁgure reported by He and Young (2006). 
We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP). Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available. In addition to presenting a fully Bayesian model for the PCFG, we also develop an efﬁcient variational inference procedure. On synthetic data, we recover the correct grammar without having to specify its complexity in advance. We also show that our techniques can be applied to full-scale parsing applications by demonstrating its effectiveness in learning state-split grammars. 
We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER). Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the ﬁrst sentence of the entry, which can be thought of as a deﬁnition part. These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER. 
This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles. 
We present an information extraction system that decouples the tasks of ﬁnding relevant regions of text and applying extraction patterns. We create a self-trained relevant sentence classiﬁer to identify relevant regions, and use a semantic afﬁnity measure to automatically learn domain-relevant extraction patterns. We then distinguish primary patterns from secondary patterns and apply the patterns selectively in the relevant regions. The resulting IE system achieves good performance on the MUC-4 terrorism corpus and ProMed disease outbreak stories. This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training. 
This paper proposes a tree kernel with contextsensitive structured parse tree information for relation extraction. It resolves two critical problems in previous tree kernels for relation extraction in two ways. First, it automatically determines a dynamic context-sensitive tree span for relation extraction by extending the widely -used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT. Second, it proposes a context-sensitive convolution tree kernel, which enumerates both context-free and contextsensitive sub-trees by considering their ancestor node paths as their contexts. Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel. Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy’s convolution tree kernel. It also shows that our tree kernel achieves much better performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features. 
Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules. 
We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactiﬁed phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We ﬁnd that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 
We compare and contrast the strengths and weaknesses of a syntax-based machine translation model with a phrase-based machine translation model on several levels. We brieﬂy describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 
We achieved a state of the art performance in statistical machine translation by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 
In text categorization, term selection is an important step for the sake of both categorization accuracy and computational efﬁciency. Different dimensionalities are expected under different practical resource restrictions of time or space. Traditionally in text categorization, the same scoring or ranking criterion is adopted for all target dimensionalities, which considers both the discriminability and the coverage of a term, such as χ2 or IG. In this paper, the poor accuracy at a low dimensionality is imputed to the small average vector length of the documents. Scalable term selection is proposed to optimize the term set at a given dimensionality according to an expected average vector length. Discriminability and coverage are separately measured; by adjusting the ratio of their weights in a combined criterion, the expected average vector length can be reached, which means a good compromise between the speciﬁcity and the exhaustivity of the term subset. Experiments show that the accuracy is considerably improved at lower dimensionalities, and larger term subsets have the possibility to lower the average vector length for a lower computational cost. The interesting observations might inspire further investigations. 
In this paper, we analyze the effect of resampling techniques, including undersampling and over-sampling used in active learning for word sense disambiguation (WSD). Experimental results show that under-sampling causes negative effects on active learning, but over-sampling is a relatively good choice. To alleviate the withinclass imbalance problem of over-sampling, we propose a bootstrap-based oversampling (BootOS) method that works better than ordinary over-sampling in active learning for WSD. Finally, we investigate when to stop active learning, and adopt two strategies, max-confidence and min-error, as stopping conditions for active learning. According to experimental results, we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions. 
This paper proposes a framework for semi-supervised structured output learning (SOL), speciﬁcally for sequence labeling, based on a hybrid generative and discriminative approach. We deﬁne the objective function of our hybrid model, which is written in log-linear form, by discriminatively combining discriminative structured predictor(s) with generative model(s) that incorporate unlabeled data. Then, unlabeled data is used in a generative manner to increase the sum of the discriminant functions for all outputs during the parameter estimation. Experiments on named entity recognition (CoNLL-2003) and syntactic chunking (CoNLL-2000) data show that our hybrid model signiﬁcantly outperforms the stateof-the-art performance obtained with supervised SOL methods, such as conditional random ﬁelds (CRFs). 
In Sequential Viterbi Models, such as HMMs, MEMMs, and Linear Chain CRFs, the type of patterns over output sequences that can be learned by the model depend directly on the model’s structure: any pattern that spans more output tags than are covered by the models’ order will be very difﬁcult to learn. However, increasing a model’s order can lead to an increase in the number of model parameters, making the model more susceptible to sparse data problems. This paper shows how the notion of output transformation can be used to explore a variety of alternative model structures. Using output transformations, we can selectively increase the amount of contextual information available for some conditions, but not for others, thus allowing us to capture longer-distance consistencies while avoiding unnecessary increases to the model’s parameter space. The appropriate output transformation for a given task can be selected by applying a hill-climbing approach to heldout data. On the NP Chunking task, our hill-climbing system ﬁnds a model structure that outperforms both ﬁrst-order and secondorder models with the same input feature set. 
Speech recognition transcripts are far from perfect; they are not of sufﬁcient quality to be useful on their own for spoken document retrieval. This is especially the case for conversational speech. Recent efforts have tried to overcome this issue by using statistics from speech lattices instead of only the 1best transcripts; however, these efforts have invariably used the classical vector space retrieval model. This paper presents a novel approach to lattice-based spoken document retrieval using statistical language models: a statistical model is estimated for each document, and probabilities derived from the document models are directly used to measure relevance. Experimental results show that the lattice-based language modeling method outperforms both the language modeling retrieval method using only the 1-best transcripts, as well as a recently proposed lattice-based vector space retrieval method. 
Query segmentation is the process of taking a user’s search-engine query and dividing the tokens into individual phrases or semantic units. Identiﬁcation of these query segments can potentially improve both document-retrieval precision, by ﬁrst returning pages which contain the exact query segments, and document-retrieval recall, by allowing query expansion or substitution via the segmented units. We train and evaluate a machine-learned query segmentation system that achieves 86% segmentationdecision accuracy on a gold standard set of segmented noun phrase queries, well above recently published approaches. Key enablers of this high performance are features derived from previous natural language processing work in noun compound bracketing. For example, token association features beyond simple N-gram counts provide powerful indicators of segmentation. 
We present two machine learning approaches to information extraction from semi-structured documents that can be used if no annotated training data are available, but there does exist a database ﬁlled with information derived from the type of documents to be processed. One approach employs standard supervised learning for information extraction by artiﬁcially constructing labelled training data from the contents of the database. The second approach combines unsupervised Hidden Markov modelling with language models. Empirical evaluation of both systems suggests that it is possible to bootstrap a ﬁeld segmenter from a database alone. The combination of Hidden Markov and language modelling was found to perform best at this task. 
In this paper, we address the problem of extracting data records and their attributes from unstructured biomedical full text. There has been little effort reported on this in the research community. We argue that semantics is important for record extraction or finer-grained language processing tasks. We derive a data record template including semantic language models from unstructured text and represent them with a discourse level Conditional Random Fields (CRF) model. We evaluate the approach from the perspective of Information Extraction and achieve significant improvements on system performance compared with other baseline systems. 
In scientiﬁc literature, sentences that cite related work can be a valuable resource for applications such as summarization, synonym identiﬁcation, and entity extraction. In order to determine which equivalent entities are discussed in the various citation sentences, we propose aligning the words within these sentences according to semantic similarity. This problem is partly analogous to the problem of multiple sequence alignment in the biosciences, and is also closely related to the word alignment problem in statistical machine translation. In this paper we address the problem of multiple citation concept alignment by combining and modifying the CRF based pairwise word alignment system of Blunsom & Cohn (2006) and a posterior decoding based multiple sequence alignment algorithm of Schwartz & Pachter (2007). We evaluate the algorithm on hand-labeled data, achieving results that improve on a baseline. 
 This paper reports on the beneﬁts of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.  
We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level — may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.  Input word lemma part-of-speech morphology word class ...  Output word lemma part-of-speech morphology word class ...  Figure 1: Factored representations of input and output words incorporate additional annotation into the statistical translation model.  
Unknown words are a well-known hindrance to natural language applications. In particular, they drastically impact machine translation quality. An easy way out commercial translation systems usually offer their users is the possibility to add unknown words and their translations into a dedicated lexicon. Recently, Stroppa and Yvon (2005) have shown how analogical learning alone deals nicely with morphology in different languages. In this study we show that analogical learning offers as well an elegant and effective solution to the problem of identifying potential translations of unknown words. 
We present a probabilistic model of diachronic phonology in which individual word forms undergo stochastic edits along the branches of a phylogenetic tree. Our approach allows us to achieve three goals with a single uniﬁed model: (1) reconstruction of both ancient and modern word forms, (2) discovery of general phonological changes, and (3) selection among different phylogenies. We learn our model using a Monte Carlo EM algorithm and present quantitative results validating the model. 
We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively reﬁned using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simpliﬁed training process, our acoustic model achieves state-of-the-art results on phone classiﬁcation (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system. 
This paper describes ETK (Ensemble of Transformation-based Keys) a new algorithm for inducing search keys for name ﬁltering. ETK has the low computational cost and ability to ﬁlter by phonetic similarity characteristic of phonetic keys such as Soundex, but is adaptable to alternative similarity models. The accuracy of ETK in a preliminary empirical evaluation suggests that it is well-suited for phonetic ﬁltering applications such as recognizing alternative cross-lingual transliterations. 
Deterministic dependency parsers use parsing actions to construct dependencies. These parsers do not compute the probability of the whole dependency tree. They only determine parsing actions stepwisely by a trained classifier. To globally model parsing actions of all steps that are taken on the input sentence, we propose two kinds of probabilistic parsing action models that can compute the probability of the whole dependency tree. The tree with the maximal probability is outputted. The experiments are carried on 10 languages, and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser. 
It is possible to reduce the bulk of phrasetables for Statistical Machine Translation using a technique based on the signiﬁcance testing of phrase pair co-occurrence in the parallel corpus. The savings can be quite substantial (up to 90%) and cause no reduction in BLEU score. In some cases, an improvement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrasetable smoothing is employed. 
A major engineering challenge in statistical machine translation systems is the efﬁcient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a sufﬁx array as an efﬁcient index to quickly lookup and extract rules on the ﬂy. Hierarchical phrasebased translation introduces the added wrinkle of source phrases with gaps. Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence. We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-ﬂy lookup feasible for source phrases with gaps. 
This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination. We give empirical evidence that the systems to be combined should be of similar quality and need to be almost uncorrelated in order to be beneﬁcial for system combination. Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the bestranked machine translation engines in the 2006 NIST machine translation evaluation. 
We present a method for learning to find English to Chinese transliterations on the Web. In our approach, proper nouns are expanded into new queries aimed at maximizing the probability of retrieving transliterations from existing search engines. The method involves learning the sublexical relationships between names and their transliterations. At run-time, a given name is automatically extended into queries with relevant morphemes, and transliterations in the returned search snippets are extracted and ranked. We present a new system, TermMine, that applies the method to find transliterations of a given name. Evaluation on a list of 500 proper names shows that the method achieves high precision and recall, and outperforms commercial machine translation systems. 
It has been widely observed that different NLP applications require different sense granularities in order to best exploit word sense distinctions, and that for many applications WordNet senses are too ﬁne-grained. In contrast to previously proposed automatic methods for sense clustering, we formulate sense merging as a supervised learning problem, exploiting human-labeled sense clusterings as training data. We train a discriminative classiﬁer over a wide variety of features derived from WordNet structure, corpus-based evidence, and evidence from other lexical resources. Our learned similarity measure outperforms previously proposed automatic methods for sense clustering on the task of predicting human sense merging judgments, yielding an absolute F-score improvement of 4.1% on nouns, 13.6% on verbs, and 4.0% on adjectives. Finally, we propose a model for clustering sense taxonomies using the outputs of our classiﬁer, and we make available several automatically sense-clustered WordNets of various sense granularities. 
This paper presents a novel approach for exploiting the global context for the task of word sense disambiguation (WSD). This is done by using topic features constructed using the latent dirichlet allocation (LDA) algorithm on unlabeled data. The features are incorporated into a modiﬁed na¨ıve Bayes network alongside other features such as part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic patterns. In both the English all-words task and the English lexical sample task, the method achieved signiﬁcant improvement over the simple na¨ıve Bayes classiﬁer and higher accuracy than the best ofﬁcial scores on Senseval-3 for both task. 
We develop latent Dirichlet allocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable. We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpus and learning the domains in which to consider each word. Using the WORDNET hierarchy, we embed the construction of Abney and Light (1999) in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts. 
This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions (MWEs) for robust grammar engineering. First we investigate the hypothesis that MWEs can be detected by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: mutual information (MI), χ2 and permutation entropy (PE). Our overall conclusion is that at least two measures, MI and PE, seem to diﬀerentiate MWEs from non-MWEs. We then investigate the inﬂuence of the size and quality of diﬀerent corpora, using the BNC and the Web search engines Google and Yahoo. We conclude that, in terms of language usage, web generated corpora are fairly similar to more carefully built corpora, like the BNC, indicating that the lack of control and balance of these corpora are probably compensated by their size. Finally, we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources. We argue that such a process improves qualitatively, if a more compositional approach to grammar/lexicon automated extension is adopted. 
In this paper, we present an election prediction system (Crystal) based on web users’ opinions posted on an election prediction website. Given a prediction message, Crystal first identifies which party the message predicts to win and then aggregates prediction analysis results of a large amount of opinions to project the election results. We collect past election prediction messages from the Web and automatically build a gold standard. We focus on capturing lexical patterns that people frequently use when they express their predictive opinions about a coming election. To predict election results, we apply SVM-based supervised learning. To improve performance, we propose a novel technique which generalizes n-gram feature patterns. Experimental results show that Crystal significantly outperforms several baselines as well as a non-generalized n-gram approach. Crystal predicts future elections with 81.68% accuracy. 
The technology of opinion extraction allows users to retrieve and analyze people’s opinions scattered over Web documents. We deﬁne an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this deﬁnition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues. Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks. 
Recognizing polarity requires a list of polar words and phrases. For the purpose of building such lexicon automatically, a lot of studies have investigated (semi-) unsupervised method of learning polarity of words and phrases. In this paper, we explore to use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the extracted polar sentences. The key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall. In order to compensate for the low recall, we used massive collection of HTML documents. Thus, we could prepare enough polar sentence corpus. 
This paper discusses automatic determination of case in Arabic. This task is a major source of errors in full diacritization of Arabic. We use a gold-standard syntactic tree, and obtain an error rate of about 4.2%, with a machine learning based system outperforming a system using hand-written rules. A careful error analysis suggests that when we account for annotation errors in the gold standard, the error rate drops to 0.8%, with the hand-written rules outperforming the machine learning-based system. 
We present in this paper methods to improve HMM-based part-of-speech (POS) tagging of Mandarin. We model the emission probability of an unknown word using all the characters in the word, and enrich the standard left-to-right trigram estimation of word emission probabilities with a right-to-left prediction of the word by making use of the current and next tags. In addition, we utilize the RankBoost-based reranking algorithm to rerank the N-best outputs of the HMMbased tagger using various n-gram, morphological, and dependency features. Two methods are proposed to improve the generalization performance of the reranking algorithm. Our reranking model achieves an accuracy of 94.68% using n-gram and morphological features on the Penn Chinese Treebank 5.2, and is able to further improve the accuracy to 95.11% with the addition of dependency features. 
Part of speech tagging is a fundamental component in many NLP systems. When taggers developed in one domain are used in another domain, the performance can degrade considerably. We present a method for developing taggers for new domains without requiring POS annotated text in the new domain. Our method involves using raw domain text and identifying related words to form a domain specific lexicon. This lexicon provides the initial lexical probabilities for EM training of an HMM model. We evaluate the method by applying it in the Biology domain and show that we achieve results that are comparable with some taggers developed for this domain. 
This paper reports a hybridization experi­ ment, where a baseline ML dependency pars­ er, LingPars, was allowed access to Con­ straint Grammar analyses provided by a rule­ based parser (EngGram) for the same data. Descriptive compatibility issues and their in­ fluence on performance are discussed. The hybrid system performed considerably better than its ML baseline, and proved more ro­ bust than the latter in the domain adaptation task, where it was the best­scoring system in the open class for the chemical test data, and the best overall system for the CHILDES test data. 
We present an adaptation of constraint satisfaction inference (Canisius et al., 2006b) for predicting dependency trees. Three different classiﬁers are trained to predict weighted soft-constraints on parts of the complex output. From these constraints, a standard weighted constraint satisfaction problem can be formed, the solution to which is a valid dependency tree. 
We present a two-stage multilingual dependency parsing system submitted to the Multilingual Track of CoNLL-2007. The parser ﬁrst identiﬁes dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem. We describe the features used in each stage. For four languages with different values of ROOT, we design some special features for the ROOT labeler. Then we present evaluation results and error analyses focusing on Chinese. 
Following (Blitzer et al., 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al., 2005). To induce the correspondences among dependency edges from different domains, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them. Three binary linear classiﬁers were trained to predict the existence of a preposition, etc, on unlabeled data and we used singular value decomposition to induce new features. During the training, the parser was trained with these additional features in addition to these described in (McDonald et al., 2005). We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003). 
In this paper, we present a three-step multilingual dependency parser based on a deterministic shift-reduce parsing algorithm. Different from last year, we separate the root-parsing strategy as sequential labeling task and try to link the neighbor word dependences via a near neighbor parsing. The outputs of the root and neighbor parsers were encoded as features for the shift-reduce parser. In addition, the learners we used for the two parsers and the shift-reduce parser are quite different (conditional random fields and the modified finite-Newton method support vector machines). We found that our method could benefit from the two-preprocessing stages. To speed up training, in this year, we employ the MFN-SVM (modified finite-Newton method support vector machines) which can be learned in linear time. The experimental results show that our method achieved the middle rank over the 23 teams. We expect that our method could be further improved via well-tuned parameter validations for different languages. 
The AVENUE machine translation system is designed for resource poor scenarios in which parallel corpora are not available. In this situation, parallel corpora are created by bilingual consultants who translate an elicitation corpus into their languages. We have described the elicitation corpus in other publications. This paper is concerned with evaluation of the elicitation corpus: is it suitably designed so that a bilingual consultant can produce reliable data without the supervision of a linguist? We evaluated two translations of the English elicitation corpus, one into Thai and one into Bengali. Two types of evaluation were conducted: an error analysis of the translations produced by the Thai and Bengali consultants, and a comparison of Example Based MT trained on the original translations and on corrected translations. 
Orthographic variation can be a serious problem for many natural language-processing applications. Japanese in particular contains orthographic variation, because the large quantity of transliteration from other languages causes many possible spelling variations. To manage this problem, this paper proposes a support vector machine (SVM)-based classiﬁer that can determine whether two terms are equivalent. We automatically collected both positive examples (sets of equivalent term pairs) and negative examples (sets of inequivalent term pairs). Experimental results yielded high levels of accuracy (87.8%), demonstrating the feasibility of the proposed approach. 
This paper presents a generative probabilistic dependency model of parallel texts that can be used for statistical machine translation and parallel parsing. Unlike syntactic models that are based on context-free dependency grammars, the dependency model proposed in this paper is based on a sophisticated notion of dependency grammar that is capable of modelling non-projective word order and island constraints, the complementadjunct distinction, as well as deletions and additions in translations. 
We present comparative empirical evidence arguing that a generalized phrase sense disambiguation approach better improves statistical machine translation than ordinary word sense disambiguation, along with a data analysis suggesting the reasons for this. Standalone word sense disambiguation, as exempliﬁed by the Senseval series of evaluations, typically deﬁnes the target of disambiguation as a single word. But in order to be useful in statistical machine translation, our studies indicate that word sense disambiguation should be redeﬁned to move beyond the particular case of single word targets, and instead to generalize to multi-word phrase targets. We investigate how and why the phrase sense disambiguation approach—in contrast to recent efforts to apply traditional word sense disambiguation to SMT—is able to yield statistically signiﬁcant yimprovements in translation quality even under large data conditions, and consistently improve SMT across both IWSLT and NIST Chinese-English text translation tasks. We discuss architectural issues raised by this change of perspective, and consider the new model architecture necessitated by the phrase sense disambiguation approach. This material is based upon work supported in part by  
This paper aims at providing a reliable method for measuring the correlations between different scores of evaluation metrics applied to machine translated texts. A series of examples from recent MT evaluation experiments are first discussed, including results and data from the recent French MT evaluation campaign, CESTA, which is used here. To compute correlation, a set of 1,500 samples for each system and each evaluation metric are created using bootstrapping. Correlations between metrics, both automatic and applied by human judges, are then computed over these samples. The results confirm the previously observed correlations between some automatic metrics, but also indicate a lack of correlation between human and automatic metrics on the CESTA data, which raises a number of questions regarding their validity. In addition, the roles of the corpus size and of the selection procedure for bootstrapping (low vs. high scores) are also examined. 
This paper proposes an EBMT method based on finite automata state transfer generation. In this method, first some links from the fragments in the input sentence to the fragments in the target sentence of the selected example are built. Then some predefined states are assigned to these links according to their link types. Finally, taking these links and their corresponding states as inputs, a finite automaton is constructed and the translation result is generated in a finite automata state transfer manner. This method can be easily replicated, and does not need too much complicated parsers either. Based on this method, we built a Chinese-Japanese bidirectional EBMT system to evaluate the proposed method, and experimental results indicate that the proposed method is effective. 
To explore the potential application of semantic roles in structural machine translation, we propose to study the automatic learning of English-Chinese bilingual predicate argument structure mapping. We describe ARG ALIGN, a new model for learning bilingual semantic frames that employs monolingual Chinese and English semantic parsers to learn bilingual semantic role mappings with 72.45% Fscore, given an unannotated parallel corpus. We show that, contrary to a common preconception, our ARG ALIGN model is superior to a semantic role projection model, SYN ALIGN, which reaches only a 46.63% F-score by assuming semantic parallelism in bilingual sentences. We present experimental data explaining that this is due to crosslingual mismatches between argument structures in English and Chinese at 17.24% of the time. This suggests that, in any potential application to enhance machine translation with semantic structural mapping, it may be preferable to employ independent automatic semantic parsers on source and target languages, rather than assuming semantic role parallelism.  
Parallel treebanks, which comprise paired source-target parse trees aligned at sub-sentential level, could be useful for many applications, particularly data-driven machine translation. In this paper, we focus on how translational divergences are captured within a parallel treebank using a fully automatic statistical tree-to-tree aligner. We observe that while the algorithm performs well at the phrase level, performance on lexical-level alignments is compromised by an inappropriate bias towards coverage rather than precision. This preference for high precision rather than broad coverage in terms of expressing translational divergences through tree-alignment stands in direct opposition to the situation for SMT word-alignment models. We suggest that this has implications not only for tree-alignment itself but also for the broader area of induction of syntaxaware models for SMT. 
We propose a method of extracting phrasal alignments from comparable corpora by using an extended phrase-based joint probability model for statistical machine translation (SMT). Our method does not require preexisting dictionaries or splitting documents into sentences in advance. By checking each alignment for its reliability by using log-likelihood ratio statistics while searching for optimal alignments, our method aims to produce phrasal alignments for only parallel parts of the comparable corpora. Experimental result shows that our method achieves about 0.8 in precision of phrasal alignment extraction when using 2,000 Japanese-English document pairs as training data. 
Most phrase-based statistical machine translation decoders rely on a dynamic programming technique for maximizing a combination of models, including one or several language models and translation tables. One implication of this choice is the design of a scoring function that can be computed incrementally on partial translations, a restriction a search engine using a complete-state formulation does not have. In this paper, we present experiments we conducted with a simple, yet effective greedy search engine. In particular, we show that when seeded with the translations produced by a stateof-the-art beam search decoder, it produces an output of signiﬁcantly higher quality than the latter taken alone, as measured by automatic metrics. 
We introduce an adaptable monolingual chunking approach–AlignmentGuided Chunking (AGC)–which makes use of knowledge of word alignments acquired from bilingual corpora. Our approach is motivated by the observation that a sentence should be chunked diﬀerently depending the foreseen end-tasks. For example, given the diﬀerent requirements of translation into (say) French and German, it is inappropriate to chunk up an English string in exactly the same way as preparation for translation into one or other of these languages. We test our chunking approach on two language pairs: French– English and German–English, where these two bilingual corpora share the same English sentences. Two chunkers trained on French–English (FE-Chunker ) and German–English (DE-Chunker ) respectively are used to perform chunking on the same English sentences. We construct two test sets, each suitable for French– English and German–English respectively. The performance of the two chunkers is evaluated on the appropriate test set and with one reference translation only, we report Fscores of 32.63% for the FE-Chunker  and 40.41% for the DE-Chunker. 
This paper describes a novel approach to syntactically-informed evaluation of machine translation (MT). Using a statistical, treebanktrained parser, we extract word-word dependencies from reference translations and then compile these dependencies into a representation that allows candidate translations to be evaluated by string comparisons, as is done in n-gram approaches to MT evaluation. This approach gains the beneﬁt of syntactic analysis of the reference translations, but avoids the need to parse potentially noisy candidate translations. Preliminary experiments using 15,242 judgments of reference-candidate pairs from translations of Chinese newswire text show that the correlation of our approach with human judgments is only slightly lower than other reported results. With the addition of multiple reference translations, however, performance improves markedly. These  results are encouraging, especially given that our system is a prototype and makes no essential use of synonymy, paraphrasing or inﬂectional morphological information, all of which would be easy to add. 
In this paper, we present a Japanese→English machine translation system that combines rule-based and statistical translation. Our system is unique in that all of its components are freely available as open source software. We describe the development of the rule-based translation engine including transfer rule acquisition from an open bilingual dictionary. We also show how translations from both translation engines are combined through a simple ranking mechanism and compare their outputs. 
We present a hybrid MT architecture, combining state-of-the-art linguistic processing with advanced stochastic techniques. Grounded in a theoretical reﬂection on the division of labor between rule-based and probabilistic elements in the MT task, we summarize per-component approaches to ranking, including empirical results when evaluated in isolation. Combining component-internal scores and a number of additional sources of (probabilistic) information, we explore discriminative re-ranking of n-best lists of candidate translations through an eclectic combination of knowledge sources, and provide evaluation results for various conﬁgurations. 
This paper presents a method to predict human assessments of machine translation (MT) quality based on the combination of binary classiﬁers using a coding matrix. The multiclass categorization problem is reduced to a set of binary problems that are solved using standard classiﬁcation learning algorithms trained on the results of multiple automatic evaluation metrics. Experimental results using a large-scale humanannotated evaluation corpus show that the decomposition into binary classiﬁers achieves higher classiﬁcation accuracies than the multiclass categorization problem. In addition, the proposed method achieves a higher correlation with human judgments on the sentence-level compared to standard automatic evaluation measures. 
 for more graceful degradation through a form of structural generalization.  In this work I look at two different paradigms of Example-Based Machine Translation (EBMT). I combine the strengths of these two systems and build a new EBMT engine that combines sub-phrasal matching with structural templates. This synthesis results in higher translation quality and more graceful degradation, yielding 1.5% to 7.5% relative improvement in BLEU scores. 
In this paper we describe a word reordering strategy for statistical machine translation that reorders the source side based on Part of Speech (POS) information. Reordering rules are learned from the word aligned corpus. Reordering is integrated into the decoding process by constructing a lattice, which contains all word reorderings according to the reordering rules. Probabilities are assigned to the different reorderings. On this lattice monotone decoding is performed. This reordering strategy is compared with our previous reordering strategy, which looks at all permutations within a sliding window. We extend reordering rules by adding context information. Phrase translation pairs are learned from the original corpus and from a reordered source corpus to better capture the reordered word sequences at decoding time. Results are presented for English → Spanish and German ↔ English translations, using the European Parliament Plenary Sessions corpus. 
This paper focuses on the inference of structural transfer rules for shallow-transfer machine translation (MT). Transfer rules are generated from alignment templates, like those used in statistical MT, that have been extracted from parallel corpora and extended with a set of restrictions that control their application. The experiments conducted using the open-source MT platform Apertium show that there is a clear improvement in translation quality as compared to word-for-word translation (when no transfer rules are used), and that the resulting translation quality is very close to the one obtained using hand-coded transfer rules. The method we present is entirely unsupervised and beneﬁts from information in the rest of modules of the MT system in which the inferred rules are applied. 
In Statistical Machine Translation (SMT), one of the main problems they are confronted with is the problem stemming from the diﬀerent word order that diﬀerent languages imply. Most works addressing this issue centre their eﬀort in pairs of languages involving Arabic, Japanese or Chinese because of their utmost diﬀerent origin with respect to western languages. However, Basque is also a language with an extremely diﬀerent word order with respect to most other European languages, linguists being unable to determine its origins with certainty. Hence, SMT systems which do not tackle the reordering problem in any way are mostly unable to yield satisfactory results. In this work, a novel source sentence reordering technique is presented, based on monotonized alignments and n-best lists, endorsed by very promissing results obtained from a Basque-Spanish translation task. 
METIS-II, the MT system presented in this paper, does not view translation as a transfer process between a source language (SL) and a target one (TL), but rather as a matching procedure of patterns within a language pair. More specifically, translation is considered to be an assignment problem, i.e. a problem of discovering each time the best matching patterns between SL and TL, which the system is called to solve by employing patternmatching techniques. Most importantly, however, METIS-II is innovative because it does not need bilingual corpora for the translation process, but exclusively relies on monolingual corpora of the target language. 
This paper concerns the use of spoken language translation as well as other technologies to support communication between clinicians and patients where the latter have limited proficiency in the majority language. The paper explores some theoretical and methodological issues, in particular the question of whether it is the patient or clinician who should be seen as the primary user of such software, and whether for certain scenarios more simple technology is preferable, especially given the huge overheads involved in developing SLT systems for under-resourced languages. A range of solutions are discussed. 
 In this paper, we describe the ﬁrst data-driven automatic sign-languageto-speech translation system. While both sign language (SL) recognition and translation techniques exist, both use an intermediate notation system not directly intelligible for untrained users. We combine a SL recognizing framework with a state-of-the-art phrase-based machine translation (MT) system, using corpora of both American Sign Language and Irish Sign Language data. In a set of experiments we show the overall results and also illustrate the importance of including a vision-based knowledge source in the development of a complete SL translation system. 
Automatic evaluation metrics are often used to compare the quality of different systems. However, a small difference between the scores of two systems does not necessary reﬂect a real difference between their performance. Because such a difference can be signiﬁcant or only due to chance, it is inadvisable to use a hard ranking to represent the evaluation of multiple systems. In this paper, we propose a clusterbased representation for quality ranking of Machine Translation systems. A comparison of rankings produced by clustering based on automatic MT evaluation metrics with those based on human judgements shows that such interpretation of automatic metric scores provides dependable means of ordering MT systems with respect to their quality. We report experimental results comparing clusterings produced by BLEU, NIST, METEOR, and GTM with those derived from human judgement (of adequacy and ﬂuency) on the IWSLT-2006 evaluation campaign data. 
In this paper, we introduce contextinformed features in a log-linear phrase-based SMT framework; these features enable us to exploit source similarity in addition to target similarity modeled by the language model. We present a memory-based classiﬁcation framework that enables the estimation of these features while avoiding sparseness problems. We evaluate the performance of our approach on Italian-to-English and Chineseto-English translation tasks using a state-of-the-art phrase-based SMT system, and report signiﬁcant improvements for both BLEU and NIST scores when adding the context-informed features. 
A novel approach is presented for extracting syntactically motivated phrase alignments. In this method we can incorporate conventional resources such as dictionaries and grammar rules into a statistical optimization framework for phrase alignment. The method extracts bilingual phrases by incrementally merging adjacent words or phrases on both source and target language side in accordance with a global statistical metric. The extracted phrases achieve a maximum F-measure of over 80 with respect to human judged phrase alignments. The extracted phrases used as training corpus for a phrase-based SMT shows better cross-domain portability over conventional SMT framework. 
This paper concerns the misuse of online machine translation (MT) systems for lexical look-up, as if they were bilingual dictionaries. Following a review of the literature on online dictionaries, the paper reports (part of) a survey carried out among 104 university students in the United Kingdom investigating their usage of free online MT services. This paper focuses in particular on the widespread use of these MT tools for a purpose that they were not designed for, i.e. the translation of single lexical items. The 104 respondents (from an original survey of 280) had used web-based MT services in the past, and 65 of these (62.5%) reported using them for single-word lookup. This finding suggests that designers and developers of online MT services should seriously consider taking a proactive approach by treating single-word translation requests as dictionary look-up rather than translation, and/or raising the awareness of users with regard to the most (in)appropriate ways of using web-based MT software. The paper argues that it would be in the interests of those who have a stake in offering and promoting MT in the online environment (e.g. system designers, developers, and ultimately the MT vendors themselves) to manage the expectations of naïve users. 1. Introduction and background 1.1 Machine translation and lexicography The two areas of machine translation (MT) and lexicography have historically been closely linked (Steffens, 1995; Wanner, 1996), and lexicography has traditionally played a key role in the development of successful operational MT systems with the provision of machine-readable lexical components (Meekhof & Clements, 2000; Gdaniec & Manandise, 2002; Koeling et al., 2003; Zajac et al., 2003). Producing lexicographic resources and rules for MT systems is a laborious and expensive process (Kilgarriff & Tugwell, 2001: 187), and a number of approaches have been proposed in MTrelated research and development to overcome the challenges posed by what Farwell et al. (1992: 532) among others have called the “lexical acquisition bottleneck”. These consist, for example, in reusing 
This paper presents a prototype MT system which does not make the distinction between a dictionary, a sub-sentential aligned parallel corpus, and post-edited information (translators output) like a translation memory. The system is based on the METIS-approach (Vandeghinste et al, 2006), and uses an XML-based dictionary format in which not only simple word-to-word translations can be included, but which also contains complex dictionary entries, including discontinuous entries, like idioms and proverbs. The presented prototype is a system that automatically adapts its dictionary and target language corpus depending on the post-edited output as made by the users of the system, and will therefore have a learning curve in its performance. 
We present the design and evaluation of a novel software application intended to help translators with rendering problematic expressions from the general lexicon. It does this dynamically by first generalising the problem expression in the source language and then searching for possible translations in a large comparable corpus. These candidate solutions are ranked and presented to the user. The method relies on measures of distributional similarity and on bilingual dictionaries. It outperforms established techniques for extracting translation equivalents from parallel corpora. The interface to the system is available at: http://corpus.leeds.ac.uk/assist/v05/. 
In this paper we will present a set of terminology extraction tools that are distributed under a Free Software License, so that users can freely download, use, distribute and modify them to meet their needs. The tools are mainly programmed in Perl and they will work under different platforms, such as Windows or Linux. These terminology extraction tools will help freelance translators, translation agencies and companies to find the best translation of a term or to build monolingual or multilingual terminological glossaries. Moreover, translators, correctors and terminologists can use The Free Terminology Extraction Suite to create a terminological database for a specialist domain so as to automatically obtain a list of domain-specific lexical units (potential terms) with their equivalent translations from bilingual corpora of domain-specific documents. 
This paper is inspired by an event that took place 15 years ago. Recently graduated, I had eagerly started up a small consultancy company in Sweden with the aim of producing new translation software. The first collaborative project I was involved in was to offer consultancy around a translation memory called Eurolang Optimizer, where my task would involve producing Swedish localisation. The translation aid, as all other translation memories on the market, had as its main feature a database where it would store previous translations and later offer them as suggestions when the same source sentence was found in subsequent translation work. 
In this paper we will focus on the compilation of virtual or ad-hoc corpora2 (i.e. corpora mined from electronic sources for a specific task) and the tools enabling their exploitation by translators. We will demonstrate how a step-by-step approach to building an adequate and/or representative corpus from resources in the Internet works in practice. Corpus design criteria and qualitative issues will be taken into account. For illustrative purposes, we will discuss a translation assignment of a specialised nature. Real examples will be presented that show how to mine a corpus and how to use it in order to meet translators’ needs as far as terminology, documentation, target text conventions and other constraints are concerned. We will illustrate how language service providers can use freeware concordances to search for translation equivalents in comparable and parallel corpora. 2. Using corpora in translation practice Resources and tools for translators lie within the realm of the broader field of translation technologies. As forwarded by standard introductory books on this topic (cf. Austermühl, 2001; Bowker, 2002; Quah, 2005), translation technologies cover machine translation systems, translation memory systems and localisation software, controlled languages, term extraction and terminology management systems, project management systems, globalisation management systems, corpus compilation and exploitation, as well as other electronic resources and information technologies. 
This paper focuses on the particular use of spoken language translation (SLT) technology in the medical domain, in particular to assist communication between patients with limited English and healthcare providers. The paper points out that the pathway to healthcare for such patients extends beyond the focal point of a doctor-patient dialogue with a GP as it is conventionally portrayed, to include interaction at various stages with a range of medical specialists and nonspecialists. The translation needs (both spoken and text) vary accordingly. The paper then critically reviews work done so far on SLT in the medical domain, in particular considering the most appropriate set up - who is the principal user, doctor or patient? - and at some factors in the deployment of multimodal interfaces to support the speech input. Finally the paper discusses whether SLT is really the most appropriate technology, and discuss some of the barriers to implementation, especially considering the fact that in many cases the languages spoken by the kinds of patients we are targeting happen also to be languages of least interest to commercial developers of language technologies. 
Key concepts ......................................................................................................................2 The actual state of translation quality assurance tools..................................................................4 QA tools: retrospection and state of the art...........................................................................4 QA tools classification..........................................................................................................5 QA checks classification ......................................................................................................6 Survey description......................................................................................................................7 Survey results and analysis ........................................................................................................7 QA tools user profiles...........................................................................................................7 Working Environment and Translatable Content....................................................................9 Usage of Translation Memory Tools and Quality Assurance Practices ................................. 12 QA Automation Tools Evaluation and Expectations.............................................................. 22 Benchmarking available quality assurance tools........................................................................ 28 QA capabilities revealed .................................................................................................... 28 Déjà Vu............................................................................................................................ 33 SDLX QA Check .............................................................................................................. 33 Star Transit ....................................................................................................................... 33 SDL Trados QA Checker ................................................................................................... 34 Wordfast ........................................................................................................................... 34 ErrorSpy ........................................................................................................................... 34 QA Distiller ....................................................................................................................... 35 XBench............................................................................................................................. 35 Error Level Comparison ..................................................................................................... 36 Looking into the future .............................................................................................................. 38 Conclusion ............................................................................................................................... 38 Acknowledgements ..................................................................................................................39 References ..............................................................................................................................39 
The article describes a new way of constructing rule-based machine translation systems (RBMT). RBMT systems are currently among the best performing machine translation systems. Most of the "big named" machine translation systems (Systran, 2007)(Promt, 2007) belong to this category, but these systems have a big drawback; construction of such systems demands a great amount of time and resources, thus resulting very expensive. The article describes methods that automate parts of the construction process. The methods were evaluated on a case study: construction of a fully functional machine translation system of closely related language pair Slovene - Serb. Slovene and Serbian language belong to the group of southern Slavic languages that were spoken mostly in former Yugoslavia. Slovenian language is mostly spoken in Slovenia, Serbian language is mostly spoken in Serbia and in Montenegro. The languages share common roots and even more importantly they share common recent historical environment, these languages were spoken in the same country, even taught in schools as languages of the surroundings. Economies of all three states are closely connected and younger generations, the post-yugoslavia breakage generations, have difficulties in mutual communication, so there is quite big interest in construction of such translation system. The system is based on Apertium (Armentano-Oller et al., 2007) (Corbí-Bellot et al., 2005), an opensource RBMT toolkit. Apertium uses a shallow-transfer machine translation engine which processes the input text in stages, as in an assembly line: de-formatting, morphological analysis, part-of-speech disambiguation, shallow structural transfer, lexical transfer, morphological generation, and re-formatting. The data needed by the presented stages can be grouped into three categories: monolingual dictionaries used by morphological analysis and morphological generation, bilingual dictionaries used in lexical transfer and structural transfer rules used in structural transfer. Each group's data creation was addressed by a particular method; monolingual dictionaries were constructed using bilingual dictionary data and applying automatic paradigm tagging techniques; bilingual dictionary was constructed using available bilingual word-list but a few methods for automatic bilingual dictionary construction were investigated; a method for automatic structural shallow-transfer rule construction (Sánchez-Martínez et al., 2006) was used to construct a set of structural transfer rules. 
This paper takes a look at employability in higher education. It presents case studies illustrating efforts to increase the employability of graduates of the BA and MA programmes of the Centre for Translation Studies at the University of Vienna. Measures on the curricular and planning levels encompass defining learning outcomes oriented towards professional requirements and encouraging didactic approaches, including e-learning, that promote transferable and translation technology skills. Measures at the teaching level include individual and group reflection processes about what relevant skills are and how they are acquired and introducing personal development planning and professional orientation to future graduates. The method and tool used for these latter purposes is the e-portfolio. The paper will also take an outlook at further steps and the required preconditions at curricular level. Introduction "Students of the BA programme Trans-cultural Communication at the University of Vienna acquire basic knowledge of research methods and results as well as practical skills required for professional activity in trans-cultural communication. Trans-cultural communication is characterised by dealing in a professional manner with linguistic and cultural diversity in all areas of society. Graduates of the BA Trans-cultural Communication are practice-orientated experts in international multilingual communication." Reading this text drawn from a curriculum description1, employers might be asking themselves: Now, what exactly does that mean? Students might be asking themselves: How can I explain 
The last decades have seen the education of translators shift to several new dimensions, thanks to rapidly-developing technology and globalization. This paper presents an overview of the innovative development in devising integrated and detailed e-Learning plans and activities to enhance the design and delivery of the course content in the BA in Translation and Interpretation (BATI) programme. In the first section of this paper, it outlines why we believe e-learning has complementary roles to play in the translator education environment. We will then describe the progressive approach in integrating online learning environment with on-site classroom environment in order to give students the best of both worlds. In the main body of the paper, we will highlight some key features of this approach that have fundamentally changed the way translators and interpreters are being trained. We will examine how e-learning platform is deployed to resolve problems in the training of translators and interpreters in the open-learning environment. Implication and issues encountered in the process of implementing e-learning plans will be discussed. It is concluded that solutions to these problems lie in the collaboration between educators and technologists and this poses challenges and opportunities to both educators and technologists in the relevant field. 1. Introduction “The boom in translation jobs comes because of – and despite – technology.” (Jeffrey Ressner, Translation Nations 2007). “Our switch from high touch to high tech, from handwritten and heartfelt to 
As Polysius AG, one of the world's leading engineering companies in the field of equipment for the cement and minerals industry, were about to replace their existing translation memory, it soon became clear to them that they were renewing much more than just a software application. The decision to implement the concept of Corporate Translation Management was made with a view to increasing efficiency on various levels and this is now being carried out step by step. Because of the company's global activities, emails, reports and above all plant-related information such as manuals, clerical, technical and legal texts are translated into a current total of 10 languages. The majority of the workload of approx. 150 translation jobs per month, i.e. around 80%, need to be translated into English and Spanish and are centrally processed by the company's documentation and translation department. With the new system the documentation and translation department now has a tool for effectively managing projects that continually supports individually defined workflows. The translation memory and the terminology system form the basis for both effective translation support and the management of standardized corporate terminology. The terminology system is available by browser via the company's Intranet to ensure simple access to both the company's extensive terminology base and to the concordance search for all staff members who need these features. This paper will describe the decision process the company undertook to develop the new system. 1. Introduction As Polysius AG, one of the world's leading engineering companies in the field of equipment for the cement and minerals industry, were about to replace their existing translation memory, it soon became clear to them that they were renewing much more than just a software application. The decision to implement the concept of Corporate Translation Management was made with a view to increasing efficiency on various levels and this is now being carried out step by step. Polysius AG erects complete production lines, supplies single machines and implements plant rebuilding concepts, including project 
LTC has almost a decade of experience in helping corporate clients and language companies implement business information and workflow control tools. Our experience has shown that many such operations focus on optimizing the linguistic side of their work, while rarely thinking to look at how they might get more out of their business and project management. As a result they often have cutting edge linguistic tools while using inefficient, fragmented systems for their business information management. In the presentation we will try to redress the balance. One of our users, HCR - Informática e Traduções, Lda, a Portuguese LSP will join us to tell their story. HCR increased its productivity and saved time and money by centralizing crucial business data and controlling workflows using our software. Although we will use our software as an example of how software can help process optimization, the presentation will take a more general view. We will look at company workflows and which steps are necessary to take a project from quote to invoice. We will look how this can differ enormously from company to company and think about pros and cons for various workflows. We will also discuss project optimisation and automation. We will see that different kinds of projects and different clients often require different workflows. We will compare workflows that are designed for different services and types of processes. We will also look at workflows that incorporate different sites in different countries and look at how time zones and national currencies and wages need to be taken into account for maximum efficiency and cost effectiveness. 1. Introduction At LTC we know that every corporate language service department and every language service department (LSP) is unique. Unique because of a number of factors: the language combinations they offer, the services they offer in these languages, as well as their unique way of working - their internal workflow and the tools they use to support this work. 
Massively collaborative sites like Wikipedia, YouTube and SecondLife are revolutionizing the way in which content is produced and consumed worldwide. These fundamentally collaborative technologies will have a profound impact on the way in which content is not only produced, but also translated. In this paper, we raise a number of questions that naturally arise in this new frontier of translation. Firstly, we look at what processes and tools might be needed to translate content that is constantly being edited collaboratively by a large, loosely coordinated community of authors. Secondly, we look at how translators might benefit from open, wiki-like translation resources. Thirdly, we look at whether collaborative semantic tagging could help improve Machine Translation by allowing large numbers of people to teach machines facts about the world. These three questions illustrate the various ways in which massive online collaboration might change the rules of the game for translation, by sometimes introducing new problems, sometimes enabling new and better solutions to existing problems, and sometimes introducing exciting new opportunities that simply were not on our minds before. Introduction Massive Online Collaboration is revolutionizing the way in which content is being produced and consumed worldwide. This is bound to also have significant impacts on how we translate content. In this paper, I discuss what some of those impacts might be, and invite the translation community as a whole (translators, tool builders and vendors, clients, researchers, educators) to start thinking about the role they want to play in homesteading this new frontier. The thoughts exposed in this paper are based on my experience and research on Massive Online Collaboration, wiki in particular (Désilets et al., 2005, 2006 and 2007). They are also grounded in an intimate understanding of translation work practices, which I gained through yet to be published ethnographic research. The later is part of a project called OPLT (a French acronym which stands for “Observing the Technological Practices of Language Workers”) in which we conduct contextual inquiries1 with professional translators while they carry out their day to day work. Massive Online Collaboration: The New Frontier We live in very exciting times. Indeed, it has become somewhat of a cliché to say that the internet’s impact on the world is comparable to that of the printing press. Yet, the only reason we perceive this to be a cliché is that it is true. In less than 15 years, the web alone has given us a world where anyone in industrialized nations has all the information and knowledge he will ever 
 transit, offering advice on the basic materials - terminology - as well as the "local currencies" - language specific problems. Finally, on the arrival platform, the accuracy of the transfer system should be vetted by the reviser/local check who should provide feedback on the efficiency of the transfer and point out any technical problems that need sorting before the content is made available to the final end-user. Currently, there are often serious misalignments between the various stages which can only be eliminated by closer cooperation. It is important to adopt a blue-print right from the start. Furthermore, this blue-print could also help the software companies to provide better tools all along the line and thus facilitate a smooth and accurate transfer. Introduction Today I would like to talk about some of the gaps encountered with the transfer of words between various languages and also between different software platforms involved in word processing, content management, web sites and translation tools. The enormous increase in volumes of language content to be transferred between different countries and ever shorter deadlines requires a new approach on the part of those planning and writing the original content, as well as those involved in its transfer. Furthermore the technology available to facilitate this process must be carefully tuned to ensure maximum efficiency. Destination It is essential to determine the precise purpose and destination of the content before departure. This may appear obvious but unfortunately this is not always the case and can result in disappointing results. When ordering a translation the customer does not always remember that it is not sufficient to indicate a language without specifying the destination country. It is a bit like going to the United States with your cell phone, without previously checking whether it is tri-band. This gap is frequently underestimated. But if we take Europe for example, and the German language, there are some quite substantial differences between the German used in Germany, Austria and Switzerland.  Now let's examine some different "destinations". Individuals wishing to communicate in another language or travel abroad for pleasure, study, medical or other reasons may require visas, letters, certificates, etc. requiring translation in another language. These documents can be highly personalized and therefore not suitable for automatic processing. Thus another "gap" between one location and another. A bit like electrical plugs and sockets. If we move to the business world, where it is possible to identify perhaps five main types of content: legal/financial, sales/marketing, software, installation and user instructions, product support, these gaps take on another dimension. Here again it is essential to define all the destinations and prepare a schedule based on consultation with the vectors. If well planned, most of this content can benefit from automation to reduce the time and cost of the transfer and most important - improve efficiency. But despite some of the promises made by some companies selling Translation processing tools, the efficiency of automation depends heavily on the preliminary authoring stages, preparation of terminology and reference material, the "transferability" of the originals and the universal applicability of any style sheets selected to bridge or smooth the gaps. 
Abstract In the model of language use proposed by philosopher H. Paul Grice, people in conversation recognize “a common purpose or set of purposes, or at least a mutually accepted direction,” and they cooperate in contributing to those purposes. Grice went on to argue, “Talking [is] a special case or variety of purposive, indeed rational, behavior.” But Grice tacitly assumed a type of omniscient rationality: People in conversation have perfect knowledge of the language and the current common ground, and they have an unlimited processing capacity in choosing what to say. In reality, people’s rationality is bounded, and that leads to quite a different view of language use. I take up some of the consequences of bounded rationality in language use. 
This paper uses an analysis of ellipsis in multi-party interaction to investigate the relative accessibility of dialogue context/common ground to direct addressees and side participants. The results show that side-participants frequently make direct use of the common ground established between a speaker and addressee despite the fact that, by deﬁnition, they did not directly collaborate with the speaker on constructing it. Different individuals can thus reach the same level of grounding through different levels of feedback. We conclude that mutliparty dialogue leads to distinct collective states of understanding that are not reducible to the component dyadic interactions. 
The identiﬁcation of occurrences of like and well that serve as discourse markers (DMs) is a classiﬁcation problem which is studied here on a corpus of dialogue transcripts with more than 4,000 occurrences of each item. Decision trees using item-speciﬁc lexical, prosodic, positional and sociolinguistic features are trained using the C4.5 method. The results demonstrate improvement over past experiments, reaching the same range as inter-annotator agreement scores. DM identiﬁcation appears to beneﬁt from itemspeciﬁc classiﬁers, which perform better than general purpose ones, thanks to the differentiated use of lexical features. 
This paper addresses the problem of identifying action items discussed in open-domain conversational speech, and does so in two stages: ﬁrstly, detecting the subdialogues in which action items are proposed, discussed and committed to; and secondly, extracting the phrases that accurately capture or summarize the tasks they involve. While the detection problem is hard, we show that we can improve accuracy by taking account of dialogue structure. We then describe a semantic parser that identiﬁes potential summarizing phrases, and show that for some task properties these can be more informative than plain utterance transcriptions. 
This paper analyzes opinion categories like Sentiment and Arguing in meetings. We ﬁrst annotate the categories manually. We then develop genre-speciﬁc lexicons using interesting function word combinations for detecting the opinions. We analyze relations between dialog structure information and opinion expression in context of multiparty discourse. Finally we show that classiﬁers using lexical and discourse knowledge have signiﬁcant improvement over baseline. 
We present a model of compliance, for domains in which a dialogue agent may become adversarial. This model includes a set of emotions and a set of levels of compliance, and strategies for changing these.  In this work we build a model of compliance - how helpful the agent will be - in a domain in which the agent may become reticent or adversarial, along with the emotional components that direct that agent’s decision. 2 Testbed Domain  
In this paper, we present an approach for automatically acquiring a dialog corpus by means of the interaction of a dialog manager and a user simulator. A random selection of the answers has been used for the operation of both modules, deﬁning stop conditions for automatically deciding if the dialog is successful or not. Therefore, an initial corpus is not necessary to develop these two modules. In this work, we use a statistical dialog manager to evaluate the behavior of the corpus acquired using this approach. This dialog manager has been learned from the simulated corpus and has been evaluated using a previous corpus acquired for the task with real users. 
In a realistic Interactive Question Answering (IQA) situation, one third of the users pose follow-up questions, i.e., go beyond a single question per dialogue. We identify two different perspectives according to which these follow-ups can be described: informational transitions and context dependency. By understanding exactly how informational transitions occur in IQA dialogues, we propose a method to guarantee that focus tree based IQA systems provide wide coverage of follow-up questions that trigger the respective set of informational transitions. 
We present an approach to decreasing the cost of collecting speech data by a) distributing experimental setups as a downloadable computer program that records data and sends it back to an experiment server and b) by ‘re-using’ subjects for instant quality evaluation of the collected data. As an example of the kind of settings in which this approach can be used, we also shortly describe an experiment we have conducted; evaluation of the collected data showed no negative effect of the ‘unsupervised’ collection method. 
We report on an experiment on the effects of inducing acoustic understanding problems in task-oriented dialogue. We found that despite causing real problems w.r.t. task performance, many instances of induced problems were not explicitly repaired by the dialogue participants. Almost all repairs referred to the immediately preceding utterance, with problems in prior utterances left unacknowledged. Clariﬁcation requests of certain forms were in this corpus more likely to trigger reformulations than repetitions, unlike in different settings. 
Once a dialogue strategy has been learned for a particular set of conditions, we need to know how well it will perform when deployed in different conditions to those it was speciﬁcally trained for, i.e. how robust it is in transfer to different conditions. We ﬁrst present novel learning results for different ASR noise models combined with different user simulations. We then show that policies trained in high-noise conditions perform signiﬁcantly better than those trained for lownoise conditions, even when deployed in low-noise environments. 
We propose dynamically selecting n for nbest outputs returned from a dialog system module. We define a selection criterion based on maximum drop among probabilities, and demonstrate its theoretical properties. Applying this method to a dialog-act detection module, we show consistent higher performance of this method relative to all other n-best methods with fixed n. The performance metric we use is based on ROC area.  teristic) area, which measures the tradeoff of false positive and false negative in a selection criterion. We test the empirical performance of n*-best vs. nbest of fixed n for the task of identifying the confidence of dialog act classification. In two very different datasets we use, we found consistent higher performance of n*-best than n-best for any fixed n. This paper is the first attempt in providing theoretical foundation for dynamically selecting n-best outputs from statistical classifiers. The ROC area measure has recently been adopted by machine learning community, and starts to see its adoption by researchers on dialog systems.  
This paper presents and evaluates a behavior-based approach to dialogue management, where a system’s complete dialogue strategy is viewed as the result of running several dialogue behaviors in parallel leading to an emergent coherent and ﬂexible dialogue behavior. The conducted overhearer evaluation of the behaviorbased conversational recommender system CORESONG indicates that the approach can give rise to informative and coherent dialogue; and that a complete dialogue strategy can be modeled as an emergent phenomenon in terms of lower-level autonomous behaviors for the studied class of recommendation dialogue interaction. 
We present a computational model for the interpretation of linguistic spatial propositions in the restricted realm of a puzzle game. Based on an experiment aimed at analyzing human judgment of spatial expressions, we establish a set of criteria that explain human preference for certain interpretations over others. Each criterion is associated to a metric that combines the semantic and pragmatic contextual information regarding the game as well as the utterance being resolved. By resorting to machine learning techniques we determine a model of spatial relationships from the data collected during the experiment. Sentence interpretation occurs by matching the potential field of each of its possible interpretations to the model at hand. The system’s explanation capabilities lead to the correct assessment of ambiguous situated utterances for a large percentage of expressions. 
We present Hassan, a virtual human who engages in Tactical Questioning dialogues. We describe the tactical questioning domain, the motivation for this character, the speciﬁc architecture and present brief examples and an evaluation. 
We describe our system for breaking a ﬁlm review (as an instance of a semi-structured document) into its formal and functional constituents. Based on a corpus study, we devised a set of 25 zone labels indicating the role that a unit can play within the review. We identify formal zones with a set of symbolic rules, while the distinction between descriptive and evaluative paragraphs is drawn with a statistical classiﬁer. The approach achieves between 70 and 79% precision in recognizing the zones in our corpus. 
In the past few years, we have been developing a robust, wide-coverage, and cognitive load-sensitive spoken dialog interface, CHAT (Conversational Helper for Automotive Tasks). New progress has been made to address issues related to dynamic and attention-demanding environments, such as driving. Specifically, we try to address imperfect input and imperfect memory issues through robust understanding, knowledge-based interpretation, flexible dialog management, sensible information communication, and user-adaptive responses. In addition to the MP3 player and restaurant finder applications reported in previous publications, a third domain, navigation, has been developed, where one has to deal with dynamic information, domain switch, and error recovery. Evaluation in the new domain has shown a good degree of success: including high task completion rate, dialog efficiency, and improved user experience. 
In this paper, we describe a telephone dialog system for location-based services. In such systems, the effectiveness with which both the user can input location information to the system and the system delivers location information to the user is critical. We describe strategies for both of these issues in the context of a dialog system for real-time information about traffic, gas prices, and weather. The strategies employed by our system were evaluated through user studies and a system employing the best strategies was deployed. The system is evaluated through an analysis of 700 calls over a two month period. 
Especially in noisy environments like in human-robot interaction, visual information provides a strong cue facilitating a robust understanding of speech. In this paper, we consider the dynamic visual context of actions perceived by a camera. Based on an annotated multi-modal corpus of people who verbally explain tasks while they perform them, we present an automatic strategy for learning action-speciﬁc language models. The approach explicitly deals with the asynchrony of actions and verbal descriptions and includes an automatic parameter optimization based on a perplexity measure. Results show that a signiﬁcant improvement of the word accuracy can be achieved using a dynamic switching of action-speciﬁc language models. 
We present our iterative approach to enabling natural dialogic interaction between human users and a wheelchair, based on the alternation of empirical studies and dialogue modelling. Our approach incorporates empirically identified conceptual problem areas and a dialogue model designed to manage the available information and to ask clarification questions. In a Wizard-of-Oz experiment employing the first version of the model, we test how verbal robotic reactions can enable users to provide the information needed by the wheelchair to carry out the spatial task. Results show that the output must be extraordinarily coherent, temporally well-placed, and aligned with the user's descriptions, as even slightly deviating reactions systematically lead to confusion. The dialogue model is improved accordingly. 
We present City Browser, a web-based platform which provides multimodal access to urban information. We concentrate on aspects of the system that make it compelling for sustained interaction, yet accessible to new users. First, we discuss the architecture’s portability, demonstrating how new databases containing Points of Interest (POIs) may easily be added. We then describe two interface techniques which mitigate the complexity of interacting with these potentially large databases: (1) contextsensitive utterance suggestions and (2) multimodal correction of speech recognition hypotheses. Finally, we evaluate the platform with data collected from users via the web. 
This paper presents the results of an analysis of user reactions towards system failures in turn-taking in human-computer dialogues. When a system utterance and a user utterance start with a small time difference, the user may stop his/her utterance. In addition, when the user utterance ends soon after the overlap starts, the possibility of the utterance being discontinued is high. Based on this analysis, it is suggested that the degradation in speech recognition performance can be predicted using utterance overlapping information. 
Empirical spoken dialog research often involves the collection and analysis of a dialog corpus. However, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users. In this paper we use Let’s Go Lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. Our ﬁrst corpus is collected by recruiting subjects to call Let’s Go in a standard laboratory setting, while our second corpus consists of calls from real users calling Let’s Go during its operating hours. We quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically signiﬁcant similarities and differences between the two corpora with respect to these measures. For example, we ﬁnd that recruited subjects talk more and speak faster, while real users ask for more help and more frequently interrupt the system. In contrast, we ﬁnd no difference with respect to dialog structure. 
We present DEAL, a spoken dialogue system for conversation training under development at KTH. DEAL is a game with a spoken language interface designed for second language learners. The system is intended as a multidisciplinary research platform where challenges and potential benefits of combining elements from computer games, dialogue systems and language learning can be explored. 
We report results on how the collaborative process of referring in task-oriented dialogue is aﬀected by the restrictive interactivity of a turn-taking policy commonly used in dialogue systems, namely push-to-talk. Our ﬁndings show that the restriction did not have a negative effect. Instead, the stricter control imposed at the interaction level favoured longer, more eﬀective referring expressions, and induced a stricter and more structured performance at the level of the task. 
In this paper we present a multidimensional approach to utterance segmentation and automatic dialogue act classif cation. We show that the use of multiple dimensions in distinguishing and annotating units not only supports a more accurate analysis of human communication, but can also help to solve some notorious problems concerning the segmentation of dialogue into functional units. We introduce the use of per-dimension segmentation for dialogue act taxonomies that feature multi-functionality and show that better classif cation results are obtained when using a separate segmentation for each dimension than when using one segmentation that f ts all dimensions. Three machine learning techniques are applied and compared on the task of automatic classif cation of multiple communicative functions of utterances. The results are encouraging and indicate that communicative functions in important dimensions are easy machinelearnable. 
Accent on a pronoun has often been assumed to signal an “unusual” antecedent, i.e. something other than the most salient compatible antecedent. However, this assumption has not received adequate empirical investigation to date, and in particular, spontaneous conversational dialogues have never been studied to verify the saliencebased proposals. I analyze a richly annotated corpus of naturalistic speech, manually labeled for coreference relations, accents, and contrast, in order to understand what factors govern the presence of accent on a pronoun and thereby gain insight into what pronominal accent may be communicating. The results suggest that not only are differences among speakers and pronouns key components in explaining the variation in pronominal accentuation, but also that pronominal accent may often be signaling contrast rather than something about the attentional status or salience of the pronoun’s referent. 
We will discuss an approach to dialogue act generation that reﬂects the multidimensionality of communication. Dialogue acts from different dimensions in the taxonomy used are generated in parallel, resulting in a buffer of candidates. The main focus of the paper will be on an additional process of evaluating these candidates, resulting in deﬁnitive combinations of dialogue acts to be generated. This evaluation process is required to deal with the interdependencies there still are between dimensions, and involves logical, strategic and pragmatic considerations. 
The paper proposes two new approaches for measuring adaptation between dialogs. These approaches permit measurement of adaptation both to conversational partner (partner adaptation) and to the local dialog context (recency adaptation), and can be used with different types of feature. We used these measures to study adaptation in the Maptask corpus of spoken dialogs. We show that for syntactic features, recency adaptation is stronger than partner adaptation; however, we ﬁnd no signiﬁcant differences for lexical adaptation using these measures. 
In this study we compare two sequence learning approaches to chunk dialogue acts within a speaker’s turn. We assign a dialogue act label to each token in the transcribed speech stream of a dialogue participant, additionally classifying if the token is at the beginning of, inside, or outside that speciﬁc dialogue act. Experimental ﬁndings show that both our approaches – conditional random ﬁelds and memory-based tagging – largely improve over local classiﬁcation methods, obtaining comparable scores on distinct datasets. We discuss the interplay between transcription granularity of turns and dialogue act chunking. 
We present a disﬂuency model derived from analysing transcriptions of the AMI meeting corpus. Our model goes beyond previous work in that it discriminates several classes that are elsewhere regarded the same. Furthermore, we provide a formal account for naturally occurring phenomena that are rarely modeled in other schemes. Our annotations show signiﬁcant occurrences of these classes. An evaluation of the annotations from four different annotators reveals a high agreement, £¥¤§¦©¨¦©¨!#"%$&¤§¦©¨ . 
We discuss the design and preliminary results of an experiment for modeling human-human multi-threaded dialogues. We found that participants tend to complete adjacency pairs in dialogues before switching to a new dialogue thread. We also have indications that, in the presence of a manualvisual task, the difficulty of the task influences switching between dialogue threads. 
We describe a system for conversation type classiﬁcation which relies exclusively on multi-participant vocal activity patterns. Using a variation on a well-studied model from stochastic dynamics, we extract features which represent the transition probabilities that characterize the evolution of participant interaction. We also show how vocal interaction can be modeled between speciﬁc participant pairs. We apply the proposed system to the task of classifying meeting types in a large multi-party meeting corpus, and achieve a three-way classiﬁcation accuracy of 84%. This represents a relative error reduction of more than 50% over a baseline which uses only individual speaker times (i.e. no interaction dynamics). Random guessing on this data yields an accuracy of 43%. 
A method is presented that helps novice users understand the language expressions that a system can accept, even from unacceptable utterances made that may contain automatic speech recognition errors. We have developed a method that dynamically generates help messages, which can avoid further unacceptable utterances from being made, by estimating a users’ knowledge from their utterances. To improve the accuracy of the estimation, we developed a method to estimate a user’s knowledge from utterance veriﬁcation results. This method estimates whether a user knows an utterance pattern that the system considers acceptable, and suppresses useless help messages from being generated. 
This paper presents a data-driven decisiontheoretic approach to making grounding decisions in spoken dialogue systems, i.e., to decide which recognition hypotheses to consider as correct and which grounding action to take. Based on task analysis of the dialogue domain, cost functions are derived, which take dialogue efficiency, consequence of task failure and information gain into account. Dialogue data is then used to estimate speech recognition confidence thresholds that are dependent on the dialogue context. 
When constructing a task-oriented dialogue system, it is usual to perform an acquisition of dialogues for the system’s task. This acquisition can be used to deﬁne the behaviour of the dialogue system, and it can be rulebased or corpus-based. In the corpus-based case, the models that deﬁne the behaviour are automatically inferred from annotated dialogues. The annotation process is timeconsuming and error-prone, and the use of assistant tools for the annotation can reduce the effort in this process. In this work, the data requirements of a previously presented annotation tool are presented, and the results show that the technique obtains its maximum performance even with a relative small amount of annotated dialogues. 
Partially Observable Markov Decision Processes (POMDPs) are attractive for dialogue management because they are made to deal with noise and partial information. This paper addresses the problem of using them in a practical development cycle. We apply factored POMDP models to three applications. We examine our experiences with respect to design choices and issues, and compare performance with hand-crafted policies. 
We develop a new mechanism to detect and respond to miscommunications in human-robot dialogs, distinguishing between computer misunderstandings vs. human inexperience. Problem indicators drive an error/help state machine, which augments the dialog state and is used in tailoring response generation. A user study shows that the task success rate and user satisfaction is improved substantially by the two-part miscommunication model. 
We propose a method for rapid development of dialogue systems where a Grammatical Framework (GF) grammar is compiled into a complete VoiceXML application. This makes dialogue systems easy to develop, maintain, localize, and port to other platforms, and can improve the linguistic quality of generated system output. We have developed compilers which produce VoiceXML dialogue managers and ECMAScript linearization code from GF grammars. Along with the existing GF speech recognition grammar compiler, this makes it possible to produce a complete mixedinitiative information-seeking dialogue system from a single GF grammar. 
 This paper presents experiments into the resolution of “you” in multi-party dialog, dividing this process into two tasks: distinguishing between generic and referential uses; and then, for referential uses, identifying the referred-to addressee(s). On the ﬁrst task we achieve an accuracy of 75% on multi-party data. We achieve an accuracy of 47% on determining the actual identity of the referent. All results are achieved without the use of visual information. 
The SIDGrid architecture provides a framework for distributed annotation, archiving, and analysis of the rapidly growing volume of multimodal data. The framework integrates three main components: an annotation and analysis client, a web-accessible data repository, and a portal to the distributed processing capability of the TeraGrid. The architecture provides both a novel integration of annotation, analysis, and search for multimodal data and a powerful framework for web-based, distributed collaborative annotation and analysis. The ﬂexibility and capabilities of the system have been demonstrated through archiving Talkbank and other spoken discourse and dialogue data and performing joint multimodal analysis of lexical, prosodic, turntaking, and other multimodal factors. 
We will introduce ScIML, a domain speciﬁc language for voice user interface (VUI) creation that is based on the generic expressive means of the Uniﬁed Modelling Language. In particular, we employ UML statecharts for interaction ﬂow modelling. 
 auxiliary material to introduce them to the users. In addition to an introduction to the service, users are  We have developed interactive multimodal software tutors to teach users how to use a spoken dialogue timetable by guiding users and monitoring their interaction. They feature a visual representation of the spoken dialogue to support error recognition and recovery and thus helping the users to learn the required interaction style. Two different versions of tutoring were compared to a static web manual in a between-subjects experiment (N=27).  often provided with some instructions on how to use the system. Such a web-based tutorial can improve the user experience and users’perception of the system (Kamm, Litman, and Walker, 1998). Another approach to introducing new applications to users is software tutoring. This is popular with graphical interfaces, particularly in video games, but it has been almost neglected in the case of speech-based applications. However, the tutorial type guidance can be embedded into a dialogue system, e.g., as a specific guided mode, which can make the system more transparent to users and thus  
Usually, human-computer dialogue systems rely on ad-hoc solutions for the component performing speech turn generation, in natural language. However, integration of taskspeciﬁc and general world knowledge in order to provide a more reliable and natural interaction with humans also through more sophisticated language generation techniques becomes needed. In this paper we present performance improvements of a module simulating in ﬁrst-order logic Segmented Discourse Representation Theory for language generation in dialogue. These improvements concern reductions in computational costs and enhancements in rhetorical coherence for the discourse structures obtained, and are obtained using speech-act related information for driving rhetorical relations computations. 
This paper describes a dialogue management method suitable for automatic troubleshooting and other problem-solving applications. The method has a theoremproving flavor, in that it recursively decomposes tasks into sequences of sub-tasks and atomic actions. An explicit objective when designing the method was that it should be usable by other people than the designers themselves, notably IVR application developers. Therefore the method has a transparent execution model, and is configurable using a simple scripting language. 
In this paper we propose the use of a novel learning paradigm in spoken language interfaces – implicitly-supervised learning. The central idea is to extract a supervision signal online, directly from the user, from certain patterns that occur naturally in the conversation. The approach eliminates the need for developer supervision and facilitates online learning and adaptation. As a first step towards better understanding its properties, advantages and limitations, we have applied the proposed approach to the problem of confidence annotation. Experimental results indicate that we can attain performance similar to that of a fully supervised model, without any manual labeling. In effect, the system learns from its own experiences with the users. * 
The problem of planning dialog moves can be viewed as an instance of the more general AI problem of planning with incomplete information and sensing. Sensing actions complicate the planning process since such actions engender potentially inﬁnite state spaces. We adapt the Linear Dynamic Event Calculus (LDEC) to the representation of dialog acts using insights from the PKS planner, and show how this formalism can be applied to the problem of planning mixedinitiative collaborative discourse. 
Recent work in the area of probabilistic user simulation for training statistical dialogue managers has investigated a new agenda-based user model and presented preliminary experiments with a handcrafted model parameter set. Training the model on dialogue data is an important next step, but non-trivial since the user agenda states are not observable in data and the space of possible states and state transitions is intractably large. This paper presents a summary-space mapping which greatly reduces the number of state transitions and introduces a tree-based method for representing the space of possible agenda state sequences. Treating the user agenda as a hidden variable, the forward/backward algorithm can then be successfully applied to iteratively estimate the model parameters on dialogue data. 
In this paper we present a simple, empirically grounded computational model of grounding in dialogue. Grounding is shown to occur as a result of the dynamics of the information states of dialogue participants. A step-by-step analysis and representation of how information states develop through dialogue utterance processing illustrates exactly how this works. 
Automatic evaluation of spoken dialog systems has gained interest among researchers in the past years. In the PARADISE framework (Walker et al. 1997), a linear regression function is trained on a dialog corpus to predict user ratings of satisfaction from interaction parameters. The accuracy of such predictions is generally measured with R2, which usually is rather low. In this paper, it is shown that predictions according to PARADISE can lead to accurate test results despite the low R2. 
 Abstract This paper addresses a method for customizing an English-Korean machine translation system from general domain to patent domain. The customizing method includes the followings: (1) extracting and constructing large bilingual terminology and the patent-specific translation patterns, (2) adapting the probabilities of POS tagger trained from general domain to the patent domain, (3) syntactically analyzing long and complex sentences by recognizing coordinate structures, and (4) selecting a proper target word using patentspecific bilingual dictionary and collocation knowledge extracted from patent corpus. The translation accuracy of the customized English-Korean patent translation system is 82.43% on the average in 5 patent categories (machinery, electronics, chemistry, medicine and computer) according to the evaluation of 7 professional patent translators. A patent MT system for electronics domain was installed and started an on-line MT service in IPAC (International Patent Assistance Center) under MOCIE (Ministry of Commerce, Industry and Energy) in Korea. In 2007, KIPO (Korean Intellectual Property Office) is expected to launch its English-Korean MT service for whole patent domain.  1. Introduction Given the growing number of foreign language patents filed in the multiple countries, it is feasible that users want to read the patent documents translated to their native language. Such users’ demand has become a hot research issue in the MT community. Also because NLP techniques associated with specificity of patent domain have promise for improving the translation quality, patent translation is recently attracting many researchers and MT-related companies. It is well known that sentence style and dominant translation for a word vary with domains. Therefore, if the domain to be translated is fixed to patents, bilingual dictionary adaptation to the patent domain and customizing natural language analyzers to the linguistic specificity of patent style are effective ways to improve the translation quality of MT system. There have been studies concerned specifically with patent MT using these domain-specific advantages (Shinmori et al., 2003; Hong et al., 2005; Kaji, 2005; Shimihata, 2005). Though intensive research has been made on patent MT for the domain-specific advantages, there still remain many issues to be tackled. In this paper, we focus on the several issues: (1) new terminology construction, (2) patent-specific probabilities of POS tagger, (3) long and complex sentence analysis, and (4) target word selection. This paper addresses the customization of an EnglishKorean MT system for patent translation. The EnglishKorean patent MT system “FromTo-EK/PAT” described in this paper is based on an English-Korean MT system developed for the web translation in a general domain. English-Korean patent MT system belongs to basically the pattern-based methodology for machine translation. It has the formalism that does English sentence analysis in  which English patent-specific patterns are used, matches the English patent pattern with its Korean patent pattern, and then generates a Korean sentence from it. EnglishKorean patent MT system consists of an English morphological analysis module based on lexicalized HMM, an English syntactic analysis module by patternbased full parsing, a pattern-based transfer, and a Korean morphological generation. Section 2 describes the issues of customizing a MT system to the patent domain. In section 3 we will introduce the customization process according to the issues described in section 2. The experimental work is presented in section 4. Lastly, in section 5, we present some conclusions. 2. Issues for Customizing MT System to Patent Domain It is important to customize translation knowledge and translation modules for adapting the existing general MT system to translation of patent documents. The customization for the translation knowledge is able to be divided into two steps: (1) tuning general translation knowledge to patent-specific translation knowledge, and (2) efficiently constructing the unknown words and the translation patterns found in patent documents. The patent customization of existing translation knowledge is closely related with the customization of the translation knowledge of module. For example, the customization of the module of target word selection is decided by the customization of existing English-Korean bilingual dictionary. The POS tagging knowledge trained from general domain also have an influence on the customization of the POS tagging module. In this respect we consider the method extracting unknown words from  patent documents and the method customizing translation modules to patent. What is firstly necessary for applying a general MT system to patent is to extract the large-scale terms found newly in patent documents and construct their translation knowledge such as the target words. We have built an English-Korean bilingual dictionary by use of exiting Korean-English bilingual dictionary of a Korean-English patent MT system developed in 2005, in order to cut cost and time for building an English-Korean bilingual dictionary. The unknown words could be constructed at maximum effect with little cost and little time by the method, where we preferred selecting the high-frequently and positively necessary words for the English-Korean translation to constructing all unknown words appearing in patent documents. In relation to POS taggers with good performance and broad coverage, they have recently become available (Brants, 2000; Pla et al., 2004), but have not been trained for patent documents. This means that there is room for doubt that the general POS taggers keep their performance in the patent domain. We can easily find an example to degrade the performance, only looking through any patent document. The example is the word “said”: the word is mainly used as a past verb (VBD) in general domain, but is almost used as a adjective (JJ) in patent domain. The words like “said” are retrained from a tagged patent corpus. It is however very difficult to construct the tagged patent corpus because we have no tagged patent corpus. In this paper, we will describe how to adapt the generalpurpose POS tagger to the patent domain by using raw patent corpus. Compared with general documents, one characteristic of patent documents is to use the abnormally long and complex sentences (Kando, 2000), which makes it difficult to apply a parser for general domain to patent domain. A usual method for treating long sentences is to segment a long sentence into several segments and to analyze each segment respectively. However, in case a long sentence is formed by coordination structure, simple segmentation can cause syntactic analysis errors if the coordination structure is not firstly recognized. For this, we will present a method for recognizing the coordination structure in patent documents to enhance parsing efficiency and performance. Target word selection in English-Korean machine translation is very important factor in that it has a direct influence on the machine translation quality. Particularly, in the case of general domain documents such as web pages, the target word selection problems of English ambiguous words occur very frequently. In general domain documents, many frequently used English words can be translated to various Korean words depending on the contexts. However, in English-Korean patent machine translation, most of words used in patent documents belong to technical terms. These technical terms have relatively low ambiguities of target word selection. Some English words used in patent domain also have a tendency to be translated to specific Korean word according to International Patent Classification (IPC) codes. Although patent documents include many technical terms, target word selection problem still remains an obstacle which  should be solved to improve the performance of machine translation system. We customized English-Korean dictionary for patent machine translation to resolve the translation ambiguity of English ambiguous words appearing in patent documents. So, some English ambiguous words contain dominant Korean target word according to specific IPC code. For target word selection ambiguities which did not resolved by dominant Korean target word of translation dictionary, we tried to disambiguate the possible senses of English words by use of other knowledge like sense vectors and Korean bigram context information. 3. Customizing Methods 3.1 Construction of Patent Terminology Terminology construction for English-Korean patent MT system described in this paper is similar to the methods of Kaji(2005), Shimohata(2005), and Kim(2005) in respect of using the existing dictionary and the existing patent corpus, but our method is different in that it contains a step inverting the existing Korean-English bilingual terminology. Extraction and construction of terminology might be represented in Figure 1. As shown in Figure 1, the patent terminology can be built by two steps. The first step is the step to convert the existing Korean-English terms into the English-Korean terms, to delete the terms overlapped with the terms in the existing English-Korean bilingual dictionary, and to construct the English-Korean bilingual terms semiautomatically. Among inverted English-Korean bilingual terms, if English terms are the nominal phrases including a prepositional phrase, a gerund, and a relative clause, they are deleted. These nominal phrases were constructed for lack of an English compound word suitable to a Korean compound word in Korean-English patent translation. If such nominal phrases are entered in the English-Korean dictionary, the structural errors such as attachment of prepositional phrase or analysis of coordination structure in parsing might be produced. For example, if “method for 1+1 line protecting switching” as 선로 an English term equivalent to Korean term “1+1 보호 절체 방법” is made an entry of English-Korean dictionary, it may give rise to the incorrect analysis of coordination structure “(NP (NN device) (CC and) (NN method for 1+1 line protecting switching))” in analysis of a English phrase such as “device and method for 1+1 line protecting switching”. Each English term in the English-Korean terms constructed by the first step may have different Korean target words. To select a dominant one among different Korean target words, we sorted Korean target words automatically according to their frequency occurring in Korean patent documents and made a selection of dominant target word manually. Through this work we could create 801,046 English-Korean terms from 3,052655 Korean-English terms. The second step is to extract the unknown words from 1,001,419 English patent documents applied to the U.S. Patent Office from 2001 to 2005 and remove the overlapped entries. We extracted about ten million English unknown words from this step, but manually constructed 1,039,189 English-Korean bilingual  terminology with high coverage by using the method ‘Setting Lexical Goals’ Hong(2005) presented.  Existing English-to-Korean Dictionary  adding  Korean-to-English Terminology English Patent Documents  Transforming into English-to-Korean Terms Extracting Unknown Words  Removing Overlapped English Entries Removing Overlapped English Entries  Semiautomatically Building Target Words  adding Manually Building Target Words  English-to-Korean Patent Dictionary  Figure 1: Customization process for building EnglishKorean patent terminology  3.2 A Domain Adaptation Method for POS Tagger  Three items were tuned for customizing a broad coverage  POS tagger based on HMM to patent domain. They are as  follows:  For customization of surface form, a tokenization  module and/or a morphological analyzer were  modified for tokenizing and/or analyzing the peculiar  surface forms found in the specific domain.  For customization of lexical information, lexical  probabilities (output probabilities) were tuned for  holding domain-specific lexical information.  For customization of context information, contextual  probabilities (transition probabilities) were controlled  for holding the domain-specific contextual  information.  In the first step ‘customization of surface form’, the  tokenization module was modified to tokenize and/or  chunk very complex symbol words, a chemical formula, a  mathematical formula, programming codes, and so on.  We improved our morphological analyzer to assign the  estimated part-of-speeches to a compound word  connected with hyphen or slash. The estimated part-of-  speeches are estimated by the part-of-speeches of their  components.  Our English POS tagger uses a lexicalized HMM (Pla et  al., 2004). The process of our POS tagger consists of  finding the sequence of POS tags of maximum probability,  that is:  T  ∏ t t t w t =  arg  max t t... 1n    
 A novel approach is presented for extracting syntactically motivated phrase alignments. In this method we can incorporate  conventional resources such as dictionaries and grammar rules into a statistical optimization framework for phrase alignment. The  method extracts bilingual phrases by incrementally merging adjacent words or phrases on both source and target language sides in  accordance with a global statistical metric. Phrase alignments are extracted from parallel patent documents using this method. The  extracted phrases used as training corpus for a phrase-based SMT showed better cross-domain portability over conventional SMT  framework.  arranged in a row and each English word in (1b) is  1. Introduction  arranged in a column. The dominant cells are shadowed  In the phrase-based SMT framework (Marcu & Wong, 2002; Och & Ney, 2004; Chiang, 2005), extraction of phrase pairs is a key issue. Currently the standard method of extracting bilingual phrases is to use a heuristics called diag-and (Koehn et. al., 2003). In this method starting with the intersection of word alignments of both translation directions additional alignment points are added according to a number of heuristics and all the  and they are considered to show a clear correspondence. For a pair of languages with similar word order, the corresponding cells tend to align diagonally, but for languages like Japanese and English which have quite different word order, the corresponding cells are scattered. Nonetheless, when we look at local correspondences like words within a phrase, the corresponding cells come to next to each other. In this representation, when we obtain  phrase pairs which are consistent with the word  alignments are collected.  Although this method is effective by itself it is very  
Since sentences in patent texts are long, they are difficult to translate by a machine. Although statistical machine translation is one of the major streams of the field, long patent sentences are difficult to translate not using syntactic analysis. We propose the combination of a rule based method and a statistical method. It is a rule based machine translation (RMT) with a statistical based post editor (SPE). The evaluation by the NIST score shows RMT+SPE is more accurate than RMT only. Manual checks, however, show the outputs of RMT+SPE often have strange expressions in the target language. So we propose a new evaluation measure NMG (normalized mean grams). Although NMG is based on n-gram, it counts the number of words in the longest word sequence matches between the test sentence and the target language reference corpus. We use two reference corpora. One is the reference translation only the other is a large scaled target language corpus. In the former case, RMT+SPE wins in the later case, RMT wins.  frequency  0-9 20-29 40-49 60-69 80-89 100-109 120-129 140-149  1. Introduction Sentences in patent texts are long. Figure 1 shows the frequency distribution of sentence length (characters) for Japanese patent text and Japanese newspaper text. The mean length of Japanese patent sentence1 is 60 characters and of Japanese news sentence is 38 characters. 80 70 60 50 40 30 20 10 0 sentence length (character) Figure 1: Frequency distribution of the sentence length of Japanese patent text and Japanese news text dark bar: patent; light bar: news Long sentences are difficult to translate by a machine, because these sentences often have complex syntactic structures. Although statistical machine translation is one of the major streams of the field, long patent sentences are difficult to translate not using syntactic analysis. Some papers show statistical machine translation gives high performance in translation word selection but it often gives syntactically strange outputs. So the combination of a rule based method and a statistical method was one candidate of high quality patent translation. Our system has a structure that combines a rule based machine 
Introduction Due to the characteristic features of patent documentation, this text type constitutes specific challenges for machine translation. This paper gives a brief description of patent documentation and how well two different MT systems are able to meet these challenges. The first section gives an introductory description of the text type, patent documentation. Section two and three contain descriptions of the MT systems, a rule-based and an SMT-based system, respectively. The following sections introduce the evaluation procedure and report on the evaluation made on the two MT systems’ translational results. The next section goes through the various error types that can be identified in the translational results. Some concluding remarks are given in the next section, summing up the observations that have been made with respect to comparing the translational results generated by the two MT systems. The final section outlines future plans on how to improve the translation quality of the SMT system. Patent documentation – text typical feature Since patent documents are official and juridical documents, they are kept in a departmental style meeting the following criteria: • try to be as factual and impartial as possible • let all information of given topic be expressed within one period. The first criterion forms part of the reason why patent documentation texts have proven suitable for automatic translation. The demand of factual language usage promotes occurrences of many non-ambiguous technical terms. In addition, only the concrete and denotative meaning of words from the general word register are used. Even though patent texts are characterized by the absence of polysemiotic readings of the words used (facilitating the MT task), the whole idea or rationale behind writing a  patent application makes certain demands that have to be met. The introduction of inventions lead per definition to coining of subject specific terms, designating the new concept in question. With respect to lexical coverage within the area of patent documentation the ratio of new terms will per definition be disproportionately high regardless of the size of the already system known terms. In other words, an important design requirement of an MT-system tailored to patent documents is that it is capable of – one way or the other – treating system unknown words in flexible and user friendly way. Otherwise it would often result in poor translation results. The second criterion entails the occurrence of very long sentences with many embedded subclauses and series of prepositional phrases. Again, in order to achieve high quality machine translation results the MT systems must be designed in such a way that treatment of very long sentences does not involve a profound decrease in translation quality. Another general feature embedded in the patent documentation text type is the frequent occurrences of entities such as references to other patents, dates, measure units and text internal references. While the above mentioned characteristics cover patent documentation in general, other elements in domain subsets of patent documentation - related to the problem of system unknown terms - require specific treatment. Focus in this context will be on the domain specific area of Chemistry. Not surprisingly this subset of patent documentation is dominated by the presence of many chemical formulae. The syntax of how chemical substances can be combined is well defined though they can be very complex, cf. the following examples: -CH2CH2N(R15)CH2CH2 N-[3-[4-(6-fluor-1,2-benzisoxazol-3-yl)-1-piperidinyl]propyl]phthalimid 2-(3-(2-(ethoxy)ethylcarbonyloxy)propyl)ethyl  In some specific cases chemical formulae are language specific and need to be translated, but in general the formulae are language neutral and can be transferred/translated directly to the target language in question. It is, however, crucial that MT systems in their design have encompassed a procedure for treating these nonverbal entities in order to obtain a reasonably high translational quality. Comparison of two MT strategies In the following a comparison of the use and performance of a statistical phrase-based MT system and a traditional rule-based MT system is made. The comparison focuses on linguistic aspects in the different kinds of error types which were identified in the output of the SMT system. First, we briefly describe the two systems to illustrate the very different nature of the two systems. The PaTrans MT system PaTrans is a rule based MT system designed for EnglishDanish translation of patent texts. PaTrans is a transfer based system directly descended from the Eurotra MT research prototype (EUROTRA, 1991). The transition from research prototype to a production MT system included extensions for optimisation, syntactic error recovery, grammatical coverage of patent document specific phenomena, integration of a part-of-speech tagger, document handling (with preservation of layout information), a rule based entity recogniser and implementation of an automatic post-editing tool (see Ørsnes et al, 1996; Povlsen & Bech, 2001 for a more detailed description). In addition, in order to facilitate the manually conducted pre-editing task, various tools have been implemented, i.a. a term-coding tool and a tool that by making lookups in the existing term databases can identify system unknown words/terms in the source document. The SpaTrans MT system The SpaTrans system is developed in a research project financed by the Danish Research Council. The research concerned evaluation of the feasibility of developing SMT for the Danish language. The focus was on translation of patents from English to Danish. Two patent translation companies participated in the project acting the role of a potential future user and evaluated the potential of SpaTrans system. The SpaTrans system is based on the phrasal SMT decoder Pharaoh (Koehn et al., 2003; Koehn 2004). The Pharaoh decoder is the translation engine and is placed in surroundings of pre and post processing components. The pre and post processing components are much simpler than the corresponding PaTrans components, but handle some of the same challenges, though they leave others unsolved for the time being. The possibility of using terminology databases and preservation of input document layout are not yet implemented in the SpaTrans system, while preservation of special characters, tokenisation and casing are handled. The SpaTrans system is based on a phrase table and a language model. The Pharaoh training  software is used to train the phrase table. The training corpus consists of translated and sentence aligned patents. Experiments using europarl languages training material in combination with the patent texts lead to poorer results on development test set, so it was decided to do the training based only on patent texts at this stage. A similar observation is done by (Simard, 2007). The training corpus size can be seen in table 1. The training resulted in a phrase table with 2.3 mill phrases.  Corpus Training Language model Devel. test Test  English words 4.2 mill - 19.464 10.035  Danish words 4.5 mill 4.5 mill 17.465 10.574  Table 1: Sizes for training and test corpus  The sentence length in the training material for Danish sentences is 25 words and for English sentences 28 words. The treatment of formulae and figures are not as elaborated in the SpaTrans system as in the PaTrans system, but regular expression substitutions are performed to solve the most widely used conversion problems between English and Danish figures and references.  The language model is trained (order 3) using srilm (Stolcke, 2002) based on the Danish part of the patent training corpus. Experiments based on human evaluation have shown that the use of the monotonic translation option is best suited for English-Danish translation. We are well aware that the quality of the translations by the SpaTrans system might improve if more training material could be added, but the issue here is mainly to investigate the potential in the use of SMT in Patent translations using domain text resources and to point out strengths and weaknesses. One important limitation of the SpaTrans system is that no terminology database is used. The input format of the decoder allows for applying information about predetermined translations of single words and multiword units. This facility can be used to apply specific terminology to the translation engine and before bringing the system in production use, this will be added to the pre processing module. Evaluation Analyses of the output of the two systems are based on BLEU metric (Papineni et al., 2002). There is much focus on evaluation of SMT and MT-systems and the used BLEU metric is only one simple way to measure quality. For a brief overview of other currently used evaluation metrics used for SMT and MT and recent experiences within the field, see Callison-Burch, 2007. The BLEU metric gives one score for each test document. It has been argued that an increase/decrease in the value of the BLEU score does not guarantee a better/worse translation quality (Callison-Burch et al., 2006). But nevertheless the metric is widely used to measure development improvements in systems.  Given one or more reference translations the BLEU metric is normally used to score a text or a larger test corpus. The BLEU metric can also be used to calculate a score for each sentence in a test corpus, and these sentence based scores are in our evaluation used to focus on sentences with a low score, excluding very short sentences which by the definition of the algorithm will have a low score. Another aspect of the evaluation is the reference translation which is a product of post editing the PaTrans output by an experienced proofreader. This gives a large advantage to the PaTrans system, and this is reflected in the BLEU scores of the test material of the two systems, see table 2.  BLEU PaTrans SpaTrans reord. SpaTrans mono. Diff (PaTrans SpaTrans mono.)  Test patent A 0.539 0.439 0.448 0.091  Test patent B 0.610 0.399 0.501 0.111  Table 2: BLEU scores for two test documents. Test patent A consists of 227 sentences and test patent B consists of 376 sentences. Evaluation – one step further  About SpaTrans in general A general observation concerning SMT systems is that the corpus used as training data per definition reflects the translation performance of the SMT system. As training data are collected within a specific text type about a domain specific subject, will in some cases involve that the SMT system suggests translations that are too narrow in their scope leading to poor evaluation results. To give a translation example from the SpaTrans system1: Further, these paints, properly formulated and applied, have the ability to remain effective for 5 years. Has been translated into: Yderligere, disse malinger, korrekt formuleret og påført, har evnen til at forblive der er effektiv i 5 år. Literally translation: Further, these paints, properly formulated and applied, have ability_the to to remain which is effective for 5 years. The translation of ‘effective’ to ‘der er effektiv’ appears at first glance to be somewhat odd and it seems surprising that SpaTrans chose that translation. The Pharoah platform gives access to the word lattice generated during the translation process containing a list of the n-best translations that the system has considered. By adding an  
 1. Introduction It is widely known that Japanese patent sentences have long and complicated structures, with up to 200 Japanese characters (50 to 60 words), such that the modifications among phrases also have complicated and difficult structures. It is difficult for even native speakers to understand and clarify such structures. If an individual wants to apply for a patent, they must retrieve the large-scale patent database in order to confirm whether or not there are similar patents. Correct and accurate retrieval requires automatic information extraction from the patent database. Recently, the necessity of global application has increased due to rapid technological progress; thus, information should be shared immediately. A patent granted in one country should be valid in another country. If such system is realized, the request of machine translation for a patent will be increased. Therefore, the correct analysis of modification for patent sentences is necessary. In this paper, we report a system that finds errors of automatic modification, and corrects these errors automatically. We describe the content of the system and the result of an evaluation (Kennendai, 2007). 2. Material and Background The material is a DVD database in which all available patent gazettes of the Japanese patent office in 2003 are included (Patent, 2005). We have made a comparison of several Japanese patents and their English translations from the database. We previously reported that the modification errors in analyzing Japanese patent sentences reflect the translation result (Yokoyama, 2005). That is, if the  modification is in error, the resulting translation also contains the erroneous modification. If these errors are corrected, correct information about Japanese patent sentences can be obtained. The development of such a system will enable connection to a Japanese proofreading system. 2.1. Comparison of Modification between Japanese and English The database stores the titles and abstracts of patents and their machine translations. We determined the existence of modification errors by comparing the machine translation data with the human translation data included in the patent database supported by the Japan Patent Office (Industrial Property). 2.2. Classification of Modification Errors The content of a patent consists of bibliographical terms (publication number, date of publication of application, inventor, title of invention, etc), abstract and solution, range of the patent, detailed explanation, and a simple explanation of figures. We previously classified the characteristic patterns of modifications occurring in patent sentences primarily written in the abstract and solution (Yokoyama, 2005). Based on this classification, we selected some patterns of modification errors. Analysis of modification is automatically performed by the “Chasen” modification software, which is commonly used by developing by the researchers at Nara Advanced Institute of Science and Technology (NAIST). (a) Proper Representation in Patent Sentences 「 本 発 明 は ～ （ 中 略 ） A で あ る 」 (This invention is A, which …) (A: noun)  本発明は (This invention is) （中略） 記憶する (memorize) （中略） 時計型温度センサーである。(clock-type temperature sensor)  (error) (correction)  Fig. 1 An example of proper representation in patent sentences  柄と (handle) 清掃用ヘッド部との(sweeping head) 連結部分が (connected part)  (correction) (error)  Fig. 2 An example of parallel structure in patent sentences  The above pattern is one of the most typical patterns in patent sentences. After the phrase “This invention is A, which,” very long modifier(s) would follow. In Fig. 1, the subject “invention” is erroneously analyzed as the predicate modifying the verb “memorize”. The correction should be made such that the subject should modify the last phrase “clock-type temperature sensor.” (b) Parallel Structure 「A と B との C が、」 (C of A and B is …) (A, B, and C are nouns.) As shown in Fig. 2, the Japanese particle “to” (“and”) is erroneously analyzed to modify the last noun. Correction is performed by the modification of the parallel property of nouns. These corrections are usually made by human operators; however, we have developed a system which performs such corrections automatically. Other classifications are conjunctives, subject-verb agreement, modification between subordinate clauses, clause of noun modification, and parallel structure with noun and comma. These categories have not been implemented in the system because of the complexity of the procedure and/or algorithm. 3. System The flowchart of the correction system, which automatically finds and corrects modification errors, is shown in Fig. 3. First, the patent sentence is input and analyzed by Cabocha. Using Cabocha, the system then finds erroneous candidates among the  modifications, primarily through keyword and pattern matching. We also use a Japanese thesaurus (Ikehara, 1997); however, correction at this stage is not sufficiently effective because patent sentences often include many new and unknown words. If modification errors are found, they are then automatically corrected. An example sentence is shown below. The correction of the sentence belongs to type (b) in the previous section, that is, the parallel structure followed by Japanese particle “to” (“and”).  Example (partial) sentence 「製造設備、検査設備の各装置個別のデータ収 集とデータ解析を下位のネットワーク上で可能 とし、」(to make possible on the sub-network the collection of data and the analysis of data for each device in production facilities and inspection facilities)  0 1D 製造設備、 (production facilities)  
Introduction Patents are a rich source of information about technological knowledge and a valuable tool in technology development. It is the area, which shows an increasing interest in high quality multilingual machine translation systems. To develop such systems requires rich knowledge resources (lexicons, grammar rules, world models), which nowadays must normally be painstakingly handcrafted from scratch for every language pair. The idea to reduce development and maintenance costs, by sharing and reusing processing methods and knowledge has been in focus of researchers’ attention for many years. For example, (Takeda, 1994) proposes portable knowledge sources for machine translation that consists of preference information on word sense, phrasal attachment, and word selection for translation. The basic idea of (Paul, 2001) is to devote efforts to the development of translation engines between the main linguistically different languages and to reuse the translation knowledge of these systems for translation into languages closely related to the target language. (Pinkham et al., 2001) describe the assembly of the French-English research MT system, which was constructed from a combination of pre-existing rule-based components and automatically created components. A patent specific research in MT where the problem of portability is addressed by suggesting the constraint domain approach has been done for Russian to English by (Sheremetyeva and Nirenburg, 1999). Among the most recent attempts to reduce development cost by reusing pre-existing application components is a Japanese-English authoring patent system, which merges the English claim authoring system AutoPat (Sheremetyeva, 2003) and the Japanese machine translation application PC-Transfer (Neumann, 2005). In this paper we present the results of further work on reusability of the AutoPat application. We describe a feasibility study on reusing the components of the existing unilingual authoring application in a full-scale multilingual MT system APTrans, and explore to which extent linguistic MT knowledge can be ported from one language to another in the patent domain.  We illustrate our findings on the example of English, Danish and French in the frame of the APTrans architecture. Our discussion will mainly address the effort saving issues of augmenting the system with every new language-pair. In what follows we shall first sketch the starting point of our research, the English patent claim authoring system AutoPat, we shall then describe the migration process from the unilingual AutoPat to the multilingual machine translation system APTrans followed by a worked out example for the three languages, - English, Danish and French. We shall also discuss other possibilities to use the APTrans architecture in machine translation. AutoPat AutoPat is a computer system for authoring patent claims in the English language. It consists of a technical knowledge elicitation module with an interactive user interface, lexicon, human input analysis module, content representation language, and generation module integrated with proofing tools (spelling, content and grammar checkers). The knowledge base of the system includes a patent corpus-based English lexicon over a rich feature space, rules and knowledge representation language. AutoPat is a fully implemented product level application described in detail in (Sheremetyeva, 2003) and available at www.lanaconsult.com. We shall thus skip the AutoPat specification but rather concentrate on a re-engineering issue. Development process: migration from unilingual authoring to multilingual MT. Our goal is to find ways to speed up the development of a multilingual machine translation system, which can be specifically supported by domain constraints. Our multiyear R&D in the patent domain gave us a strong evidence of high lexical and structural similarity of patent claims in different languages. This inspired us to extrapolate “what is already there”, - the knowledge base and program components of AutoPat, to another application, an MT system, and other languages.  Design The first step in developing APTrans was to define a subset of the existing AutoPat components that will be the basis of the multilingual application and the extension it will need. The modular architecture of AutoPat, which generates patent claims form content representations, suggested a transfer type MT architecture. All of the AutoPat components with the exception of the knowledge elicitation module can be reused for generation of the TL claim from the TL content representation. What is missing is an analyzing component, which could map raw claims into the AutoPat content representation format in a SL and a transfer module, which could convert a SL content representation into a TL content representation keeping the AutoPat format. The knowledge base should be extended with multilingual MT lexicons, rules and heuristics. Other components that will definitely be needed are output posteditors for TLs. To be a viable application that can be developed within a reasonable time a developer’s environment for knowledge acquisition and maintenance should be an integral part of the application. In our research the whole translation procedure was built “around” the existing AutoPat knowledge base and generator. We shall therefore first describe the reuse and customization of the lexicon, knowledge representation and generator and then show how the rest of the APTrans components were attached to them. From the very start we programmed APTrans as a multilingual (not just bilingual) application, so that a new language can be easily integrated into the previously developed software. Reuse and customization of existing components Lexicon and feature space The AutoPat lexicon (its vocabulary, entry format and feature space) is completely transferred to the APTrans application and used as a seed lexicon for lexical acquisition in other languages. We reused the approach to treat passive and active forms of verbs as different lexemes to simplify processing procedures. Every entry following the English lexicon format is maximally defined as a tree of features: SEM-CL [Language [POS [MORPH CASE_ROLE FILLER PATTERN], where SEM_Cl - semantic class; POS - part of speech; MORPH – morphological features, such as number, gender, etc., and domain relevant wordforms; CASE_ROLEs, - a set of lexeme case roles such as agent, theme, place, instrument, etc; FILLERs – lexical categories that can fill case-role slots of a lexeme; PATTERNs - linking features, that code both the knowledge about co-occurrences of lexemes with their case-roles and the knowledge about their linear order in the claim text. Every node in the APTrans tree of features inherits values from its ancestor. The mechanism of inheritance works in such a way that, in general, most values are in-  herited from the closest ancestor unless it is blocked or overwritten. What is not trivial and probably only possible in such a restricted domain as ours is that there is a significant cross-linguistic parallelism (portability) in the values of two features, - CASE-ROLEs, and PATTERNs. In other words, the set of case-roles for crosslingually equivalent predicates (verbs) and the order of their realization in the claim text are essentially invariant across languages. It means that in our tree of features there is not only a traditional “vertical” inheritance from parents to children, but for certain sibling nodes there is also a “horizontal” cross linguistic value inheritance which saves a lot of effort in non-English lexical acquisition.  Content representation language AutoTrans reuses the AutoPat claim content representation language on both SL and TL sides of the translation process. The format of the claim content representation as a set of predicate templates is given in Figure 1, where “label” is a unique identifier of the elementary predicateargument structure, “predicate-class” is a label of a semantic class, “predicate” is a string corresponding to a predicate from the system lexicon, “case-roles” are “ranked” according to the frequency of their cooccurrence with a certain predicate in the training corpus, “status” is a semantic status of a case-role, such as agent, theme, place, instrument, etc., and “value” is a string which fills a caserole.  Sentence::={ template){template}* template::={label predicate-class predicate role)(case-role))*} case-role::= (rank status value) value::= phrase{(phrase(word tag)*)}*  ((case-  Figure 1. A claim content representation format.  Generation module The AutoPat generation module, which takes a TL set of templates as input is what APTrans profits most of. It is fully reused from AutoPat for the English TL and, as our experiments show so far, requires only a slight updating for a non-English TL. The whole concept of AutoPat generation, its rules and algorithms were originally worked out for Russian, and they actually code the legal requirements to the claim structure, which are essentially the same all over the world. This gave us the idea to port the generation knowledge to the English AutoPat, where it is now used without any essential changes. We repeated our exercise in APTrans and ported the generation rules, this time, from English to Danish and French. For both languages only a few rules were updated, mainly to cover TL subject-predicate agreement. In those cases where updating the English generation rules for Danish or French required too much effort we left them unchanged, thus “programming” mistakes in the translation output. We found it easier to correct these predictable mistakes at a later stage of processing, by running a TL posteditor on the generator output.  Analyzer It was natural to think of the APTrans analyzer as the component to output its parse in the format of the content representation language. Trying to reuse the knowledge we have already acquired for the English AutoPat we started with the analyzer for the English language and built it “on top” of the AutoPat disambiguating tagger. A bottom-up heuristic parser with a recursive pattern matching technique was then added to recursively chunk longer phrases preserving their inner structure. It also marks the head of every noun phrase and “learns” its “singular/plural” feature. The last analyzing procedure determines the dependency relations between the chunks and predicates, and puts these chunks as fillers into case-role slots in predicate/argument structures, thus defining their semantic status (Sheremetyeva, 2003). The reuse of the AutoPat generator has the advantage of simplifying the analysis task by making it possible to skip the problems of determining a) the syntactic relations between the predicate and its arguments within every individual predicate structure (microsyntax), and b) the syntactic hierarchy of predicate/argument structures in the input claim text (macrosyntax). The generator, as was mentioned above, has the microsyntactic and macrosyntactic knowledge about the template hierarchy and the order of the phrases within predicate templates coded in its rules and lexicon. To test the compatibility of the analyzer and the generator we modeled a “translation” experiment within one (English) language, thus avoiding (for now) lexical transfer problems. Raw English claims were input into the analyzer, and parsed. The parse was input into the generator. The modules proved to be compatible and the results of such “translation” showed a reasonably small number of failures, mainly due to the incompleteness of analysis rules. We then tried to port the English analysis knowledge to the analyzers for Danish and French, the experiments show so far that a great deal of English analysis rules in our domain and approach can also be reused, though, of course, language specificity requires customization (e.g., location of adjectives in French noun phrases, lexical clues, etc.). Transfer module The APTrans transfer module takes the analyzer output, a SL set of predicate templates as input and outputs a set of TL predicate templates whose slots are filled with presumably perfectly translated TL phrases/case-role fillers. The APTrans transfer is in fact a combination of interlingual and syntactic transfer. The interlingual transfer finds TL equivalents1 for every predicate and keeps the predicate template slot structure unchanged (invariant). The syntactic transfer is responsible for the translation of case-role strings. A “real” translation procedure is thus reduced to the phrase level which, though not without problems, is still much simpler than machine translation of a full patent claim, especially when, which is often the case, it runs for a page or so. 
The MT pyramid looks like this:  interlingua  source logical form  target logical form  source syntax  target syntax  source string  target string  As generation researchers, we could just stand at the top and say: “We’re ready for our input.” There are problems with this approach: (First) It doesn’t take us anywhere. (Second) We’re definitely not ready. Imagine: here comes the input, expressed in some formalism whose BNF includes a conceptual inventory as well as where the parentheses go. We certainly don’t have the tools and knowledge resources today to generate from that input, to say nothing of measuring generation accuracy. Say we want to be ready by the year 2020. What should we be doing today? Analysis researchers are quite interested in Ontobank and related semantic annotation efforts. MT researchers have also stuck their noses into this business and said to Ontobank: you are annotating English, Chinese, and Arabic texts, but it’s vital to annotate parallel texts, so we can see how the semantic structures match up. Likewise, it’s important to make sure that this semantic annotation effort creates resources and opportunities for generation research.  
This paper presents a new method of evaluation for generation and parsing components of transfer-based MT systems where the transfer rules have been automatically acquired from parsed sentence-aligned bitext corpora. The method provides a means of quantifying the upper bound imposed on the MT system by the quality of the parsing and generation technologies for the target language. We include experiments to calculate this upper bound for both handcrafted and automatically induced parsing and generation technologies currently in use by transfer-based MT systems. 
This paper presents the construction of a data source that supports the automatic generation of cryptic crossword clues in a system called ENIGMA. Cryptic crossword clues have two layers of meaning: a surface reading that appears to be a fragment of English prose, and a puzzle reading that the solver must uncover to solve the clue. The content expressed by the clue, and the input to the generation process, is a word play puzzle, such as an anagram, perhaps. In expressing this puzzle ENIGMA must choose language creatively, so that a separate, surface reading of the text is also generated – in effect translating a semantic input via a layered text to a new semantic output. To ensure that this surface text is meaningful, ENIGMA uses corpus data to determine which words can be combined meaningfully and which cannot. 
This paper reports on progress towards developing the ﬁrst broad coverage English surface realizer for Combinatory Categorial Grammar (CCG). The paper provides initial automatic evaluation results which are roughly comparable to those reported with other formalisms when using a (nonblind) grammar derived from the development section of the CCGbank; the results are worse, though still respectable, when using the standard dev/train/test splits, highlighting the need for better lexical smoothing and more focused search. The paper also shows that factored language models that interpolate word-level n-grams with n-grams over POS tags and supertags provide similar absolute performance improvements over word-level n-grams as have been observed with parsing-inspired log-linear models. 
For statistical language model training, target task matched corpora are required. However, training corpora sometimes include both target task matched and unmatched sentences. In such a case, training set selection is effective for both model size reduction and model performance improvement. In this paper, training set selection method for statistical language model training is described. The method provides two advantages for training a language model. One is its capacity to improve the language model performance, and the other is its capacity to reduce computational loads for the language model. The method has four steps. 1) Sentence clustering is applied to all available corpora. 2) Language models are trained on each cluster. 3) Perplexity on the development set is calculated using the language models. 4) For the ﬁnal language model training, we use the clusters whose language models yield low perplexities. The experimental results we obtained indicate the language model trained on the data selected by our method gives lower perplexity on an open test set than a language model trained on all available corpora. 
Using dependency trees in natural language generation and machine translation raise the need to derive the word order from dependency trees. This task is difﬁcult for languages with (partly) free word order and comparatively easier for languages with ﬁxed word order. This paper describe (a) the two basic elements of topological models, (b) rule patterns for the mapping of dependency trees to topological trees, (c) the automatic acquisition of word order rules from corpora, annotated with dependency structures and (d) an approach for the automatic evaluation of the results. 
This paper describes a declarative approach to parsing and realization of natural language using a probabilistic dependency model of syntax within a constrained optimization framework. Such an approach is particularly well-suited for applications like machine translation. The paper describes a test-of-concept implementation applied to the classic sentence “Time ﬂies like an arrow.” and discusses the further research necessary for scaling up to general broadcoverage processing of language. 
In terms of modeling approach, components can be built using manual human “learned” rules (HL) or machine learned (ML) rules. There are many possible instances in between these two extremes that can include different degrees of manual human interference in pure machine learning: restricting the machine to a speciﬁc space of linguistic rules or more abstractly to a formal space of allowable linguistic rules. 
This short paper ﬁrst outlines an explanatory model that contrasts the evaluation of systems for which human language appears in their input with systems for which language appears in their output, or in both input and output. The paper then compares metrics for NLG evaluation with those applied to MT systems, and then with the case of reference resolution, which is the reverse task of generating referring expressions. 
Generation in MT must be based on metastructure information on the author’s communicative intentions, which must be decoded from the source language text. 
‘One-way translation’ has been proposed by Ward (2002) as an alternative to full-blown speech-to-speech translation. A one-way translation system is a cross-language communication aid located on a wearable device that uses graphical or other input to generate a spoken utterance, for example when buying a train ticket in a foreign language. We argue that this type of application is an opportunity for NLG, MT (and also dialog and speech) research to interact. 
The Attribute Selection for Generating Referring Expressions (ASGRE) Challenge was the ﬁrst shared-task evaluation challenge in the ﬁeld of Natural Language Generation. Six teams submitted a total of 22 systems. All submitted systems were tested automatically for minimality, uniqueness and ‘humanlikeness’. In addition, the output of 15 systems was tested in a task-based experiment where subjects were asked to identify referents, and the speed and accuracy of identiﬁcation was measured. This report describes the ASGRE task and the ﬁve evaluation methods, gives brief overviews of the participating systems, and presents the evaluation results. 
The ﬁrst algorithm is the full brevity algorithm combined with nearest neighbour learning technique which is used for the selection of the referring expression closest to training examples as uttered by humans. This algorithm obviously only imitates human behaviour with the goal to get high scores. With the Incremental Algorithm that is known and justiﬁed by its human like behaviour, we tried to beat the ﬁrst one. 
The entry presented by the NIL research group of the Universidad Complutense de Madrid adapts existing software previously developed for an application aimed at generating fluent texts for storytelling. The final entry is specifically geared towards system-human match over the challenge corpus. Attributes are selected based on an adaptation of Reiter and Dale’s fast efficient algorithm for referring expression generation, using relative groupings of attributes obtained empirically from the training data to determine the order in which they are considered. The results for the development data indicate that the NIL entry is dealing adequately with issues of identification of referents. Results for systemhuman match are not so good, probably due to the fact that the corpus is a sample of possible references, rather than a selection of ‘ideal’ references. 
We present four variations of our 2004 incremental algorithm (Siddharthan and Copestake, 2004), and present results on both the Furniture and People datasets. 
 Emiel Krahmer  Human Media Interaction Centre for Language Technology Communication & Cognition  University of Twente  Macquarie University  University of Tilburg  Enschede, The Netherlands  Sydney, Australia  Tilburg, The Netherlands  m.theune@utwente.nl  jviethen@ics.mq.edu.au  e.j.krahmer@uvt.nl  p.b.touset@student.utwente.nl  
We provide a simple base algorithm for the given task of attribute selection as well as two improvements to this algorithm. We report on the results of their implementation and provide an error analysis. We then report some observations and conclusions about the human attribute selection process by comparing the output of our algorithms and the attributes selected by humans. Finally, we add some observations on the difﬁculty of the attribute selection task. Note: We utilised both training and development data for the development and evaluation of our algorithm. 
This paper investigates the relationships among controlled language (CL), machine translation (MT) quality, and post-editing (PE). Previous research has shown that the use of CL improves the quality of MT. By extension, we assume that the use of CL will lead to greater productivity or reduced PE effort. The paper examines whether this three-way relationship among CL, MT quality, and PE holds. Beginning with a set of CL rules, we determine what types of CL rules have the greatest cross-linguistic impact on MT quality. We create two sets of English data, one which violates the CL rules and the other which conforms to them. We translate both sets of sentences into four typologically different languages (Dutch, Chinese, Arabic, and French) using MSR-MT, a statistical machine translation system developed at Microsoft. We measure the degree of impact of CL rules on MT quality based on the difference in human evaluation as well as BLEU scores between the two sets of MT output. Finally, we examine whether the use of CL improves productivity in terms of reduced PE effort, using character-based edit-distance.  1. Introduction Over the past several years, Microsoft has been localizing portions of its technical documentation using MSR-MT, a statistical machine translation (MT) system (Quirk, et al., 2005). In the development of this system, we have often encountered English source input that not only has presented problems for MT, but also has caused humans difficulty with translation. In an attempt to tackle the translatability problem, a controlled language (CL) in the form of authoring guidelines was proposed for content writers. (See Appendix I for the summary of CL rules used in our experiments.) Research has shown that the use of CL improves the quality of MT. 1 Given this finding, we expect, by extension, that the usage of CL will also lead to greater productivity in post-editing (PE), in a three-way relationship among CL, MT quality, and PE, which is illustrated in Figure 1. Figure 1: CL, MT Quality and PE Effort 
 Nowadays, speech translation is a research problem in machine translation. The problem arises as to how to combine speech recognition and machine translation in a suitable way. Some authors have shown that the speech translation can be improved by using word lattices as input of the translation system. The acoustic recognition scores from the word lattice are used for improving the translation quality. However, word lattices do not consider word co-occurrences between different hypothesis and those probabilities are not real probabilities but merely Viterbi approximations. In this work, we propose an improved word lattice representation for using posterior probabilities instead of acoustic scores. We present preliminary results of this approach compared against other common approaches on two different corpora. Although the results are not strongly conclusive, they show that this approach is worth exploring more deeply.  
 Abstract This paper investigates the task of training discriminatively a phrase based SMT system with millions of features using the structured perceptron and the Margin Infused Relax Algorithm (MIRA), two popular online learning algorithms. We also compare two different update strategies, one where we update towards an oracle translation candidate extracted from an N-best list vs a more aggressive approach in which we update towards an oracle extracted prior to training using a minloss decoder. We evaluate our different training algorithms on the Czech-English translation task. Our results show that while both learning algorithms achieve similar results, with the perceptron converging more rapidly, the aggressive update strategy performs signiﬁcantly worse than the more conservative strategy corroborating Liang et al. (2006)’s ﬁndings.  1. Introduction The direct maximum entropy model proposed by Och and Ney (2001) described a generalisation of the generative noisy-channel model of Brown et al. (1993) allowing the incorporation of additional knowledge sources in form of features. The standard way of training such a discriminative model in the community is to use minimum error rate training (MERT) (Och, 2003). As it name implies, such a training scheme tries to directly minimise the error on training data where the ’error’ is evaluated using a loss function of one’s choice (usually BLEU). A major shortcoming of MERT is that it can only be used to train a model with a small number of features. Typically, state of the art phrase-based SMT systems use around a dozen features. Recent work has tried to address this problem, with both Liang et al. (2006) and Tillmann and Zhang (2006) presenting online perceptron/perceptron-like training schemes to learn parameters for models with millions of features. Typically, discriminative training algorithms come in two ﬂavours, ﬁrstly likelihood-based methods which require feature expectations and secondly, margin-based methods which require either an N-best list of best outputs or a marginal distribution across the graphical structure. The perceptron algorithm (Collins, 2002) is a much simpler alternative as it only requires an arg max computation, which is precisely what the decoder is set out to do. While Liang et al. (2006) employ the standard perceptron algorithm in their work, Tillmann and Zhang (2006) present a variant in which the perceptron update is weighted with a factor dependent on the difference in translation score between the reference and the model guess and the difference in their loss. In this paper, we compare the perceptron algorithm to the Margin Infused Relaxed Algorithm(MIRA) (Crammer and Singer, 2003), a large-margin online algorithm that has given state-of-the-art results in other structured prediction tasks in NLP such as depen-  dency parsing (McDonald et al., 2005). Discriminative learning usually requires access to the gold standard, e.g. in discriminative dependency parsing (McDonald et al., 2005), the learning algorithm updates model parameters towards gold parse features. In the case of phrase based SMT, this is a problematic issue. Recall that phrases are extracted through heuristics from word alignments after symmetrisation (Koehn et al., 2003). Only phrases that are consistent with the alignments are extracted, therefore phrase-pairs that are required to reproduce the reference might end up not being extracted. Also, since multiple phrases are extracted from a given sentencepair, in cases where the reference can be reproduced, there might be multiple reference phrasal alignments. In this paper, we address this issue by comparing two strategies for selecting a gold standard translation example. The ﬁrst strategy is the local updating of Liang et al. (2006), whereby the perceptron update is done towards the “best” candidate from an n-best list. The second strategy is the one employed by Tillmann and Zhang (2006) whereby the decoder is modiﬁed to use BLEU as its scoring function and is made to ﬁnd the “best” candidate given the existing phrase table and source sentence. This is done as a pre-processing step and the extracted surrogate references are cached to be employed in the learning phase. Our results show that the local updating strategy gives better results conﬁrming Liang et al. (2006) ﬁndings that conservative updates are more effective. 2. The model In phrase-based SMT models, the input (“foreign”) sentence is segmented into so-called phrases, which are sequences of adjacent words that are not necessarily linguistically motivated. Each foreign phrase is mapped into the target language (“English”). Phrases are allowed to be reordered during translation; see Figure 1 for an illustration. Similar to Liang et al. (2006), we model translation as a structured prediction task:  Figure 1: Phrase-Based SMT: Input sentence is segmented into phrases, which are then mapped onto output phrases.  3.2. MIRA The Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) is a large-margin online algorithm which has been employed successfully for a number of structured classiﬁcation tasks in NLP, such as dependency parsing (McDonald et al., 2005). We employ the 1-best version of MIRA whose update rule is given by:  s(x, y) =  s(bi, bj, o)  (bi,bj ,o)∈y  =  w · f (bi, bj, o)  (bi,bj ,o)∈y  where x is an input (foreign) sentence, y is an output (English) sentence, f (bi, bj, o) is a multidimensional feature vector representation of sequence that produces phrase-pair bj following phrase-pair bi with orientation o and w the corresponding weight vector. For example, a feature f101 in f could be:   1  f101(bi, bj , o) = 0  if src(bi) = “ﬂiege” ∧tgt(bi) = will ﬂy otherwise  where src(bi) is the source phrase of the phrase pair bi and tgt(bi) the target phrase of the phrase pair bi. Decoding in this model amounts to ﬁnding the y for a given x that maximises s(x, y):  y = arg max s(x, y) y Since exact decoding is intractable in SMT, we approximate the argmax using beam search.  3. Parameter estimation In this section, we describe two online learning algorithms for learning the weight vector w. As usual for supervised learning, we assume a training set T = {(xt, yt)}Tt=1 3.1. Perceptron The perceptron algorithm (Collins, 2002) is an incredibly simple algorithm that only requires an argmax computation.  wi+1 = wi + Θ(xi, yt) − Θ(xi, yg)  (1)  The perceptron algorithm consists in iterating several times over the training data, decoding each training instance one at a time. Each time the decoder’s guessed solution is not equal to the correct solution, the weight vector is updated with the difference in feature vectors between the correct solution and the guess. The intermediate weight vectors are cumulated after each update and at test time, their averaged vector is used. This has been shown to alleviate overﬁtting (Collins, 2002).  min  ||wi+1 − wi||  s.t. s(xt, yt) − s(xt, y ) ≥ L(yt, y )  ∀y  ∈ best1(xt; w(i))  On each update, MIRA attempts to keep the new weight vector as close as possible to the old weight vector, subject to margin constraints that keep the score of the correct output above the score of the guessed output by an amount given by the loss of the incorrect output. Similar to the perceptron, the averaged MIRA weight vector is used at test time.  4. Loss functions and Update strategies Both the perceptron and MIRA algorithms require a feature vector representation of the correct output in order to update the weight vector. Ideally, we would like to be updating towards the feature vector representation of the gold standard output (surface form and alignment). However, the alignment information is hidden and only the gold standard surface form is visible. Even then, due to distortion limits and the way phrase-pairs are extracted, sometimes the reference surface form is just not reachable by the model. When the reference is reachable by the model, there might be multiple derivations to go from source to reference string. In this scenario in our current implementation, we pick the highest scoring derivation given the current model as the truth. When the reference is not reachable, we investigate two alternate strategies for selecting the surrogate reference. Both strategies rely on selecting a reachable translation which is closest to the reference as measured by a loss function. 4.1. Loss function As loss function, we have implemented sentence level smoothed BLEU (sBLEU) (Lin and Och, 2004) . Note that since we run our learning algorithm on the parallel training corpus, we only have one reference translation per source sentence. It is unclear how reliable a sentence level BLEU with respect to one reference is. Also while BLEU computes the brevity penalty by aggregating the length of the whole document, the sentence level BLEU computes the brevity penalty purely at the sentence level. The MIRA update rule requires a loss function L(yt, y ) which indicates the penalty to be incurred when the guessed output y is proposed instead of the correct output yt. This, in our work is given by:  L (yt, y ) = sBLEU (yt) − sBLEU (y ) (2)  4.2. Update strategy The ﬁrst update strategy we consider is one proposed in Tillmann and Zhang (2006). In this method which we call max-BLEU update, we modify our decoder to use sentence level smoothed BLEU as a scoring function. The decoder is run over the training data as a pre-processing step and the best scoring translation for each source is cached. Since we use approximate inference, search errors are possible. The cached translations are used as surrogate references in the online learning step. If at some point during learning the best guessed translation is found to have a higher BLEU than the cached surrogate, this guess becomes the surrogate from there on. The second update strategy we look at is the local update proposed in Liang et al. (2006). At each decoding step, an n-best list is generated. The translation candidate in the n-best list with the lowest loss with respect to to the true reference is chosen as surrogate. While it is expected that the quality of translations in the n-best list improves over time as the weight vector estimate become better, this is not guaranteed to happen. Therefore, the surrogate from the previous iteration is merged with the current n-best list and the best translation from the expanded n-best list is chosen as new surrogate. 5. Features Since one of the aims of this work is to show how much leverage we can get using purely discriminatively trained features, we eschew all probabilistic features such as language model, translation and reordering probabilities. Of course, one of the attractions of the discriminative framework is that we can throw in all kinds of features in the model, so in theory we could incorporate probabilistic features too. We leave this for future work. We deﬁne ﬁve feature template classes : • phrase pair • source bigram • target bigram • orientation • source distortion Taking as example the derivation in Figure 1, an example of a source bigram feature would be:   1  f100(bi, bj , o) = 0  if src(bi) = “morgen” ∧src(bj) = ich otherwise  An example of orientation feature is :  1          f1200(bi, bj , o) =        0  if src(bi) = “morgen” ∧src(bj) = ich ∧tgt(bi) = “tomorrow” ∧tgt(bj) = i ∧o = DISCON T IN U OU S otherwise  The source bigram feature allows to capture source side reordering patterns. Similarly the target side bigram feature allows to capture ﬂuency on the english side and is therefore a language model-like feature. The phrase-pair feature is a discriminative equivalent to the p(e|f ) generative feature. To alleviate data sparseness for these lexical features, we use a rolling window of 3 words. 6. Experiments We ran experiments on the Prague Czech-English Dependency Treebank (PCEDT) (Hajicˇ, 1998) which consists of 21141 sentences from the Penn WSJ corpus translated into Czech and annotated with morphological information as well as dependency structure information. About 250 sentences each for development and test were translated once into Czech and then back into English by ﬁve diferent translators. We decided to use this dataset for 2 main reasons : (a) it is a small corpus therefore allowing us to train several models in a short time and (b) it is richly annotated with linguistic infomation which we would like to incorporate as features of our model in future work. However, one of the major drawbacks of using the PCEDT is that since Czech is a morphologically very rich language and the training set is small, the corpus suffers from severe data sparseness issues (Goldwater and McClosky, 2005). We obtained word alignments by using the GIZA++ toolkit (Och and Ney, 2003) on the training corpus in both translation directions. The two sets of alignments were then symmetrised using the grow-diag-ﬁnal method previously described in (Koehn et al., 2005) and phrase-pairs consistent with the alignments were extracted. Note that both the perceptron and MIRA are error-driven algorithms in that parameter updates are performed only when the learner is unable to classify a training instance properly. Given that the same training data is being used for phrase extraction and for parameter estimation, overﬁtting is possible. We would instead like to make the training enviroment as hard as the testing one. Since Koehn et al. (2003) show that there is not much performance gain in using phrases of length longer than 3 words, we limit our phrase table to phrases upto 3 words long. 6.1. Comparing models In a ﬁrst set of experiments, we wanted to have an exact comparison between a purely discriminatve approach and a standard generative-hybrid model trained using the same amount of monolingual and parallel data. Our hybrid phrase-based MT system uses the following feature functions: • phrase translation probability (in both directions) • lexical translation probability (in both directions) • word penalty • phrase penalty • language model score • linear reordering penalty • lexicalised reordering model (Koehn et al., 2005) score  Table 1: BLEU score and length ratio on training for oracle  (O) and guessed (G) translations for 1-best MIRA  Iteration  
It is a well-known fact that Machine Translation (MT) systems greatly benefit from user feedback and constant enhancement of the system’s dictionaries. However, getting translators to provide feedback in a consistent and timely fashion is difficult given the time cToofhnpissatsprtaaitpnreatsrnspulrnaedtsieoernntswsihanicmchoemttrhabonidnsalrateitcooenrsnwtwlyitohirnktch.oeWrpPioAthraHotueOtdMfienTedSthb®eacePknAgfHirnoOem,MatnTradSn®wsliasttyhosortuse,tmtahsneytoetfrefaxontrrstalasctptoetrrnuitnlyovnoulspveoefsmutleeddniittc.itnTigohneiassryluogssgutegfsogtrieosfntuisotunarrseeutrsreaisnntgrsilcbattieitoedxnttsos. a few types of common entries and are ranked in order of priority depending on their frequency of occurrence in PAHO’s bilingual corpus. While dictionary entries are suggested automatically by the extractor, a human operator must validate the entries. The process of extraction and incorporation of dictionary entries is described in detail.  The setting PAHOMTS® is a well-established rule-based transfer MT system that has been operational at the Pan American Health Organization (PAHO) for over 25 years (Vasconcellos and León, 1988). It currently translates in six language directions (English-Spanish, SpanishEnglish, English-Portuguese, Portuguese-English, Spanish-Portuguese, and Portuguese-Spanish) and is used to process over 95% of translation jobs received at PAHO Translation Services Unit. One of the keys of the successful use of MT at PAHO is the fact that users and developers work closely together: translators and other users provide feedback to the computational linguists, who incorporate the suggestions into the MT systems. This MT feedback has always been provided in context at PAHO. We understand that contextless feedback may be counterproductive for several reasons. First, the translation error may be due to bad input (format codes, typos, incorrectly split sentences), in which case no action is necessary, or to a bad parse, in which case the action may involve manipulations to the parser or fixing certain codes in existing dictionary entries. Second, the meaning of not-found words may be better decided if the context is taken into account. Third, and most importantly, the dictionary suggestion may be contingent on some elements appearing in the immediate context (a string of words, a certain syntactic construction) or in the general context (certain key words in recent paragraphs, certain elements in the document header, a specific subject matter). The cycle of manual feedback For each translation job, PAHOMTS® creates a side-byside (SBS) file which contains the source segment, the aligned MT output, and flags from the parser indicating whether the segment was completely or partially parsed.  All feedback is provided using the SBS file, and until very recently all feedback was manually provided by translators. The process has evolved over the years:  Originally, the feedback was handwritten by translators on the SBS file, which was a text file that could not be easily manipulated on the screen.  The SBS file later became a Microsoft Word document and a feedback column was added to each segment. Translators typed their feedback in this column and e-mailed the feedback file back to the computational linguists.  The SBS file underwent a third change whereby the MT raw output now appears twice in the second column. The second instance is linked to the corresponding segment in the file with the unedited translation. This allows for synchronized postediting and ultimately creates a SBS file with three columns: the source segment, the MT output, and the final translation (see figure 2 below). Manually provided feedback is first approved by in-house revisers and later analyzed by the computational linguists, who decide which suggestions are doable and whether the change needs to take place at the dictionary level or at the level of the algorithm (format handlers, parser, synthesizer, etc.). The most common types of feedback provided by our translators are: not-found words (NFWs), multi-word expressions (names of programs, institutions, etc), incorrect translations for noun-adjective pairs, nounnoun (NN) compounds in English, noun + preposition + noun (NPN) expressions in Spanish and Portuguese, selection of alternate translations for words depending on the context (nouns, prepositions, verbs, in that order), translation of acronyms, and syntactic or morphological errors.  Manual vs. Automatic feedback PAHO translators are expected to provide feedback for the MT dictionaries for all translation jobs; while every effort has been made to create a postediting environment that encourages translators to provide as much feedback as possible, the reality is that time constraints often prevent translators from providing in-depth feedback for each document. Indeed, our translators understand the importance of feeding the MT dictionaries but the sheer volume of work and the tight deadlines make it very difficult to devote enough time to provide useful suggestions on a consistent basis. As a result, our computational linguists usually receive SBS files where, on average, 5% or less segments contain some sort of translator feedback. With a translation volume of 4.5 million words per year, computational linguists cannot scan every SBS file in search of good dictionary entries. Without feedback from translators, the effort spent on postediting is lost for future translations. To overcome this deficiency, we decided to implement a strategy that would compare the MT and final outputs from bitexts created from past translations, select certain constructions using information from the parser and transfer components, and make suggestions for the MT dictionaries. A similar approach in the 1990s (Nishida and Takamatsu, 1990) also made use of linguistic information and reverse MT engines, but it assumed heavy involvement by posteditors. Their system, while valid for a development environment, cannot be expected to become operational in a production environment, where posteditors simply have time to translate and revise, but not to hand-mark and classify all their changes in the postediting process. Our strategy cannot provide all possible dictionary suggestions, but it can indeed assist the MT developers in our search for large amounts of truly useful dictionary entries, along with their corresponding translations. The expressions in the list of suggestions are checked against an existing bilingual corpus and ranked according to their frequency of appearance in the corpus. The dictionary entry extraction module currently focuses on the following:  Not-found words  Noun + Prepositional Phrase chains in Spanish and Portuguese, and the corresponding NN compound or noun + Prepositional Phrase in English  NN compounds in English  Adjectives modifying nouns  Adverbs modifying verbs  Verb + direct object pairs  Subject + verb pairs  Translation of bound prepositions with verb, noun, or adjective heads  Insertion or deletion of definite articles  This list is open-ended and we plan to add new constructions later. We chose these particular constructions because they are the most common suggestions provided by our translators and because these entries can be easily and quickly added to the dictionaries. We also decided that it was important for the feedback extractor to suggest the type of dictionary entry: 1) Substitution Unit (entry where the single words are subsumed under the larger entry, with a single translation); 2) Analysis Unit (or AU, entry where the words are parsed separately and each have their own translations; the AU assists the parser with POS resolution and lexical selection); 3) noun-noun AU (a special type of AU used for NN constructions), or 4) Transfer Unit (TU, a lexical selection rule which specifies the conditions that must be met in order to select alternate translations for any of the words participating in the TU). All of these are stored in the PAHOMTS® dictionaries (León and Schwartz, 1986). Creating the bitexts The input text for the feedback extractor is a bitext, a document with aligned source and target segments. This bitext is a by-product of the MT process, which undergoes the steps summarized in figure 1 below. Source doc  PAHOMTS  Raw output  SBS file  Synchronized postediting FBK file  tri-aligned file/ bitext file Fig. 1. Creating the bitexts First, the source document is prepared for MT processing (spell-check, some format checks) and is then run through PAHOMTS®, which generates two documents: the unedited translation (raw file, in the same format as the original file) and the SBS file. Both are given to the translator, who then proceeds to postedit the raw translation on the screen using the PAHOMTS® postediting macros. In fact, the translator has the option to work on the raw or SBS files, which are open in synchronized mode. For each segment, the translator may view the original source segment and may also provide feedback in the feedback column of the SBS file, as seen in Figure 2 below.  Figure 2 - A row in the feedback file  Figure 3 – A row in a tri-column aligned file  The original SBS file thus becomes the feedback (fbk) file. The fbk file contains the translator’s suggestions on the third column, the MT output in the top part of the second column, and the final output in the blue shaded row of the second column. If the translator chooses to work on the SBS file, he or she will synchronize the final translation in the blue cell by copying it to the corresponding segment in the raw file. Conversely, if the translation is postedited using the raw file, the final segments will be copied from the raw to the fbk file. All synchronization tasks are carried out using proprietary PAHOMTS® macros. Once the translation has been completed and the feedback has been approved by inhouse revisers, the feedback file is sent to the computational linguists and dictionary coders, who implement the suggestions. At this point, the assistants run a macro that “cleans up” the fbk file by removing all format, parser flags, and feedback, and moving the segments with the final translation into the third column, as shown in Figure 3. The obvious final step to create the bitext is to simply remove the middle column. Thus, for each translation job processed at PAHO Translation Services, we obtain a document with quasi perfect aligned segments that is input to a translation memory, bilingual corpus, terminology extractor, and dictionary entry extractor. Extracting dictionary entries A common approach to extracting dictionary feedback is to use word n-grams and purely statistical methods (Knight and Chander, 1994; Kauchal. and Elkan, 2003; Elming, 2005). In our case, that would involve comparing, using the tri-column aligned file, the MT output and the final translation by performing string matching operations. However, we chose to use the full MT systems at our disposal and to have them guide the process in a more linguistically-oriented fashion, one that  makes use of POS resolution, Noun Phrase structure, and parse trees in order to extract linguistically significant strings. This method allows us to suggest expressions that can become dictionary entries, as opposed to random strings that could never exist in a dictionary. Another reason for preferring linguistic techniques is the inability of statistical techniques to handle long-distance dependencies. The process takes place in four steps: 1. Extraction of source segment, complete parsing, and generation of MT output. Only segments with complete parses (about 65%) are used and partial parses are discarded. As a result of this process, the following data items are created: a. Array of source words and associated dictionary entries, including unique identifiers (ID#) for each word b. Vertical path (string of parts-of-speech) for all words in the segment c. Lexical selection rules that have been triggered for all words in the segment d. Parse tree e. Array of target entries f. MT-generated target segment 2. Extraction of translator’s final segment and complete parsing. Partial parses are discarded. Data items a-d above are created for the target segment. 3. Identification of constructions in the source segment that are likely dictionary entries, along with identification of the corresponding target string, using the source and target parse trees, the source and target arrays, and the unique ID#s. If the target string cannot be identified with precision, the entire target segment is saved as a suggested translation. 4. For each construction in the list of suggestions, deciding whether the source string is a good candidate for an MT entry. The first step is to check whether the MT output and the final output match. a. If they do match, a dictionary suggestion is proposed only in the case of NN compounds for  which no dictionary rule exists. PAHOMTS® generates NN compounds in complete parses and under certain circumstances (Aymerich, 2001), so it might be advisable to add a NN Compound AU to make sure the NN compound is generated correctly regardless of the parse. b. If the MT and final outputs don’t match, the MT dictionary is checked to make sure a lexical selection rule doesn’t already exist. If there is no rule, the feedback extractor identifies the correct translation in the final segment and suggests the type of entry to add to the dictionary. c. If the entry proposed is a not-found word, Substitution Unit or Analysis Unit (both of which apply to contiguous items), its frequency is checked in the bilingual corpus and the frequency score is added to the feedback entry. d. After the entire document is processed, the frequency of each suggestion in the text is also recorded. The end result is a file in which each suggested entry contains five fields, all of which are sortable:  the type of entry (single word, Substitution Unit, Analysis Unit, NN compound AU, Transfer Unit)  the source string (for single words, SUs, and AUs) or the words participating in the lexical selection rule (for TUs)  the suggested target string (for single words, SUs, and AUs), or the suggested translations for each word in a lexical selection rule (for TUs). The target string can be the entire target segment if no good match is identified.  the frequency of the expression in the bilingual corpus. This field is used to sort the feedback entries so that entries with higher frequency appear at the top. The user can also set a frequency threshold below which feedback entries are discarded.  the frequency of the expression in the source document. The following sections describe in detail four of the more common types of dictionary entries extracted. Feedback 1: Complex Noun Phrases The parse tree is used to locate NPs in the source segment. For Spanish and Portuguese, only NPs that contain non-terminal elements are considered, and these can be Prepositional Phrases or other NPs in coordination. For English, NPs that contain more than one noun (nounnoun compounds) are also considered to be complex NPs.  For each valid NP, the feedback extractor creates a list of candidate NPs, all of which must meet the “complex NP” definition outlined above. For example, for the segment Área de Vigilancia Sanitaria y Atención de las Enfermedades, the following NPs would be added to the list of candidates: Área de Vigilancia Sanitaria y Atención de las Enfermedades (words 1-9), Vigilancia Sanitaria y Atención de las Enfermedades (words 3-9), Vigilancia Sanitaria y Atención (words 3-6), Atención de las Enfermedades (words 6-9), and Área de Vigilancia Sanitaria (words 1-4). The following are examples of NPs which would not be considered for feedback extraction because they cannot yield valid entries: hábitos higiénicos y nutricionales saludables, the four official languages (all terminal symbols), otras áreas que requieren ser legisladas, matters which will be taken up (NP contains a Relative Clause). The feedback extractor then processes all NPs in the list, one-by-one. For each element in the NP we have information about the actual word, the word number, the word’s unique identifier, the POS resolved by the parser, and head-modifier relations. Moreover, we have this same information for all the words in the target array created after the transfer component, and we must find the correspondence between the two sets. For example, consider the segments in Table 1 below. One of the NPs extracted for possible feedback is mejoramiento de los estándares de vivienda. The NP is extracted by following the chain of NPs and PPs in the parse. The first word in the NP that contains mejoramiento, after eliminating determiners, is taken as the first word in the source string. We travel down the parse tree by moving from NP to PP, and from PP to NP. The search stops at the last NP that contains only terminal symbols. The last word in the last NP in the chain (vivienda) is taken as the last word in the source string.  The following is some of the information available after the parse tree is converted into a flat table representation:  # word  ID#  pos Role Mod  14 mejoramiento 015584 N  15 de  000100 P  16 los  000013 D  17 estándares  067837 N  18 de  000100 P  19 vivienda  018238 N  Head 17 Head Head  El incremento de la alfabetización y el  The increase in literacy and the  desarrollo de destrezas vocacionales, el  development of vocational skills, the  mejoramiento de los estándares de  improvement of the housing  vivienda, el desarrollo agrícola con un  standards, the agricultural development  objetivo básico nutricional, y el crecimiento with a nutritional basic objective, and  económico con una distribución equitativa de the economic growth with an equitable  los beneficios son requisitos fundamentales distribution of the benefits are essential  para el mejoramiento de la salud en los países requirements for the improvement of  y en la Región.  health in the countries and in the  Region.  Table 1: Sample source, MT, and final segments  For example, greater literacy and the development of vocational skills, improved housing standards, agricultural development with nutrition as the primary objective, and economic growth with equitable distribution of its benefits are essential for improving health in both the countries and the Region.  The target array does not contain a translation for the second preposition de (which has been deleted by the transfer component), and two of the nouns have been reversed:  # word  ID#  pos Role Mod  14 improvement 015584 N Head  15 of 16 the 17 housing 18 standards  000100 P 000013 D 018238 N 067837 N  18 Mod 18 Head  By matching the ID#s of all Spanish head nouns (015584, 018238, 067837), we can conclude that words {14-19} in the source string correspond to words {14-18} in the MT output.  The feedback extractor then searches for the MT output segment {improvement of the housing standards} in the final translation produced by the translator. In this case, because no match is found, we must parse the translator’s version of the sentence, using the converse translation engine (English-Spanish in this case), in order to locate the corresponding English phrase. The English parser produces a parse tree and the POS resolution for the words in the segment. We cannot simply use the unique ID#s because these numbers are not the reverse image of one another in the English-Spanish and Spanish-English translation modules. The parse tree must be used instead.  The feedback extractor traverses the English parse tree, looking for NPs whose head noun is the same as the target for the Spanish head noun (improvement) or the last head noun in the Spanish NP (standards). We must look for both nouns because we don’t know whether the English NP contains a NN compound (housing standard improvement), a Prepositional Phrase (improvement of housing standards), or some other construction. After traversing the parse tree, we cannot locate a NP whose head is improvement, but we can locate one whose head is standards. Using the parse tree, we extract the word numbers for the words in this NP (words 12-14), construct the target string (improved housing standards), and add it to the feedback file. Because the Spanish string contains a preposition and the English string does not, we determine that the suggested entry should be a NN compound AU (NN1 = AU that deletes prepositions and reverses nouns).  The search for the English phrase is restricted by the number of head nouns: the English construction should have the same or less head nouns than the Spanish. We now have a source string, a target string, and an entry type. All we have to do is check the frequency of the Spanish expression mejoramiento de los estándares de vivienda in the Spanish corpus, and add this number to the feedback entry, which looks as follows:  Freq Type Spanish  English  5 NN1 mejoramiento de improved  los estándares de housing  vivienda  standards  Other complex NPs extracted for this sentence are:  Freq Type Spanish  English  114 NN1 mejoramiento de improving  la salud  health  7 NN1 estándares de housing  vivienda  standards  2 NN1 incremento de la greater  alfabetización literacy  
In this paper we compare two methods for translating into English from languages for which few MT resources have been developed (e.g. Ukrainian). The first method involves direct transfer using an MT system that is available for this language pair. The second method involves translation via a cognate language, which has more translation resources and one or more advanced translation systems (e.g. Russian for Slavonic languages). The comparison shows that it is possible to achieve better translation quality via the pivot language, leveraging on advanced dictionaries and grammars available for it and on lexical and syntactic similarities between the source and pivot languages. The results suggest that MT development efforts can be efficiently reused for families of closely related languages, and investing in MT for closely related languages can be more productive than developing systems from scratch for new translation directions. We also suggest a method for comparing the performance of a direct and pivot translation routes via automated evaluation of segments with varying translation difficulty.  1. Introduction The number of translation resources existing for some languages is far greater than for others. There are commercial systems for translation into English from well-resourced languages, such as French or Russian, that can achieve acceptable quality for many practical applications of machine translation. At the same time there are many more languages for which good quality translation resources are not available. For some of those languages MT systems have occasionally been developed, but their lexical and syntactic coverage is very far from what has been achieved for better-resourced languages. This bottleneck can be opened by using Statistical Machine Translation (Och and Ney, 2003), (Marcu and Wong, 2002), which can be trained on parallel corpora for any language pair. However, development of a good quality SMT system requires the use of large collections of parallel texts aligned at the sentence level, amounting to at least several million words. At the same time, parallel corpora of this size tend to be very rare, especially for under-resourced languages. Even for well-resourced languages such resources also tend to be specialised, e.g. Europarl (Koehn, 2005), which covers the language of debates in the European Parliament, so their performance degrades significantly when the system is applied to a slightly different domain, e.g. news (Babych et al., 2007). In this paper we investigate the performance of translation from an under-resourced language into English via a closely-related, or cognate, pivot language with well-developed translation resources. Typically any language can be used as the pivot if it covers the bridge for a language pair that is not available in a given MT system. For instance, if no system translating from French to Japanese is available, English can serve as the pivot for translation from French into English and then from English into Japanese. Sometimes ‘pivot’ is understood as an interlingua, an artificial language implemented with the intention of making MT systems portable between languages (Hutchins, 1995).  The use of a natural pivot language is frequently discouraged because of the argument concerning the loss in translation quality in the process of double translation. This argument is confirmed by anecdotal experience, but to our knowledge there has been no published evaluation of the actual drop in quality. The method proposed in this paper is novel in two respects. First, our pivot is closely related to the source language. Second, we use a parallel corpus to evaluate and compare the output quality of a direct MT process with that of a pivot MT process. MT between closely related languages has been very successful, achieving near-publishable quality (which needs very little or no post-editing) for a number of historically and structurally-related languages, such as Czech and Slovak (Hajic et al., 2000b), Catalan and Spanish (Alonso, 2005), Ukrainian and Russian (Gryaznukhina, 2004). Such engines explore similarities between the related languages (Dvorak et al., 2006) and typically rely on shallow processing techniques and knowledge-light linguistic resources (Armentano-Oller et al., 2005). High quality makes such MT systems useful in the pivot-based MT framework, which we take here to mean that the text is translated in several stages via one or more intermediate natural languages, or pivots. Overall translation quality crucially depends on the quality of the weakest link in the pipeline, which is usually the stage between more distant languages. From an engineering perspective, therefore, it is beneficial to use the best available MT system for that stage, even if there is no access to its source code. The only existing reference to an approach involving pivot-based translation via related languages is the work of Hajič and his colleagues on Česílko, an MT system for translation of software manuals from English into and between Czech and Slovak (Hajic et al., 2000b). However, their system is designed for high quality translation to Slovak and is supplemented with a translation memory system. Hajic et al. (2000b) dismiss the quality of automatic pivot translation but do not give any figures to support their position. In Section 2 of this paper we present the design of our experiment for translating via a pivot language and the  methodology for evaluating its quality. In Sections 3 and 4 we discuss the results and implications for pivot-based MT via closely related language. Then in Section 5 we outline the prospects for the development from scratch of pivot MT systems using comparable corpora.  2. Method To test the impact of the pivot framework on MT quality, we first established a baseline for pivot MT: from Russian into English via French and German (relatively distant languages). We then performed pivot MT via closely related language: from Ukrainian into English via Russian. We compared the results with direct MT from Russian and Ukrainian into English. We used a parallel corpus from a Ukrainian political newspaper Mirror Weekly (http://www.mirrorweekly.com), which is published on-line in Ukrainian, Russian and English. All texts selected for our corpus appeared between January and March 2007, but describe a broad range of topics: domestic politics, international relations, financial policy, science, information technology, etc. The majority of articles are originally written in Ukrainian, some originate in Russian and two texts – in English (these are an interview and an article by a British diplomat). Table 1 presents the characteristics of the corpus.  Language Texts Paras Sentences  Ukrainian  35 1449  4675  Russian  35 1449  4528  English  35 1449  3513  Table 1: Parameters of MT evaluation corpus  Words 64575 65181 68445  The size of our corpus is almost twice that of the DARPA 94 MT evaluation corpus of 36k words (White et al., 1994), which has been widely used for such tasks and has been shown to be sufficient for automated MT evaluation methods (e.g., BLEU) to ensure high correlation with human evaluation scores for translation adequacy and fluency (Babych et al., 2004). The corpus was aligned on the paragraph level and MT-translated into English using commercial MT systems available for Ukrainian, Russian and English. Table 2 gives the characterisitics of the MT systems used for the experiment.  MT Pragma  Version / Dev 2.0 (2002) Trident Soft  Plaj-Ruta ProMT XP  5.0 (2003) ProLingLtd. 3.0 (2002) ProMT  Systran  5.0 (2004) Systran S.A.  Table 2: MT systems  Source L Ukrainian Russian Ukrainian Russian Russian German French  Target L English Russian English Russian English German French English  The quality of MT was measured using the standard BLEU metric (Papineni et al., 2002), as well as the less commonly used WNM (Weighted N-gram Model), which on large corpus has been shown to produce a better correlation with human adequacy judgments (Babych and  Hartley, 2004). BLEU and WNM are in some sense complementary, measuring different quality parameters: WNM assigns salience scores (similar to tf.idf) to Ngrams, which rewards matches of those content words that are most important for the general text structure. So its correlation with adequacy can be expected to be higher. BLEU, however, is a better predictor for fluency, since it does not disregard matching sequences of function words. BLEU was computed with one reference and N-gram size 5 (BLEUr1n5). The automated scores were computed for direct translation from Russian and Ukrainian into English, then for each pivot pipeline and for intermediate stages of the pivot translation. We also computed these scores for each text and for each paragraph in the corpus, ranked the segments by the difference in direct translation and for pivot scores, and manually checked some extreme examples with the biggest difference. No formal human evaluation was carried out; however, since the corpus is large enough and homogeneous in terms of text genres and MT architectures (all systems are Rule-Based) we can interpret the differences in automated evaluation scores as likey differences in MT quality of the evaluated systems and pivot pipelines. 3. Results In our experiment we compared the results for the pivot translation and the (best available) direct translation for two types of pivot. The first type is translation via a traditional distant language pivot from a well-resourced language, here Russian (for which a wide range of linguistic resources is available in the public domain, as are several commercial MT systems which translate between Russian and various other European languages). The second type is translation via a closely related pivot from a relatively under-resourced language, here Ukrainian (for which there are fewer freely available resources, MT systems or MT translation directions). The purpose of the experiment was to establish whether pivot introduces in all cases a loss of quality large enough to justify the development of a direct system, or whether any loss of quality can be within acceptable bounds, allowing the developers to effectively reuse existing MT systems for supported translation directions within the pivot framework, and to concentrate on the supposedly easier tasks of developing MT between closely related languages. Determining the size of the MT evaluation corpus Automated evaluation scores such as BLEU can be used for comparison of different MT systems only on a sufficiently large corpus: on smaller texts there is little correlation with human judgments about translation quality. In addition systems compared should have been developed with the same type of MT architecture – rulebased, statistical, etc.). Otherwise, the scores can be useful for monitoring the development of the same MT system over time, but not for comparing one system with another. In our first experiment we tried to establish whether the size of our MT evaluation corpus is sufficient for comparing MT systems, to ensure a high correlation between automated scores and human judgments. For this task we used the DARPA 94 corpus, for which human evaluation scores are available. Chart 1  summarises the correlation with human adequacy judgments and standard deviation of scores on data samples of different sizes taken from the corpus: for chunks of 1, 5, 10, 20 33, 50 and 100 text (each text about 360 words). Chart 2 presents the same results for correlation with human fluency judgments.  
The paper presents and evaluates a wide coverage, rule-governed machine translation system for Danish-English. Analysis and polysemy resolution are based on Constraint Grammar dependency trees. In its 85.000 lexeme lexicon, Dan2eng uses context-sensitive lexical transfer rules linking dependencies to semantic prototype conditions, syntactic function, definiteness etc. Dependency is further exploited instead of constituent bracketing to support syntactic movement rules. A robust derivational and compound analysis, as well as a separate NER module permit the handling of unrestricted text from a wide range of genres. The system averaged TER scores of 7 (BLEU 0.55-0.6) on student tasks, but performance varied widely against raw and edited Europarl references, respectively.  
Accurate and timely information on global public health issues is key to being able to quickly assess and respond to emerging health risks around the world. The Public Health Agency of Canada has developed the Global Public Health Intelligence Network (GPHIN). Information from GPHIN is provided to the WHO, international governments and non-governmental organizations who can then quickly react to public health incidents. GPHIN is a secure Internet-based “early warning” system that gathers preliminary reports of public health significance on a near “real-time” basis, 24 hours a day, 7 days a week. This unique multilingual system gathers and disseminates relevant information on disease outbreaks and other public health events by monitoring global media sources such as news wires and web sites. This monitoring is done in nine languages with machine translation being used to translate non-English articles into English and English articles into the other languages. The information is filtered for relevancy by an automated process which is then complemented by human analysis. The output is categorized and made accessible to users. Notifications about public health events that may have serious public health consequences are immediately forwarded to users. GPHIN employs a “best-of-breed” approach when it comes to the selection of the machine translation ‘engines’. This philosophy ensures that the quality of the machine translation is the best available for whatever language pair selected. It also imposes some unique integration and operational problems. GPHIN has a broad scope. It tracks events such as disease outbreaks, infectious diseases, contaminated food and water, bio-terrorism and exposure to chemicals, natural disasters, and issues related to the safety of products, drugs and medical devices. GPHIN is managed by Health Canada’s Centre for Emergency Preparedness and Response (CEPR), which was created in July 2000 to serve as Canada’s central coordinating point for public health security. It is considered a centre of expertise in the area of  civic emergencies including natural disasters and malicious acts with health repercussions. CEPR offers a number of practical supports to municipalities, provinces and territories, and other partners involved in first response and public health security. This is achieved through its network of public health, emergency health services, and emergency social services contacts. 
Abstract We present a new unsupervised syntax-based MT system, termed U-DOT, which uses the unsupervised U-DOP model for learning paired trees, and which computes the most probable target sentence from the relative frequencies of paired subtrees. We test U-DOT on the German-English Europarl corpus, showing that it outperforms the state-of-the-art phrase-based Pharaoh system. We demonstrate that the inclusion of noncontiguous phrases significantly improves the translation accuracy. This paper presents the first translation results with the data-oriented translation (DOT) model on the Europarl corpus, to the best of our knowledge.  Introduction: Phrase-Based vs Syntax-Based Machine Translation Phrase-based and syntax-based methods in MT have complementary strengths and shortcomings. While phrase-based methods have been highly successful (Koehn et al. 2003), it has often been noted that such methods are too constrained for translating discontiguous constructions like take SB by surpise (Chiang 2005; Nesson et al. 2006). Shieber (2007) gives evidence that more than half of the entries in the HarperCollins Italian College Dictionary can be subject to ‘noncontiguity’. Yet, many syntax-based methods have achieved only small (or no) improvements over purely phrase-based methods. It has been noted that the disappointing contribution of syntactic methods may be due to the traditional notion of syntactic constituent which often harms rather than helps in finding a correct translation (see Chiang 2005). A wellknown example is the German-English pair Es gibt and There is, that are both non-constituents. Purely linguistically syntax-based systems therefore often underperform phrase-based methods (e.g. Yamada and Knight 2001). What would be needed is a system that takes into account contiguous as well as discontiguous phrases, regardless whether they form linguistically motivated constituents. In this paper we start an investigation into using a successful unsupervised parsing system, known as UDOP (Bod 2006, 2007) for providing the tree structures for bilingual corpora such as the Europarl corpus. U-DOP induces a probabilistic tree-substitution grammar (PTSG) from raw data, and has achieved some of the best unsupervised parsing results in the literature (Klein and Manning 2002, 2004; Dennis 2005; Seginer 2007). We  will use the structures induced by U-DOP for extending the so-called Data-Oriented Translation (DOT) system (Poutsma 2000; Hearne and Way 2003) towards unsupervised DOT, which we will term U-DOT. U-DOT starts by assigning all possible alignments between paired trees bootstrapped by U-DOP and uses the (smoothed) relative frequencies of the subtree pairs to compute the most probable target sentence given a source sentence. This leads to an MT model which takes into account all possible contiguous as well as discontiguous phrases. Our model is riminiscent of the hierarchical phrasebased model of Chiang (2005) and the synchronic probabilistic tree-insertion grammar model of Nesson et al. (2006), but it differs also from these models in various ways. Firstly, we make use of a structure bootstrapping model, U-DOP, which computes the probability of each tree by summing up the probabilities of its derivations by means of Viterbi n best. This probability model, which is also used in computing the best translation, makes the model sensitive to both large and small subtrees. Secondly, our model only uses substitution as a combination operation between subtrees, while Shieber (2007) has shown the importance of including the insertion operation. DOP models with the insertion operation have been developed in Hoogweg (2003), and will be extended to MT in future research. Our model is congenial to Galley et al. (2006) who also use subtrees as productive units in a synchronous tree-substitution grammar for MT. In the following, we will first briefly review the DOT model, and show how it can be generalized to unsupervised MT by extending it with U-DOP. We next discuss the algorithmic background of this new U-DOT model and present experiments involving machine  translation from German to English with the Europarl corpus. We end with a conclusion. Data-Oriented Translation (DOT) The Data-Oriented Translation model (DOT) uses the DOP model (Bod et al. 2003) as a basis for statistical MT (Poutsma 2000; Hearne and Way 2003, 2006; Groves et al. 2004). DOT starts with a bilingual treebank where each tree pair constitutes an example translation pair and where translationally equivalent constituents are linked, as e.g. in figure 1 for the English-French Click Save – Cliquez sur Enregistrer (taken from Groves et al. 2004).  S VN click save  S  V  PP  cliquez P  N  sur enregistrer  Figure 1. Aligned trees for the English-French pair Click Save – Cliquez sur Enregistrer as used in DOT  Like DOP, the DOT model then uses all linked subtree pairs from the bilingual treebank to form a probabilistic tree-substitution grammar (PTSG) where the productive units consist of linked subtrees which are used to compute the most probable translation of a target sentence given a source sentence. Linked subtrees from the tree pair in figure 1 are given in figure 2 (also the entire tree pair constitutes a linked subtree).  summing up the probabilities of all derivations (which is in practice computed by Viterbi n best derivations). The probability of a derivation is the product of the probabilities of the subtree pairs involved in it, while the probability of a subtree pair is estimated by its (smoothed) relative frequency in the aligned treebank. As to date, all experiments with DOT have been carried out on very small, manually annotated treebanks such as the HomeCentre corpus of 810 parsed and aligned sentence pairs (see Hearne and Way 2006). The extension of DOT to larger treebanks will run into formidable annotation tasks. While Groves et al. (2004) show how the alignment task can be partly automated, there is an additional issue in how far – if at all – DOT combines syntactic and phrase-based information. Since DOT is based on linguistically motivated syntax, the model equals the notion of phrase with the notion of syntactic constituent. However, it is well known by now (e.g. Koehn et al. 2003; Chiang 2005) that such an approach has difficulties in capturing phrase-pairs that go beyond constituents, such as the German-English pair Ich möchte… - I would like to… which are both nonconstituents. On the other hand, purely phrase-based methods have difficulties in capturing discontiguous translation pairs such as the English-Italian the nearest airport to Trento - l’ aeroporto più vicino a Trento, which reflects the dicontiguous translation pair nearest NP1 to NP2 - NP1 più vicino a NP2. What would be needed is a DOT model which is not based on pre-annotated trees but a DOT model which allows for any substring, be it contiguous or discontiguous, to form a potential ‘constituent’. This can be accomplished by using the unsupervised U-DOP model for learning trees, resulting in a new model which we will call U-DOT.  S VN click  S  V  PP  cliquez P  N  sur  N  N  save enregistrer  Figure 2. Linked subtrees from the translational tree pair in figure 1  The probability model for this PTSG is similar to the DOP1 model (Bod et al. 2003) and is given in Poutsma (2000), Groves et al. (2004) and others: the probability of a target sentence given a source sentence is computed by  Unsupervised Data-Oriented Translation (U-DOT)  U-DOT is based on an extension of the DOP model to unsupervised parsing known as U-DOP (Bod 2006). UDOP assigns all unlabeled binary trees to a set of given sentences (possibly tagged), and next takes (in principle) all subtrees from these binary trees to compute the most probable trees. For example, the tagged WSJ sentence Investors suffered heavy losses has a total of five different binary trees, as shown in figure 3 (where each root node is labeled with S and each internal node is labeled with X).  S  S  X X  X X  NNS VBD JJ NNS NNS VBD JJ NNS  Investors suffered heavy losses Investors suffered heavy losses  S X X  S X X  NNS VBD JJ NNS NNS VBD JJ NNS  Investors suffered heavy losses Investors suffered heavy losses  S  X  X  NNS VBD JJ NNS Investors suffered heavy losses  Figure 3. All binary trees for Investors suffered heavy losses as proposed by U-DOP  Although the number of binary trees for a sentence grows with the Catalan number, the total set of (unlabeled) binary trees can be stored efficiently by a packed parse forest. The underlying idea of U-DOP is to use (the frequencies of) all subtrees from this binary tree set to compute the most probable tree for each sentence. Subtrees from the trees in figure 3 include for example the subtrees in figure 4.  S X X NNS VBD Investors suffered  NNS Investors  S X X NNS losses  VBD suffered  X X  X NNS VBD Investors suffered  S  X X JJ NNS heavy losses  X JJ NNS heavy losses X VBD JJ suffered heavy  Figure 4. Some subtrees from the trees in figure 3  Thus U-DOP takes into account both contiguous substrings like Investors suffered X and non-contiguous substrings like Investors X losses. This property carries over to our unsupervised extension of DOT. In Bod (2007) we have shown how a parse forest of binary trees can be converted into a compact PCFG in  the vein of Goodman (2003), and which we will summarize in the next section. The PCFG reduction of parse forests allowed us to induce trees for very large corpora in Bod (2007), such as the four million sentences NANC corpus (Graff 1995). These large experiments could be accomplished also thanks to an efficient estimator known as DOP* (Zollmann and Sima’an 2005). While the resulting U-DOP model was called U-DOP* in Bod (2007), we will continue to refer to the model as UDOP in the current paper as long as no confusion arises. U-DOP was been evaluated on English, German and Chinese, obtaining some of the best unsupervised results in the literature (Bod 2007). However, compared to supervised parsers, U-DOP’s results are considerably lower: where U-DOP obtains about 70% unlabeled fscore on the standard section 23 of the Wall St Journal, many supervised parsers obtain around 91% on the same set. Yet it should be kept in mind that the evaluation on hand-parsed data unreasonably favors supervised parsers. For instance, U-DOP learns constituents for word sequences such as We would like to… and There are…, which in the Penn Treebank are non-constituents. While U-DOP is thus punished for this ‘incorrect’ prediction if evaluated on the Penn Treebank, this property of U-DOP may be beneficial if evaluated in the context of machine translation using the Bleu score. Thus U-DOP can discover phrases which are typically neglected by linguistically motivated syntax-based translation models. At the same time, the model can also learn discontiguous dependencies that are typically neglected by phrase-based MT systems (Koehn et al. 2003). The extension of DOT with U-DOP is now straightforward. Instead of starting from treebanks, we start from unlabeled corpora and use our new implementation of U-DOP in Bod (2007) to infer the ‘best’ binary trees directly for word strings from bilingual corpora. Next, we assign links between each two tree nodes for each sentence pair and compute the most probable translation for a held-out data set from the relative frequencies of the subtree pairs (see next section). We will refer to this unsupervised version of DOT as UDOT. To give a simple illustration of U-DOT, consider the German-English pair Es gibt viele Zeitungen and There are many newspapers for which U-DOP induces respectively the structures in figure 5 (we leave the words untagged): [[Es gibt]X [viele Zeitungen]X]S [[There are]X [many newspapers]X] S Figure 5. Two structures induced by U-DOP Then, U-DOT assigns links between all subtree pairs, as in figure 6:  [Es gibt]X – [There are]X [Es gibt]X – [many newspapers]X [[Es gibt]X [viele Zeitungen]X]S – [[There are]X [many newspapers]X]S [viele Zeitungen]X – [There are]X [viele Zeitungen]X – [many newspapers]X Figure 6. Possible alignments between Es gibt viele Zeitungen and There are many newspapers according to U-DOT Many of these alignments would result in incorrect translations. How does U-DOT rule out incorrect alignments such as Es gibt - many newpapers on the basis of frequency only? It is easy to see that we only need to observe one other sentence pair with Es gibt, for example Es gibt keine Mitglieder … - There are no members … to make the alignment Es gibt – There are more frequent than alternative alignments for Es gibt.1 Converting Parse Forests into PCFG Reductions In principle we can use an O(n3) CKY-style parsing algorithm for (U-)DOT which first parses the source string, after which the target string is derived from it by following the links. The main computational problem is how to deal with the large number of subtrees. There already exists an efficient supervised algorithm that parses a sentence by means of all subtrees from a treebank. This algorithm was extensively described in Goodman (2003) and converts a DOP-based PTSG into a compact PCFG reduction that generates eight rules for each node in the treebank. Goodman’s reduction is based on the following idea: every node in every tree is assigned a unique number which is called its address. The notation A@k denotes the node at address k where A is the nonterminal labeling that node. A new nonterminal is created for each node in the training data. This nonterminal is called Ak. Let aj represent the number of subtrees headed by the node A@j, and let a represent the number of subtrees headed by nodes with nonterminal A, that is a = Σj aj. Then there is a PCFG with the following property: for every subtree in the training corpus headed by A, the grammar will generate an isomorphic subderivation. For example, for a node (A@j (B@k, C@l)), the following eight PCFG rules in figure 7 are generated, where the number following a rule is its weight. 
Within the METIS-II project1, we have implemented a machine translation system which uses transfer and expander rules to build an AND/OR graph of partial translation hypotheses and a statistical ranker to ﬁnd the best path through the graph. The paper gives an overview of the architecture and an evaluation of the system for several languages. 
Most current statistical machine translation (SMT) systems make very little use of contextual information to select a translation candidate for a given input language phrase. However, despite evidence that rich context features are useful in stand-alone translation disambiguation tasks, recent studies reported that incorporating context-rich approaches from Word Sense Disambiguation (WSD) methods directly into classic word-based SMT systems, surprisingly, did not yield the expected improvements in translation quality. We argue here that, instead, it is necessary to design a contextdependent lexicon that is speciﬁcally matched to a given phrase-based SMT model, rather than simply incorporating an independently built and tested WSD module. In this approach, the baseline SMT phrasal lexicon, which uses translation probabilities that are independent of context, is augmented with a context-dependent score, deﬁned using insights from standalone translation disambiguation evaluations. This approach reliably improves performance on both IWSLT and NIST ChineseEnglish test sets, producing consistent gains on all eight of the most commonly used automated evaluation metrics. We analyze the behavior of the model along a number of dimensons, including an analysis conﬁrming that the most important context features are not available in conventional phrase-based SMT models. 
An image-based document translation system consists of several components, among which OCR (Optical Character Recognition) plays an important role. However, existing OCR software is not robust against environmental variations. Furthermore, OCR errors are often propagated into the translation component and cause, causing poor end-to-end performance. In this paper, we propose an imagebased document translation using an error correction model to correct misrecognized words from OCR output. We train our correction model from synthetic data with different fonts and sizes to simulate real world situations. We further enhance our correction model with bigrams to improve our word segmentation error correction. Experimental results show substantial improvements in both word recognition accuracy and translation quality. For instance, in an experiment using Arabic Transparent Font, the BLEU score increases from 18.70 to 33.47 with the use of our noisy channel model.  Introduction In an information society, we communicate with people and information systems through diverse media in increasingly varied environments. Advanced technologies have bridged many gaps for such communication. While Internet technology provides a shortcut overcoming distance barriers, machine translation (MT) technology helps us to overcome barriers to communicate with people who use different languages. Much information is in written form embedded in various environments, such as on a paper, on a wall, on a bulletin board, etc. As digital cameras become popular, an image-based MT system is able to capture information in a variety of environments and translate it from a source language into another target language for different applications. For example, we have developed a sign translation system to translate Chinese into English for tourist applications (Yang et al., 2001; Yang et al., 2002). In this research, we are developing an image-based system that translates Arabic document images into English. The system works as follows. After an image is captured from a digital camera, the system preprocesses the image to account for fonts, skew, rotation, illumination, shadows, glare, reflection, and other sources of variability. Subsequently, it automatically detects text regions in the image, performs recognition using off-the-shelf OCR (Optical Character Recognition) software on the text regions, and then translates the text strings into English using our state-of-the-art statistical MT system (Zhang and Vogel, 2007). An image-based MT system consists of many components. Its performance relies on not only the machine translation (text-to-text) technology, but also other component technologies. A good image-based document translation system requires robust technologies for text detection, OCR, and language translation. Traditional pipeline approaches yield error propagation, and errors in any component of an image based document translation system can affect the end-to-end performance of the entire system. The errors can be propagated through the system and even amplified during the propagation process, e.g., a  
A monolingual Chinese-to-Chinese SMT model as well as a global optimization strategy are proposed in this paper to extract equivalent Chinese terms (such as “雷射”lae-ser and “激光”giguan for “laser”) which are used in different areas of the various Chinese-speaking communities. Preliminary evaluation shows that the synonymous traditional Chinese (TC) terms for simplified Chinese (SC) terms can be identified with an accuracy of 84% on a small test set. On the other hand, the traditional-tosimplified Chinese term translation achieves 87% accuracy. Furthermore, the global optimization strategy generally improves the performance by decreasing search errors. The main idea behind the model is to create “parallelleft/right contexts” out of non-parallel web pages for term alignment. The monolingual SMT model, by its very nature to find translation equivalents can potentially be useful for finding synonym sets (synsets) for any generic monolingual lexicon. The potential for adapting such a model for mining large synsets from nonparallel corpora is therefore expectable. Introduction & Motivation Problems with Regional Variations Regional variation in language usage is an important barrier for communication between people of different regions, even though they may share a common core of one language. This situation had been observed across the Mainland China, Hong Kong, Taiwan, Singapore and some South Asia regions, where the Chinese language is used as the same core but with different variations in vocabularies. For instance, while ‘雷射’lae-ser is used in Taiwan for ‘laser’, the same term is expressed as ‘激光’ gi-guan in the Mainland China, which does not only use different characters but also represent the same meaning in a very different way. Similar variations had also been observed even across regions that use Chinese characters in different languages. This is exactly the situation between the Chinese and Japanese languages. In language processing applications, such regional variation will result in difficulties that are frequently observed in cross-language applications. For instance, to acquire information from a search engine, one may have to provide the above two forms of “laser”in order not to miss any interesting information for things like “laser printer”.  On the other hand, such variant terms can be regarded, in some sense, as synonyms. Therefore, language processing tasks using mixed corpora with regional variations as the training materials may have to normalize the corpora to the same canonical form before being used for training. In summary, many information-processing applications will suffer from such regional variation if correct translation between the variant terms cannot be well resolved. 
We introduce an approach to incorporate the constituent structure constraint into a discriminative word alignment model by presenting the constituent constraint in an explicit way and using three operations to ensure the constraint when search the best word alignment. In this way, we will be able to make use of the weak order constraint induced by the inversion transduction grammars (ITG), as well as the flexibility of the discriminative word alignment framework to incorporating any other useful features.  
2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e | f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion:  R  ˜e∗ = arg max max ˜e a  λrhr(˜e, f , a),  r=1  where ˜e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜e, and hr(˜e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides the decoder, Moses provides tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. Actually, the decoder is used to generate not only the best translation of the given source sentence but the list of Nbest translation hypotheses. The list is then augmented through the procedure detailed in the following section and ﬁnally re-ranked. Re-ranking is computed by means of a new log-linear model that includes new feature functions which are listed in Section 4. Finally, the resulting best scoring entry is output as ﬁnal translation.  3. N-best List Expansion The rationale behind our approach is that alternative translation hypotheses can be obtained by combining substrings  1http://www.statmt.org/moses/  occurring in the N-best list. In fact, it could be the case that the best scoring translation in the list is wrong and that a correct translation could be obtained by replacing some of its words with portions taken from other translations in the N-best lists. The actual implementation of our idea implies the enlargement of the N-best list with new hypotheses generated through an n-gram LM estimated on the N-best list itself. Assuming that a partial hypothesis (e1 e2 e3 e4 e5) is available (see Figure 1), this can be expanded by one word through an n-gram whose ﬁrst n − 1 words match the last n − 1 words of the hypothesis.  partial hyp  e1 e2 e3 e4 e5  n-gram  e4 e5 e6  new partial hyp e1 e2 e3 e4 e5 e6  Figure 1: Expansion of a partial hypothesis via a matching n-gram.  Input: N-best translation hypotheses of some source string  Initialization: n = some integer P HYP = ∅ F HYP = ∅ NGRAM = ∅  (size of n-grams) (set of partial translations) (set of complete translations) (set of n-grams in N-best list)  
This paper addresses the problem of word reordering in statistical machine translation. We follow a word order monotonization strategy making use of syntax information (dependency parse tree) of the source language to build a set of automatically extracted reordering rules. The input sentence is extended to a graph built with reordering hypotheses, hence, allowing for a constrained search on the syntactically motivated reorderings. Experiments are reported on the BTEC corpus (Chinese to English task) Results are presented regarding translation accuracy and computational eﬃciency, showing signiﬁcant improvements in translation quality at a reasonable computational cost. 
 †Department of Computer Science University of Illinois at Urbana-Champaign Urbana, Illinois 61801, USA ksmall@uiuc.edu  Abstract Word alignments, the mappings between source and target language words for two languages, are a critical component of statistical machine translation. A long-standing issue in statistical machine translation is that the quality of word alignments does not correlate as well as would be expected with measures of translation quality. A number of recent papers have shed light on this issue by improving on existing metrics such as Alignment Error Rate and examining the importance of word alignment quality in terms of phrase alignments. In this paper, we attempt to elucidate this situation further by ﬁrst presenting a new word alignment evaluation metric, Word Alignment Agreement F1 (WAAF1), which improves upon existing alignment quality metrics. We then present experiments which demonstrate that WAAF1 also correlates better with measures of translation quality than do previous metrics.  Introduction Statistical approaches to building machine translation systems typically includes the important step of aligning the words of translated sentence pairs, called bitexts. This alignment between source and target language words is called word alignment. A long-standing conundrum in statistical machine translation is that automatic measures of word alignment quality, such as Alignment Error Rate (AER) (Och and Ney, 2003), do not correlate as well as might be expected with automatic measures of translation quality, such as BLEU (Papineni et al., 2001). A number of recent papers have shed light on this issue by improving existing word alignment metrics like AER and examining the importance of word alignment quality in terms of phrase alignments (Fraser and Marcu, 2006; Lopez and Resnik, 2006). In this paper, we attempt to elucidate this situation further, by extending an alternative word alignment evaluation metric called Word Alignment Agreement, ﬁrst discussed in (Davis, 2002), which is a more appropriate measure for the word alignment task because it conserves the mass of each word. Word Alignment Agreement (WAA) is a symmetric measurement which treats words as the core unit when measuring alignment quality, rather than the links between words, so that the alignment quality for each word ‡Kevin Small participated in this research while working as a summer intern in Motorola’s Human Interaction Research Lab.  has an equal impact. We demonstrate that while WAA is limited to certain ideal alignment conﬁgurations, it can be straightforwardly extended to account for all types of word and phrase alignments. We next show through examples why the new metric, WAAF1, improves upon existing metrics. The remaining sections of the paper demonstrate empirically how this new metric yields good correlation with translation quality for individual types of word alignments, and, more importantly, how it yields signiﬁcantly better correlation across different types of alignments than possible with previous metrics.  Word Alignment Evaluation Background and Related Work  While AER has been the most widely used word alignment evaluation (WAE) metric, Fraser and Marcu (2006) demonstrate that it is less than ideal because it does not appropriately penalize unbalanced precision and recall. In its place, they propose to use F-measure, as shown below, where the parameter α can be used to tune the relative importance of precision and recall. F-measure is said to be balanced when α equals 0.5.  (1)  Precision(A, P ) =  |P ∩ A| |A|  (2)  Recall(A, S) =  |S ∩ A| |S|  (3)  AER(A, P, S) = 1 −  |P  ∩ A| + |S ∩ A| |A| + |S|  (4)  F-measure(A, P, S, α) =  
 This paper describes a statistical machine translation system based on freely available programs such as Moses. Several new features were added, in particular a two-pass decoding strategy using n-best lists and a continuous space language model that aims at taking better advantage of the limited training data. We also investigated lexical disambiguation methods in the translation model based on POS information. The task considered in this work is the translation of the European Parliament Plenary Sessions between English and Spanish, in the framework of the TC-STAR project. The described systems performed very well in the 2007 TC-STAR evaluation.  Introduction Automatic machine translation was one of the ﬁrst natural language processing applications investigated in computer science. From the pioneer works to today’s research, many paradigms have been explored, for instance rulebased, example-based, knowledge-based and statistical approaches to machine translation. Statistical machine translation (SMT) seems today to be the preferred approach of many industrial and academic research laboratories, each of them developing their own set of tools. In 1999 however, a summer workshop at Johns-Hopkins University hosted the creation of the EGYPT toolkit1, on which the widely used training tool Giza++ (Och and Ney, 2003) is based. Later, the Pharaoh phrase-based decoder (Koehn, 2004) became available and distributed in binary form2, but as far as we know, Pharaoh was not widely used. More recently, another workshop3 released an open source toolkit, which includes a decoder, Moses (Koehn and al., 2007), and a comprehensive set of softwares and scripts to build a complete SMT system—namely determining word alignments, extracting phrases, performing the translation and tuning system parameters. In this paper, we describe the development of a state-of-theart SMT system based on the Moses suite. Several new features were added, in particular a two-pass decoding strategy using n-best lists and a continuous space language model (CSLM) that aims at taking better advantage of the limited training data. The described system participated in the 2007 TC-STAR evaluation and achieved very good rankings. We also investigated lexical disambiguation methods based on POS information, which can be interpreted as an intermediate step between “standard” phrase-based models and factored translation models. The latter approach is meant to tightly integrate linguistic information into the translation model, and is implemented in Moses, but to the best of our knowledge, experimental results have not yet been published. This paper is organized as follows. In the next section, the 1http://www.clsp.jhu.edu/ws99/projects/mt/toolkit/ 2http://www.isi.edu/licensed-sw/pharaoh/ 3http://www.clsp.jhu.edu/ws2006/  LIMSI SMT system architecture is presented, as well as its training and tuning procedures, and its unique features. The following section describes the task on which the system is evaluated and the data available to train the models. Finally, experimental results are provided and commented. The paper concludes with a discussion of future research issues.  System architecture  The goal of statistical machine translation is to produce a target sentence e from a source sentence f that maximizes the posterior probability:  e∗ = argmax Pr(e|f ) e  = argmax Pr(e, A|f )  (1)  e  A  ≈ argmax max Pr(e, A|f )  (2)  e  A  In the above equations, A denotes a correspondence between source and target words and is called an alignment. The Moses decoder makes the so-called maximum approximation as in Equation 2. The Pr(e, A|f ) probability is modeled by a combination of feature functions, according to the maximum entropy framework (Berger et al., 1996):  Pr(e, A|f ) ∝ exp λifi(e, A|f )  (3)  
 In this paper we address the problem of translating unknown words in a statistical machine translation framework. In data-driven machine translation, words that are not seen in the data may not be translated and are either discarded or left as is in the output. They are refered to as unknown words. The unknown word problem increases when the available bilingual data is scarce. In order to address this problem, we propose to use proportional analogy at the character-level to translate unknown words. We study and report results of the integration of this approach into a statistical machine translation system translating from Japanese to English with relatively scarce resources. Objective evaluation measures suggest that the translated sentences have a higher adequacy than that produced by a baseline system , while their ﬂuency is similar.  Introduction In data-driven machine translation, computation is often performed at the level of what intuition hints at being a word in segmenting languages. In the framework of statistical machine translation (Brown et al., 1990), language resources are typically segmented into such tokens before they may be used as training data. These words or tokens may take into account other linguistic information and features made explicit by, say, morphological analysis, the presence of punctuation, etc. Therefore, there is not one segmentation scheme, but many acceptable ones. For instance in the case of machine translation, optimal segmentation highly depends on the language pair that is considered and is, paradoxically, often linguistically unintuitive. One of the most important problems of data-driven machine translation is that posed by unknown words: in the process of translating, a system is bound to encounter words that were unseen in the available training data. While this is in part due to the aforementioned segmentation issues, it is also often simply due to the lack of training data. This issue becomes exponentially plaguing as available linguistic resources get scarce, and as low-occurence content words often remain untranslated. A number of works have tackled the unknown word problem: (Sinha, 2005) uses a heuristicbased identiﬁcation and translation method, (Uchimoto et al., 2001) propose a model based on morphological analysis and large amounts of lexical information contained in a dictionary, while (Nagata, 1999) proposes to estimate Part-Of-Speech information of unknown words using a statistical model of morphology and context. In this work, we propose to address the problem of translating unknown words by going at a lower level than that of words, and to translate unknown words as  strings of characters. We propose to use proportional analogy on character strings to capture commutations that occur inside words, in order to grasp more linguistic information from the available data and to compensate insufﬁcient segmentation schemes. Following this introduction, we ﬁrst describe the mechanism of proportional analogy on character strings. We then show how proportional analogy allows to translate strings of characters given a set of example data. This translation method is then set in a statistical machine translation (SMT) framework: the system provides additional aligned data that feed the unknown word analogical translation engine, and in a second pass translates the test set with the translated unknown words. We evaluate the proposed method by performing an experiment in Japanese to English translation using data originating from the IWSLT evaluation campaign. So as to put ourselves in a position where only scarce resources are available, we reproduce the conditions of the IWSLT OPEN track, in which only 40, 000 sentence pairs where available. Translations are then evaluated using automatic evaluation measures BLEU and NIST. Finally, we discuss results in terms of translation quality and show that the proposed method yields a signiﬁcantly higher adequacy of sentences while maintaining their ﬂuency when compared to a baseline translation system. Proposed method Proportional analogy at the character-level Analogical computation exploits equations of the form: A : B :: C : x where A, B, and C are character strings. The equation may then yield a solution x = D in the form of another  character string. This operation allows to capture lexical and syntactical variations along paradigmatic and syntagmatic axes, without explicitly decomposing the strings into fragments. The operation does not require any preprocessing of the aligned examples, such as a step of segmentation into words, as it operates strictly at the character-level. While translating, proportional analogy distributes the information at a lower level than that of words, all over the target string of characters. An algorithm has been developed (Lepage, 1998) , that allows to determine if four terms are in an analogy, and to generate a fourth term if three terms are placed in an analogy. The algorithm may then yield zero, one, or many solutions to the equation. Analogical translation of unknown words In this work, we propose to use analogy on character strings in order to translate unknown words. As argued in (Denoual, 2006), Natural Language Processing approaches that rely on word tokens as the single unit of processing neglect the fact that corresponding pieces of information in different languages are distributed over the entire strings of data, and therefore do not necessarily correspond to complete words. The algorithm that we use here is similar to that exposed in (Lepage and Denoual, 2006). Suppose that we want to translate the character string D, and that we have a bilingual corpus at our disposal. We ﬁrst put D in a analogical equation1 in the source part. We then form all analogical equations with the input sentence D and with all relevant2 pairs of sentences (Ai, Bi) from the source part of the bilingual corpus: Ai : Bi :: x : D The application of the above-mentioned algorithm may allow to solve the equation and yield a solution x. If x does not belong to the source part of the bilingual corpus, we try to translate x recursively in the same manner until one solution is part of the corpus. If x = Ci,j belongs to the source part of the bilingual corpus, we may then use its translation x = Ci,j to form all possible analogical equations in the target side of the bilingual corpus: Aik : Bik :: Ci,j k : y Such equation may yield a solution y = Di,jk, which is a translation of the character string D. As different equations may yield different or identical solutions, translations are sorted by their frequencies and the top one is selected. 1Let us stress that the equation is entirely monolingual. 2Theoretically, all possible pairs of sentences in the corpus should be placed in the analogical equation. Because this is not feasible practically for a large corpus, heuristics may be used to select the most relevant pairs of sentences.  Example Suppose that we want to translate the following Japanese string into English: χϡʔϤʔΫߓ ‘3 /nyu¯ yo¯ kuko¯ / Among all possible pairs of strings in the Japanese part of the corpus, we may ﬁnd the following two Japanese strings:  ্ւ /shanhai/  ↔ Shanghai  ্ւߓ /shanhaiko¯ /  ↔ Shanghai Harbor  
 Diacritics in Arabic are optional orthographic symbols typically representing short vowels. Most Arabic text is underspeciﬁed for diacritics. However, we do observe partial diacritization depending on genre and domain. In this paper, we investigate the impact of Arabic diacritization on statistical machine translation (SMT). We deﬁne several diacritization schemes ranging from full to partial diacritization. We explore the impact of the deﬁned schemes on SMT in two different modes which tease apart the effect of diacritization on the alignment and its consequences on decoding. Our results show that none of the partial diacritization schemes signiﬁcantly varies in performance from the no-diacritization baseline despite the increase in the number of types in the data. However, a full diacritization scheme performs signiﬁcantly worse than no diacritization. Crucially, our research suggests that the SMT performance is positively correlated with the increase in the number of tokens correctly affected by a diacritization scheme and the high F-score of the automatic assignment of the particular diacritic.  
 We present pruning strategies for translation models that are based on estimating the relevance of phrase pairs. We apply the overall  translation system to a set of data and collect a number of statistics for each phrase pair. Using these statistics in various scoring terms  we are able to significantly outperform baseline pruning methods and we can show that the number of phrase pairs can be reduced by  up to 80% without significantly affecting the overall system performance.  this reason we concentrated on translation models  
Andrei Popescu-Belis  ISSCO/TIM/ETI University of Geneva 40, bd. du Pont-d’Arve 1211 Geneva, Switzerland paula.estrella@issco.unige.ch  ELDA 55-57, rue Brillat-Savarin 75013 Paris, France, and LIPN, University of Paris XIII 99, av. J.-B. Clément 93430 Villetaneuse, France hamon@elda.org  ISSCO/TIM/ETI University of Geneva 40, bd. du Pont-d’Arve 1211 Geneva, Switzerland andrei.popescu-belis@issco.unige.ch  Abstract Evaluating the output quality of machine translation system requires test data and quality metrics to be applied. Based on the results of the French MT evaluation campaign CESTA, this paper studies the statistical reliability of the scores depending on the amount of test data used to obtain them. Bootstrapping is used to compute standard deviation of scores assigned by human judges (mainly of adequacy) as well as of five automatic metrics. The reliability of the scores is measured using two formal criteria, and the minimal number of documents or segments needed to reach reliable scores is estimated. This number does not depend on the exact subset of documents that is used.  Introduction A large number of metrics have been proposed to evaluate machine translation systems, as summarized for instance in the FEMTI framework (Estrella et al. 2005). However, comparatively fewer studies have been devoted to the test data needed by evaluation metrics, and in particular to the amount of data that is required to obtain reliable scores. Indeed, both human and automatic metrics generally assign a score to each translated segment, often comparing it to one or more reference translations of the same segment. While it is commonly acknowledged that a “large” number of segments is needed to obtain statistically significant scores, the goal of this article is to provide empirical estimates of this amount based on observations from a recent MT evaluation campaign. This article thus analyzes the effect on MT evaluation scores of varying test set sizes, and proposes formal methods to study the robustness of metrics as the numbers of test documents increases. The article first discusses related work, and then the data—systems, test data and scores—used throughout the study. The bootstrapping technique is then introduced, which will be used to compute average values and standard deviations for human metrics (mainly adequacy) as well as for automatic metrics (BLEU, NIST, mWER, mPER and GTM) and its application to the study of reliability is then explained. The scores obtained for various document samples are then discussed along with the effect of document ordering. A method to compute a sufficient number of documents for each metric is further proposed, and its results for both human and automatic metrics are finally discussed. Studies of the Required Size of Test Data Studies regarding the influence of the test data on the reliability of scores are not common in MT evaluation. For instance, few guidelines indicate the number and size of documents to be used in an evaluation, or the effect on scores of various sizes of the test set. This is unlike the case of training data for statistical NLP systems, where studies of the influence of size of training data on output  quality are more frequent, e.g. for statistical or examplebased MT (Germann 2001), as well as for many other domains, e.g. question answering systems (Clarke et al. 2002; Dumais et al. 2002). Closer to our present goal, Elliott et al. (2003) explicitly attempt to answer the question of how much text to include in a multilingual corpus for MT evaluation, given the general hypothesis that more text would lead to more reliable scores. Their work concerns human metrics— fluency, adequacy and informativeness—and mainly focuses on the ranking of systems based on the results of the FR/EN, SP/EN and JP/EN DARPA 1994 MT evaluation campaign. The scores were compared for an increasing number of texts, starting with 1 and ending with 100 texts, the average length of texts being 350 words. Based on an empirical assessment of score variation, the authors estimate that systems could be reliably ranked with around 40 texts (ca. 14,000 words), and that using ten texts already separate the highest and the lowest ranked systems. These figures can be compared with the amounts used in a number of previous evaluations which generally use several hundred to several thousand sentences (Elliott et al. 2003: Table 1). Zhang and Vogel (2004) also studied the influence of the amount of test data on the reliability of automatic metrics, focusing on confidence intervals for BLEU and NIST scores. They used the data of the CH/EN track of the TIDES 2002 MT evaluation campaign (100 documents of 7-9 sentences each), with the output of the 7 participating systems and 4 reference translations. Their results show that BLEU and NIST scores become stable when using around 40% of the data (around 40 documents or 300 sentences), though stability is defined here in terms of the distance between scores of different systems. These two studies suggest that an evaluation can be reliably performed with less text than is often used. We reinforce this hypothesis here, and propose a formal method to estimate the necessary amount of test data, which evaluators could use to assess the amount of test data needed by a given metric.  Data and Metrics: CESTA EN/FR Campaign The experiments presented here were done using the test data, system outputs and evaluation metrics from the French MT evaluation campaign, CESTA (Hamon et al. 2006). The test data comes from the first run of the campaign, on the English to French translation task, in which five systems have participated. The results of the systems are anonymized, and for the present purpose the systems will simply be referred to by the codes S1 to S5 in no particular order. The systems participating to this run were: Comprendium, RALI / University of Montreal, Reverso / Softissimo, SDL, and Systran. One of the goals of the CESTA campaign was to validate the use of automatic evaluation metrics with French as a target language, by comparing the results of well-known automatic metrics with fluency and adequacy scores assigned by human judges. The following automatic metrics were applied to the translations produced by the five systems participating in the CESTA campaign, with four reference translations: mWER, multiple reference Word Error Rate (Niessen et al. 2000), mPER, position independent Word Error Rate (Tillmann et al. 1997), BLEU (Papineni et al. 2001), NIST version of BLEU (Doddington 2002). We added to this experiment the GTM (General Text Matcher) metric (Turian et al. 2003). The test data, i.e. the corpus created for the CESTA evaluation campaign, English to French first run, consists of 15 documents from the Official Journal of the European Communities (JOC, 1993), with a total of 790 segments or sentences, with an average of 25 words per segment (Hamon et al. 2006). The data consists of transcribed questions and answers in a parliamentary context, and since no particular domain was targeted when putting together the corpus, the CESTA campaign considered this as general domain data. The goal of the experiments presented here is to observe how the average scores obtained by human judges and automatic metrics evolve, as more documents are incrementally added to the evaluation corpus. More specifically, the experiments attempt to test whether these scores stabilize towards their final value as more documents are added, and to find a method to determine a sufficient amount of test data to reach this value with reasonable precision. Bootstrapping over MT Evaluation Scores This section describes the bootstrapping technique used to compute average scores and related statistics, first from a theoretical point of view, then in the setup used here. Estimating Variables Using Bootstrapping Bootstrapping is a statistical technique that is used to study the distribution of a variable based on an existing set of values (Efron and Gong 1983). This is done by randomly resampling with replacement (i.e. allowing repetition of the values) from the full existing sample and computing the desired parameters of the distribution of the samples. The method has the practical advantage of being easy to implement and the theoretical advantage of not presupposing anything about the underlying distribution of the variable. A simple programming routine can thus calculate the estimators of the mean, variance, etc., of any random variable distribution. Moreover, when the original sample is resampled a large  number of times (theoretically close to infinite), the law of large numbers ensures that the observed probability approaches (almost certainly) the actual probability. The bootstrapping algorithm can be summarized as follows: 1. Given a sample X = (X1, X2, …, Xn) from a population P, generate N random samples of size n by drawing n values from the sample, with replacement (each value having probability 1/N). 2. The resulting population P*, noted X* = (X1*, …, XN*), with Xi* = (Xi1*, Xi2*, …, Xin*), i = 1..N, constitute the N bootstrapped samples. 3. If the original estimator of a given population parameter was θ(X), with the bootstrapped samples we can calculate the same estimator as θ(X*). An important parameter for bootstrapping is N, the number of bootstrapped samples, or the number of times the process is repeated. This number should be large enough to build a representative number of samples. It appears that, for instance, N = 200 leads to slightly biased estimations (Efron and Gong 1983; Zhang and Vogel 2004), so a larger N is preferred, for example N = 1,000 (Efron and Gong 1983; Koehn 2004) or even N = 10,000 (Bisani and Ney 2004). Based on these examples, we decided to use N = 1,500. Another source of error in inference statistics is the error induced by using a particular sample to represent a whole (unknown) population. In the present case, this amounts to considering that the scores on the 15 documents (or 790 segments) are fully representative of a system’s performance on this type of text. Application to MT Evaluation Scores In the MT field, bootstrapping has been mainly used to estimate confidence intervals for automatic metrics and to compute the statistical significance of comparative performance of different MT systems, e.g. using the BLEU (Koehn 2004; Kumar and Byrne 2004; Zhang and Vogel 2004) or WER metric (Bisani and Ney 2004). Here, bootstrapping will be used to compute reliable estimators for different automatic metrics for MT, namely mean, standard deviation (often expressed as a percentage of the mean) and confidence intervals (based on standard deviations) for the mean of the bootstrapped sample. For the application of bootstrapping in MT, the original sample X is the set of text segments arranged in documents, each segment being accompanied by a list of scores obtained by each MT system, according to the metrics mentioned in the previous section. Described in pseudo code, the routine computing the various estimators is particularly simple: M is the number of segments to be considered, sample[m] is the m-th element of the sample while sample* is a pointer to the list of bootstrapped samples: for(n=0; n<N; n++){ for(m=0; m<M; m++){ sample[m] = selectRandSeg(); } scoreList[n] = calcMetric(sample*); } calcEstimators(scoreList);  The test corpus consists of 15 documents, noted d1 … d15. Despite the slight differences between their lengths, a document is the most reasonable incremental step in our bootstrapping study, since a document offers in theory the highest topical homogeneity across sentences—as the exact topics may change from one document to another. The following algorithm evaluates the systems and computes parameters related to each of the metrics, for each document subset D, incrementally constructed by adding one document at a time, starting with D = {d1}. 1. Select one system and one metric to be applied, say Sk and mi , where k = 1, 2, 3, 4, 5 and mi∈{adequacy, fluency, BLEU, NIST, GTM, mWER, mPER}. 2. Apply mi to each translated segment of D output by Sk. 3. Bootstrap N times to compute mean, relative standard deviation and confidence intervals for the mean score of mi. 4. Add one more dj to the evaluation set D, following the order j = 2, .., 15 (or a random order). 5. Repeat steps 2 – 4. The process is of course repeated for every metric and every system. At the end of the process, the mean, standard deviation and confidence intervals are available for each system and each metric. The following sections make use of these results to analyze the sufficient size of the subset of documents D based on formal criteria to get reliable scores. Variation of Average Scores Depending on the Size of the Test Data The results of bootstrapping with 1, 2, and up to 15 documents are discussed in this section, in terms of standard deviation and comparison with the global scores obtained when the full test data (15 documents) is used. The results are given first for the human metrics, then for automatic ones. The next section will then attempt to determine the minimal number of documents leading to evaluation scores that are not substantially different from those obtained on the full data set. Human Metrics Bootstrapping was performed on human metrics, computing the average scores for one document first, then for two, three, etc. The order of the documents was firstly the one used in the CESTA campaign, and secondly, in a different experiment, a random order. Figure 1 shows the evolution of average adequacy scores computed over 1, 2, …, 15 documents, for the five systems evaluated in the CESTA campaign (fluency values show a similar pattern). The observed trend is that, after some initial variation due to the heterogeneity of documents and to the systems’ performance, the scores quickly reach their final values over the entire test set. A notable exception is system S5, having scores particularly low on the first document, which penalizes also its performance on {d1, d2}, {d1, d2, d3}, etc. The lower performance of S5 on d1 may simply be due to the inevitable variation of system performance on different texts (e.g. caused by missing vocabulary) as no other cause could be identified. Similarly, S1 performs better on documents d4 and d5; however, in the case of S5 and d1, the lower performance on the first document of the series is much more perceptible graphically.  65  60  55  50  45  S1  S2  S3  S4  S5  40 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  Figure 1: Average adequacy values (on a 0-100% scale) for systems S1 to S5, computed on 1, 2, …, 15 documents Automatic Metrics Turning now to average scores of automatic metrics, Figure 2 and Figure 3 display the scores obtained using the GTM and mWER metrics (error rate means that lower scores are better). These figures display a similar pattern: after chaotic variation on the first subsets of documents, the ranking of the systems becomes quickly close to the final one, and the average scores reach their final values quite soon as well.  0.85  0.8  0.75  S1  S2  S3  S4  S5  0.7  
We introduce a novel approach to the task of lexical translation. We utilize the translation graph, a massive lexical resource where each node denotes a word in some language and each edge denotes a word sense shared by a pair of words. Our current graph contains 1,267,460 nodes and 2,315,783 edges. The graph is automatically constructed from machine-readable dictionaries and Wiktionaries. Paths through the graph suggest word translations absent from any of the input dictionaries. We deﬁne a probabilistic inference procedure that enables us to quantify our conﬁdence in a translation derived from the graph, and thus trade precision against recall. We demonstrate the graph’s utility by employing it in the PANIMAGES cross-lingual image search engine. Google retrieves images based on the words in their “vicinity”, which limits the ability of a searcher to retrieve them. Although images are universal, an English searcher will fail to ﬁnd images tagged in Chinese, and conversely. PANIMAGES addresses this problem by translating and disambiguating queries, using the translation graph, before sending them to Google. Our experiments show that, for queries in “minor” languages, PANIMAGES increases the number of correct images in the ﬁrst 15 pages of results by 75%.  
In this paper, we describe an Automatic Rule Refiner that, given online post-editing information, traces errors back to problematic translation rules and proposes concrete fixes to such rules. Evaluation results on an English-to-Spanish Transfer-based MT system show that this approach generalizes beyond sentences corrected by bilingual speakers to unseen data. We show that by applying automatic refinements, higher translation accuracy can be achieved as measured by automatic evaluation metrics.  accuracy for similar sentences that have not been corrected by bilingual speakers. Such a system is highly relevant for language pairs with little or no parallel data. However, any existing Transfer-Based MT (TBMT) system could be automatically improved by an Automatic Rule Refiner similar to the one described here. The main goal of this paper is to illustrate the impact of automatic rule refinements on previously unseen data. According to three different automatic metrics (METEOR, BLEU and NIST) the refined system outperforms the baseline system after just a few user corrections (p=0.0051). 2 Related Work  
 1. Introduction Many approaches have tried to automatically acquire translation equivalents from bilingual corpora. These approaches can be organised in a continuum according to the type of bilingual input required to acquisition. The input bitext is ranged from well aligned parallel corpora (Melamed, 1997a; Tiedemann, 1999; Vintar, 2001; Guinovar, 2004; Gamallo, 2005) to unrelated non­parallel corpora (Rapp, 1999), going through intermediate levels such as noisy parallel (Melamed, 1997b; Fung, 1995b), pseudo­parallel (Utsuro, 2002), and related non­parallel corpora (hereafter ``comparable corpora'') (Fung & McKeown, 1997; Fung & Yee, 1998; Gamallo & Pichel, 2005). The approaches relying on (well aligned, noisy, or pseudo) parallel corpora are endowed with two positive properties: on the one hand, their aligned segments can be easily used as bilingual anchors to extract translation correspondences, and on the other, external bilingual dictionaries are not required. They can be considered as knowledge­poor approaches. By contrast, their main shortcoming is the fact that parallel corpora are not easily available. As regards approaches relying on (comparable or unrelated) non­parallel corpora, they are more abundant, less expensive, and easily available via web. However, translation equivalents are not easily found because of the use of fuzzy and vague bilingual anchors. Moreover, in many cases, external bilingual dictionaries are required to remedy the lack of aligned segments and meaningful bilingual anchors within the texts. They are knowledge­ rich approaches. In this paper, we describe an unsupervised acquisition method (Section 3) provided with the positive features of related work. More precisely, our strategy aims at extracting translation equivalents from comparable corpora without requiring external bilingual resources. To find meaningful bilingual anchors within the corpus, we use some bilingual correspondences between lexico­ syntactic templates previously extracted from small parallel texts. So, our approach inherits three positive features: it makes uses of meaningful bilingual anchors, it is a knowledge­  poor strategy, and it relies on easily available non­parallel corpora to train the extractor. In Section 4.1, we perform several experiments using different lists of bilingual lexico­syntactic templates. In one of these experiments, we compare the use of bilingual templates extracted from parallel corpora with the use of those extracted from general­purpose bilingual dictionaries. The results will show that a domain­specific parallel corpus, even if very small, provides with more accurate information than general­purpose dictionaries to extract translation equivalents from comparable corpora. Unlike related work on extraction from comparable corpora, the evaluation protocol defined in this section introduces recall scores. This is one of the main contributions of this paper, since it will allow comparing our results with future work on non­parallel corpora. This simple comparison was not possible so far because, in related work, evaluation was performed measuring only the translation accuracy of the most frequent words in the training corpus. Finally, in Section 4.2 we will describe an experiment with an improved version of the extraction method. 2. Related Work 
John Hutchins 89 Christchurch Road Norwich NR2 3NG United Kingdom w_john_hutchins@yahoo.co.uk  Abstract Marking the ten-year anniversary of the launch of Babel Fish, the first ever free online machine translation (MT) service that went live on the Internet in late 1997, this paper sketches the background that led to its development, giving an account of its origins and of the early stages of its evolution. Several competitors have entered the field of web-based MT over the last decade, and the paper offers a review of the most significant contributions in the literature with a particular focus on two key issues: firstly, the role that these online MT tools have played in meeting the translation needs of the users, and secondly the impact that they have had on the MT-related industry and business. Information coming from a variety of sources, including data on current usage supplied by the online MT providers themselves for the purposes of this study, testifies to the massive increase in the use of the leading multilingual online MT services over the last ten years. On this basis, the conclusion assesses the future prospects of Internet-based MT. Keywords: online MT, Internet, Web, history  1. Origins and Early Developments Fulfilling predictions since the late 1980s that MT services would become available on the Internet, the first online MT service was provided from 1988 by the Systran Centre in Paris to subscribers of the French postal service’s Minitel network (restricted to France). Users could send texts for translation from their PC or Macintosh and receive results (22 lines a minute, at a charge of about $1.20 per page) – the language pairs offered were French to English, German to English, and English to French (Gachot, 1989). According to Ryan (1987: 100), MT software provided by Systran was potentially accessible to 4.5 million users of Minitel in France. The next concrete proposal was for an online service that would be available more widely. In September 1992, it was announced that CompuServe was investigating the possibility of offering MT to its subscribers (Harrison, 1992). This project involved a six-month evaluation period to test the output quality and the overall performance of existing MT systems for the language pair English-German (in both directions): “CompuServe’s basic goal for MT is to provide draft-quality translation directly to end users. […] We suspect that there is a market for low-cost translations, even if the quality is less than ideal” (Harrison, 1992: 11). Although not yet offering online MT as it is commonly understood today, i.e. in the form of services that users can directly log on to in order to have input texts or webpages of their choice translated, the pioneering experiences at CompuServe laid the foundations for further crucial developments in the following 15 years which are the focus of this paper. Mary Flanagan, a computational linguist based in the USA, led the Advanced Technologies Group at CompuServe from 1992 until 1998, and regularly reported the groundbreaking developments at CompuServe. For example, Flanagan & Jensen (1994) describe the early implementation of an entirely automated MT process, whereby the messages posted in English on selected  CompuServe forums were periodically collected, fed through Intergraph’s Transcend system, and the output in French and German displayed in parallel versions that could be read online by people unfamiliar with English. The original forums (in English) and the machinetranslated versions (in French and German) presented the same contents and the same structure, i.e. the threading and sequence of the multilingual postings were identical. The usage statistics reported in Flanagan (1995) are quite impressive: MT was used for translation between English and three other languages, namely French, German and Spanish, and during the first month of operation, on just one of the more than 600 specialist interest forums available on CompuServe, more than 900,000 words were translated at a speed of over 3,000 words per minute. Details were also given of further plans and services (e.g. a low-cost post-editing service for email translation) to be launched for CompuServe subscribers, with an assessment of the commercial opportunities offered by the deployment of MT in the online environment, as well as of the associated challenges. In a later report, Flanagan focuses on the usage patterns and on the reactions of the users who are exposed to MT output for the first time in the online environment: 25% of them abandon the service after receiving the first translations, possibly because they are surprised by the poor quality of the raw output and find it impossible to understand or use effectively. Interestingly, following the launch of their online MT facility, CompuServe received “hundreds of angry e-mail messages, as well as hundreds of resumes from translators” offering their services (Flanagan, 1996a: 193). The report also says that “users were overwhelmingly satisfied with the quality of the translations […] and several large users routinely submit jobs totalling more than 10,000 words per week” (ibid.: 194). Flanagan (1996b: 244) summarises the philosophy of these initial attempts to offer online MT to registered users as follows:  CompuServe has taken a pragmatic approach to MT technology, focusing on finding a market niche for what it can do – generate rapid, very rough draft, information-scanning quality translations in an environment where quick scanning for content is more important than high quality. However, retaining some quality was still a concern. Consequently, CompuServe’s Document Translation Service offered its subscribers the possibility of uploading documents for MT and requesting an optional post-editing service, which was charged at a rate per-word that was ten times higher than the rate for raw MT output (Flanagan, 1996b: 245). Flanagan (1997a: 25) states that 85% of the requests were submitted for unedited translation (i.e. raw MT output), and that additional professional post-editing at the higher rate was requested in only 15% of the cases. Commenting on CompuServe’s MT-related services, Bonthrone (1996: 4) reports that post-edited jobs tended to be larger than those for which only raw MT was requested. As a result, in terms of word count there was a more balanced ratio of roughly 60% raw MT output vs. 40% post-edited content. Flanagan perceptively referred to online translations as “MT’s new frontier” (Flanagan 1997b) – as the following years were to demonstrate. Whilst the importance of the role played by CompuServe in introducing MT to a large population of new users (more than two million, according to MT News International, 1999: 15) cannot be overestimated, the access to MT provided by CompuServe via the Web was still restricted, in that it was limited only to registered subscribers. The online MT scene changed radically on December 9, 1997 with the launch of Babel Fish, which made MT available free of charge to any Internet user. It was the result of a partnership between Systran Software Inc. and the well-known search engine AltaVista.1 After its experience with Minitel in France, Systran had started to offer online translations of webpages from its own website since 1996 (Yang & Lange, 1998: 276). Now, it made the service more widely available. 2. Online MT and Its Use It was Babel Fish that ensured the unprecedented global visibility and accessibility of MT on the Internet. However, it came with a few surprises. Yang & Lange (1998: 282), reporting on users’ feedback and usage behaviour in the first few months of its operation, observed significant usage in areas that were either not anticipated by the developers and providers of the service (e.g. as a tool for language learning, cf. McCarthy, 2004 and Somers et al., 2006), or deprecated by them (i.e. as an entertainment tool, getting it to perform ‘back-and-forth’ translations or to translate idioms). Initially Babel Fish covered ten language combinations involving five major European languages, and Yang & Lange (2003) provide an update on its usage by a growing group of Internet users (more information is available in the Appendix). They report that between 1998 and 1999 the most popular language combination was for translations from English into Spanish, followed by English-French, 
Abstract We present a novel word reordering model for phrase-based statistical machine translation suited to cope with long-span word movements. In particular, reordering of nouns, verbs and adjectives is modeled by taking into account target-to-source word alignments and the distances between source as well as target words. The proposed model was applied as a set of additional feature functions to re-score N-best translation candidates generated by a statistical machine translation system featuring state-of-the-art lexicalized reordering models. Experiments showed relative BLEU score improvement up to 7.3% on the BTEC Japanese-to-English task, and up to 1.1% on the Europarl German-to-English task.  1. Introduction In machine translation (MT), one of the main problems to handle is word reordering. A word is “reordered” when it and its translation occupy different positions within the corresponding sentence. In Statistical MT (SMT) (Brown et al., 1993), word reordering is faced from two points of view: constraints and modeling. If arbitrary wordreorderings are permitted, the exact decoding problem is NP-hard (Knight, 1999); it can be made polynomialtime by introducing proper constraints, such as IBM constraints (Berger et al., 1996a) and Inversion Transduction Grammars (ITG) constraints (Wu, 1997). Among all the allowed word-reorderings, it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is that of providing a measure of the plausibility of word movements. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model local reorderings, short idioms, insertions and deletions that are sensitive to local context. They are a simple mechanism but powerful enough to really improve performance (Koehn et al., 2003; Och and Ney, 2004). Nevertheless, they are able to capture only local phenomena. In (Chiang, 2005) an interesting extension toward hierarchical phrases was proposed, which allows one to predict long-span reordering phenomena, too. In this work we present a novel word reordering model. In particular, our goal is to model reorderings concerning three major part-of-speech (POS) classes, namely nouns, verbs and adjectives. Relevant statistics are collected from wordaligned parallel texts regarding the distance between target words and the distance between the corresponding source words. The model was applied as a set of additional feature functions for re-scoring N-best lists generated by a phrasebased SMT system. The paper is organized as follows. Section 2 highlights some relevant and typical reordering phenomena occurring between German and English, two languages which of-  original German sentence: in wien gab es eine große konferenz . literal English translation : in vienna was held a major conference . reordered English sentence: a major conference was held in vienna Figure 1: German to English translation example. ten show signiﬁcant word movements. Section 3 encompasses an overview of major approaches to the problem of word reordering. Section 4 brieﬂy introduces our phrasebased SMT system. Section 5 presents our novel reordering model. Then, in Section 6 experiments on the BTEC Japanese-to-English task and on the Europarl German-toEnglish task are described and results are discussed. Finally, some conclusion are drawn in Section 7. 2. Example of Word Reordering In many cases, German and English show very different word orders. Consider the example reported in Figure 1. If the original German sentence (ﬁrst entry) is translated word by word into English, the result is the string of the second entry. Some word movements (underlined) are required to get the syntactically correct version of the English sentence (see third row). In particular, a swap of the position of the constituents “in vienna” and “a major conference” is observed. The phenomenon occurring here is due to the fact that in English the verb follows the subject, while in German the case is the opposite. This is only a simple example, but the characteristics of the two languages often yield longdistance word movements. In order to capture such aspects of the translation in a general manner, a phrase-based system should be enhanced by means of effective distortion models. In the following section, a brief overview of the most signiﬁcant previous attempts of attacking the reordering problem is given, together with a discussion of the advantages our approach should have over them.  3. Related Work One of the main research areas in SMT is word/phrase reordering models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai, bi−1), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted ﬁnite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and inverted. Any block reordering is allowed, no matter whether it was observed in training or not. A global reordering model is presented in (Nagata et al., 2006) that explicitly models long distance reordering. It predicts four types of reordering patterns: monotone adjacent, monotone gap, reverse adjacent and reverse gap. By collapsing into the same neutral class monotone gaps and reverse gaps, it models only three possible events similarly to local reordering models (Tillmann and Zhang, 2005). The distortion model proposed in (Al-Onaizan and Papineni, 2006) assigns a probability distribution over possible relative jumps conditioned on source words. It consists of three components: outbound, inbound and pair distortion. The model’s parameters are directly estimated from word alignments. In (Lee and Roukos, 2004) and (Lee, 2006), the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. POS rules are applied for preprocessing the source side both in translation model training and in decoding. All models referred to above were tested on different language pairs, including Arabic, Chinese, English, German and Japanese languages. Apart Chinese, which is typologically inconsistent (Newmeyer, 2004), each one of other languages has its own grammatical properties which are peculiar but nevertheless comparable. Hence, the reordering model  we propose in this work tries to exploit the “grammatical compatibility” between source and target languages. In fact, we try to model the movements of three major part of speech classes (verbs, nouns and adjectives), looking at where the words translated so far are located. Our model considers the reorderings from the target language point of view, namely English. Moreover, differently from what can happen in lexicalized models, our model does not suffer from data sparseness, since statistics are collected for POS classes instead of plain words. 4. The Phrase-based SMT System Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e | f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996b; Och and Ney, 2002), the optimal translation can be searched for by exploiting a set of feature functions, designed to model different aspects of the translation process. Our translation system works in two steps. In the ﬁrst stage, the beam search decoder available in Moses (Koehn et al., 2007),1 computes an N-best list of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. In the second stage, the N-best translations are re-scored by applying additional feature functions and re-ranked: the top-ranked translation is ﬁnally output. The log-linear models used in both steps have interpolation parameters which are estimated from a development set by applying a minimum error training procedure (Och, 2003). The reordering model presented in the following section is the only additional feature function applied for re-scoring the N-best lists. 5. The POS-based Reordering Model We assume that we have a parallel training corpus provided with inverted word alignments, that is alignments from target to source positions. Let (f , e) be a source-target sentence pair, and let a be an inverted alignment which maps target positions i into source positions ai = j. For any target position i, we look for its predecessor i∗ that is aligned to the rightmost source position. Our interest is indeed in the difference between the two positions, denoted by ∆i. Formally:  ∆i =  ai − ai∗ 1,  if i > 1 if i = 1  i∗ = arg max ak w<k<i  where w denotes the window size. By setting w to zero, i∗ is searched among all the positions covered so far. Intuitively, ∆i is negative when some word reordering occurred: namely when some source position following ai has  1http://www.statmt.org/moses/  i  
 We describe an approach to automatic source-language syntactic preprocessing in the context of Arabic-English phrase-based machine translation. Source-language labeled dependencies, that are word aligned with target language words in a parallel corpus, are used to automatically extract syntactic reordering rules in the same spirit of Xia and McCord (2004) and Zhang et al. (2007). The extracted rules are used to reorder the source-language side of the training and test data. Our results show that when using monotonic decoding and translations for unigram source-language phrases only, source-language reordering gives very signiﬁcant gains over no reordering (25% relative increase in BLEU score). With decoder distortion turned on and with access to all phrase translations, the differences in BLEU scores are diminished. However, an analysis of sentence-level BLEU scores shows reordering outperforms no-reordering in over 40% of the sentences. These results suggest that the approach holds big promise but much more work on Arabic parsing may be needed.  
The paper describes an evaluation methodology to evaluate speech-to-speech translation systems and their results. The evaluation scheme uses questionnaires filled in by human judges for addressing the adequacy and fluency of audio translation outputs and was applied in the second TC-STAR evaluation campaign. The same evaluation methodology is carried out both on the outputs of an automatic system and on human translations produced by professional interpreters. The obtained results show that the professional interpreters obtain better results on fluency. But surprisingly, the automatic systems results on adequacy are higher than for the human translator. But this has to be reconsidered by the fact that humans have to translate in a real time, and select important information, while an automatic system tends to translate all the information.  Introduction The TC-STAR project1 is a long-term effort to advance research in the core technologies of Speech-to-Speech Translation (SST). The project targets a selection of speech domains (speeches and broadcast news) and three languages: UK English, European Spanish and Mandarin Chinese. To assess the advances made in all SST technologies, annual competitive evaluations are organized. In the second evaluation campaign of TCSTAR which took place in February and March 2006, an end-to-end evaluation was carried out. A Speech-to-Speech Translation system is composed of an Automatic Speech Recognizer (ASR) chained to a Spoken Language Translation (SLT) module and to a Text-To-Speech (TTS) component in order to produce the speech in the target language. Evaluations of individual components (ASR, SLT and TTS) have been done many times in the past and are also evaluated in TC-STAR. Methods, protocols and metrics for the evaluation of independent ASR, SLT and TTS modules have already been established. Performance of speech recognition systems is typically described in terms of word error rate (WER). For SLT automatic metrics such as BLEU (Papineni et al. 2001) or mWER (Nießen et al., 2000) are used. Performance of SLT systems can also be measured by human judges who can usually measure fluency, adequacy, etc. of the translations. A numerical indication of the perceived quality of TTS synthesized audio data is provided by Mean Opinion Scores (ITU-T, 1993). The MOS is generated by averaging the results of a set of standard subjective tests. To evaluate the performance of a complete speech-tospeech translation system, we need to compare the source speech used as input to the translated output speech in the target language. The proposed methodology enables to measure the fluency and the adequacy of the translated output. We first give a quick overview of the TC-STAR evaluation results. Then we describe tasks and languages of the experiments, and the end-to-end evaluation methodology and protocol. Finally, the results obtained by the TC-STAR system and the human interpreter are depicted and analyzed.  
 This paper analyzes the results of the French MT Evaluation Campaign, CESTA (2003-2006). The details of the campaign are first  briefly described. The paper then focuses on the results of the two runs, which used human metrics, such as fluency or adequacy, as  well as automated metrics, mainly based on n-gram comparison and word error rates. The results show that the quality of the systems  can be reliably compared using these metrics, and that the adaptability of some systems to a given domain – which was the focus of  CESTA’s second run – is not strictly related to their intrinsic performance.  language, had several objectives (Hamon et al. 2007). As  Introduction  stated above, the first goal was to define an evaluation  The French MT evaluation campaign, CESTA1, completed its two phases in 2006. The goal of the campaign was to evaluate the output quality of commercial and academic systems translating into French from English and Arabic, and to assess their adaptability to a new subject domain. CESTA also studied the reliability of automatic evaluation metrics with French as a target language and produced a number of reusable language resources for MT evaluation. This article analyzes the scores and rankings of the systems in various conditions, from a meta-evaluation point of view. One of the main questions that are discussed is the level of agreement between human judgments of translation quality and the scores of the automated metrics. While such metrics have been developed and studied for English as a target language,  protocol which includes human and automated quality metrics, and to assess its reliability on the EN/FR and AR/FR language pairs. Two runs were organized. The first one aimed at evaluating output quality, from absolute and comparative points of view, on a general-domain reference corpus. In addition, the concrete details of the evaluation protocol were also validated and improved after the first run. The second run aimed at measuring the capacity of systems to adapt to a new domain in a very limited amount of time. The participants received “adaptation” data from a specific domain (health), and were asked to provide translation of the test data first without using the adaptation data, and then by using it to improve their systems’ performances by terminological enrichment (Mustafa Hadi et al. 2002, 2004; Babych et al. 2004).  this article discusses their applicability to French. Other meta-evaluation questions include the reliability of human Participating Systems and Their Coding  scores, the influence of reference translations on Thirteen systems or versions of systems (Hamon et al. evaluation results, and the use of automated metrics to 2007) took part in one or both CESTA runs2. In order to  analyze reference translations.  preserve the anonymity of the results, we will adopt the  The article is organized as follows. First, the overall CESTA notational conventions: we will refer to the  organization of the campaign is outlined, focussing on the systems using indices from S1 to S13, numbered in a  human and automated evaluation metrics that were used. continuous sequence throughout the two campaigns,  Then, the data used for the two runs is described, with regardless of the fact that some systems participated in  automated evaluation scores being used to estimate the both runs (so, Sx and Sy may sometimes refer to the same  variability of the reference translations. The reliability of system). This means that it is impossible to identify the  the scores is finally discussed, first intrinsically for human same system in each of the two campaigns; since  scores, and then in terms of correlation of automatic resources and even versions changed from one campaign  scores with human scores. The results show that the to the next, it would be in any case misleading to compare  automatic scores are consistent with human ones, and that performances of the ‘same’ system in the two runs. In  they are able to indicate reliably the ranking of the addition, to make notations more readable, we specify in  systems and their capacity to adapt to a new domain.  brackets appended after the system’s code, its source  language (‘en’ or ‘ar’) and the number of the campaign (1,  Overview of CESTA  2a or 2b) in which it participated.  The CESTA MT evaluation campaign (2003-2006), the first such campaign organized with French as a target 
This paper reports on the findings of the National Science Foundation (NSF) hosted Human Language Technology Workshop on Industrial Centers that was held May 3rd and 4th, 2007. Representatives from academia, industry, and government attended this meeting to discuss the feasibility of developing an NSF center-based partnership between industry and academia in the field of Human Language Technology (HLT). Currently the HLT field does not have such a center in the US. Given the considerable advances in this field with great potential for continued success and the benefits of collaborations among academic, industrial and government partners, the time is ripe to build a better understanding of how to create a center that is not only mutually beneficial to all parties, but also supports work that simply could not be done by any partner alone.  Purpose of the Meeting On May 3rd and 4th, 2007, the National Science Foundation (NSF) in Arlington, Virginia hosted the Human Language Technology Workshop on Industrial Centers. Twenty-nine representatives from academia, industry, and government attended this workshop to discuss the feasibility of developing an NSF centerbased partnership between industry and academia in the field of Human Language Technology (HLT). Because the HLT field does not currently have an industry-oriented center in the US, the purpose of the workshop was to determine whether the time is ripe to begin plans for building such a center. Several factors justified convening the workshop: • There have been considerable advances in the field, and there is great potential for continued advances in fundamental technologies ranging from speech recognition and synthesis to machine translation, text mining, and next-generation search engines. • Planned coordination among academic, industrial, and government partners offers the potential to tackle research questions that are broader than the ones that could be addressed by any partner alone and whose solutions would be mutually beneficial. • Such collaboration has the potential to stimulate research excellence at universities, to enhance the quality of the intellectual property of US HLT companies, and to foster university-to-industry technology transition. Preparatory Materials for the Meeting In preparation for the meeting, participants were asked to read the following materials related to two types of NSF centers and to focus especially on the linkages for university and industry collaboration in each.  1. The NSF Industry/University Cooperative Research Centers (IUCRCs) program: The IUCRC program seeks to develop partnerships among industry, university, and government members to stimulate cooperation for carrying out fundamental research recommended by an Industrial Advisory Board. • The IUCRC program web site (NSF, 2007d) • The IUCRC Program Evaluation Project (Gray, 2007) • “Managing the Industry/University Cooperative Research Center: A Guide for Directors and Other Stakeholders” (Gray and Walters, 1998), in particular, chapters 1, 2, and 5 2. The NSF-sponsored Engineering Research Center program: The ERC program seeks to develop engineering systems-focused, interdisciplinary centers at universities in close partnership with industry. • The ERC program web site (NSF, 2007b) • The Engineering Research Centers Association web site (ERC Assoc., 2007) • “ERC Best Practices Manual” was developed by staff of the ERCs to assist those who are planning or setting up an ERC (Absher et al., 1998). See chapter 5 on industrial relations. Participants were also requested to consider the following issues prior to the meeting: • Is a center a viable vehicle for collaboration between academia and industry in the area of HLT? If so, what type of center would be best? • How can one optimize a mutually beneficial centerbased partnership among academia, industry, and government with respect to the following tasks?  1For affiliations of authors, see Harper et al. (2007).  o Develop a long-term, strategic vision for an emerging engineered HLT system with the potential to transform a current industry or spawn something new. o Define a research agenda that optimizes shared research interests, needs, and opportunities. o Define partnership strategies between universities and industry and determine how to best collaborate and divide up rights and responsibilities. o Determine strategies for protecting/sharing intellectual property while enabling timely publication of intellectual output of the center. o Develop mechanisms for involving graduate students in industrially relevant research that also qualifies for Master’s and Ph.D. level theses. • What breadth of research should the center fund? Which areas of research are most viable for center collaboration? • How should the center handle organizational issues? o Develop a strategic plan for integrating fundamental HLT-related science and engineering research. Is there a viable test bed that could be used to tie together the research threads and enable systems level evaluation? o Develop a strategic plan for constructing a multidisciplinary research agenda while developing a more diverse research population. Would a single site or multiple site center be more effective? o What is the best structure for an advisory board (i.e., balance between academic, industrial, and government oversight)? The Meeting The meeting was comprised of a series of presentations and breakout sessions. On the first day, there was an opening presentation by Mary Harper about the meeting’s purpose and schedule, followed by four presentations on NSF center programs, two by NSF program directors: Alex Schwartzkopf (IUCRC program) and Bruce Kramer (ERC program) and two by individual center directors: Janis Terpenny (Virginia Tech IUCRC) and Adam Powell (USC ERC). These presentations were followed by two breakout sessions in the afternoon. Homework was assigned on the evening of the first day of the workshop and was discussed first thing in the morning on the second day. Discussion of the homework was followed by a third breakout session on possible next steps. In the following subsections, some of the key issues raised by the three to four focus groups in each breakout session are summarized. Discussion Item 1 Would an HLT center be a viable vehicle for collaboration between industry and academia? What would the ideal collaboration look like?  An environment for working on large-scale problems As centers have a fairly high management and infrastructure overhead, the participants considered what the advantages of a university-industry center would be compared to individual collaborations between one university laboratory and a single industrial partner. Some participants pointed out that an individual expert may be better suited to work on immediate well-defined problems, but a group with a diverse expertise would be needed to work on larger, less well-defined problems. A center could provide just the right environment to attract high quality students and faculty and engage industry involvement to tackle bigger problems than an individual or small group could handle. It could investigate broader efforts with multiple disciplines, while educating graduate students to work in the new emerging areas of science and technology. A center would also provide industry with more revolutionary science and engineering, produce better students for industrial partners to recruit, and produce more products and services than an individual laboratory. Availability of shared resources Another advantage of a center is the availability of shared infrastructure, including various types of data, tools, and computational support (e.g., the MapReduce algorithm implemented over a grid-like computational substrate to support very large-scale computation). Large data collections are essential in the light of the data-driven methodology common in HLT, but they are often quite expensive to create, extend, document, maintain, and distribute. Some data collections require human subjects’ approval, while others may require the center to deal with copyrights. In addition to coordinating the development of and providing access to the right data to set the challenges for the center, it is also necessary for the center to provide shared computing environments. Members should be able to work on parts of an end-to-end system without needing to build an entire system by themselves. Alternative models for collaborative efforts One of the breakout groups discussed other types of models for centers or collaborative efforts that support broad multidisciplinary research in addition to IUCRCs and ERCs. These models include Centers of Excellence (CoE), e.g., the Johns Hopkins University CoE; Federally funded research and development Centers (FFRDCs), e.g., Institute for Defense Analyses (IDA), MIT Lincoln Labs, and MITRE; Universityaffiliated Research Centers (UARCs), e.g., University of Maryland Center for the Advanced Study of Language (CASL), Johns Hopkins University Applied Physics Laboratory (APL), University of Southern California Institute for Creative Technologies (ICT); Patron-based funding (such as Bambergers), e.g., Institute for Advanced Studies (IAS) at Princeton; University Centers, e.g., International Computer Science Institute (ICSI) at Berkeley; DOE National Laboratories and Technology Centers, e.g., Argonne National Laboratory, Ames Laboratory; The MOSIS  Service (in VLSI); Supercomputing Centers; NSF Science of Learning Centers (SLCs); Technology Alliances (CTAs, ITAs), e.g., Collaborative and International Technology Alliances at the Army Research Lab (ARL). Broadening partner involvement & research portfolio These models involve different types of partnerships between industry, university, and government (see Figure 1). They vary in the extent to which partners are involved in the initial establishment of the collaborations, in the planning of projects, the reviewing and selection of projects, the funding decisions, and the legal commitments that come with project funding (grants vs. cooperative agreements vs. contracts). For example, ARL currently manages several CTAs and ITAs, each with joint planning and cooperative agreements among industry, university, and government partners. Individual CTAs and ITAs are funded once for five years, with three-year add-on options. By contrast, UARCs and University CoEs have cycles of multi-year government funding, because they are intended to address their government stakeholders' interests over the long term. As there are a variety of organizational and funding options for tackling the grand-challenge problems for human language technologies, the HLT-focused IUCRC or ERC could partner with some of these other existing models for collaborations. This partnership would bring together researchers working within other arrangements in order to broaden the research portfolio of the partners and allow them to tackle potentially larger problems. Attracting diverse talent pool The advantages of a center were deemed to include the pooling of good people, ideas, and infrastructure to solve new problems, while providing a broad collection of opportunities for visiting investigators from other institutions and industry. A center would be an ideal locus for consolidating ideas and efforts from university, industry, and government researchers, each bringing different perspectives to the problems the center would tackle. The center would attract researchers that excel in their disciplines given the potential to work with other researchers with similar levels of excellence. Bringing these groups together can lead to qualitatively new research because it unifies groups that otherwise would be working from different less interdisciplinary perspectives. This consolidation of diverse, excellent researchers should also be a magnet for funding (both center-based and individual or small group awards). Addressing industry needs The participants considered what industry would want out of an industrially-oriented HLT center. Many companies care about recruiting students who are welltrained in emerging technologies that would be part of a successful center. Also, the companies would benefit from a center that produces solutions for difficult problems such as global communication aids, speech in real environments (e.g., sensor-based projects,  cocktail party challenge), and better speech synthesis. A center would help the company partners to be more competitive (both domestically and internationally) by providing the critical mass to work on hard problems that matter to them but that they cannot afford to do themselves. The center also has potential to enable a number of new companies to be created that depend on HLT. Another potential impact of a center on research companies might be that it offers a vehicle that could potentially support broader than DARPA-focused research (DARPA has recently been engaging companies to manage research teams). Figure 1. Center vehicles for collaboration between universities, industry, and government Addressing needs of university researchers The participants also considered what the university researchers would want from an industrially-oriented HLT center. Academics like to work on hard problems (e.g., deep NLP) that are not near-term. A center would provide the infrastructure and funding needed to support this type of research. Stability of funding is critical for attracting high quality students, post doctoral candidates, and faculty to the HLT center. Because obtaining center funding is challenging (especially an ERC award) and universities need steady funding to support good students (otherwise they move into other fields or leave for industry), having broad industry buy-in could help to create a stable funding base. The center would also attract visiting scholars from academia, industry, and government to help with the research agenda. Tackling a diversified research agenda Based on these discussions, the participants concluded that there is a good potential for a center to leverage the strengths of academic and industrial partners to tackle new human language technologies, such as virtual reality. A successful center would need to have a diversified portfolio of research problems; the research should be exciting, involve a multidisciplinary team, and result in innovations that can be used by industrial partners. If the center includes a sizable consortium of industry and government partners, it may be possible to build a massive infrastructure to support all of the partners. The center cannot simply produce core industrial products; it must also develop leading edge core technology, some of which may give rise to novel products given the guidance of the industrial partners. Some participants suggested that  the center should avoid tackling the large data processing problems, which are currently too expensive and so should be left to industry. Instead it may be better to focus on how to tackle, for example, low density languages (e.g., translation to and from rare languages with minimal parallel text, speech understanding with sparse per-language training data). Membership cost to participate in an IUCRC or ERC Since the preponderance of the support for an IUCRC comes from company membership fees, NSF requires a center to have at least six members with total company membership fees equalling at least $300,000 yearly. Although an ERC does not rely as heavily as an IUCRC on industrial support, NSF expects substantial financial support from industry, again typically provided through annual membership fees (usually two or three levels of membership with corresponding fees and membership benefits). Participants at the meeting believed that the cost of participating in an IUCRC or an ERC could be prohibitive for some companies, especially for smaller companies. Although it may be a challenge to obtain funding from industry, if it is clear that industrial partners have some control over how their membership fees are spent (and can leverage other funding), they will have greater interest in participating in the center. An effective IUCRC or ERC cannot take money without considering their industrial partners’ needs. Control of funds and intellectual property rights Some industrial participants expressed the concern that in a broad-based center they would lose direct control. For example, some companies already have mechanisms for educating and recruiting students; they identify and directly support faculty who train students according to their specific needs. There was concern that being part of a center would mean that less of their funding would get to those researchers they would want to support (due to overhead and center priorities). There was also concern about losing control of intellectual property (IP). Some companies, especially small ones, keep things secret, worry about the potential risk of IP leaking, and usually do not patent. Industrial participation mechanisms Industrial partners would have a number of ways to influence the center. They could negotiate with the center universities (with some limitations set by the NSF programs) either when the center proposal is being developed or after it has been funded. Also by participating on the advisory board, an industrial partner can have a strong impact on the work conducted by the center (thus leveraging all of the center’s funding) and recommend center affiliates. Also, industry partners who contribute more funding and effort to the center should receive greater center benefits than less engaged partners. Identifying focus and markets for HLT products The participants stressed the importance of identifying a multi-disciplinary focus that has an actual or potential market. If the center focus is too narrow,  then it may be hard to find enough support. If the center focus is too wide, then research efforts will be less coherent and more difficult to manage. Currently there are few money-making products in speech processing or machine translation (though the opposite is true for web-search), so it is prudent not to define HLT technologies too narrowly. Additionally, projections about plausible markets are likely to need revision with potential impact on ideal partnerships. Formulating markets where language would play a role was thought to be a useful exercise even outside of the effort to define an HLT center. Several possible avenues for potential HLT products were identified: • Social domain language-related products • Commercial targeting of potential customers (advertising) • Automating the creation of call center systems • Information integration (e.g., customer relationship management, internal and external business intelligence, and brand marketing)2 • Construction industry language problems for foreign workers (5% of revenue is spent correcting mistakes, and there are also safety problems) • Vertical high-accuracy translation markets, such as legal system translation • Hospitals’ need to provide medical help in a variety of languages • Assignment of insurance categories to medical reports • Law enforcement applications • Service to government goals or the government organization itself • Reducing language barriers in information access (e.g., cross-lingual search engines) • Question answering in any language • Translingual information mining and access across media • Communicating with the speech impaired (text-tospeech), the manually impaired (speech-to-text), the visually impaired (speech again), or linguistic minorities (machine translation) One thought was to look at 18-year olds to find where the markets will be in near future (e.g., instant messaging has moved into business, video gaming). Successful centers seem to involve many industrial partners, so it is not ideal to settle on just one market. Finally, it may be worth thinking about problems in two ways: what are the limiting factors in advancing language technology AND how is language technology itself a limiting factor in other applications? Additional industrial perspectives needed Participants raised an additional issue that should be considered more thoroughly. Since the industry representatives at this initial meeting were by and large from larger companies, some of the other important industry voices were not heard. There is a need for 2 Perhaps companies interested in the data resources may be less competitive about the core technologies.  input from companies that are the language technology consumers, but do not have their own investments in research. It would be beneficial to involve in future meetings several representatives from technologyconsuming industries that rely on HLT, but do not or cannot pay for all of the costs of HLT research and development themselves. In summary, the meeting participants would expect a viable HLT center to be challenge-centric, with experts in the necessary disciplines, a shared vision with all partners, shared infrastructure, and ample funding to attract partners from industry and government labs, and to provide a stable base for sustained research and education of students. All participants agreed that the ideal center would have a lifetime that is longer than a standard NSF grant, with an explicit goal of becoming self-sustaining. Participants estimated this would take five to ten years, although the industry partners tended to believe that shorter durations would be possible. Discussion Item 2 How can we best optimize the collaboration between industry and academia in a center environment? Most participants agreed that the center should be multi-disciplinary with multiple co-PIs per centersupported project (with a mixture of perspectives). Multiple universities, government labs, and industries of a variety of sizes would contribute to building a strong center with broad impact. The center needs to be heterogeneous and inclusive, with one institution selected as the management hub for the center. Flexibility to adjust research focus was seen as an advantage, as long as expertise to meet the requirements of the challenges set by the center is maintained. Small companies were considered critical for the vibrancy of the center since they will play an important role in technology transition and product development. 
 1. Introduction When using statistical machine translation (SMT) systems, we often notice that the phrases used to construct the translations are rather short. On average these phrases are less than two words long. This is in spite of that fact that some phrase extraction methods allow the extraction of arbitrarily long phrases. The main reason for this behavior is data sparseness; long exact matching phrases are relatively rare in the training data. In the decoder, these phrases have to compete with abundant shorter phrases. Due to this reason, Koehn et al. (2003) find that phrases longer than three words give little performance improvement. However, with limited reordering strategies used in most of the statistical machines translation systems, a combination of small short phrases does not always generate the desired translation. Zhang (2005) shows improved translation performance by using phrases of arbitrary length. Hierarchical models, such as the Hiero system (Chiang, 2005), that uses phrases with words as well as subphrases have shown better performance than standard phrase based systems. In this paper, we investigate a simplified two-level machine translations system that uses a linguistically motivated phrase decomposition. We think noun phrases (NPs) are good candidates for a hierarchical system. Semantically noun phrases describe objects and concepts using one or more nouns and adjectives. The vast majority of words in a language are nouns and hence NPs appear frequently in sentences. Noun phrases can often be translated independently into other languages irrespective of the context they appear. They tend to appear as coherent units in many languages. When using NPs as the unit of decomposition, we force it to be translated as an NP in the target language. Although this might not always  be the best choice, as Koehn (2003) shows, it is not a harmful restriction. We integrate the two-level phrases into a phrase based SMT decoder with minor modifications. It involves the following steps: • Identify NPs on both sides of the parallel training corpus and generate an NP translation table • Tag NPs in the training corpus by replacing them with a special tag “@NP” • Extract phrase translation pairs from the NPtagged corpus • Use the extracted NP translation table and NPtagged phrases in a two-level decoder to translate new sentences. In the next chapter we describe each of the above steps in more detail. We then present the experimental results and our conclusions. 2. System We build a translations system that translates Arabic text into English. Our training data consists of parallel corpora primarily of newswire genre available from LDC. Table 1 shows the statistics for the training data after it was preprocessed and English side lower-cased. 2.1 Generating NP translations The first step is to identify noun phrases in the training corpus. Essentially, we want to identify corresponding NP pairs in Arabic and English sides, and build an NP translation table. To achieve this we use a parser to extract NPs from one side of the text and a word-alignment model to induce the corresponding NPs on the other side of the parallel text.  ‫ﺣﻤﻠﺔ ﻋﺴﻜﺮﻳﺔ‬  # a military campaign  ‫اﺗﻔﺎق ﺗﻌﺎون ﻋﺴﻜﺮي‬  # a military cooperation protocol  ‫ﺣﻜﻮﻣﺔ وﺣﺪة وﻃﻨﻴﺔ‬  # a national unity government  ‫ ﺑﻌﺜﺔ ﺟﺪﻳﺪة ﻟﻼﻣﻢ اﻟﻤﺘﺤﺪة‬# a new united nations mission  ‫ اﻓﺎد ﻣﺮاﺳﻞ وآﺎﻟﺔ ﻓﺮاﻧﺲ ﺑﺮس‬# agence france presse correspondent  ‫ اﻟﻤﻨﺘﺠﺎت اﻟﺰراﻋﻴﺔ واﻟﻐﺬاﺋﻴﺔ‬# agricultural and food products  Figure 1: Sample of NP translation table  As our system translates Arabic text into English, it would be logical to start with Arabic; parsing Arabic side and extracting corresponding English NP translations. However, the Arabic parsers available did not produce desired accuracy. Therefore we use Charniak’s parser (Charniak, 2000) to parse English side of the training data. From the resulting parse trees we extract base NPs; i.e. NPs that do not contain other NPs embedded in them. As mentioned in the previous section these NPs are fairly short and are good candidates for a hierarchical system.  Sentences Tokens Vocabulary  Arabic 135K 3.5M 145K  English 135K 4.3M 63K  Table 1: Training data statistics  We generate IBM model 3 (Brown et al., 1993) alignments by running GIZA++ (Och and Ney, 2003) with the parallel text. GIZA++ training is done for both directions and the word alignments are generated by the intersection of the two.  For each English NP, we search the aligned corpus for sentences that contain the NP and read off the alignment as its translation. To compensate for alignment errors we also include partial alignments as follows: We find maximum (max) and minimum (min) Arabic word indices that are aligned to the words in the English NP. All the Arabic words between min and max are considered to be the translation of the English NP. We filter out unbalance NP translation pairs by removing entries that have a length ratio (# Arabic words / # English words) over two. Table 2 shows the details of extracted NPs. As seen in the table, translations for some English  NPs are not found due to alignment errors.  English NPs Translations found After filtering  325K 260K 205K  Table 2: Extracted NP statistics  A sample of the extracted NP translation table is shown in Figure 1. Each line contains an Arabic NP and its English equivalent separated by a hash mark. An NP in the table may contain multiple translation alternatives.  Table 3 gives the length distribution of extracted NP translations. This was calculated for Arabic and English sides independently. The average length of an Arabic NP is 2 words while the average length of an English NP is 3.  Length of NP (# words) 1 2 3 4 5 > 5  Arabic 31K 75K 52K 22K 13K 12K  English 13K 66K 73K 32K 12K 9K  Table 3: Length distribution of NPs To evaluate the accuracy of NP translations, and also to estimate how often the Arabic translation of an English NP is indeed an NP, a sample of NP translations was evaluated by an Arabic native speaker. 90% of the resulting Arabic translations were NPs. Out of these NPs, 11% had some irregularities such as missing articles in one side, etc. 10% of Arabic translations were either incorrect or not NPs (e.g. verb phrases).  EN: (NP united states) congratulates new (NP lebanese president)  AR:  ‫اﻟﺮﺋﻴﺲ اﻟﻠﺒﻨﺎﻧﻲ( اﻟﺠﺪﻳﺪ‬NP) ‫ اﻟﻮﻻﻳﺎت اﻟﻤﺘﺤﺪة ( ﺗﻬﻨﺊ‬NP)  New EN: @NP congratulates new @NP  New AR:  ‫@ اﻟﺠﺪﻳﺪ‬NP ‫@ ﺗﻬﻨﺊ‬NP  Figure 2: Tagging of NPs  ‫@ ﻧﺸﺮت ﻓﻲ‬NP # published in @NP  ‫@ ﻧﺸﺮت ﻓﻲ‬NP # published on @NP in  @NP ‫اﻓﺎد‬  # @NP reported  @NP ‫اﻓﺎد‬  # @NP stated  @NP ‫اﻓﺎد‬  # @NP said  ‫@ ﺑﺎﻟﺤﺼﻮل ﻣﻦ‬NP ‫@ ﻋﻠﻲ‬NP # to get @NP from @NP  ‫@ ﺑﺎﻟﺤﺼﻮل ﻣﻦ‬NP ‫@ ﻋﻠﻲ‬NP # get @NP from @NP  ‫@ واﻋﻠﻦ‬NP @NP ‫@ ان‬NP # @NP told @NP that @NP  @NP @NP ‫@ ﻧﻘﻠﺖ‬NP  # @NP that transported @NP to @NP  Figure 3: Examples of extracted NP-tagged phrases  2.2 Tagging NPs in the Training Corpus Once we have the NP translation table, the next step is to identify and tag NP pairs in the training corpus. From the parse tree, we already know NPs on the English side. We only have to identify NPs on the Arabic side. Although we can directly use the already generated word alignments for this purpose, we choose not to as alignments within individual sentences are less reliable. Instead we use the filtered NP translation table as follows:  For each NP in the English sentence, we look for its Arabic translation in the NP translation table. If any of the alternative translations is present in the Arabic sentence, we tag it as an NP, and replace both NPs with a special tag (@NP). If none of the translations are present in the Arabic sentence, we leave English NP untagged. Figure 2 illustrates this process. This is repeated for all the sentences to generate an NP-tagged training corpus. How much contraction has the NP-tagging introduced? We looked into the number of unique n-grams in the corpus before and after NP-tagging. Tables 4 and 5 show the comparison for Arabic and English sides of the corpus respectively.  N-gram Type unigram bigram trigrams 4-grams  Original 145K 1425K 2540K 2890K  NP-tagged 144K 1269K 2299K 2713K  Table 4: Unique N-grams in Arabic training corpus before and after NP-tagging  We see a considerable reduction in the types of n-grams.  N-gram Type unigram bigram trigrams 4-grams  Original 630K 841K 2293K 3242K  NP-tagged 628K 770K 2015K 2894K  Table 5: Unique N-grams in English training corpus before and after NP-tagging  We also compared the average length of the corpus before and after NP-tagging. These numbers are given in Table 6. Avg. length of an Arabic sentence has dropped by about  1.3 words. For English sentences, the drop is about 2 words.  Corpus Original NP-tagged  Original 28.83 27.56  NP-tagged 35.89 33.40  Table 6: Avg. length of training corpus before and after NP-tagging  2.3 Extract Phrases from NP-tagged Corpus We use the NP-tagged parallel corpus to extract phrase translation pairs. Our phrase extraction method is similar to Moore (2003) which is a variation of the IBM-1 word alignment model (Brown et al., 1993).  tAibnhislteiesnrugermesutsiaepnldegcciotnirvaptehuesstaoscrueogqrnecutteaeinnscesseneatnoetfnepcnhwecroaesrtde1Jsssii1=1t2I jj1=2t=1s.=s.i.11ttKKJj1t.h.ssa.tiIt2ji2wnisefrotathhrmeee optimal translation for this source phrase. We can estimate the quality of a translation candidate by using the IBM-1 word alignment probabilities between the source and target phrases. If the candidate is actually a good translation of the source phrase we expect higher IBM-1 probabilities between the words in the phrases than if the translation candidate was incorrect.  tIhf iws eseanstseunmceeptahiartwtejj12ciasntahneaolopgtiomuasllytraarngsulaettihoant  for the  wsii1o2 rdins  from the sentence pair that are not in these phrases must  also be translations of each other. This means the optimal  translation for the (non-contiguous) source phrase  esx1p..e.csti1  − 1 high  spi2ro+1b.a..bsiIlitiiess  tb1e..t.wt je1e−1n  thtej2w+1o.r..dtsJ  and we in these  also two  phrases.  Overall the constrained probability for this sentence split  can be calculated as:  p j1, j2 (s | t) =  i1 −1 ∏ ∑ p(si | t j ) i=1 j∉( j1... j2 )  i2 j2  I  ∏ ∑ p(si | t j ) ∏ ∑ p(si | t j )  i=i1 j= j1  i=i2 +1 j∉( j1... j2 )  If we optimize over the target side boundaries j1 and j2 we can determine the optimal sentence splitting and the best translation candidate.  ‫اﺑﺮاهﻴﻢ‬  ‫ﻳﺴﺘﻘﺒﻞ‬  ‫ﺿﺎﺑﻂ‬  ‫ﻓﻲ‬  ‫ﺑﻐﺪاد‬  @NP{ibrahim}  (a) @NP{officer}  @NP{baghdad}  ‫اﺑﺮاهﻴﻢ‬  ‫ﻳﺴﺘﻘﺒﻞ‬  {@NP ‫ ﻳﺴﺘﻘﺒﻞ‬# @NP receives}  ‫ﺿﺎﺑﻂ‬ (b)  ‫ﻓﻲ‬  ‫ﺑﻐﺪاد‬  {@NP ‫@ ﻓﻲ‬NP # @NP in @NP}  { ‫@ ﻓﻲ‬NP # in @NP}  @NP{ibrahim} receives  @NP{officer}  ‫اﺑﺮاهﻴﻢ‬  ‫ﻳﺴﺘﻘﺒﻞ‬  ‫ﺿﺎﺑﻂ‬  ‫ﻓﻲ‬  @NP{baghdad} ‫ﺑﻐﺪاد‬  { ‫ اﺑﺮاهﻴﻢ ﻳﺴﺘﻘﺒﻞ‬# ibrahim receives}  (c) {‫ ﺿﺎﺑﻂ ﻓﻲ ﺑﻐﺪاد‬# officer in baghdad} {‫ ﻓﻲ ﺑﻐﺪاد‬# in baghdad}  @NP{ibrahim} receives  @NP{officer}  ‫اﺑﺮاهﻴﻢ‬  ‫ﻳﺴﺘﻘﺒﻞ‬  ‫ﺿﺎﺑﻂ‬  ‫ﻓﻲ‬  (d)  Figure 4: Two level decoding process  @NP{baghdad} ‫ﺑﻐﺪاد‬  The same ideas can be applied if we use the IBM-1  probabilities for the reverse direction thus calculating  pproj1bja2b(itli|tise)s  and we interpolate to get the optimal  the two phrase alignment translation candidate. We  not only use the top translation candidate but all  candidates up to a certain threshold.  A more detailed description of the method is given in Vogel (2005).  A sample of NP-tagged phrases we extracted using the above method are given in Figure 3. Each line contains an Arabic phrase and its English translation separated by a hash mark. We have removed the scores attached to each phrase for clarity.  Some of the phrases extracted by the above method contain different number of NPs on Arabic and English sides. These phrases are against our assumption that Arabic NPs are translated into English NPs. Therefore we remove them from the phrase table before using in the decoder.  2.4 Two-level Decoding The decoder combines different knowledge sources including the translation model and the language model, to generate the best translation for a sentence. We use CMU STTK decoder (Vogel et al., 2003). For our experiments, the decoder uses two translation resources in two levels to generate a hierarchy of phrases. NP translation table is used in the first level to identify  possible NPs in the test sentence. NP-tagged phrase table is then used in the next level to build a translation lattice. The decoding process is organized into two steps: 1. Build a translation lattice using all available word/phrase translation resources 2. Find the best combination of partial translations by searching through the lattice In addition, it also performs minimum error-rate training (Och, 2003) to find the best scaling factors for each model used in the decoder. 2.4.1 Building the Translation Lattice The first step in decoding is building the translation lattice. We illustrate this process by using the following Arabic sentence. Note that the Arabic sentence is written from right-to-left. Arabic sentence: ‫اﺑﺮاهﻴﻢ ﻳﺴﺘﻘﺒﻞ ﺿﺎﺑﻂ ﻓﻲ ﺑﻐﺪاد‬ Reference translation: Ibrahim receives officer in Baghdad First the decoder converts the Arabic sentence into a lattice structure where words are attached to the edges (see figure 4a). Next, for each word sequence starting from the left-most node, it searches the level 1 phrase table (i.e. NP translation table) for matching entries. If an entry is found, the source word sequence is identified as a possible noun phrase. A new edge is then added to the lattice across the  corresponding nodes, along with a tag (@NP) and the English translation of the phrase. This process is repeated for every node, except the last one. In figure 4(b), three words have been identified and tagged as possible noun phrases. The same process of searching for word sequences in the lattice is repeated with level 2 phrase table (i.e. NP-tagged phrases). This time word sequences can consist of actual Arabic words as well as NP tags. Whenever a matching entry is found, it is added to the lattice the same way as before. The process is repeated over the resultant lattice, effectively building a hierarchy of NP-tagged phrase structure. As illustrated in figure 4(c), this process can add translation edges covering only Arabic words, Arabic words and NP tags, and even previously added NP-tagged phrases. However, in our current experiments we do not use the latter type of edges. This method does not require the explicit tagging of Arabic test sentences for NPs and allows alternative phrases to compete in the decoder. Finally the NP-tagged phrases are expanded using actual Arabic/English translation pairs. We make the assumption that the order in which the NPs appear in Arabic side and the English side are the same. i.e. First NP in Arabic side of the phrase corresponds to the first NP in the English side of the phrase, etc. However, this might not always be correct. To rectify this, a more elaborate decoding scheme is required which uses phrase table entries that encode the relationship between NP tags on Arabic and English sides. We plan to address this in the future. Due to the hierarchical nature of the phrases where NP tags are replaced by all possible alternative translations, and expanded into difference hypotheses, the size of the lattice grows rapidly. As the sentence gets longer, the size of the lattice grows exponentially. To keep the lattice size small and keep the decoding time within reasonable bounds, we currently employ strict pruning strategies that remove non-promising edges from the lattice. 2.4.2 Searching for the Best Path The second stage in decoding is finding a best path through the translation lattice, now also applying the language model. The search algorithm is extended to allow for word reordering. Essentially, decoding is done from left to right over the Arabic sentence, but words can be skipped within a restricted reordering window (typically 4 words) and translated later. This in effect will reorder the words in the English sentence. When a hypothesis is expanded, the language model is applied to all English words attached to the edge over which the hypothesis is expanded. In addition, the distortion model is applied, adding a cost depending on the distance of the jump made in the Arabic sentence. Hypotheses are recombined whenever the model cannot change the ranking of alternative hypotheses in the future. As typically too many hypotheses are generated, pruning is necessary. Pruning is applied at two steps in the search  algorithm: First, a hypothesis is stored only when it is within a threshold to the best hypothesis. Second, as the beam shifts whenever a new best hypothesis has been generated, pruning criterion is applied again before a hypothesis is expanded.  A more detailed description of the decoder is given in Vogel (2003).  3. Evaluation Results We used two test sets from previous NIST evaluations as our test data. MT03 was used as the development set and MT05 was used as an unseen test set. To optimize the parameters of the decoder, we performed minimum errorrate training on MT03 optimizing for Bleu (Papineni et al., 2002) metric. Table 7 gives the details for the two test sets. Both test sets have 4 reference translations per test sentence.  Sentences Tokens Avg. Sentence Length  MT03 663 16K 24.5  MT05 1056 28K 26.8  Table 7: Test set statistics  The baseline system uses the original non-NP-tagged corpus. The phrase extraction uses the same approach described in section 2.3, but no hierarchical phrases are generated while decoding. An n-gram suffix array language model (Zhang and Vogel, 2006) was used in the decoder for all the experiments which was trained using the English side of the training corpus. Baseline scores for the test sets are given in the first row in Table 8. Scores are reported in Bleu and METEOR (Banerjee and Lavie, 2005) metrics.  Baseline Baseline+ NPs NP-tagged+NPs  MT03 Bleu% MET 35.16 0.624 37.07 0.624 37.88 0.620  MT05 Bleu% MET 30.58 0.603 32.50 0.604 34.62 0.605  Table 8: Translation results (Both Bleu and MET are case insensitive scores)  First we wanted to see if the extracted NP translations can already help improve the translation quality of our baseline system. To evaluate this, we combined the NP translation table and the regular phrase table used in the baseline system. Here, NP translations are regarded as alternative translations to the baseline system. Since there is no hierarchy in the phrases, in the decoder all the phrases are used at the same level. However, we made the decoder biased towards phrases from the NP translation table, so that whenever there are alternatives, the decoder will favor an NP translation. Results for the combined phrases are given under “Baseline+NPs” in the second row of Table 8.  Both MT03 and MT05 test sets show improvements in Bleu metric over the baseline; 1.91 and 1.92 Bleu points respectively. Both improvements are statistically  
It is generally acknowledged that the performance of rulebased machine translation (RMBT) systems can be greatly improved through domain-speciﬁc system adaptation. To that end, RBMT users often choose to invest signiﬁcant resources into the development of ad hoc MT dictionaries. In this paper, we demonstrate that comparable customization effects can be achieved automatically. One effective way to do that is to post-edit the translations produced by a vanilla RBMT system using a specially-trained statistical machine translation (SMT) system. Our experiments indicate that this method is just as effective as manual customization of system dictionaries in reducing the need for manual postediting. 
We are aiming to overcome language barriers by creating a high-performance natural language processing technology, which will enable the processing of human language using computers. We have embarked on a five-year project, starting in 2006, to develop a Japanese-Chinese translation system for scientific and technological papers, as the cooperation among the National Institute of Information and Communications Technology, the Japan Science and Technology Agency, Kyoto University, the University of Tokyo, and Shizuoka University.  Introduction Today, thanks to progress in science and technology, the world can instantly share all sorts of information, and a variety of data collected from around the world now forms an increasingly significant part of daily life. Since much of this information is expressed in words, differences in languages can be an obstacle to the distribution and use of knowledge. We are aiming to overcome language barriers by creating a high-performance natural language processing technology, which will enable the processing of human language using computers. We have embarked on a fiveyear project, starting in 2006, to develop a JapaneseChinese translation system for scientific and technological papers, as the cooperation among the National Institute of Information and Communications Technology (NICT), the Japan Science and Technology Agency (JST), Kyoto University, the University of Tokyo, and Shizuoka University. Part of this research and development is carried out as a study under the auspices of the “Special Coordination Funds for Promoting Science and Technology - Research and Development Program for Resolving Critical Issues,” as “R&D for Japanese-Chinese and Chinese-Japanese Language Processing Technology.” Overview While not as problematic among Western nations, the distribution of information in English is met with difficulty in Asia. We believe that, as an Asian nation, we should develop a machine translation system for Asian languages. As the first step in this endeavor, we have begun development of a machine translation system, mainly for scientific and technological materials in Chinese and Japanese, to keep pace with the significant progress we are seeing in various fields. We have carried out cooperative research projects with China, India, and  other Southeast Asian nations, and in the future, we plan to expand the target languages of the system to include an even larger number of Asian languages. Methodology In the past, the implementation of machine translation has adopted a range of approaches, including the transfer method and the interlingua method. In the transfer method, the input text in the original language is first analyzed, and then the sentence structure is mapped out in accordance with the grammar of the original language. This sentence structure is then converted into that of the target language using transfer rules, to create a corresponding sentence. In the interlingua method (pivot method), the input sentence undergoes a deeper analysis, and is converted into an expression described in an intermediate language that is not dependent on a specific language. The sentence in the target language is then created based on the structure of the intermediate expression. Since the interlingua method carries out translation based on the identification of meaning, it allows for a more liberal translation and results in more natural phrasing. However, this demands that processing provide a deeper understanding of meaning, while at the same time handling massive volumes of information. On the other hand, the transfer method requires the description of a great number of conversion rules, which results in a proportional increase in the number of required rules when multiple languages are involved. Both methods involve compilation from various sources (grammar rules, lexicons, thesaurus, etc.), which must be performed manually, and the establishment of a coherent approach to this task of compilation is extremely difficult. Recently, Statistical Machine Translation (SMT) is widely studied and shows its promising features. However, the capability to handle pairs of languages with very different grammatical and/or lexical structure is still unclear.  
Consistency is one of the primary quality measurements of translation. However, terminology can be translated differently, depending on a given context. For instance, one source term (e.g., “file name” in English) can have multiple variations in the target language (e.g., “ファイル の名前”, “ファイル名”, “文書名” in Japanese). Having multiple translations for such terms is inevitable by nature. However, if inconsistent term translations occur in similar contexts or are used for the same product, it is confusing to users. Consistency in terminology translations is critical to the readability of localized materials. Consistency in terminology translations is also critical to any example-based/statistical machine translation (MT) quality. The quality of such MT systems depends on the quality of their training data. Any inconsistency in terminology translations could lead to lower MT quality. Terminology translation inconsistency may derive from different sources, such as a lack of standardized terminology data. For instance, product development groups in a software company could use their own terminology and translations. Under such circumstances, the task of controlling terminology and checking terminology consistency is quite challenging. Human errors might be another cause for terminology inconsistency. For instance, using a translation memory (TM) tool that recycles previous translations, a translator could blindly copy an old translation without paying attention to a given context. Again, controlling such situations is almost impossible. In this paper, we explore a way to extract a bilingual compound noun list and check terminology translation consistency of the extracted terms automatically. We start with the process of mining compound nouns from English-Japanese bilingual corpus data. This process consists of two steps: (i) extracting a list of compound nouns from the source language data (in our experiments, English is the source language) and (ii) extracting a bilingual phrase table. The first step utilizes the part-ofspeech (POS) information of the data. The second step uses the word-alignment information provided by Giza++. The intersection of the two becomes our initial list. We use a Gaussian mixture-model (GMM) classifier to find  valid phrase pair translations from this list. The final step is to measure the terminology translation consistency. We devise a consistency index utilizing HerfindahlHirschman Index (Herfindahl, 1959; Hirshman, 1964), a commonly used measurement of market concentration. The organization of the paper is as follows: Section 2 describes the data we used and the process of extracting English compound nouns. Section 3 presents the process of extracting phrase pair translations for compound nouns from our bilingual corpus data. Section 4 describes the GMM classifier used in our experiment and the results of the experiment. Section 5 addresses the problem of terminology consistency and describes the consistency index we devised. Section 6 provides a summary of the paper and concluding remarks.  2. Data & Extraction of English Compound Nouns  2.1 Data We used English-Japanese parallel corpus data from software user interface (UI) strings, such as menu items, error messages and wizard text. Table 1 provides the size of the data used in the proposed experiments as well as the number of the products involved in the data.  Sentences  English/Japanese  300K  # of Products 104  Table 1: Training data  2.2 Extraction Method We extracted English compound nouns from the training data. To identity compound nouns in the English corpus, we parsed and analyzed the part-of-speech for all English sentences, using an English parser developed at Microsoft Research (NLPWin) (Heidorn, 2000). Then, we extracted consecutive-noun terms such as N/N as in “file name” and N/N/N as in “application programming interface”. The number of noun words within one compound term was limited to five.  We identified a total of 38,519 compound nouns from the English data of the English-Japanese corpus. Table 2 provides some samples of extracted compound nouns.  The extracted bilingual phrasal translation pairs are far from perfect. For instance, Table 3 provides some examples from the initial English-Japanese phrase table,.  form template disk space user name tools menu color scheme web browser start date file type  file type data connection company accounts database retrieval service dialog boxes network access path name security information  Table 2: Sample compound nouns  3. Phrasal Alignment Process Independently of the English compound noun extraction, we extracted a phrase table from the bilingual corpus data as shown in Table 1. The Giza++ (Och and Ney, 2000c) toolkit for IBM models (Brown et al., 1994) and HMM (Och and Ney, 2000a) is used to provide word alignment for the bilingual data. During implementation, we performed five iterations of model 1, followed by five iterations of HMM and then five iterations of model 4. As in Koehn et al. (2003), the bilingual corpus is aligned bidirectionally, i.e., first we perform word alignment from English to Japanese and second from Japanese to English, respectively. Then these two alignments are combined to form the final word alignment with the heuristics described in Och and Ney (Och and Ney, 2000b). From the bidirectional word alignment, we extracted phrasal translation pairs that are consistent with the word alignment. This means that words in a phrase pair are only aligned to each other (Och et al., 1999). The maximum phrase length used in our experiments was set to four. We now have two lists: (i) the list of the English compound noun described in Section 2.2 and (ii) the list of the bilingual phrasal translation pairs. We simply extracted the overlap between the two lists; that is, the phrase pairs whose English side strings are the same strings as those in the English compound noun list. The overview of the bilingual compound noun extraction process is described in Figure 1 below.  Source (English)  Target (Japanese)  Word-alignment  Extraction of compound nouns  Extraction of phrase pair translations  server administrator サーバー の 管理者 サーバー 管理者 参加 依頼* 依頼*  network connection ネットワーク へ 接続* ネットワーク の 接続 ネットワーク は 接続* ネットワーク 接続  Table 3: Examples of phrase pair translations for English and Japanese  The items with an asterisk in the above tables are not the proper target candidates for the English compound nouns in question. We explored a statistical method to eliminate such false candidates while keeping the recall as high as possible. This method is described in Section 4.  4. Calculating Translation Validity  4.1 Features for Translation Validity When extracting phrase tables we can also estimate phrase translation probabilities. That is, if we use s to denote the phrase at the source side and t to denote the phrase at the target side, we can estimate the probability of translating s to t by relative frequency (Koehn et al., 2003).  p(t | s)  count(t, s)  count(t ', s) t'  In our implementation, for each phrase pair we actually estimate two probabilities, p(t|s) and p(s|t), for translations from the source phrase to the target phrase and from the target phrase to the source phrase, respectively. Both probabilities are estimated by relative frequency as shown above. In the following sections, we will call them MLE1 and MLE2 scores, respectively. MLE1 and MLE2 scores describe the likelihood of translating s to t and vice versa. Based on these two scores, we can further build a classifier to decide whether the translation of phrase s to t is a valid translation. In order to train a translation validity classifier, we first built a training set. We drew a number of phrase pairs from the phrase table, and asked a human annotator to mark each of them as either valid or invalid translation. In Fig. 2, we plot out the (MLE1,MLE2) scores of phrase pairs in our English-Japanese classifier training set, where the cross mark „x‟ represents an invalid translation, and the circle mark „o‟ represents a valid translation. As expected, most invalid translations have both low MLE1 and MLE2 scores.  Intersection => Bilingual Phrase Table for Compound Nouns  Figure 1: Extraction of the initial bilingual phrase table for compound nouns  
The quality of a parallel corpus is determined by the correctness of the parallel units, i.e. text snippets of the source language, aligned with their translations. If a parallel corpus is created automatically, then a 100% quality in this respect is nearly impossible to achieve. Worse still, it is extremely difficult to evaluate the correctness of alignments: we are facing the need to compare the respective meanings. In case of large parallel corpora, the question of evaluation is often not raised at all; see, for example, OPUS (Tiedemann, 2004), Europarl (Koehn, 2002), Czech–English (Bojar & Žabokrtský, 2006), Hungarian–English (Varga et al, 2005). Similarly, the LDC catalogue (http://www.ldc.upenn.edu/Catalog/) contains no information about the correctness of any of the parallel corpora it lists. In many corpora, it is the  warning in the corpus documentation that alignments have been created automatically, or that the alignments may contain errors that indicates the estimation of the alignment quality. The only way to evaluate the alignments is to compare them manually on a smaller subset, but this is extremely labour-intensive (Samy et al, 2006; Singh & Husain, 2005). We set ourselves two goals. The first goal was to evaluate the quality of the existing corpora, taking advantage of the existence of independently created alignments of the same source documents. Evaluating alignment quality of a corpus is different from evaluating the output of an aligning method: for a method, both precision and recall are important, while for a corpus, the only important characteristic is precision (instead of recall, we have corpus size). The second goal was to find features predicting the quality of alignments in a document, so that we could estimate it even in the absence of a directly comparable, alternative aligned version from the other corpus. Corpora We had two English-Estonian corpora of legislation texts at our disposal. UT corpus The corpus of the University of Tartu at http://www.cl.ut.ee/korpused/paralleel/index.php?lang=en has two parts that represent the different source languages. One part is the Estonian legislation and its translations that make up 150,000 parallel units (sentences or list elements) in 400 texts, totaling 1.7 million tokens in Estonian and 2.9 million in English. The other part consists of EU legislative texts that make up 280,000 parallel units (sentences or list elements) in 4,000 texts, totaling 3.3 million tokens in Estonian and 4.9 million in English. JRC-Acquis corpus The corpus of EU legislation, Acquis Communautaire at http://langtech.jrc.it/JRC-Acquis.html is a parallel corpus of 21 European languages with an average of 8.8 million tokens in 7,600 texts per language. The Estonian part is 7.2 million and the English part 9.9 million tokens  (Steinberger et al, 2006). Having downloaded both the texts and scripts for producing the alignments from the address above and run the scripts, we found that the resulting English-Estonian sub-corpus contains 300,000 parallel units in 7,900 texts, totaling 4.6 million tokens in Estonian and 6.8 million in English. Thus this sub-corpus has considerably less tokens per language than the original corpus where all the language pairs are present.  Alignment  The alignments of both corpora were produced with  language independent methods.  The UT corpus was aligned with Vanilla aligner  (http://nl.ijs.si/telri/Vanilla/) that uses the algorithm from  (Gale and Church, 1993). The strategy followed was  similar to the one in creating the Europarl corpus (Koehn  2002): if the formal structures of the aligned units were  too different, then these units were discarded from the  resulting corpus altogether.  The aligning procedure passed through 3 stages: first,  aligning chapters, parts and appendixes, then paragraphs,  and finally sentences. The first stage was necessary  because although it is common practice to preserve the  formal structure of the original when translating  legislative texts, one should not assume that the electronic  versions of the original and the translation follow the  same conventions for structural mark-up. One cannot be  sure also that the original and translation both contain the  same structural elements, e.g. date, heading, appendixes  or tables, and in the same order.  After every stage, the numbering of parts of the  documents or lists as the simplest anchor points was used  for checking the alignments. If two parallel texts  contained a different number of sections, articles or list  items, or if the numbered elements were not parallel, then  these texts were not included in the parallel corpus. It was  assumed that in such cases the formal structure of the  texts was too different from each other and that the simple  method used would not yield trustworthy results.  Similarly, if the number of items (parts, paragraphs,  sentences) to be aligned at a certain stage was too  different in the parallel texts – one contained more than  
In a data-driven machine translation system, the lexicon is a core component. Sometimes it is used directly in translation, and sometimes in building other resources, such as a phrase table. But up to now little attention has been paid to how the information contained in these resources can also used backwards to help build or improve the lexicon. The system we propose here alternates lexicon building and phrasal alignment. Evaluation on Arabic to English translation showed a statistically signiﬁcant 1.5 BLEU point improvement. 
. This paper presents a general framework for semi-automatic error analysis in large-scale statistical machine translation (SMT) systems. The main objective is to relate characteristics of input documents (which can be either in text or audio form) to the system's overall translation performance and thus identify particularly problematic input characteristics (e.g. source, genre, dialect, etc.). Various measurements of these factors are extracted from the input, either automatically or by human annotation, and are related to translation performance scores by means of mutual information. We apply this analysis to a state-of-the-art large-scale SMT system operating on Chinese and Arabic text and audio documents, and demonstrate how the proposed error analysis can help identify system weaknesses.  Introduction State-of-the-art large-scale statistical machine translation (SMT) systems are fairly complex: they typically consist of multiple component models (e.g. translation model, language model, reordering model), they perform multiple decoding passes, and have millions of parameters that may interact in non-transparent ways. At the same time, translation input is becoming increasingly varied, consisting not only of newstext-style documents or parliamentary proceedings but also of unstructured text sources such as emails, blogs, or newsgroup texts. As a consequence, diagnosing problems in translation performance and relating them to characteristics of the translation input is growing more and more difficult. Problems are aggravated further in the case of speech translation (of e.g. broadcast news, talkshows, etc.), where the input to the translation module is provided by an automatic speech recognition (ASR) system whose performance also influences the quality of the final translation output. During machine translation (MT) system development, automatic evaluation criteria are commonly used to judge performance, such as the BLEU score (Papineni et al. 2002), the NIST score (Doddington 2002), or, more recently, METEOR (Banerjee & Lavie 2005). Although the use of fully automated evaluation criteria is helpful in accelerating the system development cycle, all of the above criteria have shown to be inferior to human judgments of translation performance. Moreover, they do not yield any insight into precisely which input characteristics caused particular translation errors, or which system components need to be improved in order to reach the desired performance level. Human analysis of machine translation errors, on the other hand, is costly and time-intensive and can typically not be performed on a regular basis in the course of system development. Thus, an automatic or semi-automatic procedure for better error analysis would be desirable. In this paper we present a semi-automatic error analysis procedure for large-scale MT systems that is designed to identify characteristics of input documents, as well as glitches in a multi-component  pipelined system structure, that are responsible for poor translation output. Measurements of characteristics such as source, genre, style, dialect, etc. are extracted automatically or obtained from human annotations and are statistically related to measurements of the overall system performance. The various input document features are then ranked with respect to their impact on translation performance. Previous Work Most work on error analysis in statistical machine translation has made use of extensive human analysis, such as classifying unsatisfactory output into categories such as wrong word choice, missing content words, missing function words, etc. (see e.g. Koehn 2003, Och et al. 2003). Previous work on automatic or semi-automatic error analysis in SMT systems includes Niessen et al. (2000), Popovic et al. (2006a) and Popovic et al. (2006b). In Niessen et al. (2002), a graphical user interface was presented that automatically extracts various error measures for translation candidates and thus facilitates manual error analysis. In Popovic et al. (2006a) and Popovic et al. (2006b), errors in an English-Spanish statistical MT system were analyzed with respect to their syntactic and morphological origin. This was done by modifying the references and the machine translation output by eliminating morphological inflections or suspected reordered constituents, and by analyzing the resulting changes in position-independent word error rate or word error rate. This revealed system problems in specific areas of inflectional morphology and syntactic reordering. To our knowledge, there have not been any previous attempts at automatically relating a wide range of features of the translation input (e.g., document style, dialect, source, topic) to the output performance, which is the problem addressed here. Such an analysis is complementary to error analyses that are internal to the translation model: in highlighting features of the  translation input, it can result in different ways of preprocessing or pre-classifying input documents, or, in cases where the input is the output from a different processing module, it can lead to better overall system integration. Improvements in these two areas will become increasingly important for the type of large-scale, complex translation systems that are beginning to be developed.  Data and System  This analysis was performed within the context of the 2006 machine translation evaluations of the US-based GALE project. Systems participating in the 2006 evaluation were expected to translate documents in two different languages (Arabic and Chinese) and four different genres: broadcast news (BN), broadcast conversations (BC), newswire text (NW) and newsgroups text (NG). The first two genres represent audio conditions, i.e. the documents are provided as waveform files and first need to be processed by an ASR module whose output then serves as the input to the machine translation component. The latter two are text conditions. In all cases, the target language is English. The MT system used for this study is a combination of the outputs of several individual SMT systems (developed by RWTH Aachen, SRI, NRC, and University of Washington, respectively). The combination was done as described in Matusov et al. (2006). MT performance was measured by standard scoring techniques such as BLEU, METEOR, etc., and, additionally, by human translation error rate (HTER, see Snover et al. (2005)). HTER is based on a comparison of an MT output hypothesis with human reference translations that were created specifically for this output by performing edit operations (insertions, deletions, substitutions, and shifts) that transform the output into a fluent and meaning-preserving translation. The minimum number of edit operations required to exactly match the reference translation, divided by the average number of reference words, then yields the HTER score. The average performance numbers of our system in terms of BLEU and HTER for various conditions are listed in Table 1; the performance is state-of-the-art and comparable to that of other systems on this task.  BLEU HTER BLEU HTER  NW NG BN Chinese 13.55 11.13 11.70 28.19 30.35 29.16 Arabic 22.88 10.03 16.06 17.70 33.56 28.92  BC 8.47 35.59 12.41 38.23  Table 1: Average MT performance (BLEU(%) and HTER) of the system on various genres (NW = newswire, NG = newsgroups, BN = broadcast news, BC = broadcast conversations) and languages. HTER scores were used as the performance scores in our error analysis. The number of available documents annotated for HTER, as well as the number of documents selected for this study, are shown in Table 2. Since time  and resource constraints did not permit processing of all documents, a representative selection of the best/worst 2050% of documents in a given category was used. For conditions with only a small number of available documents, near-complete coverage was sought.  Arabic  Chinese  available used available used  NW  44  28  35  17  NG  27  17  35  22  BN  20  8  16  16  BC  7  6  11  11  Table 2: Number of documents used for the error analysis. Error Analysis Procedure Our overall error analysis approach is as follows: 1. Define a list of potential factors influencing MT performance. This may include e.g. dialect, source, genre, ASR performance, etc. 2. For each segment in the MT input/output, extract quantitative or categorical measurements of each factor from the input document, either automatically, or aided by human annotation. 3. Measure the mutual information between each measurement and the MT performance score for that segment. 4. Rank factors in terms of highest-to-lowest mutual information. Factors appearing at the top of the list can then be assumed to be more correlated with MT performance scores than factors at the bottom of the list. Factors The following features of input documents were established as potentially relevant to both audio and text documents: 1. Genre: one of the four genres mentioned above (BC, BN, NW, NG). Since current SMT systems are trained on large amounts of text data (and only small amounts of e.g. parallel transcriptions of conversations), it is likely that the genre will play a role in predicting MT performance. 2. Source: the identifier of the particular show or newspaper from which the document was extracted. This feature might reveal a bias of particular sources towards a vocabulary or style that is not handled well by the MT system. In the case of text documents this might indicate the preferences of individual authors; in the case of audio documents it could also indicate recording conditions or speaker effects. 3. Target language model score: the perplexity obtained by a well-trained target language model on the (manually created) reference translation of the document. Unigram, bigram and trigram scores are used separately. 4. OOV rate: the percentage of out-of-vocabulary (OOV) words in the input document, i.e. words that were not seen in the training data and thus are novel to the system.  5. Style: this feature indicates the style (spoken, written, mixed) of the document. 6. Dialect: the dialect of the source document. 7. Names: the percentage of correctly translated named entities in the translation output relative to the reference translation. Since many documents are from the news domain, where new names occur frequently, the failure to correctly translate named entities may be a significant contributor to poor overall MT performance. Features 3 and 4 are intended to measure the mismatch between training and test data. A test document that receives high perplexity under a language model trained on a large training data set can be considered mismatched either in terms of topic/domain or in terms of style. Words from novel topics or domains that are not represented in the training data will cause the language model to backoff to the unknown word probability; similarly, word sequences caused by differences in style (e.g. spoken, conversational style as opposed to written text) will receive low probabilities. A second way of measuring mismatch, in particular mismatch caused by topic/domain differences, is by computing the OOV rate. In languages with strong dialectal variation, the OOV rate may also indicate dialect effects. For audio documents, we additionally extract: 8. WER: the word error rate of the ASR system for the particular MT segment. For Chinese, character error rate (CER) is used instead. 9. % substitutions: the percentage of substitutions in the ASR output. 10. % deletions: the percentage of deletions in the ASR output. 11. % insertions: the percentage of insertions in the ASR output. 12. Dialect-ASR: the dialect rating assigned to the ASR output (as opposed to the rating based on the reference transcription (feature 6 above)). 13. Style-ASR: the style rating for the ASR output 14. Names-ASR: the percentage of correctly translated names in the ASR output . Features 1-4 measure the performance of the ASR frontend. We also include three binary features (Δ Dialect, Δ Style, Δ Names) indicating whether features 5, 6 and 7 differ from the corresponding ratings (features 12, 13, 14) based on the ASR reference transcription (e.g. whether the style of the ASR output was judged differently from that of the ASR reference transcription). If ratings differ, they may indicate problems with the ASR component. Measurements of all factors, as well as HTER scores, are computed at the document level. Although it would be desirable to choose finer-grained segments (sentences or even phrases), a document-level segmentation was the only segmentation observed by both the ASR and the MT components, as well as the HTER annotation. Internally, all components use different sub-document segmentations, which precluded the use of smaller segments. It should be noted that MT performance can vary within a single document; however, our analysis will only consider the average performance over the entire document.  Measurements Most of the factors listed above can be measured automatically. For our particular task, genre and source information were available from the information distributed with the test documents. Source and target language model scores were supplied in the form of separate unigram, bigram and trigram perplexity scores obtained by large-scale language models trained on billions of words of training data (primarily newstext but also including conversational data). The target language model scores were provided by the same English language model for both Arabic and Chinese. The source language models were the language models used by the respective Arabic and Chinese ASR front-ends in the system. Text preprocessing and tokenization were applied to the target/source texts to match the preprocessing required by the language models. The OOV rate was obtained from the best individual SMT system in the system combination; this was the system developed by RWTH Aachen. The ASR performance scores were computed by standard scoring of the ASR hypotheses against the reference transcriptions of the audio files. Dialect and style ratings, as well as the percentage of correctly translated names, were determined from human annotations specifically performed for this error analysis study. For Chinese, four dialectal categories were established: Mainland Chinese, Hong Kong Chinese, Taiwanese Chinese, and “neutral/can't tell”. Style was categorized as “spoken”, “written'', or “mixed”. For Arabic, dialect was annotated per word, i.e. each word in the document was assigned a degree of “dialectalness” ranging between 0 and 3. Level 0 is the default assigned to all pure Modern Standard Arabic (MSA) words and dialectal words that are MSA-like (these are words that are historically MSA and have remained phonologically unchanged or have slight phonological changes that are not seen in the written form). Level 1 of dialectness is given to words that have non-standard spelling: this category includes both spelling errors and dialectal words which are recognizable cognates of MSA words, e.g. ‫هﺪا‬ (instead of ‫‘ هﺬا‬this’). Level 2 is given to words that could be Level 0 except for the presence of one of a special set of dialectal affixes that do not exist in MSA. For example, the +‫ ب‬present tense prefix in Levantine and Egyptian Arabic which could attach to the otherwise Level 0 word ‫ ﻳﻜﺘﺐ‬yielding its dialectal variant: ‫ﺑﻴﻜﺘﺐ‬. Finally level 3 is assigned to words that are completely dialectal regardless of what kind of morphology they exhibit. Additionally, segment-level judgments of dialectalness ranging between 0 (perfect MSA) and 4 (pure dialect) were assigned. The difference between these two types of annotations is that segments can be judged dialectal even though all of its component words taken in isolation would be judged as MSA -- this may include e.g. certain idioms or colloquialisms that would not occur in a pure MSA text. An average dialectalness score for the entire document can then be computed either from the segmentlevel or the word-level ratings. For the present analysis the average word-level score was used. Style was not annotated separately for Arabic since it was felt that it  coincided fairly closely with dialectalness (spoken documents being automatically more dialectal). Arabic-English name annotation involved identifying names in the Arabic sentences and matching them with their translation in the target English sentences. We can distinguish seven different cases: 0. The word is not a name (default). 1. The word is not a name but is erroneously translated as a name in English. 2. A name that is not translated at all. 3. A name that is not translated as a name. 4. A name that is translated as a different (incorrect) name. 5. A name and it is translated into English correctly EXCEPT that it is not capitalized. 6. A name that is translated correctly and capitalized correctly. Of these, 1-5 constitute translation errors. The sum of all translation errors over the entire document, divided by the total number of names, is the name error rate used in the error analysis. For Chinese, a slightly different name annotation scheme is used which rewards partially correct name translations. Due to the character-based Chinese script, names consisting of multiple characters may have translations where one character was correctly translated but the others were not. A “correct” score is assigned to each correctly translated part. Human Annotation The human annotations for Chinese were performed by 6 native Chinese speakers (graduate students and postdocs) at the University of Washington. Each annotator processed a different subset of documents. Due to time and resource constraints, individual documents were not annotated multiple times, and inter-annotator agreement thus was not measured. However, annotations were spotchecked by other native speakers to ensure the correctness of the annotations. Each document took approximately 20 minutes on average to be annotated, but annotators noted a large variance in the amount of time required. Annotation speed mainly depended on whether the document was an audio or text document (audio being harder), document length, and on the font encoding (traditional vs. simplified Chinese font). The Arabic annotations were carried out at Columbia University by four native speakers of Arabic. Name annotation was divided among all four. Dialectness annotation was completed by one annotator only who received special training. The dialect annotation took 23 minutes on average per document. Documents that were mostly in MSA were the easiest and fastest to finish, while documents with many dialectal words took longer. The basic assumption in the annotation of dialects is that a word is in MSA unless it exhibits a special feature as discussed above. Name annotation took more time on average (~ 67 minutes per documents), This is due to the complexity of  the task which include recognizing the names in the source and verifying their presence in the target machine translation (as opposed to the dialectness identification task which is a monolingual task). Similarly to dialectness annotation, some documents took much longer (upwards of two hours per document) whereas others where much faster to annotate. Speech recognition errors added to machine translation errors made this task especially hard. For both languages and all annotation types, individual documents were not annotated multiple times and interannotator agreement thus was not measured due to time and resource constraints. However, annotations were spot-checked by other native speakers to ensure the correctness of the annotations. Mutual Information  In order to measure statistical dependencies between the measurements described above and HTER we chose mutual information (MI). The (discrete) mutual information between two random variables X and Y is defined as  I (X ;Y ) = ∑ ∑ p(x , y )log p(x , y )  x ∈X y ∈Y  p(x )p(y )  It is the most general way of measuring dependencies between two random variables since it can capture both linear and non-linear dependencies. Mutual information expresses the degree to which knowledge of one variable reduces the uncertainty about the other. Alternatively, it can be thought of as the distance (KL divergence) between the joint distribution p(X,Y) and the product of their marginal distributions, p(X)p(Y), the implication being that the distance should be 0 if X and Y are entirely independent.  Since our measurements involve both categorical and continuous values, we use discrete mutual information and perform histogram-based binning of the continuous values prior to computing the mutual information. Binning is based on the Freedman-Diaconis rule (Freedman & Diaconis 1981), which calculates the bin width w as: −1 w = 2 * IQR (x )N 3  where N is the number of samples in the population x and IQR is the interquartile range (the range between the first and third quantiles) of x. After the mutual information has been computed between each factor measurement and HTER, factors are ranked from highest to lowest mutual information. The factors appearing at the top of the list thus are the factors most predictive of HTER. At this point, we do not yet make use of any statistical significance tests to determine the significance of different ranks.  System Analysis In this section we present the application of our error analysis framework to our specific MT system. We  computed the mutual information based ranking of input document features in three different ways: (a) across all documents pertaining to a given language (Chinese or Arabic), (b) for the set of audio vs. text documents for each language (each set comprising two genres), and (c) separately for each genre and each language. This procedure yields a successively finer-grained analysis; however, the number of documents (and thereby the sample size from which the mutual information is computed) becomes smaller as well. In each case, binning of continuous values is redone based on the changed sample. Chinese The mutual information analysis for the set of all Chinese documents is performed only on those features that are defined for both text and audio documents, i.e. excluding e.g., the measurements of ASR performance. Based on these common features, the ranking of different factors is as shown in Table 3. According to this analysis, source and genre are the most important factors, followed by unknown words (as indicated by the target unigram score, OOV rate and the percentage of correctly translated names). Style and dialect, by contrast, do not seem to play a significant role.  Rank 1 2 3 4 5 6 7 8 9  Factor Source Genre Target unigram score OOV rate Names Target bigram score Target trigram score Style Dialect  Table 3: Ranking of factors for Chinese documents (crossgenre).  It is not surprising that genre emerges as one of the strongest predictive factors: since all component systems contributing to the combined system under investigation have mostly been trained on text data (newswire text, and parliamentary proceedings). Therefore, they generally produce low-quality translations when presented with unstructured text sources such as newsgroups data, or with ASR output. The source effect could be due to specific styles or vocabularies employed by different news sources, as well as a speaker or an acoustic effect in the case of audio files. Due to the limited sample size, the source effect could also be an individual document effect. Further investigations on a larger data set will be required to resolve this question.  Text and audio genres were subsequently analyzed separately. Table 4 shows the ten top-ranking factors for each condition (note that text documents only have nine factors in total). We see that for text translation the percentage of correctly translated names plays a significant role, as well as the source and the presence of  unknown words. For audio documents, source and unknown words seem to be relevant, in addition to errors (substitutions) in the ASR output. Finally, each genre was analyzed separately. The results are shown in Tables 5 and 6. For text genres (Table 5), name translation again emerges as one of the most important factors, as well unknown words in the test data.  Rank 1 2 3 4 5 6 7 8 9 10  Text Names Source OOV rate Target unigram score Target trigram score Target bigram score Genre Dialect Style ---  Audio Source Target unigram score Source unigram score % substitutions CER Target bigram score Source bigram score Dialect % insertions Target trigram score  Table 4: Top-ranking factors for Chinese audio and text documents.  . This is not a surprising result because names are very frequent in newstexts, and their mistranslation is a significant source of errors. In addition, there seems to be a strong source effect for Chinese newsgroups documents.  Rank 1 2 3 4 5 6 7 8  Newswire OOV rate Names Target unigram score Target bigram score Target trigram score Dialect Source Style  Newsgroups Source Names Target unigram score Target bigram score Target trigram score Dialect OOV rate Style  Table 5: Ranking of factors for Chinese text documents.  Rank 
2­ National Centre for Language Technology Dublin City University Dublin 9, Ireland {nstroppa,away}@computing.dcu.ie  Abstract In this paper, we compare the rule­based and data­driven approaches in the context of Spanish­to­Basque Machine Translation. The rule­based system we consider has been developed specifically for Spanish­to­Basque machine translation, and is tuned to this language pair. On the contrary, the data­driven system we use is generic, and has not been specifically designed to deal with Basque. Spanish­to­Basque Machine Translation is a challenge for data­driven approaches for at least two reasons. First, there is lack of bilingual data on which a data­driven MT system can be trained. Second, Basque is a morphologically­rich agglutinative language and translating to Basque requires a huge generation of morphological information, a difficult task for a generic system not specifically tuned to Basque. We present the results of a series of experiments, obtained on two different corpora, one being “in­domain” and the other one “out­of­domain” with respect to the data­driven system. We show that n­gram based automatic evaluation and edit­distance­ based human evaluation yield two different sets of results. According to BLEU, the data­driven system outperforms the rule­based system on the in­domain data, while according to the human evaluation, the rule­based approach achieves higher scores for both corpora.  
 This paper describes log-linear generation models for Example-based Machine Translation (EBMT). In the generation model, various  knowledge sources are described as the feature functions and are incorporated into the log-linear models. Six features are used in this  paper: matching score and context similarity, to estimate the similarity between the input sentence and the translation example; word  translation probability and target language string selection probability, to estimate the reliability of the translation example; language  model probability and length selection probability, to estimate the quality of the generated translation. In order to evaluate the  performance of the log-linear generation models, we build an English-to-Chinese EBMT system with the proposed generation model.  Experimental results show that our EBMT system significantly outperforms both a baseline EBMT system and a phrase-based SMT  system.  models. Such an EBMT system also achieves a significant  Introduction  improvement of 0.0378 BLEU score (17.2% relative) as  In example-based Machine Translation (EBMT), translation generation plays a crucial role (Somers, 1999; Hutchins, 2005). For EBMT systems, there are two major approaches to selecting the translation fragments and to generating the final translation. Semantic-based approaches obtain an appropriate target language fragment for each part of the input sentence by means of a thesaurus. The final translation is generated by  compared with a phrase-based SMT system. The remainder of the paper is organized as follows. The next section briefly introduces the Tree String Correspondence based EBMT method. And then we describe the log-linear generation models and the feature functions. After that, the search algorithm is described. Finally, we present the experimental results and conclude this paper.  recombining the target language fragments in a predefined order (Aramaki et al., 2003; Aramaki & Kurohashi, 2004).  Tree String Correspondence Based EBMT  This approach does not take into account the transition In this paper, we improve the Tree String Correspondence  between fragments. Therefore, the fluency of the (TSC) based EBMT method (Liu et al., 2006) with the  translation is weak. Statistical approaches select log-linear generation models.  translation fragments with a statistical model (Knight &  Hatzivassiloglou, 1995; Kaki et al., 1999; Callison-Burch Definition of TSC  & Flournoy, 2001; Akiba et al., 2002; Hearne & Way, 2003&2006; Imamura et al., 2004; Badia et al., 2005; Carl et al., 2005). The statistical model can solve the transition problem by using n-gram co-occurrence statistics.  Given a phrase-structure tree T and a subtree Ts of T, Ts is a matching-tree of T if Ts satisfies the following conditions:  However, this approach does not take into account the  1. There is more than one node in Ts.  semantic relations between the translation example and  2. In Ts, there is only one node r (the root node of  the input sentence. As a result, the accuracy of translation  Ts) whose parent node is not in Ts. All the other  is poor. Liu et al. (2006) presented a hybrid generation  nodes in Ts are descendant nodes of r.  model which combines these two approaches.  3. For any node n in Ts except r, the sibling node  In this paper, we propose log-linear generation models for  of n is also in Ts.  EBMT. Unlike the hybrid model presented in (Liu et al., 2006), our generation model uses various knowledge sources that are described as feature functions. The feature functions are incorporated into the log-linear models (Och & Ney, 2002&2004). In this paper, we use six feature functions. Matching score and context similarity are used to estimate the similarity between the input sentence and the source part of the translation example. Word translation probability and target language string selection probability are used to estimate the reliability of the translation example. Language model probability and length selection probability are used to estimate the quality of the generated translation. Experimental results show that the performance of the EBMT system is significantly improved by using the log-linear generation  Here, each node of the parse tree is labeled with its headword and category. TSC is defined as a triple <t, s, c>, where t is a matchingtree of the source language parse tree; s is a target language string corresponding to t; c denotes the word correspondence, which consists of the links between the leaf nodes of t and the substrings of s. If the leaf node of the matching-tree in TSC is a nonterminal node of the parse tree, then this kind of leaf node is also called a substitution node. The correspondence in the target language string of the substitution node is called a substitution symbol. The substitution symbol can represent a single word, or phrase that can be expanded by other matching-tree. During translation, for each  ○0 TOP (borrowed)  ○1 S (borrowed)  ○14 PUNC. (.)  ○2 NPB (Mary)  ○4 VP (borrowed)  ○4 VP (borrowed)  ○5 VBD (borrowed)  ○6 NPB (book)  ○9 PP (from)  ○7 DT ○8 NN ○10 IN (a) (book) (from)  ○11 NPB (friend)  ○12 PRP$ ○13 NN (her) (friend)  ○3 NNP ○5 VBD ○6 NPB (Mary) (borrowed) (book)  ○9 PP (from)  ○7 DT ○8 NN ○10 IN ○11 NPB (a) (book) (from) (friend)  ○12 PRP$ ○13 NN (her) (friend)  从 她 朋友 那里 借 了 一 本 书 (b) ○4 VP (borrowed)  ○5 VBD ○6 <NPB> (borrowed) (book)  ○9 PP (from)  ○10 IN ○11 <NPB> (from) (friend)  玛丽 从 她 朋友  那里 借 了 一 本 (a)  书。  从 <NPB> 那里 借 了 <NPB> (c)  Figure 1. Examples of TSC  substitution node, its corresponding substitution symbol will be replaced by the translation candidate of the TSC whose root node corresponds to this substitution node. A TSC is used to represent either of static translation examples or dynamic translation example fragments. In the TSC-based EBMT system, a preprocessed translation example is statically stored as a TSC in the example database. During the translation, a translation example fragment, which is identified to match the input, is represented as a TSC. In this paper, we use English-to-Chinese MT as a case study. Figure 1 shows three examples of English-toChinese TSC. TSC (a) indicates the following translation example: Mary borrowed a book from her friend. 玛丽 从 她 朋友 那里 借 了 一 本 书 。 (Mary from her friend there borrow a book .) In this TSC, the matching-tree of the source language and the target language string are composed of the source part and the target part of the translation example, respectively. TSC (b) and (c) are derived from TSC (a). The matchingtree of TSC (b) and (c) matches the subtrees of the TSC (a) that are rooted at Node 4. The matching-tree of TSC (b) matches all descendant nodes of Node 4 and no substitution nodes are included in matching-tree. The target language string corresponding to the subtree is considered the translation of the matching-tree. Different from TSC (b), the leaf nodes 6 and 11 in the matchingtree of TSC (c) are the non-terminal nodes of the matching-tree of TSC (a). The two nodes in this TSC are the substitution nodes. Their corresponded parts in the  target language string are the substitution symbols "<NPB>". Thus, the target language string of TSC (c) consists of the target language words and the substitution symbols. Two TSCs are homologous if their source language matching-trees are the same, which means that the same source language matching-tree can be translated into the different target language strings. A TSC forest matching a parse tree means that the source language matching-trees of the TSC forest can exactly compose the parse tree. For TSC T1 and T2 in the TSC forest, if the root node of T1 corresponds to a substitution node of TSC T2, then T1 is the child TSC of T2 and T2 is the parent TSC of T1. EBMT Based on TSC In the EBMT system based on TSC, the translation example is presented as the TSC. For an input sentence to be translated, it is first parsed into a tree. Then the TSC forest which best matches the input tree is searched out. Finally, the translation is generated by combining the target language strings of TSCs. For the parse tree of the input sentence, there are many TSC forests that match the parse tree. In Liu et al. (2006), a TSC can better match a parse tree if the TSC has more nodes or the TSC has higher matching score with the parse tree. Thus, a TSC forest best matches a parse tree if the TSC forest has the highest matching score in the TSC forest candidates. A greedy tree-matching algorithm was used to search for the TSC forest that best matches the parse tree of the input sentence. The algorithm first  searches for the best matching TSC whose root node corresponds to that of the parse tree. Then, for each substitution node of the TSC, the algorithm continues to search for the TSC that best matches the subtree of the parse tree rooted at the substitution node. The procedure is iterated until all substitution nodes are expanded. During searching for the final translation based on the TSC forest, the TSC forest is first extended by adding the homologous TSCs in order to include more possible translation candidates. Then the hybrid generation model, including matching score of TSC, word translation probability of source words and target words, and target language model probability, is used to generate the final translation. The final translation is produced by combining the target language strings in the TSC forest in a bottom-up manner. If the target language string contains the substitution symbol, then the substitution symbol is replaced with the translation of the corresponding substitution node.  Log-linear Generation Models We incorporate various features into our log-linear models. Given the input (source language sentence) f=f1J=f1,...,fj,...,fJ, the translation (target language sentence) e=e1I=e1,...,ei,...,eI with the highest probability is chosen from the possible target language sentences according to Eq. 1.  e = arg max{p(e′ | f )} e′  (1)  Based on the maximum entropy framework, we directly model the posterior probability p(e | f ) using the same method as described in (Berger et al., 1996). In this framework, there are M feature functions hm(e,f), m=1,...,M. For each feature function, there exists a model parameter λm . We can get the translation probability as described in Eq. 2.  M ∑ exp[ λmhm (e,f )]  p(e | f ) =  m=1 M  (2)  ∑ ∑ exp[ λmhm (e′,f )]  e′  m=1  Thus, we obtain the decision rule:  e = arg max{ p(e′ | f )}  e′  M ∑ = arg max{ λmhm (e′, f )}  (3)  e′  m =1  Typically, p(e|f) can be decomposed by adding hidden variables. To include the dependence on the hidden variables, we extend the feature functions by including the following hidden variables: the parse tree F of the input sentence and the TSC forest Z with K TSCs z1K=z1,...zk,...zK. Z is used to generate the final translation. Thus, we obtain M feature functions of the form hm(e,f,F,Z), and the following rule:  M  ∑ e = arg max{ λmhm (e′, f , F, Z)}  (4)  e′,F,Z m =1  In our system, only 1-best parse tree is considered, so we get Eq. 5:  M  ∑ e = arg max{ λmhm (e′, f , F, Z)}  (5)  e′,Z m =1  Feature Functions We use six feature functions in the log-linear generation models. Matching score and context similarity are used to estimate the similarity between the translation example and the input sentence. The former describes the semantic similarity of the matched fragments. The latter describes the context similarity of the sentences. Word translation probability and target language string selection probability are used to estimate the reliability of the translation example. The former is based on the word alignment probability. The latter makes use of the probability of the target language string given the source language matching-tree. Language model probability and length selection probability are used to estimate the quality of the generated translation. The former scores the fluency of the generated translation. The latter adjusts the estimation based on the target sentence length.  Matching Score The matching score between TSC and F is defined as the sum of the semantic similarity between the nodes in TSC and the matched nodes in F. It is calculated as shown in Eq. 6:  ∑ M (< t, s,c >,F) = Sim(ni , ni′)  (6)  ni∈t  Here ni is the ith node in t; ni' is the corresponding node of ni in F; Sim(ni,ni') is the semantic similarity between the headwords of ni and ni'. The semantic similarity between English words is calculated based on WordNet (Fellbaum, 1998). We employ the same method as described in (Lin, 1998).  Sim(n1, n2 ) = Sim( f1, f2 )  = 2× log p(C0 )  (7)  log p(C1) + log p(C2 )  Here C1 and C2 are the headwords of n1 and n2, respectively; C1 and C2 are the concepts subsuming the words f1 and f2, respectively; C0 is the nearest common ancestor in the semantic hierarchy that subsumes both C1 and C2; p(Ci) is the probability of encountering an instance of Ci in the corpus. Hence, we get the feature function hMS:  K  ∏ hMS (e,f ,F,Z) = log M (zk ,F)  (8)  k =1  Context Similarity Context similarity is used in various NLP tasks (Karov & Edelman, 1998; Schafer & Yarowsky, 2002). We use it to select the approximate translation example and to further improve translation selection. The context similarity is  defined as the word-based cosine distance between sentences.  CS(f , z) =  V ⋅ V′  ∑ ∑ (vi )2 ×  (v′j )2  (9)  i  j  Here V denotes the sequence of the word count of f; V' denotes the sequence of the word count of the source language sentence of the example from which z is derived. Thus, the context similarity hCS is defined as shown in Eq. 10:  K  ∏ hCS (e,f ,F, Z) = log CS(f , zk )  (10)  k =1  Word Translation Probability The quality of word alignment in the translation example affects the quality of the translation. To estimate the quality of the word alignment in TSC, we use the singleword translation probability between the source language matching-tree and the target language string. The word translation probability of TSC is defined in Eq. 11:  ∏ W (< t, s,c >) = ( p(ei | f j ))1/|c|  (11)  ( j,i)∈c  Here ei is the correspondence word of fj; p(ei|fj) is the word translation probability, which is derived from the word-aligned translation examples using Eq. 12.  ∑ p(e | f ) = C(e, f ) C(e′, f )  (12)  e′  Here C(e, f ) is the count of aligned word pair (e, f ) in the word-aligned corpus. The word translation probability hWTP is described in Eq. 13:  K  ∏ hWTP (e,f ,F, Z) = log W (zk )  (13)  k =1  Target Language String Selection Probability The target language string selection probability is described in Eq. 14:  T (< t, s, c >) = p(s | t)  ∑ = C(t, s) C(t, s′)  (14)  ∀(t , s ′, c )  Here p(s | t) is the probability of s given t. Higher target language string selection probability may result in more reliable target language string. Thus, we define the target language string selection probability hSSP as in Eq. 15:  K  ∏ hSSP (e,f ,F, Z) = log T (zk )  (15)  k =1  Language Model Probability A trigram language model is used to calculate the probability of the translation fragment occurring in the target language. This feature function hLMP is defined in Eq. 16:  I  ∏ hLMP (e,f , F, Z) = log( p(ei | ei−2 ,ei−1))1/ I  (16)  i=1  Here the geometric mean of the probability is used to prevent preference for a short translation.  Length Selection Probability Generally, the length of the target language sentence depends on that of the source language sentence. To ensure that the translation does not become too long or too short, we use a sentence length selection model to calculate the probability of the length of the target language sentence given the source language sentence.  ∑ p(I | J ) = C(I, J ) C(I ′, J )  (17)  I′  Here the word number of the sentence is defined as the length of the sentence. The model is trained on the bilingual example database. In the translation process, we need to score the length of the translation fragment, instead of the length of the sentence. The length of the fragment is more flexible. It is difficult to directly model the fragment length selection. The target language length tends to follow a normal distribution on the fixed source language length in the parallel corpus (Brown et al., 1991). Based on the normal distribution, we approximately model the fragment length selection using the ratio of the target language fragment length to the source language fragment length.  pN (r; μ,σ 2 ) = σ  
 This paper reports on the development of the Dutch Parallel Corpus: a high quality sentence-aligned parallel corpus of 10 million  words for the language pairs Dutch-English and Dutch-French. The corpus is composed of different text types. All steps of processing  the corpus including alignment and linguistic annotation undergo quality control on different levels. Four categories of potential users  of the DPC can be distinguished: developers of HLT-applications, linguists conducting more fundamental research, human translators  and language learners. This paper focuses on two types of intended users: MT developers and human translators. The paper describes  different characteristics of the corpus relevant for such users, concentrating on corpus design, processing of the corpus data and the  exploitation of the corpus.  computer-assisted language learning (Desmet and  Introduction  Paulussen, 2005).  This paper highlights the development of the Dutch Parallel Corpus (DPC): a high quality sentence-aligned parallel corpus of 10 million words for the language pairs Dutch-English and Dutch-French.  Apart from the more technological applications, parallel corpora can be used to conduct more fundamental research in the fields of contrastive linguistics and translation studies (Olohan, 2004).  Since the development of a parallel corpus is timeconsuming and costly, the DPC project aims at the creation of a multifunctional resource to satisfy the needs of a diverse group of potential users. The following characteristics of the corpus contribute to fulfilling this aim: Firstly, the DPC is being aligned on a sentence and, partially, on a sub-sentence level to guarantee its usability in such research areas as Machine Translation, ComputerAssisted Language Learning, Computer-Assisted Translation and other multilingual applications. Secondly, the DPC will be enriched with linguistic annotations to broaden the scope of its application. Thirdly, the corpus has a balanced composition of different text types. And last but not least, all data processing steps undergo quality control on different levels, including automatic and semi-automatic verification, as well as consistent manual checks. At the moment of the paper submission, the DPC project is going through its second stage, concentrating on data alignment. The next step will be linguistic annotation and the development of corpus exploitation tools. The paper is structured as follows: in the initial part we elaborate on the multifunctionality of the DPC, while the main part of the paper concentrates on the description of the corpus design, processing of the corpus data and the exploitation of the corpus. Multifunctional purpose Aligned parallel corpora are an indispensable resource for a wide range of multilingual applications: machine translation, especially corpus-based MT like statistical MT (Koehn, 2005) and example-based MT (Carl and Way, 2003), computer-assisted translation tools (Hutchins, 2005), multilingual information extraction and  Generally speaking, four categories of users can be distinguished: developers of HLT-applications, linguists conducting more fundamental research, human translators and language learners. Each of these four groups has its own requirements relating to corpus design, kind and degree of annotation and required metadata of a parallel corpus. In this paper we will focus on two types of intended users: MT developers and human translators. Machine Translation Aligned parallel corpora are used in MT as training and test material for corpus-based MT systems (SMT or EBMT). The most wide-spread parallel corpora used in MT cover a small set of domains or text types, and mostly contain texts of governments of multilingual countries, such as Canada (the Hansard Corpus English/French, consisting of the proceedings of the Canadian Parliament), or multinational institutions such as the United Nations (UN Parallel Text English/French/Spanish, containing archive documents of the Office of Conference Services in the period between 1988 and 1993) or the European institutions (Erjavec et al., 2005; Koehn, 2005). There is a need for more diversity in the types of texts compiled. Macken (2007) examined the problem of translational correspondence in different text types (user manuals, press releases and proceeding of plenary debates) in view of different heuristics used in existing sub-sentential alignment modules. She showed that for certain text types, it is sufficient to focus on contiguous translation units of maximally three words. However, the problem of translational correspondence was found to be more complex in text types where a freer or more target language-oriented translation style was adopted.  The DPC, which is currently being compiled, contains texts from a wide range of text types (fiction and nonfiction), and diverse domains. Full text corpora as translator’s aid The analysis of the TransSearch log files (Simard and Macklovitch, 2005) has shown that parallel corpora as such are a useful resource for professional translators to solve translation difficulties. With a bilingual concordancing system, translators can query a large corpus of aligned translated material in order to identify the more appropriate target language equivalents and idiomatic expressions for a difficult source language passage. The sentences matching the search query are retrieved and displayed together with their aligned translation. It is only recently that the potential of full text corpora as translation aid has been recognized. According to Bowker and Barlow (2004), bilingual concordancing systems in conjunction with aligned parallel corpora can be seen as complementary to translation memories. The decision on which tool to use depends on a number of factors, among others the nature of the job, the text type, the translator’s working style and the translator’s experience. According to Simard and Macklovitch, TransSearch processes thousands of queries every day, submitted by professional translators. Multitrans (Gervais, 2003) is another example of a translation support tool based on a repository of full text translations. Full text parallel corpora are extremely useful for translators as they can retrieve translations of words in context. Human translators are very demanding users of a parallel corpus and expect high-quality translations and high-quality alignments. Corpus Design The design principles of the DPC were based on two sources: on the one hand, the information available about other parallel corpus projects, and on the other hand the user requirements study, which was carried out within the DPC project. To identify the requirements of the user group with respect to corpus design, a questionnaire was put online on the DPC-website1. All members of the predefined user group, composed of academic and industrial specialists from different application and research domains, were asked to fill in the form. In addition, other interested parties were invited to participate. In total 34 respondents completed the questionnaire, of whom 17 are computational linguists. The analysis confirmed a strong need for a parallel corpus with Dutch as a central language. The analysis also showed that the quality of text materials as well as the quality of alignments and linguistic annotations are crucial for users of corpus applications. The users opted for a high variety of text types and rich metadata, and, in general, stated that inclusion of full texts is not a  necessary condition for them as long as fragments of different text types are present. Based on the user requirements analysis, motivated choices were made regarding the balancing criteria, text typology, sampling criteria, kind and degree of annotations and required metadata. The details are presented below. Language pairs and translation directions As stated earlier, the DPC consists of two language pairs: Dutch-English and Dutch-French and is bi-directional (Dutch as a source and a target language). A part of the corpus will be trilingual and will contain Dutch texts translated into both English and French. An important balancing criterion in a parallel corpus is the translation direction. The authors are not aware of any study investigating the impact of the translation direction on MT systems. However, translated texts tend to show certain idiosyncrasies. In translation studies, where among others the differences between translated and nontranslated texts are studied as a means to study the translation process, these idiosynchrasies are also known as translation universals. Baker (1995) mentions four features typical of a translated text: ‘simplification’ of the language or the message, ‘explicitation’, ‘normalization’, i.e. using only typical patterns of the target language, and ‘levelling out’ variations in the source text by converging towards the middle. Therefore, the DPC will be balanced according to the translation direction. Information about the translation direction of the texts included in the corpus, and about how the texts were translated (human translation, computer-assisted translation or machine translation corrected by a human) is documented in the metadata. The corpus will be balanced proportionally with respect to language pairs and translation directions. For this purpose the target figure of minimally 2 million words per translation direction has been set. Text types The DPC is designed to represent as wide a range of translated Dutch texts as possible. In order to get a wellbalanced corpus, texts are selected from different domains. The data in the corpus originates from two main sources: commercial publishers and institutions (both profit and non-profit), and this division is used to separate the text material into two big groups according to the type of text provider. Each group has been subsequently divided into several text types but the criteria for this division are not of the same nature. Those coming from commercial publishers are recognised genres: literature and journalistic texts. The institution texts were divided on the basis of their function and purpose: they instruct, document, inform and/or persuade.  
Statistical machine translation (SMT) is widely advocated as a promising approach to achieving translation quality at least comparable to the best rule-based machine translation (RBMT) systems, with greatly reduced effort to adapt to new language pairs and new domains, provided that sufﬁcient parallel training data is available. Such claims are hotly debated, but there is little argument that, to date, SMT systems have been much slower than the best RBMT systems. For example, Language Weaver, currently the only commercial provider of SMT systems, claims to translate 5,000 words per minute per CPU,1, while SYSTRAN, the market leader in commercial RBMT, claims to translate up to 450 words per second (27,000 words per minute) per CPU.2 In this paper, we present two modiﬁcations to the algorithm implemented in the widely-used Pharaoh phrasal SMT decoder (Koehn, 2003; Koehn 2004a; Koehn 2004b) that together permit much faster decoding without losing translation quality as measured by the BLEU metric (Papineni et al., 2002). The ﬁrst modiﬁcation improves the estimated cost function used by Pharaoh to rank partial hypotheses, by incorporating an estimate of the distortion penalty to be incurred in translating the rest of the sentence. The second modiﬁcation uses early pruning of possible next-phrase translations to cut down the overall size of the search space. These modiﬁcations enable decoding speed-ups of an order of magnitude or more, with no reduction in the BLEU score of the resulting translations. 2. A Phrasal SMT Model Phrasal SMT, as described by Koehn et al. (2003) translates a source sentence into a target sentence by decomposing the source sentence into a sequence of source phrases, which can be any contiguous sequences of words (or tokens treated as words) in the source sentence. For each source phrase, a target phrase translation is selected, and the target phrases are arranged in some order to produce the complete 1http://www.languageweaver.com/page.asp?intNodeID=862 &intPageID=851 2http://www.systransoft.com/index/Products/Server-Products /SYSTRAN-Enterprise-Global-Server/System-Requirements  translation. A set of possible translation candidates created in this way is scored according to a weighted linear combination of feature values, and the highest scoring translation candidate is selected as the translation of the source sentence. Symbolically, n tˆ = arg max λifi(s, a, t) t,a i=1 where s is the input sentence, t is a possible output sentence, and a is a phrasal alignment that speciﬁes how t is constructed from s, and tˆ is the selected output sentence. The weights λi associated with each feature fi are tuned to maximize the quality of the translation hypothesis selected by the decoding procedure that computes the arg max. We use a fairly standard phrasal SMT model that includes the following features: • the sum of the log probabilities3 of each source phrase in the hypothesis given the corresponding target phrase, • the sum of the log probabilities of each target phrase in the hypothesis given the corresponding source phrase, • the sum of lexical scores for each source phrase given the corresponding target phrase, • the sum of lexical scores for each target phrase given the corresponding source phrase, • the log of the target language model probability for the sequence of target phrases in the hypothesis, • the total number of words in the target phrases in the hypothesis, • the total number of source/target phrase pairs composing the hypothesis, 3Koehn describes the translation model and the operation of Pharaoh in terms of products of probabilities rather than sums of log probabilities. Our choice is completely equivalent, since the product of a set of probabilities is monotonically related to the corresponding sum of log probabilities.  • a distortion penalty reﬂecting the degree of divergence of the order of the target phrases from the order of the source phrases. The probabilities of source phrases given target phrases and target phrases given source phrases are estimated from a word-aligned bilingual corpus. The lexical scores are computed as the log of the unnormalized probability of the Viterbi alignment for a phrase pair under IBM wordtranslation Model 1 (Brown et al., 1993). For each phrase pair extracted from the word-aligned corpus, the values of these four features are stored in a “phrase table”. The target language model is a trigram model smoothed with bigram and unigram language models, estimated from the target language half of the bilingual training corpus. The distortion penalty is computed as required by the Pharaoh decoder, which we explain in Section 4. We train the feature weights for the overall translation model to maximize the BLEU metric using Och’s (2003) minimum-errorrate training procedure. 3. Description of Pharaoh The Pharaoh decoder uses a beam search to try to ﬁnd the translation of an input source sentence that has the highest score according to the phrasal SMT model. It creates a set of possible translations, building each target language string from left to right. At each step, it extends a partial translation hypothesis by picking a source phrase covering words that have not yet been translated in that partial hypothesis, and a possible target language translation for that phrase, and appending the target language phrase to the incomplete target language string. The search through the partial hypotheses proceeds in order of the number of source words translated. All the partial hypotheses that cover the same number of source words are compared to each other, and this set is pruned before any members of the set are extended. This core algorithm is presented in Figure 1, taken from Koehn (2003; 2004a; 2004b). There are at least two key features of Pharaoh that are not revealed at the level of detail presented in Figure 1. First, in addition to beam-search pruning, Pharaoh also performs lossless pruning whenever multiple partial hypotheses agree in • the source words already translated • the last two target words produced • the position of the ﬁnal word of last source phrase translated In this situation, any given hypothesis completion will incur the same incremental cost starting from any of these hypotheses; so, the best scoring member of a set of such hypotheses cannot be surpassed by any other in the set. Pharaoh keeps only the highest scoring such hypothesis in the beam search, although the others are saved in case multiple translation hypotheses are desired. The second key feature of Pharaoh not revealed in Figure 1 is how Pharaoh computes the partial hypothesis scores used for pruning. The score that Pharaoh uses to compare  competing hypotheses consists of two components, an exact score for the part of the translation that the hypothesis is committed to, and an estimated score for the portion of the source sentence remaining to be translated. To compute the estimated scores, before starting to translate a sentence Pharaoh ﬁnds the best possible estimated phrase pair score for each source phrase in the phrase table that matches some contigous subsequence of the input source sentence. An estimated score for every contiguous subsequence of the input is then computed by ﬁnding the sequence of source phrases covering the input subsequence with the highest sum of estimated scores. This is computed in O(n2) time by dynamic programming. The estimated score for each phrase pair is computed as the sum of the feature values in the phrase table for that phrase pair, along with the target word count and phrase pair count, plus an approximate target language score for the target phrase in the pair, all weighted by the corresponding translation model weights. The target language model score can only be approximated, because we don’t yet know what the language model context will be if the phrase pair in question is actually used to complete the translation of the input source sentence. The approximate target language model score therefore uses the unigram probabilty estimate for the ﬁrst word of the target phrase, the bigram probability estimate for the second word of the target phrase, and the full trigram probabililty estimate only for the third and subsequent words of the target phrase. 4. Distortion Penalty Estimation Our ﬁrst improvement to the algorithm implemented by Pharaoh is to incorporate an estimate of the distortion penalty yet to be incurred into the estimated score for the portion of the source sentence remaining to be translated. Such an estimate is notably absent from the score used by Pharaoh for pruning sets of competing partial hypotheses. The value of the distortion penalty feature used by Pharaoh is the sum of the distances between source phrases whose target phrase translations are adjacent in the target language string. Speciﬁcally, Koehn (2004a) deﬁnes the incremental distortion penalty for each pair of adjacent target phrases as: d = abs( last word position of previously translated phrase + 1 - ﬁrst word position of newly translated phrase ) We can break this down into two simple cases using the following defnitions: • ∆d is the distortion penalty increment for a partial hypothesis, relative to the immediate predecessor it was formed from by adding a translation for the source phrase S. • S is the last source phrase translated in the immediate predecessor, • L(S) and L(S ) are the length in words of S and S , respectively. • D(S, S ) is the number of words between S and S .  initialize hypothesisStack[0 .. nf]; create initial hypothesis hyp_init; add to stack hypothesisStack[0]; for i=0 to nf-1: for each hyp in hypothesisStack[i]: for each new_hyp that can be derived from hyp: nf[new_hyp] = number of foreign words covered by new_hyp; add new_hyp to hypothesisStack[nf[new_hyp]]; prune hypothesisStack[nf[new_hyp]]; find best hypothesis best_hyp in hypothesisStack[nf]; output best path that leads to best_hyp;  Figure 1: Pharaoh beam search algorithm  In terms of these symbols, the two cases are: • If S is to the right of S , ∆d = D(S, S ) • If S is to the left of S , ∆d = D(S, S ) + L(S) + L(S ) We estimate the distortion penalty yet to be incurred by a partial hypothesis to be the minimum possible additional distortion penalty, given the source words translated so far and the ﬁnal word position of the last source phrase translated. It is easy to prove by induction on the number of untranslated words that, for any partial hypothesis, the minimum additional distortion penalty is that produced by picking as the next source phrase to translate one that begins with the left-most untranslated source word and proceeding left-to-right covering all the remaining untranslated source words in order.4 The computationally simplest way to take this minimum possible additional distortion penalty into account is just to fold it into the distortion penalty as we incrementally accumulate it. To describe this modiﬁcation, we use the previous deﬁnitions, and we also deﬁne S to be the longest fully-translated initial segement of the source sentence prior to translating S, and D(S, S ) to be the number of words between S and S . Note that S immediately precedes the left-most untranslated word. The computation of the modiﬁed ∆d can be broken down four cases: • If S is adjacent to S , ∆d = 0 • Otherwise, if S is to the left of S , ∆d = 2L(S) • Otherwise, if S is a subsequence of S ∆d = 2(D(S, S ) + L(S)) • Otherwise, 4The proof requires assuming that a distortion penalty increment is incurred if the last source phrase translated does not occur at the end the source sentence. None of the written descriptions of Pharaoh state whether this is the case.  ∆d = 2(D(S, S ) + L(S)) This modiﬁed distortion penalty can be shown to have the same value as that used in Pharaoh, over an entire, completed translation hypothesis,5 but it “front-loads” the accumulation of the distortion penalty. For example, if we skip over a single word towards the beginning of a source sentence and then translate a number of phrases monotonically, the distortion penalty as calculated by Pharaoh will be 1, until we ﬁnally jump back to translate the skipped word. Using our modiﬁed distortion penalty, as we translate more and more words beyond the skipped word, we accumulate a progressively larger distortion penalty, because we know that we must eventually go back to translate the skipped word. 5. Early Pruning Our second modiﬁcation of the Pharaoh algorithm addresses the sixth line in Figure 1, which says: for each new hyp that can be derived from hyp: This means that (subject to static phrase table and distortion limits discussed later) every possible translation of every possible next phrase (not involving words already translated) will be considered as an extension to a given partial hypothesis. No pruning of any possible extension is considered until an estimated score for the extension has been computed as described in Section 3. Recall that in order to have an estimated score for each possible subsequence of the input source ready, we have precomputed an estimated score for each possible phrase translation that includes all aspects of the translation model, except for the distortion penalty and a language model score adjustment that replaces the unigram and bigram scores for the ﬁrst two words of the target phrase with their full trigram scores. We can prune the search earlier than Pharaoh does, in a way that lets us eliminate multiple possible next source phrases and multiple possible translations for source phrases not eliminated, without even examining them, provided we are willing to forgo having the pruning take into account the language model score adjustment for the last phrase translated in a given partial hypothesis. 5This requires making the same assumption about a sentenceﬁnal distortion penalty noted eariler.  We introduce additional points at which the search is pruned by comparing each partial hypothesis to its possible extensions, and stopping the search for extensions when the estimated score of the extensions (before making the language model score adjustment) is worse than the estimated score of the partial hypothesis we are extending by more than a ﬁxed early pruning threshold. We do this in addition to performing Pharaoh’s pruning step, which compares all partial hypotheses that cover the same number of source words using an estimated score that does include the language model score adjustment. Several observations help us organize the search through possible extensions to a given partial hypothesis. First, for any given starting point for the next phrase to be translated, a phrase of length 1 will produce the minimum additional distortion penalty. Second, the minimum additional distortion penalty given a starting point never decreases as we move the starting point from left to right. Third, for any given starting point, the additional distortion penalty never decreases as we increase the length of the source phrase to be translated. With these facts in mind, we search from left to right through the possible starting positions for the next source phrase to translate. For each position, we compute the minimum additional distortion penalty for a source phrase starting at that position. If we ﬁnd a possible starting position such that the minimum additional distortion penalty (weighted by the corresponding translation model weight) is greater than our early pruning threshold, we stop looking for possible next source phrases to translate, because all the ones that we have not considered will also have additional distortion penalties greater than the threshold. For each possible starting position that passes this test, we search through possible ending positions from left to right. If we ﬁnd a possible ending position such that the weighted additional distortion penalty for the phrase spanning the starting and ending positions is greater than the threshold, we stop looking for possible ending positions for that starting position, because all the ones that we have not considered will also have weighted additional distortion penalties greater than the threshold. Each starting and ending position pair that passes this test deﬁnes a possible next source phrase to translate. For each such source phrase that has entries in the phrase table, we search through its possible translations, from best scoring to worst scoring, having sorted the phrase table in this way ofﬂine. For each translation, we compute the estimated score of the resulting partial hypothesis, taking into account everything except the language model score adjustment. If the difference between this estimated score and that of the hypothesis we are extending is greater than the early pruning threshold, we stop looking at possible translations for this source phrase, because all the translations that we have not considered will also yield estimated score differences greater than the threshold. 6. Evaluation We have carried out experiments evaluating three different algorithms: the original Pharaoh algorithm, the Pharaoh algorithm plus distortion penalty estimation, and  the Pharaoh algorithm plus distortion penalty estimation and early pruning.6 In order to measure the effects of our modiﬁcations to the Pharaoh algorithm as accurately as possible, we have reimplemented the algorithm described by Koehn in such a way that the three systems are identical except for the algorithmic differences under evaluation. We have implemented all three algorithms in Perl, which is a byte-code interpreted language, so the absolute time measurements are slower that what would be expected from implementations that compile to native machine code. The relative timings should still be indicative of the relative efﬁciency of the algorithms, however. Moreover, we also report a measure of the search space explored that should be independent of other implementation details: the number of partial hypotheses evaluated per source word. Since decoding effort depends on several pruning parameters, a fair evaluation of the Pharaoh algorithm and its variants requires testing many combinations of settings for these parameters. There are four main pruning parameters: • T-table threshold: the maximum difference in estimated score between the best translation and the worst translation in the phrase table for a given source phrase • Beam threshold: the maximum difference in estimated score between the best partial hypothesis and the worst partial hypothesis retained for a given number of source words covered • T-table limit: the maximum number of translations in the phrase table for a given source phrase • Beam limit: the maximum number of partial hypotheses retained for a given number of source words covered Below, when we discuss particular vectors of pruning parameter settings, we will give them in the order above. In Koehn’s implemetation of Pharaoh, the two threshold parameters are expresssed as ratios of probabilities. Our threshold parameters have exactly the same effect, but at different speciﬁc settings. A ﬁfth parameter that can be viewed as a pruning parameter is the distortion limit, which restricts the maximum distortion increment permitted between source phrases whose translations are adjacent in the output target sentence. We prefer to view this as a model parameter, however, because setting it to an optimum value usually improves translation quality over leaving it unrestricted. For all the experiments reported here, we set the distortion limit to 5. This seems to be within the range of typical settings for using Pharaoh, and it also appeared in informal experimentation that for settings greater than 5, translation quality started to decline markedly given our data and models. 6Without distortion penalty estimation, early pruning can lead to failure to ﬁnd a translation, because it is possible for all extensions to fail the early pruning test for all partial hypotheses within the beam. This cannot happen if our distortion penalty estimate is used, because in that case, at least one extension of each partial hypothesis will have an estimated score (without the language model score adjustment) identical to the estimated score of the hypothesis it extends.  The exact version of distortion limit we implemented allows one more word in the backward direction than the forward direction (otherwise a distortion limit of 1 would allow no distortion at all, since the minimum cost of a backwards jump is 2), and we also disallowed conﬁgurations where jumping back to the left-most untranslated word would violate the distortion limit.7 For all three algorithms tested, we used Koehn’s deﬁnition of distortion for applying the distortion limit, even when the modiﬁed version was used in the beam search. For the decoder with early pruning, the early pruning threshold might also be treated as an independent parameter. However, there is a close connection between the early pruning threshold and the T-table threshold. If the T-table threshold is increased beyond the early pruning threshold, none of the additional phrase table entries will ever survive early pruning. We therefore always used the same setting for the T-table and early pruning thresholds. We performed a hill-climbing search for combinations of settings of the four pruning parameters that produce good trade-offs of decoding time vs. BLEU score. We tried ﬁve different settings for each of the pruning paramenters; 0.5, 1.0, 1.5, 2.0, and 2.5 for the threshold parameters, and 5, 10, 15, 20, and 25 for the limit paramters. For the modiﬁed algorithms, this appeared to be a sufﬁcient range to ﬁnd the operating points that produced the highest BLEU score, but the baseline Pharaoh algorithm seemed to require a greater beam limit to avoid losing translation quality. So, we also tested beam limits of 30, 35, 40, 45, 50, 60, 75, and 100 with the baseline system, with the other parameters set to selected combinations of settings that produce good timequality tradeoffs at lower beam limits. Our training and test data came from an English-French bilingual corpus of Canadian Hansards parliamentary proceedings supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea & Pedersen, 2003). Automatic sentence alignment of this data was provided by Ulrich Germann. We used 500,000 sentences pairs from this corpus for training both the phrase translation models and IBM Model 1 lexical scores. This training data was word-aligned using a state-of-the-art word-alignment method (Moore et al., 2006), and all pairs of phrases up to 7 words in length were extracted and their translation probabilities estimated using the method described by Koehn et al. (2003). A separate set of 500 sentence pairs was used to train the translation model weights, and an additional 2000 sentence pairs were used for test data. For each combination of pruning parameter settings tested, we measured the time required for decoding in milliseconds per word, the size of the search space in partial hypotheses evaluated per word, and the BLEU score of the resulting translations on a scale of 0–100 (BLEU[%]). The scatter plots in Figures 2 and 3 display BLEU score vs. decoding time, and BLEU score vs. search space. Since the scatter plots for the three algorithms overlap somewhat, we have highlighted the limits of each algorithm as deﬁned by the upper convex hull of points for that algo- 7We believe this is more-or-less how the distortion limit works in Koehn’s implementation of Phraraoh, but the published descriptions do not go into these details.  rithm. This picks out what are arguably the best points in terms of the trade off between decoding effort and translation quality as measured by our metrics. Note that the horizontal axes are presented logarithmically to make the differences in decoding effort clear at all scales. From Figures 2 and 3, we see that all three algorithms eventually produce the same highest value for the BLEU score (30.22 BLEU[%]), but the algorithm that employs distortion penalty estimation does so with much less decoding effort than the baseline algorithm, and the algorithm that uses both distortion penalty estimation and early pruning requires even less decoding effort. This is true whether decoding effort is measured in terms of time or search space. Indeed, for each of the three algorithms, the correlation between the decoding time and the number of partial hypotheses evaluated is greater than 0.99. The pruning parameter vectors that produced the highest BLEU score were (1.5, 1.0, 20, 10) for both of the modiﬁed algorithms, and (1.5, 1.0, 20, 75) for the Pharaoh baseline algorithm. Comparing the decoding times needed to obtain the highest BLEU score, the Pharaoh algorithm takes 106.0 milliseconds per word, adding distortion penalty estimation brings this down to 34.2 milliseconds per word, and adding early pruning to that reduces the time to 9.02 milliseconds per word. If we are willing to accept a score 0.02 BLEU[%] lower (30.20), the Pharaoh algorithm takes 38.6 milliseconds per word, adding distortion penalty estimation yields a time of 14.7 milliseconds per word, and adding early pruning yields 3.59 milliseconds per word. The ratio of decoding times for the Pharaoh algorithm compared to that for the best system is 11.8 to 1 to reach the highest BLEU score, and 10.8 to 1 to reach a score of 30.20 BLEU[%]. The ratios of search space for the Pharaoh algorithm compared to that for the best system are even more dramatic. The ratio to reach the highest BLEU score is 18.4, and the ratio to reach a score of 30.20 BLEU[%] is 20.5. We can cast some light on where the speed-ups are coming from by comparing the algorithms at the same pruning settings, looking at differences in BLEU score and decoding time ratios. Comparing the original Pharaoh algorithm to the Pharaoh algorithm plus distortion penalty estimation, up to a beam limit of 25, the decoding time ratio for the same pruning settings ranged from 0.88 to 1.19 — very little difference. However, the difference in BLEU score when distortion penalty estimation was used ranged from +0.17 BLEU[%] to +0.68 BLEU[%]. Thus the speed-up from distortion penalty estimation came from being able to obtain a given BLEU score at much tighter pruning settings than were necessary with the baseline alogorithm, rather than speeding up decoding at a given combination of pruning settings. Comparing distortion penalty estimation to distortion penalty estimation plus early pruning, the difference in BLEU score at the same pruning settings ranged only from −0.024 BLEU[%] to +0.016 BLEU[%]. The decoding time ratio, however, ranged from 1.37 to 6.36 times faster for the decoder with early pruning. Thus early pruning makes almost no difference in BLEU score at a given combination of pruning settings, but it makes decoding up to six times faster.  BLEU[%]  30.3 30.2 30.1 30 29.9 29.8 29.7 29.6 29.5 29.4 0.1  
In this paper, we investigate the feasibility of combining two data-driven machine translation (MT) systems for the translation of sign languages (SLs). We take the MT systems of two prominent data-driven research groups, the MaTrEx system developed at DCU and the Statistical Machine Translation (SMT) system developed at RWTH Aachen University, and apply their respective approaches to the task of translating Irish Sign Language and German Sign Language into English and German. In a set of experiments supported by automatic evaluation results, we show that there is a deﬁnite value to the prospective merging of MaTrEx’s Example-Based MT chunks and distortion limit increase with RWTH’s constraint reordering. 
Abstract In this paper, we propose a new method for phrase alignment using a dependency type distance and a distance-score function. With this method, appropriate correspondences can be selected among correspondence candidates that often include ambiguous or incorrect ones. Furthermore, this method makes it possible to measure the overall alignment consistency. We conduct an alignment experiment using 500 parallel sentences on newspaper domain, and achieve an F-measure improvement of 35 points over the simple statistical method (GIZA++), and 3.0 points over a baseline system. We also conducted a translation experiment and achieved a BLEU score improvement of 0.4 points over a baseline system.  
This paper describes two studies on the effectiveness of Controlled Language (CL) rules for MT. Both studies investigated the language pair English-German and used corpora from the IT domain. However, they differ in terms of the MT engines employed (Systran vs. IBM WebSphere) and the evaluative methodologies used. Study A examines the effectiveness of CL rules by measuring temporal, technical and post-editing effort. Study B examines the effectiveness of rules by measuring comprehensibility. Both Study A and Study B concluded that some CL rules had a high impact for MT while other rules had a moderate, low or no impact. The results are compared in order to determine what, if any, common conclusions can be drawn. Our conclusions are that rules governing misspelling, incorrect punctuation, sentences longer than 25 words, and the use of personal pronouns with no antecedent in a sentence had a high impact on both post-editing effort and comprehensibility. Further, we found that the use of personal pronouns with antecedents in the same sentence and stand-alone demonstrative pronouns had a low impact, while the rule advocating the use of "in order to" in purposive clauses had no impact in either study. The paper also discusses contrasting results for both studies.  
This paper describes a novel method of improving machine transliteration by using multiple transliteration hypotheses and re-ranking them. We constructed seven machine-transliteration engines to produce a set of transliteration hypotheses. We then re-ranked the hypotheses to select the correct transliteration hypothesis. We propose a re-ranking method that makes use of conﬁdence-score, languagemodel, and Web-frequency features and combines them with machine-learning algorithms including support vector machines and the maximum entropy model. Our testing of English-to-Japanese and English-to-Korean transliterations revealed that the individual transliteration engines used in our approach performed comparably to previous approaches and that re-ranking improved word accuracy compared to the best individual engine from about 65 to 88%.  1. Introduction Transliteration is particularly used to translate proper names and technical terms from languages using the Roman alphabet into ones using non-Roman alphabets such as Chinese, Japanese, or Korean. Because transliteration is one of the main causes of the out-of-vocabulary (OOV) problem, machine transliteration has received a signiﬁcant degree of attention as a tool to support machine translation (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002) and cross-language information retrieval (Fujii and Tetsuya, 2001). A variety of paradigms for machine transliteration have been developed over the years: grapheme1-based model (GM) (Kang and Kim, 2000; Goto et al., 2003), phoneme2-based model (PM) (Knight and Graehl, 1998; Kang, 2001), hybrid model (HM) (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004), and correspondence-based model (CM) (Oh and Choi, 2002; Oh and Choi, 2005). These models are classiﬁed in terms of the information sources used for transliteration or the units that are transliterated. GM, PM, HM, and CM make use of source graphemes, source phonemes, both source graphemes and source phonemes, and the correspondence between source graphemes and phonemes, respectively. Transliteration is generally a phonetic rather than an orthographic process (Knight and Graehl, 1998). However, both the source grapheme and source phoneme or either of them can affect the target-language transliteration (e.g. a Japanese or Korean transliteration). For this reason, there are transliterations that are grapheme-based, ones that are phoneme-based, and ones that are a combination of grapheme-based and phoneme-based transliterations. For example, the respective Korean transliterations of data, amylase, and neomycin are the phoneme-based transliteration ‘de-i-teo (X<s')’3, the grapheme-based translit- 1Graphemes refer to the basic units (or the smallest contrastive units) of a written language: e.g., English has 26 graphemes or letters 2Phonemes are the simplest signiﬁcant unit of sound. 3In this paper, target-language transliterations are represented  eration ‘a-mil-la-a-je (x9]j)’, and ‘ne-o-ma-i-sin (W1¸s)’, which is a combination of the graphemebased transliteration ‘ne-o (W1¸)’ and the phoneme-based transliteration ‘ma-i-sin (s)’. However, because each of the transliteration models depends on a particular information source, each can produce transliterations with errors. Moreover, different transliteration models usually produce different errors and different transliterations. This means we should be able to improve transliteration by combining various transliteration models into one machinetransliteration system that combines the advantages of the individual models and suffers from few of their disadvantages. A similar idea has been successfully applied to automatic speech recognition (ASR) and machine translation (Fiscus, 1997; Nomoto, 2004). In our previous work (Oh et al., 2006b), we have shown that this idea is also helpful for improving machine transliteration. It used transliteration hypotheses derived from four transliteration engines and re-ranked them with the product of two ranking functions, each of which was based on the rank of hypotheses in each transliteration engine and Web frequency. Even though the re-ranking in Oh et al. (2006b) performed well, it had limitations in taking various features into account and effectively combining them. To address this problem, we developed SVM-based and MEM-based re-ranking methods, which are able to effectively combine various features. We describe our framework in Sections 2. and 3. We then describe our evaluation in Section 4. and review related work in Section 5. The paper is concluded in Section 6. 2. Producing Transliteration Hypotheses We used multiple transliteration engines based on GM, PM, HM, and CM to produce transliteration hypotheses. GM, PM, and CM can generally function alone as transliteration engines, while HM depends on other transliteration models to estimate its parameters. Therefore, we called a in their Romanized form with single quotation marks and hyphens between syllables.  transliteration engine based on GM, PM, or CM a “singlemodel engine” and one based on HM a “hybrid-model engine.” We used seven transliteration engines. Three were single-model engines corresponding to GM, PM, and CM. The other four were hybrid-model engines. Three of these corresponded to HM using two of GM, PM, and CM — HM(G+P), HM(G+C), and HM(P+C) — and the last was based on HM using all three (HM(G+P+C)). Note that HM(G+P) has previously been described (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004), and the other HMs are newly proposed here. 2.1. Single-Model Engines Let SW be a source word, PSW be the pronunciation of SW , TSW be a target word corresponding to SW , and CSW be the correspondence between SW and PSW . PSW and TSW can be segmented into a series of sub-strings, each of which corresponds to a source grapheme. We can thus write SW = s1, · · · , sn = sn1 , PSW = p1, · · · , pn = pn1 , TSW = t1, · · · , tn = tn1 , and CSW = c1, · · · , cn = cn1 , where si, pi, ti, and ci = < si, pi > respectively represent the ith source grapheme, source phonemes corresponding to si, target graphemes corresponding to si and pi, and the correspondence between si and pi. Table 1 shows an example of correspondence between si, pi, and ti, where SW = acetylcholine, PSW =“AH S EH T AH L K OW L IY N”4, and TSW = ‘a-se-ti-ru-ko-rin (¢»Áë³êó)’ in Japanese, and TSW =‘a-se-til-kol-lin ([j9?+J2;)’ in Korean. With this deﬁnition, GM (SW → TSW ), PM (SW → PSW and PSW → TSW ), and CM (SW → PSW and CSW → TSW ) can respectively be represented as Eqs. (1), (2), and (3). Given the assumption that each transliteration model depends on the size of the context, k, Eqs. (1), (2), and (3) can be simpliﬁed into a series of products.  P rG = P rG(TSW |SW ) = P r(tn1 |sn1 )  (1)  ≈ P r(ti|tii−−1k, sii+−kk)  i  P rP = P rP (TSW |SW )  (2)  = P r(pn1 |sn1 ) × P r(tn1 |pn1 )  ≈ P r(pi|pii−−1k, sii+−kk) × P r(ti|tii−−k1 , pii+−kk)  i  P rC = P rC(TSW |SW )  (3)  = P r(pn1 |sn1 ) × P r(tn1 |cn1 )  ≈ P r(pi|pii−−1k, sii+−kk) × P r(ti|tii−−k1 , cii+−kk)  i  To estimate the probabilities, P r(ti|tii−−k1 , sii−+kk),  P r(pi|pii−−1k, sii+−kk),  P r(ti|tii−−1k, pii+−kk),  and  P r(ti|tii−−1k, cii+−kk), in Eqs. (1), (2), and (3), we use  the maximum entropy model (Berger et al., 1996). Event  ev in this model is composed of a target event (te) and a  4ARPAbet symbols are used to represent English phonemes. ARPAbet is one of the methods used for coding English phonemes into ASCII characters (http://www.cs.cmu. edu/˜laura/pages/arpabet.ps)  history event (he) and is represented by a bundle of feature functions (fi(he, te)) that represent certain characteristics in event ev. The feature functions enable a model based on the maximum entropy model to estimate probability (Berger et al., 1996). Therefore, designing the feature functions, which effectively support certain decisions made by the model, is important. Our basic strategy in designing the feature functions was that the context information collocated with the unit of interest was important. On the basis of this strategy, we designed feature functions with the following features. Table 2 shows examples of feature functions based on the following features.  • Single features in sii+−kk, pii+−kk, cii+−kk, and tii−−1k (e.g., si, pi, ci, and ti−1) • Combinations between features of the same type including bigram and trigram (e.g., {sii−2}, {pi−2, pi, pi+2}, {cii+2}, and {tii−−12}) • Combinations between features of different type (e.g., {sii−1, pi}, {si−1, tii−−12}, {ci, ti−1}, and {pii−−23, ti−2}) – between sii+−kk and pii+−kk – between sii+−kk and tii−−1k – between pii+−kk and tii−−1k – between cii+−kk and tii−−1k A conditional maximum entropy model is generally an exponential log-linear model that gives the conditional probability of event ev =< te, he >, as described in Eq. (4), where λi is the parameter to be estimated (Berger et al., 1996).  P r(te|he) =  exp( i λifi(he, te)) te exp( i λifi(he, te))  (4)  Using Eq. (4) and feature functions, we can estimate the conditional probabilities in Eqs. (1), (2), and (3), as P r(ti|tii−−1k, cii+−kk) = P r(te|he), because we can respectively represent te and he as ti and tuples < tii−−1k, cii+−kk >. In the same way, P r(ti|tii−−1k, sii+−kk), P r(ti|tii−−1k, pii+−kk), and P r(pi|pii−−1k, sii+−kk) can be represented as P r(te|he) with their corresponding target events and history events. We used the “Maximum Entropy Modeling Toolkit”5 to esti- mate the probabilities and the LBFGS algorithm to ﬁnd λi in Eq. (4).  2.2. Hybrid-Model Engines Using the deﬁnition of HM in Al-Onaizan and Knight (2002) and Bilac and Tanaka (2004), we can represent four hybrid-model engines in a straightforward manner — Eqs. (5), (6), (7), and (8), where 0 < α, β, γ, δ1, δ2, δ3 < 1 and δ1 + δ2 + δ3 = 1. Note that P rG, P rP , and P rC in Eqs. (5), (6), (7), and (8) correspond to Eqs. (1), (2), and (3), respectively.  5Available at http://homepages.inf.ed.ac.uk/ s0450736/maxent_toolkit.html  English Japanese Korean  ci  si pi  ti  tsj  ti  tsj  acety l c  AH S EH T AH L K  ‘a’ ‘s’ ‘e’ ‘t’ ‘i’ ‘ru’ ‘k’  ¢  »  Á  ë  ‘a’ ‘s’ ‘e’ ‘t’ ‘i’ ‘l’ ‘k’    [j  9  ho ε OW ε ‘o’ ³ ε ‘o’ ?+J  l L ‘r’ ê ‘l’ ‘l’  i ne IY N ε ‘i’ ‘n’ ε ó ‘i’ ‘n’ ε ;2  Table 1: Examples of correspondence between si, pi, and ti. ε means a NULL character and tsj represents target-language syllables.  fj  sii+−kk  pii+−kk  tii−−1k  ti  f1  sii+−11 = cet  –  f2  cii−1 =< ce,“S EH”>  ti−1=‘s’ ti=‘e’  –  ti=‘e’  f3  –  pi=“EH”  –  ti=‘e’  f4  sii+1 = et  pii+2=“EH T AH”  –  ti=‘e’  f5 sii−−12 = ac and si+1 = t  –  tii−−12=‘a-s’ ti=‘e’  f6  ci =< e,“EH”>  tii−−12=‘a-s’ ti=‘e’  f7  si = e  pi−3=$ and pii+1=“EH T”  –  ti=‘e’  Table 2: Examples of feature functions derived from Table 1. $ represents the start of words  P rHM(G+P)(TSW |SW )  (5)  = α × P rP + (1 − α) × P rG  P rHM(G+C)(TSW |SW )  (6)  = β × P rC + (1 − β) × P rG  P rHM(P+C)(TSW |SW )  (7)  = γ × P rC + (1 − γ) × P rP  P rHM(G+P+C)(TSW |SW )  (8)  = δ1 × P rG + δ2 × P rP + δ3 × P rC  We ﬁrst produce n-best transliteration hypotheses using a stack decoder (Schwartz and Chow, 1990) for each transliteration engine. We then make a set of transliteration hypotheses comprising the n-best transliteration hypotheses produced by the seven transliteration engines.  3. Re-Ranking Transliteration Hypotheses The transliteration hypotheses in the set are re-ranked to enable a correct hypothesis to be identiﬁed. Re-ranking has been successfully applied to several NLP problems including statistical parsing (Collins and Koo, 2005; Daume III and Marcu, 2004; Shen and Joshi, 2003), machine translation (Shen et al., 2004), and name entity taggers (Ji et al., 2006). We selected support vector machines (SVMs) and the maximum entropy model (MEM) (Daume III and Marcu, 2004; Ji et al., 2006) from the machine-learning algorithms used for re-ranking. Let hi ∈ H be the ith transliteration hypothesis of source word s, hcorrect be a correct transliteration hypothesis corresponding to s, xi ∈ X be a feature vector of hi, and yi be the training label for xi. What we need to do is devise a rank function, g(xi), in Eq. (9) that ranks hcorrect higher and the others lower.  g(xi) : X → {r : r is ordering of hi ∈ H}  (9)  We ﬁrst train SVMs and MEM with training data set D = {xi, yi}, where xcorrect is a positive sample (ycorrect = positive) and xi=correct is a negative sample (yi=correct = negative). The SVMs assign a value to each transliteration hypothesis (hi) using  gSV M (xi) = w · xi + b  (10)  where w denotes a weight vector. We did not use the pre- dicted class of xi in SVM-based re-ranking but the predicted value of gSV M (xi) because our re-ranking function, as represented by Eq. (9), determines the relative ordering between hi and hj in H. A re-ranking function based on MEM assigns probability to hi using  gMEM (xi) = P r(hcorrect|xi)  (11)  We can ﬁnally obtain a ranked list for given H and X — the higher the g(xi) value, the better the hi. We used SV M light (Joachims, 2002) and the “Maximum Entropy Modeling Toolkit” for our re-ranking.  3.1. Features for Re-ranking We needed to design a suitable feature to measure the relevance between a source word, s, and a transliteration hypothesis, hi, of s to train the SVMs and MEM for our reranking. We introduced three types of features. Conﬁdence Score (8 features toal): Each transliteration engine produces transliteration hypotheses and their corresponding conﬁdence scores using Eqs. (1)–(8). We use the scores as a feature for re-ranking. Let CSM(s, hi) be a conﬁdence score function in M ∈ {G, P, C, HM(G + P), HM(G + C), HM(P + C), and HM(G + P + C)}, each of which corresponds to Eqs. (1), (2), (3), (5), (6), (7), and (8), and ACSM(s, hi) be the average of CSM(s, hi) over M. The conﬁdence score features for hi can then be ac-  quired using Eq. (12).  CSM(s, hi) = P rM(hi|s)  (12)  ACS(s, hi)  =  
 This paper presents a method to eﬀectively introduce translation dictionaries into phrase-based SMT. Though SMT systems can be built with only a parallel corpus, translation dictionaries are more widely available and have many more entries than parallel corpora. A simple and low-cost method to introduce a translation dictionary is to attach a dictionary entry into a phrase table. This, however, does not work well. Target word order and even whole target sentences are often incorrect. To solve this problem, the proposed method uses high-frequency word in the training corpus. The high-frequency words may already be trained well, in other words, may appear in the phrase table and therefore be translated with correct word order. Experimental results show the proposed method as far superior to simply attaching dictionary entries into phrase tables.  Introduction Statistical Machine Translation (SMT) systems are built solely based on a large parallel corpus. The performance of SMT has improved by using “phrase” units for translation, rather that “word” units. With phrase-based SMT (Koehn et al., 2003), the term “phrase” is used to mean a sequence of words, as opposed to a linguistical phrase. Within the phrase, the selection of target words and the order of target words are learned in advance and left unchanged during the translation process. SMT systems are trained using only a parallel corpus. SMT cannot translate unknown words, words that do not appear in the parallel corpus. Furthermore, unknown words often destroy the entire translation, resulting in a sequence of scattered words that is beyond comprehension. There are many translation dictionaries available and they are a more common resource than parallel corpora. There are general dictionaries, such as EIJIRO1 and EDICT2. Additionally, there are dictionaries with more technical/specialized terminology - dictionaries for patent, engineering, medical, legal, sport, entertainment, et cetera, such as LSD3. These are considered dependable because they have evolved not by using a fully automated process (which is exploited in the SMT paradigm), but by long-term human eﬀort. It is reasonable to incorporate these valuable resources into SMT. We cannot expect a parallel corpus to include all necessary words. Even the publicly available and largest parallel corpus for the NIST MT competition (consisting 1http://www.alc.co.jp/ 2http://csse.monash.edu.au/ jwb/j edict.html 3http://lsd.pharm.kyoto-u.ac.jp/en/  of 8 million Chinese and English sentences) does not include many names of places and people. A mechanism is needed for handling words unseen in the parallel corpus. This paper puts focus on proper nouns because they are typically not found in the training corpus. Phrase-based SMT Here, we explain phrase-based SMT, which is now regarded as the de facto standard. Before we explain phrase-based SMT, however, we shall brieﬂy describe word-based SMT (Brown et al., 1993). This system is based on the noisy channel model. According to Bayes’ law, the translation probability for translating source sentence f into target sentence e is represented as argmaxep(e|f ) = argmaxep(f |e) × p(e) (1) p(f |e) represents the translation model and p(e) represents the language model. Whereas word-based SMT bases translations word-by-word, phrase-based SMT is based on a phrase-by-phrase translation model. In phrase-based SMT as discussed in this paper, the right-hand side of equation 1 is as follows: argmaxepϕ(f |e)×pLM (e)×pD(e, f )×ωlength(e) (2) where pϕ(f |e) is a phrase translation model, pLM (e) is a language model, pD(e, f ) is a distortion model and ωlength(e) is a word penalty. These are weighted. The translation process of phrase-based SMT is as follows: 1. Segment the source sentence into phrases  2. Translate the source phrases in any order stochastically 3. Adjust the position of the target phrases stochastically Figure 1 shows phrase-based SMT with source phrases translated into target phrases. In each phrase, the word order is correctly maintained. All of the phrases in the translation process appear in a phrase table. The phrase table is a translation model for phrase-based SMT and consists of source language phrases and corresponding target language phrases and these probabilities. Figure 2 shows an example of a phrase table. Figure 1: Phrase-based SMT Baseline Method A simple and low-cost method to introduce a translation dictionary into phrase-based SMT is to add parallel word pairs in the translation dictionary to the phrase table with appropriate probabilities. This method, however, does not work well. Although words are translated correctly, positions of translated words are not always correct. This is very serious, especially for language pairs in which word order is very diﬀerent, such as Japanese and English. Figure 3 shows how this method works in a case where the source sentence is the same as that used in ﬁgure 1, but the Japanese word “χϡʔϤʔ Ϋ/nyuyoku (new york)” is replaced by the untrained Japanese word “ΧʔσΟϑ/kadifu (cardiﬀ)” (a local place name in Wales, UK). In this example, the Japanese word “ΧʔσΟϑ /kadifu” is translated to the English word “cardiﬀ” correctly, but its position and the entire sentence become incorrect. This is because the Japanese word “ΧʔσΟϑ/kadifu” is not included in any source  Figure 3: Translation example of the baseline method phrases of the phrase table apart from the one we just added, even though constraint of word order in phrase-based SMT deeply depends on the phrase itself. The language model which controls word order also does not include the English word “cardiﬀ” and so cannot decide the word position correctly. Proposed method Basic idea Figure 4 shows the process of the proposed method by example. Instead of using the baseline method which has the above problem, we propose a method which uses the high-frequency words in the training corpus. Prior to the translation, the untrained words in source sentences are replaced with high-frequency words in the training corpus. The target sentences are then acquired by translating the modiﬁed source sentences. Finally, the high-frequency words in the target sentences are replaced with target words for the untrained words. The reason why we use highfrequency words is that they may already be trained well, in other words, the high-frequency words may already appear frequently in phrase tables and therefore provide ample statistics. It is also important that the high-frequency word be of the same category as the untrained word. By using high-frequency words of the same category, the contexts of both the untrained words and the high-frequency words are usually the same. The modiﬁed source sentence with the high-frequency word is then translated as the original source sentence. Using ﬁgure 4 , we describe the process step by step. First, the untrained word “ΧʔσΟϑ/kadifu” is replaced in the  Figure 2: A example of the phrase table for Japanese to English translation  Because p(wOOV )/p(wfreq) is a constant, the language model scores of both p(wOOV |wi−2, wi−1) and p(wfreq|wi−2, wi−1) are same up to constant factors. Likewise, from equation 4,  wOOV , p(wi+1, wi+2) = wfreq, p(wi+1, wi+2)  p(wOOV )  p(wf req )  Figure 4: Translation process of the proposed method source sentence with the high-frequency and welltrained word, “χϡʔϤʔΫ/nyuyoku”. Both “Χʔ σΟϑ/kadifu” and “χϡʔϤʔΫ/nyuyoku” are of the same category “place-name”. Next, the entire modiﬁed source sentence is translated and the target sentence is acquired. Next, the target sentence is searched for the translated word “new york” . Finally, the high-frequency word “new york” is replaced with “cardiﬀ”. Formal explanation Using high-frequency words assumes that the untrained word and the high-frequency word will have the same context once replaced, as both the previous word and the following word remain the same. So, in the case of a language model, this is a trigram: p(wi−2, wi−1|wOOV ) = p(wi−2, wi−1|wfreq) (3)  p(wi+1, wi+2|wOOV ) = p(wi+1, wi+2|wfreq) (4)  where wk are words in context, wOOV is the untrained word and wfreq is the high-frequency word. From equation 3,  p(wi−2, wi−1, wOOV ) = p(wi−2, wi−1, wfreq)  p(wOOV )  p(wf req )  
 Ralf D. Brown  Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 aphillips@cmu.edu  Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 violetta@cs.cmu.edu  Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 ralf@cs.cmu.edu  Abstract Example Based Machine Translation (EBMT) is limited by the quantity and scope of its training data. Even with a reasonably large corpus, we will not have examples that cover everything we want to translate. This problem is especially severe in Arabic due to its rich morphology. We demonstrate a novel method that exploits the regular nature of Arabic morphology to increase the quality and coverage of machine translation. Through the use of generalization and rewrite rules, we are able to recover the English translation of phrases that do not exist in the training corpora. Furthermore, this system shows improvement in BLEU even with a training corpus of 1.4 million sentence pairs.  Introduction As the world grows more interconnected, the need for translating other languages into English becomes more pervasive. In particular, the current stage of world affairs has cast a spotlight on Arabic. As a result, in the last few years Arabic has become the major focus of many machine translation projects. This focus on Arabic has resulted in many rich resources available for the language. We now have a GigaWord Arabic corpus, several million sentence pairs of bilingual text, and a handful of morphological analyzers. However, the presence of these tools does not mean that we have solved the problem of integrating them together and building an effective translation platform. One of the key features of the Arabic language, in sharp contrast to English, is its rich morphology. At the core of an Arabic word is a root -- a sequence of three consonants (more rarely four) -- that indicates a general concept or class of words. For example, the triconsonantal root k-t-b refers to writing and is used in the words that are translated in English as ‘writing’, ‘book’, ‘letter’, ‘library’, ‘school’, ‘typewriter’, and ‘dictation’. Words are formed by combining the root with a different vowel pattern to form a stem.1 Thus, kAtib ( ِ ‫ )آ‬is ‘writer’ and kitAb (‫ ) ِآ ب‬is ‘book’.2 Analysis of these stems is complicated by the fact that most vowels are omitted in normal writing. Arabic words must also be conjugated, so in addition to the vowel pattern, affixes are attached to represent information such as number, person, or gender. On the other hand, English has very little morphology and one English word will often have several different representations in Arabic. Data-driven machine translation is typically limited by the quantity and scope of its training data. This is problematic in Arabic due to its complex morphology. If we consider every combination of a stem and morphological affixes to be a separate word, the vocabulary of our training corpus 
†Microsoft Research India 196/36 2nd Main Sadashivnagar, Banglaore 560 080 India {chrisq,raghavu,arulm}@microsoft.com Abstract The development of broad domain statistical machine translation systems is gated by the availability of parallel data. A promising strategy for mitigating data scarcity is to mine parallel data from comparable corpora. Although comparable corpora seldom contain parallel sentences, they often contain parallel words or phrases. Recent fragment extraction approaches have shown that including parallel fragments in SMT training data can signiﬁcantly improve translation quality. We describe efﬁcient and effective generative models for extracting fragments, and demonstrate that these algorithms produce competitive improvements on cross-domain test data without suffering in-domain degradation even at very large scale.  1. Introduction Statistical Machine Translation (SMT) systems are most inﬂuenced by two key components: language models and channel models. Language modeling still must overcome several obstacles (e.g. n-gram models are brittle especially with respect to morphology) but luckily data acquisition is not one of them – between large LDC corpora and the easy availability of web data, gigantic amounts of monolingual data are available. For channel modeling, however, the situation is much less promising. Most approaches require parallel data for training channel models, and derive continuing returns from larger datasets. Yet there are few large parallel corpora currently available. Even the largest (Arabic-English and Chinese-English) are orders of magnitude smaller than the available monolingual training data. These data sources also tend to be drawn from a single domain, and SMT systems trained on one domain suffer signiﬁcant quality degradation when tested in other domains. If we hope to improve translation quality within a language pair and domain, expand to new domains, or acquire new language pairs, we must ﬁnd ways to exploit non-parallel data sources. 1.1. Related work There are many ways that we can identify and exploit comparable data. Finding comparable documents is a useful way point in this difﬁcult task: we can signiﬁcantly reduce the search space of further steps in the pipeline if we limit our attention to information in similar documents. Document pairs can be found from the web by exploiting URL structure, document structure, and lexical similarity amongst other clues (see for instance Resnik and Smith (2003), Zhang et al. (2006), Shi et al. (2006)). Alternatively we can search within large newswire corpora, which  can be a rich source of translation information. Crosslingual information retrieval techniques can ﬁnd promising document pairs from large newswire corpora in different languages, as in (Zhao and Vogel, 2002). Although the web data is likely to be larger and more diverse, it presents obstacles to controlled experimentation (the web is constantly changing) and is seldom as carefully edited as newswire data. Therefore we focus on the latter source, but we expect that the techniques developed here should also apply to other sources of comparable data. We loosely use the term “comparable” to describe the document pairs that can be extracted from these newswire sources, though the pairs differ signiﬁcantly in translational equivalence. Occasionally the articles contain sentence-forsentence translations of one another; there have been several efforts to search for whole-sentence translation pairs within comparable corpora (e.g., Zhao and Vogel (2002), Fung and Cheung (2004b), Fung and Cheung (2004a), Cheung and Fung (2004)). More often it appears that either two reporters have witnessed the same events and written similar accounts or perhaps one reporter has read another reporter’s account and subsequently written a new text with some common information. The latter articles contain few sentence-for-sentence translation pairs. Many researchers have instead tried to gather a bilingual lexicon from these sources, which could then be used by an MT system or even human translators (Fung and Yee (1998), Rapp (1999), Diab and Finch (2000), Koehn and Knight (1999), Gaussier et al. (2004), Shao and Ng (2004)). However comparable corpora contain multi-word translation information that is overlooked by these methods. For instance, quoted material from primary sources is often translated literally, as are person names, institution names, and other named entities. We believe that one of the most promising ideas is to  identify parallel sub-sentential fragments within comparable corpora, as proposed by Munteanu and Marcu (2006). Starting with a non-parallel corpus consisting of news articles from three sources (the BBC, the Romanian newspapers ‘Evenimentul Zilei’ and ‘Ziua’) they ﬁrst produce a set of similar article pairs using a cross-lingual information retrieval system. Restricting their attention to sentence pairs that contain at least minimal lexical overlap, they search for parallel fragments using an approach inspired by signal processing. Using a set of parameters derived from LLR scores, they annotate each word with a value between −1 and 1 indicating the likelihood that this word has some translational equivalent in the other sentence by performing a greedy alignment. (Note that this step is performed without regard for position in the sentence, number of words generated by a single word, etc.) This stream of values is then treated as a signal and passed through a moving average ﬁlter. To ﬁnd fragments—substrings of the original sentence that are likely to have a translation pair in the other side, they identify the longest spans that have only positive signal values. All fragments longer than some threshold (3 words) are concatenated to form a subsequence of the sentence that is likely to have a translation on the other side. The same process is repeated on the other sentence, and the resulting fragment pair is assumed to be parallel. They report substantial positive effect on BLEU scores when mined fragments are appended to a baseline parallel corpus. We believe that there are several ways to improve this approach. Since the greedy alignment is performed independently in each direction, there is no guarantee that the words aligned to a fragment in one sentence will appear in the fragment from the other sentence. Nor is the number of spans guaranteed to match; the resulting spans are simply concatenated, which could produce odd phrases spanning fragment boundaries. The method does not model phenomena that have proven very important in the related task of word alignment, such as locality and fertility. Finally, the structure of the model is somewhat heuristic and thus difﬁcult to optimize or chain in a pipelined process. 1.2. Our approach We are primarily interested in the problem of extracting parallel fragments, particularly in developing theoreticallygrounded, effective models. We present two algorithms for mining parallel fragments from similar sentence pairs, both based on generative models of semi-parallel data. First we train translational equivalence models from parallel data and monolingual generation models from source and target language data. Next, given two news wire corpora, we identify promising sentence pairs using methods very similar to those used in Munteanu and Marcu (2006). Our main innovation comes in identifying the parallel fragments from these comparable sources. In section 2., we describe two new models for extracting parallel fragments, and provide algorithms for effectively using them. We describe the experimental setup and empirical ﬁndings in section 3.. In section 4. we discuss our results and we propose some ideas for future exploration.  2. Generative models of fragment alignment In most prior work (e.g. Brown et al. (1993), Vogel et al. (1996)), generative models are used to approximate the translation process. Given a sentence in one language (arbitrarily designed the source, denoted s = sm 1 ), we can ﬁnd a probability distribution over sentences in the other language (designated target, denoted t = tn1 ). While these models do allow for a certain degree of deviation between sentences, the deviations are assumed to be systematic (e.g. the Spanish word de must often be inserted when generating based on an English string). In noisy comparable sentences, the situation is markedly different: words may be inserted or deleted seemingly at random depending on what information each sentence happened to include. We describe two models to handle these phenomena: a conditional model of loose rather than exact translation, and a joint model of simultaneous generation. 2.1. Model A: Conditional generation In the case of noisy translation, we assume that source language sentence has already been written, and the target language sentence is generated conditionally based on that sentence. Unlike standard word alignment models, however, we also allow words to be generated completely independently of the source language, based on prior target language words only. The intuition is as follows: say we already have a monolingual generation model (e.g., an ngram language model), and a model of translational equivalence (e.g., an HMM word alignment model). We hypothesize that parallel fragments are more likely to be produced by translating the source sentence rather than a monolingual model: Pr(t|s) > Pr(t). Before describing the particulars of this model, we review some standard word alignment models. The generative framework behind IBM Models 1, 2, and the HMM model produces target language sentences in left-to-right order in the following manner. First the target sentence length is drawn according to an unspeciﬁed distribution – this detail is not important for the word alignment case. Next, for each target position, the position of the source word that generated this word is picked. Then the target word in that position is drawn according to the source word that generated that position. Let sm 1 be the source sentence, tn1 be the target sentence, and an1 ∈ {0..m}n be the hidden state denoting the position of the source word generating each target word; aj = 0 indicates that tj was generated from the null word. Then we can model a sentence and an alignment as follows: Pr (an1 , tn1 |sm 1 ) = Pr(|t| = n)· n Pr(aj |aj1−1, tj1−1, sm 1 ) · j=1 Pr(tj |aj1, t1j−1, sm 1 ) All three models draw Pr(tj|aj1, tj1−1, sm 1 ) from e(tj|saj ), a multinomial distribution conditioned on saj . By drawing aj from a uniform distribution, from a multinomial distribution based on j, or a multinomial distribution based on aj−1, we produce IBM Model 1, Model 2, and the HMM model respectively.  “ El fraude es lo normal en la República Dominicana en la actual situación , ” afirmo Bosch  “ Fraud is normal in the Dominican Republic , ” he said to reporters GENERATE  Figure 1: An example pair with a Model A alignment, demonstrating generation of the Spanish sentence given the English sentence. Each column contains a single box indicating the hidden state generating each Spanish word. In all but the last row, empty boxes represent null alignments and solid boxes represent actual alignments. The last row represents the hidden state indicating that the Spanish word was generated monolingually, independently of the English side.  To handle comparable data alignment, Model A augments the hidden space to include an additional state signifying that the current word is generated monolingually, without any corresponding source word. Note that this situation is distinct from the null word case: rather than drawing tj according to source language information, we draw tj according to previously generated target material. Let aj = −1 indicate the monolingual generation state. Model A has the following structure for generating target words:  Pr(tj |aj1, tj1−1, sm 1 ) =  e(tj |tj1−1) e(tj |saj )  if aj = −1 otherwise  and has a ﬁrst-order dependence in the hidden states:  Pr(aj |aj1−1, tj1−1, sm 1 ) = d(aj |aj−1)  Parameter estimation. Model A parameters are deﬁned in terms of standard HMM word alignment parameters and n-gram language model parameters. We begin by estimating the HMM parameters from a parallel corpus using EM; let the emission parameters be denoted with e (t|s) and the transition parameters be denoted d (a|a ). In addition, we estimate a target language model based on monolingual data; the parameters be deﬁned as: e (tj|tj1−1). Finally we are given free parameters ϕ indicating the probability of transitioning between bilingual BI and monolingual MO states.  Model A has the same generative structure as the IBM models 1 and 2 and the HMM model, so it sufﬁces to deﬁne how states and words are drawn. Transitioning between states is a straightforward mixture of ϕ and the HMM state model d , except that when jumping into a bilingual state, we consider all starting points equally likely:  Pr(aj |aj1−1, tj1−1, sm 1 ) =  ϕ(BI|BI)d   (aj |aj1−1)    ϕ(BI|MO)1/m  ϕ(MO|BI)   ϕ(MO|MO)  if aj−1 = −1, aj = −1 if aj−1 = −1, aj = −1 if aj−1 = −1, aj = −1 if aj−1 = −1, aj = −1  When generating words, we switch between monolingual and bilingual generation models according to the current hidden state.  Pr(tj |aj1, tj1−1, sm 1 ) =  e (tj |tj1−1) e (tj |saj )  if aj = −1 otherwise  Fragment extraction. To ﬁnd parallel fragments given a similar sentence pair, we ﬁrst ﬁnd the most likely hidden structure according to Model A. Since Model A has an HMM like structure, the familiar Viterbi algorithm will ﬁnd the most likely alignment aˆ = arg maxa{Pr(t, a|s)}. Next we consider each maximal bilingual span (k, l); that is, for all k ∈ k..l, ak = −1, and ak−1 = al+1 = −1.  Let i and j be the minimal and maximal non-zero values taken on by aˆ between k and l. Then we consider sji and tlk to be a bilingual fragment pair if the following conditions hold: 1. Both fragments are at least the minimum length (currently 3). 2. The fraction of “holes” (unaligned words) in either span does not exceed a given threshold (30%). 3. The fraction of stop-words in either span does not exceed a given threshold (70%). This model is attractive given that it is sound, relatively easy to implement, and quick to evaluate. However there are several aspects of the model which we might hope to improve. First, there are many free parameters to be tuned, including the transition probabilities ϕ and the threshold values. Second, the asymmetry of the model means that it does not forbid the same source fragment from generating multiple potentially overlapping fragments, nor does it evaluate the likelihood of the segmentation of the source side. 2.2. Model B: Joint generation To address some of these limitations, we explore a joint model based on a slightly different generative decomposition. Rather than conditioning on one of the sentences, we generate the pair jointly. We imagine a process that chooses between three options: generate a source-only fragment, generate a target-only fragment, or generate a bilingual fragment in tandem. To further simplify the story, we can assume that the fragments are again generated left-to-right in both the source and target sentences. Although this assumption could potentially screen out non-monotone fragments, it simpliﬁes the model structure and search and is probably sufﬁcient for language pairs with similar word order, such as English-Spanish. The intuition behind Model B is similar to that of Model A, though phrased in joint rather than conditional probabilities. We hypothesize that the probability of generating source and target language fragments s and t jointly should be more likely than generating them independently (i.e. Pr(s, t) > Pr(s) · Pr(t)) if and only if they are parallel. However few joint models of translation have proven effective in practice; conditional models have proven more effective empirically on most MT tasks. We can use Bayes’ rule to combine a marginal probability and a conditional probability to estimate a joint probability: Pr(s, t) = Pr(s)Pr(t|s) = Pr(t)|Pr(t|s), though each direction is likely to give a different estimate of this joint probability. To optimize the precision of the extracted fragments, we use the minimum of either decomposition as the estimate of the joint probability.1 In this model, the hidden structure is a series of fragments f . Each fragment fi is a 2 tuple where fi,1 and fi,2 indicate the last source and target words covered by 1This is equivalent to saying that a fraction is considered parallel iff Pr(s|t) > Pr(s) and Pr(t|s) > Pr(t).  fragment fi. The generative framework takes the following form:  Pr(f1p, sm 1 , tn1 ) = Pr(|f | = p)·  p  Pr(fi  |f1i−1  ,  sfi−1,1 
 Machine translation of spoken language has made significant progress in recent years, however, translation quality is still limited due  to specific idiosyncrasies of spoken language; including the lack of well-formed sentences and the presence of disfluencies. In this  paper, we investigate the effect of disfluencies on Statistical Machine Translation (SMT) and introduce an Automatic Disfluency  Removal scheme as a pre-processing step prior to translation. On Broadcast Conversation (BC) transcripts the proposed approach  demonstrates that up to 8% relative improvement in BLEU can be obtained via Automatic Disfluency Removal. Furthermore, we show  that the detrimental effect of disfluencies on SMT differs across disfluency types.  1. Introduction  However, with the exception of results in (Harper et. al  2005) where it is shown that disfluency removal aids  Disfluencies are generally defined as “phenomena that parsing of conversational speech, most of the work in  interrupt the flow of speech and do not add propositional disfluency removal has so far been motivated by rich  content to an utterance”. (Fox Tree, 1995). In other words, transcription tasks with a view to generate well-formed  disfluencies are those parts of spontaneous speech which sentences with improved readability. While it has been  when removed yield sentences that were originally argued that disfluency removal might help downstream  intended. These sentences are shorter and less ill-formed. NLP tasks such as question answering and Spoken  Although several different types of disfluencies have Language Translation (SLT), to the best of our knowledge  been defined (Shriberg, 1994), in this paper we deal with there has been no quantitative study establishing the  3 types of disfluencies.  claim.  Presence of disfluencies can hurt translation quality in  ● Fillers – These are words that the speaker uses to two different ways. Firstly, disfluencies such as fillers and  control the conversation indicating that she/he repetitions make utterances longer without adding  intends to continue with the utterance. These may semantic information. On the other hand, corrections add  also indicate hesitation on the part of the speaker. spurious content and therefore produce inaccurate  translations. Secondly, since SLT systems are trained on  ● b) Repetitions - Speaker repeats some part of the large amounts of text data that have well-formed  utterance  sentences, disfluencies cause a mismatch between training  and evaluation data and can result in poor translation. In  ● c) Corrections – Speaker modifies the utterance this paper, we demonstrate the adverse effect of  mid-way while generally maintaining original disfluencies on SLT and show how automatic DFR can  syntax.  improve SLT performance.  In the past decade, speech disfluencies have become a subject of inter-disciplinary research. While linguists have focussed on aspects such as role of disfluencies in discourse and speech comprehension, researchers in human language technologies have worked on automatically removing disfluencies from conversational speech. Several approaches for automatic Disfluency Removal (DFR) have been proposed. A decision tree based classifier with a combination of prosodic features such as pitch, energy and fundamental frequency and lexical features has been shown to be successful for disfluency removal. (Liu et. al, 2006). Honal and Schultz use a noisy-channel approach where disfluency removal is modelled as the translation of disfluent speech to clean speech (Honal and Schultz, 2003). There has also been some work where deeper syntactic knowledge used within the noisy-channel model. Johnson and Charniak use a Tree Adjoining Grammar (TAG) to represent cross-serial dependencies between the reparandum and correction regions of disfluencies. (Johnson and Charniak, 2004).  2 Disfluency Removal System The end-to-end pipeline proposed for SLT tasks is shown in Fig 1. S1 and S2 denote 2 scenarios that we explore in this paper - namely translation of broadcast conversation transcripts and ASR first-best output. A detailed description of the ASR and SMT systems is provided in the next section. Fig. 1 : The end-to-end spoken language translation system We use the CMU Disfluency Removal system (Honal and Schultz, 2003). This system is based on the noisy-channel  model widely used in ASR and Statistical Machine Translation (SMT) systems. Here the problem of disfluency removal is viewed as a translation task where source language is the disfluent speech and the target language is non-disfluent speech. However, unlike in SMT systems, translation during DF removal involves only deciding which source words are to be deleted. No word insertions or reordering is permitted. Five translation models are used capturing features such as a) position of disfluency in a sentence b) position of the word within the current disfluent region c) words in the context of a disfluency d) distance from a fragment and e) information about previous deletions in the context of a disfluency. These models are combined log-linearly using weights learnt via gradient descent. (Honal and Schultz, 2005)  3 Data and System Description The DFR system is trained on 46300 words (19 Al Jazeera shows) of Arabic BC) transcripts that were annotated for disfluencies by a native Arabic speaker. In addition, 2 shows designated as the development set were used to estimate weights for 5 disfluency models using gradient descent. Table 1 shows relevant corpus characteristics for training and evaluation data. Dataset Sentences Words Disfluencies (%)  Training  6370  46300  6.50  BCAD05  691  10570  3.48  GALE06  256  4480  5.86  
This paper proposes a method to improve word alignment by combining various clues. Our method first trains a baseline statistical IBM word alignment model. Then we improve it with various clues, which are mainly based on features such as lemmatization, translation dictionary, named entities, and chunks. We incorporate these features into an unified framework. Experimental results show that our method improves word alignment quality by achieving a relative error rate reduction of 39.8%. We also conduct phrase-based machine translation based on the word alignment results. Using BLEU as an evaluation metric, our method achieves an absolute improvement of about 0.02 (about 18% relative) over a baseline method.  Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005) or association measures (Smadja et al., 1996; Ker and Chang, 1997; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links. One of the main problems in the existing methods is the null alignment. In two different languages, some words in one language have no counterparts in the other. And such information is not available in bilingual dictionaries. In order to solve this problem, the basic IBM model (Brown et al., 1993) trained a probability for all null alignments without consideration of each individual word, which made the accuracy of the null alignment relatively low. Wu and Wang (2004) showed that the accuracy of word alignment decreased if the null alignment was considered in evaluation. Moore (2004) tried to improve null alignment by re-weighting it in the Expectation Maximization (EM) training procedure, but failed to improve word alignment results. The second problem is the alignment of multi-word units such as phrasal compounds, idiomatic expressions, and complex terms. It is difficult to align them because it depends on the context. Wu and Wang (2004) showed that the accuracy of multi-word alignments is much lower than that of single-word alignments. Previous methods improved multi-word alignment by using either iterative procedures (Smadja et al., 1996; Melamed, 1997) or preprocessing steps for the identification of token Ngrams (Ahrenberg et al., 1998; Tiedemann, 1999). Wu and Wang (2004) used a rule-based translation system to identify and disambiguate the multi-word units and improved the multi-word alignment results. Tiedemann (2003) used chunks and n-grams. The third issue is how to make use of more linguistic information. The basic statistical word alignment method works on the word level of the plain text. In recent years, some discriminative methods are proposed to integrate various syntactic and lexical clues into the alignment models to improve alignment quality (Liu et al., 2005; Moore et al., 2006; Blunsom and Cohn, 2006; Taskar et  al., 2005). In these methods, part-of-speech (POS), association measure between bilingual words, and translation dictionaries are usually used. More linguistic information, such as named entity and chunk information, may be useful for word alignment. In this paper, we propose an unified method to address all of the three problems mentioned above. The method first trains a weight for each null alignment to improve word alignment. Secondly, we use dictionaries, including human crafted translation dictionaries and automatically trained dictionaries, to improve both precision and recall of word alignment. Finally, we use linguistic tools, including named entity recognizers and chunkers, to improve multi-word alignment. Using all of these clues, we propose a method to combine them to improve word alignment. Experimental results show that our null alignment model can achieve an error rate reduction of 12.35% as compared with the baseline. And the dictionaries and linguistic features such as named entities and chunks can further improve the word alignment by achieving an error rate reduction of 39.8%. We also apply the aligned corpus for phrase-based statistical machine translation. Using BLEU as an evaluation metric, our method improves translation quality by achieving an absolute improvement of about 0.02 (18% relative) over the baseline method. The remainder of this paper is organized as follows. The next section describes our method combining multiple clues to improve word alignment. And then we will show the experimental results on both word alignment and statistical machine translation. After that, we will compare our methods to some related work. In the last section, we will conclude this paper and present the future work. Methodology Och and Ney (2003) proved that the statistical word alignment models proposed by (Brown et al., 1993) outperform the heuristic methods based on the association measures. However, the statistical models still have some deficiencies. For example, the models use simple methods to handle null alignments, and cannot handle multi-word alignment and do not take context into account. In this section, we use IBM model 4 (Brown et al., 1993) as a baseline, and use various clues to improve word alignment quality.  Definition For convenience, we use the following definitions in this paper. z f represents a source language sentence f1, f 2 ,..., f m . z e represents a target language sentence e1, e2 ,..., en . z The link (ei , f j ) represents that ei is aligned to f j . z Af->e is defined as the alignment set in the source to target direction produced by the IBM model 4. z Ae->f is defined as the alignment set in the target to source direction produced by the IBM model 4. z A∩ is the intersection set of Ae->f and Af->e . z A∪ is the union set of Ae->f and A f->e .  Morphological Analysis The IBM models need bilingual corpus for training. Since a large bilingual corpus is not always available, it is subject to the problem of data sparseness. One possible way to solve this problem is to perform morphological analysis on the bilingual corpus. In this paper, we use the lemmatized form of English words in the bilingual corpus to perform statistical word alignment. For example, the lemmatized form of "verified" is "verify". Based on the alignment results, we assign a weight to each alignment link as follows.  W0 (ei ,  fj)=  p(ei  |  fj)+ 2  p( f j  | ei )  (1)  Where p(x | y) describes the translation probability obtained from the alignment results Ae->f and Af->e trained with the lemmatized corpus.  Null Alignment Model Some words in one language have no counterparts in the other. And such kind of information is not available in translation dictionaries. Although Brown et al. (1993) train a probability for all the null alignments, it does not condition the null translation probability on individual words. In order to solve this problem, we estimate the confidence score for each null alignment link. The score is based on an association measure (Taskar et al., 2005), namely Dice coefficient, which is shown in Equations (2) and (3).  W1(e, NULL)  ∑ ∑ =  2 * count(e, NULL)  count(e, f ') + count(e', NULL)  (2)  f'  e'  W1(NULL, f )  ∑ ∑ =  2 * count(NULL, f )  count(NULL, f ') + count(e', f )  (3)  f'  e'  Where count(e, f ) is the occurring frequency of the word alignment link (e, f ) in the union alignment set A∪ .  Translation Dictionary  Handcraft Dictionary For some language pairs, there exist handcraft translation dictionaries of high quality. In order to improve alignment accuracy, we use these dictionaries as a clue in this paper. For each entry in the dictionary, we also assign a weight for it, which is shown in (4).  W2 (e, f )  =  2  (4)  | { f '| (e, f ' ) ∈ HD} | + | {e'| (e', f ) ∈ HD} |  Where |{ f '| (e, f ') ∈ HD}| and |{e'| (e', f ) ∈ HD} | describe the number of alternative translations in the handcraft dictionary HD for the word e and f , respectively.  Automatically Trained Dictionary Although the handcraft translation dictionary has high quality translation, it cannot cover word or phrase translations in all kinds of specific domains. Thus, we also automatically train a translation dictionary from the alignment results obtained with IBM model 4. To build the translation dictionary, we first get the intersection set A∩ . Then the alignment links in A∩ are extended by iteratively adding word alignment links from A∪ into it as described in (Och and Ney, 2003). Finally, to filter some noise caused by the error alignment links, we only retain those translation pairs whose translation probabilities are above a threshold or co-occurring frequencies are above a threshold. We estimate a weight for each entry in this dictionary using the same method as described in Equation (2) or (3), which is rewritten as shown in (5).  W3 (e, f )  ∑ ∑ =  2 * count(e, f )  count(e, f ') + count(e', f )  (5)  f'  e'  Named Entity It is difficult to align named entities because of the following reasons. First, they often consist of several words, forming multi-word units. Second, it is difficult for existing dictionaries to contain all of them because they are dynamic words, which results in out-of-vocabulary (OOV) problem. Third, most of the named entities do not frequently occur in the corpus, which results in data sparseness problem. Fortunately, for some languages, named entities reorganization tools are available. Meulder and Daelemans (2003) showed that their method can achieve a precision of 88.99% and a recall of 88.54% on English named entity recognition. Sun et al. (2002) showed that recognizers can achieve a precision of 82.28% and a recall of 85.53% on Chinese named entity recognition. In this paper, we use available tools to recognize the named entities in the source language and the target language. The types of the named entities include time, data, number, person names, organization, and locations.  Two named entities N f = f j , f j+1,..., f j+t and Ne = ei , ei+1,..., ei+s are consistent if Ne and Nf belong to the same named entity type (such as persons) and one of the following conditions is satisfied: z There is no other named entity in the sentence pair having the same type as that of Ne and Nf . z For any non-null alignment link (ex , f y ) in the alignment set A 1 , i ≤ x ≤ i + s if and only if j≤ y≤ j+t. Given e ∈ Ne , f ∈ Nf , we assign the weight for the link (e, f ) as follows:  W4 (e, f )  ⎧1 = ⎩⎨0  if Ne and Nf are consistent Otherwise  (6)  Chunk  In this paper, chunks are defined as phrases where  syntactically related words become members of the same  phrase. For some languages, shallow parsers are available  to identify chunks with high precision. For example,  Koeling (2000) showed that their method can achieve a  precision of 93.45% and a recall of 93.51% on English  chunking.  In this paper, we only use base chunks of the source  language. If we use shallow parsers to identify the chunks  in both source language and target language, it is difficult  to align them because of structure divergence (Al-  Adhaileh, 2002). Thus, we only use a shallow parser in  one language (for examples, source language) to identify  the chunk, and then obtain the corresponding chunk in  another language based on the word alignments.  Two  chunks  Cf = f j , f j+1,..., f j+t  and  Ce = ei , ei+1,..., ei+s are consistent if the following condition is satisfied: z For any non-null alignment link (ex , f y ) in the alignment set A 2 , i ≤ x ≤ i + s if and only if j≤ y≤ j+t  Given e ∈ Ce , f ∈Cf , we assign the weight for the link (e, f ) as follows:  W5 (e, f )  =  ⎧1 ⎨  if Ce and Cf are consistent  (7)  ⎩0 Otherwise  Combination of Clues With all of the above clues, we combine them to improve word alignment quality. First, we create a word alignment matrix for the bilingual sentence pair (e, f ) as shown in Table 1.  f 0  f 1  f 2  …  fm  e0 --  c01  c 02  … c0m  e1 c10  c11  c12  …  c1m  e2 c20  c  21  c 22  …  c2m  …… … … … …  en cn0  cn1  cn2  …  cnm  Table 1. The Alignment Matrix In Table 1, cij describes the alignment association strength of ei and f j . It can be estimated as described in (8).  5  ∑ cij = Wk (ei, f j )  (8)  k =0  Where Wk (ei , f j ) is the corresponding weight described in the above subsections. With the alignment matrix, we use a best-first strategy to add the word alignment links. We first select the link with the highest score cij to the final word alignment set A , and then select the link with the second high score, and so on. The procedure is repeated until no alignment link can be added whose alignment score is above a fixed threshold3.  Experimental Results on Word Alignment  Data In this section, we take English-Chinese word alignment as a case study. The English-Chinese bilingual training data is provided by Chinese Linguistic Data Consortium (CLDC)4. The catalog number is CLDC-LAC-2003-004. It contains about 150,000 sentence pairs, with about 3 million English words and about 5 million Chinese characters. The development set and the test set for word alignment are from the corpora distributed for the 2005 HTRDP evaluation of machine translation 5 . It can also be obtained from CLDC (catalog number 2005-863-001). The test set contains 505 sentence pairs, with 6,866 sure links and 4,106 possible links in the reference word alignment set.  
1. Introduction Translation lexicons play a vital role in several applications related to machine translation (MT). Such lexicons are used for cross-language information search (Reiter et al., 2007; Gey et al., 2006; Hull and Grefenstette, 1996), for Web-based word translation tools (e.g. Google WordTranslator), and for knowledge-based MT systems (Bond et al., 2005; Carbonell et al., 2006). To be useful for these tasks, a translation lexicon must distinguish which translations are appropriate to which word sense. Such lexicons exist for only a few language pairs with adequate coverage; creating them manually is a major knowledge acquisition bottleneck. A few multilingual lexical resources are under construction, such as EuroWordNet (Vossen, 1998) and the Wiktionary project (www.wiktionary.org), but scaling up these resources beyond a small number of languages is a laborious process. On the other hand, some lexical resources are becoming more and more prevalent – vast amounts of monolingual text and bilingual machine readable dictionaries (MRDs) that do not distinguish word senses and often have spotty coverage. We present the PanLexicon system that takes advantage of these plentiful resources to create a word-sense-  distinguished multilingual lexicon in a fully automatic fashion. PanLexicon utilizes a combination of bilingual MRDs to find translation sets, where each translation set has one or more words in each of k languages that all represent the same word sense. PanLexicon‟s source dictionaries alone are insufficient for maintaining word senses. They provide a set of translations in languages L2, …Lk from a word in language L1. There is no guarantee that a given translation in L2 corresponds to a translation in L3, since they may not each share the same word sense with the word from L1. PanLexicon maintains the same word sense across languages by finding word usage contexts from monolingual corpora and computing similarity of contexts across languages as described in Section 3.2. These contexts also serve the place of glosses in the lexicon, giving an example of the intended meaning to either a human reader or an automatic word sense disambiguation (WSD) tool. We make the following contributions in this paper:  We present PanLexicon, a scalable automated mechanism to build word-sense-distinguished multilingual lexicons from bilingual MRDs and monolingual corpora.  We show that PanLexicon is scalable to a large number of languages with time and space linear in the number of languages and corpus size. Additional languages require only a bilingual lexicon with a hub language, a monolingual corpus, a word stemmer, and a stop word list.  We evaluate performance of PanLexicon on EnglishSpanish-Chinese translation, and show how parameter settings can effect a tradeoff between coverage and precision. The paper continues by describing the output of PanLexicon in Section 2 and the mechanics of creating a lexicon in Section 3. Section 4 presents an evaluation of its quality. After reviewing related work in Section 5, we conclude with some thoughts on future research. 2. An Example Lexicon Before describing the details of PanLexicon, we present an example of the type of lexicon entry it creates. Figure 1 shows a PanLexicon translation set for the factory sense of “plant”. A translation set is a multilingual extension of a WordNet synset (Fellbaum, 1998). It contains words that express a given sense in each of the k languages that comprise the lexicon, where each language may have multiple synonyms for that sense.  plant  English aluminum smelting plant that employs about 930 workers  factory food warehouses, an insecticide plant and a fertilizers factory  planta fábrica  Spanish materiales nucleares de las plantas de energía para fabricar armas atómics trabajadores de una fábrica privada estaban fundiendo pedazos de aluminio  Chinese  厂  工人到厂里来,就是来干活的  厂房 工厂  该厂有8间厂房、5间仓库 生产车间作为工厂的“特区”  Figure 1: Example lexicon entry for the concept of “industrial plant.” This concept is expressed by two English words, two Spanish words, and three Chinese words, with a usage illustration for each word to indicate its meaning.  Each word in a translation set includes a number of contexts to illustrate the intended sense. Only the first context for each word is shown in Figure 1. PanLexicon may be applied to a set of k languages, where k is only limited by available resources, although the figure shows only three languages, English, Spanish, and Chinese. The underlying meaning of “industrial plant” is expressed as “plant” and “factory” in English, as “planta” and “fábrica” in Spanish, and by “厂”, “厂房”, and “工” in Chinese. We have three goals for each entry:  Intra-language consistency – Words within a language should be synonyms. In the English portion of the example, the contexts of “plant” and “factory” make it clear that the words are synonyms in the sense of “industrial plant.”  Inter-language consistency – Words from different languages should be translations of each other, and their contexts should illustrate the same word sense.  Complete sense – PanLexicon attempts to create complete entries. For example, it could be argued that our example entry is incomplete, because it is missing the English word “mill,” which can also be used in the sense of “industrial plant.” 3. Building the Lexicon PanLexicon begins with lexical resources for a set of k languages, where one of the languages is designated as the hub language and the others as spoke languages. There is a monolingual corpus for each language and one or more bilingual dictionaries between the hub language and each of the spoke languages. A preliminary step is to index each corpus, giving us efficient access to the contexts surrounding each word. PanLexicon iterates through each word w in the hub language and uses the bilingual dictionaries to find possible translations t1, … tn in the other languages. The bilingual dictionary also assists in translating context  words to compute a matching score between each context of w and each context of a translation ti. This score is discussed in more detail in Section 3.2.2. Figure 2 shows an example of this schematically. There is a strong similarity score for the first context of “plant” and the context of “fábrica,” indicating that this usage of “plant” and this usage of “fábrica” share the same word sense. The second context of “plant” has a strong similarity to the given context of “planta.” PanLexicon uses these context similarity scores to begin building translation sets that include the hub word w. For each context of w, the matching process returns one best translation with its best matching context from each of the spoke languages. For example, one context of “plant” may match best with a context of “fábrica” in Spanish and a context of “厂” (chǎng) in Chinese, while another context of “plant” may match best with “planta” in Spanish and “植物” (zhí wù) in Chinese. Matches that score below a threshold value are discarded as unlikely to be mutual translations across all the languages. The remaining matched contexts of w are partitioned into groups that share the same set of best matching translations. These sets of translations form preliminary translation sets, but they contain only one word from each language. The final step of PanLexicon is to merge together similar translation sets based on the similarity of the contexts in the different translation sets. The algorithm iterates through each of the hub language words. For each word w, the system uses the bilingual dictionaries to find a set of potential synonyms of w. A linear-time clustering algorithm then merges translations sets of w and its synonyms based on the similarity of a vector representation of their matched contexts. Section 3.2.3 gives the details of synonym finding and translation set merging. After merging, PanLexicon creates one entry for each merged translation set. The entry contains the union of the words from the original translation sets, together with their ranked contexts.  English: plant “aluminum smelting plant that employs about 930 workers” English: plant “enjoying the flowering plants, ripening vegetables, and lush fruit trees”  Spanish: fábrica “trabajadores de una fábrica privada estaban fundiendo pedazos de aluminio” Spanish: planta “cultivos de 50 especies de plantas como flores , arroz , vegetales , o árboles frutales”  Figure 2: Two contexts of the English word “plant” are matched with contexts of two Spanish translations. The heavy lines indicate strong matches with many context words being translations of each other, while thin lines represent poor matches.  Figure 3 shows an example of three preliminary translation sets containing the hub word “plant.” Translation sets 1 and 2 are likely to merge as they share most of their top 10 ranked vector components, while translation set 3 is unlikely to merge with either 1 or 2, as it shares only a single top 10 ranked component with translation set 1 and none with translation set 2. 3.1 Required Resources Because our goal for PanLexicon is to scale it to a very large number of languages, we designed it to require a minimal set of resources. The first required resource is a set of bilingual lexicons between each of the k-1 spoke languages and a single hub language. Secondly, the system requires a monolingual corpus in each language. The corpora do not need to be aligned, but they must have some overlap on topic matter. Because we use the bilingual lexicons to match contexts between the corpora, PanLexicon requires word segmenters and morphological tools as necessary to convert text as it appears in the corpora into words as they appear in the bilingual lexicons. Finally, a stop word list is also required for each language. 3.2 System Details We now discuss some of the details concerning indexing, matching, and merging in the PanLexicon system. 3.2.1 Preprocessing We build the corpora for each language, indexing each sentence as a separate document. The index stores each sentence as a string of space separated tokens. Additionally each sentence is indexed on lowercased, stemmed versions (if applicable) of each token. The system tabulates co-occurrence counts between the lowercased, stemmed tokens, where two tokens are considered to co-occur if they appear in the same sentence together. Next, the system collects and stores the contexts for each word w in another inverted index. We are currently using language dependent, heuristic methods for determining the context around each word, but these are  based only on stop word lists and punctuation. Each context is stored in tokenized form and is indexed by the lowercased, stemmed versions of each non-stop-word token that contains no punctuation or numeric digits.  3.2.2 Finding the Best Matching Contexts To find the context of a translation t that best matches context c of hub word w, the system queries the context index with the bag of words consisting of all of the translations of all of the non-punctuation, non-stop-word tokens from c. The original words from c are also included in the query bag, in order to facilitate matching on named entities. The contexts are returned from the index sorted by Lucene's internal sorting algorithm.1 The top n contexts returned by the index are then re-ranked using a scoring metric discussed next. The highest ranked context is returned as the best match. To produce the similarity score, the context words of t are weighted by their pointwise mutual information (PMI) with respect to t and the context words of w are weighted by their PMI with respect to w. The PMI score between words x and y is defined as:  log  P(x^ y) P( x)P( y)    where P(x) is the probability that x appears in a given sentence, and P(x^y) is the probability that both x and y appear in the sentence. PMI is used to weight the context words because it helps select contexts that are highly predictive of the given word. Context words of w with a translation appearing in the context of t are called matching context words of w. Similarly, matching contexts words of t are those with a translation appearing in the context of w. We obtain a score for each context by squaring the sum of the weights of the matching context words and then dividing by the length of the context. This scoring method provides a good tradeoff between favoring short and long contexts.  
 Conﬁdence Estimation has been extensively used in Speech Recognition and now it is also being applied in Statistical Machine Translation. Its basic goal is to estimate a conﬁdence measure for each word in a given hypothesis, in order to locate those words, if any, that are likely to be incorrectly recognised or translated. It can be seen as a two-class pattern recognition problem in which each hypothesized word is transformed into a feature vector and then classiﬁed as either correct or incorrect. This view provides a solid, well-known framework, within which accurate dichotomizers (two-class classiﬁers) can be derived. In this paper, we study the performance of certain pattern features along with a smoothed Naive Bayes dichotomizer. Good empirical results are reported on a translation task of technical manuals.  
Abstract This paper addresses an effective method to write an English paper suitable for international conferences using our Korean-English paper MT system supported by efficient user interaction environment. Our original Korean-English paper MT system is quite useful for understanding, but not for writing. We analyzed the problem of our system and found 3 main reasons, that is, the errors in the source sentence itself, the errors of our MT system, and the absence of the appropriate domain-specific expression information. In this paper, we provide an effective method for each problem within our user interaction environment. Representative sentence error patterns are obtained through large amount of paper corpus analysis and the user is reported on those kinds of errors for modification. Error candidates of the MT system are reported to the user and the corrections from the user are feedbacked to the system. Finally, the system detects English expressions with low frequency and also proposes more suitable domain-specific expression candidates. The final translation sentences we can get from our system shows 93.3 % accuracy, which, we think, is almost as the level suitable for conference submission.  
 Keywords: Interlingua, UNL, Syntax Planning, Morphotactics, BLEU Scores, Generation, Fluency, Adequacy, Faithfulness  1. Introduction Generation of natural language from a machine processable, precise knowledge representation has to grapple with the problem of redundancy and impreciseness inherent in any natural language. An additional challenge is the requirement of keeping the generated language natural and native speaker acceptable. In this paper, we present HinD- a Hindi Deconverter (i.e., generator) from Universal Networking Language (UNL), which is an Interlingua for knowledge representation in the context of machine translation. We exploit the common features of many Indian languages to generate acceptable sentences. Contributions of this paper are the following: 1) We present the design and implementation of a Hindi Deconverter. Our thrust is on the simplicity of specification while maintaining the fluency of the generated sentences. 2) We observe strong correlation between the fluency and the BLEU scores, as well as between fluency and adequacy scores. Since fluency evaluation does not require reference translations, this correlation facilitates large scale evaluation of generation systems without translating large number of UNL sentences. 2. Universal Networking Language (UNL): The Framework UNL is an electronic language for computers to express and exchange information (Uchida et. al., 1999). The three building blocks of UNL are (i) Semantic Relations, (ii) Attributes and (iii) Universal Words. The UNL representation of a sentence is expressed in the form of a semantic net called UNL graph. Consider sentence (1). (1) John ate rice with a spoon. The UNL expression for (1) is given below:  (2) [UNL:1] agt(eat(icl>do).@entry.@past, John(iof>person)) obj(eat(icl>do).@entry.@past, rice(icl>food)) ins(eat(icl>do).@entry.@past, spoon(icl>artifact)) [\UNL] In this expression, agt (agent), obj (object) and ins (instrument) are the semantic relations. The relatas eat(icl>do), John(iof >person), rice (icl>food), and spoon (icl>artifact) are the Universal Words (UW). These are language words with restrictions mentioned in parentheses for the purpose of denoting a unique sense. icl stands for inclusion and iof stands for instance of. UWs can be annotated with attributes like number, tense, etc., which provide further information about how the concept is being used in the specific sentence. Of special significance is the @entry attribute, typically attached to the main predicate. 2.1 UNL Scopes: Representing Embeddings UNL represents coherent sentence parts (like clauses and phrases) through Compound UWs also called scope nodes. These scope nodes are like graphs within graphs. These sub graphs have their own environment and the @entry node. For example, the UNL expression for sentence (3) is given in (4) and the graph illustrating the UNL relations is given in Figure 1. (3) For this, you contact the farmers of Manchar region or of Khatav taluka. (4) [UNL] obj(contact(icl>communicate(agt>person,obj>person)):0W.@i mperative.@entry,farmer(icl>creator):1T.@pl.@def) pur(contact(icl>communicate(agt>person,obj>person)):0W.@i mperative.@entry,this:04) agt(contact(icl>communicate(agt>person,obj>person)):0W.@i mperative.@entry,you(icl>persons):0J) plc(farmer(icl>creator):1T.@pl.@def,:01)  or:01(region(icl>location):38.@entry, taluka(icl>geographical area):4A) nam:01(region(icl>location):38.@entry, Manchar(icl>geographical place):2R) nam:01(taluka(icl>geographical area):4A, Khatav(icl>geographical area):3U) [\UNL]  obj  contact  agt  farmer pur  you  plc  this  :01 or taluka nam  region  nam  manchar  khatav  Figure 1: The UNL Graph for UNL Expression 4 The phrase ‘Manchar region or of Khatav taluka’ is considered as being within a scope. Note that the scope is given a compound UW ID:01 to denote a separate environment of knowledge representation. UNL relations help representing the argument frame of the sentence and also draw a distinction between the argument and the non-argument links of a predicate. The information for number, tense, aspect, mood, negation, etc., are represented using UNL attributes while gender and language specific morphological attributes likevowel ending of nouns, adjectives, verbs, etc., are stored in the UNL-Target language dictionary. 3. Why UNL? Contrasted to the more popular transfer approach (Hutchins and Somers 1992), the Interlingua approach admits of parallel development of various knowledge resources for analyzing source language sentences and generating target language sentences. Being at the top of the Vauquois Triangle (Hutchins and Somers 1992), elaborate knowledge bases and tools are needed for morphological, syntactic, and semantic processing, both for analysis and generation. The UNL representation has the right level of expressive power and granularity. UNL has 45 semantic relations and 87 attributes (which can be augmented with the user defined ones) to express the semantic content of a sentence. In 1992, Interlingua KANT (Nyberg and Mitamura 1992) was designed for large scale MT of technical documentation. However, KANT is a sublanguage system, and handles only constrained technical English. Many phenomena are left out of consideration, which are handled by UNL. UNITRAN- the Interlingua and the eponymous MT- is too detailed a framework for meaningful practical implementation (Dorr 1993).  ULTRA (Farwel and Wilks, 1991) uses Prolog based grammar for the intermediate representation, and is necessarily restricted in its scope for handling language phenomena. UNL has been influenced by a number of linguisticsheavy Interlingua based Japanese MT systems in the 1980s- notably the ATLAS-II system [Uchida 1989]. However, the presence of researchers from Indo-Iranian, Germanic and Baltic-Slavic language families in the committee for UNL specifications (UNL Specifications 2005, www.undl.org) since 2000, has lent UNL a much more universal character compared to the interlingua used in ATLAS-II. Comparing and contrasting UNL with primitive based interlingua like Conceptual Dependency (Schank 1972) and Conceptual Structures (Sowa 2000), we observe that like UNITRAN, they too are too detailed to admit of practical implementations. 4. Language Generation Though traditionally, language analysis has held sway over language generation- as it involves various disambiguation tasks- early 90s saw the reemergence of Natural Language Generation (NLG) problem, mainly because of the fluency and adequacy requirement in the output produced (Reiter and Dale 2000). Add to it the need for discourse preservation, and the task becomes a real challenge. NLG research in recent times is witnessing a flurry of activities in Dialogue Systems in which the generation component addresses the problems of sentential fluency, text planning and discourse coherence (SIGGEN conferences 2003-06). We, however, have concentrated on single sentence generation. The reasons for traversing a trodden path are- (i) the gradual re-emergence of knowledge based machine translation that needs generating target language output from an interlingua (ii) the viability of interlingua based MT for Indian languages which number many, but are closely knit in terms of kinship relations and finally (iii) the absence of a generalized framework for Indian languages generation from semantic representation. Several UNL Deconversion (NLG) systems (Dhanbalan T. and Geetha T. 2003; Daoud D. 2005), including an earlier effort by us, used the universal deconverter tool Deco, provided by the UNL foundation (www.undl.org). Similar to experiences reported by Manati project (Pelizzoni J. and Nunes M. 2005); we too were unsatisfied with Deco. The source code for Deco is not available and its rule-format is abstruse requiring, since it aims to be Turing complete. Manati, while being simpler than Deco, is still a complex framework since it also is a universal deconverter. In contrast, our design is considerably simpler since our scope is a subset of Indian languages only and we aim to exploit their common features. The Chinese Deconverter reported in (Shi and Chen 2005) makes assumptions stronger than our system (discussed in Section 6.5), and mentions that for Chinese,  they only have to deal with case marker insertion, but not with morphology generation in general. The French Deconverter reported in (Blanc E. 2005) also converts the graph to the tree and feeds the tree to an existing transfer program. 5. Stages in the Generation Process The generation process consists of three main stagesmorphological generation of lexical words, function words insertion, and syntax planning. For example, in order to translate the sentence (1) into Hindi, a machine has to generate the form ‘khaaya’ (ate) from ‘khaa’ (eat) using the information for tense (past), number (singular), and gender (masculine) associated with ‘khaa’. The case markers ‘ne’ and ‘se’ also need to be inserted after the subject ‘John’ and the object ‘rice’ respectively. All the words can finally be arranged to construct a valid sentence in Hindi- ‘jaun ne chammach se chaawal khaaya’, for (1). 5.1 Morphological Generation of Lexical Words 5.1.1 Noun Hindi nouns inflect for number and case, and can be described as having major categories of the forms based on the oppositions direct-oblique and singular-plural. They can be categorized into masculine and feminine gender in terms of their agreement with adjectives and verbs. In UNL, plural nouns are represented using the attribute @pl, and singular ones remain unspecified (absence of @pl refers to a singular noun). Direct or oblique case is identified using the relation a noun has with a verb or with another noun in a sentence (typically the genitive case). Gender and vowel endings are stored in the UNL-Hindi dictionary. The morphological rules based on word paradigms generate a noun form using all this information, viz., lexical, relational, and UNL attributes. A noun that carries an attribute NOTCH (not changeable form) in its dictionary entry remains unchanged, and does not inflect for number or case. 5.1.2 Adjective Like nouns, adjectives in Hindi also inflect for case, number, and gender, and exhibit concordance with their head nouns (few adjectives, e.g., sundar (beautiful), bhaarii (heavy) do not inflect to agree with their head nouns). Their heads are identified using relation labels. A form in agreement with the head noun is generated using morphological rules. 5.1.3 Verb Hindi verbs inflect based on GNPTAM information, voice, and vowel ending. Inflections are marked either on the main verb or on its auxiliaries that appear as free morphemes. The information for number, tense, aspect, mood, negation, etc., is represented using the UNL attributes like @pl, @present, @past, @possible,  @must, etc., while vowel ending is stored in the UNLTarget language dictionary. A verb takes passive morphology if the noun it is related to has the attribute@TOPIC in its UW. Gender information of the noun a verb agrees with is gathered using the UNL relation which dictates whether the situation is subject controlled (kartrari prayoga) or object controlled (karmaNi prayoga). Agreement with noun Hindi verbs always agree with their nominative subjects or with the object, in case the subject is oblique. They take the default form- singular, masculine when all nouns are oblique. In order to generate a verb form that is in concordance with the unmarked noun (subject or object), the noun’s gender and number values are passed on to the verb’s list of attributes. Rest of the information, i.e., for tense, aspect, mood, vowel ending, etc., is provided either by UNL attributes or by UNL-Hindi dictionary. Morphological rules generate morphemes (verbal inflection as well as auxiliaries) for a verb that correspond to the value of these attributes. For example, a verb with UNL attributes- @present and @progress, the dictionary attribute for vowel ending @VA, and with the attributes F (feminine) and @pl of the noun it agrees with, will be generated as- khel rahii hain (are playingfeminine). Non-finite verbs that do not inflect for tense are of three kinds- gerunds, participles and infinitives. Gerunds are nominal verbs that take the position of nouns but retain their verbal traits like- taking an object or adverbial qualifiers. A verb is identified as a gerund if in a UNL expression it has the attribute @progress, and it appears as a child of the aoj relation with a noun or of the obj relation with a verb. Gerund forms are generated by attaching –naa suffix to a verbal root. Verb participles act as verbal adjectives or verbal adverbs in a sentence. In Hindi, verbal adjectives are formed by using –taa huaa to denote progressive aspect, e.g., ugtaa huaa sooraj (rising sun) and –aa/yaa huaa to denote perfective aspect, e.g., thakaa huaa aadmii (tired man). Verbal adverbs are formed by attaching –kar or – te huye to verb root, e.g., ‘khaakar aayaa’ (came after eating) and ‘khaate huye aayaa’ (came eating). In UNL, verbal adjectives can be identified if the verb has the attributes @progress or @complete and also appears as a child in the mod (modifier of) relation with a noun. Likewise, a verbal adverb appears as child in a relation with another verb. Infinitives are identified as those verbs which do not have @progress or @complete and always appear as child in an obj relation with another verb. Infinitives are generated by attaching –naa suffix to a verbal root. Conjunct verb Expressing a single word concept in one language may require two or more words in another language. Many verbs in English can only be translated into Hindi by  Figure 2: The Architecture of the Generation System  using a noun-verb or an adjective-verb sequence (Chakrabarti D. 2006). Such verbs are called conjunct verbs. The UW translations of these verbs are stored in the dictionary as a noun-verb or as an adjective-verb sequence. The morphological attributes of these verbs remain the same as other verbs. All inflections are marked only on the verb, and the noun or the adjective in the sequence remains uninflected. Many of these verbs are formed by adding nouns or adjectives to the verbs- kar (do) (e.g., shaadi kar (marry), snaan kar (bathe) etc.) or ho (be) (e.g. samaapt ho (finish), laagu ho (promulgate) etc.) Such verbs carry additional attributes- @link and @lnk respectively in their dictionary entries. 5.2 Function word insertion UNL encodes case information by using relation labels assigned as per the properties of the connected nodes. Consider, for example, the translation of sentence (1). जॉन ने चàमच से चावल खाया | jaun ne cammaca se caawal khaaya--- (7) Here, the case markers ne and se are inserted to derive the relation jaun and cammaca have with the verb ‘eat’. Given a node along with all its lexical attributes from the UNLHindi dictionary, an appropriate case marker is inserted. Similarly, other function words like- conjunctions, disjunctions, particles, etc., are also inserted to represent clausal information. 5.3 Syntax Planning Syntax planning is the process of linearizing the lexemes in the Semantic hyper-graph. The use of overt case-markers makes the word-order in these languages flexible. But, some orders are considered more natural than others, and hence, we assign relative positions to various words based on the relations they share with the head-word in a clause. 6. Generation System Architecture The previous section described the linguistic foundations of our Deconverter HinD. This section concentrates on the architecture of HinD, shown in Figure 2.  Section 6.7), in particular related to Scopes. Currently, we handle these errors using some heuristic rules for graph repair. For example, any two nodes having ‘cnt’ (content) relation are put into a Scope.  6.2 Lexeme Selection Each UW along with its restrictions is looked up in the language specific dictionary, and the corresponding lexeme is obtained. Fortunately, the Deconverter does not have to deal with the WSD problem. It is handled during source language to UNL enconversion by associating restrictions with a UW to uniquely represent a sense. For example, the following two UWs entries correspond to two different senses of the word ‘water’: [paanii]{}"water(icl>liquid)"(N,INANI,OBJCT,PHSCL,FRM,LQ D,M,NOTCH,UNCNT,NI) [paani de]{}"water(icl>wet(agt>person,obj>thing))" (V,VOA, VLTN,,CJNCT,N-V,Ve)(Water plants/trees). The entries in parentheses are morpho-syntactic and semantic attributes of Hindi words which control various generation decisions like choosing specific case markers. 6.3 Case Identification and Morphological Generation As discussed earlier, Hindi morphology is decided by GNPTAM and ending vowels. We next show some sample rules for noun morphology generation in Table 1.  Suffix uoM U I iyoM oM  Attribute values @N,@NU,@M,@pl,@oblique @N,@NU,@M,@sg,@oblique @N,@NI,@F,@sg,@oblique @N,@NI,@F,@pl,@oblique @N,@NA,@NOTCH,@F,@pl,@oblique  Table 1: Sample Noun Morphology Rules  6.1 UNL Parsing and Graph Repair The input UNL expression is parsed into a graph-structure. Based on our error analysis, we observed that some errors are common in the input UNL expressions (discussed in  Noun inflections are handled using attribute values mainly for gender, number, case, and vowel ending. Inflections are added to a word stem to generate a desired form. For example, an ‘U’ ending masculine noun- ‘aaluu’ (potato)which is stored as ‘aal-’ in the dictionary along with the  attributes like N, NU, M, and also has UNL attributes @pl and @oblique- will match the first rule of the sample rules given above, and will be outputted as ‘aaluoM’.  Suffix Tense -e rahaa @past thaa  Aspect @progress  Mood -  N  Gen P V  E  @sg @male 3rd e  -taa hai @present @custom  -  @sg @male 3rd -  -iyaa thaa @past @complete  -  @sg @male 3rd I  saktii hain @present  -  @ability @pl @female 3rd A  Table 2: Sample Verb morphology rules Verbs, as mentioned previously, inflect for GNPTAM, vowel ending and voice. A few rules for verb morphology generation are given in Table 2. For example, the first rule in the table is read as- attach -e rahaa thaa to a verb root (e.g., ‘de’ and ‘le’ which are stored as ‘d-’ and ‘l-’ in the UNL-Hindi dictionary) which has the attributes- @past for tense, @progress for aspect, mood unspecified, shows agreement with a singular (@sg), masculine (@male), 3rd person noun, and ends with the vowel ‘e’. The forms generated using this rule would be ‘de rahaa thaa’ (was giving) or ‘le rahaa thaa’ (was taking). 6.4 Function Word Insertion Having inflected the words as per morphological rules, function words like case markers, conjunctions, relative pronouns etc., need to be inserted. The rules for inserting function words depend on UNL relations and the restrictions specified with the parent and child nodes. A rule has the following five components: 1. Relation name 2. Necessary Conditions for Parent node 3. Negative Conditions that should not be present at Parent node 4. Necessary Conditions for Child node 5. Negative Conditions that should not be present at Child node Based on these components, a decision is made about inserting a function word before or after parent and child nodes. Consider sentence (1) and its UNL again. The Case marker rule applicable for this sentence is: agt : @past#V : VINT : N : null => null : null : null : ने This rule says that in the ‘agt’ relation, if the parent UW is a verb with @past attribute, and is not an intransitive verb, and if the child UW is a noun, insert the case marker ‘ने’ after the child UW, e.g., after John in Sentence 7. Sisi:mainladr:lnyu,ltlh:neurlul:l@e fcoornitnrasesrt:tinnugllt=h>e ncuolnl:juलnेिcकtiनon:nलulेिlक:nनu(llbut) Note that we do not consider all the properties of the  6.5 Syntax Planning Syntax planning is the process of linearizing the lexemes in the Semantic hyper-graph, i.e., it decides the word-order in the generated sentence. To make this process rule driven, we make several important assumptions: Semantic Independence: The relative word order of a UNL relation’s relata does not depend on the semantic properties of the relata. Context Independence: The relative word order of a relation’s relata does not depend on the rest of the expression. Local Ordering: The relative word order of various relations sharing a relata does not depend on the rest of the expression. Note that the last two assumptions are weak in that, in theory, they help us avoid making the strong Compositionality assumption [Shi X. and Chen Y. 2005], which states that the sentence for a whole tree can be composed from the sentences of its sub trees. Say, a tree is of the form A->B->C. Then, the compositionality assumption states that A can only be either at the beginning or at the end of the generated sentence. Whereas, HinD allows A to occur in between B and C. In practice, we found that whenever Compositionality assumption is violated, it is due to the improper use of Scope, i.e., if our system generates BAC then A->B should have been a Scope in the first place. However, given that imprecise UNLs are a fact of life, it is important that our system should be able to handle them. Based on these assumptions, we break down the graph linearization problem into following subcomponents: • For a given node, decide whether each of its untraversed parents (there can be multiple parents) and children nodes should be ordered before or after the current node. • For nodes in each of the ‘before’ and ‘after’ group, decide their relative orderings. Both of these ordering decisions are done based on the UNL relation between the node under consideration, and the parent or the child node. 6.5.1 Parent-Child Positioning For each UNL relation, a rule-file states whether the parent should be ordered before or after the child. Currently, ‘aoj’, ‘seq’, ‘and’, ‘or’, ‘fmt’, and ‘cnt’ relations place the parent first, and the rest of the relations place the child first. 6.5.2 Prioritizing the Relations In our system, a Priority-Matrix describes the Left-or-Right relative position of two UNL relations when they have a common relata. Consider Sentence 1 and its Hindi translation- Sentence 9. In English, the order of the arguments in the sentence is agent-object-instrument. On the other hand, the default order for its Hindi equivalent is agent-instrument-object. Table 3 (L: towards left, R: towards right) shows a subsection of the Priority-Matrix for Hindi. Treating this matrix as an adjacency list representation of a directed graph, where L (R) indicates incoming (outgoing)  edge, graph vertices are topologically sorted. The sorted output is ranked in descending order, i.e., the relation that should appear leftmost gets the highest rank. In case a cycle is found in the graph during sorting, the user is requested to break the cycle.  agt aoj obj Ins  Agt  L L L  Aoj  L L  Obj  R  Ins Table 3: A subsection of the Priority Matrix  6.5.3 Syntax Planning Algorithm The following algorithm does syntax planning by using the Parent-Child Positioning rules and the Relation Priorities. Initialization: Mark the Entry node and put it on Stack. Begin-Algo While Stack is non-empty: 1. Pop the top node from the Stack and make it Current. 2. If the current node has unmarked relata 2.1. Divide the unmarked relata of the Current node in ‘Before-Current’ and ‘After-Current’ groups based on the Parent-Child Positioning Rules, and mark all of them. 2.2. Sort each group in ascending order based on their ranks in the topological sort output. 2.3 Push them on the stack in sorted ‘After-Current’, Current, sorted ‘Before-Current’ order. 3. If the Current node has no unmarked relata: 3.1 If the Current node is a Scope node, then recurse. 3.2 Else, output the Current node. End-Algo Table 4 shows a step-through algorithm for the UNL shown in Sentence 4 (corresponding to English Sentence 3). Step number X-Y.Z means iteration X, algorithm step Y.Z. Only some of the steps and some of the variables are shown. Note that for ‘or’ relation, the parent is placed before the child and for all other relations, the child is placed first.  6.6 Language Specific and Language Independent Components As described so far, all components of HinD use language independent algorithms with language dependent data. UNL expression parsing and lexeme selection are algorithmic processes independent of language. The syntax planning component can be applied to any language by just adopting the priority matrix for the specific language. Case marker generaton and morph-synthesis too are, engines that make use of Hindi specific configuration files, i.e., rules.  6.7 Limitations of Generating from UNL Unlike Deco (Uchida et. al. 1999) and Manati (Pelizzoni J. and Nunes M. 2005), simplicity is one of the explicit aims of HinD, even at the expense of some Fluency. That is, given that the hard part of analyzing a sentence is already done during the enconversion process, we hope that a user  for a given Indian language should be able to use HinD by writing some simple rule files without having to worry about complicated interaction between word-forms, semantic relations, and syntax planning. In practice, we face several obstacles in generating high quality sentences from such a simple scheme:  a) UNL Expressiveness: In certain situations, UNL has limited expressive power. This issue is discussed in detail in (Boguslavsky I. 2005). Here we give just one example: ‘aoj’ relation is used both for attributive and predicative adjectives. Hence, the same UNL expression can give rise to ‘red leaf’ as well as ‘leaf is red’.  Step  State  1-1  Stack = {},Current = contact, Output= {}  1-2.1 1-2.2 1-2.3 2-1 2-5.2 3-5.2 4-2.3 5-3.1 6-1 6-2.1 6-2.3 7-1 7-5.2 8-5.2 9-2.3 10-5.2 13-5.2  Before-Current = {farmer,this,you} Sorted-Before-Current = {farmer,you,this} Stack = {contact,farmer,you,this} Stack = {contact,farmer,you},Current={this} Stack = {contact,farmer,you},Output={this} Stack = {contact,farmer},Output={this,you} Stack = {contact,farmer,:01} Stack = {contact,farmer},Recurse{:01} Stack = {contact,farmer,region} Before-Current = {manchar}, After-Current = {taluka} Stack = {contact,farmer,taluka,region,manchar} Current = {manchar} Stack = {contact,farmer,taluka,region}, Output = {this,you,manchar} Stack = {contact,farmer,taluka }, Output = {this,you,Manchar,Region } Stack = {contact,farmer,taluka,khatav} Stack = {contact,farmer,taluka }, Output = {this,you,manchar,region ,khatav} Stack={},Output={this,you, manchar,region,khatav,taluka, farmer,contact}  Table 4: An example of Syntax Planning  b) Imprecise UNL Expressions: Whether manual or automatic, semantic graph creation from a natural language sentence is an error-prone process. We find that many a times, scopes are not handled properly, or some relations are confused with each other, say ‘obj’ and ‘plc’. c) Syntax Planning Assumptions: To keep the system simple, HinD makes several assumptions, discussed in Section 6.5. For example, in case of ‘X seq Y’, HinD always generates ‘X before Y’ and never ‘Y after X’. d) Word Properties: HinD is guided by UNL relations and the attributes associated with UWs. Sometimes, two semantically similar Hindi words show different morphosyntactic behavior. For example, subah (morning) and raat (night) can be substituted for shaam in - vah shaam ko aayaa (He came in the evening). It is only subah that does not take the case marker ko while others do. HinD does not handle this properly. Similarly we generate को in the example in Table 5 instead of से because we do not consider all the properties of the Hindi word संपक[ (contact).  This concludes our discussion of the Generation system. Table 5 shows an example illustrating various stages of the generation (* in the table shows a stem on which a suffix is to be attached).  Module Original English Sentence UNL Expression  Output For this, you contact the farmers of Manchar region or of Khatav taluka See Sentence 4 and Figure 1  Lexeme Selection Case Identification  संपक[ िकसान ् यह आप ¢े] ् तालकु ् मचं र खटाव contact farmer this you region taluka manchar khatav संपक[ िकसान*् यह आप ¢े]*् तालुक्* मंचर खटाव contact farmer* this you region* taluka* manchar  Morphology Generation  khatav  संपक[ कीिजए  िकसानɉ  contact .@imperative farmer.@pl तालुके मंचर खटाव  यह आप ¢े] this you region  Function Word Insertion  taluka manchar Khatav संपक[ कीिजए िकसानɉ को इसके िलए  contact  farmers this for  या तालुके के मंचर  खटाव  आप ¢े] you region  or taluka of Manchar Khatav  Syntax Planning  इसके िलए आप मंचर ¢े] या खटाव This for you manchar region or khatav तालुके के िकसानɉ को संपक[ कीिजए |  taluka of farmers  contact  Table 5: An example output at various generation stages  7. Evaluation The problem being tackled in this work is the generation of NL sentences from semantic graphs which represent meaning. What is important is the faithful capturing and the rendering of this meaning in the generated sentences. Measuring this faithfulness requires careful comparison of the generated sentences with UNL expressions. However, finding evaluators outside our project, who are native Hindi speakers and also expert in UNL, is a tall task. In any case, this would be highly time-consuming and a subjective process. Hence, we compromise by generating reference Hindi sentences from original English sentences, and measuring the adequacy of the machine generated sentences with respect to reference Hindi sentences. Assuming that the reference sentences are faithful to the UNL expressions, we indirectly measure the faithfulness of the generated sentences in addition to directly measuring fluency, the ‘syntactic quality’ of the generation sentence.  7.1 Input Preparation We evaluated the generation of 901 Hindi sentences from Agricultural domain. These sentences are taken from the script of Question-Answer threads between farmers and Agriculture experts. The original sentences were in Marathi, which were manually translated to English and then to UNL. Single reference Hindi translations were generated from English sentences. BLEU scores (Papineni et al.., 2002) were computed using single reference  translations. Median sentence length was 14 words with a Standard Deviation of 7.5. 7.2 Manual Evaluation Guidelines We adapt the evaluation guidelines from (LDC 2004) and (Sumita E. et al.. 1999). After some trial evaluations with various schemes, and discussions with evaluators, we decided to convert the 5 point scale in (LDC 2004) to a 4 point scale, since too fine-grained a distinction may result in evaluators worrying a lot about making an accurate call, and intuitive judgment may get affected. It also makes the evaluation even more subjective. Our final evaluation guidelines are shown in Figure 3.  Fluency of the given translation is: (4) Perfect: Good grammar (3) Fair: Easy-to-understand but flawed grammar (2)Acceptable: Broken - understandable with effort (1) Nonsense: Incomprehensible Adequacy: How much meaning of the reference sentence is conveyed in the translation? (4) All: No loss of meaning (3) Most: Most of the meaning is conveyed (2) Some: Some of the meaning is conveyed (1) None: Hardly any meaning is conveyed  Figure 3: The Evaluation Guidelines As per (LDC 2004), the evaluators were asked to provide their intuitive reaction to the output and to work as quickly as comfortable. Adequacy judgments were taken after the fluency judgments, and the judges were asked to look at the reference Hindi translations only after the fluency judgment was over.  Geometric Average Arithmetic Average Standard Deviation Correlation BLEU Correlation Fluency  BLEU 0.34 0.41 0.25 1.00 0.59  Fluency 2.54 2.71 0.89 0.59 1.00  Adequacy 2.84 3.00 0.89 0.50 0.68  Table 6: Average Scores  7.3 Evaluation Results All three matrices were computed separately for all 901 sentences. Various statistics are shown in Table 6. From these results we conclude that our system is able to generate slightly flawed but easy to understand sentences that convey most of the meaning. Our BLEU score also seems impressive, until one realizes that our system does not deal with the WSD problem, and the use of UNL Scope makes the handling of clauses and phrases easy. We observe that there is good correlation between Fluency and the BLEU scores, and strong correlation between Fluency and Adequacy scores. The relation between adequacy and fluency is explored further in Figure 4. Figure 4 shows the distribution of Adequacy scores for various values of Fluency.  Flue ncy vs Ade quacy  Numberof sentences  200 150 100 50 0  3134 20 
 Abstract While Machine Translation (MT) quality is usually interpreted as the linguistic quality of the translations themselves, there are other aspects that should be considered, as well. These aspects may depend on the specific system or provider, as there are various types of applications for MT. CLS Communication as a commercial MT service provider has to meet its clients' needs and set itself apart from competitors. We have established a simple and flexible quality management approach that allows us to measure and improve MT quality on an ongoing basis. From a practitioner's point of view, we show how we collect input for the further development of our offer and implement the respective findings in our environment. Only few resources are needed for this approach, and it can easily be adapted to our future requirements.  
 Abstract One of the mottos of pure statistical MT system promoters is that it is possible to “build a new language pair” overnight, but the development of a new language pair for a proficient rule-based translator requires a great amount of effort in linguistic rules and resources description. Therefore, we are interested in rapid development techniques for rule-based systems. In this paper, we present the work that was conducted at SYSTRAN during the past year: we successfully developed 12 new language pairs in one year. This led us to shift some architecture paradigms in our translators, to expand implementations with the notion of Linguistic Families, to open our interfaces to more readable formats, and to design ways to work with fully multisource / multitarget aligned dictionaries in order to save time, in particular with the coding effort.  Introduction SYSTRAN’s effort over the past couple of years has focused on the development of parsers for new languages 22 parsers are now available- and on implementing a paradigm shift in order to build more modular, streamlined translators (Attnäs et al., 2005). At the beginning of 2006, SYSTRAN’s offer included 38 commercial language pairs and 15 additional language pairs developed for specific customers. One strategic goal is to complete the matrix of available language pairs, including the new parsers and new target languages. The ideal situation for an organization whose purpose is to develop Machine Translation systems is the ability to easily add a new language pair to its already existing offer in a very short span of time. Developing a translation engine of a new language pair for a rule-based system (such as SYSTRAN’s) requires much effort in terms of manpower and duration. This effort can of course be reduced by using a strong modular approach and powerful tools for the construction of linguistic resources. To the contrary, proponents of statistics-based Machine Translation claim they are able to build a new engine overnight, provided a reasonable amount of training data is available (Koehn, 2005; Och & Ney, 2000), yet further studies proved this approach is not so easy to implement either (Foster et al., 2003).  weight to linguistic rules applied in the translator, based on statistical observation of training corpora. This paper presents the strategies we applied during this development phase, the main problems that arose and how we solved them. Last, we draw upon a few important methodological conclusions and discuss open issues as we begin to bring these translators to market. Modular Development An obvious entry point for rapid development of Machine Translation systems is to take advantage of the classical architecture of transfer-based MT systems such as SYSTRAN translators. Figure 1 recalls the schematic classical architecture with three main phases (or modules): analysis (parsing), transfer and generation.  Transfer  Language independent representation  Analysis  Post-analysis representation  Pre-generation representation Generation  During the year 2006, SYSTRAN decided to embark on an ambitious project: to develop a set of 12 new language pairs in less than a year, and to do this with a minimal team of linguists and developers. The systems were to leverage the existing code and resources by further enhancing the modularity of SYSTRAN’s code. A radically new approach of lexical resources also had to be designed in order to rapidly build and maintain 12 bilingual dictionaries under the form of a unique multisource - multitarget dictionary. And finally, we wanted these translators to interface with a statistical approach: to use automatically extracted dictionaries of multiword expressions in addition to the regular dictionaries, and be able to assign  Surface level  Source sentence  Target sentence  Figure 1: Classical Transfer Architecture The ideal situation in such architecture would be to reach an abstract, language independent representation of the source sentence after analysis (Sérasset & Boitet, 2000). From this representation, the generation would build the target sentence from pure semantic and rhetorical information. But in reality, the analysis only reaches an intermediary level of abstraction, with much syntactical information about the source structure, and this has to be transferred into another intermediary structure with syn-  tactical information about the target structure. Maintaining an “intermediate” level of the analysis is also strategic in terms of translation quality – based on contrastive grammar; transfer modules correctly handle similar phenomena in source and target languages and do not require “perfect” linguistic analysis of these phenomena. In our case, we wanted to develop new language pairs consisting of all the possible combinations of source and target languages among German (De), Spanish (Es), Italian (It), and Portuguese (Pt). There are 12 combinations: DeEs, EsDe, DeIt, ItDe, DePt, PtDe, EsIt, ItEs, EsPt, PtEs, ItPt, and PtIt. Additionally, we wanted to reuse as much existing code as possible and take advantage of similarities within language families. In fact, in most available SYSTRAN translators, the modules architecture is a little bit more complex: the paradigm in use is analysis + transfer + synthesis + rearrangement. The analysis module depends on the source language; the synthesis module depends on the target language only, while both transfer and rearrangement of words depend on source and target languages.  Language independent representation  Analysis  Postanalysis  Transfer Synthesis  Rearrangement  Surface level  Source sentence  Target sentence  Figure 2: SYSTRAN’s Two Module-Based Architectures  By shifting from the current paradigm to the analysis + transfer + generation paradigm, we aim to reduce the costs. Transfer is the only module that in principle is not reusable when developing a new language pair (but we will see below that it is not entirely true). Analysis and generation, which jointly represent almost 90% of the code, can be reused without modification, as shown in Figure 3, where non-reusable code is displayed in light grey.  Reusing Existing Modules – Analysis Except for our continuous maintenance effort, not much has to be done for these critical modules. We kept the existing analysis modules from existing language pairs since they are already stable for all the languages.  Modularity regarding language families already exists for Slavic and Romance languages. For the Romance languages, we have a strong “Romance Language Trunk Parser”, feeding into small modules for the individual languages (Es, It, Pt). The German parser is also stable and usable for translations from German into any target language. In fact, it will be easy to create a  “Germanic Language Trunk Parser” for a number of closely related Germanic languages as soon as we developing more new language pairs with various Germanic source languages.  Analysis + Transfer + Synthesis + Rearrangement Paradigm Analysis (80%) Language-family rules  Language-specific rules  Source module  Analysis + Transfer + Generation Paradigm Analysis (80%) Language-family rules Language-specific rules  Transfer (10%) Synthesis (5%) Rearrangement (5%)  Source-Target module Target module  Transfer (10%) Language-family transfer Language-pair rules Generation (10%) Language-family rules Language-specific rules  Figure 3: Paradigm Shift  No parser is ever perfect and we want to highlight that we had to make some minor changes to the system in order to make the analyses slightly more detailed (to go a little higher on the abstraction scale of Figure 1), as we worked on language pairs for which the source and the target languages pertained to more distinctive language families than before.  A simple example concerns disambiguation of verb forms. When translating from German to English, it did not matter whether the ambiguity infinitive/finite verb was correctly analyzed when the word appeared all by itself (“laufen” – “run”). But with Romance languages we needed to refine our disambiguation rule to get the infinitive form (“laufen” – “correr” rather than “corren”)  Analysis changes thus amount to refinement and corrections of issues that were overlooked because they were not as visible in the previous language pair’s configuration.  Adapting Existing Modules – Transfer With 12 new language-pair combinations, we might have looked at 12 new transfer modules. However, there would have been much repetition and overlap. We therefore decided to use a model based on our already existing “Slavic to English Transfer” module, which serves for the transfer of six Slavic source languages to the English target.  In order to factorize as much code as possible at the transfer level for our 12 new language pairs, three special submodules were developed: a Romance-to-Romance transfer, a German-to-Romance transfer and a Romance-toGerman transfer.  This division is understandable with a bit of linguistic intuition: the pairs Spanish/Italian/Portuguese to/from Spanish/Italian/Portuguese do not require much work in transfer as the source and target linguistic structures are very similar (they are closely related to Romance languages). In contrast, the Spanish-to-English pair requires  some more effort on transfer, but not that much because modern Spanish shares most of its verbal tenses system with English (without considering the subjunctive). But Spanish/Italian/Portuguese and German are radically different, and more detailed transfer rules in each direction are needed if we want to handle word reordering correctly. The transfer modules are written in traditional SYSTRAN rule format. Their output is an abstract representation that can be used directly by other traditional synthesis + rearrangement modules or via a new XML interface by the new generation modules described below. One part of transfer code involves “lexical routines”. These are specialized rules triggered by some source language words and/or syntactic/semantic features. They are set up in such a way that one set of programs handles the analysis of the source language phenomena and outputs an abstract “meaning/structure identifier”, which can be used by target language programs. The con-  cept of a language family is important here as well. We apply the same division for the target lexical routines as for the rest of transfers: Romance-to-Romance, Germanto-Romance, and Romance-to-German language families as needed. New XML-Based Interface During this effort, we seized the opportunity of creating a new generation module to introduce a new format for passing information from the transfer to the generation. After streamlining our architecture (Attnäs et al., 2005) we wanted to introduce a new open interface based on XML, according to the well-known framework of featurebased representations. To do this we introduced a mapping module from SYSTRAN’s proprietary, closed format into explicit, open features. This module is also responsible for synthesizing syntagms and clauses from SYSTRAN’s internal flat representation of word relationships (see Figure 4).  el  cacao  es  saludable  para  DET  NOUN  VERB  ADJ  PREP  
Although many high-quality dictionaries contain a sufﬁcient number of idioms for their intended users, the methods available for looking up entries in both paper and electronic dictionaries as well as in machine translation systems are not satisfactory. Providing an adequate automatic look-up function is complicated by the existence of idiom variants, which sometimes can be very creative. The problem is further complicated by the fact that the possible range of idiom variations has not been described in a computationally tractable way. Against this backdrop, we analysed the variation patterns of idioms using manually-created idiom variation data, and, on the basis of that, developed an idiom look-up system that automatically matches idiom variants in English texts with the canonical forms of idiom entries in dictionaries. The experimental results showed our system performs sufﬁciently well to be used in real-world settings, including as an aid for translators, which is our overall aim.  Introduction We are currently developing a system that aids Englishto-Japanese volunteer translators who translate online documents and publish translated documents online. Among the many reference functions and aspects of reference content that require enhancement, translators have identiﬁed improvement in idiom look-up functions as a key issue they would like to see addressed by translation aid systems (Kageura et. al., 2006). Although idioms provided by many high-quality dictionaries (e.g. Sanseido, 2004; McCaleb and Iwasaki, 2003) are basically satisfactory for translators, the methods available for looking up idiom entries are far from satisfactory, not only in paper dictionaries but also in electronic dictionaries. This is partly because the user has to guess the core constituent words of an idiom in order to consult a dictionary. Automatic look-up methods embodied in machine translation systems are not satisfactory, either. Take, for instance, the following examples: (1) He said that with his tongue in his cheek. (2) He said that with his big fat tongue in his big fat cheek. Although many available machine translation systems successfully detect the idiomatic expression “with one’s tongue in one’s cheek” in (1), none among those we checked (e.g. Excite, 2005; Fujitsu, 2005; LogoVista, 2005; Sharp, 2004; Toshiba, 2005)1 could properly translate (2). Most existing methods for looking up idioms cannot deal with the rich variations in idioms, that are abundant in ordinary texts2. In the ﬁeld of natural language processing (NLP), much research has been carried out into the automatic extraction of collocations and idioms (e.g. Piao, 2006; Smadja, 1993; Widdows and Dorow, 2005), but not much work has been devoted to the automatic matching of idiom entries to their occurrences in running texts. As translators are basically satisﬁed with the idioms provided in existing high-quality dictionaries but frustrated with the poor look-up functions, 1Sharp (2004) can detect some gapped idiom occurrences, but it fails to detect complex idiom variations. 2For instance, our rough survey of online documents revealed that the idiom “hang on” in its basic form, “hang on” with insertion, and “hang on” with passivisation (with or without insertion) occur in roughly the same frequency. The same is true for “dumb down”.  to develop enhanced look-up functions for idioms is of utmost importance from the point of view of aiding translators. Although a few important related studies exist (Carl and Rascu, 2006; Jacquemin, 2001; Yoshihashi et. al., 2005), and many translation memory systems realise ﬂexible approximate matching of similar sentence or phrasal constructions (Similis, 2006; Trados, 2006), the task of ﬂexible automatic look-up of idioms is yet to be fully explored. Against this backdrop, we are developing a mechanism that automatically matches English idiom occurrences in texts and their possible variations with idiom entries in dictionaries, as part of an overall project that aims at developing a system to aid English-to-Japanese online volunteer translators. In the following, we will ﬁrst clarify translators’ basic requirements. Then we will provide the basic patterns of idiom variations based on manually-constructed idiom variation data, and explain the automatic idiom look-up system that takes into account major syntagmatic idiom variations. Finally, we will provide an evaluation of the method and outline areas for further improvement. Translators’ basic requirements In order to clarify requirements for translation-aid system, we consulted eight translators working online. We also sent a questionnaire to other online translators and obtained 12 replies. In relation to idiom look-up functions, two important features became clear. Firstly, translators do not want the system to provide a single idiom entry that matches textual occurrences. This is more to do with the fact that checking multiple possibilities is an inherent and essential part of proper translation than with the fact that it is difﬁcult to develop a satisfactorily high-performance automatic idiom look-up system. In other words, from the point of view of translators, for the system to provide multiple possibilities of matching idioms is not a defect but a necessity if the system is to be useful for them. After all, translators check many candidates that they do not eventually use in their translations. What is important is reducing translators’ burden as well as the quality of candidates the system proposes. Secondly, translators — as language practitioners — want the system to be able to deal with variations in a more ﬂexible way than described by linguists. For instance, for language  practitioners “shoot the breeze” could be passivised (or they could imagine a situation in which they face the passive form of this idiom or they passivise the idiom by themselves in the process of writing), while according to Numberg et. al. (1994), it is not possible. The idiom “go halves” can be used in the form “go exact halves”, while Nicolas (1995) claimed that this is not possible. In a sense, even what descriptive (i.e. non-prescriptive) linguistics provides is too prescriptive for the reality of texts that translators face in their daily activities. This is especially the case for online documents (Aitchison and Lewis, 2003), which are dealt with by the translators our system targets. From the point of view of system speciﬁcations, these two requirements mean that the system can — and should — provide overmatching results, with recall as close as 100%. They also mean that, in the evaluation of system performance, the concept of “precision” should be deﬁned not in terms of the “correct” choice of candidate but in terms of its usefulness for translators. We will come back to this point later when we evaluate the performance of our system. Idiom variation patterns There are studies and reference books that describe (English) idiom variations at a variety of levels (Benson, 1985; Biber, 1999; Cˇ erma´k, 1970; Fraser, 1970; Moon, 1998; Nicolas, 1995; Numberg et. al., 1994; Quirk et. al., 1985). On the basis of these, and taking into account the comments we obtained from translators, we ﬁrst classiﬁed the idiom variation patterns as follows: (1) Type variants or families: Types of idiom variations that are (or theoretically should be) registered in dictionary entries. An example is “run around [round] like a bluearsed ﬂy”. From the practical point of view, this type of variant can be dealt with by the variation indications in idiom entries in dictionaries. (2) Variations created by external factors: Passivisation (“the breeze was shot”) and topicalisation (“It is these strings that he pulled”) are typical examples of this type of variation. These variations are generally created by applying syntactic operations deﬁned outside the idioms themselves, and could be dealt with uniformly by a few basic rules. (3) Variations applied to parts or within the construction of idioms: many syntagmatic insertions (“go halves” → “go exact halves”) and paradigmatic replacements (“head screwed on right” → “head screwed on wrong/left”) fall under this category. This type of variation is expected to be neither straightforwardly clear nor completely unmanageable. (4) Highly creative variations: “point of view” → “ballpoint pen of view”. This class of variation is expected to be unmanageable for the time being. We focus on the third category (3) of idiom variations in this work, for reasons mentioned above. In order to develop a look-up function for idiom entries, existing linguistic studies have three limitations: (i) as mentioned, they tend to be too restrictive from the point of view of the reality of texts that translators deal with; (ii)  the description of variations is not given in a computationally tractable way; and (iii) the number of variations given in these studies is small3. Given the dearth of basic idiom variation data, we started by constructing idiom variation data manually. We took idiom entries from a widely-used English-Japanese idiom dictionary (McCaleb and Iwasaki, 2003), and asked three native English speakers (two of whom were professional editors) to create idiom variations with examples. We asked the informants to imagine they were writing or editing articles in the culture section of newspapers and be as creative as possible within that restriction. Table 1 shows the basic quantities of the idiom variation data. The quantity of data is rather small, and we are intending to augment it further in the same manner, asking informants to construct variations.  Informants h j s Total  (a) idioms 475 661 777 1913  (b) variants 469 890 822 2181  (b)/(a) 0.99 1.35 1.06 1.14  Table 1: Basic quantities of idiom variation data (‘h’, ‘j’ and ‘s’ indicate the three informants)  Note, however, that using large corpora to collect the data is not our priority, for a few reasons: (i) it is difﬁcult to extract idiom variations from large corpora, except for easily predictable regular types which could be covered by rules; (ii) it is difﬁcult to identify the threshold of possible variations, which tend to occur in low frequencies (translators do not choose a text by the representativeness of the language in the text or by the overall frequencies in the corpora of idiom variations used in the text); (iii) translators are dealing with individual texts and not representative language expressions, so frequencies in large corpora do not automatically mean importance for translators; and (iv) the frequency of idiom variations detected in large corpora correlates with the frequency of individual idioms, and frequently occurring idioms tend to be the ones that translators are least interested in and therefore the variations of which are less important from the translators’ point of view. Our intention in referring to the data is not to observe common usage or dominant patterns but to deﬁne the tractable range of variation patterns, which would hopefully correspond to the range of practically possible variations; we have no interest in covering only frequently occurring patterns at the expense of less frequent but computationally tractable patterns. As such, it is expected that the use of large corpora would not cover up the shortcomings of the size of the manually constructed data. For instance, one of the informants provided the variation “take the wild plunge” of the idiom “take the plunge,” which only brings up four hits in a Google search. From the point of view of translators, this and other rare variations, if technically possible, should be covered when they occur4. That our manually-constructed 3Some reference books (e.g. Oxford, 2001; Collins, 2002) give useful information, though not fully for variation patterns, for some class of idiomatic expressions, so we referred to them whenever was useful. 4One of the translators we spoke with commented: “Linguists?  Type (tag) Paradigmatic replacement Syntagmatic augmentation Deletion Dependent multiple replacement Dependent multiple augmentation Replacement and augmentation Deletion and replacement Deletion and augmentation Others  Example the boiling point → the burning point take off → take right off not get to ﬁrst base → got to ﬁrst base more dead than alive → more alive than dead can swing it → can swing it no problem weak as a baby → strong as a baby ox go back to the basics → plunge into the basics people will talk → people happily talk take it from me → rely on me  # 759 1203 3 39 101 91 20 8 95  Table 2: Broad classiﬁcation of idiom variations  dataset includes such rare cases, therefore, is not a demerit but a merit (on condition, of course, that basic patterns are covered). On the other hand, one might argue that this will result in the system potentially facing the problem of overmatching. But rare cases, if they do not occur, do not cause problems, because the basic task here is matching dictionary entries to textual occurrences, i.e. both ends are given. This manually-constructed data was then analysed and variation patterns were identiﬁed (Kageura and Toyoshima, 2006). Table 2 shows the basic variation patterns. In the table, such types as “dependent multiple replacement” etc. indicate that more than one type of mutually dependent variations was observed. As can be seen from Table 2, the major patterns are syntagmatic augmentations and paradigmatic replacement. Here we focus on variations by syntagmatic augmentation, and formalise the descriptions of syntagmatic augmentation patterns. In doing so, we assume the use of POS-taggers and morphological analysers. Though high-performance parsers exist, we did not use them for two main reasons: (i) idiomatic expressions often cross over the border of constituents given by parsers (in which case we would need to ﬂatten the parse tree anyway), and (ii) to achieve recall as close to 100% as possible is most important, and for that aim the loose deﬁnition of patterns provides a better starting point than the rigid description of variations using structural information. There are two different approaches for describing POS level patterns for variations of syntagmatic augmentations: (a) taking all constituents of the idiom into account, or (b) taking only binary constituents adjacent to words inserted into the idiom. For instance, assuming that the idiom expression “take the plunge” has as a variation “take the wild plunge”, we can describe the POS level patterns as “Verb Det Noun” → “Verb Det Adj Noun” in approach (a), or “Det Noun” → “Det Adj Noun” in approach (b). In the current work we took approach (b) because we assume that: (i) inserted words are mostly bound by the adjacent words and their grammatical categories; (ii) to take into account the overall grammatical patterns would immediately lead us to taking into account the individual idioms with lexical substance; and (iii) the requirement of high recall is of utmost importance at the current stage. Using the POS-information of adjacent elements, we for- Ah, those who cannot read literary texts but still have the audacity to think themselves to be language specialists!” As computational linguists we should try to bridge this gap between linguists and translators.  mulated the basic patterns of idiom variations by syntagmatic augmentation as shown in Table 3 (“Prep”, “Adj”, “Adv”, “PosPro” and “PerPro” denote preposition, adjective, adverb, possessive pronoun (e.g. “my”, “her” etc.) and personal pronoun (e.g. “I”, “he” etc.), respectively).  POS tags of constituents of idioms (Noun, Noun) (Noun, Prep) (Noun, Adj) (Noun, Verb) (Adj, Noun) (Adj, Prep) (Adv, Adj) (Adv, Adv) (Adv, Prep) (Adv, Verb) (Conj, Verb) (Conj, Prep) (Conj, Noun) (Verb, Adv) (Verb, Noun) (Verb, Adj) (Verb, Prep) (Prep, Noun) (Prep, Adj) (Det, Adj) (Det, Noun) (PosPro, Noun) (PerPro, Noun) (PerPro, Adv) (PerPro, Prep)  POS of inserted word Noun, Adj Noun, Adj, Adv Adj, Adv Adv, Aux Noun, Adj, Adv Noun Adv Adv Adv Adv Adv Adv Adj Adv, Adj Noun, Adj Adv, Adj Adv, Adj Noun, Adj Adv, Adj, Noun Adj, Adv, Noun Noun, Adj, Adv Adj, Noun Adj Adv Adj, Adv  Table 3: Idiom variation rules for insertion Each variation rule in Table 3 indicates the POS sequence of constituents of an idiom and the POS tag of the inserted word. For instance, the POS pattern of (Det, Noun) in constituents of an idiom can take either a noun, adjective or adverb. This rule covers the variation from “take the plunge” to “take the wild plunge”. Note that the described range of variation patterns, when incorporated into automatic matching algorithms, can be overgenerative. We can, however, reasonably expect that the overmatching will be within the manageable range, because, as mentioned, the computational problem is deﬁned here as a problem of matching when both ends are given, rather than  a problem of generating acceptable variations. The idiom look-up system The system consists of three processing modules: (1) the preprocessing module in which the input text is processed to facilitate automatic matching, including POS-tagging and normalisation of expressions, (2) the surface matching module in which all the possible idiom entries are detected by using AND matching of constituent elements of idioms with words occurring in texts, and (3) the ﬁltering module in which the undesired candidates detected in the surface matching module are ﬁltered out by using the POS-based variation restriction rules constructed based on the patterns given in Table 3. The input of the target system is an English text and the output is idiom candidates occurring in the text with their Japanese translations provided in the dictionary. Figure 1 shows the overall ﬂow of the system. We will elaborate each of these modules below and illustrate the system interface.  Input text (in English) Pre-processing of the text Pre-processed text Surface matching Idiom candidates Rule-based filtering Output (list of idioms)  Idiom entries in a dictionary  (4) Personal pronouns are standardised into basic forms.  Table 4 shows the basic standardisation patterns of word forms. Note that in the actual matching, we also retain the original forms. In addition to these, we applied a small amount of pre-processing such as splitting hyphenated words, etc.  Input word (1) verb (2) plural noun (3) particle (4) my, his, etc. (4) myself, herself, etc.  Pattern surface, basic form, do, doing surface, singular noun a, an, the surface, one’s surface, oneself  Table 4: Standardisation of words  Surface matching In the surface matching module, we carry out an extensive retrieval in which all the possible idiom candidates can be detected. We retrieve idiom entries whose constituents all match the textual sequences of words in order. In the dictionary entries, such examples as “make A of B” or “have ... in” exist. In the surface matching module, we deal with these “position ﬁllers” as wild cards. Figure 2 shows an example of surface matching. In Figure 2, the dictionary entry “have one’s eye on” is detected as an idiom candidate, because its constituent words all match the input words in the text.  - when an idiom is detected  input sentence  ...... had his eye on ......  Figure 1: The overall ﬂow of the system  idiom entry in dictionary  O have one’s eye on  Pre-processing In the pre-processing module, we ﬁrst assign POS information to the input text by using Tree-tagger (TreeTagger, 2004). After that, we apply the standardisation rules to the surface word forms that occur in texts. This is because, in many cases, the word form in the text is different from the word form in dictionary entries. For example, “took his seat” can be found in the text, but what is registered in the dictionary is “take one’s seat”. We adjusted the word form in the text so that it could be matched to the dictionary entries. Four types of formal standardisations are applied at this stage: (1) Inﬂected forms of verbs are transformed into basic forms. In addition, we added “do” or “doing” to absorb the matching of idioms whose entries are registered as something like “cannot help doing” in the dictionary. 
In this work, we present a MT system from Turkmen to Turkish. Our system exploits the similarity of the languages by using a modified version of direct translation method. However, the complex inflectional and derivational morphology of the Turkic languages necessitate special treatment for word-by-word translation model. We also employ morphology-aware multi-word processing and statistical disambiguation processes in our system. We believe that this approach is valid for most of the Turkic languages and the architecture implemented using FSTs can be easily extended to those languages.  Introduction Implementing a fully-automatic machine translation (MT) system capable of producing high-quality translations and handling unrestricted text, is still one of the most challenging tasks in natural language processing community. The more apart the source language (SL) and target language (TL) in terms of lexical, morphological and syntactical structures, the harder is the translation process (Nagao 1984; Jurafsky & Martin 2000). Recent advances in statistical machine translation has provide a new avenue to alleviate the complexities of MT but such systems rely on the availability of large amounts of parallel text which is not available for many language pairs. On the other hand, MT between close language pairs can be relatively easier and can still benefit from simple(r) paradigms in MT. In this paper, we present a MT system from Turkmen language to Turkish. Both SL and TL are cognate Turkic languages sharing very similar linguistic features such as agglutinative morphology and word order. Our system is designed to translate using a direct translation motivated method augmented with a disambiguation post-processing stage based on statistical language models. The very productive inflectional and derivational morphology of Turkic languages, however, necessitate considerable modifications be made for not only conventional components of standard direct translation model but also in the application of statistical language modelling techniques. We start with a summary of previous work on MT between related languages. Following that, we present brief information about Turkmen and Turkish, together with the common linguistic properties and divergences. We then describe the details of our MT system and define the main components of the implementation. Finally, we explain our evaluation methodology and give resulting scores and sample translations. The last section is devoted to the conclusions and future directions.  Related Work While MT has had a long history, work on MT between close language pairs is relatively recent. As far as we know, the first effort was the RUSLAN project which aimed at translating main-frame documents from Czech to Russian (Hajič 1987). Experience from this project was used in another MT project, Česilko, between two Slavonic languages, Czech and Slovak (Hajič et al. 2000). This work was extended to cover some other Slavonic languages like Polish and Lower Serbian (Dvořák et al. 2006) and Lithuanian (Hajič et al. 2003), which is actually a Baltic language. Following the development of interNOSTRUM project translating between Catalan and Spanish (Canals-Marote et al. 2000), additional work for other Romance Language pairs emerged. A Portuguese-Spanish MT system was implemented in the same manner (GarridoAlenda et al. 2003). As a result of these projects, an open source shallow-transfer MT engine for the Romance languages of Spain has been implemented and made available to the public (M.Corbi-Bellot et al. 2005).All of the systems mentioned above have very similar word-byword translation architectures that have four basic components: (1) a morphological analyzer, (2) a POSTagger, (3) a transfer module and (4) a morphological generator. As they operate on very close language pairs for which syntactical analysis stage is unnecessary during translation, a word-by-word translation is usually adequate (except for the Czech-Lithuanian case where a shallow parser was used). Even homonyms preserve their homonymy after translation, so one-to-one word mapping works fine for these language pairs. Work on MT between Turkic languages is also relatively limited. Hamzaoğlu (1993) presented a lexicon based MT system from Turkish to Azerbaijani, which is probably the language closest to Turkish. A more recent work involves a Turkish to Crimean Tatar MT system which is able to generate ambiguous translations with a limited dictionary (Altıntaş & Çiçekli 2001).  Turkish and Turkmen Languages As a subfamily of Ural-Altaic language family, Turkic languages comprise over 40 languages: Turkish, Azerbaijani, Uzbek, Turkmen, Kyrgyz, Kazakh, Uighur, Chagatai, Karagas, Tatar, Yakut, Chuvash being the more prominent ones1. Turkish is the largest Turkic language having more than 70 million native speakers while Turkmen language is used by approximately 11 million people. Turkmen language shares many common linguistic properties with Turkish to some extent. However, many divergences due to regional and historical reasons prevent the mutual intelligibility across these languages.  Morphology Both Turkish and Turkmen language have very productive derivational and inflectional morphologies where suffixes are affixed to a root word or another suffix (Oflazer 1995). Here are two examples word formation in these languages:2  Turkish : sağlamlaştırdık  (we made it strong)  sağlam+Adj ^DB+Verb+Become ^DB+Verb+Caus+Pos+Past+A1pl  Turkmen : baglanyşyklydyr  ((it) is related to …)  baglanyşyk+Noun+A3sg+Pnon+Nom ^DB+Adj+With ^DB+Verb+Zero+Pres+Cop+A3sg  Although Turkmen and Turkish languages use different alphabets in their orthography, most of the morphophonetic rules are common. For example, both languages exhibit vowel harmony and consonant mutation rules.  Both of the languages include similar suffixes with same or very close semantics. However, divergences like different tense aspect moods or different subject-verb agreement properties are observed frequently. For instance, the +makçı/+mekçi Turkmen suffix3 does not have counterpart in Turkish. Similarly, the definite future tense suffix attached to a Turkmen verb never accepts a person agreement marker after it; on the contrary, such a marker is a must in Turkish.  Turkmen morphotactics is very similar to Turkish, essentially for nominals. However, Turkmen verbal morphotactics differ from Turkish in many cases like denoting polarity and the order of causative, passive or reflexive suffixes.  The most problematic morphological issue for Turkic languages is ambiguity. Surface forms can be segmented in various ways and thus can result in different root words and/or different suffix combinations. An example of morphological ambiguity for the Turkish surface form izin is given below (Hakkani-Tür et al. 2002).  1. iz+Noun+A3sg+Pnon+Gen (trace/print) 2. iz+Noun+A3sg+P2sg+Nom (your trace/print) 3. izin+Noun+A3sg+Pnon+Nom (permission) Similar kinds of ambiguities are also observed for Turkmen language. Syntactic Structure All Turkic languages are free constituent order languages; phrases can be arranged freely within the sentence based on discourse requirements. However, the unmarked order is SOV. Morphological case markers of some constituents determine their grammatical role in the sentence. From the point of view of syntactical structure, almost one-to-one mapping can be observed between Turkish and Turkmen. However, word-by-word correspondence fails in many situations. Some Turkmen multi-word units (MWU) may be translated into Turkish as a single word. In many cases with adjective participles, it is inevitable to change the position of some morphemes among other words in the adjectival phrase. A sample alignment between a Turkmen sentence and a Turkish sentence is given below: Figure 1-Alignment Example In this alignment, one can readily see the replacement of Turkmen +iñiz morpheme4 with its Turkish equivalent +iniz, and also the positional change of the morpheme from the noun to the participle adjectival form. Additionally, a typical instance of a case where SL MWU is aligned with a single TL word, occurs in the end of the example sentence. These and many other examples show that in spite of the syntactical similarities, word-by-word translation is not sufficient solely, and additional sentence level processing must be employed. Lexicon Since the origins of the Turkic languages are same, their lexicons share considerable amount of root words sometimes with only minor variations. Most of the variations are observed in orthography whereas spoken languages have more common patterns. Personal pronouns, date/time expressions, organ names, main color names, numbers are nearly same for all Turkic languages. As an example, the personal pronoun ben (I) in Turkish is preserved in most of the Turkic languages as its original state or with small phonetic variations (like men in Turkmen).  
Data-driven approaches to machine translation (MT) achieve state-of-the-art results. Many syntax-aware approaches, such as ExampleBased MT and Data-Oriented Translation, make use of tree pairs aligned at sub-sentential level. Obtaining sub-sentential alignments manually is time-consuming and error-prone, and requires expert knowledge of both source and target languages. We propose a novel, language pair-independent algorithm which automatically induces alignments between phrase-structure trees. We evaluate the alignments themselves against a manually aligned gold standard, and perform an extrinsic evaluation by using the aligned data to train and test a DOT system. Our results show that translation accuracy is comparable to that of the same translation system trained on manually aligned data, and coverage improves.  1.  Introduction  The majority of approaches to data-driven Machine Translation (MT) focus on string-to-string models, despite the fact that tree-to-tree models achieve promising results (Hearne & Way, 2006; Nesson et al., 2006). Some tree-totree models, such as Data-Oriented Translation (DOT) (Poutsma, 2003; Hearne & Way, 2003, 2006), require source and target tree pairs that are aligned at subsentential level. In most previous experiments with DOT systems the training tree pairs were aligned manually. However, such a task is time-consuming and error-prone, and requires considerable expertise in both the source and target languages, and so there is an obvious need to induce the alignments automatically. A considerable amount of research has been carried out on the subject of sub-sentential alignment between structured representations of sentence pairs. However, many of the solutions presented share one or both of the following characteristics: (i) the alignment process is tightly coupled with the intended application, to the extent that it is difﬁcult to see how to generalise the alignment methodology so that the output could be used for other applications; (ii) the alignment strategy edits the source and/or target linguistic representations such that the original linguistic structures cannot be retrieved. We present a novel, language pair-independent and taskindependent algorithm whose output may be useful in many applications. The algorithm induces alignments between paired linguistic structures from which the constituent surface word order can be determined. It handles complex, non-isomorphic structures in a fast and consistent manner, and the resulting output can be ported to many other translation tasks such as Phrase-Based Statistical MT, Example-Based MT, DOT and translation template extraction. We describe experiments where we apply our algorithm to context-free phrase-structure tree pairs. We evaluate the alignments themselves against a manually aligned goldstandard, and also perform an extrinsic evaluation by using the aligned data to train and test a DOT system. Our results show that translation accuracy is comparable to that of the same translation system trained on manually  aligned data from English to French, and coverage improves signiﬁcantly. The remainder of this paper is organised as follows: Section 2 details related work and in Section 3 we present our novel alignment algorithm. Section 4 describes our experiments including the MT system used, and ﬁnally in Sections 5 and 6 we conclude and discuss avenues for further research.  2.  Related Work  Previous approaches to automatic sub-sentential alignment can be loosely grouped according to whether they focus on aligning dependency structures or phrasestructure trees. Many approaches do not view alignment as an independent task, but rather as a means to achieving another goal such as solving parse ambiguities or acquiring translation templates. Some such approaches view factors like non-isomorphism as obstacles, and alter the trees as part of the alignment process. Other related work in the area of alignment in general views the use of tree structures as a negative aspect which may result in the loss of generalisation ability (Wellington et al., 2006). However, we chose to align pre-determined tree structures without editing them; our motivation is that the structural and translational divergences that exist between source and target structures should be captured during the alignments process rather than smoothed away in order to allow for higher recall, cf. (Hearne et al., 2007). We do not view the parse trees as constraints, but rather as accurate syntactic representations of the text which can help to guide the alignment process.  2.1. Dependency Structures We are particularly interested in aligning phrase-structure trees, but solutions which have been applied to the alignment of dependency structures are also relevant. Ding et al. (2003) present a strategy for inducing word alignments over dependency structures. However, dependency analyses contain only lexically headed phrases, and we want to capture links between non-lexically headed phrases not described in dependency representations. Matsumoto et al. (1993) induce alignments in dependency structures, but with the intention of using the align-  ments to resolve parse ambiguities. Their algorithm only aligns simple sentences: alignment of complex sentences is done by ﬁrst segmenting the sentence into smaller chunks and then aligning those chunks, and the original tree structures are not retrievable. Eisner (2003) develops a tree-mapping algorithm for use on dependency structures which he claims is adaptable for use on phrase-structure trees. However, the alignment process is heavily linked to the translation strategy of which it forms part. We prefer alignment to be a separate ofﬂine process which can then be applied to numerous different tasks. 2.2. Phrase-structure Trees Groves et al. (2004) present a rule-based aligner which builds upon automatically induced word alignments. While their algorithm is in theory language pairindependent, in later experiments it performed poorly when evaluated on language pairs other than those used in development. Gildea (2003) proposes a method for aligning nonisomorphic phrase-structure trees using a stochastic treesubstitution grammar (STSG). This approach involves the altering of the tree structure in order to impose isomorphism, which impacts on its portability to other domains. Lu et al. (2001) describe a stochastic inversion transduction grammar, based on (Wu, 1995), which uses a monolingual grammar to parse the source sentence and builds a target language parse based on this, while simultaneously inducing alignments. These alignments are then extracted and converted into translation templates. Imposition of source language structure onto the target language is not always desirable. Nevertheless, on the evidence presented here, tree-to-string alignment models warrant further investigation. Wang et al. (2002) develop an interesting method for structural alignment which they call “bilingual chunking”. Given a pair of phrase-structure trees, they perform word alignment on the surface forms and then extract chunks from both trees simultaneously. The chunking is guided by the tree structure and constraints which ensure word alignments do not cross chunks. The chunks are then POS-tagged using an HMM tagger. Again, the original tree structures are lost during the alignment process. While the methods outlined above all achieve competitive results, those presented by Lu et al. (2001) and Wang et al. (2002) are most closely aligned with our objectives. 3. Our Sub-Tree Alignment Algorithm The novel algorithm we present here is designed to discover an optimal set of alignments between the tree pairs in a bilingual treebank while adhering to the following principles: (i) independence with respect to language pair and constituent labelling schema; (ii) preservation of the given tree structures; (iii) minimal external resources required; (iv) word-level alignments not ﬁxed a priori. The algorithm makes use of a single external resource, namely target-to-source and source-to-target word transla-  tion probabilities generated by running an automatic word aligner over the sentence pairs encoded in the bilingual treebank. The algorithm does not, however, ﬁx a priori on a single word-alignment between the source and target terminals of each sentence pair. Rather, word-level alignment decisions can be inﬂuenced by links made higher up in the tree pair. The alignment algorithm does not edit or transform the source and target trees in any way; signiﬁcant structural and translational divergences are to be expected and the aligned tree pair should encode these divergences (Hearne et al., 2007). Finally, the algorithm accesses no language-speciﬁc information beyond the (automatically induced) word-alignment probabilities and does not make use of the node labels in the tree pairs. 3.1. Alignment Well-Formedness Criteria Links are induced between tree pairs such that they meet the following well-formedness criteria: (i) a node can only be linked once; (ii) descendants of a source linked node may only link to descendants of its target linked counterpart; (iii) ancestors of a source linked node may only link to ancestors of its target linked counterpart. These criteria are akin to the “crossing constraints” described in (Wu, 1997) which forbid alignments between constituents that cross each other. Our criteria differ from those of Wu because we impose them on a pair of fully monolingually parsed trees, thus our criteria are more strict. The constraints in (Wu, 1997), on the other hand, are imposed inherently during the bilingual parsing and alignment process. In what follows, a hypothesised alignment is ill-formed with respect to the existing alignments if it violates any of these criteria. 3.2. Algorithm In this section we describe how our algorithm scores and selects links. In some instances, we present alternative methods by which a decision can be taken, and at the end of the section we summarise the corresponding set of possible aligner conﬁgurations. 3.2.1. Selecting Links For a given tree pair 〈S, T〉, the alignment process is initialised by proposing all links 〈s, t〉 between nodes in S and T as hypotheses and assigning scores γ(〈s, t〉) to them. All zero-scored hypotheses are blocked before the algorithm proceeds. The selection procedure then iteratively ﬁxes on the highest-scoring link, blocking all hypotheses that contradict this link and the link itself, until no nonblocked hypotheses remain. These initialisation and selection procedures are given in Algorithm 1 basic. Figure 1 illustrates the Algorithm 1 basic procedure. The constituents in the source and target tree pair are numbered. The numbers down the left margin of the grid correspond to the source constituents while the numbers across the top correspond to the target constituents, and each cell in the grid corresponds to a scored hypothesis. Within each cell, circles denote selected links and brackets denote blocked links. The number inside a given cell indi-  HEADER-1  PP-1  PP-2  COLON-9  P-2  P-3  NP-4  : P-3 D-5 P-6  from D-5  NP-6 a` partir de  a  N-7  N-8  Windows Application  NP-7  D-8  NP-9  une N-10  N-11  application Windows  
We describe a Japanese-English patent parallel corpus created from the Japanese and US patent data provided for the NTCIR-6 patent retrieval task. The corpus contains about 2 million sentence pairs that were aligned automatically. This is the largest Japanese-English parallel corpus, which will be available to the public after the 7th NTCIR workshop meeting. We estimated that about 97% of the sentence pairs were correct alignments and about 90% of the alignments were adequate translations whose English sentences reﬂected almost perfectly the contents of the corresponding Japanese sentences.  1. Introduction The rapid and steady progress in corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora, such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006) and the Europarl corpus (Koehn, 2005) consisting of 11 European languages. However, large parallel corpora do not exist for many language pairs. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Munteanu and Marcu (2005) have extracted parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Utiyama and Isahara (2003) have extracted JapaneseEnglish parallel sentences from a noisy-parallel corpus. We have recently aligned Japanese and English sentences in Japanese and US patent data provided for the NTCIR-6 patent retrieval task (Fujii et al., 2007). We used Utiyama and Isahara’s method to extract clean sentence alignments. The number of extracted sentence alignments was about 2 million. These sentence pairs and all alignment data that were produced during the alignment procedure are planed to be used in the NTCIR-7 patent MT task.1 This is the largest Japanese-English parallel corpus, which will be available to the public after the 7th NTCIR workshop meeting. In Section 2., we describe the resources used to develop the patent parallel corpus. In Sections 3., 4., and 5., we describe the alignment procedure, the basic statistics of the patent parallel corpus, and the MT experiments conducted on the patent corpus. 2. Resources Our patent parallel corpus was constructed from the patent data provided for the NTCIR-6 patent retrieval task. The patent data consists of • Unexamined Japanese patent applications published in 1993-2002 1We also plan to extend these alignment data with new patent data.  • USPTO patent data published in 1993-2000. The Japanese patent data consists of about 3.5 million documents, and the English data consists of about 1 million documents. We identiﬁed 84,677 USPTO patents that originated from Japanese patents by using the priority information described in the USPTO patents.2 We examined these 84,677 Japanese and English patent pairs and found that the “Detailed Description of the Preferred Embodiments” part (embodiment part for short) and the “Background of the Invention” part (background part for short) of each application tend to be literal translations of each other. We, thus, decided to use these parts to construct our patent parallel corpus. We used simple pattern matching programs to extract the embodiment and background parts from the whole document pairs and obtained 77,014 embodiment part pairs and 72,589 background part pairs. We then applied the alignment procedure described in Section 3. to these 149,603 pairs. We call these embodiment and background parts documents. 3. Alignment procedure 3.1. Score of sentence alignment We used Utiyama and Isahara’s method (Utiyama and Isahara, 2003) to score sentence alignments. We ﬁrst aligned sentences3 in each document by using a standard DP matching method (Gale and Church, 1993; Utsuro et al., 1994). We allowed 1-to-n, n-to-1 (0 ≤ n ≤ 5), or 2-to-2 alignments when aligning the sentences. A concise description of the algorithm used is described elsewhere (Utsuro et 2Some USPTO patents have priority information that identify foreign applications for the same subject matters. Higuchi et al. (2001) have used such corresponding patents ﬁled in both Japan and the United States to extract bilingual lexicons. 3We split the Japanese documents into sentences by using simple heuristics and split the English documents into sentences by using a maximum entropy sentence splitter available at http://www2.nict.go.jp/x/x161/members/ mutiyama/maxent-misc.html. We manually prepared about 12,000 English patent sentences to train this sentence splitter. The precision of the splitter was over 99% for our testset.  al., 1994).4 Here, we only discuss the similarities between Japanese and English sentences used to calculate scores of sentence alignments. Let Ji and Ei be the word tokens of the Japanese and English sentences for i-th alignment. The similarity between Ji and Ei is:5  2× SIM(Ji, Ei) =  δ(j,e) j∈Ji e∈Ei deg(j) deg(e) |Ji| + |Ei|  (1)  where j and e are word tokens and  |Ji| = no. of Japanese word tokens in i-th alignment  |Ei| = no. of English word tokens in i-th alignment  δ(j, e) = 1if j and e can be a translation pair otherwise 0  deg(j) = deg(e) =  e∈Ei δ(j, e) j∈Ji δ(j, e)  Ji and Ei were obtained as follows: We used ChaSen6 to morphologically analyze the Japanese sentences and extract content words, which consisted of Ji. We used a maximum entropy tagger7 to POS-tag the English sentences and extract content words. We also used WordNet’s library8 to obtain lemmas of the words, which consisted of Ei. To calculate δ(j, e), we looked up an EnglishJapanese dictionary that was created by combining en- tries from the EDR Japanese-English bilingual dictionary, the EDR English-Japanese bilingual dictionary, the EDR Japanese-English bilingual dictionary of technical terms, and the EDR English-Japanese bilingual dictionary of technical terms.9 The combined dictionary had over 450,000 entries. After obtaining the maximum similarity sentence align- ments using DP matching, we calculated the similarity between a Japanese document, J, and an English document, E, (AVSIM(J, E)), as deﬁned by (Utiyama and Isahara, 2003), using:  AVSIM(J, E) =  m i=1  SIM(Ji,  Ei)  m  (2)  where (J1, E1), (J2, E2), . . . (Jm, Em) are the sentence alignments obtained using DP matching. A high AVSIM(J, E) value occurs when the sentence alignments in J and E take high similarity values. Thus, AVSIM(J, E) measures the similarity between J and E. We also calculated the ratio of the number of sentences between J and E (R(J, E)) using:  R(J,  E)  =  min(  |J | |E|  ,  |E| |J |  )  (3)  4The sentence alignment program we used is avail- able at http://www2.nict.go.jp/x/x161/members/ mutiyama/software.html 5To penalize 1-to-0 and 0-to-1 alignments, we assigned SIM(Ji, Ei) = −1 to these alignments instead of the similarity obtained by using Eq. 1 6http://chasen-legacy.sourceforge.jp/ 7http://www2.nict.go.jp/x/x161/members/ mutiyama/maxent-misc.html 8http://wordnet.princeton.edu/ 9http://www2.nict.go.jp/r/r312/EDR/  where |J| is the number of sentences in J, and |E| is the number of sentences in E. A high R(J, E) value occurs when |J| ∼ |E|. Consequently, R(J, E) can be used to measure the literalness of translation between J and E in terms of the ratio of the number of sentences. Finally, we deﬁned the score of alignment Ji and Ei as Score(Ji, Ei) = SIM(Ji, Ei) × AVSIM(J, E) × R(J, E) (4) A high Score(Ji, Ei) value occurs when • sentences Ji and Ei are similar • documents J and E are similar • numbers of sentences |J| and |E| are similar Score(Ji, Ei) combines both sentence and document similarities to discriminate between correct and incorrect alignments.  3.2. Noise reduction in sentence alignments  We used the following procedure to reduce noise in the  sentence alignments obtained by using the previously de-  scribed aligning method on the 149,603 document pairs.  The number of sentence alignments obtained was about 7  million. From these alignments, we extracted only one-to-  one sentence alignments because this type of alignment is  the most important category for sentence alignment. As  a result, about 4.2 million one-to-one sentence alignments  were extracted. We sorted these alignments in decreasing  order of scores and removed alignments whose Japanese  sentences did not end with periods to reduce alignment  pairs considered as noise. We also removed all but one of  the identical alignments. Two individual alignments were  determined to be identical if they had the same Japanese  and English sentences. Consequently, the number of align-  ments obtained was about 3.9 million.  We examined 20 sentence alignments ranked between  1,999,981 and 2,000,000 from the 3.9 million alignments  to determine if they were accurate enough to be included in  a parallel corpus. We found that 17 of the 20 alignments  were almost literal translations of each other and 2 of the  20 alignments had more than 50% overlap in their contents.  We also examined 20 sentence alignments ranked between  2,499,981 and 2,500,000 and found that 13 of the 20 align-  ments were almost literal translations of each other and 6  of the 20 alignments had more than 50% overlap. Based on  these observations, we decided to extract the top 2 million  one-to-one sentence alignments. Finally, we removed some  sentence pairs from this top 2 million alignments that were  too long (more than 100 words in either sentence) or too  imbalanced  (  length length  of of  longer shorter  sentence sentence  >  5).  The  number  of sentence alignments thus obtained was 1,988,732. We  call these 1,988,732 sentence alignments the ALL data set  (ALL for short) in this paper.  We also asked a translation agency to check the validity of  1000 sentence alignments randomly extracted from ALL.  The translation agency conducted a two-step procedure for  veriﬁcation. In the ﬁrst step, they marked a sentence align-  ment as:  IPC G H B O Total  ALL (%) 19340 (37.9) 16145 (31.6) 7287 (14.3) 8287 (16.2) 51059 (100.0)  Source (%) 28849 (34.1) 24270 (28.7) 13418 (15.8) 18140 (21.4) 84677 (100.0)  Table 1: Number of patents  IPC G H B O Total  ALL (%) 946872 (47.6) 624406 (31.4) 204846 (10.3) 212608 (10.7) 1988732 (100.0)  Source (%) 1813078 (43.4) 1269608 (30.4) 536007 (12.8) 559519 (13.4) 4178212 (100.0)  Table 2: Number of sentence alignments  TRAIN DEV DEVTEST TEST Total  G 17524 630  610  576 19340  H 14683 487  493  482 16145  B 6642 201  226  218 7287  O 7515 262  246  264 8287  ALL 46364 1580  1575  1540 51059  Table 3: Number of patents  TRAIN DEV DEVTEST TEST Total  G 854136 33133 27505 32098 946872  H 566458 20125 19784 18039 624406  B 185778 6239  6865  5964 204846  O 193320 6232  6437  6619 212608  ALL 1799692 65729 60591 62720 1988732  Table 4: Number of sentence alignments  • A if the Japanese and English sentences matched as a whole. • B if these sentences had more than 50% overlap in their contents. • C otherwise. to check if the alignment was correct. The number of alignments marked as A was 973, B was 24 and C was 3. In the second step, they marked an alignment as: • A if the English sentence reﬂected almost perfectly the contents of the Japanese sentence. • B if about 80% of the contents were shared. • C if less than 80% of the contents were shared. • X if they could not determine the alignment as A, B, or C. to check if the alignment was an adequate translation pair. The number of alignments marked as A was 899, B was 72, C was 26, and X was 3. Based on these evaluations, we concluded that the sentence alignments in ALL are useful for training and testing MT systems. In the next section, we describe the basic statistics of this patent parallel corpus. In Section 5., we describe the MT experiments conducted on ALL. 4. Statistics of the patent parallel corpus 4.1. Comparison of ALL and source data sets We compared the statistics of ALL with those of the source patents and sentences from which ALL was extracted to see how ALL represented the sources. To achieve this, we used the primary international patent classiﬁcation (IPC) code assigned to each USPTO patent. The IPC consists of eight sections, ranging from A to H. We only used sections G (Physics), H (Electricity) and B (Performing operations; Transporting). We categorized patents as O (Other) if they were not covered by these three sections.  As described in Section 2., 84,677 patent pairs were ex-  tracted from the original patent data. These patents were  classiﬁed into G, H, B or O, as listed in the “Source” col-  umn of Table 1. We counted the number of patents included  in each section of ALL. We regarded a patent to be included  in ALL when some sentence pairs in that patent were in-  cluded in ALL. The number of such patents are listed in the  “ALL” column of Table 1. Table 1 shows that about 60%  (  51059 84677  ×  100)  of  the  source  patent  pairs  were  included  in  ALL. It also shows that the distributions of patents with  respect to the IPC code were similar between ALL and  Source.  Table 2 lists the numbers of one-to-one sentence alignments  in ALL and Source, where Source means the about 4.2  million one-to-one sentence alignments described in Sec-  tion 3.2. The results in this table show that about 47.6 %  (  1988732 4178212  ×  100)  sentence  alignments  were  included  in  ALL.  The results also show that the distribution of the sentence  alignments are similar between ALL and Source.  Based on these observations, we concluded that ALL rep-  resented Source well.  In the following, we use “G,”H,”B”,and “O” to denote the  data in ALL whose IPC were G, H, B, and O, respectively.  4.2. Basic statistics We measured the basic statistics of G, H, B, O, and ALL. We ﬁrst randomly divided patents from each of G, H, B and O into training (TRAIN), development (DEV), development test (DEVTEST), and test (TEST) data sets. One unit of the sampling was a single patent. That is, G, H, B and O consisted of 19340, 16145, 7287, and 8287 patents (See Table 1), and the patents from each group were divided into TRAIN, DEV, DEVTEST, and TEST. We assigned 91% of the patents to TRAIN, and 3% of the patents to DEV, DEVTEST, and TEST. We merged the TRAIN, DEV, DEVTEST and TEST of G, H, B, and O to create those of ALL. Table 3 lists the number of patents in these data sets and Table 4 lists the number of sentence alignments. We then counted the number of types (distinct words) and  TRAIN DEV DEVTEST TEST Whole  G 124804 18091 16909 17303 132939  H 86127 13915 13149 12975 91620  B 40556 7573  7974  7479 42685  O 47947 7941  7898  8335 50296  ALL 198076 27425 25853 26093 211265  Table 5: Number of types (English)  TRAIN DEV DEVTEST TEST Whole  G 77079 14671 13759 13932 81323  H 53307 11077 10740 10602 55899  B 30804 6642  6969  6514 32079  O 36129 6910  6994  7396 37577  ALL 116856 21276 20547 20504 123169  Table 6: Number of types (Japanese)  TRAIN DEV DEVTEST TEST Whole  G 27.75 1.08  0.90  1.04 30.76  H 18.47 0.66  0.64  0.59 20.35  B 6.27 0.21  0.23  0.20 6.92  O 6.52 0.21  0.22  0.22 7.17  ALL 59.01 2.15  1.99  2.05 65.20  Table 7: Number of tokens in million (English)  TRAIN DEV DEVTEST TEST Whole  G 30.15 1.18  0.97  1.14 33.44  H 20.16 0.72  0.71  0.64 22.22  B 6.76 0.23  0.25  0.22 7.47  O 7.04 0.22  0.23  0.24 7.74  ALL 64.12 2.35  2.16  2.24 70.87  Table 8: Number of tokens in million (Japanese)  tokens (running words) in these datasets. Tables 5 and 6 list the number of types for English and Japanese sentences. Tables 7 and 8 list the number of tokens. To count these numbers, we used ChaSen to segment Japanese sentences into tokens and used a simple tokenizer10 to tokenize English sentences. All English words were lowercased. In these tables, the ﬁgures in columns “TRAIN”, “DEV”, “DEVTEST” and “TEST” are the number of types and tokens in these datasets and the ﬁgures in the “Whole” columns are the number of types and tokens in each G, H, B, O, and ALL. 4.3. Statistics pertaining to MT We measured some statistics pertaining to MT. We ﬁrst measured the distribution of sentence length (in words) in ALL. The mode of the length (number of words) was 23 for the English sentences and was 27 for the Japanese sentences. Figure 1 shows the percentage of sentences for English (en) and Japanese (ja) with respect to their lengths. This ﬁgure shows that the distributions of sentence length were relatively ﬂat and that there were many long sentences  Percentage  3.5  en  3  ja  2.5  2  1.5  
This paper describes a project on building a Machine Translation system for television and ﬁlm subtitles. We report on the speciﬁc properties of the text genre, the language pair SwedishDanish, and the large training corpus. We focus on the evaluation of the system output against independent and post-edited translations. We show that evaluation results against post-edited translations are higher by a margin of up to 19 points BLEU score. 
 This paper comparatively analyzes six different word alignment heuristics and their impacts on translation quality. We also propose a  method to filter the noise in the phrase tables extracted by these heuristic methods and examine the effectiveness of combination of the  methods. Experiments are performed on the Europarl corpus, where a multilingual in-domain training corpus, an in-domain test set,  and an out-of-domain test set are available. Results indicate that (1) the heuristics show similar tendencies in the word alignment task  on both test sets, but they perform differently in the translation task on the in-domain and out-of-domain test sets; (2) in general, the  relationship between word alignment and machine translation performance is difficult to be predicted, depending on domains of the  training and testing corpora besides other factors such as evaluation metrics and the characteristics of translation systems; (3) noise  filtering and combination of these heuristic methods achieve larger improvement on the out-of-domain test set than on the in-domain  test set.  In addition, although Lopez and Resnik (2006) pointed  Introduction  out that it may be more useful to handle noise in phrase  Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been conducted to explore the relationship of word alignment quality measures and machine translation quality. The main points are concluded as follows. 1. It is difficult to find a direct correlation between word alignment measures (such as alignment error rate) and automated MT metrics (Ayan and Dorr, 2006; Fraser and Marcu, 2006). 2. Large gains in alignment performance under any metric are confirmed to achieve relatively small gains in translation performance (Lopez and Resnik, 2006). 3. Better feature mining can lead to substantial gain in translation quality (Lopez and Resnik, 2006). 4. It is better to generate alignments adapted to the characteristics of the translation models that will make use of this alignment information (Vilar et al., 2006). However, all of the above conclusions are made on the indomain test sets and never on the out-of-domain test sets.  extraction than to improve word alignment quality, they did not provide detailed information to verify this point. In this paper, we will use different heuristics to generate word alignments, and examine the impacts of these heuristics on machine translation quality. And then we will re-evaluate the relationship of word alignment and their impacts on machine translation quality on both indomain and out-of-domain test sets. Furthermore, in order to examine the noise in the phrase pairs extracted using different alignment heuristics, we propose a method to filter the noise in the phrase tables using association measures. And we will also investigate whether combining the phrase tables extracted by different heuristics improves translation quality. We performed experiments on the Europarl corpus (Koehn, 2005; Koehn and Monz, 2006), where a multilingual in-domain training corpus, an in-domain test set, and an out-of-domain test set are available. We obtained the following results: 1. Word alignment results show that the compromise method, which makes compromise between precision and recall, performs the best on both in-domain and out-of-domain alignment test sets. 2. Translation results indicate that the heuristic methods perform differently on the in-domain and out-of-domain test sets. On the in-domain test set, the recall-oriented heuristic methods yield better translation quality. On the out-of-domain test set, the precision-oriented heuristic methods yield better translation quality. On both of the test sets, the compromise method achieves satisfying translation quality. 3. The relationship between word alignment and machine translation performance depends on domains of the training and testing corpora besides other factors such as evaluation metrics and the characteristics of the translation systems used. 4. Filtering the noise in the phrase tables and combining different phrase tables achieve larger improvement on the out-of-domain test set than on the in-domain test set.  The remainder of this paper is organized as follows. First, we will describe phrase-based machine translation and the corresponding word alignment heuristics used in this paper. Then we will propose a method to filter the noise in phrase pairs. Following this, we will propose methods to combine the phrase pairs extracted by different methods. After that, we will present the experimental results. Lastly, we will conclude this paper.  Phrase-Based Statistical Machine Translation In phrase-based SMT systems, the unit of translation is any contiguous sequence words, which is called phrase. It includes two steps: training and translation. During training, parallel corpus is employed to induce phrase alignment in the sentence pairs and estimate translation probabilities. Target monolingual corpus is employed to train a language model. During translation, the source sentence is first segmented into phrases and then translated into target phrases using learned phrase pairs. The target phrases are then recombined to form a target sentence.  Log-Linear Model Given a source sentence f , the best target translation ebest can be obtained according to the following log-linear model  ebest = arg maxe p(e | f )  M  (1)  ≈ arg maxe ∑ λmhm (e, f)  m=1  Where hm (e, f) represents feature functions, and λm is the weight assigned to the corresponding feature function. In this paper, we will use the Pharaoh system (Koehn, 2004). Eight different features are used in this system. 1. a phrase translation probability 2. an inverse phrase translation probability 3. a lexical weight: measuring the quality of word alignment inside the phrase pair 4. an inverse lexical weight 5. language model 6. phrase penalty 7. word penalty 8. reordering  For phrase translation probability, lexical weight, and reordering, we use the same models in (Koehn et al., 2003). We use n-grams for language modelling. For the phrase penalty and word penalty, we use the same heuristics in (Zen and Ney, 2004).  Word Alignment Heuristics One important component used in the Pharaoh system is the phrase translation table. Since DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models for phrase pair extraction, we use heuristic methods in this paper. We first align the words in the training parallel corpus, extract phrase pairs that are consistent with the word alignments, and then assign probabilities to the obtained phrase pairs.  Word alignments are obtained by using the GIZA++ toolkit1 in both translation directions and then symmetrize the two alignments. In statistical translation models implemented in GIZA++, only one-to-one and more-toone word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). In this paper, we use six kinds of symmetrization methods. Let A1 and A2 represent the two alignments in source to target and target to source translation directions, the six symmetrization methods can be described as follows. 1. intersection: A = A1 ∩ A2 2. union: A = A1 ∪ A2 3. grow: the alignments in the intersection set of the two alignments are first added. And then neighboring alignment points in the union sets directly in the left, right, top, or bottom directions are added. 4. grow-diag: besides the neighboring points in the grow method, the diagonally neighboring alignment points are also included. 5. grow-diag-final: in addition to the alignment points in grow-diag, the non-neighboring alignment points between words, of which at least one is currently unaligned, are added in a final step. 6. grow-final: In addition to the alignment points in grow, the non-neighboring alignment points between words, of which at least one is currently unaligned, are added in a final step.  Phrase Extraction With the word alignment results obtained by the above six heuristic methods, we extract phrase pairs that satisfy the following restrictions: 1. all source words within a phrase are aligned only to target words within a phrase 2. all target words within a phrase are aligned only to source words within a phrase More formally, the set of bilingual phrases consistent with a word alignment A is defined as  BP( f1J ,e1I , A)  =  {(  f  j+m j  , eii+n  )  |  ∀(i',  j')  ∈  A  :  (2)  j ≤ j'≤ j + m ⇔ i ≤ i'≤ i + n}  The phrase translation probability is defined as  p( f | e) = count( f , e)  (3)  ∑ f ' count( f ', e)  Where count( f , e) describes the frequency of the phrase f is aligned with the phrase e in the parallel corpus. Given a phrase pair ( f , e) and a word alignment a between the source word positions i = 1,..., n and the target word positions j = 1,..., m , the lexical weight can be estimated according to the following method (Koehn et al., 2003).  
1. Introduction Statistical machine translation (SMT) addresses the problem of automatically translating a text in one language into a text in another language using machine learning techniques and statistical modeling approaches. In SMT, models are trained from parallel and monolingual corpora. The quality and quantity of the data and the underline modeling approach together mostly determines the quality of the translation output. With the increasing availability of parallel corpora and a better modeling approach, a signiﬁcant improvement of the translation quality has been achieved in the recent years. While translation performance has been advanced substantially in general, translation style and domain issue leave much room for further improvements. For instance, translating an utterance can be quite different than translating a written sentence in selecting words and phrases and their orders. Short phrases such as “what’s up” are more likely to be observed in an informal situation than in written form. This offers a challenge to genre adaptation of SMT systems but causes at the same time a rise to potential improvement if the issue can be handled properly. In this work, we approach domain adaptation in machine translation with classiﬁcation methods. Two main problems need to be solved: the ﬁrst one is how to build domain speciﬁc SMT systems in training; the second is how to perform domain adaptation during decoding. For the ﬁrst problem, we use the domain dependent language modeling or feature weights combination. When translating a test document, we are going to automatically identify its domain and then apply a corresponding decoding setup. Different text classiﬁcation methods are going to be investigated and compared. Furthermore, we are going to show their impact on translation performance. We are going to review our baseline translation system in section 2., then we are going to discuss how to build domain speciﬁc SMT systems in section 3. and how to do  domain adaptation during testing in section 4. In section 5., we are going to present the experimental setup and are going to show the classiﬁcation and the translation results, followed by the conclusions and future work.  2. Review of the Baseline Translation System In statistical machine translation, we are given a source (’Foreign’) language sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language (‘English’) sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we will choose the sentence with the highest probability:  eˆI1 = argmax P r(eI1|f1J ) eI1  = argmax P r(eI1) · P r(f1J |eI1)  (1)  eI1  The decomposition into two knowledge sources in Equation 1 is known as the source-channel approach to statistical machine translation (Brown et al., 1990). It allows an independent modeling of the target language model P r(eI1) and the translation model P r(f1J |eI1). The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. The notational convention will be as follows: we use the symbol P r(·) to denote general probability distributions with (nearly) no speciﬁc assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·).  2.1. Log-linear Model for SMT As an alternative to the classical source-channel model, the log-linear model (Och and Ney, 2003) has become popular and proved to be effective in directly modeling the distribution of a target sentence when given a source sentence:  P r(eI1|f1J )  =  
This paper reports on a pilot study of aspect marker generation in an English-to-Chinese translation scenario. Our classiﬁer combines a number of linguistic features in a Maximum Entropy framework and achieves an overall accuracy of 78%. We also investigate the impact of different clusters of linguistic features; we ﬁnd that syntactic features have the highest utility and lexical aspectual properties associated with verbs do not have signiﬁcant contribution to the prediction of aspect markers. Furthermore, we have demonstrated converging evidence that there is only marginal sequential dependency between the aspect markers of different verbs in the same sentence. 
Freely available tools and language resources were used to build the VoiceTRAN statistical machine translation (SMT) system. Various configuration variations of the system are presented and evaluated. The VoiceTRAN SMT system outperformed the baseline conventional rule-based MT system in both English-Slovenian in-domain test setups. To further increase the generalization capability of the translation model for lower-coverage out-of-domain test sentences, an “MSD-recombination” approach was proposed. This approach not only allows a better exploitation of conventional translation models, but also performs well in the more demanding translation direction; that is, into a highly inflectional language. Using this approach in the out-of-domain setup of the EnglishSlovenian JRC-ACQUIS task, we have achieved significant improvements in translation quality.  Introduction Machine translation (MT) systems automatically convert text strings from a source language (SL) into text strings in the target language (TL). They often allow for customization by application domain (e.g., weather), which improves the output by limiting the scope of allowable substitutions. This technique is particularly effective in domains in which formal or formulaic language is used, and therefore machine translation of government and legal documents more readily produces usable output than translation of less standardized texts or even spoken language. Some initial machine translation attempts have been reported for translation from Slovenian into English (Vičič, 2002; Romih & Holozan, 2002; Žganec Gros et al., 2006; Sepesy Maučec et al., 2006). However, very little has been done for the opposite translation direction, from English into Slovenian (Žganec Gros et al., 2006). We have performed experiments in both translation directions, in which especially the latter proved to be a demanding task due to the highly inflectional nature of Slovenian. This paper continues with a description of the VoiceTRAN statistical machine translation (SMT) experiment. The goal was to evaluate the performance of various SMT configuration variations against the performance of a baseline conventional rule-based MT system in order to find out, which MT system should be used in the VoiceTRAN speech-to-speech translation system. Language Resources The following language resources were used in the experiments. The bilingual language resources always refer to the English-Slovenian language pair: · bilingual text corpora: the VoiceTRAN applicationspecific corpus and two freely available corpora: the JRCACQUIS corpus (Steinberger et al. 2006) and the IJSELAN corpus (Erjavec, 2002);  · the monolingual FDV-IJS Slovenian corpus, collected at the University of Ljubljana, and annotated within the VoiceTRAN project; 
 2School of Computer Harbin Institute of Technology Harbin 150001 {hfjiang, lishen}@mtlab.hit.edu.cn  3School of Computing National University of Singapore Singapore 117543 {sunjun, tancl}@comp.nus.edu.sg  Abstract This paper presents a novel statistical machine translation (SMT) model that uses tree-to-tree alignment between a source parse tree and a target parse tree. The model is formally a probabilistic synchronous tree-substitution grammar (STSG) that is a collection of aligned elementary tree pairs with mapping probabilities (which are automatically learned from word-aligned bi-parsed parallel texts). Unlike previous syntax-based SMT models, this new model supports multi-level global structure distortion of the tree typology and can fully utilize the source and target parse tree structure features, which gives our system more expressive power and flexibility. The experimental results on the HIT bi-parsed text show that our method performs significantly better than Pharaoh, a state-of-the-art phrase-based SMT system, and other syntax-based methods, such as the synchronous CFG-based method on the small dataset.  Keywords: statistical machine translation, syntax-based statistical machine translation, tree-to-tree alignment, synchronous tree-substitution grammar, elementary tree  Motivation Phrase-based SMT Phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) to statistical machine translation (SMT) has recently achieved significant improvements in translation accuracy over the original IBM word-alignment-based model (Brown et al., 1993). In phrase-based models, a phrase can be any string of adjacent words without constraints imposed by any syntactic theory. These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context. These make it a simple and powerful mechanism for machine translation. However, there exist many open issues to be resolved in phrase-based models. For examples, the handling of discontiguous phrases and modeling of global reordering, estimation of phrase translation probabilities and phrase partition probabilities are not yet effectively addressed in phrase-based models (Quirk and Menezes, 2006). Much research has been carried out to look into the above issues. One natural extension is to utilize syntax-based structure features for SMT. Syntax-based SMT Recent work in SMT has evolved from the word-based and phrase-based models to syntax-based models, that include hierarchical phrase models (Wu, 1997; Chiang, 2007), bilingual synchronous grammars (Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006;) and other syntax-based models (Yamada and Knight, 2001; Gildea, 2003; Och et al, 2004b; Liu et al., 2006). Wu (1997) and Chiang (2007)’s methods are formally syntax-based, i.e., their methods are not informed by any linguistically syntactic theory. Wu (1997) proposes Inversion Transduction Grammars (ITGs, an instance of synchronous CFGs), treating translation as a process of parallel parsing of the source and target  languages via ITGs. Chiang (2007) uses a formal binary synchronous CFG to model hierarchical phrase structures. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Och et al (2004) explore using various morphologic and syntactic features to re-rank the translation outputs of a phrasebased system. Ding and Palmer (2005) propose a syntaxbased translation model based on a probabilistic synchronous dependency insertion grammar, a version of synchronous grammar defined on dependency trees. Quirk et al. (2005) propose a dependency treelet-based translation model. They project the source dependency parse onto the target sentence, extract dependency treelet translation pairs and train a tree-based ordering model. Cowan et al. (2006) propose a feature-based discriminative model for prediction of the target language syntactic structures, given the source language parse trees. Riezler and Maxwell III (2006) present an approach to SMT that combines ideas from phrase-based SMT and traditional grammar-based SMT. They incorporate the concept of multi-word translation units into transfer of dependency structure snippets, and model and train statistical components according to phrase-based SMT system. Zhang et al. (2006) study the synchronous rule binarization for MT. They devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rules set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. Zollmann and Venugopal (2006) present a syntax-based machine translation method that generates translation results by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. The motivation behind all these advances is to exploit syntactic structure features to model translation process: lexical selection, reordering, structure transfer and generation. Structural Divergences between Languages One of the major challenges in applying syntax to SMT is structural divergences between languages (Dorr, 1994),  which are due to either systematic differences between two languages in expressing a concept syntactically or relatively free translations in the training corpora. As a result, syntax-based MT systems have to transduce between non-isomorphic tree structures that is beyond the power of Synchronous CFGs (only sibling nodes are allowed to reorder independently prior to translation). For example, the S(VO) structure in English cannot be translated into a VSO word order in Arabic by any SCFGs. Many researchers have investigated and studied the above issues. Fox (2002) examines the issue of linguistic phrasal cohesion between English and French and discovers that while there is less cohesion than we might desire, there is still a large amount of regularity in constructions where breakdowns occur. The paper also examines the differences in cohesion phrase-structure-based parse tree, trees with flatten verb phrases and dependency structures, and concludes that the highest degree of cohesion is presented in dependency structures. Eisner (2003) studies how to learn non-isomorphic tree-to-tree or tree-to-string mappings for machine translation. The paper sketches an EM algorithm to learn the probabilities of elementary tree pairs by training on pairs of full trees, and a Viterbi decoder to find optimal translations. However, the above two papers do not verify their methods empirically on a real MT system. Gildea (2003) proposes a new subtree cloning operation to either tree-to-tree or tree-to-string alignment algorithms for MT. His method is evaluated on word alignment rather than machine translation. Galley et al. (2004) propose a theory that gives formal semantics to word-alignments defined over parallel corpora and use the theory to derive from word-aligned parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. They find that local transformations (primarily child-node re-orderings) of one-level parent-children substructures are an inadequate model for parallel corpora, so they learn rules involving much larger tree fragments. Melamed (2004) studies how to infer the synchronous structures hidden in parallel texts for the syntax-aware SMT by generalizing ordinary parsing algorithms to synchronous ones. Huang et al. (2006) study a tree substitution grammar-based treeto-string alignment model for SMT. Liu et al. (2006) propose a tree-to-string alignment template-based method for SMT. Wellington et al. (2006) study empirically the lower bounds on alignment failure rates with and without gaps for bilingual/monolingual bitexts under the constraints of word alignment alone or with one or both side parse trees. Their study finds surprisingly many examples of translational equivalence that could not be analyzed using binary-branching structures without discontinuities. Previous research discussed above suggests using more powerful grammars whose rules can be applied to larger tree fragments to address the non-isomorphic issue. Shieber and Schabes (1990) introduce synchronous treeadjoining grammar (STAG) preliminary for semantics and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG without adjunctions (Chiang, 2006), for machine translation. STAGs and STSGs use elementary tree structure, which is beyond the scope of two-level contextfree rules, to generate more tree relations than SCFGs.  Synchronous TSG-based Tree-to-Tree Alignment In this paper, we propose a synchronous TSG-based treeto-tree alignment model for machine translation. Specifically, we use elementary tree-based structure alignments, which are automatically learned from wordaligned bi-parsed parallel texts, to model the translation process. We separate the source language analysis from the recursive transformation. Therefore, to translate a source sentence, we first employ a CFG-based Treebank parser to produce a source parse tree and then use the set of learned elementary tree pairs to transform the source parse tree to a target parse tree, which is then used to generate target sentence. There are two major benefits of our STSG-based tree-totree alignment model. First, it is possible to explicitly model the syntax of the target language, thereby improve the grammaticality of target sentence. Second, our model has more expressive power and flexibility since it allows multi-level global structure distortion of the tree typology and fully utilizes source and target parse tree structure features. Therefore, it can solve the deficiencies in SCFG and phrase-based models such as non-isomorphic tree alignment, global reordering and discontiguous phrase. To the best of our knowledge, no previous work explores empirically STSG-based tree-to-tree alignment over phrase-structure parse trees for machine translation. Eisner (2003) studies STSG-based alignment on dependency trees, but no empirical verification on machine translation is done. Compared to Eisner (2003), we use different training and decoding algorithms and modeling methods. Graehl and Knight (2004) define tree transducers that have multi-level trees only on the sourceside. Yamada and Knight (2001) and Zollmann and Venugopal (2006) and Galley et al. (2004) only utilize target parse tree information. Ding and Palmer (2005) and Chris et al (2005) work on dependency grammars while Huang et al (2006) and Liu et al (2006) work on tree-tostring alignment models. Our method, in terms of modeling, training and decoding algorithms are different from theirs at one or more points. In the rest of this paper, we elaborate our modeling, training and decoding methods and report our experimental results in detail. Tree-to-Tree Alignment-based Model In this section, we first introduce what STSG is and then based on which we define our tree-to-tree alignmentbased SMT model. Finally, we present the modeling process based on log-linear framework. Synchronous TSG (STSG) for SMT Shieber (2004) gives a formal and general definition of STSG. Here we give a more concrete definition of STSG with respect to its application in SMT. A STSG is a septet G =< Σ s, Σ t, Ns, Nt, Ss, St, P > , where: • Σ s and Σ t are source and target terminal alphabets (POSs or lexical words), respectively, and • Ns and Nt are source and target non-terminal alphabets (linguistic phrase tag, i.e., NP/VP…), respectively, and  Figure 1: A word-aligned parse tree pairs of a Chinese sentence and its English translation  Figure 2: Examples of elementary trees  VP VBA  VO ,  NP PP  E1:  P NG 1 VG 2 R  VBP 2 DT  NN 1 TO PRP  把  我1  (NULL)  (me)  the  to me 1  NG NN ， E2: 钢笔 1 pen 1 (pen)  VG VBP ， E3: 给 1 Give 1 (give)  Figure 3: Three examples of PET  • Ss ∈ Ns and St ∈ Nt are the source and target start symbols (roots of source and target parse trees), and • P is a production rule set, where a production rule is a pair of elementary tree (ξ s ↔ ξ t ) with linking relation between leaf nodes in source elementary tree ( ξ s ) and leaf nodes in target elementary tree ( ξ t ).  In TSG and STSG, an elementary tree is a tree fragment whose leaf nodes can be either non-terminal symbols or terminal symbols. For example, Figure 2 illustrates two  examples of elementary trees which belong to the English parse tree Tt shown in Figure 1. Obviously, a normal subtree (whose leaf nodes must be terminal symbols) is an elementary tree but not always true vice versa.  In STSG, a production or a rule is a pair of elementary tree with alignment information (hereafter, PET). We can define a PET as a triple < ξ s ,ξ t , A >, where: • ξ s is a source elementary tree, and • ξ t is a target elementary tree, and • A is the alignments between leaf nodes of two elementary trees. It is defined as a subset of the Cartesian product of source and target leaf node positions: A ⊆ {(i, j) : i is the position of ith leaf node of ξ s ; j is the position of jth leaf node of ξ t } Figure 3 shows three examples of PET extracted from the word-aligned parse tree pair in Figure 1. We use boxed and circled indices to indicate non-terminal and terminal alignments, respectively. Obviously, PET allows any tree node insertion, deletion and substitution between the two elementary trees. We believe this property of PET can well address the issues of non-isomorphic structures, global reordering and phrase gaps that we discussed in the previous section.  STSG-based Tree-to-Tree Alignment We use a STSG to represent tree-to-tree alignment, i.e., a STSG-based tree-to-tree alignment template is a PET < ξ s , ξ t , A >. In the following, we formally describe how to develop PETs into probabilistic dependencies to model the translation process.  Given the source and target sentences f1J and e1I , we first introduce two hidden variable Ts and Tt that denote the source and target parse trees, respectively, then we have1:  ∑ Pr(e1I | f1J ) = Pr(e1I ,Tt ,Ts | f1J ) Ts ,Tt ∑ = (Pr(Ts | f1J ) ⋅ Pr(Tt | Ts , f1J ) (1) Ts ,Tt ⋅ Pr(e1I | Tt ,Ts , f1J ))  Next, we introduce another hidden variable D to detach  Ts  and Tt  into  a  sequence  of  K  PETs  <  ξ  1, s  K  ,  ξ  1, t  K  ,  A1,K  >.  We  assume  that  each  source  elementary  tree  ξ  i s  produces  a target elementary tree ξ ti independently and they are  aligned by A i . Then, we have:  
 Abstract The statistical machine translation (SMT) approach has taken a lead place in the field of Machine Translation for its better translation quality and lower cost in training compared to other approaches. However, due to the high demand of computing resources, an SMT system can not be directly run on hand-held devices. Most existing hand-held translation systems are either interlingua-based, which require non-trivial human efforts to write grammar rules, or using the client/server architecture, which are constrained by the availability of wireless connections. In this paper we present PanDoRA, a two-way phrase-based statistical machine translation system for stand-alone hand-held devices. Powered by special designs such as integerized computation and compact data structure, PanDoRA can translate dialogue speech on off-the-shelf PDAs in real time. PanDoRA uses 64K words vocabulary and millions of phrase pairs for each translation directions. To our knowledge, PanDoRA is the first large-scale SMT system with build-in reordering models running on hand-held devices. We have successfully developed several speech-to-speech translation systems using PanDoRA and our experiments show that PanDoRA's translation quality is comparable to that of the state-of-the-art phrase-based statistical machine translation systems such as Pharaoh and STTK.  Introduction The world today sees great demands of portable translation devices. Speech translation systems running on hand-held devices are of great interest to international tourism, business and humanitarian aids.  pairs1. On a typical set up, PanDoRA translates a sentence in about 10ms on a PDA. Its translation quality is comparable to the state-of-the-art SMT systems.  Statistical machine translation (SMT), especially the phrase-based SMT has shown great advantages over other MT approaches in recent years for its translation quality and its ease to be adapted to new language pairs and new domains. As an emerging trend, SMT systems have been used in many areas such as webpage translation, live broadcasting translation, lecture translation and so on.  To use an SMT system on a hand-held device, however, is not that easy. SMT systems use large amount of data to train the statistical models. The resulting models could easily go up to several gigabytes when loaded into the memory. Hand-held devices, such as mobile phones and PDAs, have very limited dynamic memory. In addition, the CPUs on most hand-held devices are weak. Their frequencies are less than 1/4 of those used in the regular PCs and they do not have numerical co-processors, which are critical for calculating various probabilities in SMT systems. All these restrictions make it a great challenge to develop a practical SMT system for hand-held devices. In this paper, we present PanDoRA, a two-way phrasebased SMT system for hand-held devices (Figure 1). PanDoRA has been successfully applied to PDA-based speech-to-speech translation systems for various language pairs, including Arabic/English, Chinese/English, Japanese/English, Spanish/English, and Thai/English, in tourist, medical aid and force protection domains. PanDoRA uses two translation models, one for each translation direction and each can have millions of phrase  Figure 1: PanDoRA system running on a PDA  Vocabulary  Source Language Target Language  Src. → Tgt. pairs  Translation Tgt. → Src. pairs  Model Uniq. Src. phrases  Uniq. Tgt. phrases  Language Type  Model Size  Up to 64K Up to 64K Up to 4 billion Up to 4 billion Up to 256 million Up to 256 million 3-gram LM No limitation2  Table 1: Technical Specifications of PanDoRA  1The data structure allows 4 billion phrase pairs for each translation direction. This theoretic bound is subject to the capacity limitations of the storage devices. 2 The size of the language model is subject to the storage capacity limitation of the device running the system.  The remainder of this paper is organized as follows: we first describe the general concepts of phrase-based statistical machine translation systems and then we introduce the PanDoRA system and its major components. We show the performance of PanDoRA system running on a PDA with standard training/testing data sets and discuss the results in the experiments section.  Phrase-based Statistical Machine Translation In statistical machine translation (SMT), we are given a source language sentence f1J = f1... f j... f J , which is to be translated into a target language sentence e1I = e1...ei...eI . Among all possible target language sentences, the decoder will choose the one with the highest probability such that the output translation:  e* = arg max P(e | f1J ) e = arg max P( f1J | e) ⋅ P(e) e  (Eq. 1)  The decomposition of P(e | f ) in Eq. 1 is based on the source-channel approach which allows us to make use of two types of knowledge sources: translation model (TM) P( f | e) and language model (LM) P(e) . TM models how likely a source sentence is the translation of the target sentence and LM describes the well-formedness of the generated translation.  The original SMT work described in Brown et al. (1990) models the translation process as a word-to-word mapping. In recent years, various approaches have been developed to use phrase-to-phrase translation models to encapsulate more local context inside the phrases during the translation process (Och et al. 1999; Zhang et al. 2003; Koehn et al. 2003; Vogel 2005). The so-called “phrases” are not linguistically motivated and they could be n-grams running across linguistic constituent boundaries such as phrase “the spokesman said today at.” Phrase-based SMT systems outperform word-based systems and -- despite their lack of linguistic grounding -- have become one of the dominant approaches in machine translation research. Figure 2 shows some examples of Arabic→English phrase translation pairs extracted automatically from the bilingual training data using the PESA method (Vogel 2005).  ‫ احتفال المدرسة‬# school festival is # 0.0034 ‫ احتفال المدرسة‬# school festival is hold # 0.0031 ‫ احتفال المدرسة عقد‬# school festival is hold # 0.9980 ‫ احتفال دل‬# dolls' festival # 0.5431 ‫ احتفال دل للفتيات‬# dolls' festival # 0.3999 ‫ إحتفاليا‬# festive # 0.7535 ‫ إحتفاليا تتمناه‬# no sekku # 0.5081 ‫ إحتفاليا تتمناه‬# sekku # 0.5081 ‫ إحتفاليا تتمناه‬# tango # 0.5081 ‫ إحتفاليا تتمناه‬# tango no # 0.5081 ‫ إحتفاليا تتمناه للنمو‬# no sekku # 0.3066  Figure 2. Phrase Translation Pairs  Another alternative to the classical source-channel approach is the direct modeling of the posterior probability P(e | f ) using the log-linear model (Och and Ney, 2002):  ∑ P(e1I | f1J ) = exp(  λ M m= 1  mφ  m (e1I  ,  f1J  ))  (Eq. 2)  Z  Each φ m is a feature function that estimates some feature values from (e, f). The two knowledge sources used in the classical source-channel approaches can be converted into two feature functions such that:  φ 1 = P(e) φ 2 = P( f | e)  (Eq. 3)  The denominator Z serves as a normalization factor and depends only on the source sentence f. For different translation alternatives of f, the normalization factors Z are all the same. Therefore the decoder only needs to calculate the numerator part in Eq. 2 to search for the optimal translation e* for f.  Under the log-linear model, we could convert the translation model, language model, distortion model, sentence length model and other models into feature functions. This allows us to incorporate more knowledge sources than is the case in the classical source-channel approach. The weights for each feature function are trained using the Minimum Error (MER) optimization on the development set. MER optimizes the feature weights to minimize the errors, or equivalently, maximizing the BLEU/NIST scores (Och, 2003).  PanDoRA System Based on the general concepts of phrase-based SMT, PanDoRA is engineered from scratch to cope with the limitations on hand-held devices. The code base for PanDoRA is completely different from our phrase-based SMT system for PC platforms.  Compact Data Structure When running SMT systems on PCs, we usually load all models into memory. The size of phrase-based SMT models can become very large when the training data size increases, or when we consider longer phrases in the translation model. Callison-Burch(2005) estimates that if we consider phrases up to 10 words long, storing all the phrase translation pairs for the NIST-2004 Arabic-English training data3 would need 30.62 GB in memory. Phrasebased SMT systems are very demanding in resources. Various approaches have been proposed to cut-down memory needs by applying either the delayed phrase construction (Zhang and Vogel, 2005; Callison-Burch, 2005), or phrase table pruning (Eck 2007).  Loading models of several gigabyte into the dynamic memory (SDRAM) is out of question for hand-held devices. Even though the training data for portable speech 3 The corpus contains 3.75 million sentence pairs and has 127 million words in English, and 106 million words in Arabic.  Src. Speech  Src.  Tgt.  ASR  TTS  Tgt. Speech  PanDoRA (Src↔Tgt)  Src. Speech  Src.  Tgt.  TTS  ASR  Tgt. Speech  Figure 3. PanDoRA system in a two-way speech translation system for language pair Src. and Tgt. translation systems is usually limited to domains such as travel and medical, and is usually much smaller than the training data used for the newswire translation, the phrase translation model and language model are still too large to be fit into the dynamic memory of a PDA. In PanDoRA, we designed a compacted data structure for the translation model and the language model so that: 1. The resulting models are small in size; 2. Decoder can directly access the information without loading the model into the dynamic memory. Three techniques are developed to achieve these two goals: 1) converting words into integer symbol IDs (vocId); 2) cross-indexing the phrase translation models to reduce the redundancy in model representation; and 3) serializing the model structure to make it directly accessible from disk. PanDoRA is used mainly for the speech-to-speech translation (SST) systems. It is placed between the Automatic Speech Recognition (ASR) and the Text-toSpeech (TTS) modules in the SST system (Fig. 3). All the input to PanDoRA comes from the ASR output, which means that we operate under a closed vocabulary situation. In other words, there are no unknown word types to the translation system and we can map all the words used in the translation/language models into unique integer vocIds to avoid the hassle of string operations. By doing this, information objects such as 3-grams and their probabilities, have fixed sizes no matter how many characters each word in the 3-gram has. The fixed object size makes it possible to directly access an object through its index by visiting start address + index • sizeof(object). During translation, the transcription output from the ASR is first mapped from text to a sequence of vocIds, and vocIds are used throughout the decoding process. After the decoding, vocIds of the translation result are mapped into words of the target language for the Text-To-Speech (TTS) module. Two-byte integers are used for vocIds and the system can handle a vocabulary of 64K words for the source language and 64K words for the target language. The information in the src→tgt and tgt→src phrase translation tables have a lot of redundancy. Even though  the src→tgt table is not symmetric to the tgt→src translation table, most phrases occur in both tables. By cross-indexing, phrases are converted into integer Ids and phrase tables only need to store probability of translating from one phrase ID to another phrase ID. For hand-held devices the synchronous dynamic random access memory (SDRAM) is usually small (e.g. <64MB). All applications have to share this limited memory. The external storage devices such as CF-cards and SD-cards are much larger in capacity (e.g. 2GB) and can be read/write in reasonably fast rate (about 10MB/second). We serialize the model files in a way such that the decoder can directly access the needed information from the serialized model file on the external storage card. The model does not need to be loaded into the precious SDRAM and the dynamic memory can be saved for the search process during decoding. For our Thai↔English system, the bilingual training corpus contains 200K sentence pairs, about 1.65 million words on the English side. Using the PESA system, the extracted Thai→English phrase translation table contains 2.6 million entries and the English→Thai direction has 3 million phrase pairs. The two phrase tables are 585 MB when stored on disk in the plain text format. After the cross-indexing and conversion into compact binary format, the two translation models take only 65.27 MB on disk. Integerized Computation In standard SMT systems, translation model, language model, and other models use floating points to store probabilities and feature values. On PDAs, there are no built-in floating point co-processors. This means that we have to either use a “soft float” scheme to simulate the floating point calculation which slows down the computation or to integerize all the floating values in the TM/LM. A pilot study on the PC-based SMT system shows that the overall translation quality does not degrade when all the probabilities are cast from float/double to integers. Table 2 shows both BLEU (Papineni, 2001) and NIST (NIST, 2003) scores of the SMT system using a mid-sized phrase table and tested on the TIDES MT03 Chinese-English evaluation data. Translations from different combinations give almost the same results. This finding corroborate with the study done by Marcellon and Bertoldi (2006), where probabilities are quantized to 2h number of levels and only h-bits are used to represent a floating point value. Experiments show that quantization with h=8 bits does not affect performance, and gives even slightly better scores. Even when h=4, which corresponds to only 16 bins, the translation quality only loses 1.60% relative in BLEU. We apply a very simple and straightforward quantization method on the probabilities. We convert all the probabilities into minus log probabilities (cost) and map the costs into integers ranging from 0 to 4095. In other words, the probabilities are quantized to 4096 bins. For those belonging to the same bin, their differences are  ignored. The decoder uses the integer costs to calculate the probability of a hypothesis during decoding. Thus the decoder only needs to use the integer addition and multiplication operations.  TM Float Int Float Int  LM Float Float Int Int  BLEU 19.87 19.82 19.94 19.93  NIST 8.03 7.99 8.03 8.04  Table 2. Pilot study on integerize float values in the translation model and language model  Language Model The n-gram language model is converted from its text representation to the binary format. By doing so, each ngram is represented by n vocIds and a fixed number of bytes for conditional probabilities and back-off weights. We store all the n-grams in a binary file in sorted order. The decoder can directly look up an n-gram for its information in the file without loading the LM into the RAM. Similar to the compact translation model, this saves the limited SDRAM for the decoding process, which requires random allocation of memory for dynamic data structures.  Discriminative Language Model n-gram language models are generative models, i.e., it models the stochastic process of how a sentence is generated. Usually n-gram LMs are trained from collections of sentences which are considered to be “correct” and “grammatical”. With a trained LM, one can estimate how likely any sentence could be generated given all these “right” examples. The generative n-gram model assumes that any sequence of words could be generated and the total probability sums up to 1, i.e., for any sentence e , no matter how bad it is, P(e)>0, and for all possible sentences that could be generated.  However, there are certain phenomena in natural languages which we know should never happen. For example, “the” should never occur at the end of an English sentence, and particle “ga” should never be placed at the beginning of a Japanese sentence. In the case of the generative n-gram LM, it has never seen such events in the training data since it is trained only from those “correct” examples. n-gram LM uses various smoothing techniques to estimate the probability for such unseen events and hopefully it will assign low probabilities to them. But assigning a low probability to “the </s>” or “<s> ga” can not prevent them from being generated by the SMT decoder. In PanDoRA, language model plays an important role in deciding the correct reordering pattern during decoding, we need to explicitly model the “negative examples” to prevent those ungrammatical n-grams from being generated. In addition to the standard n-gram language model, we use a discriminative language model to alleviate the limitations of the generative LM. Discriminative training has been shown to improve the translation quality (Liang et al., 2006). The idea of using  an “anti-language model” has also been tried in speech recognition (Stolcke et al., 2000). We use the perceptron algorithm as described in (Collins, 2002) to train a discriminative language model. Given the current translation model and generative language model, we translate the source side of the bilingual training corpus f into e’. Unlike Example-based Machine Translation (EBMT) systems, SMT systems usually can not reproduce the same translation as used in the training data, thus the target side of the training corpus e is usually different from e’. We enumerate all the n-grams from the union of e and e’. For each n-gram, we increase the n-gram’s weight if its frequency in e’ is less than its frequency in e and decrease its weight if it has been over generated. The adjusted weights for n-grams are then used as a feature function in the log-linear model (Eq. 2) for the next iteration of decoding. In other words, we iteratively adjust the weight of each ngram in the discriminative language model to push the generated translation results towards the reference translation. Decoding Given a testing sentence, PanDoRA applies the translation model on the sentence and builds a translation lattice. The decoder then searches in this lattice for the optimal path as the output translation for the input sentence. PanDoRA implements two types of search method in its decoder: a left-to-right monotone decoding and a bottomup CKY-parsing using the Inverted Transduction Grammar (ITG, Wu 1997). Monotone Decoding The monotone decoding in the PanDoRA system is a beam search decoder based on the idea described in Vogel et al. (2003). Once the complete translation lattice has been built, a best-first search through this lattice is performed. In addition to the translation costs, the language model costs are added and the path which minimizes the combined cost is returned. Starting with a special begin-of-sentence hypothesis attached to the first node in the translation lattice, hypotheses are expanded over all outgoing edges from the current node. The decoder allows for recombination of hypotheses in a flexible way. It is important to keep hypotheses apart if the partial translations end in different words, as this will result in different scores from the language model during the next expansion step. In addition, we can distinguish hypotheses if the length of the translation generated so far is different. This comes into effect when a sentence length model is applied at the sentence end. The search space becomes very large for long sentences and when there are many alternative translations for each matching source phrase, heavy pruning is enforced during the decoding to make the search space reasonably small to minimize memory usage. Our monotone decoder realizes a standard beam search, where a best hypothesis is stored based on the features used for hypothesis recombination,  and all hypotheses which are worse by some margin are deleted.  With these PDA-specific designs in the decoder, translating one sentence takes less than 10 ms in the monotone decoding mode.  ITG Reordering Decoding The monotone translation mode works reasonably well for language pairs which have very similar word orders, for example, Spanish and English, but it works poorly for language pairs which have very different word orders. Translating Japanese to English monotonically can result in sentences such as “An entry visa do I need a?” and “In a taxi I left my bag.” To cope with this type of longdistance reordering phenomena, we implement the ITGstyle reordering in PanDoRA decoder.  The Stochastic Inversion Transduction Grammar (ITG) introduced by Wu (1997) is a transduction grammar which assumes that a pair of source/target sentences are simultaneously generated in a context-free manner. At each step, a non-terminal X can generate its span in two ways: either straight:  or inverted:  X → < f1 f2, e1e2 > ,  X → < f1 f2 , e2e1 > , where e1 is the translation for f1 and e2 for f2.  Even though it is quite simple and straightforward, ITG has been shown to have high expressiveness. In other words, most of the reordering patterns in natural language translation can be expressed by ITG.  The ITG-style decoding in the PanDoRA system is a CKY  parser with beam search. The idea of translating by  parsing is similar to the approach used in the Hiero system  (Chiang, 2005). Given a source sentence f, the decoder  finds the best derivation that generates <f, e> for some e.  Unlike the monotone decoder which works on the source  sentence from left to right, the CKY parser works bottom-  
We launched a 5-year-project in 2006 to develop a Japanese-Chinese machine translation system for translating scientific and technical papers. As part of that project, we are currently building a Japanese-Chinese translation dictionary based on the EDR Japanese-English bilingual dictionary. This paper presents the design and construction of the Japanese-Chinese translation dictionary, including specifications for translating Japanese information into Chinese and annotating related information, tools developed for assisting manual annotation, and some result that have already been achieved.  
Most natural language applications have some degree of preprocessing of data: tokenisation, stemming and so on. In the domain of Statistical Machine Translation (SMT) it has been shown that word reordering as a preprocessing step can help the translation process, but it is unclear why. We propose two possible reasons for the observed improvement: (1) that the reordering explicitly matches the syntax of the source language more closely to that of the target language; or (2) that it ﬁts the data better to the mechanisms of phrasal SMT. In previous work from German to English, for example, hand-written language-speciﬁc reordering rules both match the German more closely to English syntax, and compress heads and dependants into the PSMT phrasal window. Whether the source of the improvement is (1) or (2) has not been determined, although most other work assumes the former. To identify the effects of each possible cause, we carry out two sets of experiments. For (1) we reverse the language-dependent syntactic reordering such that heads and dependants are moved apart. For (2), we propose a generic approach to minimising dependency distances in reordering that does not explicitly match target language word order and that does not require language-speciﬁc rules; the aim of which, rather than to beat state-of-the-art systems, is to investigate. The results show that (1) and (2) individually do still lead to improvements in translation quality, but each weaker than the original, suggesting that both features are necessary for a strong improvement. A consequence of this is that is possible to gain half the improvement of language-speciﬁc rules through one generic one.  Much to learn, you still have Jedi Master Yoda  
Technology shipped in Word 97 grammar checker Emphasis on deeper “Logical Form” processing, Mindnet, & information retrieval applications followed Using NL components created in 6 languages, work on machine translation began in 1999 First external visibility of MT was Customer Support KB pilot in 2002 Today, ongoing NL research includes paraphrasing, sentiment analysis, ESL grammar & style checking, summarization, parsing, and of course, machine translation 
Thus the goals of the workshop are to familiarize the participants with the experiment and its history and potential, while also observing the collaborative processes that emerge as the day unfolds. 
In this article we address the task of automatic text structuring into linear and non-overlapping thematic episodes. Our investigation reports on the use of various lexical, acoustic and syntactic features, and makes a comparison of how these features influence performance of automatic topic segmentation. Using datasets containing multi-party meeting transcriptions, we base our experiments on a proven state-of-the-art approach using support vector classification.
Cognates are pairs of words in different languages similar in spelling and meaning. They can help a second-language learner on the tasks of vocabulary expansion and reading comprehension. False friends are pairs of words that have similar spelling but different meanings. Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. In this article we present a method to automatically classify a pair of words as cognates or false friends, by using several measures of orthographic similarity as features for classification. We use this method to create complete lists of cognates and false friends between two languages. We also disambiguate partial cognates in context. We applied all our methods to French and English, but they can be applied to other pairs of languages as well. We built a tool that takes the produced lists and annotates a French text with equivalent English cognates or false friends, in order to help second-language learners improve their reading comprehension skills and retention rate.
The use/use for relationship a thesaurus is usually more complex than the (para-) synonymy recommended in the ISO-2788 standard describing the content of these controlled vocabularies. The fact that a non preferred term can refer to multiple preferred terms (only the latter are relevant in controlled indexing) makes this relationship difficult to use in automatic annotation applications : it generates ambiguity cases. In this paper, we present the CARROT algorithm, meant to rank the output of our Information Extraction pipeline, and how this algorithm can be used to select the relevant preferred term out of different possibilities. This selection is meant to provide suggestions of keywords to human annotators, in order to ease and speed up their daily process and is based on the structure of their thesaurus. We achieve a 95 {\%} success, and discuss these results along with perspectives for this experiment.
In this paper we give an overview of the 2007 evaluation campaign for the International Workshop on Spoken Language Translation (IWSLT)1. As with previous evaluation campaigns, the primary focus of the workshop was the translation of spoken language in the travel domain. This year there were four language pairs; the translation of Chinese, Italian, Arabic, and Japanese into English. The input data consisted of the output of ASR systems for read speech and clean text. The exceptions were the challenge task of the Italian English language pair which used spontaneous speech ASR outputs and transcriptions and the Chinese English task which used only clean text. A new characteristic of this year{'}s evaluation campaign was an increased focus on the sharing of resources. Participants were requested to submit the data and supplementary resources used in building their systems so that the other participants might be able to take advantage of the same resources. A second new characteristic this year was the focus on the human evaluation of systems. Each primary run was judged in the human evaluation for every task using a straightforward ranking of systems. This year's workshop saw an increased participation over last year's workshop. This year 24 groups submitted runs to one or more of the tasks, compared to the 19 groups that submitted runs last year [1]. Automatic and human evaluation were carried out to measure MT performance under each condition, ASR system outputs for read speech, spontaneous travel dialogues, and clean text.
The goal of this work is to improve current translation models by taking into account additional knowledge sources such as semantically motivated segmentation or statistical categorization. Specifically, two different approaches are discussed. On the one hand, phrase-based approach, and on the other hand, categorization. For both approaches, both statistical and linguistic alternatives are explored. As for translation framework, finite-state transducers are considered. These are versatile models that can be easily integrated on-the-fly with acoustic models for speech translation purposes. In what the experimental framework concerns, all the models presented were evaluated and compared taking confidence intervals into account.
Inspired by previous chunk-level reordering approaches to statistical machine translation, this paper presents two methods to improve the reordering at the chunk level. By introducing a new lattice weighting factor and by reordering the training source data, an improvement is reported on TER and BLEU. Compared to the previous chunklevel reordering approach, the BLEU score improves 1.4{\%} absolutely. The translation results are reported on IWSLT Chinese-English task.
The paper describes our portable two-way speech-to-speech translation system using a completely eyes-free/hands-free user interface. This system translates between the language pair English and Iraqi Arabic as well as between English and Farsi, and was built within the framework of the DARPA TransTac program. The Farsi language support was developed within a 90-day period, testing our ability to rapidly support new languages. The paper gives an overview of the system{'}s components along with the individual component objective measures and a discussion of issues relevant for the overall usage of the system. We found that usability, flexibility, and robustness serve as severe constraints on system architecture and design.
 Using the Phrase-Based SMT  Using a log-linear model  M  ∑ e* = arg max λmhm(e, f )  e  m=1  Chinese-English translation task  Data  --Data Collection   Downloading all the open resources from the web;  Filtering these corpus which is highly correlative with the released train data by IWSLT 2007.  Data  --Data Preprocessing   Chinese word segmenting using the software ICTCLAS3.0 (http://www.nlp.org.cn)  Removing the noises words or characters in the training data  Transforming the SBC case into DBC case in Chinese corpus  Tokenizing the English words  Word Alignments  Obtaining the initial word alignments by GIZA++ (http://www.fjoch.com/GIZA++.html)  Using the method grow-diag-final method to modify the initial alignments  Using our method to modify the word alignments by using the dictionary and jumping-distance  Word Alignments (Cont.)  Dictionary:  The bilingual dictionary download from the open resources on the web ( http://isslt07.itc.it/menu/resources.html)  From the bi-directional dictionaries generated by GIZA++, only using such word pairs with the highest probabilities.  Word Alignments (Cont.)  If the word pair (fi, ej) which is inexistent in the bilingual dictionary but existent in the word alignments of the sentence pair ;  If the word pair (fi, ej) is inexistent both in the bilingual dictionary and the word alignments, we will not deal with such case;  If the word pair (fi, ej) is existent in the bilingual dictionary but inexistent in word alignments we will add the word pair alignment information;  If the word pair (fi, ej) is existent both in the bi-lingual dictionary and in the word alignments, we will keep the word pair alignment information.  Word Alignments (Cont.)  For example:  我  能  在  哪个 窗口 预定  ？  At which window can I make a reservation ?  我  能  在  哪个 窗口 预定 ？  At which window can I make a reaservation ?  Phrase-Extraction  Och’s method:  Simple and easy to realized  Totally consistent with word alignments  Extend Och’s method:  Using a flexible scale to extract the phrase pairs  Phrase-Extraction (Cont.) ( f, e)∈ BP <=> ∀fi ∈ f : ( fi , ej) ∈ A → ej ∈e AND ∀ej ∈ e : ( fi , ej )∈ A→ fi ∈ f  Phrase-Extraction  ( f , e) ∈ BP <=>  ∀fi ∈ f : ( fi , ej ) ∈ A → ej ∈e  AND    OR   ∀e j ∈ e : ( fi , ej ) ∈ A → fi ∈ f  {e  j | (ej , {e j | e  fi ) j∈  ∈ A} e}  ≥  Threshold  ，      e j is not a functional word , → arg max p( f i | e j ) ∈ f    { fi| ( fi ,e j )∈A }  Phrase-Extraction (Cont.)  Computing the percentage of consistent target words in e ;  Judging if these non-consistent target words are functional words;  Checking if the source words that the nonconsistent and non-functional target word is aligned to are all outside f;  Extracting the phrases.  Phrase-Extraction (Cont.)  For example: 那 要 看 交通 状况 it depends on the traffic conditions  Phrase-Extraction (Cont.)  那 要 看 ||| it depends  Och’s phrase table  那 要 看 交通 状况 ||| it depends on the traffic conditions 交通 ||| traffic 交通 ||| the traffic  那 要 看 ||| it depends  交通 状况 ||| on the traffic conditions  那 要 看 交通 ||| it depends on the traffic 那 要 看 交通 状况 ||| it depends on the traffic conditions 要 看 交通 ||| depends on the traffic 要 看 交通 状况 ||| depends on the traffic conditions  Our phrase table  看 交通 ||| depends on the traffic  看 交通 状况 ||| depends on the traffic conditions  交通 ||| traffic  交通 ||| the traffic  Phrase-Extraction (Cont.)  φ(  f  | e ) =  N ( f ,e ) ∑ N ( f ', e)  f '  φ (e  |  f )  =  N ( f ,e ) ∑ N ( f , e ')  e '  ∏ ∑ lex( f  | e ,a ) =  i2 i =i1  |{j  
The GREYC machine translation (MT) system is a slight evolution of the ALEPH machine translation system that participated in the IWLST 2005 campaign. It is a pure example-based MT system that exploits proportional analogies. The training data used for this campaign were limited on purpose to the sole data provided by the organizers. However, the training data were expanded with the results of sub-sentential alignments. Thesystemparticipatedinthetwoclassicaltasks of translation of manually transcribed texts from Japanese to English and Arabic to English.
In this paper, we describe the system and approach used by Institute for Infocomm Research (I2R) for the IWSLT 2007 spoken language evaluation campaign. A multi-pass approach is exploited to generate and select best translation. First, we use two decoders namely the open source Moses and an in-home syntax-based decoder to generate N-best lists. Next we spawn new translation entries through a word-based n-gram language model estimated on the former N-best entries. Finally, we join the N-best lists from the previous two passes, and select the best translation by rescoring them with additional feature functions. In particular, this paper reports our effort on new translation entry generation and system combination. The performance on development and test sets are reported. The system was ranked first with respect to the BLEU measure in Chinese-to-English open data track.
This paper describes the CMU-UKA statistical machine translation systems submitted to the IWSLT 2007 evaluation campaign. Systems were submitted for three language-pairs: Japanese→English, Chinese→English and Arabic→English. All systems were based on a common phrase-based SMT (statistical machine translation) framework but for each language-pair a specific research problem was tackled. For Japanese→English we focused on two problems: first, punctuation recovery, and second, how to incorporate topic-knowledge into the translation framework. Our Chinese→English submission focused on syntax-augmented SMT and for the Arabic→English task we focused on incorporating morphological-decomposition into the SMT framework. This research strategy enabled us to evaluate a wide variety of approaches which proved effective for the language pairs they were evaluated on.
In this paper, we give a description of the machine translation system developed at DCU that was used for our second participation in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT 2007). In this participation, we focus on some new methods to improve system quality. Specifically, we try our word packing technique for different language pairs, we smooth our translation tables with out-of-domain word translations for the Arabic{--}English and Chinese{--}English tasks in order to solve the high number of out of vocabulary items, and finally we deploy a translation-based model for case and punctuation restoration. We participated in both the classical and challenge tasks for the following translation directions: Chinese{--}English, Japanese{--}English and Arabic{--}English. For the last two tasks, we translated both the single-best ASR hypotheses and the correct recognition results; for Chinese{--}English, we just translated the correct recognition results. We report the results of the system for the provided evaluation sets, together with some additional experiments carried out following identification of some simple tokenisation errors in the official runs.
This paper reports on the participation of FBK (formerly ITC-irst) at the IWSLT 2007 Evaluation. FBK participated in three tasks, namely Chinese-to-English, Japanese-to-English, and Italian-to-English. With respect to last year, translation systems were developed with the Moses Toolkit and the IRSTLM library, both available as open source software. Moreover, several novel ideas were investigated: the use of confusion networks in input to manage ambiguity in punctuation, the estimation of an additional language model by means of the Google{'}s Web 1T 5-gram collection, the combination of true case and lower case language models, and finally the use of multiple phrase-tables. By working on top of a state-of-the art baseline, experiments showed that the above methods accounted for significant BLEU score improvements.
This paper describes the HKUST experiments in the IWSLT 2007 evaluation campaign on spoken language translation. Our primary objective was to compare the open-source phrase-based statistical machine translation toolkit Moses against Pharaoh. We focused on Chinese to English translation, but we also report results on the Arabic to English, Italian to English, and Japanese to English tasks.
This paper presents the University of Washington{'}s submission to the 2007 IWSLT benchmark evaluation. The UW system participated in two data tracks, Italian-to-English and Arabic-to-English. Our main focus was on incorporating out-of-domain data, which contributed to improvements for both language pairs in both the clean text and ASR output conditions. In addition, we compared supervised and semi-supervised preprocessing schemes for the Arabic-to-English task and found that the semi-supervised scheme performs competitively with the supervised algorithm while using a fraction of the run-time.
The MIT-LL/AFRL MT system implements a standard phrase-based, statistical translation model. It incorporates a number of extensions that improve performance for speech-based translation. During this evaluation our efforts focused on the rapid porting of our SMT system to a new language (Arabic) and novel approaches to translation from speech input. This paper discusses the architecture of the MIT-LL/AFRL MT system, improvements over our 2006 system, and experiments we ran during the IWSLT-2007 evaluation. Specifically, we focus on 1) experiments comparing the performance of confusion network decoding and direct lattice decoding techniques for machine translation of speech, 2) the application of lightweight morphology for Arabic MT preprocessing and 3) improved confusion network decoding.
This paper describes the NiCT-ATR statistical machine translation (SMT) system used for the IWSLT 2007 evaluation campaign. We participated in three of the four language pair translation tasks (CE, JE, and IE). We used a phrase-based SMT system using log-linear feature models for all tracks. This year we decoded from the ASR n-best lists in the JE track and found a gain in performance. We also applied some new techniques to facilitate the use of out-of-domain external resources by model combination and also by utilizing a huge corpus of n-grams provided by Google Inc.. Using these resources gave mixed results that depended on the technique also the language pair however, in some cases we achieved consistently positive results. The results from model-interpolation in particular were very promising.
The NTT Statistical Machine Translation System employs a large number of feature functions. First, k-best translation candidates are generated by an efficient decoding method of hierarchical phrase-based translation. Second, the k-best translations are reranked. In both steps, sparse binary features {---} of the order of millions {---} are integrated during the search. This paper gives the details of the two steps and shows the results for the Evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007.
In this paper, we give an overview of the ICT statistical machine translation systems for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007. In this year{'}s evaluation, we participated in the Chinese-English transcript translation task, and developed three systems based on different techniques: a formally syntax-based system Bruin, an extended phrase-based system Confucius and a linguistically syntax-based system Lynx. We will describe the models of these three systems, and compare their performance in detail. We set Bruin as our primary system, which ranks 2 among the 15 primary results according to the official evaluation results.
We present the machine translation system used by L2F from INESC-ID in the evaluation campaign of the International Workshop on Spoken Language Translation (2007), in the task of translating spontaneous conversations in the travel domain from Italian to English.
In this paper we describe the statistical machine translation system developed at ITI/UPV, which aims especially at speech recognition and statistical machine translation integration, for the evaluation campaign of the International Workshop on Spoken Language Translation (2007). The system we have developed takes advantage of an improved word lattice representation that uses word posterior probabilities. These word posterior probabilities are then added as a feature to a log-linear model. This model includes a stochastic finite-state transducer which allows an easy lattice integration. Furthermore, it provides a statistical phrase-based reordering model that is able to perform local reorderings of the output. We have tested this model on the Italian-English corpus, for clean text, 1-best ASR and lattice ASR inputs. The results and conclusions of such experiments are reported at the end of this paper.
This paper is a description of the system presented by the LIG laboratory to the IWSLT07 speech translation evaluation. The LIG participated, for the first time this year, in the Arabic to English speech translation task. For translation, we used a conventional statistical phrase-based system developed using the moses open source decoder. Our baseline MT system is described and we discuss particularly the use of an additional bilingual dictionary which seems useful when few training data is available. The main contribution of this paper concerns the proposal of a lattice decomposition algorithm that allows transforming a word lattice into a sub word lattice compatible with our MT model that uses word segmentation on the Arabic part. The lattice is then transformed into a confusion network which can be directly decoded into moses. The results show that this method outperforms the conventional 1-best translation which consists in translating only the most probable ASR hypothesis. The best BLEU score, from ASR output obtained on IWSLT06 evaluation data is 0.2253. The results confirm the interest of full CN decoding for speech translation, compared to traditional ASR 1-best approach. Our primary system was ranked 7/14 for IWSLT07 AE ASR task with a BLEU score of 0.3804.
In this paper, we describe our machine translation system which was used for the Chinese-to-English task in the IWSLT2007 evaluation campaign. The system is a statistical machine translation (SMT) system, while containing an example-based decoder. In this way, it will help to solve the re-ordering problem and other problems for spoken language MT, such as lots of omissions, idioms etc. We report the results of the system for the provided evaluation sets.
This paper describes MISTRAL, the lattice translation system that we developed for the Italian-English track of the International Workshop on Spoken Language Translation 2007. MISTRAL is a discriminative phrase-based system that translates a source word lattice in two passes. The first pass extracts a list of top ranked sentence pairs from the lattice and the second pass rescores this list with more complex features. Our experiments show that our system, when translating pruned lattices, is at least as good as a fair baseline that translates the first ranked sentences returned by a speech recognition system.
Our statistical machine translation system that uses large Japanese-English parallel sentences and long phrase tables is described. We collected 698,973 Japanese-English parallel sentences, and we used long phrase tables. Also, we utilized general tools for statistical machine translation, such as {''}Giza++{''}[1], {''}moses{''}[2], and {''}training-phrasemodel.perl{''}[3]. We used these data and these tools, We challenge the contest for IWSLT07. In which task was the result (0.4321 BLEU) obtained.
In this paper, an overview of the XMU statistical machine translation (SMT) system for the 2007 IWSLT Speech Translation Evaluation is given. Our system is a phrase-based system with a reordering model based on chunking and reordering of source language. In this year{'}s evaluation, we participated in the open data track for Clean Transcripts for the Chinese-English translation direction. The system ranked the 12th among the 15 participating systems.
The RWTH system for the IWSLT 2007 evaluation is a combination of several statistical machine translation systems. The combination includes Phrase-Based models, a n-gram translation model and a hierarchical phrase model. We describe the individual systems and the method that was used for combining the system outputs. Compared to our 2006 system, we newly introduce a hierarchical phrase-based translation model and show improvements in system combination for Machine Translation. RWTH participated in the Italian-to-English and Chinese-to-English translation directions.
This paper describes TALPtuples, the 2007 N-gram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the system of previous years. Mainly, these include optimizing alignment parameters in function of translation metric scores and rescoring with a neural network language model. Results on two translation directions are reported, namely from Arabic and Chinese into English, thoroughly explaining all language-related preprocessing and translation schemes.
We describe the TÜBITAK-UEKAE system that participated in the Arabic-to-English and Japanese-to-English translation tasks of the IWSLT 2007 evaluation campaign. Our system is built on the open-source phrase-based statistical machine translation software Moses. Among available corpora and linguistic resources, only the supplied training data and an Arabic morphological analyzer are used in the system. We present the run-time lexical approximation method to cope with out-of-vocabulary words during decoding. We tested our system under both automatic speech recognition (ASR) and clean transcript (clean) input conditions. Our system was ranked first in both Arabic-to-English and Japanese-to-English tasks under the {``}clean{''} condition.
This paper describes the University of Maryland statistical machine translation system used in the IWSLT 2007 evaluation. Our focus was threefold: using hierarchical phrase-based models in spoken language translation, the incorporation of sub-lexical information in model estimation via morphological analysis (Arabic) and word and character segmentation (Chinese), and the use of n-gram sequence models for source-side punctuation prediction. Our efforts yield significant improvements in Chinese-English and Arabic-English translation tasks for both spoken language and human transcription conditions.
