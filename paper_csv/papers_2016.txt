This paper presents methods to predict retrieval terms from relevant/surrounding words or descriptive texts in Japanese by using deep learning methods, which are implemented with stacked denoising autoencoders (SdA), as well as deep belief networks (DBN). To determine the effectiveness of using DBN and SdA for this task, we compare them with conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM). We also compare their performance in case of using three regularization methods, the weight decay (L2 regularization), sparsity (L1 regularization), and dropout regularization. The experimental results show that (1) adding automatically gathered unlabeled data to the labeled data for unsupervised learning is an effective measure for improving the prediction precision, and (2) using DBN or SdA results in higher prediction precision than using SVM or MLP, whether or not regularization methods are used. 
The clusters of review sentences on the viewpoints from the products‚Äô evaluation can be applied to various use. The topic models, for example Unigram Mixture (UM), can be used for this task. However, there are two problems. One problem is that topic models depend on the randomly-initialized parameters and computation results are not consistent. The other is that the number of topics has to be set as a preset parameter. To solve these problems, we introduce PageRank Topic Model (PRTM), that approximately estimates multinomial distributions over topics and words in a vocabulary using network structure analysis methods to Word Co-occurrence Graphs. In PRTM, an appropriate number of topics is estimated using the Newman method from a Word Co-occurrence Graph. Also, PRTM achieves consistent results because multinomial distributions over words in a vocabulary are estimated using PageRank and a multinomial distribution over topics is estimated as a convex quadratic programming problem. Using two review datasets about hotels and cars, we show that PRTM achieves consistent results in sentence clustering and an appropriate estimation of the number of topics for extracting the viewpoints from the products‚Äô evaluation. 
In this paper, the use of two modals (can and may) in four varieties of English (British, India, Philippines, and USA) was compared and the characteristics of each variety were statistically analyzed. After all the sample sentences were extracted from each component of the ICE corpus, a total of twenty linguistic factors were encoded. Then, the collected data were statistically analyzed with R. Through the analysis, the following facts were observed: (i) India and Philippine speakers used can more frequently than natives, (ii) Three linguistic factors interacted with CORPUS, and (iii) The distinctions between American and British were more influential than those of the Inner Circle vs. the Outer Circle. 
This paper examines the English particle placements of EFL learners‚Äô writings in three East Asian countries (Chinese, Japan, and Korea). Three parts of the TOEFL11 corpus were chosen, and all the sentences with particles were extracted. The ICE-GB was chosen as a native speakers‚Äô English. Then, eleven linguistic factors were manually encoded. The collected data were analyzed with R. Correlation tests and a hierarchical clustering analysis was adopted. Through the analysis, the following two facts were observed: (i) each linguistic factor affected differently in four varieties of English and (ii) Japanese English was similar to native speakers‚Äô counterparts whereas Korean and Chinese formed another group. 
The given work is focused on the principal ways by which new words and expressions enter the Chinese Internet language, the sources of new meanings for old words and phrases; neologisms and chengyu with modified meaning and structure. Some new tendencies in developing of the Chinese Internet language, such as wide use of dialect-originated words, archaic characters and monosyllabic words, are introduced as well. 
Sentiment analysis with features addition to opinion words has been an appealing area in recent studies. Some research has been conducted for Ô¨Ånding relationship between sentiments, topics and temporal sentiment analysis. Nevertheless, Email sentiment analysis received relatively less attention due to the complexity of its structure and indirectness of its language. This paper introduces a systematic framework for sentiment clustering using topic and temporal features for large Email datasets. Interesting Email and sentiment distribution patterns are summarized and discussed with empirical results. 
Type information is an important component of linked data. Unfortunately, many linked datasets lack of type information, which obstructs linked data applications such as question answering and recommendation. In this paper, we study how to automatically identify entity type information from Chinese linked data and present a novel approach by integrating classification and entity linking techniques. In particular, entity type information is inferred from internal clues such as entity‚Äôs abstract, infobox and subject using classifiers. Moreover, external evidence is obtained from other knowledge bases using entity linking techniques. To evaluate the effectiveness of the approach, we conduct preliminary experiments on a real-world linked dataset from Zhishi.me 1 . Experimental results indicate that our approach is effective in identifying entity types. 
 The field of Natural Language Processing  (NLP) in the country has been continually  developing. However, the transition between  Tagalog to the progressing Filipino language  left tools and resources behind. This paper  introduces a Statistical Machine Translation  Part-of-Speech (POS) Tagger for Filipino  (SMTPOST), with the purpose of reviving,  updating and widening the scope of  technologies in the POS` tagging domain,  catering to the changes made by the Filipino  language. Resources built are comprised  mainly of a tagset (218 tags), parallel corpus  (2,668 sentences), affix rules (59 rules) and  word-tag dictionary (309 entries). SMTPOST  was tested to different tagsets and domains,  producing 84.75% as its highest accuracy  score, at least 3.75% increase from the  available Tagalog POS taggers. Despite  SMTPOST‚Äôs utilization of Filipino resources  and good performance, there are room for  improvements  and  opportunities.  Recommendations include a better feature  extractor (preferably a morphological  analyzer), an increase in scope for all of the  resources, implementation of pre- and/or post-  processing, and the utilization of SMTPOST  research to other NLP applications.  
Using queries to explore corpora is today routine practice not only among researchers in various Ô¨Åelds with an empirical approach to discourse, but also among nonspecialists who use search engines or concordancers for language learning purposes. While keyword-based queries are quite common, non-specialists are less likely to explore syntactic constructions. Syntax-based queries usually require the use of regular expressions with grammatical words combined with morphosyntactic tags, meaning that users need to master both the query language of the tool and the tagset of the annotated corpus. However, non-specialists such as language learners may prefer to focus on the output rather than spend time and efforts mastering a query language. To address this shortcoming, we propose a methodology including a syntactic parser and using common similarity measures to compare sequences of automatically produced morphosyntactic tags. 
Verb resultative complement (VC) is a common structure of Chinese language with abundant forms of collocation. It makes much sense for VC research to analyze the general rules of argument integration in light of diversities of predicate & complement and the complexity of argument integration in the forming of VC with predicate & complement. This article has analyzed and summarized the existing research outcome of VC, and then gives further analyses thereby on argument integration process and multi-valence phenomena. 
                                                              !       "                                                                                                                                    !    "          "             #       $           "    "                            "       %"" #&'()$         "       " *  ++    #         $    #     $   "              %                     , !   "     "           -         . "                        " "   .    /     " *  " 0   ,      11    21      &  &1      ++               "  "           "  "          ,            "      "                       "           3             "                    "        -            " 4  # $% $&% $'% $(% $&!%    4  ++ , ++     # 2 5 $  6& ++   " # $  7 ++           8 ++     "    62 ++  # $                      " .    ,                         & -                2 # 5&(62$    "                "         !  30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 417  )      "   9  /    7   :        !;  /   
It has been observed that the relation of possession contributes to the formation of socalled adversity causatives, whose subject is understood as a possessor of an object referent. This interpretation is reflected at face value in some studies, and it is assumed there that the subject argument is introduced as a possessor in syntax. This paper addresses the question of whether the observed relation should be directly encoded as such and argues that the subject argument is introduced as merely an event participant whose manner is underspecified. Moreover, it argues that the possessor interpretation arises from inference based on both linguistic and extralinguistic contexts, such as the presence of a possessum argument. This view is implemented as an analysis making use of a kind of applicative head (Pylkk√§nen, 2008) in conjunction with the postsyntactic inferential strategy (Rivero, 2004). 
 In this paper, we present our collective  effort to gather, annotate, and model  various language resources for use in  different research projects. This includes  those that are available online such as  tweets, Wikipedia articles, game chat,  online radio, and religious text. The  different applications, issues and  directions are also discussed in the paper.  Future works include developing a  language web service. A subset of the  resources will be made temporarily  available  online  at:  http://bit.ly/1MpcFoT.  
In this work, we aim at identifying potential problems of ambiguity, completeness, conformity, singularity and readability in system and software requirements speciÔ¨Åcations. Those problems arise particularly when they are written in Natural Language. We describe them from linguistic point of view but the business impacts of each potential error will be considered in system engineering context where our corpus come from. Several standards give the criteria on writing good requirements to guide requirement authors. These properties are linguistically observable because they appear as lexical, syntactic, semantic and discursive problems in documents. We investigate error patterns heavily used, by analyzing manually the corpus. This analysis is based on the requirements grammar that we developed in this work. We then propose an approach to identify them automatically by applying the rules developed from the error patterns to the POS tagged and parsed corpus. By using error annotated corpus, we can train the error model using CRFs and evaluate it. We obtain overall 79.17% F1 score for the error label annotation task. 
This paper adopts a comparable corpus-based approach to light verb variations in two varieties of Mandarin Chinese and proposes a transitivity (Hopper and Thompson 1980) based theoretical account. Light verbs are highly grammaticalized and lack strong collocation restrictions; hence it has been a challenge to empirical accounts. It is even more challenging to consider their variations between different varieties (e.g. Taiwan and Mainland Mandarin). This current study follows the research paradigm set up in Lin et al. (2014) for differentiating different light verbs and Huang et al. (2014) for automatic discovery of light verb variations. In our study, a corpus-based statistical approach is adopted to show that both internal variety differences between light verbs and external differences between different variants can be detected effectively. The distributional differences between Mainland and Taiwan can also shed light on the re-classification of syntactic types of the taken complement. We further argue that the variations in selection of arguments of light verb in two Mandarin variants can in fact be accounted for in terms of their different degree of transitivity. Higher degree of transitivity in Taiwan Mandarin in fact show that light verbs are less grammaticalized and hence consistent with the generalization that varieties away from the main speaking community should be more conservative. 
 This paper proposes a new methodology in investigating the semantic and pragmatic properties of SFPs in Mandarin Chinese. A case study of the interaction and correlation between SFP-Ne and SpOAs--Shenzhi, Qishi, and Nanguai has been conducted. Two semantic features of [+unexpectedness] and [+intersubjectivity] have been summarized on SFP-Ne.  
The goal of the current study is to suggest a novel paradigm of epistemic modal operator originated from the disjunction. Our main data is Korean disjunction operator nka which forms a non-factual question. Examining how the modal effect in question is induced by nka, I propose that the prerequisite of nka brings about non-homogenous nonveridical (i.e. modal) spaces partitioned in equipoised epistemic spaces, thus there is no bias between them. I furthermore show how the distinct notions of disjunction, question, and possibility modal can be captured under the theory of nonveridical equilibrium (Giannakidou 2013, Giannakidou and Mari 2016).  possibility at the same time. The statement is therefore marked by nka. It contrasts with the factual question marker ni in (1b) without such presumption by the speaker:  (1) Context: Mary, a reporter, was waiting for  John and Bill who were competing with each other  for the win in the finals of the chess competition.  After the match, John and Bill came out of the  room. John had a very subtle smile and Bill had a  poker face. Given their facial expressions, she  raises the possibility that John might have won.  Mary says:  a. Con-i  wusungca-i-nka?  John-Nom  winner-be-NKA  ‚ÄòCould John possibly be the winner?‚Äô  b. Con-i  wusungca-i-ni?  John-Nom winner-be-Q  ‚ÄòIs John the winner?‚Äô  
This paper investigates the non-gustatory uses of the gustatory word ‡™£ wei ‚Äòtaste‚Äô in Chinese Buddhist texts, in particular, in the ƒÄgamas. The non-gustatory uses of ‡™£ wei ‚Äòtaste‚Äô basically fall into two categories: the synaesthetic category and the metaphorical category. The former features the use of ‡™£ wei ‚Äòtaste‚Äô as an umbrella sensory term which can collocate with all the other sensory words, whereas the latter shows that ‡™£ wei ‚Äòtaste‚Äô can modify abstract and sublime Buddhist terms, such as ‚åÖ fa ‚Äòdhamma‚Äô and ‰Äì„ùõ jietuo ‚Äòenlightenment‚Äô, for the sake of concretization. These two categories of uses have one sense in common: the sense of ‚Äúpleasure and joy‚Äù, which can be interpreted in both mundane and supra-mundane levels, depending on the context. Moreover, we find that the versatile uses of ‡™£ wei ‚Äòtaste‚Äô are most likely to be influenced by its equivalent in the PƒÅli Buddhist texts. This finding sheds light on the history of Chinese language development, specifically, how Chinese language has been influenced by Buddhist text translation.  
Korean has locative construction as other languages do such as English. Although L2 acquisition of locative construction has been examined in L2 English research, few experimental investigations of Korean L2 acquisition have been conducted. The current study focused on the syntactic alternation among Figure Framed sentence, Ground Framed sentence, Figure only sentence and Ground only sentence. Forced choice task on 72 locative construction have been conducted by 21 Native Korean speakers and 20 advanced L1 English learners of Korean. L2ers showed different acceptability judgment on Korean locative construction which was distinct from their L1 argument structure. The results showed that these asymmetries were driven by L1 effect when the learnability problem arises due to insufficient input. Introductions Locative construction in languages imposes intriguing phenomenon in terms of case marking1. Locative verbs compose two different structures with a transitive verb. This phenomenon is known  as ‚Äòfigure/ground‚Äô alternation or locative alternation. Locative verbs denote a transfer of a substance or a set of objects (theme, content, or locatum) into or onto a container or surface (the goal, container, or location) as investigated in Pinker (1989). A substance or a set of objects are often referred as ‚Äòfigure‚Äô and a container or surface is referred as ‚Äòground‚Äô in the locative alternation studies. For example, English locative verb ‚Äòload‚Äô can have two structures of figure direct object [Figure Frame, henceforth FF] as in (1a) and ground direct object [Ground Frame, henceforth GF] as in (1b). (1) a. Irv loaded hay into the wagon. [Figure Frame] b. Irv loaded the wagon with hay. [Ground Frame] Semantically locative sentences which alternate between FF and GF have different interpretation, often called as ‚Äòholistic interpretation‚Äô 2 . Syntactically, FF locative constructions whose figure NPs are denoted as objects are argued as unmarked compared to GF. Since FF has unmarked case marking, they have canonical/unmarked  
This paper discusses predicative resultative constructions in Korean and argues that they are actually a kind of clausal resultative construction (see the two types of resultatives in Wechsler and Noh, 2001). In particular, I propose the following hypotheses: (i) the resultative predicate, X-key, is morphosyntactically an adverb rather than an adjective, (ii) X-key forms a fully saturated clause (i.e., result clause) (sometimes with the predication subject omitted), and (iii) the result clause is a complement of the main verb in a resultative sentence. Based on these properties, a unified analysis of the resultative constructions is formalized in Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003). 
This article, examining the qualia roles retrieved from the metaphorically/metonymically used body part terms in news texts, addresses the similarities and differences of such uses in Taiwan and China. Analyzing the behavior of ’à xue ‚Äòblood‚Äô and ‡¨Æ ‚Äòbone‚Äô, two corporeal terms with relatively high visibilities compared with ‘∫ rou ‚Äòflesh‚Äô and ‡´á mai ‚Äòmeridian‚Äô (Duann and Huang 2015) in the Chinese Gigaword Version 2 (Huang 2009), this research have the following findings: (1) For the use of ’à xue ‚Äòblood‚Äô, the agentive role predominates in both Taiwan and China, which is not in line with the argument in Duann and Huang (2015). (2) Regarding the use of ‡¨Æ gu ‚Äòbone‚Äô, the telic role predominates. However, China uses it in personification much more often than Taiwan does. (3) The unique dimension of a place triggers the use exclusive to the place. 
 2 Multimodal dialogue with a robot  In recent years, with the spread of the household robots, the necessity to enhance the communication capabilities of those robot to people has been increasing. The objective of this study is to build a framework for a dialogue system dealing with multimodal information that a robot observes. We have applied partially observable Markov Decision Process to modeling multimodal interaction between a human and a robot. Through the experiments, we have conÔ¨Årmed that our proposed framework functions properly and achieves effective multimodal interaction with a robot. 
We distinguished the two different uses of factive and NonFactive (NF) in the verb al-ta ‚Äòknow‚Äô in Korean and the distinction is crucially made by the different complement cases of factive ‚Äìul ACC and NF ‚Äìuro Directional (oblique). The NF use is possible with nonveridical/negative contexts in English and other languages but it is possible with a positive sentence with the Directional case in Korean uniquely (Hungarian only is similar in this respect and Japanese has no NF ‚Äòknow‚Äô). The NF ‚Äìuro al-ta verb, however, is different from other weaker epistemic verbs meaning ‚Äòbelieve‚Äô/‚Äòthink‚Äô in that it strongly tends to show some piece of evidence for JTB but the evidential justification may turn out to fall short of knowledge. We conducted experiments to clearly show that the NF ‚Äìuro al-ta has the relation of neg-raising between the high neg S and the low (complement) neg S, which are truthconditionally equivalent. It implies that this NF verb ‚Äìuro al-ta is identical in neg-raisability with other weaker epistemic verbs meaning ‚Äòbelieve‚Äô and ‚Äòthink‚Äô in Korean. An exerpt from Sejong Corpus indicates that the NF ‚Äòknow‚Äô in Korean typically accompies some piece of evidence that led the speaker to hold a firmer belief than other epistemic verbs meaning ‚Äòbelieve‚Äô/‚Äòthink‚Äô in Korean. 
This paper addresses the task of assigning Ô¨Ånegrained NE type labels to Wikipedia articles. To address the data sparseness problem, which is salient particularly in Ô¨Åne-grained type classiÔ¨Åcation, we introduce a multi-task learning framework where type classiÔ¨Åers are all jointly learned by a neural network with a hidden layer. In addition, we also propose to learn article vectors (i.e. entity embeddings) from Wikipedia‚Äôs hypertext structure using a Skipgram model and incorporate them into the input feature set. To conduct large-scale practical experiments, we created a new dataset containing over 22,000 manually labeled instances. The dataset is available. The results of our experiments show that both ideas gained their own statistically signiÔ¨Åcant improvement separately in classiÔ¨Åcation accuracy. 
Automatic detection and identifying emotions in large call center calls are essential to spot conversations that require further action. Most often statistical models generated using annotated emotional speech are used to design an emotion detection system. But annotation requires substantial amount of human intervention and cost; and may not be available for call center calls because of the infrastructure issues. Therefore detection systems use models that are generated form the readily available annotated emotional (clean) speech datasets and produce erroneous output due to mismatch in training-testing datasets. Here we propose a framework to automatically identify the similar affective spoken utterances in large number of call center calls by using the emotion models that are trained with the freely available acted emotional speech. Further, to reliably detect the emotional content, we incorporate the available knowledge associated with the call (time lapse of the utterances in a call, the contextual information derived from the linguistic contents, and speaker information). For each audio utterance, the emotion recognition system generates similarity measures (likelihood scores) in arousal and valence dimension using pretrained emotional models, and further they are combined with the scores from the contextual knowledge-based systems, which are used to reliably detect the similar affective contents in large number of calls. Experiments demonstrate that there is a signiÔ¨Åcant improvement in detection accuracy when the knowledge-based framework is used.  Index Terms:Affective content analysis; mining call center audio; spontaneous emotional speech; knowledge-based systems; similar affective states 
‚ÄúCase‚Äù and ‚Äúgrammatical function‚Äù are central to syntactic theories, but rigorous treatments of these notions in surface-oriented grammars like Dynamic Syntax (DS) are pending. Within DS, it is simply held that a case particle resolves structural uncertainty (i.e., unfixed node) in the course of incremental tree update. We model the relation between ‚Äúcase‚Äù and ‚Äúgrammatical function‚Äù with special reference to Japanese. In this language, the nominative case particle ga normally marks a ‚Äúsubject‚Äù NP, but it may mark an ‚Äúobject‚Äù NP. Moreover, ga may occur more than once within a single clause. We will address these issues by proposing the ‚Äúmaximal exclusion‚Äù approach to structural uncertainty. 
This paper presents open language resources for Korean. It includes several language processing models and systems including morphological analysis, part-of-speech tagging, syntactic parsing for Korean, and standard evaluation Korean-English machine translation data with the Korean-English statistical machine translation baseline system. We make them publicly available to pave the way for further development regarding Korean language processing. 
The typical measurement by which the nature of second language grammars is evaluated is the input of native speakers. This paper reports on data from Mandarin speakers of English (n = 19), with an average of 10;3 (year;month) length of residence in the U.S., and native American English speakers (n = 19), and looks at how they dealt with causatives, resultatives, and depictives under four experimental conditions. It was found that native participants did not always behave reliably; they altered, swung, and oscillated just like nonnative counterparts, and there were multiple cases where their fluctuation rates were way higher than those of the latter. Such variances were brought about by the effects of construction, task, or modality. These results cast doubt on the common practice of assessing second language grammars in terms of native intuitions and call on researchers to reconsider the assumption that second language grammars that are legitimate must be native-like. 
Language modeling is a fundamental research problem that has wide application for many NLP tasks. For estimating probabilities of natural language sentences, most research on language modeling use n-gram based approaches to factor sentence probabilities. However, the assumption under n-gram models is not robust enough to cope with the data sparseness problem, which affects the Ô¨Ånal performance of language models. At the point, Hierarchical Word Sequence (abbreviated as HWS) language models can be viewed as an effective alternative to normal n-gram method. In this paper, we generalize HWS models into a framework, where different assumptions can be adopted to rearrange word sequences in a totally unsupervised fashion, which greatly increases the expandability of HWS models. For evaluation, we compare our rearranged word sequences to conventional n-gram word sequences. Both intrinsic and extrinsic experiments verify that our framework can achieve better performance, proving that our method can be considered as a better alternative for ngram language models. 
This paper took an experimental approach and investigated how Korean EFL learners process the English island constructions. Since there are some controversies on the existence of the island effects in Korean, the L1 transfer effect may make it difficult for the Korean EFL learners to learn island constructions in English. To examine if the difference between English and Korean affects the acquisition of English island constructions, four different types of target sentences were made for English island phenomena: Complex-NP, whether, subject, and adjunct island. The acceptability scores of Korean EFL learners were measured with Magnitude Estimation (ME). Then, the collected data were statistically analyzed. The analysis results showed that, unlike previous studies, the Korean EFL learners correctly identified all of the English island constructions. This finding showed that the island status of the Korean language did not affect the acquisition of island constructions in English. 
In this paper, we present a method of predicting emotions from multi-label conversation transcripts. The transcripts are from a movie dialog corpus and annotated partly by 3 annotators. The method includes building an emotion lexicon bootstrapped from Wordnet following the notion of Plutchik‚Äôs basic emotions and dyads. The lexicon is then adapted to the training data by using a simple Neural Network to Ô¨Åne-tune the weights toward each basic emotion. We then use the adapted lexicon to extract the features and use them for another Deep Network which does the detection of emotions in conversation transcripts. The experiments were conducted to conÔ¨Årm the effectiveness of the method, which turned out to be nearly as good as a human annotator. 
A major claim in the literature is that a distribution of anaphoric elements either obeys or disobeys locality conditions. In addition, it has long been noted that the presence of a first (or second) person pronoun intervening between Chinese ziji and a higher potential antecedent blocks long-distance binding. However, this paper proposes that a third person antecedent can be a blocker in a given discourse, based on Kuno and Kaburaki‚Äôs (1977) system. If this is on the right track, the blocking effect in East Asian languages, especially Chinese ziji, Korean caki, and Japanese zibun, can be accounted for with a unified treatment. 
The Hierarchical Sub-Sentential Alignment (HSSA) method is a method to obtain aligned binary tree structures for two aligned sentences in translation correspondence. We propose to use the binary aligned tree structures delivered by this method as training data for preordering prior to machine translation. For that, we learn a Bracketing Transduction Grammar (BTG) from these binary aligned tree structures. In two oracle experiments in English to Japanese and Japanese to English translation, we show that it is theoretically possible to outperform a baseline system with a default distortion limit of 6, by about 2.5 and 5 BLEU points and, 7 and 10 RIBES points respectively, when preordering the source sentences using the learnt preordering model and using a distortion limit of 0. An attempt at learning a preordering model and its results are also reported. 
Symmetrization of word alignments is the fundamental issue in statistical machine translation (SMT). In this paper, we describe an novel reformulation of Hierarchical Subsentential Alignment (HSSA) method using F-measure. Starting with a soft alignment matrix, we use the F-measure to recursively split the matrix into two soft alignment submatrices. A direction is chosen as the same time on the basis of Inversion Transduction Grammar (ITG). In other words, our method simpliÔ¨Åes the processing of word alignment as recursive segmentation in a bipartite graph, which is simple and easy to implement. It can be considered as an alternative of growdiag-Ô¨Ånal-and heuristic. We show its application on phrase-based SMT systems combined with the state-of-the-art approaches. In addition, by feeding with word-to-word associations, it also can be a real-time word aligner. Our experiments show that, given a reliable lexicon translation table, this simple method can yield comparable results with state-of-theart approaches. 
This paper investigates how the French second person pronouns, tu and vous, are acquired by Korean learners of French. This is specifically approached from an interlanguage pragmatics research viewpoint, focusing upon the status of the learners‚Äô pragmalinguistic and sociopragmatic knowledge (whether they are explicit or implicit). It is hypothesized that Korean learners of French will face difficulties acquiring vous, but not with tu due to the similarities between French and Korean second person pronoun use in requests, mediated by their implicit/explicit knowledge. Using a discourse completion task and an error correction task, the findings support the hypothesis, showing the interplay between language transfer and their second language developmental status. Moreover, this was detectible by using a combination of tasks which allows pinpointing of knowledge used. The implications for explicit/implicit knowledge status in relation to the use of pragmatic knowledge are discussed against the degree of control learners have over tu and vous.  in the target language being acquired. The focus of this article is to investigate the system of politeness surrounding French tu and vous, from a Korean learner of French‚Äôs perspective. The acquisition of tu/vous is no easy matter; syntactically it may be straightforward to acquire two pronominal forms and use them grammatically, pragmatically however it is very difficult as your grammatical competence will not be much assistance to you in selecting the appropriate form for the situation you might find yourself in (to be discussed). Moreover, this may be compounded by influence from your first language which may not have the politeness concepts the target language has, as illustrated by the below examples (the meanings follow the English): 1a) Do you know what time it is? 1b) Vous avez l‚Äôheure? You have the time 1c) √∏/nuh myutsiinji ah-seyo/-ni? √∏/you what time-INT know-HON Q/nonHON Q  
Part-whole relation, or meronymy plays an important role in many domains. Among approaches to addressing the part-whole relation extraction task, the Espresso bootstrapping algorithm has proved to be effective by significantly improving recall while keeping high precision. In this paper, we Ô¨Årst investigate the effect of using Ô¨Åne-grained subtypes and careful seed selection step on the performance of extracting part-whole relation. Our multitask learning and careful seed selection were major factors for achieving higher precision. Then, we improve the Espresso bootstrapping algorithm for part-whole relation extraction task by integrating word embedding approach into its iterations. The key idea of our approach is utilizing an additional ranker component, namely Similarity Ranker in the Instances Extraction phase of the Espresso system. This ranker component uses embedding offset information between instance pairs of part-whole relation. The experiments show that our proposed system achieved a precision of 84.9% for harvesting instances of the partwhole relation, and outperformed the original Espresso system. 
Yoon (2008, 2009) claimed that there are two distinct Subjects in Multiple Subject Constructions (MSCs) in Korean. The crux of his argument hangs on reinterpreting the traditionally proposed subject diagnostics as distinguishing between the Grammatical Subject (GS) and the Major Subject (MS) in MSCs. The claimed diagnostics for GS and MS were examined experimentally in MSCs and corresponding Single Subject Constructions (SSCs). We found that: (i) MS diagnostics and GS diagnostics were differentiated even in SSCs and (ii) there was no statistically significant difference between MS and GS diagnostics in MSCs. Implications of these findings are discussed. 
This corpus study of the distribution of Mandarin minimizer negative polarity items connects word order patterns and focal constructions. The OV word order pattern is claimed to be a focal construal. However, the corpus analysis shows the majority of them stay in VO. This distribution is constrained by the information structure of Mandarin word order patterns and negative constructions. The requirement of focal prominence is clearly reflected in the types of cooccurring modifiers in VO and OV. This study of minimizers shows how emphatic pragmatic inferences are construed through the interaction between focal construction, negation, and numeral phrases. Key words: minimizers, negation, focus, word order, Mandarin Chinese 
Various unsupervised and semi-supervised methods have been proposed to tag and parse an unseen language. We explore delexicalized parsing, proposed by (Zeman and Resnik, 2008), and delexicalized tagging, proposed by (Yu et al., 2016). For both approaches we provide a detailed evaluation on Universal Dependencies data (Nivre et al., 2016), a de-facto standard for multi-lingual morphosyntactic processing (while the previous work used other datasets). Our results conÔ¨Årm that in separation, each of the two delexicalized techniques has some limited potential when no annotation of the target language is available. However, if used in combination, their errors multiply beyond acceptable limits. We demonstrate that even the tiniest bit of expert annotation in the target language may contain signiÔ¨Åcant potential and should be used if available. 
Comparable corpus is the most important resource in several NLP tasks. However, it is very expensive to collect manually. Lexical borrowing happened in almost all languages. We can use the loanwords to detect useful bilingual knowledge and expand the size of donor-recipient / recipient-donor comparable corpora. In this paper, we propose a recurrent neural network (RNN) based framework to identify loanwords in Uyghur. Additionally, we suggest two features: inverse language model feature and collocation feature to improve the performance of our model. Experimental results show that our approach outperforms several sequence labeling baselines. 
In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation. 
 How can we describe the relations between objects in a picture? As recent deep neural networks have exhibited impressive performance in identifying individual entities in a picture, in this study we turn our attention to recognize inter-object relations. To recognize open-domain relations, (a) we propose collecting relational concepts automatically from an image-text corpus. In addition, using collected relational instances, (b) we train a classiÔ¨Åer to recognize inter-object relations. A relation recognition experiment conducted in our study suggests that relative information calculated from objects improves relation recognition effectively. 
This paper discusses the implications of human word association norms on the modelling of word associations from large corpora and the relevance of different types of associations in the process of translation, with a focus on adjectives. It is observed that the proportion of paradigmatic responses found in English norms tends to be higher, whereas a clear preference for syntagmatic associations is exhibited in Chinese norms. Further comparison with corpus-based extracted associations, using various functions in the Sketch Engine, shows that collocational associations might be more effectively extracted, but there is also considerable individual variation for different words. It is suggested that although free associations elicited in isolated context serve to reveal a wide range of potential lexical relations, their usefulness and relevance in real language applications should consider the actual task and its information demand. A purposebased approach to construct cross-lingual word webs for computer-aided translation is thus proposed. 
Sentence alignment plays an essential role in building bilingual corpora which are valuable resources for many applications like statistical machine translation. In various approaches of sentence alignment, length-and-word-based methods which are based on sentence length and word correspondences have been shown to be the most effective. Nevertheless a drawback of using bilingual dictionaries trained by IBM Models in length-and-word-based methods is the problem of out-of-vocabulary (OOV). We propose using word similarity learned from monolingual corpora to overcome the problem. Experimental results showed that our method can reduce the OOV ratio and achieve a better performance than some other lengthand-word-based methods. This implies that using word similarity learned from monolingual data may help to deal with OOV problem in sentence alignment. Keywords: sentence alignment, out-ofvocabulary, word similarity, monolingual data 
Entity linking (EL) is the task of connecting mentions in texts to entities in a large-scale knowledge base such as Wikipedia. In this paper, we present a pipeline system for Japanese EL which consists of two standard components, namely candidate generation and candidate ranking. We investigate several techniques for each component, using a recently developed Japanese EL corpus. For candidate generation, we Ô¨Ånd that a concept dictionary using anchor texts of Wikipedia is more effective than methods based on surface similarity. For candidate ranking, we verify that a set of features used in English EL is effective in Japanese EL as well. In addition, by using a corpus that links Japanese mentions to Japanese Wikipedia entries, we are able to get rich context information from Japanese Wikipedia articles and beneÔ¨Åt mention disambiguation. It was not directly possible with previous EL corpora, which associate mentions to English Wikipedia entities. We take this advantage by exploring several embedding models that encode context information of Wikipedia entities, and show that they improve candidate ranking. As a whole, our system achieves 82.27% accuracy, signiÔ¨Åcantly outperforming previous work. 
Knowledge of usable goods (e.g., toothbrush is used to clean the teeth and treadmill is used for exercise) is ubiquitous and in constant demand. This study proposes semantic labels to capture aspects of knowledge of usable goods and builds a benchmark corpus, Usable Goods Corpus, to explore this new semantic labeling task. Our human annotation experiment shows that human annotators can generally identify pieces of information of usable goods in text. Our Ô¨Årst attempt toward the automatic identiÔ¨Åcation of such knowledge shows that a model using conditional random Ô¨Åelds approaches the human annotation (F score 73.2%). These results together suggest future directions to build a large-scale corpus and improve the automatic identiÔ¨Åcation of knowledge of usable goods. 
The main purpose of this paper is to provide a syntax-based analysis of the differences between the two Korean causal clauses, i.e. ese-clauses and nikka-clauses. Focusing on the various aspects of Mood distinction, we claim that nikka and eseclauses can be analyzed as indicatives and subjunctives, respectively. Such an analysis enables us to provide syntactic explanations for issues‚Äîwhat we call the 1st person restriction of ese-clauses and its obviation‚Äîwhich might be considered merely semantic/pragmatic issues. 
In this paper I examine what have often been considered the syntactic properties of Gapping constructions (Ross, 1970) and show that they are in fact discourse-pragmatic in nature. I offer a novel analysis of Gapping constructions by extending recent Question Under Discussion (QUD)-based accounts in Head-Driven Phrase Structure Grammar (Ginzburg and Sag, 2000; Ginzburg, 2012).  (Roberts, 1996/2012) accounts of non-sentential utterances in HPSG. In Section 2, I review three previous proposals and discuss their problems. In Section 3, I examine some widely accepted assumptions that have been used to characterize the syntax of Gapping constructions and show that they are not fully justiÔ¨Åed by empirical data. After discussing the relevance of Gapping constructions to QUD, I present a novel QUD-based analysis in Section 4. Section 5 concludes the paper.  
M√ºller and √òrsnes, In Preparation), 3. Persian (Indo-Iranian, DFG/ANR MU 2822/3-1, M√ºller, 2010, M√ºller und Ghayoomi, 2010), 4. Maltese (Semitic, M√ºller, 2009), 5. Mandarin Chinese (Sino-Tibetian, DFG MU 2822/5-1, Lipenkova, 2008; M√ºller and Lipenkova, 2009), 6. Spanish (Romance, SFB 632, A6), 7. French (Romance, SFB 632, A6), 8. Yiddisch (Germanic, M√ºller and √òrsnes, 2011) and 9. Hindi. The approach to developing the core grammar is bottom-up in that we do not assume a genetically determined Universal Grammar and try to prove its existence in language after language. Rather we treat every language in its own right and try to generalize over sets of languages only later. Some of this knowledge might be part of an UG in the above sense, but we do not make any claims on this issue. We also do not make an explicit core-periphery distinction while working on individual languages. Rather what belongs to the core is determined empirically by comparing languages. If we find a phenomenon in more than one language and think that it is correct to describe the phenomenon by the same means, the respective representations are kept in one file that is used by the respective grammars. This results in a grouping of languages that share the same code, with files containing very general constraints being used by all languages. 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 3 
30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 7   
Japan is faced with an imminent challenge of cultivating ‚Äòglobal human resources‚Äô, as the whole society delves into the global information society. The Course of Study defined by the Ministry of Education, Culture, and Sports and Technology (MEXT) of the Japanese government has emphasized communicative competence and / or ‚Äòcommunication skills‚Äô as a focus of the foreign language subjects (e.g., English) since 1900‚Äôs and the Courses of Study for most other subject also mention similar needs. During the academic year of 2014, the Educational Reform Working Group within the leading Liberal Democratic Party proposed the use of TOEFL iBT as an obligatory part of university entrance examination procedures. Furthermore, in 2015, MEXT advised the consideration of utilizing existing 4-skills English language proficiency tests that external test publishers have made available in Japan. Pearson offers various English tests that are automatically scored. Its spoken English test, Versant English Test, uses automated speech recognition and technologies. Versant Writing Test measures reading and writing skills and is scored automatically by using Latent Semantic Analysis. Approximately 60 firstyear students at the undergraduate School of Law at Waseda University took Versant  English Test and Versant Writing Test four times and the sores are compared to those of Oxford Placement English Test that the same students took three times. Oxford Quick Placement Test is designed to measure vocabulary, collocation, and grammar through reading-based multiple choice tasks. The present study reports results of analyses of these test scores and estimated CEFR levels, and then investigates challenges that Japanese learners and teachers of English are facing. 
This paper proposes a system to automatically summarise news articles in a manner suitable for children by deriving and combining statistical ratings for how important, positively oriented and easy to read each sentence is. Our results demonstrate that this approach succeeds in generating summaries that are suitable for children, and that there is further scope for combining this extractive approach with abstractive methods used in text simpliÔ¨Åcation. 
During content planning, a typical discourse generation system receives as input a library of facts and selects facts to include as the content for utterances. However, storytellers do not need to be completely constrained by a set of facts and instead can invent facts which support the storytellers goals, subsequently constructing the storyworld around those facts. We present a discourse-driven approach to narrative generation leveraging automated planning which can interleave construction of story and discourse while preserving modularity. Introduction ArtiÔ¨Åcial intelligence (AI) automated planning research (Ghallab et al., 2004) is a popular source of data structures and algorithms for understanding, generating, and reasoning about stories (Young et al., 2014). Narratologists frequently distinguish the fabula (i.e., story) of narrative from the discourse (e.g., the narration of the story to a spectator) (Genette and Lewin, 1983; Bruner, 1991; Herman, 2013), and plans have proven useful for modeling both story and discourse (Young, 2007); they are effective for modeling discourse because a coherent sequence of communicative actions is plan-like (Cohen and Perrault, 1979; Lambert and Carberry, 1991; Young and Moore, 1994), and plans are effective for modeling stories because stories are composed of events with cause-effect relations and characters also form plans to achieve their goals (Trabasso and Sperry, 1985; Riedl and Young,  2010). Behavioral research demonstrates that plans capture many key features that spectators use to understand narrative discourse (Trabasso and Sperry, 1985; Christian and Young, 2004; Ware et al., 2014; Radvansky et al., 2014; Cardona-Rivera et al., 2016). A typical approach to discourse generation is to supply a program with a library of facts about the domain of interest from which to select content for utterances (Meteer, 1991; Reiter and Dale, 1997). Narrative discourse generation systems are usually no different (Lo¬®nneker, 2005; Callaway and Lester, 2002); story is generated to meet some user-provided goals (i.e., a story plan solves a story problem) and passed through a pipeline as input for generating discourse and narration (e.g., text or animation) to solve a discourse problem (Callaway and Lester, 2002; Young et al., 2004; Jhala and Young, 2010; Cheong and Young, 2015) (see also (Young et al., 2014)). This pipeline architecture is amenable to the task of generating different discourse plans about the same set of events. However, analysis of these programs reveals that if there are story constraints associated with discourse planning operations, an input plan that solves a story problem may not meet those constraints needed to solve the discourse problem (i.e., the story plan is incompatible with the discourse goals), even though a compatible solution to the story problem exists. For example, to tell stories about characters who courageously navigate a dangerous terrain, one discourse action might be to convey that an obstacle, such as a bridge, is dangerous and has the constraint that some character dies at 
We present a method for generating English sentences from Abstract Meaning Representation (AMR) graphs, exploiting a parallel corpus of AMRs and English sentences. We treat AMR-to-English generation as phrase-based machine translation (PBMT). We introduce a method that learns to linearize tokens of AMR graphs into an English-like order. Our linearization reduces the amount of distortion in PBMT and increases generation quality. We report a Bleu score of 26.8 on the standard AMR/English test set. 
Our system generates summaries of hospital stays by combining information from two heterogenous sources: physician discharge notes and nursing plans of care. It extracts medical concepts from both sources; concepts that are identiÔ¨Åed as ‚Äúcomplex‚Äù by our metric are explained by providing deÔ¨Ånitions obtained from three external knowledge sources. Finally, relevant concepts (with or without deÔ¨Ånition) are realized by SimpleNLG. 
Standard algorithms for attribute choice in the generation of referring expressions have little to say about the role of Proper Names in referring expressions. We discuss the implications of letting these algorithms produce Proper Names and expressions that have Proper Names as parts. 
In this paper we introduce the task of abstractive caption or scene description compression. We describe a parallel dataset derived from the FLICKR30K and MSCOCO datasets. With this data we train an attention-based bidirectional LSTM recurrent neural network and compare the quality of its output to a Phrasebased Machine Translation (PBMT) model and a human generated short description. An extensive evaluation is done using automatic measures and human judgements. We show that the neural model outperforms the PBMT model. Additionally, we show that automatic measures are not very well suited for evaluating this text-to-text generation task. 
 2 Prior work in question generation  We present a fresh approach to automatic question generation that signiÔ¨Åcantly increases the percentage of acceptable questions compared to prior state-of-the-art systems. In our evaluation of the top 20 questions, our system generated 71% more acceptable questions by informing the generation process with Natural Language Understanding techniques. The system also introduces our DeconStructure algorithm which creates an intuitive and practical structure for easily accessing sentence functional constituents in NLP applications. 
We present a supervised approach to automatically labelling topic clusters of reader comments to online news. We use a feature set that includes both features capturing properties local to the cluster and features that capture aspects from the news article and from comments outside the cluster. We evaluate the approach in an automatic and a manual, task-based setting. Both evaluations show the approach to outperform a baseline method, which uses tf*idf to select comment-internal terms for use as topic labels. We illustrate how cluster labels can be used to generate cluster summaries and present two alternative summary formats: a pie chart summary and an abstractive summary. 
In research on automatic generation of narrative text, story events are often formally represented as a causal graph. When serializing and realizing this causal graph as natural language text, simple approaches produce cumbersome sentences with repetitive syntactic structure, e.g. long chains of ‚Äúbecause‚Äù clauses. In our research, we show that the Ô¨Çuency of narrative text generated from causal graphs can be improved by applying rule-based grammatical transformations to generate many sentence variations with equivalent semantics, then selecting the variation that has the highest probability using a probabilistic syntactic parser. We evaluate our approach by generating narrative text from causal graphs that encode 100 brief stories involving the same three characters, based on a classic Ô¨Ålm of experimental social psychology. Crowdsourced workers judged the writing quality of texts generated with ranked transformations as significantly higher than those without, and not signiÔ¨Åcantly lower than human-authored narratives of the same situations.  used to model and manipulate narrative elements including suspense (Cheong and Young, 2014), conÔ¨Çict (Ware and Young, 2011), Ô¨Çashback and foreshadowing (Bae and Young, 2008). Elson (2012) elaborates the causal network model by relating it to both the temporal ordering of story-world events and an author‚Äôs textual realization, creating a three-layer Story Intention Graph. Causal graph representations of narrative create new opportunities for natural language generation (NLG) of narrative text. For example, Lukin et al. (2015) describe a narrative NLG pipeline for Story Intention Graphs, generating variations of an original text that can be parameterized for particular discourse goals. When serializing and realizing a causal graph structure as natural language text, some care must be taken to avoid the generation of cumbersome sentences with repetitive syntactic structure, e.g. as a long chain of ‚Äúbecause‚Äù clauses. Lukin et al. (2015) directly compared readers‚Äô overall preferences for certain causal connectives over others, Ô¨Ånding that no single class of variations will produce sentences that are preferable to a human author‚Äôs stylistic choices.  
The emergence of the internet has led to a whole range of possibilities to not only collect large, but also highly specified text corpora for linguistic research. This paper introduces the Multilingual Affective Soccer Corpus. MASC is a collection of soccer match reports in English, German and Dutch. Parallel texts are collected manually from the involved soccer clubs‚Äô homepages with the aim of investigating the role of affect in sports reportage in different languages and cultures, taking into account the different perspectives of the teams and possible outcomes of a match. The analyzed aspects of emotional language will open up new approaches for biased automatic generation of texts. 
Given a controversial issue, argument mining from texts in natural language is extremely challenging: besides linguistic aspects, domain knowledge is often required together with appropriate forms of inferences to identify arguments. A major challenge is then to organize the arguments which have been mined to generate a synthesis that is relevant and usable. We show that the Generative Lexicon (GL) Qualia structure, enhanced in different manners and associated with inferences and language patterns, allows to capture the typical concepts found in arguments and to organize a relevant synthesis. 
The provision of personalized patient information has been encouraged as a means of complementing information provided during patient-doctor consultations, and linked to better health outcomes through patient compliance with prescribed treatments. The generation of such texts as a controlled fragment of Runyankore, a Bantu language indigenous to Uganda, requires the appropriate tense and aspect, as well as a method for verb conjugation. We present how an analysis of corpora of explanations of prescribed medications was used to identify the simple present tense and progressive aspect as appropriate for our selected domain. A CFG is deÔ¨Åned to conjugate and generate the correct form of the verb. 
Aiming to improve the human-likeness of natural language generation systems, this study investigates different sources of variation that might inÔ¨Çuence the production of referring expressions (REs), namely the effect of task demands and inter- intra- individual variation. We collected REs using a discrimination game and varied the instructions, telling speakers that they would get points for being fast, creative, clear, or no incentive would be mentioned. Our results show that taskdemands affected REs production (number of words, number of attributes), and we observe a considerable amount of variation among the length of REs produced by single speakers, as well as among the REs of different speakers referring to the same targets. 
In this paper, we introduce a content selection method where the communicative goal is to describe entities of different categories (e.g., astronauts, universities or monuments). We argue that this method provides an interesting basis both for generating descriptions of entities and for semi-automatically constructing a benchmark on which to train, test and compare data-to-text generation systems. 
Getting travel tips from the experienced bloggers and online forums has been one of the important supplements to the travel guidebook in the web society. In this paper we present a novel approach by identifying and extracting evaluative patterns, providing a different linguistically-motivated framework for automated evaluative text generation. We target at domain-specific observation in online travel blogs in Chinese. Results suggest that the semantic prosody accompanying the patterns demonstrates that online travel bloggers prefer to employ tacit pragmatic strategy in presenting their sentiment polarity in comments. The extracted patterns and their differentiation can be beneficial to identifying and characterizing evaluative language for further automated opinion summarization and macro/micro planning in natural language generation (NLG) as well. 
 rithm . However, these existing approaches have dif-  We discuss a fully statistical approach to the expression of quantitative information in English. We outline the approach, focussing on the problem of Lexical Choice. An initial evaluation experiment suggests that it is worth investigating the method further.  Ô¨Åculty handling situations in which a word expresses a combination of data dimensions, for example as when the word ‚Äúmilde¬®xpresses a combination of warm temperatures and low wind speed. In this paper, we discuss a new approach to the problem; the approach is fully statistical and it is  able to handle situations in which a word or phrase  
 Oliver Lemon Interaction Lab Heriot-Watt University o.lemon@hw.ac.uk  We present a multi-modal dialogue system for construct the more complex, total type of the visual  interactive learning of perceptually grounded word scene. This representation then acts not only as (1)  meanings from a human tutor (Yu et al., ). The the non-linguistic context of the dialogue for DS-  system integrates an incremental, semantic, and bi- TTR, for the resolution of e.g. deÔ¨Ånite references 
We present a novel approach to sentence simpliÔ¨Åcation which departs from previous work in two main ways. First, it requires neither hand written rules nor a training corpus of aligned standard and simpliÔ¨Åed sentences. Second, sentence splitting operates on deep semantic structure. We show (i) that the unsupervised framework we propose is competitive with four state-of-the-art supervised systems and (ii) that our semantic based approach allows for a principled and effective handling of sentence splitting. 
Currently, there is a lack of text complexity awareness in NLG systems. Much attention has been given to text simplification. However, based upon results of an experiment, we unveiled that sophisticated readers in fact would rather read more sophisticated text, instead of the simplest text they could get. Therefore, we propose a technique that considers different readability levels during the micro planning phase of an NLG system. Our technique considers grammatical and syntactic choices, as well as lexical items, when generating text. The application uses the domain of descriptive summaries of line graphs as its use case. The technique proposed uses learning for identifying features of text complexity; a graph search algorithm for efficient aggregation given a target reading level, and a combination of language modeling and word vectors for the creation of a domain-aware synset which allows the creation of disambiguated lexicon that is appropriate to different reading levels. We found that generating text at different target reading levels is indeed preferred by readers with varying reading abilities. To the best of our knowledge, this is the first time readability awareness is considered in the micro planning phase of NLG systems.  
As language-based interaction becomes more ubiquitous and is used by in a larger and larger variety of different situations, the challenge for NLG systems is to not only convey a certain message correctly, but also do so in a way that is appropriate to the situation and the user. From various studies, we know that humans adapt the way they formulate their utterances to their conversational partners and may also change the way they say things as a function of the situation that the conversational partner is in (e.g. while talking to someone who is driving a car). Approaches from psycholinguistics (using information-theoretic measures as well as other complexity metrics) provide a way to formulate and quantify the demands that a certain formulation places on a hearer. In this talk, I will brieÔ¨Çy survey ways of assessing human cognitive load in realistic settings, present current models of information density at the content level, and discuss the extent to which these measures have been found to drive choice of formulation in humans. 132 Proceedings of The 9th International Natural Language Generation conference, page 132, Edinburgh, UK, September 5-8 2016. c 2016 Association for Computational Linguistics 
In this paper we propose content selection methods for question generation (QG) which exploit domain knowledge. Traditionally, QG systems apply syntactical transformation on individual sentences to generate open domain questions. We hypothesize that a QG system informed by domain knowledge can ask more important questions. To this end, we propose two lightly-supervised methods to select salient target concepts for QG based on domain knowledge collected from a corpus. One method selects important semantic roles with bootstrapping and the other selects important semantic relations with Open Information Extraction (OpenIE). We demonstrate the effectiveness of the two proposed methods on heterogeneous corpora in the business domain. This work exploits domain knowledge in QG task and provides a promising paradigm to generate domain-speciÔ¨Åc questions. 
Most of the existing natural language generation (NLG) techniques employing statistical methods are typically resource and time intensive. On the other hand, handcrafted rulebased and template-based NLG systems typically require signiÔ¨Åcant human/designer efforts. In this paper, we proposed a statistical NLG technique which does not require any semantic relational knowledge and takes much less time to generate output text. The system can be used in those cases where source non-textual data are in the form of tuple in some tabular dataset. We carried out our experiments on the Prodigy-METEO wind forecasting dataset. For the evaluation purpose, we used both human evaluation and automatic evaluation. From the evaluation results we found that the linguistic quality and correctness of the texts generated by the system are better than many existing NLG systems. 
One of the limitations of semantic parsing approaches to open-domain question answering is the lexicosyntactic gap between natural language questions and knowledge base entries ‚Äì there are many ways to ask a question, all with the same answer. In this paper we propose to bridge this gap by generating paraphrases of the input question with the goal that at least one of them will be correctly mapped to a knowledge-base query. We introduce a novel grammar model for paraphrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the Ô¨Çexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines. 
 the (different) sets of templates employed to render  We propose a competitive shared evaluation task for Narrative Generation. It would involve the generation of new stories for a given domain from common ground knowledge shared by all systems. A set of source materials will be provided for development,  them. A common approach to acquiring knowledge resources is to mine a set of reference stories, to obtain from them the required knowledge. These resources usually make explicit two types of information that is implicit in the stories: relation between events in the story and latent variables relevant to  represented in Controlled Natural Language (CNL), which should also be used to phrase the text outputs of participating systems. By having all participating systems operate from the same sources for knowledge and generate in a compatible output format, comparability of the results will be enhanced. Submitted re-  it ‚Äì such as causality, emotion, afÔ¨Ånities between characters, narratological concepts... ‚Äì, and/or information about typical/acceptable sequencing between events ‚Äì depending on the degree of reÔ¨Ånement of the system, sometimes based on the latent variables.  sults will be subject to both automatic and human evaluation.  The present proposal revolves around the idea of developing a Controlled Natural Language (CNL)  
In the highly multilingual setting in South Africa, developing computational tools to support the 11 ofÔ¨Åcial languages will facilitate effective communication. The exigency to develop these tools for healthcare applications and doctor-patient interaction is there. An important component in this set-up is generating sentences in the language isiZulu, which involves part-whole relations to communicate, for instance, which part of one‚Äôs body hurts. From a NLG viewpoint, the main challenge is the Ô¨Çuid use of terminology and the consequent complex agreement system inherent in the language, which is further complicated by phonological conditioning in the linguistic realisation stage. Through using a combined approach of examples and various literature, we devised verbalisation patterns for both meronymic and mereological relations, being structural/general parthood, involvement, containment, membership, subquantities, participation, and constitution. All patterns were then converted into algorithms and have been implemented as a proof-of-concept. 
 guage speciÔ¨Åc features, like word order, inÔ¨Çection  and selection of functional words.  This paper describes the SimpleNLG-IT realiser, i.e. the main features of the porting of the SimpleNLG API system (Gatt and Reiter, 2009) to Italian. The paper gives some details about the grammar and the lexicon employed by the system and reports some results about  Surface realisers can be classiÔ¨Åed on the basis of their input. Fully Ô¨Çedged realisers accept as input an unordered and uninÔ¨Çected proto-syntactic structure enriched with semantic and pragmatic features that are used to produce the most plausible output string.  a Ô¨Årst evaluation based on a dependency treebank for Italian. A comparison is developed with the previous projects developed for this task for English and French, which is based on the morpho-syntactical differences and similarities between Italian and these languages.  OpenCCG is a member of this category of realisers (White, 2006). Indeed, OpenCCG accepts as input a semantic graph representing a set of hybrid logic formulas. The hybrid logic elements are indeed the semantic speciÔ¨Åcation of syntactic CCG structures deÔ¨Åned in the grammar realiser. The semantic  
We tackle the sub-task of content selection as part of the broader challenge of automatically generating image descriptions. More speciÔ¨Åcally, we explore how decisions can be made to select what object instances should be mentioned in an image description, given an image and labelled bounding boxes. We propose casting the content selection problem as a learning to rank problem, where object instances that are most likely to be mentioned by humans when describing an image are ranked higher than those that are less likely to be mentioned. Several features are explored: those derived from bounding box localisations, from concept labels, and from image regions. Object instances are then selected based on the ranked list, where we investigate several methods for choosing a stopping criterion as the ‚Äòcut-off‚Äô point for objects in the ranked list. Our best-performing method achieves state-of-the-art performance on the ImageCLEF2015 sentence generation challenge. 
We explore a novel application of Question Generation (QG) for authentication use, where questions are widely used to verify user identity for online accounts. In our approach, we prompt users to provide a few sentences about their personal life events. We transform user-provided input sentences into a set of simple fact-based authentication questions. We compared our approach with previous QG systems, and evaluation results show that our approach yielded better performance and the promise of future personalized authentication question generation. 
The Learning Analytics Report Card (LARC) is a pilot system which takes time-series data from a student‚Äôs course-related activity in a Virtual Learning Environment and generates automatic textual summaries in real time. Students are able to generate reports as often as they like, and to choose which aspects of their behaviour are included in each report. As well as rating a student‚Äôs scores against set standards, the generated texts make comparisons with the individual student‚Äôs previous behaviour from the same course, and with the average scores of their student cohort. In addition, we carry out sentiment analysis on the student‚Äôs forum posts, and generate a summary using quantiÔ¨Åers. We report some student reactions to initial trials of the system.  testing phases of LARC were informed by formal student representation, motivated by a general concern for ethical practices in data collection. Students can experience learning analytics applied to them as individuals as ‚Äúsnooping‚Äù (Parr, 2014), and the LARC project aimed to avoid this by giving students a chance to interact with their data. The students taking part in the pilot project were studying either ‚ÄúUnderstanding Learning in the Online Environment‚Äù or ‚ÄúDigital Futures for Learning‚Äù and were asked to provide feedback about the LARC system. We intended that some of the generated texts would be controversial, and would provoke strong reactions from students, to cause them to consider aspects of data interpretation and ownership. 2 Related Work  
Question generation (QG) is the problem of automatically generating questions from inputs such as declarative sentences. The Shared Evaluation Task Challenge (QG-STEC) Task B that took place in 2010 evaluated several state-of-the-art QG systems. However, analysis of the evaluation results was affected by low inter-rater reliability. We adapted Nonaka & Takeuchi‚Äôs knowledge creation cycle to the task of improving the evaluation annotation guidelines with a preliminary test showing clearly improved inter-rater reliability. 
In this paper, we challenge a form of paragraph-to-question generation task. We propose a question generation system which can generate a set of comprehensive questions from a body of text. Besides the tree kernel functions to assess the grammatically of the generated questions, our goal is to rank them by using community-based question answering systems to calculate the importance of the generated questions. The main assumption behind our work is that each body of text is related to a topic of interest and it has a comprehensive information about the topic. 
We introduce a corpus for the study of proper name generation. The corpus consists of proper name references to people in webpages, extracted from the Wikilinks corpus. In our analyses, we aim to identify the different ways, in terms of length and form, in which a proper names are produced throughout a text. 
We investigate the characteristics and quantiÔ¨Åable predispositions of both n-gram and recurrent neural language models in the framework of language generation. In modern applications, neural models have been widely adopted, as they have empirically provided better results. However, there is a lack of deep analysis of the models and how they relate to real language and its structural properties. We attempt to perform such an investigation by analyzing corpora generated by sampling from the models. The results are compared to each other and to the results of the same analysis applied to the training corpus. We carried out these experiments on varieties of KneserNey smoothed n-gram models and basic recurrent neural language models. Our results reveal a number of distinctive characteristics of each model, and offer insights into their behavior. Our general approach also provides a framework in which to perform further analysis of language models. 
 some research came close (Mitchell et al., 2012;  Kulkarni et al., 2013; Yang et al., 2011). Elliott &  In this paper, we look at automatic generation  Keller (Elliott and Keller, 2013) did address the sub-  of spatial descriptions in French, more particularly, selecting a spatial preposition for a pair of objects in an image. Our focus is on assessing the effect on accuracy of (i) increasing data set size, (ii) removing synonyms from the set  task, but with hardwired rules for just eight preposition. The work reported by Ramisa et al. (2015) is closely related to our work and also uses geometric and label features to predict prepositions.  of prepositions used for annotation, (iii) optimising feature sets, and (iv) training on best prepositions only vs. training on all accept-  2 Data The new data set we have created for the experi-  able prepositions. We describe a new data set  ments in this paper is a set of photographs in which  where each object pair in each image is annotated with the best and all acceptable prepositions that describe the spatial relationship between the two objects. We report results for three new methods for this task, and Ô¨Ånd that the best, 75% Accuracy, is 25 points higher than our previous best result for this task.  objects in 20 classes are annotated with bounding boxes and class labels, and each object pair with prepositions that describe the spatial relationship between the objects. The data was derived from the VOC‚Äô08 data (Everingham et al., 2010) by selecting images with 2 or 3 bounding boxes, and adding the preposition annotations. The data has twice as many  
We introduce QGASP, a system that performs question generation by using lexical, syntactic and semantic information. QGASP uses this information both to learn patterns and to generate questions. In this paper, we brieÔ¨Çy describe its architecture.  
The current interest in data acquisition and analysis has resulted in a large number of solutions available to the public. However, anyone other than professionals in the Ô¨Åeld can Ô¨Ånd it difÔ¨Åcult to make sense of this sea of data. This demo showcases a tool that produces general static reports (as opposed to query or intention based systems of past NLG interest) of combined text and graphics given any spreadsheet sent by email. 
 2014; Gkatzia et al., 2015) have become available  and can now serve as a realistic test bed for mod-  Colour terms have been a prime phenomenon for studying language grounding, though previous work focussed mostly on descriptions of simple objects or colour swatches. This paper investigates whether colour terms can be learned from more realistic and potentially  els of language grounding. In this paper, we use the ReferIt corpus (Kazemzadeh et al., 2014) to assess the performance of classiÔ¨Åers that predict colour terms from low-level visual representations of their corresponding image regions.  noisy visual inputs, using a corpus of referring expressions to objects represented as regions in real-world images. We obtain promising results from combining a classiÔ¨Åer that grounds colour terms in visual input with a recalibration model that adjusts probability distributions over colour terms according to contextual and object-speciÔ¨Åc preferences.  A number of studies on colour naming have looked at experimental settings where speakers referred to simple objects or colour swatches instantiating a single value in a colour space. Even in these controlled settings, speakers use colour terms in Ô¨Çexible, context-dependent ways (Baumgaertner et al., 2012; Meo et al., 2014). Therefore, probabilistic  models and classiÔ¨Åers, allowing for variable thresh-  
This paper discusses the importance of computing relative properties and not just retrieving absolute properties when generating geographic referring expressions such as ‚Äúnorthern France‚Äù. We describe an algorithm that computes spatial properties at run-time by means of spatial operations such as intersecting and analyzing parts of wholes. The evaluation of the algorithm suggests that part-whole relations are key in geographic expressions. 
 quality training data consisting of meaning repre-  sentations (MR) paired with Natural Language (NL)  Recent advances in corpus-based Natural Lan-  utterances, augmented by alignments between MR  guage Generation (NLG) hold the promise of being easily portable across domains, but require costly training data, consisting of meaning representations (MRs) paired with Natural Language (NL) utterances. In this work, we propose a novel framework for crowdsourcing high quality NLG training data, using automatic quality control measures and  elements and NL words. Recent work (DusÀáek and JurcÀá¬¥ƒ±cÀáek, 2015; Wen et al., 2015) removes the need for alignment, but the question of where to get indomain training data of sufÔ¨Åcient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evalu-  evaluating different MRs with which to elicit data. We show that pictorial MRs result in better NL data being collected than logicbased MRs: utterances elicited by pictorial MRs are judged as signiÔ¨Åcantly more natural,  ating different meaning representations. So far, we collected 1410 utterances using this framework. The data will be released as part of this submission. 2 Background  more informative, and better phrased, with a signiÔ¨Åcant increase in average quality ratings (around 0.5 points on a 6-point scale), compared to using the logical MRs. As the MR becomes more complex, the beneÔ¨Åts of pictorial stimuli increase. The collected data will  Apart from (Mairesse et al., 2010), this research is the Ô¨Årst to investigate crowdsourcing for collecting NLG data. So far, crowdsourcing is mainly used for evaluation in the NLG community, e.g. (Rieser et al., 2014; Dethlefs et al., 2012). Recent efforts in  be released as part of this submission.  corpus creation via crowdsourcing have proven to  be successful in related tasks. For example, (Zaidan  
This paper presents work on how we can link word lists derived from learner corpora to target proÔ¨Åciency levels for lexical complexity analysis. The word lists present frequency distributions over different proÔ¨Åciency levels. We present a mapping approach which takes these distributions and maps each word to a single proÔ¨Åciency level. We are also investigating how we can evaluate the mapping from distribution to proÔ¨Åciency level. We show that the distributional proÔ¨Åle of words from the essays, informed with the essays‚Äô levels, consistently overlaps with our frequency-based method, in the sense that words holding the same level of proÔ¨Åciency as predicted by our mapping tend to cluster together in a semantic space. In the absence of a gold standard, this information can be useful to see how often a word is associated with the same level in two different models. Also, in this case we have a similarity measure that can show which words are more central to a given level and which words are more peripheral. 
Most previous research on young infants‚Äô spoken word comprehension has focused on monolinguals. These results may not generalize to non-monolingual populations because lexical processing may be more intricate for infants exposed to more than one language. Do toddlers learning multiple languages recognize words similarly to their monolingual peers? Answering this question will require extensive efforts, to which we contribute word comprehension data collected through a procedure aiming to be both precise and ecological. French-learning toddlers (N = 38; age range 1;11-3;4) were tested in their daycare, using a French-spoken promptto-picture matching task implemented on a child-friendly touch screen. Our results document some differences in accuracy, but not response time or number of trials completed, among toddlers differing in the number of languages they routinely hear. Additionally, these data suggest that it is feasible to collect good quality data from multiple children tested at once in daycares, opening the path to largerscale studies. Future research could disentangle the many factors that are often empirically confounded with monolingual versus bilingual/multilingual status. 
The Language ENvironment Analysis (LENA) System is a relatively new recording technology that can be used to investigate typical child language acquisition and populations with language disorders. The purpose of this paper is to familiarize language acquisition researchers and speech-language pathologists with how the LENA System is currently being used in research. The authors outline issues in peer-reviewed research based on the device. Considerations when using the LENA System are discussed. 
Models of cross-language perception suggest that listeners‚Äô native language plays a significant role in perceiving another language, and propose that listeners assimilate non-native speech sounds to similar sounds in their native language. In this study, the effect of native language on the perception of Mandarin tones by Swedish learners is examined. Swedish learners participated in an identification task, and their performance was analyzed in terms of accuracy percentages and error patterns. The ranking of difficulty level among the four lexical tones by Swedish listeners differs from that found among English native listeners in previous studies. The error patterns also reveal that Swedish listeners confuse Tone 1 and 2, Tone 3 and 4, and Tone 2 and 4, the first two pairs rarely being confused by English listeners. These findings may be explained with the assimilation account: Swedish learners assimilate Tone 3 and 4 to Swedish pitch accents, thus they exhibit a unique pattern when perceiving the tones in Mandarin. 
We present a language-independent tool, called Varseta, for extracting variation sets in child-directed speech. This tool is evaluated against a gold standard corpus annotated with variation sets, MINGLE-3-VS, and used to explore variation sets in 26 languages1 in CHILDES-26-VS, a comparable corpus derived from the CHILDES database. The tool and the resources are freely available for research.2 
Bundled gap Ô¨Ålling exercises (Wojatzki et al., 2016) were recently introduced as a promising new exercise type to complement or even replace single gap-Ô¨Åll tasks. However, it is not yet conÔ¨Årmed that the applied creation method works properly and it is still to be investigated if bundled gap-Ô¨Åll tests are a suitable method for assessing language proÔ¨Åciency. In this paper, we address both issues by varying the construction methods and by conducting a user study with 75 participants in which we also measure externally validated language proÔ¨Åciency. We Ô¨Ånd that the originally proposed way to construct bundles is indeed minimizing their ambiguity, but that further investigation is needed to determine which aspects of language proÔ¨Åciency they are actually measuring. 
Speech-enabled dialogue systems developed within an iCALL framework offer a potentially powerful tool for dealing with the challenges of teaching/learning an endangered language where learners have limited access to native speaker models of the language and limited exposure to the language in a truly communicative setting. This paper explores the major potential of virtual conversational agent systems with inbuilt simulated ‚Äòintelligence‚Äô for the Irish (endangered) language context.  dialogue systems, which can still have impact in the teaching/learning context. Following a brief discussion of the sociolinguistic context of current developments for Irish, this paper presents (i) a simulated intelligent dialogue partner, constructed for Irish language tuition, using synthetic voices and an animated avatar (a talking monkey), (ii) a discussion on how, in the absence of NLP-based resources (yet to be developed for Irish), specific strategies are adopted which allow the impression of ‚Äòintelligent‚Äô discourse with an agent, and (iii) an outline of the steps envisaged to allow a fuller, more ‚Äòintelligent‚Äô system, using NLP resources.  
In this paper we describe an open learner corpus of Russian. The Russian Learner Corpus (RLC) is the Ô¨Årst corpus with clear distinction between foreign language learners and heritage speakers. We discuss the structure of the corpus, its development and the annotation principles. This paper describes the platform of the RLC which combines online tools for text uploading, processing, error annotation and corpus search.  automatic error detection (Leacock et al., 2014), etc. The goal of this paper is to present a recently created Russian Learner Corpus (RLC)1. The novelty of the RLC is threefold: 1. It is the Ô¨Årst open learner corpus for the Russian language enabling search over lemma, grammatical features, and error tags; 2. It is the Ô¨Årst learner corpus that draws a clear distinction between HL (heritage language)2 and L2 (second language) speakers;  
This paper presents a new lexical resource for learners of Swedish as a second language, SweLLex, and a know-how behind its creation. We concentrate on L2 learners‚Äô productive vocabulary, i.e. words that they are actively able to produce, rather than the lexica they comprehend (receptive vocabulary). The proposed list covers productive vocabulary used by L2 learners in their essays. Each lexical item on the list is connected to its frequency distribution over the six levels of proÔ¨Åciency deÔ¨Åned by the Common European Framework of Reference (CEFR) (Council of Europe, 2001). To make this list a more reliable resource, we experiment with normalizing L2 word-level errors by replacing them with their correct equivalents. SweLLex has been tested in a prototype system for automatic CEFR level classiÔ¨Åcation of essays as well as in a visualization tool aimed at exploring L2 vocabulary contrasting receptive and productive vocabulary usage at different levels of language proÔ¨Åciency. 
Moses is a well-known representative of the phrase-based statistical machine translation systems family, which are known to be extremely poor in explicit linguistic knowledge, operating on Ô¨Çat language representations, consisting only of tokens and phrases. Treex, on the other hand, is a highly linguistically motivated NLP toolkit, operating on several layers of language representation, rich in linguistic annotations. Its main application is TectoMT, a hybrid machine translation system with deep syntax transfer. We review a large number of machine translation systems that have been built over the past years by combining Moses and Treex/TectoMT in various ways. 
While much work has been done to inform Hierarchical Phrase-Based SMT (Chiang, 2005) models linguistically, the adjunct/argument distinction has generally not been exploited for these models. But as Shieber (2007) points out, capturing this distinction allows to abstract over ‚Äòintervening‚Äô adjuncts, and is thus relevant for (machine) translation in general. We contribute an adjunction-driven approach to hierarchical phrase-based modelling that uses source-side adjuncts to relax extraction constraints‚Äìallowing to capturing long-distance dependencies‚Äì, and to guide translation through labelling. The labelling scheme can be reduced to two adjunct/non-adjunct labels, and improves translation over Hiero by up to 0.6 BLEU points for English-Chinese. 
This paper presents a Hybrid Approach to Deep Machine Translation in the language direction from English to Bulgarian. The set-up uses pre- and post-processing modules as well as two-level transfer. The language resources that have been incorporated are: WordNets for both languages; a valency lexicon for Bulgarian; aligned parallel corpora. The architecture comprises a predominantly statistical component (factor-based SMT in Moses) with some focused rule-based elements. The experiments show promising results and room for further improvements within the MT architecture. 
This paper describes a hybrid Machine Translation (MT) system built for translating from English to German in the domain of technical documentation. The system is based on three different MT engines (phrase-based SMT, RBMT, neural) that are joined by a selection mechanism that uses deep linguistic features within a machine learning process. It also presents a detailed source-driven manual error analysis we have performed using a dedicated ‚Äútest suite‚Äù that contains selected examples of relevant phenomena. While automatic scores show huge differences between the engines, the overall average number or errors they (do not) make is very similar for all systems. However, the detailed error breakdown shows that the systems behave very differently concerning the various phenomena. 
In this paper, we focus on the incorporation of a valency lexicon into TectoMT system for Czech-Russian language pair. We demonstrate valency errors in MT output and describe how the introduction of a lexicon inÔ¨Çuenced the translation results. Though there was no impact on BLEU score, the manual inspection of concrete cases showed some improvement. 
 While vector representations of word meaning  are capable of capturing important semantic fea-  Popular distributional approaches to se-  tures of words and performing tasks like meaning  mantics allow for only a single embedding  comparison and analogizing, one of their short-  of any particular word. A single embed-  comings is their implicit assumption that a sin-  ding per word conÔ¨Çates the distinct mean-  gle written word type has exactly one meaning  ings of the word and their appropriate con-  (or distribution) in a language. But many words  texts, irrespective of whether those usages  clearly have different senses corresponding to dis-  are related or completely disjoint. We  tinct appropriate contexts. Building distributional  compare models that use the graph struc-  vector space models that account for this polyse-  ture of the knowledge base WordNet as  mous behavior would allow for better performance  a post-processing step to improve vector-  on tasks involving context-sensitive words, most  space models with multiple sense embed-  obviously word sense disambiguation. Previous  dings for each word, and explore the ap-  research that attempted to resolve this issue is dis-  plication to word sense disambiguation.  cussed at length in the next section. Most common  Keywords: Vector Semantics, WordNet, Synonym Selection, Word Sense Disambiguation  methods either use clustering or introduce knowledge from an ontology. The goal of the present research is to develop or improve upon methods  
Rule-based machine translation (RBMT) and Statistical machine translation (SMT) are two well-known approaches for translation which have their own benefits. System architecture of SMT often complements RBMT, and the vice-versa. In this paper, we propose an effective method of serial coupling where we attempt to build a hybrid model that exploits the benefits of both the architectures. The first part of coupling is used to obtain good lexical selection and robustness, second part is used to improve syntax and the final one is designed to combine other modules along with the best phrase reordering. Our experiments on a English-Hindi product domain dataset show the effectiveness of the proposed approach with improvement in BLEU score. 
 still has difÔ¨Åculty handling larger phrase structures  The use of distributional semantics to represent the meaning of a single word has proven to be very effective, but there still is difÔ¨Åculty representing the meaning of larger constituents, such as a noun phrase. In general, it is unclear how to Ô¨Ånd a representation of phrases that preserves syntactic distinctions and the relationship between a compound‚Äôs constituents. This paper is an attempt to Ô¨Ånd the best representation of nominal compounds in Spanish and English, and evaluates the performance of different compositional models by using correlations with human similarity judgments and by using compositional representations as input into an SVM classifying the semantic relation between nouns within a compound. This paper also evaluates the utility of different function‚Äôs compositional representations, which give our model a slight advantage in accuracy over other state-of-the-art semantic relation classiÔ¨Åers.  and function words, as opposed to just isolated content words (Mitchell and Lapata, 2008). Vectors for larger phrases cannot be reliably used due to the sparseness of data (Erk, 2012). Ways of representing compositional models for constituents larger than a single word that preserve the lexical and syntactic function of a word in a phrase and best represent the relation between the constituents of a phrase is desired in creating a more general and powerful framework for natural language semantics. (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), and (Guevara, 2010) have compared and empirically tested the effectiveness of different mathematical compositions in representing adjective-noun, verb-object, and noun-noun compounds, but there has been little research into representing nominal compounds that are longer than two words, and the vast majority of research has been in English, without crosslinguistic inquiries (Mitchell and Lapata, 2010).  Keywords compositional distributional semantics, nominal compounds, nominal compounds in Spanish  This paper will investigate the effectiveness of a variety of different compositional functions using two metrics: correlation of the model‚Äôs cosine similarity predictions with human similarity judgments for  
 cial Organisations are therefore incorporating sen-  Sentiment analysis is a discipline of Natural Language Processing which deals with analysing the subjectivity of the data. It is an important task with both commercial and academic functionality. Languages like English have several resources which assist in the task of sentiment analysis. SentiWordNet for English is one such important lexical resource that contains subjective polarity for each lexical item. With growing data in native vernacular, there is a need for language-speciÔ¨Åc SentiWordNet(s). In this paper, we discuss a generic approach followed for the development of a Tamil SentiWordNet using currently available resources in English. For Tamil SentiWordNet, a substantial agreement Fleiss Kappa score of 0.663 was obtained after veriÔ¨Åcation from Tamil annotators. Such a resource would serve as a baseline for future improvements in the task of sentiment analysis speciÔ¨Åc to Tamil data.  timent analysis systems1 for customer feedback and product review. For good governance, feedback from the public through social media and other surveys is monitored at a large scale. The public prefers to give feedback in its own vernacular. Analysing the sentiment in their feedback in various Indian languages hence demands language speciÔ¨Åc subjective lexicons. This served as the motivation for the creation of SentiWordNet for Tamil. A translation based approach has been adopted to build this resource using various lexicons in English. Each of these lexicons comprises of English words with certain polarity. After several levels of preprocessing, a Ô¨Ånal set of English words was obtained. These words were then translated into Tamil using Google Translate2. The Ô¨Ånal set of words were annotated with either positive or negative polarity based on its prior polarity in English. The Ô¨Ånal lexicon was checked by Tamil annotators to remove any ambiguous entries and also for accuracy of translation. The various tools used for the construction of SentiWordNet for Tamil include English Senti-  
 ing similar attributes is called as named entity.  Named Entity Disambiguation (NED) is gaining popularity due to its applications in the Ô¨Åeld of information extraction. Entity linking or Named Entity Disambiguation is the task of discovering entities such as persons, locations, organizations, etc. and is challenging due to the high ambiguity of entity names in natural language text. In this paper, we propose a modiÔ¨Åcation to the existing state of the art for NED, Accurate Online Disambiguation of Entities (AIDA) framework. As a mention‚Äôs name in a text can appear many times in shorter forms, we propose to use corefer-  Names can be rigid or non-rigid. Rigid names refer to one and only one thing like ‚ÄúNarendra Modi‚Äù. Non-rigid names refer to different objects like ‚ÄúHome Minister‚Äù (Home Minister of India or Srilanka). In general, we can say proper names are rigid and common names are non-rigid. Articles on the web consist of names of persons, locations, organizations, events etc. The same name can have a different meaning. For example, consider the following sentence: Example 1.1 ‚Äú Michael is the father of two relational database systems, Ingres and Postgres developed at Berkeley. Page and Brin did research at Stanford.‚Äù  ence resolution on the detected mentions.  Here ‚ÄúMichael‚Äù refers to the person Michael  Entity mentions within the document are  Stonebraker who is a computer scientist and  clustered to their longer form. We use the  not the singer Michael Jackson, ‚ÄúBerkeley‚Äù and  popularity of candidate entities to prune  ‚ÄúStandford‚Äù refer to the universities- University of  them and based on the similarity measure  California, Berkeley and Standford University and  of AIDA the entity for a mention is cho-  not to the places Berkeley and Standford, ‚ÄúPage‚Äù  sen. The mentions are broadly classiÔ¨Åed  refers to Larry Page the founder of Google and  into four categories person, location, orga-  not Jimmy Page who is a guitarist. Looking at the  nization and miscellaneous and the effect  sentence, humans barely notice the ambiguity as  of coreference and pruning were analyzed  they subconsciously resolve it. The ability to un-  on each category.  derstand single words was made possible by asso-  
 synthesizing story style speech in Spanish. The  main focus was on the analysis of prosodic pat-  In this work, we have proposed an auto-  terns (such as pitch, intensity, and tempo) based on  matic discourse prediction model. It pre-  discourse modes. The three discourse modes con-  dicts the discourse information for a sen-  sidered for the study are narrative, descriptive and  tence. In this study, three discourse modes  dialogue. Further, the authors introduced narrative  considered are descriptive, narrative and  situations such as neutral narrative, post-character,  dialogue. The proposed model is devel-  suspense and affective situations within the narra-  oped using story corpus. The story cor-  tive mode. The discourse information was man-  pus comprises of audio and its correspond-  ually assigned to each sentence of the story by  ing text transcription of short children sto-  text experts. Based on the discourse modes, the  ries. The development of this model en-  sentence was grouped, and prosodic rules are de-  tails two phases: feature extraction and  rived. These prosodic rules implemented using  classiÔ¨Åcation of the discourse. The feature  Harmonic plus Noise Model (HNM) for synthe-  extraction is carried out using ‚ÄòWord2Vec‚Äô  sizing storytelling style speech. In (Delmonte and  model. The classiÔ¨Åcation of discourse at  Tripodi, 2015), an analysis based on discourse  sentence-level is explored by using Sup-  mode was carried out to make TTS system more  port Vector Machines (SVM), Convolu-  expressive for English. The analysis of text was  tional Neural Network (CNN) and a com-  carried out at phonetic, phonological, syntactic  bination of CNN-SVM. The main focus of  and semantic level. The prosodic manager is pro-  this study is on the usage of CNN for de-  posed which takes discourse structures as input  veloping the model because it has not been  and uses the information to modify the parameters  explored much for the problems related to  of the TTS. The authors further carried out stud-  text classiÔ¨Åcation. Experiments are carried  ies by proposing various discourse relations (Del-  out to Ô¨Ånd the best model parameters (such  monte, 2008; Delmonte et al., 2007).  as the number of the Ô¨Ålter, Ô¨Ålter-height, cross-validation number, dropout rate, and batch-size) for the CNN. The proposed model achieves its best accuracy 72.6% when support vector machine (SVM) is used for classiÔ¨Åcation and features are extracted from CNN (which is trained using the word2vec feature). This model can leverage the utilization of the discourse as a suprasegmental feature from the perspective of speech.  In storytelling style speech (Theune et al., 2006), a storyteller uses his/her skill by putting variation in the speech for the better understanding of the listeners (especially children). The variation in speech is produced by mimicking various character‚Äôs voices present in the story, making various sound effects, and using prosody to convey emotions. It creates a pleasant listening experience for the listeners. In Indian Languages, development of TTS systems for Hindi, Bengali and Telugu are carried out in (Verma et al., 2015;  
 daily. These contents need to be organized in  In this paper we propose a stacked generalization (or stacking) model for event extraction in bio-medical text. Event extraction deals with the process of extracting detailed biological phenomenon, which is more challenging compared to the traditional binary relation extraction such as protein-protein interaction. The overall process consists of mainly three steps: event trigger detection, argument extraction by edge detection and Ô¨Ånding correct combination of arguments. In stacking, we use Linear Support Vector ClassiÔ¨Åcation (Linear SVC), Logistic Regression (LR) and Stochastic Gradient Descent (SGD) as base-level learning algorithms. As meta-level learner we use Linear SVC. In edge detection step, we Ô¨Ånd out the arguments of triggers detected in trigger detection step using a SVM classiÔ¨Åer. To Ô¨Ånd correct combination of arguments, we use rules generated by studying the properties of bio-molecular event expressions, and form an event expression consisting of event trigger, its class and arguments. The output of trigger detection is fed to edge detection for argument extraction. Experiments on benchmark datasets of BioNLP2011 show the recall, precision and Fscore of 48.96%, 66.46% and 56.38%, respectively. Comparisons with the existing systems show that our proposed model attains state-of-the-art performance.  a more principled way so as to enable advanced search and efÔ¨Åcient information retrieval and information extraction methods. This can be beneÔ¨Åcial to the practitioners and researchers in biology, medicine and the other allied disciplines. Success of text mining (TM) is evident from the organization of different shared-task evaluation campaigns. The bulk of research in the Ô¨Åeld of biomedical natural language processing (BioNLP) have mainly focused on the extraction of simple binary relations. Some of the very popular bio-text mining evaluation challenges include TREC Genomics track (Voorhees, 2007), JNLPBA1, LLL (Nedellec, 2005) and BioCreative (Lynette Hirschman, 2007). While the Ô¨Årst two evaluation challenges were concerned with the issues of information retrieval and named-entity recognition (NER), the last two addressed the issues of information extraction and seeking relations between bio-molecules. Relations among biomedical entities (i.e. proteins and genes) must be extracted automatically from a large collection of biomedical datasets since they are very important in understanding biomedical phenomena. Simple binary relations are not itself sufÔ¨Åcient for capturing the detailed phenomenon, and there is a growing demand for capturing more detailed and complex relations. Two large corpora, BioInfer (Pyysalo S, 2007) and GENIA (Tomoko Ohta and Tsujii, 2009), have been proposed for this purpose. In recent times there has been a trend for Ô¨Ånegrained information extraction from text (Kim JD, 2009). This was addressed in three consecutive text mining challenges, BioNLP-2009 (Hyoung-  
 Grammar (henceforth, T-G Grammar). Under  this situation, the paper seeks to investigate how  Conversation is often considered as the most knowledge of language is put to use. More spe-  problematic area in the field of formal linguistics, primarily because of its dynamic emerging nature. The degree of complexity is also high in comparison to traditional sentential analysis. The challenge for develop-  cifically, the paper intends to explore how efficiently the semantics and pragmatics of conversation can be explained within the existing theoretical framework of T-G Grammar. Consider the following example:  ing a formal account for conversational analysis is bipartite: Since the smallest structural unit at the level of conversational analysis is utterance, existing theoretical framework has to be developed in such a manner so that it  1. Speaker_1 Speaker_2  su≈õƒ´l-…∏  ƒÅs-…∏-b-e  to  Sushil-Nom come-…∏-fut-3.fut  prt  Will Sushil come?  ƒÅ su≈õƒ´l-…∏  ƒÅs-…∏-b-e  yes Sushil-Nom  come-…∏-fut-3.fut  Yes, Sushil will come.  can take an account of the utterance. In addi-  tion to this, a system should be developed to explain the interconnections of the utterances in a conversation. This paper tries to address these two tasks within the transformational and generative framework of Minimalism, proposed by Chomsky, with an emphasis on  In this piece of communication, Speaker_1 asks a question about the arrival of Sushil. In response to Speaker_1‚Äôs query Speaker_2 confirms Sushil‚Äôs arrival. The current status of linguistic enquiry in the field of syntax and semantics does not deal with this type of connected speech  the Bengali particle to ‚Äì traditionally classi- which we encounter often in our daily life. In  fied as indeclinable.  most of the cases, idealized sentential representa-  tion is discussed to unveil the grammatical intri-  cacies. Interestingly, what falls outside of the  
 impressive capabilities of making generalizations  In the paper we try to show that a lazy functional language such as Haskell is a convenient framework not only for implementing dependency parsers but also for expressing dependency grammars directly in the programming language in a compact, readable and mathematically clean way. The parser core, supplying necessary types and functions, is presented together with two examples of grammars: one trivial and one more elaborate, allowing to express a range of complex grammatical constraints such as long distance agreement. The complete Haskell code of the parser core as well the grammar examples is included.  in all conceivable dimensions in a large and highly multilingual language model including morphological, syntactic and semantic layers. Some other works, which may be mentioned here, are due to Ljungl√∂f (2004), de Kok and Brouwer (2009), Eijck (2005). As far as dependency-based parsing and language description is concerned (Kubler et al., 2009), the author is not aware of any attempts to apply functional programming techniques. Below we try to show that a lazy functional language such as Haskell is a convenient framework not only for implementing dependency parsers but also for expressing dependency grammars directly as Haskell code in a compact, readable, and mathematically clean way. A question may arise: why the ability to write  
 written using Roman scripts. Such contents create  A large amount of user-generated transliterated contents in Roman scripts are available in the Web for the languages that use non-Roman based indigenous scripts. This creates a mixed script space which is mono-lingual or multilingual having more than one script. Information retrieval (IR) in the mixed-script space is challenging as both query and documents can be written in either native or Roman script, or may be in both the scripts. Moreover, due to lack of any standard ways of spelling a word in a non-native script, transliterated contents can be written with different spelling variations. In this paper, we propose the effective techniques for query expansion and query classiÔ¨Åcation for mixed-script IR. The proposed techniques are based on deep learning, word embedding and traditional TF-IDF. We generate our own resources for creating the test-bed for our experiments. Extensive empirical analyses show that our proposed methods achieve signiÔ¨Åcantly better performance (20.44% increase in MRR, 22.43% increase in NDCG@1 & 15.61% increase in MAP) over a state-of-the-art baseline model.  a mono-lingual or multi-lingual space with more than one script which we refer to as the MixedScript space. Information retrieval in the mixedscript space, known as Mixed-Script IR (MSIR), is more challenging because queries can be written in both the native as well as Roman scripts, and these should also be matched to the documents written in both the scripts. Transliteration (Lopez, 2008) is the process of phonetically describing the words of a given language using a non-native script. For both the web documents and intended search queries to retrieve those documents, transliteration, especially into Roman script, is generally used. Since no standard ways of spelling any word into a non-native script exist, transliterated contents offer extensive spelling variations; typically, we can transliterate a native term into Roman script in many ways (Gupta et al., 2012). For example, the word khusboo (‚Äùfragrance‚Äù) can be written in Roman script using different variations such as kushboo, khusbu, khushbu and so on. This type of problem is termed as a non-trivial term matching problem for search engines with the aim to match the native-script or Roman-transliterated query with the documents in multiple scripts after considering the spelling variations. Many single (native) script queries and documents with spelling variations have been studied (French et al., 1997; Zobel and Dart, 1996)  
 siÔ¨Åcation also called feature-based opinion min-  With the increase of unstructured social media data, sentiment analysis can be applied to infer useful information to assist organizations and their customers. We propose a model for feature-based sentiment analysis using ontology to address queries like:‚Äúwhich car is more comfortable?, which car has better performance and interior?‚Äù. Feature based sentiment analysis is done using SentiWordNet with word sense disambiguation and an ontology that is developed by us. Data Tables are prepared from the RDF triples of parsed ontology and the sentiment ranks of car attributes. To relate the RDBM data to the built ontology of car, mapping axioms are proposed to connect them using OBDA model. Using SPARQL query, the results of the proposed model are compared with a dictionary-based method with respect to different car attributes. The performance of our model is better than dictionary based method.  ing which covers both entities and aspects. In our approach, there are two major tasks: feature extraction and feature sentiment classiÔ¨Åcation to determine sentiments on targets, for example, ‚ÄúThe speed of bmw 3.0 is great but the navigation is not good.‚Äù. Here there are two attributes of car (bmw3.0); speed and navigation. According to Liu (2012), there are four principle approaches to recognize every assessment expression and its objective from the opinion: ‚Äúextraction based on frequent nouns and noun phrases, extraction by exploiting opinion and target relations, extraction using supervised learning and topic modeling‚Äù. We used the second approach using nltk1 lexical resources. Featurebased sentiment analysis (FBSA) is done on car customers‚Äô reviews in which features are extracted using our ontology on car domain. According to Gruber (1993), an ontology is an ‚Äúexplicit and formal speciÔ¨Åcation of a conceptualization‚Äù. We proposed FBSA on car reviews using SentiWordNet with word sense disambiguation (WSD) and an ontology associated with mappings.  
 not available for all languages, or only small tree-  While statistical methods have been very effective in developing NLP tools, the use of linguistic tools and understanding of language structure can make these tools better. Cross-lingual parser construction has been used to develop parsers for languages with no annotated treebank. Delexicalized parsers that use only POS tags can be transferred to a new target language. But the success of a delexicalized transfer parser depends on the syntactic closeness between the source and target languages. The understanding of the linguistic similarities and differences between the languages can be used to improve the parser. In this paper, we use a method based on cross-lingual model transfer to transfer a Hindi parser to Bengali. The technique does not need any parallel corpora but makes use of chunkers of these languages. We observe that while the two languages share broad similarities, Bengali and Hindi phrases do not have identical construction. We can improve the transfer based parser if the parser is transferred at the chunk level. Based on this we present a method to use chunkers to develop a cross-lingual parser for Bengali which results in an improvement of unlabelled attachment score (UAS) from 65.1 (baseline parser) to 78.2.  banks may be available. In recent years, considerable efforts have been put to develop dependency parsers for low-resource languages. In the absence of treebank for a language, there has been research in using cross-lingual parsing methods (McDonald et al., 2011) where a treebank from a related source language (SL), is used to develop a parser for a target language (TL). In such work, an annotated treebank in SL and other resources in are used to develop a parser model for TL. Most of the existing work assume that although annotated treebanks are not available for the target language TL, there are other resources available such as parallel corpus between the source and the target languages (Xiao and Guo, 2015; Rasooli and Collins, 2015; Tiedemann, 2015). However, developing a parallel corpus is also expensive if such parallel corpus is not available. In this work, our goal is to look at methods for developing a cross-lingual transfer parser for resource poor Indian language for which we have access to a small or no treebank. We assume the availability of a monolingual corpus in target language and a small bilingual (source-target) dictionary. Given our familiarity with Bengali and Hindi, and availability of a small treebank we aim to test our approach in Hindi-Bengali transfer parsing. We choose Hindi as the source language as it is syntactically related to Bengali and a Hindi treebank (Nivre et al., 2016) is freely available which can be used to train a reasonably accu-  
This paper presents a new method for the conversion of one style of dependency treebanks into another, using contextual, Constraint Grammar-based transformation rules for both structural changes (attachment) and changes in syntacticfunctional tags (edge labels). In particular, we address the conversion of traditional syntactic dependency annotation into the semantically motivated dependency annotation used in the Universal Dependencies (UD) Framework, evaluating this task for the Portuguese Floresta Sint√°(c)tica treebank. Finally, we examine the effect of the UD converter on a rulebased dependency parser for English (EngGram). Exploiting the ensuing comparability and using the existing UD Web treebank as a gold standard, we discuss the parser's performance and the validity of UD-mediated evaluation. 
 of internet has made the users to post their experi-  ences with any product, service or application very  Getting labeled data in each domain is al-  frequently. Consequently, there is a high increase  ways an expensive and a time consuming  in number of domains in which sentimental data  task. Hence, cross-domain sentiment anal-  is available. Getting sentiment (positive or neg-  ysis has emerged as a demanding research  ative) annotated data manually in each domain is  area where a labeled source domain facil-  not feasible due to the cost incurred in annotation  itates classiÔ¨Åer in an unlabeled target do-  process.  main. However, cross-domain sentiment analysis is still a challenging task because of the differences across domains. A word which is used with positive polarity in the source domain may bear negative polarity in the target domain or vice versa. In addition, a word which is used very scarcely in the source domain may have high impact in the target domain for sentiment classiÔ¨Åcation. Due to these differences across domains, cross-domain sentiment analysis suffers from negative transfer. In this paper, we propose that senses of words in  Cross-domain SA provides a solution to build a classiÔ¨Åer in the unlabeled target domain from a labeled source domain. But, due to the differences across domains, cross-domain SA suffers from negative transfer. A word which is used with positive polarity in the source domain may bear negative polarity in the target domain or vice versa. We call such words as changing polarity words. In most of the cases, the difference in polarity occurs due to the use of the word with different senses. Example of such changing polarity word is as follows:  place of words help to overcome the differences across domains. Results show that  1a. His behavior is very cheap. (Negative)  senses of words provide a better sentiment classiÔ¨Åer in the unlabeled target domain in comparison to words for 12 pairs of source  1b. Jet airways provides very cheap Ô¨Çight tickets. (Positive)  and target domains.  In the Ô¨Årst case, the word cheap is used with  
 sets, which are used to train a POS tagging ex-  Part of speech taggers generally perform well on homogeneous data sets, but their performance often varies considerably across different genres. In this paper we investigate the adaptation of POS  pert for each topic. Test sentences are also clustered into the same topics, and each test sentence is annotated by the corresponding POS tagging expert. We investigate different methods of converting topics into expert training sets.  taggers to individual genres by creating  Thus, our method is related to domain adap-  POS tagging experts. We use topic model-  tation approaches (Khan et al., 2013; Miller et  ing to determine genres automatically and  al., 2007) in that it focuses on adapting to spe-  then build a tagging expert for each genre.  ciÔ¨Åc characteristics of texts, but it is more gener-  We use Latent Dirichlet Allocation to clus-  ally applicable because it determines the domains  ter sentences into related topics, based on  and the experts automatically. It is also related  which we create the training experts for  to approaches of mitigating domain effects (e.g.,  the POS tagger. Likewise, we cluster the  (S√∏gaard, 2013)), but in contrast to those methods,  test sentences into the same topics and an-  we obtain individual experts that can be used and  notate each sentence with the correspond-  investigated separately.  ing POS tagging expert. We show that using topic model experts enhances the accuracy of POS tagging by around half a percent point on average over the random baseline, and the 2-topic hard clustering model and the 10-topic soft clustering model improve over the full training set.  Our results show that the topic modeling experts are sensitive to different genres (Ô¨Ånancial news vs. medical text) as well as to smaller differences between the Wall Street sentences. On average, the improvement over randomly selected subsets is around 0.5-1 percent point. Our results also show that one major difference between the POS tag-  
 ics. Under the influence of these approaches, the  current paper seeks to investigate how grammati-  The paper is an investigation into the  cal regulations are crucial in imposing constraint  graph theoretic interpretation of the  on the structure with a special reference to the  Bangla traditional grammar to under-  traditional Bangla grammar within the frame-  stand the way grammatical information is  work of graph theoretic enquiry.  structurally encoded in language. The hi-  erarchical and the linear structural princi-  Translating linguistic structure into the tree struc-  ples of grammatical compositionality is  ture is not new. In fact, the Transformational-  discussed in terms of certain graph theo-  Generative (hereafter, TG) grammar has shown  retic concepts like tree, subtree, inverse  quite successfully how syntacto-semantic princi-  tree etc.  ples can be talked about in terms of tree struc-  tures. The present work differs in certain respects  Translating linguistic structure into the  from the assumptions of TG grammarians, pri-  tree structure is not new. In fact, the  marily because of the type of grammar and lan-  Transformational-Generative grammar,  guage it is dealing with. Not only the TG gram-  Tree adjoining grammar etc. have shown  mar, tree adjoining (hereafter, TA) grammar has  quite successfully how syntacto-semantic  also made a good use of the graph theory.  principles can be talked about in terms of  Though both TG and TA have made use of the  tree structures. The present work differs  graph theory but definitely from the two different  in certain respects from the assumptions  perspectives.  of TG grammarians, primarily because of  the type of grammar and language it is  The current proposal resembles TA grammar  dealing with.  more closely than the TG grammar; and as a re-  sult, the proposed model of language representa-  
In the era of information overload, text summarization can be defined as the process of extracting useful information from a large space of available content using traditional filtering methods. One of the major challenges in the domain of extraction based summarization is that a single statistical measure is not sufficient to produce efficient summaries which would be close to human-made ‚Äògold standard‚Äô, since each measure suffers from individual weaknesses. We deal with this problem by proposing a text summarization model that combines various statistical measures so that the pitfalls of an individual technique could be compensated by the strengths of others. Experimental results are presented to demonstrate the effectiveness of the proposed method using the TAC 2011 Multiling pilot dataset for English language and ROUGE summary evaluation tool. 
 many other Indian languages provide some inher-  In this paper we describe an end to end Neural Model for Named Entity Recognition (NER) which is based on BiDirectional RNN-LSTM. Almost all NER systems for Hindi use Language SpeciÔ¨Åc features and handcrafted rules with gazetteers. Our model is language independent and uses no domain speciÔ¨Åc features or any handcrafted rules. Our models rely on semantic information in the form of word vectors which are learnt by an unsupervised learning algorithm on an unannotated corpus. Our model attained state of the art performance in both English and Hindi without the use of any morphological analysis or without using gazetteers of any sort.  ent difÔ¨Åculties in many NLP related tasks. The structure of the languages contain many complexities like free-word ordering (which affect n-gram based approaches signiÔ¨Åcantly), no capitalization information and its inÔ¨Çectional nature (affecting hand-engineered approaches signiÔ¨Åcantly). Also, in Indian languages there are many word constructions that can be classiÔ¨Åed as Named Entities (Derivational/InÔ¨Çectional constructions) etc and these constraints on these constructions vary from language to language hence carefully crafted rules need to be made for each language which is a very time consuming and expensive task. Another major problem in Indian languages is the fact that we have scarce availability of annotated data for indian languages. The task is hard for rule-based NLP tools, and the scarcity of labelled data renders many of the statistical ap-  
 with resources having limited memory and com-  In this paper, we introduce Vaidya, a spoken dialog system which is developed as part of the ITRA1 project. The system is capable of providing an approximate diagnosis by accepting symptoms as freeform speech in real-time on both laptop and hand-held devices. The system focuses on challenges in speech recognition speciÔ¨Åc to Indian languages and capturing the intent of the user. Another challenge is to create models which are memory and CPU efÔ¨Åcient for hand-held devices. We describe our progress, experiences and approaches in building the system that can handle English as the input speech. The system is evaluated using subjective statistical measure (Fleiss‚Äô kappa) to assess the usability of the system.  putational power such as a handheld devices. Spoken dialog systems (SDS) have been an ac- tive area of research for the past few decades. But a large body of work has gone in developing SDS for English. There are several active systems currently in use for travel and healthcare in English. Research projects in India focus on understanding the linguistic structure of Indian languages and to make them easily representable in digital form. Structural analysis of languages coupled with SDS can create viable solutions for healthcare. There is a huge necessity of SDS in Indian healthcare systems since 1) medical knowledge is readily available through well-crafted disease ontologies which can be easily queried and 2) the mortality rate in rural areas is much higher due to lack of advanced diagnosis3. Most of the recent language technologies being developed currently are feasible on a standard  
 behavior using statistical analysis.  In the last decade there have been signiÔ¨Åcant ef-  The paper reports work on investigating societal forts in opinion and sentiment mining as well  sentiment using the Schwartz values and ethics as inferring emotion from text. Classical senti-  model, and applying it to social media text of ment/emotion analysis systems classify text into  users from 20 most populous cities of India to rep- either one of the classes positive, negative or  resent geo-speciÔ¨Åc societal sentiment map of In- neutral, or into Ekman‚Äôs classes of happy, sad,  dia. For the automatic detection of societal sen- anger, fear, surprise, and disgust. Personality  timent we propose psycholinguistic analysis, that models [John and Srivastava1999] can be seen as  reveals how a user‚Äôs social media behaviour and an augmentation to the basic deÔ¨Ånition of sen-  language is related to his/her ethical practices. In- timent analysis, where the target is to under-  dia is a multi-cultural country, values and ethics stand sentiment/personality at person level rather  of each Indian are highly diverse and dependent than only at message level. Here in this paper  on the region or society s/he belongs to. Sev- our motivation is to augment to one more level  eral experiments were carried out incorporating i.e., to understand societal sentiment, i.e., values  Linguistic Inquiry Word Count analysis, n-grams, and ethics. To understand the societal sentiment  topic modeling, psycholinguistic lexica, speech- we borrow the psychological values and ethics  acts, and non-linguistic features, while experi- model introduced by schwartz1990toward, deÔ¨Ånes  menting with a range of machine learning algo- ten basic and distinct ethical values (henceforth  rithms including Support Vector Machines, Logis- only values). The deÔ¨Ånitions of these 10 val-  tic Regression, and Random Forests to identify the ues are as following. Achievement: sets goals  best linguistic and non-linguistic features for auto- and achieves them; Benevolence: seeks to help  matic classiÔ¨Åcation of values and ethics.  others and provide general welfare; Conformity:  obeys clear rules, laws and structures; Hedo-  
 quired knowledge. However, automating assess-  Automatic short answer grading (ASAG) techniques are designed to automatically assess short answers written in natural language having a length of a few words to a few sentences. In this paper, we report an intriguing Ô¨Ånding that the set of short answers to a question, collectively, share signiÔ¨Åcant lexical commonalities. Based on this Ô¨Ånding, we propose an unsupervised ASAG technique that only requires sequential pattern mining in the Ô¨Årst step and an intuitive scoring process in the second step. We demonstrate, using multiple datasets, that the proposed technique effectively exploits wisdom of students to deliver comparable or better performance than prior ASAG techniques as well as distributional semantics-based approaches that require heavy training with a large corpus. Moreover, by virtue of being independent of instructor provided model answers, our technique offers consistency  ment of such answers is non-trivial owing to linguistic variations (a given answer could be articulated in different ways); subjective nature of assessment (multiple possible correct answers or no correct answer); lack of consistency in human rating (non-binary scoring on an ordinal scale within a range); etc. Consequently, this has remained a repetitive and tedious job for teaching instructors and is often seen as an overhead and nonrewarding. This paper is about a computational technique for automatically grading constructed student answers in natural language. In particular, we are interested in short answers: a few words to a few sentences long (everything in between Ô¨Ållin-the-gap and essay type answers (Burrows et al., 2015)) and refer to the task as Automatic Short Answer Grading (ASAG). An example ASAG task is shown in Table 1.  Question Model Ans Stud#1 Stud#2  How are overloaded functions differentiated by the compiler? Based on the function signature. When an overloaded function is called, the compiler will Ô¨Ånd the function whose signature is closest to the given function call. It looks at the number, types, and order of arguments in the function call By the number, and the types and order of the parameters.  by overcoming the limitation of undesired variability in performance exhibited by existing unsupervised techniques.  Table 1: Example of question, model answer, and student answers from an undergraduate computer science course (Mohler and Mihalcea, 2009).  
Electronic Medical Records contains a rich source of information for medical Ô¨Ånding. However, the access to the medical record is limited to only de-identiÔ¨Åed form so as to protect the conÔ¨Ådentiality of patient. According to Health Insurance Portability and Accountability Act, there are 18 PHI categories that should be enclosed before making the EMR publicly available. With the rapid growth of EMR and a limited amount of de-identiÔ¨Åed text, the manual curation is quite unfeasible and time-consuming, which has drawn the attention of several researchers to propose automated de-identiÔ¨Åcation system. In this paper, we proposed deep neural network based architecture for de-identiÔ¨Åcation of 7 PHI categories with 25 associated subcategories. We used standard benchmark dataset from i2b2-2014 de-identiÔ¨Åcation challenge and performed the comparison with very strong baseline based on Conditional Random Field. We also perform the comparison with the state-of-art. Results show that our proposed system achieves signiÔ¨Åcant improvement over baseline and comparable performance over state-of-art. 
 identiÔ¨Åcation methodology and different features  used are presented in Section 3. Section 4 focuses  Systems that simultaneously identify and  on classiÔ¨Åcation of the identiÔ¨Åed named entities  classify named entities in Twitter typically  and their linking to DBpedia. Experimental results  show poor recall. To remedy this, the task  and a discussion of those appear in Section 5 and  is here divided into two parts: i) named en-  Section 6, respectively, while Section 7 addresses  tity identiÔ¨Åcation using Conditional Ran-  future work and concludes.  dom Fields in a multi-objective framework built on Differential Evolution, and  2 Related Work  ii) named entity classiÔ¨Åcation using Vector Space Modelling and edit distance techniques. Differential Evolution is an evolutionary algorithm, which not only optimises the features, but also identiÔ¨Åes the proper context window for each selected feature. The approach obtains F-scores of 70.7% for Twitter named entity extraction and 66.0% for entity linking to the DBpedia database.  The noisiness of the texts makes Twitter named entity (NE) extraction a challenging task, but several approaches have been tried: Li et al. (2012) introduced an unsupervised strategy based on dynamic programming; Liu et al. (2011) proposed a semi-supervised framework using a k-Nearest Neighbors (kNN) approach to label the Twitter names and gave these labels as an input feature to a Conditional Random Fields, CRF (Lafferty et al., 2001) classiÔ¨Åer, achieving almost 80% ac-  
In this paper, we show that generative classiÔ¨Åers are capable of learning non-linear decision boundaries and that non-linear generative models can outperform a number of linear classiÔ¨Åers on some text categorization tasks. We Ô¨Årst prove that 3-layer multinomial hierarchical generative (Bayesian) classiÔ¨Åers, under a particular independence assumption, can only learn the same linear decision boundaries as a multinomial naive Bayes classiÔ¨Åer. We then go on to show that making a different independence assumption results in nonlinearization, thereby enabling us to learn non-linear decision boundaries. We Ô¨Ånally evaluate the performance of these non-linear classiÔ¨Åers on a series of text classiÔ¨Åcation tasks. 
Indian epics have not been analyzed computationally to the extent that Greek epics have. In this paper, we show how interesting insights can be derived from the ancient epic Mahabharata by applying a variety of analytical techniques based on a combination of natural language processing, sentiment/emotion analysis and social network analysis methods. One of our key Ô¨Åndings is the pattern of significant changes in the overall sentiment of the epic story across its eighteen chapters and the corresponding characterization of the primary protagonists in terms of their sentiments, emotions, centrality and leadership attributes in the epic saga. 
Sanskrit is an accented language. The accent is very prominent in Vedic Sanskrit but classical Sanskrit does not make use of it. Accent is a disambiguation device which can help to understand the correct sense of a word. Words in Sanskrit tend to fall in certain grammatical categories like gha√±antas, vocatives etc. Words in a category tend to have an accent governed by either a general or a special rule of accentuation. PƒÅ·πáini‚Äôs special (apavƒÅda) rules of accentuation for a specific category of words can be studied along with the general (utsarga) rules of accentuation for the same class. The resulting body of rules states all the conditions necessary for accentuation of words from that class. The conditions reveal a set of features which can be used for determining the correct accent. If the features of accentuation for a class of words is understood, computers can be trained to accentuate unaccented words from that class. This paper discusses some features of gha√±anta words and how the features can be used to restore accent on unaccented gha√±anta words. 
 judgments, appraisals or feelings toward entities,  Subjective sentences describe people‚Äôs opinions, points-of-view, interpretations, comparisons, sentiments, judgments, appraisals or feelings toward entities, events and their properties. Identifying subjective sentences is the basis for opinion mining and sentiment analysis and is important in applications like political analysis, social media analytics and product review analytics. We use standard classiÔ¨Åers to build models of SUBJECTIVE vs. NON-SUBJECTIVE sentences and demonstrate that they outperform the approaches reported in the literature on several interesting datasets. We discuss two novel applications of this work: prevalence of subjective sentences in performance appraisal text and scientiÔ¨Åc papers. We demonstrate that scientiÔ¨Åc papers also contain a substantial fraction of subjective sentences. We compare the nature of subjective sentences in performance appraisals text and scientiÔ¨Åc papers, and observe different reasons why some sentences in these domains are subjective. We propose the need to further investigate the linguistic and semantic basis for subjective sentences across different domains.  events and their properties (Liu, 2010). Objective information is typically fact-based, measurable, observable and veriÔ¨Åable. For example, (S1) (Table1) is a factual sentence. In contrast, most opinion sentences express some sentiment, usually having a positive or negative polarity. However, some opinion sentences are neutral i.e., they do not explicitly express any particular sentiment. For example, (S2) is an opinion sentence that expresses some sentiment (mostly positive), whereas (S3), (S4) and (S5) are opinion sentences that do not express any particular sentiment. Some factual sentences may be mixed i.e., they may also contain subjective expressions, with or without the presence of an explicit sentiment polarity. Sentences (S6) and (S7) are mainly factual, but (S6) contain an opinion along with a positive sentiment whereas (S7) expresses an opinion without much sentiment. Any expression about the private (internal) state of mind of a person is, by deÔ¨Ånition, a subjective sentence; e.g., (S8). Opinion sentences often contain subjective expressions other than sentiments, such as opinions, points of view (S4), judgements (S5), predictions (S9), interpretations, comparisons (S10) etc. In this paper, we are interested in automatically identifying subjective sentences, which contain subjective expressions, but which may or may not contain explicit sentiment markers. We are also in-  
 and this is expected to rise to 2 billion users in  With the phenomenal growth in social media, citizens are coming forward to participate more in discussions on socially relevant topics including government policies, public health etc. India is not an exception to this, and the website mygov.in launched by the Government of India acts as a platform for discussion on such topics. People raise their viewpoints as comments and blogs on various topics. In India, being a diverse country, citizens write their opinions in different languages, which are often in mixed-languages. Code-Mixing refers to the mixing of two or more languages in speech or in a text, and this poses several challenges. In this paper, we propose a deep learning based system for opinion mining in an environment of codemixed languages. The insights obtained by analyzing the techniques lay the foundation for better lives of citizens, by improving the efÔ¨Åcacy and efÔ¨Åciency of public services, and satisfying complex information needs arising within this context. Moreover, understanding the deep feelings can help government to anticipate deep social changes and adapt to population expectations, which will help building Smart city.  2016, led by India. The research also reveals that users daily spend approximately 8 hours on digital media including social medias and and mobile internet usages. At the heart of this interest is the ability for users to create and share contents via a variety of platforms such as blogs, microblogs, collaborative wikis, multimedia sharing sites, social networking sites etc. The unprecedented volume and variety of user-generated contents, as well as the user interaction networks constitute new opportunities for understanding social behavior and building socially intelligent systems. Therefore, it is important to investigate tools and methods for knowledge extraction from social media data. In social media, contents are often written in mixed-languages, and this phenomenon is known as code-mixing. Code-Mixing or code-switching is deÔ¨Åned as the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language. This phenomenon is prevalent among bi-lingual and multilingual individuals. This is a well-known trait in speech patterns of the average bilingual in any human society all over in the world. With the phenomenal growth in social media, people from different dialects participate on web portals to showcase their opinions. This diversity of users contributes to the non-uniformity in texts and as a result the data generated lead to code-mixed. There  
 and Das, 2016). Thus, code-mixing while chatting  In social media code-mixing is getting very popular due to which there is enormous generation of noisy and sparse multilingual text which exhibits high dispersion of useful topics which people discuss. Also, the semantics is expressed across random occurrence of code-mixed words. In this paper, we propose  has become prevalent in current times. However, exponentially increasing large volumes of short and long code-mixed messages contain lot of noise and has useful information highly dispersed. Unfortunately, it is not an easy task to retrieve useful knowledge from such data as code-mixing occurs at different levels of code-complexity and imposes fundamental challenges namely:  code-mixed knowledge based LDA (cmkLDA), which infers latent topic based aspects from code-mixed social media data. We experimented on FIRE 2014, a code-  1. Code-mixed social media data is multilingual, usually bilingual (San, 2009). Therefore, semantics is spread across languages.  mixed corpus and showed that with the  2. Social media data do not have speciÔ¨Åc termi-  help of semantic knowledge from mul-  nology (Eisenstein, 2013).  tilingual external knowledge base, cmkLDA learns coherent topic-based aspects across languages and improves topic interpretibility and topic distinctiveness better than the baseline models . The same is shown to have agreed with human judgment. 
Feature selection is a major hurdle for the CRF and SVM based POS tagging. The features are of course listed which will have a very good impact with the identification of POS. Among the listed features, the feature selection is purely a manual effort with hit and trail methods among them. The best way for better output is to design a system where the system itself identifies the best combination of features. A Genetic Algorithm (GA) system is design so that the best possible combination can be sort out instead of a manual hit and trail method in feature selection. The system shows a Recall of 80.00%, Precision (P) of 90.43% and F-score (F) of 84.90%. 
Sentence comprehension is an integral and important part of whole text comprehension. It involves complex cognitive actions, as a reader has to work through lexical, syntactic and semantic aspects in order to understand a sentence. One of the vital features of a sentence is word order or surface forms. Different languages have evolved different systems of word orders, which reflect the cognitive structure of the native users of that language. Therefore, word order affects the cognitive load exerted by a sentence as experienced by the reader. Computational modeling approach to quantify the effect of word order on difficulty of sentence understanding can provide a great advantage in study of text readability and its applications. Plethora of works have been done in English and other languages to address the issue. However, Bangla, which is the fifth mostly spoken languages in the world and a relatively free word order language, still does not have any computational model to quantify the reading difficulty of a sentence. In this paper, we have developed models to predict the comprehending difficulty of a simple sentence according to its different surface forms in Bangla. In the course of action, we have also established that difficulty measures for English do not hold in Bangla. Our model has been validated against a number of user survey.  
 without using any speciÔ¨Åc algorithm (Kira and  The aim of text classiÔ¨Åcation is to classify the text documents into a set of pre-deÔ¨Åned categories. But the complexity of natural languages, high dimensional feature space and low quality of feature selection become the main problem for text classiÔ¨Åcation process. Hence, in order strengthen the classiÔ¨Åcation technique, selection of important features, and consequently removing the unimportant ones is the need of the day. The Paper proposes an approach called Commonality-Rarity Score Computation (CRSC) for selecting top features of a corpus and highlights the importance of ML-ELM feature space in the domain of text classiÔ¨Åcation. Experimental results on two benchmark datasets signify the prominence of the proposed approach compared to other established approaches.  Rendell, 1992), and hence preferred over wrapper methods. Most Ô¨Ålter methods give a ranking of the best features rather than one single set of best features. Wrapper methods use a predecided learning algorithm i.e. a classiÔ¨Åer to evaluate the features and hence computationally expensive (Kohavi and John, 1997). Also, they have a higher possibility of overÔ¨Åtting than Ô¨Ålter methods. Hence, large scale problems like text categorization mostly do not use wrapper methods (Forman, 2003). Embedded methods tend to combine the advantages of both the aforementioned methods. The computational complexity of the embedded methods, thus, lies in between that of the Ô¨Ålters and the wrappers. Ample research work has already been done in this domain (Qiu et al., 2011)(Lee and Kim, 2015)(Meng et al., 2011)(NovovicÀáova¬¥ et al., 2007)(Yang et al., 2011)(Aghdam et al., 2009)(Thangamani and Thangaraj, 2010)(Azam and Yao, 2012)(Liu et al.,  Keywords: ClassiÔ¨Åcation; ELM; Feature selec- 2005).  tion; ML-ELM; Rarity  Selection of a good classiÔ¨Åer plays a vital role in  
 The conventional approach to tackle this problem is  a pipeline of three steps: (1) Ô¨Ånd noun compounds from  Sequences of long nouns, i.e., noun compounds, occur frequently and are productive.  text, (2) parse them if required, and (3) extract the semantic relationships between components of the noun  Their interpretation is important for a variety  compounds. The task of extracting semantic relations  of tasks located at various layers of NLP. Major reasons behind the poor performance of au-  between components of a noun compound, or paraphrasing it using verbs and/or prepositions is known as  tomatic noun compound interpretation are: (a)  interpretation of noun compound (or noun compound  lack of a well deÔ¨Åned inventory of semantic relations and (b) non-availability of sufÔ¨Åcient,  interpretation). Our primary interest resides in interpretation of two-  annotated, high-quality dataset.  word noun compounds using predeÔ¨Åned semantic la-  Tratz and Hovy (2010) presented an inventory of semantic relations. They compared existing inventories with their two-level hierarchy, and created a large annotated dataset. We performed both theoretical as well as datadriven analysis of this inventory. Theoretical analysis reveal ambiguities in the coarse relations. Data-driven analysis report similar performance for coarse as well as Ô¨Åne relations prediction. Our experiments show that improving the coarse classiÔ¨Åcation accuracy can  bels as classes. The labels have been arranged in a two level hierarchy - coarse classes and Ô¨Åne classes. In this paper, we report the technical and linguistic challenges that we faced while performing classiÔ¨Åcation task. Particularly, we discuss the challenges with coarse level classiÔ¨Åcation. The rest of the paper is organized as follows: Section 2 covers the related work. Section 3 discusses our approach, the experiments and results for the same are shown in Section 4. Section 5 discusses the results, which is followed by conclusion and future work.  improve the performance of Ô¨Åne class predictor by 13 to 30 points in F-score.  2 Related Work In computational domain, most work uses either of two  
The sentence is incomplete without a verb in a language. A verb is majorly responsible for giving the meaning to a sentence. Any sentence can be represented in the form of a verb frame. Verb frames are mainly developed as a knowledge resource which can be used in various semantic level Natural Language Processing (NLP) activities. This paper presents the Verbframator ‚Äì a verb frame annotator tool which automatically extracts and generates verb frames of example sentences from Marathi wordnet. It also helps in generating Shakti Standard Format (SSF) files of the given example sentences. The generated verb frames and SSF files can be used in the dependency tree banking and other NLP applications like machine translation, paraphrasing, natural language generation, etc. 
 1.1 Background  In this paper we present Surukam-NLI ‚Äî a novel system of building a natural language interface to databases, which composes the earlier work on using linguistic syntax trees for parsing natural language queries with, the latest advances in natural language processing such as distributed language embedding models for semantic mapping of the natural language and the database schema. We will be evaluating the performance of our system on a sample online transaction processing (OLTP) database called as AdventureWorksDB and show that we achieve partial domain independence by handling queries about three different scenarios ‚Äî Human Resources, Sales & Marketing and Product scenarios. Since there is no baseline for query performance on OLTP databases, we report f-measure statistics on an internally curated query dataset.  Building a natural language interface to relational databases is almost as old as the concept of relational databases itself. Codd (1970) deÔ¨Åned relational database as a digital database whose organization is based on the relational model of data in his seminal paper of 1970, while the earliest documented Natural Language Interface(NLI) is the Lunar Sciences Natural Language Information System (LSNLIS), (Woods et al., 1972). It was a question answering system built in 1972, that enabled lunar geologists to query the data collected during the Apollo missions. Despite there being many NLIs since the LSNLIS such as PRECISE, PARLANCE, NaLIR, SEEKER and TEAM (Popescu et al., 2003; Bates, 1989; Li and Jagadish, 2014; Smith et al., 2014; Grosz et al., 1987), there has not been an encouraging adoption of this technology in the software industry, probably because of lengthy conÔ¨Åguration phases and domain portability issues. SQL is still the preferred mode of querying relational databases which have highly complex architecture  and for sensitive operations like inserting, updat-  
This paper describes the vowels characteristics of three languages of Nagaland namely Nagamese, Ao and Lotha. For this study, nucleus vowel duration, formant structure (1st and 2nd formant i.e. F1 and F2) and intensity of vowels are investigated and analyzed for these languages. This paper includes the nasal context for different vowels and tries to examine its importance in different languages. A detailed analysis is carried out for six vowels namely ÔÄØÔÅ©ÔÄØÔÄ¨ÔÄ†ÔÄØÔÅ•ÔÄØÔÄ¨ÔÄ†ÔÄØÔÅ°ÔÄØÔÄ¨ÔÄ†ÔÄØÔÇ´ÔÄØÔÄ¨ÔÄ†ÔÄØÔÅØÔÄØÔÄ¨ÔÄ†ÔÄØÔÅµÔÄØ for readout speech of Nagamese, Ao and Lotha. Result shows that the vowel duration and formants play important roles in differentiating vowels characteristics. On the other hand, intensity of vowels do not play significant role in the characteristics of the vowels across the languages is observed. This initial study unveil the importance of vowels characteristics and may help to do research and development in the area of language identification, synthesis, speech recognition of three north-eastern languages of Nagaland. 
 on the syntactic constituents. According to which,  This paper proposes an algorithm for Ô¨Ånding phonological phrase boundaries in sentences with neutral focus spoken in both normal and fast tempos. A perceptual experiment is designed using Praat‚Äôs experiment MFC program to investigate the phonological phrase boundaries. Phonological phrasing and its relation to syntac-  the right edge of each syntactic XP coincides with the right edge of a œÜ-phrase. A syntactic XP is a phrase where X represents the head of that phrase. In short, end-based rule is written as Align(XP, R, œÜ, R), i.e., the right edge of each XP must be aligned to the right edge of œÜ-phrase (Truckenbrodt, 1995). See the following examples from (Truckenbrodt, 1995)  tic structure in the framework of the endbased rules proposed by (Selkirk, 1986),  1.  and relation to purely phonological rules, i.e., the principle of increasing units proposed by (Ghini, 1993) are investigated. In addition to that, this paper explores the acoustic cues signalling phonological phrase boundaries in both normal and fast tempos speech. It is found that phonological phrasing in Hindi follows both endbased rule (Selkirk, 1986) and the princi-  a. [ V NP ]V P ‚Üí ( V NP )œÜ e.g., (ingile mtana:ni)œÜ /entered the room/ b. [ V PP ]V P ‚Üí ( V PP )œÜ e.g., (mapendo ya maski:ni)œÜ /the love of a poor man/ c.[ N AP ]NP ‚Üí ( N AP )œÜ e.g., (nthi:-khavu) / dry land/ 2. [NP V] ‚Üí ( NP )œÜ ( V )œÜ  ple of increasing units (Ghini, 1993). The  e.g., ( maski:ni ha:tali) /a poor man does not  end-based rules are used for phonological  choose/  phrasing and the principle of increasing  3. For complex sentence having more than one  units is used for phonological phrase re-  NP following phrasing pattern is applied.  structuring.  
This paper presents a system for classifying disaster-related tweets. The focus is on Twitter data generated before, during, and after Hurricane Sandy, which impacted New York in the fall of 2012. We propose an annotation schema for identifying relevant tweets as well as the more Ô¨Åne-grained categories they represent, and develop feature-rich classiÔ¨Åers for relevance and Ô¨Åne-grained categorization. 
In this paper we present a Ô¨Ålter for identifying posts from eyewitnesses to various event types on Twitter, including shootings, police activity, and protests. The Ô¨Ålter combines sociolinguistic markers and targeted language content with straightforward keywords and regular expressions to yield good accuracy in the returned tweets. Once a set of eyewitness posts in a given semantic context has been produced by the Ô¨Ålter, eyewitness events can subsequently be identiÔ¨Åed by enriching the data with additional geolocation information and then applying a spatio-temporal clustering. By applying these steps we can extract a complete picture of the event as it occurs in real-time, sourced entirely from social media.  
Depression is a major threat to public health, accounting for almost 12% of all disabilities and claiming the life of 1 out of 5 patients suffering from it. Since depression is often signaled by decreasing social interaction, we explored how analysis of online health forums may help identify such episodes. We collected posts and replies from users of several forums on healthboards.com and analyzed changes in their use of language and activity levels over time. We found that users in the Depression forum use fewer social words, and have some revealing phrases associated with their last posts (e.g., cut myself ). Our models based on these Ô¨Åndings achieved 94 F1 for detecting users who will withdraw from a Depression forum by the end of a 1-year observation period. 
Work on cross document coreference resolution (CDCR) has primarily focused on news articles, with little to no work for social media. Yet social media may be particularly challenging since short messages provide little context, and informal names are pervasive. We introduce a new Twitter corpus that contains entity annotations for entity clusters that supports CDCR. Our corpus draws from Twitter data surrounding the 2013 Grammy music awards ceremony, providing a large set of annotated tweets focusing on a single event. To establish a baseline we evaluate two CDCR systems and consider the performance impact of each system component. Furthermore, we augment one system to include temporal information, which can be helpful when documents (such as tweets) arrive in a speciÔ¨Åc order. Finally, we include annotations linking the entities to a knowledge base to support entity linking. Our corpus is available: https: //bitbucket.org/mdredze/tgx 
Understanding expression of emotions in support forums has great value and NLP methods are key to automating this. Many approaches use subjective categories which are more Ô¨Ånegrained than a straightforward polarity-based spectrum. However, the deÔ¨Ånition of such categories is non-trivial, and we argue for a need to incorporate communicative elements even beyond subjectivity. To support our position, we report experiments on a sentiment-labelled corpus of posts from a medical support forum. We argue that a more Ô¨Åne-grained approach to text analysis important, and also simultaneously recognising the social function behind affective expressions enables a more accurate and valuable level of understanding. 
 For social media analysts or social scientists interested in better understanding an audience or demographic cohort, being able to group social media content by demographic characteristics is a useful mechanism to organise data. Social roles are one particular demographic characteristic, which includes work, recreational, community and familial roles. In our work, we look at the task of detecting social roles from English Twitter proÔ¨Åles. We create a new annotated dataset for this task. The dataset includes approximately 1,000 Twitter proÔ¨Åles annotated with social roles. We also describe a machine learning approach for detecting social roles from Twitter proÔ¨Åles, which can act as a strong baseline for this dataset. Finally, we release a set of word clusters obtained in an unsupervised manner from Twitter proÔ¨Åles. These clusters may be useful for other natural language processing tasks in social media. 
This paper investigates the problem of identifying participants in online discussions whose contribution can be considered sensible. Sensibleness of a participant can be indicative of the inÔ¨Çuence a participant may have on the course/outcome of the discussion, as well as other participants in terms of persuading them towards his/her stance. The proposed sensibleness model uses features based on participants‚Äô contribution and the discussion domain to achieve an F1-score of 0.89 & 0.78 for Wikipedia: Articles for Deletion and 4forums.com discussions respectively. 
Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release emoji2vec, pre-trained embeddings for all Unicode emoji which are learned from their description in the Unicode emoji standard.1 The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to estimate a representation. 
Many social media platforms offer a mechanism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the latent modes can be used to weight text features thereby improving prediction accuracy. 
Identifying witness accounts is important for rumor debunking, crises management, and basically any task that involves on the ground eyes. The prevalence of social media has provided citizen journalism with scale and eye witnesses prominence. However, the amount of noise on social media also makes it likely that witness accounts get buried too deep in the noise and are never discovered. In this paper, we explore automatic witness identiÔ¨Åcation in Twitter during emergency events. We attempt to create a generalizable system that not only detects witness reports for unseen events, but also on true out-of-sample ‚Äúreal time streaming set‚Äù that may or may not have witness accounts. We attempt to detect the presence or surge of witness accounts, which is the Ô¨Årst step in developing a model for detecting crisis-related events. We collect and annotate witness tweets for diÔ¨Äerent types of events (earthquake, car accident, Ô¨Åre, cyclone, etc.) explore the related features and build a classiÔ¨Åer to identify witness tweets in real time. Our system is able to signiÔ¨Åcantly outperform prior methods with an average F-score of 89.7% on previously unseen events. 
Previous work on opinion mining and sentiment analysis mainly concerns product, movie, or literature reviews; few applied this technique to analyze the publicity of person. We present a novel document modeling method that utilizes embeddings of emotion keywords to perform reader‚Äôs emotion classiÔ¨Åcation, and calculates a publicity score that serves as a quantiÔ¨Åable measure for the publicity of a person of interest. Experiments are conducted on two Chinese corpora that in total consists of over forty thousand users‚Äô emotional response after reading news articles. Results demonstrate that the proposed method can outperform state-ofthe-art reader-emotion classiÔ¨Åcation methods, and provide a substantial ground for publicity score estimation for candidates of political elections. We believe it is a promising direction for mining the publicity of a person from online social and news media that can be useful for propaganda and other purposes. 
Social media messages‚Äô brevity and unconventional spelling pose a challenge to language identiÔ¨Åcation. We introduce a hierarchical model that learns character and contextualized word-level representations for language identiÔ¨Åcation. Our method performs well against strong baselines, and can also reveal code-switching. 
 We present a dataset in which the contribution of each sentence of a review to the reviewlevel rating is quantiÔ¨Åed by human judges. We deÔ¨Åne an annotation task and crowdsource it for 100 audiobook reviews with 1,662 sentences and 3 aspects: story, performance, and overall quality. The dataset is suitable for intrinsic evaluation of explicit document models with attention mechanisms, for multiaspect sentiment analysis and summarization. We evaluated one such document attention model which uses weighted multiple-instance learning to jointly model aspect ratings and sentence-level rating contributions, and found that there is positive correlation between human and machine attention especially for sentences with high human agreement. 
 Assisted text input techniques can save time and effort and improve text quality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models signiÔ¨Åcantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal beneÔ¨Åts. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more inÔ¨Çuence on the document level than on individual word probabilities. 
Motivated by the need to automate medical information extraction from free-text radiological reports, we present a bi-directional long short-term memory (BiLSTM) neural network architecture for modelling radiological language. The model has been used to address two NLP tasks: medical named-entity recognition (NER) and negation detection. We investigate whether learning several types of word embeddings improves BiLSTM‚Äôs performance on those tasks. Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser. Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks. 
In this paper we are introducing work in progress towards the development of an infrastructure (i.e., design, methodology, creation and description) of linguistic and extra-linguistic data samples acquired from people diagnosed with subjective or mild cognitive impairment and healthy, age-matched controls. The data we are currently collecting consists of various types of modalities; i.e. audio-recorded spoken language samples; transcripts of the audio recordings (text) and eye tracking measurements. The integration of the extra-linguistic information with the linguistic phenotypes and measurements elicited from audio and text, will be used to extract, evaluate and model features to be used in machine learning experiments. In these experiments, classification models that will be trained, that will be able to learn from the whole or a subset of the data to make predictions on new data in order to test how well a differentiation between the aforementioned groups can be made. Features will be also correlated with measured outcomes from e.g. language-related scores, such as word fluency, in order to investigate whether there are relationships between various variables. 
Online health communities and support groups are a valuable source of information for users suffering from a physical or mental illness. Users turn to these forums for moral support or advice on speciÔ¨Åc conditions, symptoms, or side effects of medications. This paper describes and studies the linguistic patterns of a community of support forum users over time focused on the used of anxious related words. We introduce a methodology to identify groups of individuals exhibiting linguistic patterns associated with anxiety and the correlations between this linguistic pattern and other word usage. We Ô¨Ånd some evidence that participation in these groups does yield positive effects on their users by reducing the frequency of anxious related word used over time. 
Estimation of the semantic relatedness between biomedical concepts has utility for many informatics applications. Automated methods fall into two broad categories: methods based on distributional statistics drawn from text corpora, and methods based on the structure of existing knowledge resources. In the former case, taxonomic structure is disregarded. In the latter, semantically relevant empirical information is not considered. In this paper, we present a method that retroÔ¨Åts the context vector representation of MeSH terms by using additional linkage information from UMLS/MeSH hierarchy such that linked concepts have similar vector representations. We evaluated the method relative to previously published physician and coder‚Äôs ratings on sets of MeSH terms. Our experimental results demonstrate that the retroÔ¨Åtted word vector measures obtain a higher correlation with physician judgments. The results also demonstrate a clear improvement on the correlation with experts‚Äô ratings from the retroÔ¨Åtted vector representation in comparison to the vector representation without retroÔ¨Åtting. 
Automatic simpliÔ¨Åcation of clinical notes continues to be an important challenge for NLP systems. A frequent obstacle to developing more robust NLP systems for the clinical domain is the lack of annotated training data. This study investigates unsupervised techniques for one key aspect of medical text simpliÔ¨Åcation, viz. the expansion and disambiguation of acronyms and abbreviations. Our approach combines statistical machine translation with document-context neural language models for the disambiguation of multi-sense terms. In addition we investigate the use of mismatched training data and self-training. These techniques are evaluated on nursing progress notes and obtain a disambiguation accuracy of 71.6% without any manual annotation effort. 
In this paper we present a simple yet effective approach to automatic OCR error detection and correction on a corpus of French clinical reports of variable OCR quality within the domain of foetopathology. While traditional OCR error detection and correction systems rely heavily on external information such as domain-speciÔ¨Åc lexicons, OCR process information or manually corrected training material, these are not always available given the constraints placed on using medical corpora. We therefore propose a novel method that only needs a representative corpus of acceptable OCR quality in order to train models. Our method uses recurrent neural networks (RNNs) to model sequential information on character level for a given medical text corpus. By inserting noise during the training process we can simultaneously learn the underlying (character-level) language model and as well as learning to detect and eliminate random noise from the textual input. The resulting models are robust to the variability of OCR quality but do not require additional, external information such as lexicons. We compare two different ways of injecting noise into the training process and evaluate our models on a manually corrected data set. We Ô¨Ånd that the best performing system achieves a 73% accuracy. 
Automated citation analysis (ACA) can be important for many applications including author ranking and literature based information retrieval, extraction, summarization and question answering. In this study, we developed a new compositional attention network (CAN) model to integrate local and global attention representations with a hierarchical attention mechanism. Training on a new benchmark corpus we built, our evaluation shows that the CAN model performs consistently well on both citation classiÔ¨Åcation and sentiment analysis tasks. 
The scientiÔ¨Åc community is facing raising concerns about the reproducibility of research in many Ô¨Åelds. To address this issue in Natural Language Processing, the CLEF eHealth 2016 lab offered a replication track together with the Clinical Information Extraction task. Herein, we report detailed results of the replication experiments carried out with the three systems submitted to the track. While all results were ultimately replicated, we found that the systems were poorly rated by analysts on documentation aspects such as ‚Äùease of understanding system requirements‚Äù (33%) and ‚Äùprovision of information while system is running‚Äù (33%). As a result, simple steps could be taken by system authors to increase the ease of replicability of their work, thereby increasing the ease of re-using the systems. Our experiments aim to raise the awareness of the community towards the challenges of replication and community sharing of NLP systems. 
Performing systematic reviews is a critical yet manual, labor-intensive step in evidencebased medicine. Automating systematic reviews is an active area of research, requiring innovations in machine learning and computational linguistics. We examine how coreference resolution can aid in identifying the arms of a study, an often overlooked piece of information needed to synthesize the results in a systematic review. A classiÔ¨Åcation model1 that performs better with the coreference features supports the intuition that coreference is able to capture the discourse salience of arms. We note that control arms do not beneÔ¨Åt as much from these features. 
ICD-10 coding of death certiÔ¨Åcates has received renewed attention recently with the organization of the CLEF eHealth 2016 clinical information extraction task (CLEF eHealth 2016 Task 2). This task has been addressed either with dictionary projection methods or with supervised machine learning methods, but none of the participants have tried to design hybrid methods to process these data. The goal of the present paper is to explore such hybrid methods. It proposes several hybrid methods which outperform both plain dictionary projection and supervised machine learning on the training set. On the ofÔ¨Åcial test set, it obtains an F-measure of 0.8586 which is 1pt above the best published results so far on this corpus (p < 10‚àí4). Moreover, it does so with no manual dictionary tuning, and thus has potential for generalization to other languages with little effort. 
Identifying relevant studies from the entire scientific literature is an important task in biomedical research. Past efforts have incorporated semantically recognized biological entities and medical ontologies into biomedical literature search. However, semantic relations are largely overlooked by biomedical search engines. In this work, we aim to discover synonymous biomedical semantic relations between entities and explore their uses in query (semantics) understanding for improved retrieval performance. Specifically, we discover synonymous semantic relations from PubMed queries and apply them to query expansion and specification. In these two real-world scenarios, better PubMed retrieval effectiveness, in terms of recall and precision, can be achieved, demonstrating the utility of our proposed approach. 
In this paper, we focus on the synthetic understanding of documents, speciÔ¨Åcally reading comprehension (RC). A current problem with RC is the need for a method of analyzing the RC system performance to realize further development. We propose a methodology for examining RC systems from multiple viewpoints. Our methodology consists of three steps: deÔ¨Åne a set of basic skills used for RC, manually annotate questions of an existing RC task, and show the performances for each skill of existing systems that have been proposed for the task. We demonstrated the proposed methodology by annotating MCTest, a freely available dataset for testing RC. The results of the annotation showed that answering RC questions requires combinations of multiple skills. In addition, our deÔ¨Åned RC skills were found to be useful and promising for decomposing and analyzing the RC process. Finally, we discuss ways to improve our approach based on the results of two extra annotations. 
An important goal in text understanding is making sense of events. However, there is a gap between computable representations on the one hand and expressive representations on the other hand. We aim to bridge this gap by inducing distributional semantic clusters as labels in a frame structural representation. 
We describe some of our recent efforts in learning statistical models of co-occurring events from large text corpora using Recurrent Neural Networks. 
Entities and events in the world have no frequency, but our communication about them and the expressions we use to refer to them do have a strong frequency proÔ¨Åle. Language expressions and their meanings follow a ZipÔ¨Åan distribution, featuring a small amount of very frequent observations and a very long tail of low frequent observations. Since our NLP datasets sample texts but do not sample the world, they are no exception to Zipf‚Äôs law. This causes a lack of representativeness in our NLP tasks, leading to models that can capture the head phenomena in language, but fail when dealing with the long tail. We therefore propose a referential challenge for semantic NLP that reÔ¨Çects a higher degree of ambiguity and variance and captures a large range of small real-world phenomena. To perform well, systems would have to show deep understanding on the linguistic tail. 
We introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses. Our approach is completely unsupervised, and our experiments show that Neural Network models perform much better (about 20% more accurate) than traditional feature-rich baseline models. We also present a new dataset for Biomedical Language Processing which, with only about 25% of the original corpus vocabulary, still captures the essential distributional semantics of the corpus. 
This paper describes a new spoken dialog portal that connects systems produced by the spoken dialog research community and gives them access to real users. We introduce a prototype dialog framework that affords easy integration with various remote dialog agents as well as external knowledge resources. To date, the DialPort portal has successfully connected to two dialog systems and several public knowledge APIs. We present current progress and envision our future plan.  Our proposed solution is DialPort, a data gathering portal that groups various types of dialog systems, gives potential users a variety of interesting applications, and shares the collected data amongst all participating research groups. The connected dialog systems are not simply listed on a website. They are fully integrated into a single virtual agent. From the user‚Äôs perspective, DialPort is a dialog system that can provide information in many domains and it becomes increasingly more attractive as new research groups join and resulting more functionalities to discover.  
This paper introduces a deceptively simple entity extraction task intended to encourage more interdisciplinary collaboration between Ô¨Åelds that don‚Äôt normally work together: diarization, dialog and entity extraction. Given a corpus of 1.4M call center calls, extract mentions of trouble ticket numbers. The task is challenging because Ô¨Årst mentions need to be distinguished from conÔ¨Årmations to avoid undesirable repetitions. It is common for agents to say part of the ticket number, and customers conÔ¨Årm with a repetition. There are opportunities for dialog (given/new) and diarization (who said what) to help remove repetitions. New information is spoken slowly by one side of a conversation; conÔ¨Årmations are spoken more quickly by the other side of the conversation. 
We present the problem of ‚Äúbringing text to life‚Äù via 3D interactive storytelling, where natural language processing (NLP) techniques are used to transform narrative text into events in a virtual world that the user can interact with. This is a challenging problem, which requires deep understanding of the semantics of a story and the ability to ground those semantic elements to the actors and events of the 3D world‚Äôs graphical engine. We show how this problem has motivated interesting extensions to some classic NLP tasks, identify some of the key lessons learned from the work so far, and propose some future research directions. 
Linguistic style conveys the social context in which communication occurs and deÔ¨Ånes particular ways of using language to engage with the audiences to which the text is accessible. In this work, we are interested in the task of stylistic transfer in natural language generation (NLG) systems, which could have applications in the dissemination of knowledge across styles, automatic summarization and author obfuscation. The main challenges in this task involve the lack of parallel training data and the difÔ¨Åculty in using stylistic features to control generation. To address these challenges, we plan to investigate neural network approaches to NLG to automatically learn and incorporate stylistic features in the process of language generation. We identify several evaluation criteria, and propose manual and automatic evaluation approaches. 
In this paper, we present the concept of using language groundings for contextsensitive text prediction using a semantically informed, context-aware language model. We show initial Ô¨Åndings from a preliminary study investigating how users react to a communication interface driven by context-based prediction using a simple language model. We suggest that the results support further exploration using a more informed semantic model and more realistic context. Keywords‚Äî Grounded language, context sensitive generation, predictive text 
Humans continuously adapt their style and language to a variety of domains. However, a reliable deÔ¨Ånition of ‚Äòdomain‚Äô has eluded researchers thus far. Additionally, the notion of discrete domains stands in contrast to the multiplicity of heterogeneous domains that humans navigate, many of which overlap. In order to better understand the change and variation of human language, we draw on research in domain adaptation and extend the notion of discrete domains to the continuous spectrum. We propose representation learningbased models that can adapt to continuous domains and detail how these can be used to investigate variation in language. To this end, we propose to use dialogue modeling as a test bed due to its proximity to language modeling and its social component. 
A probabilistic or weighted grammar implies a posterior probability distribution over possible parses of a given input sentence. One often needs to extract information from this distribution, by computing the expected counts (in the unknown parse) of various grammar rules, constituents, transitions, or states. This requires an algorithm such as inside-outside or forward-backward that is tailored to the grammar formalism. Conveniently, each such algorithm can be obtained by automatically differentiating an ‚Äúinside‚Äù algorithm that merely computes the log-probability of the evidence (the sentence). This mechanical procedure produces correct and efÔ¨Åcient code. As for any other instance of back-propagation, it can be carried out manually or by software. This pedagogical paper carefully spells out the construction and relates it to traditional and nontraditional views of these algorithms. 
Natural Language Inference (NLI) is a fundamentally important task in natural language processing that has many applications. It is concerned with classifying the logical relation between two sentences. In this paper, we propose attention memory networks (AMNs) to recognize entailment and contradiction between two sentences. In our model, an attention memory neural network (AMNN) has a variable sized encoding memory and supports semantic compositionality. AMNN captures sentence level semantics and reasons relation between the sentence pairs; then we use a Sparsemax layer over the output of the generated matching vectors (sentences) for classiÔ¨Åcation. Our experiments on the Stanford Natural Language Inference (SNLI) Corpus show that our model outperforms the state of the art, achieving an accuracy of 87.4% on the test data. 
In Rhetorical Structure Theory, discourse units participate in asymmetric relationships, with one element acting as the nucleus and the other as the satellite. In the resulting tree-like nuclearity structure, the importance of each discourse unit can be measured by the number of relations in which it acts as the nucleus or as the satellite. Existing approaches to automatically parsing such structures suffer from two problems: they employ local inference techniques that do not capture documentlevel structural regularities, and they rely on annotated training data, which is expensive to obtain at the discourse level. We investigate the SampleRank structure learning algorithm as a potential solution to both problems. SampleRank allows us to incorporate arbitrary document-level features in a global stochastic inference algorithm. Furthermore, it enables the training of a joint model of discourse structure and summarization, which can be learned from document-level summaries alone, without discourse-level supervision. We obtain mixed results in the fully supervised case, and negative results for the joint model of discourse structure and summarization. 
We propose a multi-task learning objective for training joint structured prediction models when no jointly annotated data is available. We use conditional random Ô¨Åelds as the joint predictive model and train their parameters by optimizing the marginal likelihood of all available annotations, with additional posterior constraints on the distributions of the latent variables imposed to enforce agreement. Experiments on named entity recognition and part-of-speech tagging show that the proposed model outperforms independent task estimation, and the posterior constraints provide a useful mechanism for incorporating domainspeciÔ¨Åc knowledge. 
Global features have proven effective in a wide range of structured prediction problems but come with high inference costs. Imitation learning is a common method for training models when exact inference isn‚Äôt feasible. We study imitation learning for Semantic Role Labeling (SRL) and analyze the effectiveness of the Violation Fixing Perceptron (VFP) (Huang et al., 2012) and Locally Optimal Learning to Search (LOLS) (Chang et al., 2015) frameworks with respect to SRL global features. We describe problems in applying each framework to SRL and evaluate the effectiveness of some solutions. We also show that action ordering, including easy Ô¨Årst inference, has a large impact on the quality of greedy global models. 
We introduce DRAIL, a new declarative framework for specifying Deep Relational Models. Our framework separates structural considerations, which express domain knowledge, from the learning architecture to simplify the process of building complex structural models. We show the DRAIL formulation of two NLP tasks, Twitter Part-of-Speech tagging and Entity-Relation extraction. We compare the performance of different deep learning architectures for these structural learning tasks. 
In this work, we present the Ô¨Årst results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag induction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context. 
This paper addresses challenges of Natural Language Processing (NLP) on non-canonical multilingual data in which two or more languages are mixed. It refers to code-switching which has become more popular in our daily life and therefore obtains an increasing amount of attention from the research community. We report our experience that covers not only core NLP tasks such as normalisation, language identiÔ¨Åcation, language modelling, part-of-speech tagging and dependency parsing but also more downstream ones such as machine translation and automatic speech recognition. We highlight and discuss the key problems for each of the tasks with supporting examples from different language pairs and relevant previous work. 
One of the beneÔ¨Åts of language identiÔ¨Åcation that is particularly relevant for code-switching (CS) research is that it permits insight into how the languages are mixed (i.e., the level of integration of the languages). The aim of this paper is to quantify and visualize the nature of the integration of languages in CS documents using simple language-independent metrics that can be adopted by linguists. In our contribution, we (a) make a linguistic case for classifying CS types according to how the languages are integrated; (b) describe our language identiÔ¨Åcation system; (c) introduce an Integration-index (I-index) derived from HMM transition probabilities; (d) employ methods for visualizing integration via a language signature (or switching proÔ¨Åle); and (e) illustrate the utility of our simple metrics for linguists as applied to Spanish-English texts of different switching proÔ¨Åles. 
Codeswitching is a very common behavior among Swahili speakers, but of the little computational work done on Swahili, none has focused on codeswitching. This paper addresses two tasks relating to Swahili-English codeswitching: word-level language identiÔ¨Åcation and prediction of codeswitch points. Our two-step model achieves high accuracy at labeling the language of words using a simple feature set combined with label probabilities on the adjacent words. This system is used to label a large Swahili-English internet corpus, which is in turn used to train a model for predicting codeswitch points. 
Multilingual users of social media sometimes use multiple languages during conversation. Mixing multiple languages in content is known as code-mixing. We annotate a subset of a trilingual code-mixed corpus (Barman et al., 2014) with part-of-speech (POS) tags. We investigate two state-of-the-art POS tagging techniques for code-mixed content and combine the features of the two systems to build a better POS tagger. Furthermore, we investigate the use of a joint model which performs language identiÔ¨Åcation (LID) and partof-speech (POS) tagging simultaneously. 
This paper describes the HHU-UH-G system submitted to the EMNLP 2016 Second Workshop on Computational Approaches to Code Switching. Our system ranked Ô¨Årst place for Arabic (MSA-Egyptian) with an F1-score of 0.83 and second place for Spanish-English with an F1-score of 0.90. The HHU-UHG system introduces a novel uniÔ¨Åed neural network architecture for language identiÔ¨Åcation in code-switched tweets for both SpanishEnglish and MSA-Egyptian dialect. The system makes use of word and character level representations to identify code-switching. For the MSA-Egyptian dialect the system does not rely on any kind of language-speciÔ¨Åc knowledge or linguistic resources such as, Part Of Speech (POS) taggers, morphological analyzers, gazetteers or word lists to obtain state-ofthe-art performance. 
We present SAWT, a web-based tool for the annotation of token sequences with an arbitrary set of labels. The key property of the tool is simplicity and ease of use for both annotators and administrators. SAWT runs in any modern browser, including browsers on mobile devices, and only has minimal server-side requirements. 
Pinyin is the most widely used romanization scheme for Mandarin Chinese. We consider the task of language identiÔ¨Åcation in Pinyin-English codeswitched texts, a task that is signiÔ¨Åcant because of its application to codeswitched text input. We create a codeswitched corpus by extracting and automatically labeling existing Mandarin-English codeswitched corpora. On language identiÔ¨Åcation, we Ô¨Ånd that SVM produces the best result when using word-level segmentation, achieving 99.3% F1 on a Weibo dataset, while a linear-chain CRF produces the best result at the letter level, achieving 98.2% F1. We then pass the output of our models to a system that converts Pinyin back to Chinese characters to simulate codeswitched text input. Our method achieves the same level of performance as an oracle system that has perfect knowledge of token-level language identity. This result demonstrates that Pinyin identiÔ¨Åcation is not the bottleneck towards developing a ChineseEnglish codeswitched Input Method Editor, and future work should focus on the Pinyinto-Chinese character conversion step. 
Code-mixing is a prevalent phenomenon in modern day communication. Though several systems enjoy success in identifying a single language, identifying languages of words in code-mixed texts is a herculean task, more so in a social media context. This paper explores the English-Bengali code-mixing phenomenon and presents algorithms capable of identifying the language of every word to a reasonable accuracy in speciÔ¨Åc cases and the general case. We create and test a predictorcorrector model, develop a new code-mixed corpus from Facebook chat (made available for future research) and test and compare the efÔ¨Åciency of various machine learning algorithms (J48, IBk, Random Forest). The paper also seeks to remove the ambiguities in the token identiÔ¨Åcation process. 
A common step in the processing of any text is the part-of-speech tagging of the input text. In this paper, we present an approach to tackle code-mixed text from three different languages Bengali, Hindi, and Tamil apart from English. Our system uses Conditional Random Field, a sequence learning method, which is useful to capture patterns of sequences containing code switching to tag each word with accurate part-of-speech information. We have used various pre-processing and post-processing modules to improve the performance of our system. The results were satisfactory, with a highest of 75.22% accuracy in Bengali-English mixed data. The methodology that we employed in the task can be used for any resource poor language. We adapted standard learning approaches that work well with scarce data. We have also ensured that the system is portable to different platforms and languages and can be deployed for real-time analysis. 
We address the problem of Part of Speech tagging (POS) in the context of linguistic code switching (CS). CS is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances, known as intra-sentential or inter-sentential CS, respectively. Processing CS data is especially challenging in intrasentential data given state of the art monolingual NLP technology since such technology is geared toward the processing of one language at a time. In this paper we explore multiple strategies of applying state of the art POS taggers to CS data. We investigate the landscape in two CS language pairs, Spanish-English and Modern Standard Arabic-Arabic dialects. We compare the use of two POS taggers vs. a uniÔ¨Åed tagger trained on CS data. Our results show that applying a machine learning framework using two state fof the art POS taggers achieves better performance compared to all other approaches that we investigate. 
Half of the world‚Äôs population is estimated to be at least bilingual. Due to this fact many people use multiple languages interchangeably for effective communication. At the Second Workshop on Computational Approaches to Code Switching, we are presented with a task to label codeswitched, Spanish-English (ES-EN) and Modern Standard Arabic-Dialect Arabic (MSA-DA), tweets. We built a Conditional Random Field (CRF) using wellrounded features to capture not only the two languages but also the other classes. On the Spanish-English(ES-EN) classiÔ¨Åcation task, we obtained weighted F1-score of 0.88 on the tweet level and an accuracy of 96.5% on the token level. On the MSA-DA classiÔ¨Åcation task, our system managed to obtain F1-score of 0.66 on tweet level and overall token level accuracy of 74.7%. 
Codeswitching is a widely observed phenomenon among bilingual speakers. By combining subword information enriched word vectors with linear-chain Conditional Random Field, we develop a supervised machine learning model that identiÔ¨Åes languages in a English-Spanish codeswitched tweets. Our computational method achieves a tweet-level weighted F1 of 0.83 and a token-level accuracy of 0.949 without using any external resource. The result demonstrates that named entity recognition remains a challenge in codeswitched texts and warrants further work. 
Two contrasting paradigms for structuring news events and storylines are identiÔ¨Åed and described: the automated paradigm and the manual paradigm. A speciÔ¨Åc manual news structuring system is described, and the highlevel results of three reporting experiments conducted using the system are presented. In light of these results I then compare automated and manual approaches and argue that they are complementary. A proposal for integrating automated and manual techniques within a structured news ecosystem is presented, and recommendations for integrated approaches are provided. 
Topic Detection and Tracking (TDT) is an important research topic in data mining and information retrieval and has been explored for many years. Most of the studies have approached the problem from the event tracking point of view. We argue that the deÔ¨Ånition of stories as events is not reÔ¨Çecting the full picture. In this work we propose a story tracking method built on crowd-tagging in social media, where news articles are labeled with hashtags in real-time. The social tags act as rich meta-data for news articles, with the advantage that, if carefully employed, they can capture emerging concepts and address concept drift in a story. We present an approach for employing social tags for the purpose of story detection and tracking and show initial empirical results. We compare our method to classic keyword query retrieval and discuss an example of story tracking over time. 
News events and social media are composed of evolving storylines, which capture public attention for a limited period of time. Identifying storylines requires integrating temporal and linguistic information, and prior work takes a largely heuristic approach. We present a novel online non-parametric Bayesian framework for storyline detection, using the distance-dependent Chinese Restaurant Process (dd-CRP). To ensure efÔ¨Åcient linear-time inference, we employ a Ô¨Åxed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluate on the TREC Twitter Timeline Generation (TTG), obtaining encouraging results: despite using a weak baseline retrieval model, the dd-CRP story clustering method is competitive with the best entries in the 2014 TTG task. 
The style of narrative news affects how it is interpreted and received by readers. Two key stylistic characteristics of narrative text are point of view and diegesis: respectively, whether the narrative recounts events personally or impersonally, and whether the narrator is involved in the events of the story. Although central to the interpretation and reception of news, and of narratives more generally, there has been no prior work on automatically identifying these two characteristics in text. We develop automatic classiÔ¨Åers for point of view and diegesis, and compare the performance of different feature sets for both. We built a goldstandard corpus where we double-annotated to substantial agreement (Œ∫ > 0.59) 270 English novels for point of view and diegesis. As might be expected, personal pronouns comprise the best features for point of view classiÔ¨Åcation, achieving an average F1 of 0.928. For diegesis, the best features were personal pronouns and the occurrences of Ô¨Årst person pronouns in the argument of verbs, achieving an average F1 of 0.898. We apply the classiÔ¨Åer to nearly 40,000 news texts across Ô¨Åve different corpora comprising multiple genres (including newswire, opinion, blog posts, and scientiÔ¨Åc press releases), and show that the point of view and diegesis correlates largely as expected with the nominal genre of the texts. We release the training data and the classiÔ¨Åer for use by the community. 
There have been a wide range of recent annotated corpora concerning events, either regarding event coreference, the temporal order of events, hierarchical ‚Äúsubevent‚Äù structure of events, or causal relationships between events. However, although some believe that these different phenomena will display rich interactions, relatively few corpora annotate all of those layers of annotation in a uniÔ¨Åed fashion. This paper describes the annotation methodology for the Richer Event Descriptions corpus, which annotates entities, events, times, their coreference and partial coreference relations, and the temporal, causal and subevent relationships between the events. It suggests that such rich annotations of within-document event phenomena can be built with high quality through a multi-stage annotation pipeline, and that the resultant corpus could be useful for systems hoping to transition from the detection of isolated mentions of events toward a richer understanding of events grounded in the temporal, causal, referential and bridging relations that deÔ¨Åne them. 
In this paper, we investigate the distribution of narrative schemas (Chambers and Jurafsky, 2009) throughout different document categories and how the structure of narrative schemas is conditioned by document category, the converse of the relationship explored in Simonson and Davis (2015). We evaluate cross-category narrative differences by assessing the predictability of verbs in each category and the salience of arguments to events that narrative schemas highlight. For the former, we use the narrative cloze task employed in previous work on schemas. For the latter, we introduce a task that employs narrative schemas called narrative argument salience through entities annotated, or NASTEA. We compare the schemas induced from the entire corpus to those from the subcorpora for each topic using these two types of evaluation. Results of each evaluation vary by each topical subcorpus, in some cases showing improvement, but the NASTEA task additionally reveals that some the documents within some topics are signiÔ¨Åcantly more rigid in their narrative structure, instantiating a limited number of schemas in a highly predictable fashion. 
This paper illustrates a proposal for the development of an annotation scheme and a corpus for storyline extraction and evaluation from large collections of documents clustered around a topic. The scheme extends existing annotation efforts for event coreference and temporal processing, introducing additional layers and addressing shortcomings. We also show how a storyline can be derived from the annotated data. 
Computational linguists have long relied on a distinction between semantic similarity and semantic association to explain and evaluate what is being learned by NLP models. In the present work, we take these same concepts and explore how they apply to an entirely different question - how individuals label other people. Leveraging survey data made public by NLP researchers, we develop our own survey to connect semantic similarity and semantic association to the process by which humans label other people. The result is a set of insights applicable to how we think of semantic similarity as NLP researchers and a new way of leveraging NLP models of semantic similarity and association as researchers of social science. 
Informal genres such as tweets provide large quantities of data in real time, which can be exploited to obtain, through ranking and classiÔ¨Åcation, a succinct summary of the events that occurred. Previous work on tweet ranking and classiÔ¨Åcation mainly focused on salience and social network features or rely on web documents such as online news articles. In this paper, we exploit language independent journalism and content based features to identify news from tweets. We propose a novel newsworthiness classiÔ¨Åer trained through active learning and investigate human assessment and automatic methods to encode it on both the tweet and trending topic levels. Our Ô¨Åndings show that content and journalism based features proved to be effective for ranking and classifying content on Twitter. 
The vast availability of textual data on social media has led to an interest in algorithms to predict user attributes such as gender based on the user‚Äôs writing. These methods are valuable for social science research as well as targeted advertising and proÔ¨Åling, but also compromise the privacy of users who may not realize that their personal idiolects can give away their demographic identities. Can we automatically modify a text so that the author is classiÔ¨Åed as a certain target gender, under limited knowledge of the classiÔ¨Åer, while preserving the text‚Äôs Ô¨Çuency and meaning? We present a basic model to modify a text using lexical substitution, show empirical results with Twitter and Yelp data, and outline ideas for extensions. 
It has been claimed that people are more likely to be inÔ¨Çuenced by those who are similar to them than those who are not. In this paper, we test this hypothesis by measuring the impact of author traits on the detection of inÔ¨Çuence. The traits we explore are age, gender, religion, and political party. We create a single classiÔ¨Åer to detect the author traits of each individual. We then use the personal traits predicted by this classiÔ¨Åer to predict the inÔ¨Çuence of contributors in a Wikipedia Talk Page corpus. Our research shows that the inÔ¨Çuencer tends to have the same traits as the majority of people in the conversation. Furthermore, we show that this is more pronounced when considering the personal traits most relevant to the conversation. Our research thus provides evidence for the theory of social proof. 
Automatically generated political event data is an important part of the social science data ecosystem. The approaches for generating this data, though, have remained largely the same for two decades. During this time, the Ô¨Åeld of computational linguistics has progressed tremendously. This paper presents an overview of political event data, including methods and ontologies, and a set of experiments to determine the applicability of deep neural networks to the extraction of political events from news text. 
This paper presents a novel method for user proÔ¨Åling in social media that makes use of geo-location information associated with social media posts to avoid the need for selfreported data. These posts are combined with two publicly available sources of demographic information to automatically create data sets in which posts are labelled with socio-economic status. The data sets are linked by identifying each user‚Äôs ‚Äòhome location‚Äô. Analysis indicates that the nature of the demographic information is an important factor in performance of this approach. 
We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and ofÔ¨Åcial summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more Ô¨Ånegrained word sources: individual Presidents and (2-year) Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vectorarithmetic-based investigations of complex relationships between word sources based on their texts. We are extending this to create a more comprehensive legal semantic map.  vec(‚Äúking‚Äù) - vec(‚Äúman‚Äù) + vec(‚Äúwoman‚Äù) returns a vector close to vec(‚Äúqueen‚Äù) (Mikolov et al. 2013a). Applying this powerful notion of distributed continuous vector space representations of words, we embed representations of institutions and the words from their law and policy documents into shared semantic space. We can then combine positively and negatively weighted word and government vectors into the same query, enabling complex, targeted and subtle similarity computations. For instance, which government branch is more characterized by ‚Äúvalidity and truth,‚Äù or ‚Äúlong-term government career‚Äù? We apply this method, Gov2Vec, to a unique corpus of Supreme Court opinions, Presidential actions, and ofÔ¨Åcial summaries of Congressional bills. The model discerns meaningful differences between House, Senate, President and Court vectors. We also learn more Ô¨Åne-grained institutional representations: individual Presidents and Congresses (2-year terms). The method implicitly learns important latent relationships between these government actors that was not provided during training. For instance, their temporal ordering was learned from only their text. The resulting vectors are used to explore differences between actors with respect to policy topics.  
We combine social theory and NLP methods to classify English-speaking Twitter users‚Äô online social identity in proÔ¨Åle descriptions. We conduct two text classiÔ¨Åcation experiments. In Experiment 1 we use a 5-category online social identity classiÔ¨Åcation based on identity and self-categorization theories. While we are able to automatically classify two identity categories (Relational and Occupational), automatic classiÔ¨Åcation of the other three identities (Political, Ethnic/religious and Stigmatized) is challenging. In Experiment 2 we test a merger of such identities based on theoretical arguments. We Ô¨Ånd that by combining these identities we can improve the predictive performance of the classiÔ¨Åers in the experiment. Our study shows how social theory can be used to guide NLP methods, and how such methods provide input to revisit traditional social theory that is strongly consolidated in ofÔ¨Çine settings. 
Politicians often use Twitter to express their beliefs, stances on current political issues, and reactions concerning national and international events. Since politicians are scrutinized for what they choose or neglect to say, they craft their statements carefully. Thus despite the limited length of tweets, their content is highly indicative of a politician‚Äôs stances. We present a weakly supervised method for understanding the stances held by politicians, on a wide array of issues, by analyzing how issues are framed in their tweets and their temporal activity patterns. We combine these components into a global model which collectively infers the most likely stance and agreement patterns. 
Understanding the ways in which users interact with different online communities is crucial to social network analysis and community maintenance. We present an unsupervised neural model to learn linguistic descriptors for a user‚Äôs behavior over time within an online community. We show that the descriptors learned by our model capture the functional roles that users occupy in communities, in contrast to those learned via a standard topic-modeling algorithm, which simply reÔ¨Çect topical content. Experiments on the social media forum Reddit show how the model can provide interpretable insights into user behavior. Our model uncovers linguistic differences that correlate with user activity levels and community clustering. 
There have been recent efforts to use social media to estimate demographic characteristics, such as age, gender or income, but there has been little work on investigating the effect of data acquisition methods on producing these estimates. In this paper, we compare four different Twitter data acquisition methods and explore their effects on the prediction of one particular demographic characteristic: occupation (or profession). We present a comparative analysis of the four data acquisition methods in the context of estimating occupation statistics for Australia. Our results show that the social network-based data collection method seems to perform the best. However, we note that each different data collection approach has its own beneÔ¨Åts and limitations. 
Anxiety has a special importance in politics since the emotion is tied to decision-making under uncertainty, a feature of democratic institutions. Yet, measuring speciÔ¨Åc emotions like anxiety in political settings remains a challenging task. The present study tackles this problem by making use of natural language processing (NLP) tools to detect anxiety in a corpus of digitized parliamentary debates from Canada. I rely upon a vector space model to rank parliamentary speeches based on the semantic similarity of their words and syntax with a set of common expressions of anxiety. After assessing the performance of this approach with annotated corpora, I use it to test an implementation of state-trait anxiety theory. The Ô¨Åndings support the hypothesis that political issues with a lower degree of familiarity, such as foreign affairs and immigration, are more anxiogenic than average, a conclusion that appears robust to estimators accounting for unobserved individual traits. 
We present a corpus for protest event mining that combines token-level annotation with the event schema and ontology of entities and events from protest research in the social sciences. The dataset uses newswire reports from the English Gigaword corpus. The token-level annotation is inspired by annotation standards for event extraction, in particular that of the Automated Content Extraction 2005 corpus (Walker et al., 2006). Domain experts perform the entire annotation task. We report competitive intercoder agreement results. 
The lack of demographic information available when conducting passive analysis of social media content can make it difÔ¨Åcult to compare results to traditional survey results. We present DEMOGRAPHER,1 a tool that predicts gender from names, using name lists and a classiÔ¨Åer with simple character-level features. By relying only on a name, our tool can make predictions even without extensive user-authored content. We compare DEMOGRAPHER to other available tools and discuss differences in performance. In particular, we show that DEMOGRAPHER performs well on Twitter data, making it useful for simple and rapid social media demographic inference. 
Social scientists who do not have specialized natural language processing training often use a unigram bag-of-words (BOW) representation when analyzing text corpora. We offer a new phrase-based method, NPFST, for enriching a unigram BOW. NPFST uses a partof-speech tagger and a Ô¨Ånite state transducer to extract multiword phrases to be added to a unigram BOW. We compare NPFST to both ngram and parsing methods in terms of yield, recall, and efÔ¨Åciency. We then demonstrate how to use NPFST for exploratory analyses; it performs well, without conÔ¨Åguration, on many different kinds of English text. Finally, we present a case study using NPFST to analyze a new corpus of U.S. congressional bills. For our open-source implementation, see http://slanglab.cs.umass.edu/phrases/. 
What is the information content of news-based measures of sentiment? How are they related to aggregate economic Ô¨Çuctuations? I construct a sentiment index by measuring the net amount of positive expressions in the corpus of Economic news articles produced by Reuters over the period 1987 - 2013 and across 12 countries. The index successfully tracks Ô¨Çuctuations in Gross Domestic Product (GDP) at the country level, is a leading indicator of GDP growth and contains information to help forecast GDP growth which is not captured by professional forecasts. This suggests that forecasters do not appropriately incorporate available information in predicting future states of the economy.  I build my sentiment index by measuring the net amount of positive expressions in the collection of Economic news articles from Reuters covering 12 countries over the period 1987 - 2013. The index successfully tracks GDP growth over time and across countries. Is sentiment a leading indicator of GDP growth? I estimate an autoregressive model GDP growth to which I add news-based sentiment measures. CoefÔ¨Åcients on news-based sentiment measures are jointly signiÔ¨Åcant at the country level for 10 out of 12 countries in my sample. Sentiment variables reduce in-sample forecast errors of GDP growth by 9.1% on average across countries compared to an autogressive process. This indicates that news sentiment is a leading indicator of GDP growth.  
Computational social science is, at its core, a blending of disciplines‚Äîthe best of human experience, judgement, and anecdotal case studies fused with novel computational methods to extract subtle patterns from immense data. Jointly leveraging such diverse approaches effectively is decidedly nontrivial, but with tremendous potential beneÔ¨Åts. We provide frank assessments from our work bridging the computational linguistics and psychology communities during a range of short and longterm engagements, in the hope that these assessments might provide a foundation upon which those embarking on novel computational social science projects might structure their interactions. 
Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inÔ¨Çuence of annotator knowledge of hate speech on classiÔ¨Åcation models by comparing classiÔ¨Åcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We Ô¨Ånd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.  to the resources required to annotate large data sets and the possibility of distributing the load onto the crowd (Warner and Hirschberg, 2012; Nobata et al., 2016). Ross et al. (2016) investigate annotator reliability for hate speech annotation, concluding that ‚Äúhate speech is a fuzzy construct that requires signiÔ¨Åcantly better deÔ¨Ånitions and guidelines in order to be annotated reliably‚Äù. Hate speech is hard to detect for humans (Sue et al., 2007), which warrants a thorough understanding of the beneÔ¨Åts and pitfalls of crowdsourced annotation. This need is reinforced by previous studies, which utilize crowdsourcing of hate speech without knowledge on the quality of crowdsourced annotations for hate speech labeling. In addition, it is important to understand how different manners of obtaining labeling can inÔ¨Çuence the classiÔ¨Åcation models and how it is possible to obtain good annotations, while ensuring that annotators are not likely to experience adverse effects of annotating hate speech.  
We present a methodology based on topic modeling that can be used to identify and quantify sociolinguistic differences between groups of people, and describe a regression method that can disentangle the inÔ¨Çuences of different attributes of the people in the group (e.g., culture, gender, age). As an example, we explore the concept of personal values, and present a cross-cultural analysis of valuebehavior relationships spanning writers from the United States and India. 
A model is proposed showing how automatically extracted and manually written association rules can be used to build the structure of a narrative from real-life temporal data. The generated text‚Äôs communicative goal is to help the reader construct a causal representation of the events. A connecting associative thread allows the reader to follow associations from the beginning to the end of the text. It is created using a spanning tree over a selected associative sub-network. The results of a text quality evaluation show that the texts were understandable, but that Ô¨Çow between sentences, although not bad, could still be improved. 
We propose a method of probabilistic natural language generation observing both a syntactic structure and an input of situational content. We employed Monte Carlo Tree Search for this nontrivial search problem, employing context-free grammar rules as search operators and evaluating numerous putative generations from these two aspects using logistic regression and n-gram language model. Through several experiments, we conÔ¨Årmed that our method can effectively generate sentences with various words and phrasings. 
Stories are sequential in nature but they are used to package human experience that involves many things happening at the same time, to several people or in several locations. The mechanics of this packaging process constitute an instance of content planning that has not ben addressed in sufÔ¨Åcient detail in existing NLG work. The present paper reviews a number of traditional stories in the light of the basic concepts of narratology that would be involved in the decisions involved in planning the content for tellings of these stories, proposes a number of basic principles to understand what is happening, and explores a possible way in which these principles may translate to basic heuristics for narrative content planning. 
Our software system simulates the classical collaborative Japanese poetry form, renga, made of linked haikus. We used NLP methods wrapped up as web services. This approach is suitable for collaborative human-AI generation, as well as purely computer-generated poetry. Evaluation included a blind survey comparing AI and human haiku. To gather ideas for future work, we examine related research in semiotics, linguistics, and computing. 
We produced two stories by using a computer program and submitted them to the third Hoshi Shinichi Award, a Japanese literary award open to non-humans as well as humans. This paper reports what system we implemented for the submission and how we made the stories by using the system.  Table 1: Number of Stories in 3rd Hoshi Shinichi Award  Adult Student Junior (U-26) (U-16)  submitted  1449 349  763  1st screening  n/a  n/a  n/a  2nd screening n/a  n/a  n/a  3rd screening  16  11  15  Ô¨Ånal (awarded) 6  3  5  
In task-oriented dialogues, there is often only one right answer the system can give. However, a lack of variation can seem repetitive and unnatural. Humans change the way they express something, e.g. by being more or less concise. We aim to approximate this ability by automatically varying the level of verbosity and directness of a given system action. In this work, we illustrate how verbosity and directness may be utilised in adaptive dialogue management and present different approaches to automatically generate varying levels of verbosity and directness for given system actions. Thereby, new and unforeseen system actions can be created dynamically. 
Storytelling systems are computational systems designed to tell stories. Every story generation system deÔ¨Ånes its speciÔ¨Åc knowledge representation for supporting the storytelling process. Thus, there is a shared need amongst all the systems: the knowledge must be expressed unambiguously to avoid inconsistencies. However, when trying to make a comparative assessment between the storytelling systems, there is not a common way for expressing this knowledge. That is when a form of expression that covers the different aspects of the knowledge representations becomes necessary. A suitable solution is the use of a Controlled Natural Language (CNL) which is a good half-way point between natural and formal languages. A CNL can be used as a common medium of expression for this heterogeneous set of systems. This paper proposes the use of Controlled Natural Language for expressing every storytelling system knowledge as a collection of natural language sentences. In this respect, an initial grammar for a CNL is proposed, focusing on certain aspects of this knowledge. 
This paper presents and evaluates a novel system for computer generated poetry. Framed within contemporary theoretical trends in the evaluation of computational creativity, we investigate how evidence of generative process inÔ¨Çuences readers‚Äô opinions of computer generated textual output. In addition to a technical description of our system, we present results from a study asking respondents to evaluate short computer generated poems prefaced with different types of descriptions, in some cases objectively presenting the poem as the product of a statistical analysis of corpora and in some cases subjectively presenting the computer as a self-aware agent. 
The paper presents a reconstruction of the automatic poetry generation system realized in Italy in 1961 by Nanni Balestrini to compose the poem Tape Mark I. The major goal of the paper is to provide a critical comparison between the high-level approach that seems to be suggested by the poet, and the low-level combinatorial algorithm that was actually implemented. This comparison allows to assess the relevance of how the available technology constrained and shaped the work of the poet, to reveal some of his aesthetic assumptions, and to discuss some aspects of the relation between human and the machine in the creative process. 
In this paper, we extend an existing annotation scheme ISO-Space for annotating necessary spatial information for the task placing an specified object at a specified location with a specified direction according to a natural language instruction. We call such task the spatial placement problem. Our extension particularly focuses on describing the object direction, when the object is placed on the 2D plane. We conducted an annotation experiment in which a corpus of 20 situated dialogues were annotated. The annotation result showed the number of newly introduced tags by our proposal is not negligible. We also implemented an analyser that automatically assigns the proposed tags to the corpus and evaluated its performance. The result showed that the performance for entity tag was quite high ranging from 0.68 to 0.99 in F-measure, but not the case for relation tags, i.e. less than 0.4 in F-measure.
This paper proposes a methodology for building a specialized Japanese data set for recognizing temporal relations and discourse relations. In addition to temporal and discourse relations, multi-layered situational relations that distinguish generic and specific states belonging to different layers in a discourse are annotated. Our methodology has been applied to 170 text fragments taken from Wikinews articles in Japanese. The validity of our methodology is evaluated and analyzed in terms of degree of annotator agreement and frequency of errors.
This article proposes a Universal Dependency Annotation Scheme for Mandarin Chinese, including POS tags and dependency analysis. We identify cases of idiosyncrasy of Mandarin Chinese that are difficult to fit into the current schema which has mainly been based on the descriptions of various Indo-European languages. We discuss differences between our scheme and those of the Stanford Chinese Dependencies and the Chinese Dependency Treebank.
The approach which formulates the automatic text summarization as a maximum coverage problem with knapsack constraint over a set of textual units and a set of weighted conceptual units is promising. However, it is quite important and difficult to determine the appropriate granularity of conceptual units for this formulation. In order to resolve this problem, we are examining to use components of presentation slides as conceptual units to generate a summary of lecture utterances, instead of other possible conceptual units like base noun phrases or important nouns. This paper explains our developing corpus designed to evaluate our proposing approach, which consists of presentation slides and lecture utterances aligned to presentation slide components.
This paper presents VSoLSCSum, a Vietnamese linked sentence-comment dataset, which was manually created to treat the lack of standard corpora for social context summarization in Vietnamese. The dataset was collected through the keywords of 141 Web documents in 12 special events, which were mentioned on Vietnamese Web pages. Social users were asked to involve in creating standard summaries and the label of each sentence or comment. The inter-agreement calculated by Cohen{'}s Kappa among raters after validating is 0.685. To illustrate the potential use of our dataset, a learning to rank method was trained by using a set of local and social features. Experimental results indicate that the summary model trained on our dataset outperforms state-of-the-art baselines in both ROUGE-1 and ROUGE-2 in social context summarization.
Paratactic syntactic structures are difficult to represent in syntactic dependency tree structures. As such, we propose an annotation schema for syntactic dependency annotation of Japanese, in which coordinate structures are split from and overlaid on bunsetsu-based (base phrase unit) dependency. The schema represents nested coordinate structures, non-constituent conjuncts, and forward sharing as the set of regions. The annotation was performed on the core data of {`}Balanced Corpus of Contemporary Written Japanese{'}, which comprised about one million words and 1980 samples from six registers, such as newspapers, books, magazines, and web texts.
Treebanks are curial for natural language processing (NLP). In this paper, we present our work for annotating a Chinese treebank in scientific domain (SCTB), to address the problem of the lack of Chinese treebanks in this domain. Chinese analysis and machine translation experiments conducted using this treebank indicate that the annotated treebank can significantly improve the performance on both tasks. This treebank is released to promote Chinese NLP research in scientific domain.
This paper introduces the NIFTY-Serve corpus, a large data archive collected from Japanese discussion forums that operated via a Bulletin Board System (BBS) between 1987 and 2006. This corpus can be used in Artificial Intelligence researches such as Natural Language Processing, Community Analysis, and so on. The NIFTY-Serve corpus differs from data on WWW in three ways; (1) essentially spam- and duplication-free because of strict data collection procedures, (2) historic user-generated data before WWW, and (3) a complete data set because the service now shut down. We also introduce some examples of use of the corpus.
This paper describes various Indonesian language resources that Agency for the Assessment and Application of Technology (BPPT) has developed and collected since mid 80{'}s when we joined MMTS (Multilingual Machine Translation System), an international project coordinated by CICC-Japan to develop a machine translation system for five Asian languages (Bahasa Indonesia, Malay, Thai, Japanese, and Chinese). Since then, we have been actively doing many types of research in the field of statistical machine translation, speech recognition, and speech synthesis which requires many text and speech corpus. Most recent cooperation within ASEAN-IVO is the development of Indonesian ALT (Asian Language Treebank) has added new NLP tools.
This paper describes a Japanese political corpus created for interdisciplinary political research. The corpus contains the local assembly minutes of 47 prefectures from April 2011 to March 2015. This four-year period coincides with the term of office for assembly members in most autonomies. We analyze statistical data, such as the number of speakers, characters, and words, to clarify the characteristics of local assembly minutes. In addition, we identify problems associated with the different web services used by the autonomies to make the minutes available to the public.
Many NLP tasks involve sentence-level annotation yet the relevant information is not encoded at sentence level but at some relevant parts of the sentence. Such tasks include but are not limited to: sentiment expression annotation, product feature annotation, and template annotation for Q{\&}A systems. However, annotation of the full corpus sentence by sentence is resource intensive. In this paper, we propose an approach that iteratively extracts frequent parts of sentences for annotating, and compresses the set of sentences after each round of annotation. Our approach can also be used in preparing training sentences for binary classification (domain-related vs. noise, subjectivity vs. objectivity, etc.), assuming that sentence-type annotation can be predicted by annotation of the most relevant sub-sentences. Two experiments are performed to test our proposal and evaluated in terms of time saved and agreement of annotation.
Summarization of multi-party conversation is one of the important tasks in natural language processing. In this paper, we explain a Japanese corpus and a topic segmentation task. To the best of our knowledge, the corpus is the first Japanese corpus annotated for summarization tasks and freely available to anyone. We call it {``}the Kyutech corpus.{''} The task of the corpus is a decision-making task with four participants and it contains utterances with time information, topic segmentation and reference summaries. As a case study for the corpus, we describe a method combined with LCSeg and TopicTiling for a topic segmentation task. We discuss the effectiveness and the problems of the combined method through the experiment with the Kyutech corpus.
In this paper we present two methods for automatic common sense knowledge evaluation for Japanese entries in ConceptNet ontology. Our proposed methods utilize text-mining approach: one with relation clue words and WordNet synonyms, and one without. Both methods were tested with a blog corpus. The system based on our proposed methods reached relatively high precision score for three relations (MadeOf, UsedFor, AtLocation), which is comparable with previous research using commercial search engines and simpler input. We analyze errors and discuss problems of common sense evaluation, both manual and automatic and propose ideas for further improvements.
Although MWE are relatively morphologically and syntactically fixed expressions, several types of flexibility can be observed in MWE, verbal MWE in particular. Identifying the degree of morphological and syntactic flexibility of MWE is very important for many Lexicographic and NLP tasks. Adding MWE variants/tokens to a dictionary resource requires characterizing the flexibility among other morphosyntactic features. Carrying out the task manually faces several challenges since it is a very laborious task time and effort wise, as well as it will suffer from coverage limitation. The problem is exacerbated in rich morphological languages where the average word in Arabic could have 12 possible inflection forms. Accordingly, in this paper we introduce a semi-automatic Arabic multiwords expressions resource (SAMER). We propose an automated method that identifies the morphological and syntactic flexibility of Arabic Verbal Multiword Expressions (AVMWE). All observed morphological variants and syntactic pattern alternations of an AVMWE are automatically acquired using large scale corpora. We look for three morphosyntactic aspects of AVMWE types investigating derivational and inflectional variations and syntactic templates, namely: 1) inflectional variation (inflectional paradigm) and calculating degree of flexibility; 2) derivational productivity; and 3) identifying and classifying the different syntactic types. We build a comprehensive list of AVMWE. Every token in the AVMWE list is lemmatized and tagged with POS information. We then search Arabic Gigaword and All ATBs for all possible flexible matches. For each AVMWE type we generate: a) a statistically ranked list of MWE-lexeme inflections and syntactic pattern alternations; b) An abstract syntactic template; and c) The most frequent form. Our technique is validated using a Golden MWE annotated list. The results shows that the quality of the generated resource is 80.04{\%}.
This paper describes our attempt to build a sentiment analysis system for Indonesian tweets. With this system, we can study and identify sentiments and opinions in a text or document computationally. We used four thousand manually labeled tweets collected in February and March 2016 to build the model. Because of the variety of content in tweets, we analyze tweets into eight groups in total, including pos(itive), neg(ative), and neu(tral). Finally, we obtained 73.2{\%} accuracy with Long Short Term Memory (LSTM) without normalizer.
Distributional Semantic Models (DSMs) have recently received increased attention, together with the rise of neural architectures for scalable training of dense vector embeddings. While some of the literature even includes terms like {`}vectors{'} and {`}dimensionality{'} in the definition of DSMs, there are some good reasons why we should consider alternative formulations of distributional models. As an instance, I present a scalable graph-based solution to distributional semantics. The model belongs to the family of {`}count-based{'} DSMs, keeps its representation sparse and explicit, and thus fully interpretable. I will highlight some important differences between sparse graph-based and dense vector approaches to DSMs: while dense vector-based models are computationally easier to handle and provide a nice uniform representation that can be compared and combined in many ways, they lack interpretability, provenance and robustness. On the other hand, graph-based sparse models have a more straightforward interpretation, handle sense distinctions more naturally and can straightforwardly be linked to knowledge bases, while lacking the ability to compare arbitrary lexical units and a compositionality operation. Since both representations have their merits, I opt for exploring their combination in the outlook.
Notwithstanding the success of the notion of construction, the computational tradition still lacks a way to represent the semantic content of these linguistic entities. Here we present a simple corpus-based model implementing the idea that the meaning of a syntactic construction is intimately related to the semantics of its typical verbs. It is a two-step process, that starts by identifying the typical verbs occurring with a given syntactic construction and building their distributional vectors. We then calculated the weighted centroid of these vectors in order to derive the distributional signature of a construction. In order to assess the goodness of our approach, we replicated the priming effect described by Johnson and Golberg (2013) as a function of the semantic distance between a construction and its prototypical verbs. Additional support for our view comes from a regression analysis showing that our distributional information can be used to model behavioral data collected with a crowdsourced elicitation experiment.
Regular polysemy was extensively investigated in lexical semantics, but this phenomenon has been very little studied in distributional semantics. We propose a model for regular polysemy detection that is based on sense vectors and allows to work directly with senses in semantic vector space. Our method is able to detect polysemous words that have the same regular sense alternation as in a given example (a word with two automatically induced senses that represent one polysemy pattern, such as ANIMAL / FOOD). The method works equally well for nouns, verbs and adjectives and achieves average recall of 0.55 and average precision of 0.59 for ten different polysemy patterns.
Recognizing various semantic relations between terms is beneficial for many NLP tasks. While path-based and distributional information sources are considered complementary for this task, the superior results the latter showed recently suggested that the former{'}s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. The empirical results show that this method is effective in the multiclass setting as well. We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information.
The identification of semantic relations between terms within texts is a fundamental task in Natural Language Processing which can support applications requiring a lightweight semantic interpretation model. Currently, semantic relation classification concentrates on relations which are evaluated over open-domain data. This work provides a critique on the set of abstract relations used for semantic relation classification with regard to their ability to express relationships between terms which are found in a domain-specific corpora. Based on this analysis, this work proposes an alternative semantic relation model based on reusing and extending the set of abstract relations present in the DOLCE ontology. The resulting set of relations is well grounded, allows to capture a wide range of relations and could thus be used as a foundation for automatic classification of semantic relations.
The interaction between roots and patterns in Arabic has intrigued lexicographers and morphologists for centuries. While roots provide the consonantal building blocks, patterns provide the syllabic vocalic moulds. While roots provide abstract semantic classes, patterns realize these classes in specific instances. In this way both roots and patterns are indispensable for understanding the derivational, morphological and, to some extent, the cognitive aspects of the Arabic language. In this paper we perform lemmatization (a high-level lexical processing) without relying on a lookup dictionary. We use a hybrid approach that consists of a machine learning classifier to predict the lemma pattern for a given stem, and mapping rules to convert stems to their respective lemmas with the vocalization defined by the pattern.
In this paper we present a clean, yet effective, model for word sense disambiguation. Our approach leverage a bidirectional long short-term memory network which is shared between all words. This enables the model to share statistical strength and to scale well with vocabulary size. The model is trained end-to-end, directly from the raw text to sense labels, and makes effective use of word order. We evaluate our approach on two standard datasets, using identical hyperparameter settings, which are in turn tuned on a third set of held out data. We employ no external resources (e.g. knowledge graphs, part-of-speech tagging, etc), language specific features, or hand crafted rules, but still achieve statistically equivalent results to the best state-of-the-art systems, that employ no such limitations.
Language production is largely a matter of words which, in the case of access problems, can be searched for in an external resource (lexicon, thesaurus). In this kind of dialogue the user provides the momentarily available knowledge concerning the target and the system responds with the best guess(es) it can make given this input. As tip-of-the-tongue (ToT)-studies have shown, people always have some knowledge concerning the target (meaning fragments, number of syllables, ...) even if its complete form is eluding them. We will show here how to tap on this knowledge to build a resource likely to help authors (speakers/writers) to overcome the ToT-problem. Yet, before doing so we need a better understanding of the various kinds of knowledge people have when looking for a word. To this end, we asked crowdworkers to provide some cues to describe a given target and to specify then how each one of them relates to the target, in the hope that this could help others to find the elusive word. Next, we checked how well a given search strategy worked when being applied to differently built lexical networks. The results showed quite dramatic differences, which is not really surprising. After all, different networks are built for different purposes; hence each one of them is more or less suited for a given task. What was more surprising though is the fact that the relational information given by the users did not allow us to find the elusive word in WordNet better than without it.
This paper deals with a seldom studied object/oblique alternation phenomenon in Japanese, which. We call this the bump alternation. This phenomenon, first discussed by Sadanobu (1990), is similar to the English with/against alternation. For example, compare hit the wall with the bat [=immobile-as-direct-object frame] to hit the bat against the wall [=mobile-as-direct-object frame]). However, in the Japanese version, the case frame remains constant. Although we fundamentally question Sadanobu{'}s acceptability judgment, we also claim that the causation type (i.e., whether the event is an instance of onset or extended causation; Talmy, 1988; 2000) could make an improvement. An extended causative interpretation could improve the acceptability of the otherwise awkward immobile-as-direct-object frame. We examined this claim through a rating study, and the results showed an interaction between the Causation type (extended/onset) and the Object type (mobile/immobile) in the direction we predicted. We propose that a perspective shift on what is moving causes the {``}extended causation{''} advantage.
German particle verbs represent a frequent type of multi-word-expression that forms a highly productive paradigm in the lexicon. Similarly to other multi-word expressions, particle verbs exhibit various levels of compositionality. One of the major obstacles for the study of compositionality is the lack of representative gold standards of human ratings. In order to address this bottleneck, this paper presents such a gold standard data set containing 400 randomly selected German particle verbs. It is balanced across several particle types and three frequency bands, and accomplished by human ratings on the degree of semantic compositionality.
This paper presents a method to discover possible terminological relationships from tweets. We match the histories of terms (frequency patterns). Similar history indicates a possible relationship between terms. For example, if two terms (t1, t2) appeared frequently in Twitter at particular days, and there is a {`}similarity{'} in the frequencies over a period of time, then t1 and t2 can be related. Maintaining standard terminological repository with updated relationships can be difficult; especially in a dynamic domain such as social media where thousands of new terms (neology) are coined every day. So we propose to construct a raw repository of lexical units with unconfirmed relationships. We have experimented our method on time-sensitive Arabic terms used by the online Arabic community of Twitter. We draw relationships between these terms by matching their similar frequency patterns (timelines). We use dynamic time warping as a similarity measure. For evaluation, we have selected 630 possible terms (we call them preterms) and we matched the similarity of these terms over a period of 30 days. Around 270 correct relationships were discovered with a precision of 0.61. These relationships were extracted without considering the textual context of the term.
A lexical function represents a type of relation that exists between lexical units (words or expressions) in any language. For example, the antonymy is a type of relation that is represented by the lexical function Anti: Anti(big) = small. Those relations include both paradigmatic relations, i.e. vertical relations, such as synonymy, antonymy and meronymy and syntagmatic relations, i.e. horizontal relations, such as objective qualification (legitimate demand), subjective qualification (fruitful analysis), positive evaluation (good review) and support verbs (pay a visit, subject to an interrogation). In this paper, we present the Lexical Functions Ontology Model (lexfom) to represent lexical functions and the relation among lexical units. Lexfom is divided in four modules: lexical function representation (lfrep), lexical function family (lffam), lexical function semantic perspective (lfsem) and lexical function relations (lfrel). Moreover, we show how it combines to Lexical Model for Ontologies (lemon), for the transformation of lexical networks into the semantic web formats. So far, we have implemented 100 simple and 500 complex lexical functions, and encoded about 8,000 syntagmatic and 46,000 paradigmatic relations, for the French language.
The objectives of the work described in this paper are: 1. To list the differences between a general language resource (namely FrameNet) and a domain-specific resource; 2. To devise solutions to merge their contents in order to increase the coverage of the general resource. Both resources are based on Frame Semantics (Fillmore 1985; Fillmore and Baker 2010) and this raises specific challenges since the theoretical framework and the methodology derived from it provide for both a lexical description and a conceptual representation. We propose a series of strategies that handle both lexical and conceptual (frame) differences and implemented them in the specialized resource. We also show that most differences can be handled in a straightforward manner. However, some more domain specific differences (such as frames defined exclusively for the specialized domain or relations between these frames) are likely to be much more difficult to take into account since some are domain-specific.
The present paper investigates the phenomenon of antonym canonicity by providing new behavioural and distributional evidence on Italian adjectives. Previous studies have showed that some pairs of antonyms are perceived to be better examples of opposition than others, and are so considered representative of the whole category (e.g., Deese, 1964; Murphy, 2003; Paradis et al., 2009). Our goal is to further investigate why such canonical pairs (Murphy, 2003) exist and how they come to be associated. In the literature, two different approaches have dealt with this issue. The lexical-categorical approach (Charles and Miller, 1989; Justeson and Katz, 1991) finds the cause of canonicity in the high co-occurrence frequency of the two adjectives. The cognitive-prototype approach (Paradis et al., 2009; Jones et al., 2012) instead claims that two adjectives form a canonical pair because they are aligned along a simple and salient dimension. Our empirical evidence, while supporting the latter view, shows that the paradigmatic distributional properties of adjectives can also contribute to explain the phenomenon of canonicity, providing a corpus-based correlate of the cognitive notion of salience.
Understanding the semantic relationships between terms is a fundamental task in natural language processing applications. While structured resources that can express those relationships in a formal way, such as ontologies, are still scarce, a large number of linguistic resources gathering dictionary definitions is becoming available, but understanding the semantic structure of natural language definitions is fundamental to make them useful in semantic interpretation tasks. Based on an analysis of a subset of WordNet{'}s glosses, we propose a set of semantic roles that compose the semantic structure of a dictionary definition, and show how they are related to the definition{'}s syntactic configuration, identifying patterns that can be used in the development of information extraction frameworks and semantic models.
Although quantifiers/classifiers expressions occur frequently in everyday communications or written documents, there is no description for them in classical bilingual paper dictionaries, nor in machine-readable dictionaries. The paper describes a corpus and dictionary development for quantifiers/classifiers, and their usage in the framework of French-Japanese machine translation (MT). They often cause problems of lexical ambiguity and of set phrase recognition during analysis, in particular for a long-distance language pair like French and Japanese. For the development of a dictionary aiming at ambiguity resolution for expressions including quantifiers and classifiers which may be ambiguous with common nouns, we have annotated our corpus with UWs (interlingual lexemes) of UNL (Universal Networking Language) found on the UNL-jp dictionary. The extraction of potential classifiers/quantifiers from corpus is made by UNLexplorer web service. Keywords : classifiers, quantifiers, phraseology study, corpus annotation, UNL (Universal Networking Language), UWs dictionary, Tori Bank, French-Japanese machine translation (MT).
We present Kathaa, an Open Source web-based Visual Programming Framework for Natural Language Processing (NLP) Systems. Kathaa supports the design, execution and analysis of complex NLP systems by visually connecting NLP components from an easily extensible Module Library. It models NLP systems an edge-labeled Directed Acyclic MultiGraph, and lets the user use publicly co-created modules in their own NLP applications irrespective of their technical proficiency in Natural Language Processing. Kathaa exposes an intuitive web based Interface for the users to interact with and modify complex NLP Systems; and a precise Module definition API to allow easy integration of new state of the art NLP components. Kathaa enables researchers to publish their services in a standardized format to enable the masses to use their services out of the box. The vision of this work is to pave the way for a system like Kathaa, to be the Lego blocks of NLP Research and Applications. As a practical use case we use Kathaa to visually implement the Sampark Hindi-Panjabi Machine Translation Pipeline and the Sampark Hindi-Urdu Machine Translation Pipeline, to demonstrate the fact that Kathaa can handle really complex NLP systems while still being intuitive for the end user.
The US National Science Foundation (NSF) SI2-funded LAPPS/Galaxy project has developed an open-source platform for enabling complex analyses while hiding complexities associated with underlying infrastructure, that can be accessed through a web interface, deployed on any Unix system, or run from the cloud. It provides sophisticated tool integration and history capabilities, a workflow system for building automated multi-step analyses, state-of-the-art evaluation capabilities, and facilities for sharing and publishing analyses. This paper describes the current facilities available in LAPPS/Galaxy and outlines the project{'}s ongoing activities to enhance the framework.
Most tools for natural language processing today are based on machine learning and come with pre-trained models. In addition, third-parties provide pre-trained models for popular NLP tools. The predictive power and accuracy of these tools depends on the quality of these models. Downstream researchers often base their results on pre-trained models instead of training their own. Consequently, pre-trained models are an essential resource to our community. However, to be best of our knowledge, no systematic study of pre-trained models has been conducted so far. This paper reports on the analysis of 274 pre-models for six NLP tools and four potential causes of problems: encoding, tokenization, normalization and change over time. The analysis is implemented in the open source tool Model Investigator. Our work 1) allows model consumers to better assess whether a model is suitable for their task, 2) enables tool and model creators to sanity-check their models before distributing them, and 3) enables improvements in tool interoperability by performing automatic adjustments of normalization or other pre-processing based on the models used.
In this research, we introduce and implement a method that combines human inputters and machine translators. When the languages of the participants vary widely, the cost of simultaneous translation becomes very high. However, the results of simply applying machine translation to speech text do not have the quality that is needed for real use. Thus, we propose a method that people who understand the language of the speaker cooperate with a machine translation service in support of multilingualization by the co-creation of value. We implement a system with this method and apply it to actual presentations. While the quality of direct machine translations is 1.84 (fluency) and 2.89 (adequacy), the system has corresponding values of 3.76 and 3.85.
Complaint classification aims at using information to deliver greater insights to enhance user experience after purchasing the products or services. Categorized information can help us quickly collect emerging problems in order to provide a support needed. Indeed, the response to the complaint without the delay will grant users highest satisfaction. In this paper, we aim to deliver a novel approach which can clarify the complaints precisely with the aim to classify each complaint into nine predefined classes i.e. acces-sibility, company brand, competitors, facilities, process, product feature, staff quality, timing respec-tively and others. Given the idea that one word usually conveys ambiguity and it has to be interpreted by its context, the word embedding technique is used to provide word features while applying deep learning techniques for classifying a type of complaints. The dataset we use contains 8,439 complaints of one company.
The Universal Dependencies (UD) Project seeks to build a cross-lingual studies of treebanks, linguistic structures and parsing. Its goal is to create a set of multilingual harmonized treebanks that are designed according to a universal annotation scheme. In this paper, we report on the conversion of the Uyghur dependency treebank to a UD version of the treebank which we term the Uyghur Universal Dependency Treebank (UyDT). We present the mapping of the Uyghur dependency treebank{'}s labelling scheme to the UD scheme, along with a clear description of the structural changes required in this conversion.
In this paper we describe a non-expert setup for Vietnamese speech recognition system using Kaldi toolkit. We collected a speech corpus over fifteen hours from about fifty Vietnamese native speakers and using it to test the feasibility of our setup. The essential linguistic components for the Automatic Speech Recognition (ASR) system was prepared basing on the written form of the language instead of expertise knowledge on linguistic and phonology as commonly seen in rich resource languages like English. The modeling of tones by integrating them into the phoneme and using the phonetic decision tree is also discussed. Experimental results showed this setup for ASR systems does yield competitive results while still have potentials for further improvements.
Annotated corpora are crucial language resources, and pre-annotation is an usual way to reduce the cost of corpus construction. Ensemble based pre-annotation approach combines multiple existing named entity taggers and categorizes annotations into normal annotations with high confidence and candidate annotations with low confidence, to reduce the human annotation time. In this paper, we manually annotate three English datasets under various pre-annotation conditions, report the effects of ensemble based pre-annotation, and analyze the experimental results. In order to verify the effectiveness of ensemble based pre-annotation in other languages, such as Chinese, three Chinese datasets are also tested. The experimental results show that the ensemble based pre-annotation approach significantly reduces the number of annotations which human annotators have to add, and outperforms the baseline approaches in reduction of human annotation time without loss in annotation performance (in terms of F1-measure), on both English and Chinese datasets.
Fragmentation and recombination is a key to create customized language environments for supporting various intercultural activities. Fragmentation provides various language resource components for the customized language environments and recombination builds each language environment according to user{'}s request by combining these components. To realize this fragmentation and recombination process, existing language resources (both data and programs) should be shared as language services and combined beyond mismatch of their service interfaces. To address this issue, standardization is inevitable: standardized interfaces are necessary for language services as well as data format required for language resources. Therefore, we have constructed a hierarchy of language services based on inheritance of service interfaces, which is called language service ontology. This ontology allows users to create a new customized language service that is compatible with existing ones. Moreover, we have developed a dynamic service binding technology that instantiates various executable customized services from an abstract workflow according to user{'}s request. By using the ontology and service binding together, users can bind the instantiated language service to another abstract workflow for a new customized one.
Different types of users require different functions in NLP software. It is difficult for a single platform to cover all types of users. When a framework aims to provide more interoperability, users are required to learn more concepts; users{'} application designs are restricted to be compliant with the framework. While an interoperability framework is useful in certain cases, some types of users will not select the framework due to the learning cost and design restrictions. We suggest a rather simple framework for the interoperability aiming at developers. Reusing an existing NLP platform Kachako, we created an API oriented NLP system. This system loosely couples rich high-end functions, including annotation visualizations, statistical evaluations, an-notation searching, etc. This API do not require users much learning cost, providing customization ability for power users while also allowing easy users to employ many GUI functions.
Methods based on deep learning approaches have recently achieved state-of-the-art performance in a range of machine learning tasks and are increasingly applied to natural language processing (NLP). Despite strong results in various established NLP tasks involving general domain texts, there is only limited work applying these models to biomedical NLP. In this paper, we consider a Convolutional Neural Network (CNN) approach to biomedical text classification. Evaluation using a recently introduced cancer domain dataset involving the categorization of documents according to the well-established hallmarks of cancer shows that a basic CNN model can achieve a level of performance competitive with a Support Vector Machine (SVM) trained using complex manually engineered features optimized to the task. We further show that simple modifications to the CNN hyperparameters, initialization, and training process allow the model to notably outperform the SVM, establishing a new state of the art result at this task. We make all of the resources and tools introduced in this study available under open licenses from \url{https://cambridgeltl.github.io/cancer-hallmark-cnn/}.
End-to-end neural network models for named entity recognition (NER) have shown to achieve effective performances on general domain datasets (e.g. newswire), without requiring additional hand-crafted features. However, in biomedical domain, recent studies have shown that hand-engineered features (e.g. orthographic features) should be used to attain effective performance, due to the complexity of biomedical terminology (e.g. the use of acronyms and complex gene names). In this work, we propose a novel approach that allows a neural network model based on a long short-term memory (LSTM) to automatically learn orthographic features and incorporate them into a model for biomedical NER. Importantly, our bi-directional LSTM model learns and leverages orthographic features on an end-to-end basis. We evaluate our approach by comparing against existing neural network models for NER using three well-established biomedical datasets. Our experimental results show that the proposed approach consistently outperforms these strong baselines across all of the three datasets.
This paper proposes several network construction methods for collections of scarce scientific literature data. We define scarcity as lacking in value and in volume. Instead of using the paper{'}s metadata to construct several kinds of scientific networks, we use the full texts of the articles and automatically extract the entities needed to construct the networks. Specifically, we present seven kinds of networks using the proposed construction methods: co-occurrence networks for author, keyword, and biological entities, and citation networks for author, keyword, biological, and topic entities. We show two case studies that applies our proposed methods: CADASIL, a rare yet the most common form of hereditary stroke disorder, and Metformin, the first-line medication to the type 2 diabetes treatment. We apply our proposed method to four different applications for evaluation: finding prolific authors, finding important bio-entities, finding meaningful keywords, and discovering influential topics. The results show that the co-occurrence and citation networks constructed using the proposed method outperforms the traditional-based networks. We also compare our proposed networks to traditional citation networks constructed using enough data and infer that even with the same amount of enough data, our methods perform comparably or better than the traditional methods.
We propose an approach for named entity recognition in medical data, using a character-based deep bidirectional recurrent neural network. Such models can learn features and patterns based on the character sequence, and are not limited to a fixed vocabulary. This makes them very well suited for the NER task in the medical domain. Our experimental evaluation shows promising results, with a 60{\%} improvement in F 1 score over the baseline, and our system generalizes well between different datasets.
The increasing amount of biomedical information that is available for researchers and clinicians makes it harder to quickly find the right information. Automatic summarization of multiple texts can provide summaries specific to the user{'}s information needs. In this paper we look into the use named-entity recognition for graph-based summarization. We extend the LexRank algorithm with information about named entities and present EntityRank, a multi-document graph-based summarization algorithm that is solely based on named entities. We evaluate our system on a datasets of 1009 human written summaries provided by BioASQ and on 1974 gene summaries, fetched from the Entrez Gene database. The results show that the addition of named-entity information increases the performance of graph-based summarizers and that the EntityRank significantly outperforms the other methods with regard to the ROUGE measures.
Electronic health records show great variability since the same concept is often expressed with different terms, either scientific latin forms, common or lay variants and even vernacular naming. Deep learning enables distributional representation of terms in a vector-space, and therefore, related terms tend to be close in the vector space. Accordingly, embedding words through these vectors opens the way towards accounting for semantic relatedness through classical algebraic operations. In this work we propose a simple though efficient unsupervised characterization of Adverse Drug Reactions (ADRs). This approach exploits the embedding representation of the terms involved in candidate ADR events, that is, drug-disease entity pairs. In brief, the ADRs are represented as vectors that link the drug with the disease in their context through a recursive additive model. We discovered that a low-dimensional representation that makes use of the modulus and argument of the embedded representation of the ADR event shows correlation with the manually annotated class. Thus, it can be derived that this characterization results in to be beneficial for further classification tasks as predictive features.
The development of text mining techniques for biomedical research literature has received increased attention in recent times. However, most of these techniques focus on prose, while much important biomedical data reside in tables. In this paper, we present a corpus created to serve as a gold standard for the development and evaluation of techniques for the automatic extraction of information from biomedical tables. We describe the guidelines used for corpus annotation and the manner in which they were developed. The high inter-annotator agreement achieved on the corpus, and the generic nature of our annotation approach, suggest that the developed guidelines can serve as a general framework for table annotation in biomedical and other scientific domains. The annotated corpus and the guidelines are available at \url{http://www.csse.monash.edu.au/research/umnl/data/index.shtml}.
In some plain text documents, end-of-line marks may or may not mark the boundary of a text unit (e.g., of a paragraph). This vexing problem is likely to impact subsequent natural language processing components, but is seldom addressed in the literature. We propose a method which uses no manual annotation to classify whether end-of-lines must actually be seen as simple spaces (soft line breaks) or as true text unit boundaries. This method, which includes self-training and co-training steps based on token and line length features, achieves 0.943 F-measure on a corpus of short e-books with controlled format, F=0.904 on a random sample of 24 clinical texts with soft line breaks, and F=0.898 on a larger set of mixed clinical texts which may or may not contain soft line breaks, a fairly high value for a method with no manual annotation.
This paper describes a Natural language processing system developed for automatic identification of explicit connectives, its sense and arguments. Prior work has shown that the difference in usage of connectives across corpora affects the cross domain connective identification task negatively. Hence the development of domain specific discourse parser has become indispensable. Here, we present a corpus annotated with discourse relations on Medline abstracts. Kappa score is calculated to check the annotation quality of our corpus. The previous works on discourse analysis in bio-medical data have concentrated only on the identification of connectives and hence we have developed an end-end parser for connective and argument identification using Conditional Random Fields algorithm. The type and sub-type of the connective sense is also identified. The results obtained are encouraging.
Social media has emerged into a crucial resource for obtaining population-based signals for various public health monitoring and surveillance tasks, such as pharmacovigilance. There is an abundance of knowledge hidden within social media data, and the volume is growing. Drug-related chatter on social media can include user-generated information that can provide insights into public health problems such as abuse, adverse reactions, long-term effects, and multi-drug interactions. Our objective in this paper is to present to the biomedical natural language processing, data science, and public health communities data sets (annotated and unannotated), tools and resources that we have collected and created from social media. The data we present was collected from Twitter using the generic and brand names of drugs as keywords, along with their common misspellings. Following the collection of the data, annotation guidelines were created over several iterations, which detail important aspects of social media data annotation and can be used by future researchers for developing similar data sets. The annotation guidelines were followed to prepare data sets for text classification, information extraction and normalization. In this paper, we discuss the preparation of these guidelines, outline the data sets prepared, and present an overview of our state-of-the-art systems for data collection, supervised classification, and information extraction. In addition to the development of supervised systems for classification and extraction, we developed and released unlabeled data and language models. We discuss the potential uses of these language models in data mining and the large volumes of unlabeled data from which they were generated. We believe that the summaries and repositories we present here of our data, annotation guidelines, models, and tools will be beneficial to the research community as a single-point entry for all these resources, and will promote further research in this area.
Electronic Health Records (EHRs) are increasingly available in modern health care institutions either through the direct creation of electronic documents in hospitals{'} health information systems, or through the digitization of historical paper records. Each EHR creation method yields the need for sophisticated text reuse detection tools in order to prepare the EHR collections for efficient secondary use relying on Natural Language Processing methods. Herein, we address the detection of two types of text reuse in French EHRs: 1) the detection of updated versions of the same document and 2) the detection of document duplicates that still bear surface differences due to OCR or de-identification processing. We present a robust text reuse detection method to automatically identify redundant document pairs in two French EHR corpora that achieves an overall macro F-measure of 0.68 and 0.60, respectively and correctly identifies all redundant document pairs of interest.
An important subtask in clinical text mining tries to identify whether a clinical finding is expressed as present, absent or unsure in a text. This work presents a system for detecting mentions of clinical findings that are negated or just speculated. The system has been applied to two different types of German clinical texts: clinical notes and discharge summaries. Our approach is built on top of NegEx, a well known algorithm for identifying non-factive mentions of medical findings. In this work, we adjust a previous adaptation of NegEx to German and evaluate the system on our data to detect negation and speculation. The results are compared to a baseline algorithm and are analyzed for both types of clinical documents. Our system achieves an F1-Score above 0.9 on both types of reports.
Effective knowledge resources are critical for developing successful clinical decision support systems that alleviate the cognitive load on physicians in patient care. In this paper, we describe two new methods for building a knowledge resource of disease to medication associations. These methods use fundamentally different content and are based on advanced natural language processing and machine learning techniques. One method uses distributional semantics on large medical text, and the other uses data mining on a large number of patient records. The methods are evaluated using 25,379 unique disease-medication pairs extracted from 100 de-identified longitudinal patient records of a large multi-provider hospital system. We measured recall (R), precision (P), and F scores for positive and negative association prediction, along with coverage and accuracy. While individual methods performed well, a combined stacked classifier achieved the best performance, indicating the limitations and unique value of each resource and method. In predicting positive associations, the stacked combination significantly outperformed the baseline (a distant semi-supervised method on large medical text), achieving F scores of 0.75 versus 0.55 on the pairs seen in the patient records, and F scores of 0.69 and 0.35 on unique pairs.
Author name disambiguation (AND) in publication and citation resources is a well-known problem. Often, information about email address and other details in the affiliation is missing. In cases where such information is not available, identifying the authorship of publications becomes very challenging. Consequently, there have been attempts to resolve such cases by utilizing external resources as references. However, such external resources are heterogeneous and are not always reliable regarding the correctness of information. To solve the AND task, especially when information about an author is not complete we suggest the use of new features such as journal descriptors (JD) and semantic types (ST). The evaluation of different feature models shows that their inclusion has an impact equivalent to that of other important features such as email address. Using such features we show that our system outperforms the state of the art.
Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports the first topic model for sarcasm detection, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as {`}work{'}, {`}gun laws{'}, {`}weather{'} are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25{\%}.
In this paper, we aim at identifying uncertainty cues in Hungarian social media texts. We present our machine learning based uncertainty detector which is based on a rich features set including lexical, morphological, syntactic, semantic and discourse-based features, and we evaluate our system on a small set of manually annotated social media texts. We also carry out cross-domain and domain adaptation experiments using an annotated corpus of standard Hungarian texts and show that domain differences significantly affect machine learning. Furthermore, we argue that differences among uncertainty cue types may also affect the efficiency of uncertainty detection.
There has been extensive work on detecting the level of committed belief (also known as {``}factuality{''}) that an author is expressing towards the propositions in his or her utterances. Previous work on English has revealed that this can be done as a sequence tagging task. In this paper, we investigate the same task for Chinese and Spanish, two very different languages from English and from each other.
The utilization of social media material in journalistic workflows is increasing, demanding automated methods for the identification of mis- and disinformation. Since textual contradiction across social media posts can be a signal of rumorousness, we seek to model how claims in Twitter posts are being textually contradicted. We identify two different contexts in which contradiction emerges: its broader form can be observed across independently posted tweets and its more specific form in threaded conversations. We define how the two scenarios differ in terms of central elements of argumentation: claims and conversation structure. We design and evaluate models for the two scenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to represent claims and conversation structure implicitly in a generic inference model, while previous studies used explicit or no representation of these properties. To address noisy text, our classifiers use simple similarity features derived from the string and part-of-speech level. Corpus statistics reveal distribution differences for these features in contradictory as opposed to non-contradictory tweet relations, and the classifiers yield state of the art performance.
Negation and modality are two important grammatical phenomena that have attracted recent research attention as they can contribute to extra-propositional meaning aspects, among with factuality, attribution, irony and sarcasm. These aspects go beyond analysis such as semantic role labeling, and modeling them is important as a step towards a higher level of language understanding, which is needed for practical applications such as sentiment analysis. In this talk, I will go beyond English, and I will discuss how negation and modality are expressed in other languages. I will also go beyond sentiment analysis and I will present some challenges that the two phenomena pose for machine translation (MT). In particular, I will demonstrate how contemporary MT systems fail on them, and I will discuss some possible solutions.
This paper presents the main sources of disagreement found during the annotation of the Spanish SFU Review Corpus with negation (SFU ReviewSP -NEG). Negation detection is a challenge in most of the task related to NLP, so the availability of corpora annotated with this phenomenon is essential in order to advance in tasks related to this area. A thorough analysis of the problems found during the annotation could help in the study of this phenomenon.
This paper discusses the need for a dictionary of affixal negations and regular antonyms to facilitate their automatic detection in text. Without such a dictionary, affixal negations are very difficult to detect. In addition, we show that the set of affixal negations is not homogeneous, and that different NLP tasks may require different subsets. A dictionary can store the subtypes of affixal negations, making it possible to select a certain subset or to make inferences on the basis of these subtypes. We take a first step towards creating a negation dictionary by annotating all direct antonym pairs inWordNet using an existing typology of affixal negations. By highlighting some of the issues that were encountered in this annotation experiment, we hope to provide some insights into the necessary steps of building a negation dictionary.
Learning functional expressions is one of the difficulties for language learners, since functional expressions tend to have multiple meanings and complicated usages in various situations. In this paper, we report an experiment of simplifying example sentences of Japanese functional expressions especially for Chinese-speaking learners. For this purpose, we developed {``}Japanese Functional Expressions List{''} and {``}Simple Japanese Replacement List{''}. To evaluate the method, we conduct a small-scale experiment with Chinese-speaking learners on the effectiveness of the simplified example sentences. The experimental results indicate that simplified sentences are helpful in learning Japanese functional expressions.
In learning Asian languages, learners encounter the problem of character types that are different from those in their first language, for instance, between Chinese characters and the Latin alphabet. This problem also affects listening because learners reconstruct letters from speech sounds. Hence, special attention should be paid to listening practice for learners of Asian languages. However, to our knowledge, few studies have evaluated the ease of listening comprehension (listenability) in Asian languages. Therefore, as a pilot study of listenability in Asian languages, we developed a measurement method for learners of English in order to examine the discriminability of linguistic and learner features. The results showed that the accuracy of our method outperformed a simple majority vote, which suggests that a combination of linguistic and learner features should be used to measure listenability in Asian languages as well as in English.
We propose a new approach for extracting argument structure from natural language texts that contain an underlying argument. Our approach comprises of two phases: Score Assignment and Structure Prediction. The Score Assignment phase trains models to classify relations between argument units (Support, Attack or Neutral). To that end, different training strategies have been explored. We identify different linguistic and lexical features for training the classifiers. Through ablation study, we observe that our novel use of word-embedding features is most effective for this task. The Structure Prediction phase makes use of the scores from the Score Assignment phase to arrive at the optimal structure. We perform experiments on three argumentation datasets, namely, AraucariaDB, Debatepedia and Wikipedia. We also propose two baselines and observe that the proposed approach outperforms baseline systems for the final task of Structure Prediction.
We address the problem of automatic short answer grading, evaluating a collection of approaches inspired by recent advances in distributional text representations. In addition, we propose an unsupervised approach for determining text similarity using one-to-many alignment of word vectors. We evaluate the proposed technique across two datasets from different domains, namely, computer science and English reading comprehension, that additionally vary between highschool level and undergraduate students. Experiments demonstrate that the proposed technique often outperforms other compositional distributional semantics approaches as well as vector space methods such as latent semantic analysis. When combined with a scoring scheme, the proposed technique provides a powerful tool for tackling the complex problem of short answer grading. We also discuss a number of other key points worthy of consideration in preparing viable, easy-to-deploy automatic short-answer grading systems for the real-world.
Word embeddings are now ubiquitous forms of word representation in natural language processing. There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done. This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD. Our simplified method leads to comparable state-of-the-art performance without expensive retraining. Cross-Lingual WSD {--} where the word senses of a word in a source language come from a separate target translation language {--} can also assist in language learning; for example, when providing translations of target vocabulary for learners. Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking. We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well. We discuss the ramifications of this outcome.
In the process of learning and using Chinese, foreigners may have grammatical errors due to negative migration of their native languages. Currently, the computer-oriented automatic detection method of grammatical errors is not mature enough. Based on the evaluating task {---} CGED2016, we select and analyze the classification model and design feature extraction method to obtain grammatical errors including Mission(M), Disorder(W), Selection (S) and Redundant (R) automatically. The experiment results based on the dynamic corpus of HSK show that the Chinese grammatical error automatic detection method, which uses CRF as classification model and n-gram as feature extraction method. It is simple and efficient which play a positive effect on the research of Chinese grammatical error automatic detection and also a supporting and guiding role in the teaching of Chinese as a foreign language.
We offer a fluctuation smoothing computational approach for unsupervised automatic short answer grading (ASAG) techniques in the educational ecosystem. A major drawback of the existing techniques is the significant effect that variations in model answers could have on their performances. The proposed fluctuation smoothing approach, based on classical sequential pattern mining, exploits lexical overlap in students{'} answers to any typical question. We empirically demonstrate using multiple datasets that the proposed approach improves the overall performance and significantly reduces (up to 63{\%}) variation in performance (standard deviation) of unsupervised ASAG techniques. We bring in additional benchmarks such as (a) paraphrasing of model answers and (b) using answers by k top performing students as model answers, to amplify the benefits of the proposed approach.
This paper introduces Japanese lexical simplification. Japanese lexical simplification is the task of replacing difficult words in a given sentence to produce a new sentence with simple words without changing the original meaning of the sentence. We purpose a method of supervised regression learning to estimate difficulty ordering of words with statistical features obtained from two types of Japanese corpora. For the similarity of words, we use a Japanese thesaurus and dependency-based word embeddings. Evaluation of the proposed method is performed by comparing the difficulty ordering of the words.
Due to the huge population that speaks Spanish and Chinese, these languages occupy an important position in the language learning studies. Although there are some automatic translation systems that benefit the learning of both languages, there is enough space to create resources in order to help language learners. As a quick and effective resource that can give large amount language information, corpus-based learning is becoming more and more popular. In this paper we enrich a Spanish-Chinese parallel corpus automatically with part of-speech (POS) information and manually with discourse segmentation (following the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988)). Two search tools allow the Spanish-Chinese language learners to carry out different queries based on tokens and lemmas. The parallel corpus and the research tools are available to the academic community. We propose some examples to illustrate how learners can use the corpus to learn Spanish and Chinese.
We present a novel approach to Computer Assisted Language Learning (CALL), using deep syntactic parsers and semantic based machine translation (MT) in diagnosing and providing explicit feedback on language learners{'} errors. We are currently developing a proof of concept system showing how semantic-based machine translation can, in conjunction with robust computational grammars, be used to interact with students, better understand their language errors, and help students correct their grammar through a series of useful feedback messages and guided language drills. Ultimately, we aim to prove the viability of a new integrated rule-based MT approach to disambiguate students{'} intended meaning in a CALL system. This is a necessary step to provide accurate coaching on how to correct ungrammatical input, and it will allow us to overcome a current bottleneck in the field {---} an exponential burst of ambiguity caused by ambiguous lexical items (Flickinger, 2010). From the users{'} interaction with the system, we will also produce a richly annotated Learner Corpus, annotated automatically with both syntactic and semantic information.
This paper describes a corpus of nearly 10K French-Chinese aligned segments, produced by post-editing machine translated computer science courseware. This corpus was built from 2013 to 2016 within the PROJECT{\_}NAME project, by native Chinese students. The quality, as judged by native speakers, is ad-equate for understanding (far better than by reading only the original French) and for getting better marks. This corpus is annotated at segment-level by a self-assessed quality score. It has been directly used as supplemental training data to build a statistical machine translation system dedicated to that sublanguage, and can be used to extract the specific bilingual terminology. To our knowledge, it is the first corpus of this kind to be released.
Much research in education has been done on the study of different language teaching methods. However, there has been little investigation using computational analysis to compare such methods in terms of readability or complexity progression. In this paper, we make use of existing readability scoring techniques and our own classifiers to analyze the textbooks used in two very different teaching methods for English as a Second Language {--} the grammar-based and the communicative methods. Our analysis indicates that the grammar-based curriculum shows a more coherent readability progression compared to the communicative curriculum. This finding corroborates with the expectations about the differences between these two methods and validates our approach{'}s value in comparing different teaching methods quantitatively.
Grammatical error diagnosis is an essential part in a language-learning tutoring system. Based on the data sets of Chinese grammar error detection tasks, we proposed a system which measures the likelihood of correction candidates generated by deleting or inserting characters or words, moving substrings to different positions, substituting prepositions with other prepositions, or substituting words with their synonyms or similar strings. Sentence likelihood is measured based on the frequencies of substrings from the space-removed version of Google n-grams. The evaluation on the training set shows that Missing-related and Selection-related candidate generation methods have promising performance. Our final system achieved a precision of 30.28{\%} and a recall of 62.85{\%} in the identification level evaluated on the test set.
Mandarin is not simple language for foreigner. Even using Mandarin as the mother tongue, they have to spend more time to learn when they were child. The following issues are the reason why causes learning problem. First, the word is envolved by Hieroglyphic. So a character can express meanings independently, but become a word has another semantic. Second, the Mandarin{'}s grammars have flexible rule and special usage. Therefore, the common grammatical errors can classify to missing, redundant, selection and disorder. In this paper, we proposed the structure of the Recurrent Neural Networks using Long Short-term memory (RNN-LSTM). It can detect the error type from the foreign learner writing. The features based on the word vector and part-of-speech vector. In the test data found that our method in the detection level of recall better than the others, even as high as 0.9755. That is because we give the possibility of greater choice in detecting errors.
Computational approaches for dialectometry employed Levenshtein distance to compute an aggregate similarity between two dialects belonging to a single language group. In this paper, we apply a sequence-to-sequence autoencoder to learn a deep representation for words that can be used for meaningful comparison across dialects. In contrast to the alignment-based methods, our method does not require explicit alignments. We apply our architectures to three different datasets and show that the learned representations indicate highly similar results with the analyses based on Levenshtein distance and capture the traditional dialectal differences shown by dialectologists.
We recently witnessed an exponential growth in dialectal Arabic usage in both textual data and speech recordings especially in social media. Processing such media is of great utility for all kinds of applications ranging from information extraction to social media analytics for political and commercial purposes to building decision support systems. Compared to other languages, Arabic, especially the informal variety, poses a significant challenge to natural language processing algorithms since it comprises multiple dialects, linguistic code switching, and a lack of standardized orthographies, to top its relatively complex morphology. Inherently, the problem of processing Arabic in the context of social media is the problem of how to handle resource poor languages. In this talk I will go over some of our insights to some of these problems and show how there is a silver lining where we can generalize some of our solutions to other low resource language contexts.
Machine translation between closely related languages is less challenging and exibits a smaller number of translation errors than translation between distant languages, but there are still obstacles which should be addressed in order to improve such systems. This work explores the obstacles for machine translation systems between closely related South Slavic languages, namely Croatian, Serbian and Slovenian. Statistical systems for all language pairs and translation directions are trained using parallel texts from different domains, however mainly on spoken language i.e. subtitles. For translation between Serbian and Croatian, a rule-based system is also explored. It is shown that for all language pairs and translation systems, the main obstacles are differences between structural properties.
The identification of the language of text/speech input is the first step to be able to properly do any language-dependent natural language processing. The task is called Automatic Language Identification (ALI). Being a well-studied field since early 1960{'}s, various methods have been applied to many standard languages. The ALI standard methods require datasets for training and use character/word-based n-gram models. However, social media and new technologies have contributed to the rise of informal and minority languages on the Web. The state-of-the-art automatic language identifiers fail to properly identify many of them. Romanized Arabic (RA) and Romanized Berber (RB) are cases of these informal languages which are under-resourced. The goal of this paper is twofold: detect RA and RB, at a document level, as separate languages and distinguish between them as they coexist in North Africa. We consider the task as a classification problem and use supervised machine learning to solve it. For both languages, character-based 5-grams combined with additional lexicons score the best, F-score of 99.75{\%} and 97.77{\%} for RB and RA respectively.
One of the purposes of the VarDial workshop series is to encourage research into NLP methods that treat human languages as a continuum, by designing models that exploit the similarities between languages and variants. In my work, I am using a continuous vector representation of languages that allows modeling and exploring the language continuum in a very direct way. The basic tool for this is a character-based recurrent neural network language model conditioned on language vectors whose values are learned during training. By feeding the model Bible translations in a thousand languages, not only does the learned vector space capture language similarity, but by interpolating between the learned vectors it is possible to generate text in unattested intermediate forms between the training languages.
Automatic Language Identification (ALI) is the detection of the natural language of an input text by a machine. It is the first necessary step to do any language-dependent natural language processing task. Various methods have been successfully applied to a wide range of languages, and the state-of-the-art automatic language identifiers are mainly based on character n-gram models trained on huge corpora. However, there are many languages which are not yet automatically processed, for instance minority and informal languages. Many of these languages are only spoken and do not exist in a written format. Social media platforms and new technologies have facilitated the emergence of written format for these spoken languages based on pronunciation. The latter are not well represented on the Web, commonly referred to as under-resourced languages, and the current available ALI tools fail to properly recognize them. In this paper, we revisit the problem of ALI with the focus on Arabicized Berber and dialectal Arabic short texts. We introduce new resources and evaluate the existing methods. The results show that machine learning models combined with lexicons are well suited for detecting Arabicized Berber and different Arabic varieties and distinguishing between them, giving a macro-average F-score of 92.94{\%}.
We present an approach for automatic verification and augmentation of multilingual lexica. We exploit existing parallel and monolingual corpora to extract multilingual correspondents via tri-angulation. We demonstrate the efficacy of our approach on two publicly available resources: Tharwa, a three-way lexicon comprising Dialectal Arabic, Modern Standard Arabic and English lemmas among other information (Diab et al., 2014); and BabelNet, a multilingual thesaurus comprising over 276 languages including Arabic variant entries (Navigli and Ponzetto, 2012). Our automated approach yields an F1-score of 71.71{\%} in generating correct multilingual correspondents against gold Tharwa, and 54.46{\%} against gold BabelNet without any human intervention.
A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.
In this study we apply classification methods for detecting subdialectal differences in Sorani Kurdish texts produced in different regions, namely Iran and Iraq. As Sorani is a low-resource language, no corpus including texts from different regions was readily available. To this end, we identified data sources that could be leveraged for this task to create a dataset of 200,000 sentences. Using surface features, we attempted to classify Sorani subdialects, showing that sentences from news sources in Iraq and Iran are distinguishable with 96{\%} accuracy. This is the first preliminary study for a dialect that has not been widely studied in computational linguistics, evidencing the possible existence of distinct subdialects.
Massive Open Online Courses have been growing rapidly in size and impact. Yet the language barrier constitutes a major growth impediment in reaching out all people and educating all citizens. A vast majority of educational material is available only in English, and state-of-the-art machine translation systems still have not been tailored for this peculiar genre. In addition, a mere collection of appropriate in-domain training material is a challenging task. In this work, we investigate statistical machine translation of lecture subtitles from English into Croatian, which is morphologically rich and generally weakly supported, especially for the educational domain. We show that results comparable with publicly available systems trained on much larger data can be achieved if a small in-domain training set is used in combination with additional in-domain corpus originating from the closely related Serbian language.
Discriminating between closely-related language varieties is considered a challenging and important task. This paper describes our submission to the DSL 2016 shared-task, which included two sub-tasks: one on discriminating similar languages and one on identifying Arabic dialects. We developed a character-level neural network for this task. Given a sequence of characters, our model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. We primarily focused on the Arabic dialect identification task and obtained an F1 score of 0.4834, ranking 6th out of 18 participants. We also analyze errors made by our system on the Arabic data in some detail, and point to challenges such an approach is faced with.
The paper describes the QCRI submissions to the task of automatic Arabic dialect classification into 5 Arabic variants, namely Egyptian, Gulf, Levantine, North-African, and Modern Standard Arabic (MSA). The training data is relatively small and is automatically generated from an ASR system. To avoid over-fitting on such small data, we carefully selected and designed the features to capture the morphological essence of the different dialects. We submitted four runs to the Arabic sub-task. For all runs, we used a combined feature vector of character bi-grams, tri-grams, 4-grams, and 5-grams. We tried several machine-learning algorithms, namely Logistic Regression, Naive Bayes, Neural Networks, and Support Vector Machines (SVM) with linear and string kernels. However, our submitted runs used SVM with a linear kernel. In the closed submission, we got the best accuracy of 0.5136 and the third best weighted F1 score, with a difference less than 0.002 from the highest score.
Cancer (a.k.a neoplasms in a broader sense) is one of the leading causes of death worldwide and its incidence is expected to exacerbate. To respond to the critical need from the society, there have been rigorous attempts for the cancer research community to develop treatment for cancer. Accordingly, we observe a surge in the sheer volume of research products and outcomes in relation to neoplasms. In this talk, we introduce the notion of entitymetrics to provide a new lens for understanding the impact, trend, and diffusion of knowledge associated with neoplasms research. To this end, we collected over two million records from PubMed, the most popular search engine in the medical domain. Coupled with text mining techniques including named entity recognition, sentence boundary detection, string approximate matching, entitymetrics enables us to analyze knowledge diffusion, impact, and trend at various knowledge entity units, such as bio-entity, organization, and country. At the end of the talk, the future applications and possible directions of entitymetrics will be discussed.
The present paper explores a novel method that integrates efficient distributed representations with terminology extraction. We show that the information from a small number of observed instances can be combined with local and global word embeddings to remarkably improve the term extraction results on unigram terms. To do so we pass the terms extracted by other tools to a filter made of the local-global embeddings and a classifier which in turn decides whether or not a term candidate is a term. The filter can also be used as a hub to merge different term extraction tools into a single higher-performing system. We compare filters that use the skip-gram architecture and filters that employ the CBOW architecture for the task at hand.
In the paper, we address the problem of recognition of non-domain phrases in terminology lists obtained with an automatic term extraction tool. We focus on identification of multi-word phrases that are general terms and discourse function expressions. We tested several methods based on domain corpora comparison and a method based on contexts of phrases identified in a large corpus of general language. We compared the results of the methods to manual annotation. The results show that the task is quite hard as the inter-annotator agreement is low. Several tested methods achieved similar overall results, although the phrase ordering varied between methods. The most successful method with the precision about 0.75 at the half of the tested list was the context based method using a modified contextual diversity coefficient.
This article presents a domain-driven algorithm for the task of term sense disambiguation (TSD). TSD aims at automatically choosing which term record from a term bank best represents the meaning of a term occurring in a particular context. In a translation environment, finding the contextually appropriate term record is necessary to access the proper equivalent to be used in the target language text. The term bank TERMIUM Plus, recently published as an open access repository, is chosen as a domain-rich resource for testing our TSD algorithm, using English and French as source and target languages. We devise an experiment using over 1300 English terms found in scientific articles, and show that our domain-driven TSD algorithm is able to bring the best term record, and therefore the best French equivalent, at the average rank of 1.69 compared to a baseline random rank of 3.51.
In this paper, we propose a method of augmenting existing bilingual terminologies. Our method belongs to a {``}generate and validate{''} framework rather than extraction from corpora. Although many studies have proposed methods to find term translations or to augment terminology within a {``}generate and validate{''} framework, few has taken full advantage of the systematic nature of terminologies. A terminology of a domain represents the conceptual system of the domain fairly systematically, and we contend that making use of the systematicity fully will greatly contribute to the effective augmentation of terminologies. This paper proposes and evaluates a novel method to generate bilingual term candidates by using existing terminologies and delving into their systematicity. Experiments have shown that our method can generate much better term candidate pairs than the existing method and give improved performance for terminology augmentation.
The extraction of data exemplifying relations between terms can make use, at least to a large extent, of techniques that are similar to those used in standard hybrid term candidate extraction, namely basic corpus analysis tools (e.g. tagging, lemmatization, parsing), as well as morphological analysis of complex words (compounds and derived items). In this article, we discuss the use of such techniques for the extraction of raw material for a description of relations between terms, and we provide internal evaluation data for the devices developed. We claim that user-generated content is a rich source of term variation through paraphrasing and reformulation, and that these provide relational data at the same time as term variants. Germanic languages with their rich word formation morphology may be particularly good candidates for the approach advocated here.
We investigate how both model-related factors and application-related factors affect the accuracy of distributional semantic models (DSMs) in the context of specialized lexicography, and how these factors interact. This holistic approach to the evaluation of DSMs provides valuable guidelines for the use of these models and insight into the kind of semantic information they capture.
We propose and evaluate a method for identifying co-hyponym lexical units in a terminological resource. The principles of term recognition and distributional semantics are combined to extract terms from a similar category of concept. Given a set of candidate terms, random projections are employed to represent them as low-dimensional vectors. These vectors are derived automatically from the frequency of the co-occurrences of the candidate terms and words that appear within windows of text in their proximity (context-windows). In a $k$-nearest neighbours framework, these vectors are classified using a small set of manually annotated terms which exemplify concept categories. We then investigate the interplay between the size of the corpus that is used for collecting the co-occurrences and a number of factors that play roles in the performance of the proposed method: the configuration of context-windows for collecting co-occurrences, the selection of neighbourhood size ($k$), and the choice of similarity metric.
Despite advances in computer technology, terminologists still tend to rely on manual work to extract all the semantic information that they need for the description of specialized concepts. In this paper we propose the creation of new word sketches in Sketch Engine for the extraction of semantic relations. Following a pattern-based approach, new sketch grammars are devel-oped in order to extract some of the most common semantic relations used in the field of ter-minology: generic-specific, part-whole, location, cause and function.
This paper presents the construction and evaluation of Japanese and English controlled bilingual terminologies that are particularly intended for controlled authoring and machine translation with special reference to the Japanese municipal domain. Our terminologies are constructed by extracting terms from municipal website texts, and the term variations are controlled by defining preferred and proscribed terms for both the source Japanese and the target English. To assess the coverage of the terms/concepts in the municipal domain and validate the quality of the control, we employ a quantitative extrapolation method that estimates the potential vocabulary size. Using Large-Number-of-Rare-Event (LNRE) modelling, we compare two parameters: (1) uncontrolled and controlled and (2) Japanese and English. The results show that our terminologies currently cover about 45{--}65{\%} of the terms and 50{--}65{\%} of the concepts in the municipal domain, and are well controlled. The detailed analysis of growth patterns of terminologies also provides insight into the extent to which we can enlarge the terminologies within the realistic range.
By its own nature, the Natural Language Processing (NLP) community is a priori the best equipped to study the evolution of its own publications, but works in this direction are rare and only recently have we seen a few attempts at charting the field. In this paper, we use the algorithms, resources, standards, tools and common practices of the NLP field to build a list of terms characteristic of ongoing research, by mining a large corpus of scientific publications, aiming at the largest possible exhaustivity and covering the largest possible time span. Study of the evolution of this term list through time reveals interesting insights on the dynamics of field and the availability of the term database and of the corpus (for a large part) make possible many further comparative studies in addition to providing a test field for a new graphic interface designed to perform visual time analytics of large sized thesauri.
Annotating medical text such as clinical notes with human phenotype descriptors is an important task that can, for example, assist in building patient profiles. To automatically annotate text one usually needs a dictionary of predefined terms. However, do to the variety of human expressiveness, current state-of-the art phenotype concept recognizers and automatic annotators struggle with specific domain issues and challenges. In this paper we present results of an-notating gold standard corpus with a dictionary containing lexical variants for the Human Phenotype Ontology terms. The main purpose of the dictionary is to improve the recall of phenotype concept recognition systems. We compare the method with four other approaches and present results.
We propose a semi-automatic method for the acquisition of specialised ontological and terminological knowledge. An ontology and a terminology are automatically built from domain experts{'} annotations. The ontology formalizes the common and shared conceptual vocabulary of those experts. Its associated terminology defines a glossary linking annotated terms to their semantic categories. These two resources evolve incrementally and are used for an automatic annotation of a new corpus at each iteration. The annotated corpus concerns the evaluation of French higher education and science institutions.
With many hospitals digitalizing clinical records it has opened opportunities for researchers in NLP, Machine Learning to apply techniques for extracting meaning and make actionable insights. There has been previous attempts in mapping free text to medical nomenclature like UMLS, SNOMED. However, in this paper, we had analyzed diagnosis in clinical reports using ICD10 to achieve a lightweight, real-time predictions by introducing concepts like WordInfo, root word identification. We were able to achieve 68.3{\%} accuracy over clinical records collected from qualified clinicians. Our study would further help the healthcare institutes in organizing their clinical reports based on ICD10 mappings and derive numerous insights to achieve operational efficiency and better medical care.
Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In NMTs, words that are out of vocabulary are represented by a single unknown token. In this paper, we propose a method that enables NMT to translate patent sentences comprising a large vocabulary of technical terms. We train an NMT system on bilingual data wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Further, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using SMT. We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens. Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.
Concomitant with the globalization of food culture, demand for the recipes of specialty dishes has been increasing. The recent growth in recipe sharing websites and food blogs has resulted in numerous recipe texts being available for diverse foods in various languages. However, little work has been done on machine translation of recipe texts. In this paper, we address the task of translating recipes and investigate the advantages and disadvantages of traditional phrase-based statistical machine translation and more recent neural machine translation. Specifically, we translate Japanese recipes into English, analyze errors in the translated recipes, and discuss available room for improvements.
This paper describes our UT-KAY system that participated in the Workshop on Asian Translation 2016. Based on an Attention-based Neural Machine Translation (ANMT) model, we build our system by incorporating a domain adaptation method for multiple domains and an attention-based unknown word replacement method. In experiments, we verify that the attention-based unknown word replacement method is effective in improving translation scores in Chinese-to-Japanese machine translation. We further show results of manual analysis on the replaced unknown words.
When translating formal documents, capturing the sentence structure specific to the sublanguage is extremely necessary to obtain high-quality translations. This paper proposes a novel global reordering method with particular focus on long-distance reordering for capturing the global sentence structure of a sublanguage. The proposed method learns global reordering models from a non-annotated parallel corpus and works in conjunction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations.
This paper presents an improved lexicalized reordering model for phrase-based statistical machine translation using a deep neural network. Lexicalized reordering suffers from reordering ambiguity, data sparseness and noises in a phrase table. Previous neural reordering model is successful to solve the first and second problems but fails to address the third one. Therefore, we propose new features using phrase translation and word alignment to construct phrase vectors to handle inherently noisy phrase translation pairs. The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments.
This paper presents our machine translation system that developed for the WAT2016 evalua-tion tasks of ja-en, ja-zh, en-ja, zh-ja, JPCja-en, JPCja-zh, JPCen-ja, JPCzh-ja. We build our system based on encoder{--}decoder framework by integrating recurrent neural network (RNN) and gate recurrent unit (GRU), and we also adopt an attention mechanism for solving the problem of information loss. Additionally, we propose a simple translation-specific approach to resolve the unknown word translation problem. Experimental results show that our system performs better than the baseline statistical machine translation (SMT) systems in each task. Moreover, it shows that our proposed approach of unknown word translation performs effec-tively improvement of translation results.
System architecture, experimental settings and experimental results of the group for the WAT2016 tasks are described. We participate in six tasks: en-ja, zh-ja, JPCzh-ja, JPCko-ja, HINDENen-hi and HINDENhi-ja. Although the basic architecture of our sys-tems is PBSMT with reordering, several techniques are conducted. Especially, the system for the HINDENhi-ja task with pivoting by English uses the reordering technique. Be-cause Hindi and Japanese are both OV type languages and English is a VO type language, we can use reordering technique to the pivot language. We can improve BLEU score from 7.47 to 7.66 by the reordering technique for the sentence level pivoting of this task.
This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task.
This paper describes the NICT-2 translation system for the 3rd Workshop on Asian Translation. The proposed system employs a domain adaptation method based on feature augmentation. We regarded the Japan Patent Office Corpus as a mixture of four domain corpora and improved the translation quality of each domain. In addition, we incorporated language models constructed from Google n-grams as external knowledge. Our domain adaptation method can naturally incorporate such external knowledge that contributes to translation quality.
We participate in scientific paper subtask (ASPEC-EJ/CJ) and patent subtask (JPC-EJ/CJ/KJ) with phrase-based SMT systems which are trained with its own patent corpora. Using larger corpora than those prepared by the workshop organizer, we achieved higher BLEU scores than most participants in EJ and CJ translations of patent subtask, but in crowdsourcing evaluation, our EJ translation, which is best in all automatic evaluations, received a very poor score. In scientific paper subtask, our translations are given lower scores than most translations that are produced by translation engines trained with the in-domain corpora. But our scores are higher than those of general-purpose RBMTs and online services. Considering the result of crowdsourcing evaluation, it shows a possibility that CJ SMT system trained with a large patent corpus translates non-patent technical documents at a practical level.
Simultaneous interpretation is a very challenging application of machine translation in which the input is a stream of words from a speech recognition engine. The key problem is how to segment the stream in an online manner into units suitable for translation. The segmentation process proceeds by calculating a confidence score for each word that indicates the soundness of placing a sentence boundary after it, and then heuristics are employed to determine the position of the boundaries. Multiple variants of the confidence scoring method and segmentation heuristics were studied. Experimental results show that the best performing strategy is not only efficient in terms of average latency per word, but also achieved end-to-end translation quality close to an offline baseline, and close to oracle segmentation.
This paper illustrates the similarity between Thai and Laotian, and between Malay and Indonesian, based on an investigation on raw parallel data from Asian Language Treebank. The cross-lingual similarity is investigated and demonstrated on metrics of correspondence and order of tokens, based on several standard statistical machine translation techniques. The similarity shown in this study suggests a possibility on harmonious annotation and processing of the language pairs in future development.
We propose a method for integrating Japanese empty category detection into the preordering process of Japanese-to-English statistical machine translation. First, we apply machine-learning-based empty category detection to estimate the position and the type of empty categories in the constituent tree of the source sentence. Then, we apply discriminative preordering to the augmented constituent tree in which empty categories are treated as if they are normal lexical symbols. We find that it is effective to filter empty categories based on the confidence of estimation. Our experiments show that, for the IWSLT dataset consisting of short travel conversations, the insertion of empty categories alone improves the BLEU score from 33.2 to 34.3 and the RIBES score from 76.3 to 78.7, which imply that reordering has improved For the KFTT dataset consisting of Wikipedia sentences, the proposed preordering method considering empty categories improves the BLEU score from 19.9 to 20.2 and the RIBES score from 66.2 to 66.3, which shows both translation and reordering have improved slightly.
We describe here our approaches and results on the WAT 2016 shared translation tasks. We tried to use both an example-based machine translation (MT) system and a neural MT system. We report very good translation results, especially when using neural MT for Chinese-to-Japanese translation.
This paper reports our systems (UT-AKY) submitted in the 3rd Workshop of Asian Translation 2016 (WAT{'}16) and their results in the English-to-Japanese translation task. Our model is based on the tree-to-sequence Attention-based NMT (ANMT) model proposed by Eriguchi et al. (2016). We submitted two ANMT systems: one with a word-based decoder and the other with a character-based decoder. Experimenting on the English-to-Japanese translation task, we have confirmed that the character-based decoder can cover almost the full vocabulary in the target language and generate translations much faster than the word-based model.
This paper describes the SENSE machine translation system participation in the Third Workshop for Asian Translation (WAT2016). We share our best practices to build a fast and light phrase-based machine translation (PBMT) models that have comparable results to the baseline systems provided by the organizers. As Neural Machine Translation (NMT) overtakes PBMT as the state-of-the-art, deep learning and new MT practitioners might not be familiar with the PBMT paradigm and we hope that this paper will help them build a PBMT baseline system quickly and easily.
Unlike European languages, many Asian languages like Chinese and Japanese do not have typographic boundaries in written system. Word segmentation (tokenization) that break sentences down into individual words (tokens) is normally treated as the first step for machine translation (MT). For Chinese and Japanese, different rules and segmentation tools lead different segmentation results in different level of granularity between Chinese and Japanese. To improve the translation accuracy, we adjust and balance the granularity of segmentation results around terms for Chinese{--}Japanese patent corpus for training translation model. In this paper, we describe a statistical machine translation (SMT) system which is built on re-tokenized Chinese-Japanese patent training corpus using extracted bilingual multi-word terms.
In machine translation, we must consider the difference in expression between languages. For example, the active/passive voice may change in Japanese-English translation. The same verb in Japanese may be translated into different voices at each translation because the voice of a generated sentence cannot be determined using only the information of the Japanese sentence. Machine translation systems should consider the information structure to improve the coherence of the output by using several topicalization techniques such as passivization. Therefore, this paper reports on our attempt to control the voice of the sentence generated by an encoder-decoder model. To control the voice of the generated sentence, we added the voice information of the target sentence to the source sentence during the training. We then generated sentences with a specified voice by appending the voice information to the source sentence. We observed experimentally whether the voice could be controlled. The results showed that, we could control the voice of the generated sentence with 85.0{\%} accuracy on average. In the evaluation of Japanese-English translation, we obtained a 0.73-point improvement in BLEU score by using gold voice labels.
This paper presents our Chinese-to-Japanese patent machine translation system for WAT 2016 (Group ID: ntt) that uses syntactic pre-ordering over Chinese dependency structures. Chinese words are reordered by a learning-to-rank model based on pairwise classification to obtain word order close to Japanese. In this year{'}s system, two different machine translation methods are compared: traditional phrase-based statistical machine translation and recent sequence-to-sequence neural machine translation with an attention mechanism. Our pre-ordering showed a significant improvement over the phrase-based baseline, but, in contrast, it degraded the neural machine translation baseline.
In this paper we describe the system that we develop as part of our participation in WAT 2016. We develop a system based on hierarchical phrase-based SMT for English to Hindi language pair. We perform re-ordering and augment bilingual dictionary to improve the performance. As a baseline we use a phrase-based SMT model. The MT models are fine-tuned on the development set, and the best configurations are used to report the evaluation on the test set. Experiments show the BLEU of 13.71 on the benchmark test data. This is better compared to the official baseline BLEU score of 10.79.
To enhance Neural Machine Translation models, several obvious ways such as enlarging the hidden size of recurrent layers and stacking multiple layers of RNN can be considered. Surprisingly, we observe that using naively stacked RNNs in the decoder slows down the training and leads to degradation in performance. In this paper, We demonstrate that applying residual connections in the depth of stacked RNNs can help the optimization, which is referred to as residual stacking. In empirical evaluation, residual stacking of decoder RNNs gives superior results compared to other methods of enhancing the model with a fixed parameter budget. Our submitted systems in WAT2016 are based on a NMT model ensemble with residual stacking in the decoder. To further improve the performance, we also attempt various methods of system combination in our experiments.
fast align is a simple and fast word alignment tool which is widely used in state-of-the-art machine translation systems. It yields comparable results in the end-to-end translation experiments of various language pairs. However, fast align does not perform as well as GIZA++ when applied to language pairs with distinct word orders, like English and Japanese. In this paper, given the lexical translation table output by fast align, we propose to realign words using the hierarchical sub-sentential alignment approach. Experimental results show that simple additional processing improves the performance of word alignment, which is measured by counting alignment matches in comparison with fast align. We also report the result of final machine translation in both English-Japanese and Japanese-English. We show our best system provided significant improvements over the baseline as measured by BLEU and RIBES.
This paper presents the comparison of how using different neural network based language modeling tools for selecting the best candidate fragments affects the final output translation quality in a hybrid multi-system machine translation setup. Experiments were conducted by comparing perplexity and BLEU scores on common test cases using the same training data set. A 12-gram statistical language model was selected as a baseline to oppose three neural network based models of different characteristics. The models were integrated in a hybrid system that depends on the perplexity score of a sentence fragment to produce the best fitting translations. The results show a correlation between language model perplexity and BLEU scores as well as overall improvements in BLEU.
We present a novel method of comparable corpora construction. Unlike the traditional methods which heavily rely on linguistic features, our method only takes image similarity into consid-eration. We use an image-image search engine to obtain similar images, together with the cap-tions in source language and target language. On the basis, we utilize captions of similar imag-es to construct sentence-level bilingual corpora. Experiments on 10,371 target captions show that our method achieves a precision of 0.85 in the top search results.
We present an algorithm for predicting translation equivalents between two languages, based on the corresponding WordNets. The assumption is that all synsets of one of the languages are linked to the corresponding synsets in the other language. In theory, given the exact sense of a word in a context it must be possible to translate it as any of the words in the linked synset. In practice, however, this does not work well since automatic and accurate sense disambiguation is difficult. Instead it is possible to define a more robust translation relation between the lexemes of the two languages. As far as we know the Finnish WordNet is the only one that includes that relation. Our algorithm can be used to predict the relation for other languages as well. This is useful for instance in hybrid machine translation systems which are usually more dependent on high-quality translation dictionaries.
Traditional machine translation evaluation metrics such as BLEU and WER have been widely used, but these metrics have poor correlations with human judgements because they badly represent word similarity and impose strict identity matching. In this paper, we propose some modifications to the traditional measures based on word embeddings for these two metrics. The evaluation results show that our modifications significantly improve their correlation with human judgements.
We describe experiments in Machine Translation using word sense disambiguation (WSD) information. This work focuses on WSD in verbs, based on two different approaches {--} verbal patterns based on corpus pattern analysis and verbal word senses from valency frames. We evaluate several options of using verb senses in the source-language sentences as an additional factor for the Moses statistical machine translation system. Our results show a statistically significant translation quality improvement in terms of the BLEU metric for the valency frames approach, but in manual evaluation, both WSD methods bring improvements.
We introduce a new statistical machine translation approach specifically geared to learning translation from low resource languages, that exploits monolingual English semantic parsing to bias inversion transduction grammar (ITG) induction. We show that in contrast to conventional statistical machine translation (SMT) training methods, which rely heavily on phrase memorization, our approach focuses on learning bilingual correlations that help translating low resource languages, by using the output language semantic structure to further narrow down ITG constraints. This approach is motivated by previous research which has shown that injecting a semantic frame based objective function while training SMT models improves the translation quality. We show that including a monolingual semantic objective function during the learning of the translation model leads towards a semantically driven alignment which is more efficient than simply tuning loglinear mixture weights against a semantic frame based evaluation metric in the final stage of statistical machine translation training. We test our approach with three different language pairs and demonstrate that our model biases the learning towards more semantically correct alignments. Both GIZA++ and ITG based techniques fail to capture meaningful bilingual constituents, which is required when trying to learn translation models for low resource languages. In contrast, our proposed model not only improve translation by injecting a monolingual objective function to learn bilingual correlations during early training of the translation model, but also helps to learn more meaningful correlations with a relatively small data set, leading to a better alignment compared to either conventional ITG or traditional GIZA++ based approaches.
We defend that bilingual lexicons automatically extracted from parallel corpora, whose entries have been meanwhile validated by linguists and classified as correct or incorrect, should constitute a specific parallel corpora. And, in this paper, we propose to use word-to-word translations to learn morph-units (comprising of bilingual stems and suffixes) from those bilingual lexicons for two language pairs L1-L2 and L1-L3 to induce a bilingual lexicon for the language pair L2-L3, apart from also learning morph-units for this other language pair. The applicability of bilingual morph-units in L1-L2 and L1-L3 is examined from the perspective of pivot-based lexicon induction for language pair L2-L3 with L1 as bridge. While the lexicon is derived by transitivity, the correspondences are identified based on previously learnt bilingual stems and suffixes rather than surface translation forms. The induced pairs are validated using a binary classifier trained on morphological and similarity-based features using an existing, automatically acquired, manually validated bilingual translation lexicon for language pair L2-L3. In this paper, we discuss the use of English (EN)-French (FR) and English (EN)-Portuguese (PT) lexicon of word-to-word translations in generating word-to-word translations for the language pair FR-PT with EN as pivot language. Generated translations are filtered out first using an SVM-based FR-PT classifier and then are manually validated.
This paper proposes a new idea that uses Wikipedia categories as answer types and defines candidate sets inside Wikipedia. The focus of a given question is searched in the hierarchy of Wikipedia main pages. Our searching strategy combines head-noun matching and synonym matching provided in semantic resources. The set of answer candidates is determined by the entry hierarchy in Wikipedia and the hyponymy hierarchy in WordNet. The experimental results show that the approach can find candidate sets in a smaller size but achieve better performance especially for ARTIFACT and ORGANIZATION types, where the performance is better than state-of-the-art Chinese factoid QA systems.
Commonsense knowledge is essential for fully understanding language in many situations. We acquire large-scale commonsense knowledge from humans using a game with a purpose (GWAP) developed on a smartphone spoken dialogue system. We transform the manual knowledge acquisition process into an enjoyable quiz game and have collected over 150,000 unique commonsense facts by gathering the data of more than 70,000 players over eight months. In this paper, we present a simple method for maintaining the quality of acquired knowledge and an empirical analysis of the knowledge acquisition process. To the best of our knowledge, this is the first work to collect large-scale knowledge via a GWAP on a widely-used spoken dialogue system.
This paper describes a hierarchical neural network we propose for sentence classification to extract product information from product documents. The network classifies each sentence in a document into attribute and condition classes on the basis of word sequences and sentence sequences in the document. Experimental results showed the method using the proposed network significantly outperformed baseline methods by taking semantic representation of word and sentence sequential data into account. We also evaluated the network with two different product domains (insurance and tourism domains) and found that it was effective for both the domains.
An Entity-based approach to Answering recurrent and non-recurrent questions with Past Answers Abstract Community question answering (CQA) systems such as Yahoo! Answers allow registered-users to ask and answer questions in various question categories. However, a significant percentage of asked questions in Yahoo! Answers are unanswered. In this paper, we propose to reduce this percentage by reusing answers to past resolved questions from the site. Specifically, we propose to satisfy unanswered questions in entity rich categories by searching for and reusing the best answers to past resolved questions with shared needs. For unanswered questions that do not have a past resolved question with a shared need, we propose to use the best answer to a past resolved question with similar needs. Our experiments on a Yahoo! Answers dataset shows that our approach retrieves most of the past resolved questions that have shared and similar needs to unanswered questions.
In an era where highly accurate Question Answering (QA) systems are being built using complex Natural Language Processing (NLP) and Information Retrieval (IR) algorithms, presenting the acquired answer to the user akin to a human answer is also crucial. In this paper we present an answer presentation strategy by embedding the answer in a sentence which is developed by incorporating the linguistic structure of the source question extracted through typed dependency parsing. The evaluation using human participants proved that the methodology is human-competitive and can result in linguistically correct sentences for more that 70{\%} of the test dataset acquired from QALD question dataset.
Question answering (QA) systems need to provide exact answers for the questions that are posed to the system. However, this can only be achieved through a precise processing of the question. During this procedure, one important step is the detection of the expected type of answer that the system should provide by extracting the headword of the questions and identifying its semantic type. We have annotated the headword and assigned UMLS semantic types to 643 factoid/list questions from the BioASQ training data. We present statistics on the corpus and a preliminary evaluation in baseline experiments. We also discuss the challenges on both the manual annotation and the automatic detection of the headwords and the semantic types. We believe that this is a valuable resource for both training and evaluation of biomedical QA systems. The corpus is available at: \url{https://github.com/mariananeves/BioMedLAT}.
The paper describes topic shifting in dialogues with a robot that provides information from Wiki-pedia. The work focuses on a double topical construction of dialogue coherence which refers to discourse coherence on two levels: the evolution of dialogue topics via the interaction between the user and the robot system, and the creation of discourse topics via the content of the Wiki-pedia article itself. The user selects topics that are of interest to her, and the system builds a list of potential topics, anticipated to be the next topic, by the links in the article and by the keywords extracted from the article. The described system deals with Wikipedia articles, but could easily be adapted to other digital information providing systems.
Building accurate knowledge graphs is essential for question answering system. We suggest a crowd-to-machine relation extraction system to eventually Ô¨Åll a knowledge graph. To train a relation extraction model, training data Ô¨Årst have to be prepared either manually or automatically. A model trained by manually labeled data could show a better performance, however, it is not scalable because another set of training data should be prepared. If a model is trained by automatically collected data the performance could be rather low but the scalability is excellent since automatically collecting training data can be easily done. To expand a knowledge graph, not only do we need a relation extraction model with high accuracy, but also the model is better to be scalable. We suggest a crowd sourcing system with a scalable relation extraction model to Ô¨Åll a knowledge graph. 
Wikipedia has become a reference knowledge source for scores of NLP applications. One of its invaluable features lies in its multilingual nature, where articles on a same entity or concept can have from one to more than 200 different versions. The interlinking of language versions in Wikipedia has undergone a major renewal with the advent of Wikidata, a unified scheme to identify entities and their properties using unique numbers. However, as the interlinking is still manually carried out by thousands of editors across the globe, errors may creep in the assignment of entities. In this paper, we describe an optimization technique to match automatically language versions of articles, and hence entities, that is only based on bags of words and anchors. We created a dataset of all the articles on persons we extracted from Wikipedia in six languages: English, French, German, Russian, Spanish, and Swedish. We report a correct match of at least 94.3{\%} on each pair.
In this paper, we present an open information extraction system so-called SRDF that generates lexical knowledge graphs from unstructured texts. In semantic web, knowledge is expressed in the RDF triple form but the natural language text consist of multiple relations between arguments. For this reason, we combine open information extraction with the reification for the full text extraction to preserve meaning of sentence in our knowledge graph. And also our knowledge graph is designed to adapt for many existing semantic web applications. At the end of this paper, we introduce the result of the experiment and a Korean template generation module developed using SRDF.
Natural language questions are interpreted to a sequence of patterns to be matched with instances of patterns in a knowledge base (KB) for answering. A natural language (NL) question answering (QA) system utilizes meaningful patterns matching the syntac-tic/lexical features between the NL questions and KB. In the most of KBs, there are only binary relations in triple form to represent relation between two entities or entity and a value using the domain specific ontology. However, the binary relation representation is not enough to cover complex information in questions, and the ontology vocabulary sometimes does not cover the lexical meaning in questions. Complex meaning needs a knowledge representation to link the binary relation-type triples in KB. In this paper, we propose a frame semantics-based semantic parsing approach as KB-independent question pre-processing. We will propose requirements of question interpretation in the KBQA perspective, and a query form representation based on our proposed format QAF (Ques-tion Answering with the Frame Semantics), which is supposed to cover the requirements. In QAF, frame semantics roles as a model to represent complex information in questions and to disambiguate the lexical meaning in questions to match with the ontology vocabu-lary. Our system takes a question as an input and outputs QAF-query by the process which assigns semantic information in the question to its corresponding frame semantic structure using the semantic parsing rules.
Nowadays, a question answering (QA) system is used in various areas such a quiz show, personal assistant, home device, and so on. The OKBQA framework supports developing a QA system in an intuitive and collaborative ways. To support collaborative development, the framework should be equipped with some functions, e.g., flexible system configuration, debugging supports, intuitive user interface, and so on while considering different developing groups of different domains. This paper presents OKBQA controller, a dedicated workflow manager for OKBQA framework, to boost collaborative development of a QA system.
Men are from Mars and women are from Venus - or so the genre of relationship literature would have us believe. But there is some truth in this idea, and researchers in fields as diverse as psychology, sociology, and linguistics have explored ways to better understand the differences between genders. In this paper, we take another look at the problem of gender discrimination and attempt to move beyond the typical surface-level text classification approach, by (1) identifying semantic and psycholinguistic word classes that reflect systematic differences between men and women and (2) finding differences between genders in the ways they use the same words. We describe several experiments and report results on a large collection of blogs authored by men and women.
Recent studies have demonstrated gender and cultural differences in the recognition of emotions in facial expressions. However, most studies were conducted on American subjects. In this paper, we explore the generalizability of several findings to a non-American culture in the form of Danish subjects. We conduct an emotion recognition task followed by two stereotype questionnaires with different genders and age groups. While recent findings (Krems et al., 2015) suggest that women are biased to see anger in neutral facial expressions posed by females, in our sample both genders assign higher ratings of anger to all emotions expressed by females. Furthermore, we demonstrate an effect of gender on the fear-surprise-confusion observed by Tomkins and McCarter (1964); females overpredict fear, while males overpredict surprise.
Many methods have been used to recognise author personality traits from text, typically combining linguistic feature engineering with shallow learning models, e.g. linear regression or Support Vector Machines. This work uses deep-learning-based models and atomic features of text, the characters, to build hierarchical, vectorial word and sentence representations for trait inference. This method, applied to a corpus of tweets, shows state-of-the-art performance across five traits compared with prior work. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits.
We exploit the Facebook reaction feature in a distant supervised fashion to train a support vector machine classifier for emotion detection, using several feature combinations and combining different Facebook pages. We test our models on existing benchmarks for emotion detection and show that employing only information that is derived completely automatically, thus without relying on any handcrafted lexicon as it{'}s usually done, we can achieve competitive results. The results also show that there is large room for improvement, especially by gearing the collection of Facebook pages, with a view to the target domain.
This paper proposes a graphical framework to extract opinionated sentences which highlight different contexts within a given news article by introducing the concept of diversity in a graphical model for opinion detection.We conduct extensive evaluations and find that the proposed modification leads to impressive improvement in performance and makes the final results of the model much more usable. The proposed method (OP-D) not only performs much better than the other techniques used for opinion detection as well as introducing diversity, but is also able to select opinions from different categories (Asher et al. 2009). By developing a classification model which categorizes the identified sentences into various opinion categories, we find that OP-D is able to push opinions from different categories uniformly among the top opinions.
Automatic detection of five language components, which are all relevant for expressing opinions and for stance taking, was studied: positive sentiment, negative sentiment, speculation, contrast and condition. A resource-aware approach was taken, which included manual annotation of 500 training samples and the use of limited lexical resources. Active learning was compared to random selection of training data, as well as to a lexicon-based method. Active learning was successful for the categories speculation, contrast and condition, but not for the two sentiment categories, for which results achieved when using active learning were similar to those achieved when applying a random selection of training data. This difference is likely due to a larger variation in how sentiment is expressed than in how speakers express the other three categories. This larger variation was also shown by the lower recall results achieved by the lexicon-based approach for sentiment than for the categories speculation, contrast and condition.
We investigate the application of kernel methods to representing both structural and lexical knowledge for predicting polarity of opinions in consumer product review. We introduce any-gram kernels which model lexical information in a significantly faster way than the traditional n-gram features, while capturing all possible orders of n-grams n in a sequence without the need to explicitly present a pre-specified set of such orders. We also present an effective format to represent constituency and dependency structure together with aspect terms and sentiment polarity scores. Furthermore, we modify the traditional tree kernel function to compute the similarity based on word embedding vectors instead of exact string match and present experiments using the new models.
This paper explores humour recognition for Twitter-based hashtag games. Given their popularity, frequency, and relatively formulaic nature, these games make a good target for computational humour research and can leverage Twitter likes and retweets as humour judgments. In this work, we use pair-wise relative humour judgments to examine several measures of semantic relatedness between setups and punchlines on a hashtag game corpus we collected and annotated. Results show that perplexity, Normalized Google Distance, and free-word association-based features are all useful in identifying {``}funnier{''} hashtag game responses. In fact, we provide empirical evidence that funnier punchlines tend to be more obscure, although more obscure punchlines are not necessarily rated funnier. Furthermore, the asymmetric nature of free-word association features allows us to see that while punchlines should be harder to predict given a setup, they should also be relatively easy to understand in context.
Despite a substantial progress made in developing new sentiment lexicon generation (SLG) methods for English, the task of transferring these approaches to other languages and domains in a sound way still remains open. In this paper, we contribute to the solution of this problem by systematically comparing semi-automatic translations of common English polarity lists with the results of the original automatic SLG algorithms, which were applied directly to German data. We evaluate these lexicons on a corpus of 7,992 manually annotated tweets. In addition to that, we also collate the results of dictionary- and corpus-based SLG methods in order to find out which of these paradigms is better suited for the inherently noisy domain of social media. Our experiments show that semi-automatic translations notably outperform automatic systems (reaching a macro-averaged F1-score of 0.589), and that dictionary-based techniques produce much better polarity lists as compared to corpus-based approaches (whose best F1-scores run up to 0.479 and 0.419 respectively) even for the non-standard Twitter genre.
Detecting depression or personality traits, tutoring and student behaviour systems, or identifying cases of cyber-bulling are a few of the wide range of the applications, in which the automatic detection of emotion is a crucial element. Emotion detection has the potential of high impact by contributing the benefit of business, society, politics or education. Given this context, the main objective of our research is to contribute to the resolution of one of the most important challenges in textual emotion detection task: the problems of emotional corpora annotation. This will be tackled by proposing of a new semi-automatic methodology. Our innovative methodology consists in two main phases: (1) an automatic process to pre-annotate the unlabelled sentences with a reduced number of emotional categories; and (2) a refinement manual process where human annotators will determine which is the predominant emotion between the emotional categories selected in the phase 1. Our proposal in this paper is to show and evaluate the pre-annotation process to analyse the feasibility and the benefits by the methodology proposed. The results obtained are promising and allow obtaining a substantial improvement of annotation time and cost and confirm the usefulness of our pre-annotation process to improve the annotation task.
We created a model to estimate personality trait from authors{'} text written in Japanese and measured its performance by conducting surveys and analyzing the Twitter data of 1,630 users. We used the Big Five personality traits for personality trait estimation. Our approach is a combination of category- and Word2Vec-based approaches. For the category-based element, we added several unique Japanese categories along with the ones regularly used in the English model, and for the Word2Vec-based element, we used a model called GloVe. We found that some of the newly added categories have a stronger correlation with personality traits than other categories do and that the combination of the category- and Word2Vec-based approaches improves the accuracy of the personality trait estimation compared with the case of using just one of them.
On June 23rd 2016, UK held the referendum which ratified the exit from the EU. While most of the traditional pollsters failed to forecast the final vote, there were online systems that hit the result with high accuracy using opinion mining techniques and big data. Starting one month before, we collected and monitored millions of posts about the referendum from social media conversations, and exploited Natural Language Processing techniques to predict the referendum outcome. In this paper we discuss the methods used by traditional pollsters and compare it to the predictions based on different opinion mining techniques. We find that opinion mining based on agreement/disagreement classification works better than opinion mining based on polarity classification in the forecast of the referendum outcome.
The conundrum of understanding and classifying sarcasm has been dealt with by the traditional theorists as an analysis of a sarcastic utterance and the ironic situation that surrounds it. The problem with such an approach is that it is too narrow, as it is unable to sufficiently utilize the two indispensable agents in making such an utterance, viz. the speaker and the listener. It undermines the necessary context required to comprehend a sarcastic utterance. In this paper, we propose a novel approach towards understanding sarcasm in terms of the existing knowledge hierarchy between the two participants, which forms the basis of the context that both agents share. The difference in relationship of the speaker of the sarcastic utterance and the disparate audience found on social media, such as Twitter, is also captured. We then apply our model on a corpus of tweets to achieve significant results and consequently, shed light on subjective nature of context, which is contingent on the relation between the speaker and the listener.
A growing body of research exploits social media behaviors to gauge psychological character-istics, though trait empathy has received little attention. Because of its intimate link to the abil-ity to relate to others, our research aims to predict participants{'} levels of empathy, given their textual and friending behaviors on Facebook. Using Poisson regression, we compared the vari-ance explained in Davis{'} Interpersonal Reactivity Index (IRI) scores on four constructs (em-pathic concern, personal distress, fantasy, perspective taking), by two classes of variables: 1) post content and 2) linguistic style. Our study lays the groundwork for a greater understanding of empathy{'}s role in facilitating interactions on social media.
This paper outlines a pilot study on multi-dimensional and multilingual sentiment analysis of social media content. We use parallel corpora of movie subtitles as a proxy for colloquial language in social media channels and a multilingual emotion lexicon for fine-grained sentiment analyses. Parallel data sets make it possible to study the preservation of sentiments and emotions in translation and our assessment reveals that the lexical approach shows great inter-language agreement. However, our manual evaluation also suggests that the use of purely lexical methods is limited and further studies are necessary to pinpoint the cross-lingual differences and to develop better sentiment classifiers.
In this paper, we address the issue of automatic prediction of readers{'} mood from newspaper articles and comments. As online newspapers are becoming more and more similar to social media platforms, users can provide affective feedback, such as mood and emotion. We have exploited the self-reported annotation of mood categories obtained from the metadata of the Italian online newspaper corriere.it to design and evaluate a system for predicting five different mood categories from news articles and comments: indignation, disappointment, worry, satisfaction, and amusement. The outcome of our experiments shows that overall, bag-of-word-ngrams perform better compared to all other feature sets; however, stylometric features perform better for the mood score prediction of articles. Our study shows that self-reported annotations can be used to design automatic mood prediction systems.
Most work in NLP analysing microblogs focuses on textual content thus neglecting temporal and spatial information. We present a new interdisciplinary method for emotion classification that combines linguistic, temporal, and spatial information into a single metric. We create a graph of labeled and unlabeled tweets that encodes the relations between neighboring tweets with respect to their emotion labels. Graph-based semi-supervised learning labels all tweets with an emotion.
We explore a domain-agnostic approach for analyzing speech with the goal of opinion prediction. We represent the speech signal by mel-frequency cepstral coefficients and apply long short-term memory neural networks to automatically learn temporal regularities in speech. In contrast to previous work, our approach does not require complex feature engineering and works without textual transcripts. As a consequence, it can easily be applied on various speech analysis tasks for different languages and the results show that it can nevertheless be competitive to the state-of-the-art in opinion prediction. In a detailed error analysis for opinion mining we find that our approach performs well in identifying speaker-specific characteristics, but should be combined with additional information if subtle differences in the linguistic content need to be identified.
Considering the importance of public speech skills, a system which makes a prediction on where audiences laugh in a talk can be helpful to a person who prepares for a talk. We investigated a possibility that a state-of-the-art humor recognition system can be used in detecting sentences inducing laughters in talks. In this study, we used TED talks and laughters in the talks as data. Our results showed that the state-of-the-art system needs to be improved in order to be used in a practical application. In addition, our analysis showed that classifying humorous sentences in talks is very challenging due to close distance between humorous and non-humorous sentences.
Major depressive disorder, a debilitating and burdensome disease experienced by individuals worldwide, can be defined by several depressive symptoms (e.g., anhedonia (inability to feel pleasure), depressed mood, difficulty concentrating, etc.). Individuals often discuss their experiences with depression symptoms on public social media platforms like Twitter, providing a potentially useful data source for monitoring population-level mental health risk factors. In a step towards developing an automated method to estimate the prevalence of symptoms associated with major depressive disorder over time in the United States using Twitter, we developed classifiers for discerning whether a Twitter tweet represents no evidence of depression or evidence of depression. If there was evidence of depression, we then classified whether the tweet contained a depressive symptom and if so, which of three subtypes: depressed mood, disturbed sleep, or fatigue or loss of energy. We observed that the most accurate classifiers could predict classes with high-to-moderate F1-score performances for no evidence of depression (85), evidence of depression (52), and depressive symptoms (49). We report moderate F1-scores for depressive symptoms ranging from 75 (fatigue or loss of energy) to 43 (disturbed sleep) to 35 (depressed mood). Our work demonstrates baseline approaches for automatically encoding Twitter data with granular depressive symptoms associated with major depressive disorder.
The goal of this paper is to examine the impact of simple feature engineering mechanisms before applying more sophisticated techniques to the task of medical NER. Sometimes papers using scientifically sound techniques present raw baselines that could be improved adding simple and cheap features. This work focuses on entity recognition for the clinical domain for three languages: English, Swedish and Spanish. The task is tackled using simple features, starting from the window size, capitalization, prefixes, and moving to POS and semantic tags. This work demonstrates that a simple initial step of feature engineering can improve the baseline results significantly. Hence, the contributions of this paper are: first, a short list of guidelines well supported with experimental results on three languages and, second, a detailed description of the relevance of these features for medical NER.
Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research. For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems). State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define. For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings. The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge.
Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients{'} privacy, Protected Health Information (PHI) must be removed from the patient notes before they can be legally released, a process known as patient note de-identification. The main objective for a de-identification system is to have the highest possible recall. Recently, the first neural-network-based de-identification system has been proposed, yielding state-of-the-art results. Unlike other systems, it does not rely on human-engineered features, which allows it to be quickly deployed, but does not leverage knowledge from human experts or from electronic health records (EHRs). In this work, we explore a method to incorporate human-engineered features as well as features derived from EHRs to a neural-network-based de-identification system. Our results show that the addition of features, especially the EHR-derived features, further improves the state-of-the-art in patient note de-identification, including for some of the most sensitive PHI types such as patient names. Since in a real-life setting patient notes typically come with EHRs, we recommend developers of de-identification systems to leverage the information EHRs contain.
Semi-supervised clustering is an attractive alternative for traditional (unsupervised) clustering in targeted applications. By using the information of a small annotated dataset, semi-supervised clustering can produce clusters that are customized to the application domain. In this paper, we present a semi-supervised clustering technique based on a multi-objective evolutionary algorithm (NSGA-II-clus). We apply this technique to the task of clustering medical publications for Evidence Based Medicine (EBM) and observe an improvement of the results against unsupervised and other semi-supervised clustering techniques.
Rapid growth in Electronic Medical Records (EMR) has emerged to an expansion of data in the clinical domain. The majority of the available health care information is sealed in the form of narrative documents which form the rich source of clinical information. Text mining of such clinical records has gained huge attention in various medical applications like treatment and decision making. However, medical records enclose patient Private Health Information (PHI) which can reveal the identities of the patients. In order to retain the privacy of patients, it is mandatory to remove all the PHI information prior to making it publicly available. The aim is to de-identify or encrypt the PHI from the patient medical records. In this paper, we propose an algorithm based on deep learning architecture to solve this problem. We perform de-identification of seven PHI terms from the clinical records. Experiments on benchmark datasets show that our proposed approach achieves encouraging performance, which is better than the baseline model developed with Conditional Random Field.
Paraphrase generation is important in various applications such as search, summarization, and question answering due to its ability to generate textual alternatives while keeping the overall meaning intact. Clinical paraphrase generation is especially vital in building patient-centric clinical decision support (CDS) applications where users are able to understand complex clinical jargons via easily comprehensible alternative paraphrases. This paper presents Neural Clinical Paraphrase Generation (NCPG), a novel approach that casts the task as a monolingual neural machine translation (NMT) problem. We propose an end-to-end neural network built on an attention-based bidirectional Recurrent Neural Network (RNN) architecture with an encoder-decoder framework to perform the task. Conventional bilingual NMT models mostly rely on word-level modeling and are often limited by out-of-vocabulary (OOV) issues. In contrast, we represent the source and target paraphrase pairs as character sequences to address this limitation. To the best of our knowledge, this is the first work that uses attention-based RNNs for clinical paraphrase generation and also proposes an end-to-end character-level modeling for this task. Extensive experiments on a large curated clinical paraphrase corpus show that the attention-based NCPG models achieve improvements of up to 5.2 BLEU points and 0.5 METEOR points over a non-attention based strong baseline for word-level modeling, whereas further gains of up to 6.1 BLEU points and 1.3 METEOR points are obtained by the character-level NCPG models over their word-level counterparts. Overall, our models demonstrate comparable performance relative to the state-of-the-art phrase-based non-neural models.
The proliferation of deep learning methods in natural language processing (NLP) and the large amounts of data they often require stands in stark contrast to the relatively data-poor clinical NLP domain. In particular, large text corpora are necessary to build high-quality word embeddings, yet often large corpora that are suitably representative of the target clinical data are unavailable. This forces a choice between building embeddings from small clinical corpora and less representative, larger corpora. This paper explores this trade-off, as well as intermediate compromise solutions. Two standard clinical NLP tasks (the i2b2 2010 concept and assertion tasks) are evaluated with commonly used deep learning models (recurrent neural networks and convolutional neural networks) using a set of six corpora ranging from the target i2b2 data to large open-domain datasets. While combinations of corpora are generally found to work best, the single-best corpus is generally task-dependent.
In this work we present a fine-grained annotation schema to detect named entities in German clinical data of chronically ill patients with kidney diseases. The annotation schema is driven by the needs of our clinical partners and the linguistic aspects of German language. In order to generate annotations within a short period, the work also presents a semi-automatic annotation which uses additional sources of knowledge such as UMLS, to pre-annotate concepts in advance. The presented schema will be used to apply novel techniques from natural language processing and machine learning to support doctors treating their patients by improved information access from unstructured German texts.
In recent years, detecting Alzheimer disease (AD) in early stages based on natural language processing (NLP) has drawn much attention. To date, vocabulary size, grammatical complexity, and fluency have been studied using NLP metrics. However, the content analysis of AD narratives is still unreachable for NLP. This study investigates features of the words that AD patients use in their spoken language. After recruiting 18 examinees of 53{--}90 years old (mean: 76.89), they were divided into two groups based on MMSE scores. The AD group comprised 9 examinees with scores of 21 or lower. The healthy control group comprised 9 examinees with a score of 22 or higher. Linguistic Inquiry and Word Count (LIWC) classified words were used to categorize the words that the examinees used. The word frequency was found from observation. Significant differences were confirmed for the usage of impersonal pronouns in the AD group. This result demonstrated the basic feasibility of the proposed NLP-based detection approach.
The number of unstructured medical records kept in hospital information systems is increasing. The conditions of patients are formulated as outcomes in clinical pathway. A variance of an outcome describes deviations from standards of care like a patient{'}s bad condition. The present paper applied text mining to extract feature words and phrases of the variance from admission records. We report the cases the variances of {``}pain control{''} and {``}no neuropathy worsening{''} in cerebral infarction.
Clinical narratives in electronic health record systems are a rich resource of patient-based information. They constitute an ongoing challenge for natural language processing, due to their high compactness and abundance of short forms. German medical texts exhibit numerous ad-hoc abbreviations that terminate with a period character. The disambiguation of period characters is therefore an important task for sentence and abbreviation detection. This task is addressed by a combination of co-occurrence information of word types with trailing period characters, a large domain dictionary, and a simple rule engine, thus merging statistical and dictionary-based disambiguation strategies. An F-measure of 0.95 could be reached by using the unsupervised approach presented in this paper. The results are promising for a domain-independent abbreviation detection strategy, because our approach avoids retraining of models or use case specific feature engineering efforts required for supervised machine learning approaches.
In this paper, we propose to use a subfield of machine learning {--}grammatical inference{--} to measure linguistic complexity from a developmental point of view. We focus on relative complexity by considering a child learner in the process of first language acquisition. The relevance of grammatical inference models for measuring linguistic complexity from a developmental point of view is based on the fact that algorithms proposed in this area can be considered computational models for studying first language acquisition. Even though it will be possible to use different techniques from the field of machine learning as computational models for dealing with linguistic complexity -since in any model we have algorithms that can learn from data-, we claim that grammatical inference models offer some advantages over other tools.
In this paper, we introduce for the first time a Distributional Model for computing semantic complexity, inspired by the general principles of the Memory, Unification and Control framework(Hagoort, 2013; Hagoort, 2016). We argue that sentence comprehension is an incremental process driven by the goal of constructing a coherent representation of the event represented by the sentence. The composition cost of a sentence depends on the semantic coherence of the event being constructed and on the activation degree of the linguistic constructions. We also report the results of a first evaluation of the model on the Bicknell dataset (Bicknell et al., 2010).
We present a novel approach to the automatic assessment of text complexity based on a sliding-window technique that tracks the distribution of complexity within a text. Such distribution is captured by what we term {``}complexity contours{''} derived from a series of measurements for a given linguistic complexity measure. This approach is implemented in an automatic computational tool, CoCoGen {--} Complexity Contour Generator, which in its current version supports 32 indices of linguistic complexity. The goal of the paper is twofold: (1) to introduce the design of our computational tool based on a sliding-window technique and (2) to showcase this approach in the area of second language (L2) learning, i.e. more specifically, in the area of L2 writing.
This study demonstrates a weakness in how n-gram and PCFG surprisal are used to predict reading times in eye-tracking data. In particular, the information conveyed by words skipped during saccades is not usually included in the surprisal measures. This study shows that correcting the surprisal calculation improves n-gram surprisal and that upcoming n-grams affect reading times, replicating previous findings of how lexical frequencies affect reading times. In contrast, the predictivity of PCFG surprisal does not benefit from the surprisal correction despite the fact that lexical sequences skipped by saccades are processed by readers, as demonstrated by the corrected n-gram measure. These results raise questions about the formulation of information-theoretic measures of syntactic processing such as PCFG surprisal and entropy reduction when applied to reading times.
Computational approaches to readability assessment are generally built and evaluated using gold standard corpora labeled by publishers or teachers rather than being grounded in observations about human performance. Considering that both the reading process and the outcome can be observed, there is an empirical wealth that could be used to ground computational analysis of text readability. This will also support explicit readability models connecting text complexity and the reader{'}s language proficiency to the reading process and outcomes. This paper takes a step in this direction by reporting on an experiment to study how the relation between text complexity and reader{'}s language proficiency affects the reading process and performance outcomes of readers after reading We modeled the reading process using three eye tracking variables: fixation count, average fixation count, and second pass reading duration. Our models for these variables explained 78.9{\%}, 74{\%} and 67.4{\%} variance, respectively. Performance outcome was modeled through recall and comprehension questions, and these models explained 58.9{\%} and 27.6{\%} of the variance, respectively. While the online models give us a better understanding of the cognitive correlates of reading with text complexity and language proficiency, modeling of the offline measures can be particularly relevant for incorporating user aspects into readability models.
Studies on the role of memory as a predictor of reading time latencies (1) differ in their predictions about when memory effects should occur in processing and (2) have had mixed results, with strong positive effects emerging from isolated constructed stimuli and weak or even negative effects emerging from naturally-occurring stimuli. Our study addresses these concerns by comparing several implementations of prominent sentence processing theories on an exploratory corpus and evaluating the most successful of these on a confirmatory corpus, using a new self-paced reading corpus of seemingly natural narratives constructed to contain an unusually high proportion of memory-intensive constructions. We show highly significant and complementary broad-coverage latency effects both for predictors based on the Dependency Locality Theory and for predictors based on a left-corner parsing model of sentence processing. Our results indicate that memory access during sentence processing does take time, but suggest that stimuli requiring many memory access events may be necessary in order to observe the effect.
Lexical complexity plays a central role in readability, particularly for dyslexic children and poor readers because of their slow and laborious decoding and word recognition skills. Although some features to aid readability may be common to most languages (e.g., the majority of {`}easy{'} words are of low frequency), we believe that lexical complexity is mainly language-specific. In this paper, we define lexical complexity for French and we present a pilot study on the effects of text simplification in dyslexic children. The participants were asked to read out loud original and manually simplified versions of a standardized French text corpus and to answer comprehension questions after reading each text. The analysis of the results shows that the simplifications performed were beneficial in terms of reading speed and they reduced the number of reading errors (mainly lexical ones) without a loss in comprehension. Although the number of participants in this study was rather small (N=10), the results are promising and contribute to the development of applications in computational linguistics.
In this paper we will be dealing with different levels of complexity in the processing of Italian, a Romance language inheriting many properties from Latin which make it an almost free word order language . The paper is concerned with syntactic complexity as measurable on the basis of the cognitive parser that incrementally builds up a syntactic representation to be used by the semantic component. The theory behind will be LFG and parsing preferences will be used to justify one choice both from a principled and a processing point of view. LFG is a transformationless theory in which there is no deep structure separate from surface syntactic structure. This is partially in accordance with constructional theories in which noncanonical structures containing non-argument functions FOCUS/TOPIC are treated as multifunctional constituents. Complexity is computed on a processing basis following suggestions made by Blache and demonstrated by Kluender and Chesi
Previous researches have shown that learning multiple representations for polysemous words can improve the performance of word embeddings on many tasks. However, this leads to another problem. Several vectors of a word may actually point to the same meaning, namely pseudo multi-sense. In this paper, we introduce the concept of pseudo multi-sense, and then propose an algorithm to detect such cases. With the consideration of the detected pseudo multi-sense cases, we try to refine the existing word embeddings to eliminate the influence of pseudo multi-sense. Moreover, we apply our algorithm on previous released multi-sense word embeddings and tested it on artificial word similarity tasks and the analogy task. The result of the experiments shows that diminishing pseudo multi-sense can improve the quality of word representations. Thus, our method is actually an efficient way to reduce linguistic complexity.
In this paper, we present a comparative analysis of statistically predictive syntactic features of complexity and the treatment of these features by humans when simplifying texts. To that end, we have used a list of the most five statistically predictive features obtained automatically and the Corpus of Basque Simplified Texts (CBST) to analyse how the syntactic phenomena in these features have been manually simplified. Our aim is to go beyond the descriptions of operations found in the corpus and relate the multidisciplinary findings to understand text complexity from different points of view. We also present some issues that can be important when analysing linguistic complexity.
Pause analysis of key-stroke logged translations is a hallmark of process based translation studies. However, an exact definition of what a cognitively effortful pause during the translation process is has not been found yet (Saldanha and O{'}Brien, 2013). This paper investigates the design of a key-stroke and subject dependent identification system of cognitive effort to track complexity in translation with keystroke logging (cf. also (Dragsted, 2005) (Couto-Vale, in preparation)). It is an elastic measure that takes into account idiosyncratic pause duration of translators as well as further confounds such as bi-gram frequency, letter frequency and some motor tasks involved in writing. The method is compared to a common static threshold of 1000 ms in an analysis of cognitive effort during the translation of grammatical functions from English to German. Additionally, the results are triangulated with eye tracking data for further validation. The findings show that at least for smaller sets of data a dynamic pause assessment may lead to more accurate results than a generic static pause threshold of similar duration.
Data driven approaches to readability analysis for languages other than English has been plagued by a scarcity of suitable corpora. Often, relevant corpora consist only of easy-to-read texts with no rank information or empirical readability scores, making only binary approaches, such as classification, applicable. We propose a Bayesian, latent variable, approach to get the most out of these kinds of corpora. In this paper we present results on using such a model for readability ranking. The model is evaluated on a preliminary corpus of ranked student texts with encouraging results. We also assess the model by showing that it performs readability classification on par with a state of the art classifier while at the same being transparent enough to allow more sophisticated interpretations.
Informed by research on readability and language acquisition, computational linguists have developed sophisticated tools for the analysis of linguistic complexity. While some tools are starting to become accessible on the web, there still is a disconnect between the features that can in principle be identified based on state-of-the-art computational linguistic analysis, and the analyses a second language acquisition researcher, teacher, or textbook writer can readily obtain and visualize for their own collection of texts. This short paper presents a web-based tool development that aims to meet this challenge. The Common Text Analysis Platform (CTAP) is designed to support fully configurable linguistic feature extraction for a wide range of complexity analyses. It features a user-friendly interface, modularized and reusable analysis component integration, and flexible corpus and feature management. Building on the Unstructured Information Management framework (UIMA), CTAP readily supports integration of state-of-the-art NLP and complexity feature extraction maintaining modularization and reusability. CTAP thereby aims at providing a common platform for complexity analysis, encouraging research collaboration and sharing of feature extraction components{---}to jointly advance the state-of-the-art in complexity analysis in a form that readily supports real-life use by ordinary users.
We bring together knowledge from two different types of language learning data, texts learners read and texts they write, to improve linguistic complexity classification in the latter. Linguistic complexity in the foreign and second language learning context can be expressed in terms of proficiency levels. We show that incorporating features capturing lexical complexity information from reading passages can boost significantly the machine learning based classification of learner-written texts into proficiency levels. With an F1 score of .8 our system rivals state-of-the-art results reported for other languages for this task. Finally, we present a freely available web-based tool for proficiency level classification and lexical complexity visualization for both learner writings and reading texts.
Arabic writing is typically underspecified for short vowels and other markups, referred to as diacritics. In addition to the lexical ambiguity exhibited in most languages, the lack of diacritics in written Arabic adds another layer of ambiguity which is an artifact of the orthography. In this paper, we present the details of three annotation experimental conditions designed to study the impact of automatic ambiguity detection, on annotation speed and quality in a large scale annotation project.
Computational linguistic approaches to sign languages could benefit from investigating how complexity influences structure. We investigate whether morphological complexity has an effect on the order of Verb (V) and Object (O) in Swedish Sign Language (SSL), on the basis of elicited data from five Deaf signers. We find a significant difference in the distribution of the orderings OV vs. VO, based on an analysis of morphological weight. While morphologically heavy verbs exhibit a general preference for OV, humanness seems to affect the ordering in the opposite direction, with [+human] Objects pushing towards a preference for VO.
Language complexity is an intriguing phenomenon argued to play an important role in both language learning and processing. The need to compare languages with regard to their complexity resulted in a multitude of approaches and methods, ranging from accounts targeting specific structural features to global quantification of variation more generally. In this paper, we investigate the degree to which morphological complexity measures are mutually correlated in a sample of more than 500 languages of 101 language families. We use human expert judgements from the World Atlas of Language Structures (WALS), and compare them to four quantitative measures automatically calculated from language corpora. These consist of three previously defined corpus-derived measures, which are all monolingual, and one new measure based on automatic word-alignment across pairs of languages. We find strong correlations between all the measures, illustrating that both expert judgements and automated approaches converge to similar complexity ratings, and can be used interchangeably.
Comparable or parallel corpora are beneficial for many NLP tasks. The automatic collection of corpora enables large-scale resources, even for less-resourced languages, which in turn can be useful for deducing rules and patterns for text rewriting algorithms, a subtask of automatic text simplification. We present two methods for the alignment of Swedish easy-to-read text segments to text segments from a reference corpus. The first method (M1) was originally developed for the task of text reuse detection, measuring sentence similarity by a modified version of a TF-IDF vector space model. A second method (M2), also accounting for part-of-speech tags, was developed, and the methods were compared. For evaluation, a crowdsourcing platform was built for human judgement data collection, and preliminary results showed that cosine similarity relates better to human ranks than the Dice coefficient. We also saw a tendency that including syntactic context to the TF-IDF vector space model is beneficial for this kind of paraphrase alignment task.
This work presents a framework for the automatic construction of large Web corpora classified by readability level. We compare different Machine Learning classifiers for the task of readability assessment focusing on Portuguese and English texts, analysing the impact of variables like the feature inventory used in the resulting corpus. In a comparison between shallow and deeper features, the former already produce F-measures of over 0.75 for Portuguese texts, but the use of additional features results in even better results, in most cases. For English, shallow features also perform well as do classic readability formulas. Comparing different classifiers for the task, logistic regression obtained, in general, the best results, but with considerable differences between the results for two and those for three-classes, especially regarding the intermediary class. Given the large scale of the resulting corpus, for evaluation we adopt the agreement between different classifiers as an indication of readability assessment certainty. As a result of this work, a large corpus for Brazilian Portuguese was built, including 1.7 million documents and about 1.6 billion tokens, already parsed and annotated with 134 different textual attributes, along with the agreement among the various classifiers.
This work investigates the application of a measure of surprisal to modeling a grammatical variation phenomenon between near-synonymous constructions. We investigate a particular variation phenomenon, word order variation in Dutch two-verb clusters, where it has been established that word order choice is affected by processing cost. Several multifactorial corpus studies of Dutch verb clusters have used other measures of processing complexity to show that this factor affects word order choice. This previous work allows us to compare the surprisal measure, which is based on constraint satisfaction theories of language modeling, to those previously used measures, which are more directly linked to empirical observations of processing complexity. Our results show that surprisal does not predict the word order choice by itself, but is a significant predictor when used in a measure of uniform information density (UID). This lends support to the view that human language processing is facilitated not so much by predictable sequences of words but more by sequences of words in which information is spread evenly.
The relative contributions of meaning and form to sentence processing remains an outstanding issue across the language sciences. We examine this issue by formalizing four incremental complexity metrics and comparing them against freely-available ROI timecourses. Syntax-related metrics based on top-down parsing and structural dependency-distance turn out to significantly improve a regression model, compared to a simpler model that formalizes only conceptual combination using a distributional vector-space model. This confirms the view of the anterior temporal lobes as combinatory engines that deal in both form (see e.g. Brennan et al., 2012; Mazoyer, 1993) and meaning (see e.g., Patterson et al., 2007). This same characterization applies to a posterior temporal region in roughly {``}Wernicke{'}s Area.{''}
This paper investigates the use of automatic speech recognition (ASR) errors as indicators of the second language (L2) learners{'} listening difficulties and in doing so strives to overcome the shortcomings of Partial and Synchronized Caption (PSC) system. PSC is a system that generates a partial caption including difficult words detected based on high speech rate, low frequency, and specificity. To improve the choice of words in this system, and explore a better method to detect speech challenges, ASR errors were investigated as a model of the L2 listener, hypothesizing that some of these errors are similar to those of language learners{'} when transcribing the videos. To investigate this hypothesis, ASR errors in transcription of several TED talks were analyzed and compared with PSC{'}s selected words. Both the overlapping and mismatching cases were analyzed to investigate possible improvement for the PSC system. Those ASR errors that were not detected by PSC as cases of learners{'} difficulties were further analyzed and classified into four categories: homophones, minimal pairs, breached boundaries and negatives. These errors were embedded into the baseline PSC to make the enhanced version and were evaluated in an experiment with L2 learners. The results indicated that the enhanced version, which encompasses the ASR errors addresses most of the L2 learners{'} difficulties and better assists them in comprehending challenging video segments as compared with the baseline.
Eye-tracking reading times have been attested to reflect cognitive processes underlying sentence comprehension. However, the use of reading times in NLP applications is an underexplored area of research. In this initial work we build an automatic system to assess sentence complexity using automatically predicted eye-tracking reading time measures and demonstrate the efficacy of these reading times for a well known NLP task, namely, readability assessment. We use a machine learning model and a set of features known to be significant predictors of reading times in order to learn per-word reading times from a corpus of English text having reading times of human readers. Subsequently, we use the model to predict reading times for novel text in the context of the aforementioned task. A model based only on reading times gave competitive results compared to the systems that use extensive syntactic features to compute linguistic complexity. Our work, to the best of our knowledge, is the first study to show that automatically predicted reading times can successfully model the difficulty of a text and can be deployed in practical text processing applications.
The article presents results of entropy rate estimation for human languages across six languages by using large, state-of-the-art corpora of up to 7.8 gigabytes. To obtain the estimates for data length tending to infinity, we use an extrapolation function given by an ansatz. Whereas some ansatzes of this kind were proposed in previous research papers, here we introduce a stretched exponential extrapolation function that has a smaller error of fit. In this way, we uncover a possibility that the entropy rates of human languages are positive but 20{\%} smaller than previously reported.
The morphological complexity of languages differs widely and changes over time. Pathways of change are often driven by the interplay of multiple competing factors, and are hard to disentangle. We here focus on a paradigmatic scenario of language change: the reduction of morphological complexity from Latin towards the Romance languages. To establish a causal explanation for this phenomenon, we employ three lines of evidence: 1) analyses of parallel corpora to measure the complexity of words in actual language production, 2) applications of NLP tools to further tease apart the contribution of inflectional morphology to word complexity, and 3) experimental data from artificial language learning, which illustrate the learning pressures at play when morphology simplifies. These three lines of evidence converge to show that pressures associated with imperfect language learning are good candidates to causally explain the reduction in morphological complexity in the Latin-to-Romance scenario. More generally, we argue that combining corpus, computational and experimental evidence is the way forward in historical linguistics and linguistic typology.
The availability of Language Technology Resources and Tools generates a considerable methodological potential in the Digital Humanities: aspects of research questions from the Humanities and Social Sciences can be addressed on text collections in ways that were unavailable to traditional approaches. I start this talk by sketching some sample scenarios of Digital Humanities projects which involve various Humanities and Social Science disciplines, noting that the potential for a meaningful contribution to higher-level questions is highest when the employed language technological models are carefully tailored both (a) to characteristics of the given target corpus, and (b) to relevant analytical subtasks feeding the discipline-specific research questions. Keeping up a multidisciplinary perspective, I then point out a recurrent dilemma in Digital Humanities projects that follow the conventional set-up of collaboration: to build high-quality computational models for the data, fixed analytical targets should be specified as early as possible {--} but to be able to respond to Humanities questions as they evolve over the course of analysis, the analytical machinery should be kept maximally flexible. To reach both, I argue for a novel collaborative culture that rests on a more interleaved, continuous dialogue. (Re-)Specification of analytical targets should be an ongoing process in which the Humanities Scholars and Social Scientists play a role that is as important as the Computational Scientists{'} role. A promising approach lies in the identification of re-occurring types of analytical subtasks, beyond linguistic standard tasks, which can form building blocks for text analysis across disciplines, and for which corpus-based characterizations (viz. annotations) can be collected, compared and revised. On such grounds, computational modeling is more directly tied to the evolving research questions, and hence the seemingly opposing needs of reliable target specifications vs. {``}malleable{''} frameworks of analysis can be reconciled. Experimental work following this approach is under way in the Center for Reflected Text Analytics (CRETA) in Stuttgart.
We examine two different methods for finding rising words (among which neologisms) and falling words (among which archaisms) in decades of magazine texts (millions of words) and in years of tweets (billions of words): one based on correlation coefficients of relative frequencies and time, and one based on comparing initial and final word frequencies of time intervals. We find that smoothing frequency scores improves the precision scores of both methods and that the correlation coefficients perform better on magazine text but worse on tweets. Since the two ranking methods find different words they can be used in side-by-side to study the behavior of words over time.
Multimodal question answering in the cultural heritage domain allows visitors to ask questions in a more natural way and thus provides better user experiences with cultural objects while visiting a museum, landmark or any other historical site. In this paper, we introduce the construction of a golden standard dataset that will aid research of multimodal question answering in the cultural heritage domain. The dataset, which will be soon released to the public, contains multimodal content including images of typical artworks from the fascinating old-Egyptian Amarna period, related image-containing documents of the artworks and over 800 multimodal queries integrating visual and textual questions. The multimodal questions and related documents are all in English. The multimodal questions are linked to relevant paragraphs in the related documents that contain the answer to the multimodal query.
In this paper a social network is extracted from a literary text. The social network shows, how frequent the characters interact and how similar their social behavior is. Two types of similarity measures are used: the first applies co-occurrence statistics, while the second exploits cosine similarity on different types of word embedding vectors. The results are evaluated by a paid micro-task crowdsourcing survey. The experiments suggest that specific types of word embeddings like word2vec are well-suited for the task at hand and the specific circumstances of literary fiction text.
We present an approach to detect differences in lexical semantics across English language registers, using word embedding models from distributional semantics paradigm. Models trained on register-specific subcorpora of the BNC corpus are employed to compare lists of nearest associates for particular words and draw conclusions about their semantic shifts depending on register in which they are used. The models are evaluated on the task of register classification with the help of the deep inverse regression approach. Additionally, we present a demo web service featuring most of the described models and allowing to explore word meanings in different English registers and to detect register affiliation for arbitrary texts. The code for the service can be easily adapted to any set of underlying models.
We are constructing an annotated diachronic corpora of the Japanese language. In part of thiswork, we construct a corpus of Manyosyu, which is an old Japanese poetry anthology. In thispaper, we describe how to align the transcribed text and its original text semiautomatically to beable to cross-reference them in our Manyosyu corpus. Although we align the original charactersto the transcribed words manually, we preliminarily align the transcribed and original charactersby using an unsupervised automatic alignment technique of statistical machine translation toalleviate the work. We found that automatic alignment achieves an F1-measure of 0.83; thus, each poem has 1{--}2 alignment errors. However, finding these errors and modifying them are less workintensiveand more efficient than fully manual annotation. The alignment probabilities can beutilized in this modification. Moreover, we found that we can locate the uncertain transcriptionsin our corpus and compare them to other transcriptions, by using the alignment probabilities.
Arabic is a widely-spoken language with a rich and long history spanning more than fourteen centuries. Yet existing Arabic corpora largely focus on the modern period or lack sufficient diachronic information. We develop a large-scale, historical corpus of Arabic of about 1 billion words from diverse periods of time. We clean this corpus, process it with a morphological analyzer, and enhance it by detecting parallel passages and automatically dating undated texts. We demonstrate its utility with selected case-studies in which we show its application to the digital humanities.
We here describe a novel methodology for measuring affective language in historical text by expanding an affective lexicon and jointly adapting it to prior language stages. We automatically construct a lexicon for word-emotion association of 18th and 19th century German which is then validated against expert ratings. Subsequently, this resource is used to identify distinct emotional patterns and trace long-term emotional trends in different genres of writing spanning several centuries.
Historical treebanks tend to be manually annotated, which is not surprising, since state-of-the-art parsers are not accurate enough to ensure high-quality annotation for historical texts. We test whether automatic parsing can be an efficient pre-annotation tool for Old East Slavic texts. We use the TOROT treebank from the PROIEL treebank family. We convert the PROIEL format to the CONLL format and use MaltParser to create syntactic pre-annotation. Using the most conservative evaluation method, which takes into account PROIEL-specific features, MaltParser by itself yields 0.845 unlabelled attachment score, 0.779 labelled attachment score and 0.741 secondary dependency accuracy (note, though, that the test set comes from a relatively simple genre and contains rather short sentences). Experiments with human annotators show that preparsing, if limited to sentences where no changes to word or sentence boundaries are required, increases their annotation rate. For experienced annotators, the speed gain varies from 5.80{\%} to 16.57{\%}, for inexperienced annotators from 14.61{\%} to 32.17{\%} (using conservative estimates). There are no strong reliable differences in the annotation accuracy, which means that there is no reason to suspect that using preparsing might lower the final annotation quality.
In this paper we will discuss a method for data visualization together with its potential usefulness in digital humanities and philosophy of language. We compiled a multilingual parallel corpus from different versions of \textit{Wittgenstein{'}s Tractatus Logico-philosophicus}, including the original in German and translations into English, Spanish, French, and Russian. Using this corpus, we compute a similarity measure between propositions and render a visual network of relations for different languages.
We introduce the third major release of WebAnno, a generic web-based annotation tool for distributed teams. New features in this release focus on semantic annotation tasks (e.g. semantic role labelling or event annotation) and allow the tight integration of semantic annotations with syntactic annotations. In particular, we introduce the concept of slot features, a novel constraint mechanism that allows modelling the interaction between semantic and syntactic annotations, as well as a new annotation user interface. The new features were developed and used in an annotation project for semantic roles on German texts. The paper briefly introduces this project and reports on experiences performing annotations with the new tool. On a comparative evaluation, our tool reaches significant speedups over WebAnno 2 for a semantic annotation task.
Although spanning thousands of years and genres as diverse as liturgy, historiography, lyric and other forms of prose and poetry, the body of Latin texts is still relatively sparse compared to English. Data sparsity in Latin presents a number of challenges for traditional Named Entity Recognition techniques. Solving such challenges and enabling reliable Named Entity Recognition in Latin texts can facilitate many down-stream applications, from machine translation to digital historiography, enabling Classicists, historians, and archaeologists for instance, to track the relationships of historical persons, places, and groups on a large scale. This paper presents the first annotated corpus for evaluating Named Entity Recognition in Latin, as well as a fully supervised model that achieves over 90{\%} F-score on a held-out test set, significantly outperforming a competitive baseline. We also present a novel active learning strategy that predicts how many and which sentences need to be annotated for named entities in order to attain a specified degree of accuracy when recognizing named entities automatically in a given text. This maximizes the productivity of annotators while simultaneously controlling quality.
We present ANNISVis, a webapp for comparative visualization of geographical distribution of linguistic data, as well as a sample deployment for a corpus of Middle High German texts. Unlike existing geographical visualization solutions, which work with pre-existing data sets, or are bound to specific corpora, ANNISVis allows the user to formulate multiple ad-hoc queries and visualizes them on a map, and it can be configured for any corpus that can be imported into ANNIS. This enables explorative queries of the quantitative aspects of a corpus with geographical features. The tool will be made available to download in open source.
In the Danish CLARIN-DK infrastructure, chaining language technology (LT) tools into a workflow is easy even for a non-expert user, because she only needs to specify the input and the desired output of the workflow. With this information and the registered input and output profiles of the available tools, the CLARIN-DK workflow management system (WMS) computes combinations of tools that will give the desired result. This advanced functionality was originally not envisaged, but came within reach by writing the WMS partly in Java and partly in a programming language for symbolic computation, Bracmat. Handling LT tool profiles, including the computation of workflows, is easier with Bracmat{'}s language constructs for tree pattern matching and tree construction than with the language constructs offered by mainstream programming languages.
Machine Translation (MT) plays a critical role in expanding capacity in the translation industry. However, many valuable documents, including digital documents, are encoded in non-accessible formats for machine processing (e.g., Historical or Legal documents). Such documents must be passed through a process of Optical Character Recognition (OCR) to render the text suitable for MT. No matter how good the OCR is, this process introduces recognition errors, which often renders MT ineffective. In this paper, we propose a new OCR to MT framework based on adding a new OCR error correction module to enhance the overall quality of translation. Experimentation shows that our new system correction based on the combination of Language Modeling and Translation methods outperforms the baseline system by nearly 30{\%} relative improvement.
In this paper we describe how the complexity of human communication can be analysed with the help of language technology. We present the HuComTech corpus, a multimodal corpus containing 50 hours of videotaped interviews containing a rich annotation of about 2 million items annotated on 33 levels. The corpus serves as a general resource for a wide range of re-search addressing natural conversation between humans in their full complexity. It can benefit particularly digital humanities researchers working in the field of pragmatics, conversational analysis and discourse analysis. We will present a number of tools and automated methods that can help such enquiries. In particular, we will highlight the tool Theme, which is designed to uncover hidden temporal patterns (called T-patterns) in human interaction, and will show how it can applied to the study of multimodal communication.
Most modern and post-modern poems have developed a post-metrical idea of lyrical prosody that employs rhythmical features of everyday language and prose instead of a strict adherence to rhyme and metrical schemes. This development is subsumed under the term free verse prosody. We present our methodology for the large-scale analysis of modern and post-modern poetry in both their written form and as spoken aloud by the author. We employ language processing tools to align text and speech, to generate a null-model of how the poem would be spoken by a na{\"\i
This paper presents a tool to investigate the design of multimodal instructions (MIs), i.e., instructions that contain both text and pictures. The benefit of including pictures in information presentation has been established, but the characteristics of those pictures and of their textual counterparts and the rela-tion(s) between them have not been researched in a systematic manner. We present the PAT Work-bench, a tool to store, annotate and retrieve MIs based on a validated coding scheme with currently 42 categories that describe instructions in terms of textual features, pictorial elements, and relations be-tween text and pictures. We describe how the PAT Workbench facilitates collaborative annotation and inter-annotator agreement calculation. Future work on the tool includes expanding its functionality and usability by (i) making the MI annotation scheme dynamic for adding relevant features based on empirical evaluations of the MIs, (ii) implementing algorithms for automatic tagging of MI features, and (iii) implementing automatic MI evaluation algorithms based on results obtained via e.g. crowdsourced assessments of MIs.
The increasing amount of multilingual text collections available in different domains makes its automatic processing essential for the development of a given field. However, standard processing techniques based on statistical clues and keyword searches have clear limitations. Instead, we propose a knowledge-based processing pipeline which overcomes most of the limitations of these techniques. This, in turn, enables direct comparison across texts in different languages without the need of translation. In this paper we show the potential of this approach for semantically indexing multilingual text collections in the history domain. In our experiments we used a version of the Bible translated in four different languages, evaluating the precision of our semantic indexing pipeline and showing its reliability on the cross-lingual text retrieval task.
This paper presents on-going work on creating NLP tools for under-resourced languages from very sparse training data coming from linguistic field work. In this work, we focus on Ingush, a Nakh-Daghestanian language spoken by about 300,000 people in the Russian republics Ingushetia and Chechnya. We present work on morphosyntactic taggers trained on transcribed and linguistically analyzed recordings and dependency parsers using English glosses to project annotation for creating synthetic treebanks. Our preliminary results are promising, supporting the goal of bootstrapping efficient NLP tools with limited or no task-specific annotated data resources available.
This paper gives an overview of the MultiTal project, which aims to create a research infrastructure that ensures long-term distribution of NLP tools descriptions. The goal is to make NLP tools more accessible and usable to end-users of different disciplines. The infrastructure is built on a meta-data scheme modelling and standardising multilingual NLP tools documentation. The model is conceptualised using an OWL ontology. The formal representation of the ontology allows us to automatically generate organised and structured documentation in different languages for each represented tool.
This article describes work on enabling the addition of temporal information to senses of words in linguistic linked open data lexica based on the lemonDia model. Our contribution in this article is twofold. On the one hand, we demonstrate how lemonDia enables the querying of diachronic lexical datasets using OWL-oriented Semantic Web based technologies. On the other hand, we present a preliminary version of an interactive interface intended to help users in creating lexical datasets that model meaning change over time.
(This is the abstract for the submission.) Large-scale comparisons between the poetry of Tang and Song dynasties shed light on how words and expressions were used and shared among the poets. That some words were used only in the Tang poetry and some only in the Song poetry could lead to interesting research in linguistics. That the most frequent colors are different in the Tang and Song poetry provides a trace of the changing social circumstances in the dynasties. Results of the current work link to research topics of lexicography, semantics, and social transitions. We discuss our findings and present our algorithms for efficient comparisons among the poems, which are crucial for completing billion times of comparisons within acceptable time.
The following paper describes the first steps in the development of an ontology for the textbook research discipline. The aim of the project WorldViews is to establish a digital edition focussing on views of the world depicted in textbooks. For this purpose an initial TEI profile has been formalised and tested as a use case to enable the semantical encoding of the resource {`}textbook{'}. This profile shall provide a basic data model describing major facets of the textbook{'}s structure relevant to historians.
In this paper we present a new combination of existing language tools for Polish with a popular data mining platform intended to help researchers from digital humanities perform computational analyses without any programming. The toolset includes RapidMiner Studio, a software solution offering graphical setup of integrated analytical processes and Multiservice, a Web service offering access to several state-of-the-art linguistic tools for Polish. The setting is verified in a simple task of counting frequencies of unknown words in a small corpus.
Real world data differs radically from the benchmark corpora we use in NLP, resulting in large performance drops. The reason for this problem is obvious: NLP models are trained on limited samples from canonical varieties considered standard. However, there are many dimensions, e.g., sociodemographic, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language. In this talk, I review the notion of canonicity, and how it shapes our community{'}s approach to language. I argue for the use of fortuitous data. Fortuitous data is data out there that just waits to be harvested. It includes data which is in plain sight, but is often neglected, and more distant sources like behavioral data, which first need to be refined. They provide additional contexts and a myriad of opportunities to build more adaptive language technology, some of which I will explore in this talk.
Entity linking and semantic parsing have been shown to be crucial to important applications such as question answering and document understanding. These tasks often require structured learning models, which make predictions on multiple interdependent variables. In this talk, I argue that carefully designed structured learning algorithms play a central role in entity linking and semantic parsing tasks. In particular, I will present several new structured learning models for entity linking, which jointly detect mentions and disambiguate entities as well as capture non-textual information. I will then show how to use a staged search procedure to building a state-of-the-art knowledge base question answering system. Finally, if time permits, I will discuss different supervision protocols for training semantic parsers and the value of labeling semantic parses.
This talk presents two NLP systems that were developed for helping disaster victims and rescue workers in the aftermath of large-scale disasters. DISAANA provides answers to questions such as {``}What is in short supply in Tokyo?{''} and displays locations related to each answer on a map. D-SUMM automatically summarizes a large number of disaster related reports concerning a specified area and helps rescue workers to understand disaster situations from a macro perspective. Both systems are publicly available as Web services. In the aftermath of the 2016 Kumamoto Earthquake (M7.0), the Japanese government actually used DISAANA to analyze the situation.
In this paper we present a series of experiments on discriminating between private and corporate accounts on Twitter. We define features based on Twitter metadata, morphosyntactic tags and surface forms, showing that the simple bag-of-words model achieves single best results that can, however, be improved by building a weighted soft ensemble of classifiers based on each feature type. Investigating the time and language dependence of each feature type delivers quite unexpecting results showing that features based on metadata are neither time- nor language-insensitive as the way the two user groups use the social network varies heavily through time and space.
User-generated content presents many challenges for its automatic processing. While many of them do come from out-of-vocabulary effects, others spawn from different linguistic phenomena such as unusual syntax. In this work we present a French three-domain data set made up of question headlines from a cooking forum, game chat logs and associated forums from two popular online games (MINECRAFT {\&} LEAGUE OF LEGENDS). We chose these domains because they encompass different degrees of lexical and syntactic compliance with canonical language. We conduct an automatic and manual evaluation of the difficulties of processing these domains for part-of-speech prediction, and introduce a pilot study to determine whether dependency analysis lends itself well to annotate these data. We also discuss the development cost of our data set.
Information extraction from user-generated text has gained much attention with the growth of the Web.Disaster analysis using information from social media provides valuable, real-time, geolocation information for helping people caught up these in disasters. However, it is not convenient to analyze texts posted on social media because disaster keywords match any texts that contain words. For collecting posts about a disaster from social media, we need to develop a classifier to filter posts irrelevant to disasters. Moreover, because of the nature of social media, we can take advantage of posts that come with GPS information. However, a post does not always refer to an event occurring at the place where it has been posted. Therefore, we propose a new task of classifying whether a flood disaster occurred, in addition to predicting the geolocation of events from user-generated text. We report the annotation of the flood disaster corpus and develop a classifier to demonstrate the use of this corpus for disaster analysis.
We present a data-driven method for determining the veracity of a set of rumorous claims on social media data. Tweets from different sources pertaining to a rumor are processed on three levels: first, factuality values are assigned to each tweet based on four textual cue categories relevant for our journalism use case; these amalgamate speaker support in terms of polarity and commitment in terms of certainty and speculation. Next, the proportions of these lexical cues are utilized as predictors for tweet certainty in a generalized linear regression model. Subsequently, lexical cue proportions, predicted certainty, as well as their time course characteristics are used to compute veracity for each rumor in terms of the identity of the rumor-resolving tweet and its binary resolution value judgment. The system operates without access to extralinguistic resources. Evaluated on the data portion for which hand-labeled examples were available, it achieves .74 F1-score on identifying rumor resolving tweets and .76 F1-score on predicting if a rumor is resolved as true or false.
A major challenge for statistical machine translation (SMT) of Arabic-to-English user-generated text is the prevalence of text written in Arabizi, or Romanized Arabic. When facing such texts, a translation system trained on conventional Arabic-English data will suffer from extremely low model coverage. In addition, Arabizi is not regulated by any official standardization and therefore highly ambiguous, which prevents rule-based approaches from achieving good translation results. In this paper, we improve Arabizi-to-English machine translation by presenting a simple but effective Arabizi-to-Arabic transliteration pipeline that does not require knowledge by experts or native Arabic speakers. We incorporate this pipeline into a phrase-based SMT system, and show that translation quality after automatically transliterating Arabizi to Arabic yields results that are comparable to those achieved after human transliteration.
Name Variation in Community Question Answering Systems Abstract Community question answering systems are forums where users can ask and answer questions in various categories. Examples are Yahoo! Answers, Quora, and Stack Overflow. A common challenge with such systems is that a significant percentage of asked questions are left unanswered. In this paper, we propose an algorithm to reduce the number of unanswered questions in Yahoo! Answers by reusing the answer to the most similar past resolved question to the unanswered question, from the site. Semantically similar questions could be worded differently, thereby making it difficult to find questions that have shared needs. For example, {``}Who is the best player for the Reds?{''} and {``}Who is currently the biggest star at Manchester United?{''} have a shared need but are worded differently; also, {``}Reds{''} and {``}Manchester United{''} are used to refer to the soccer team Manchester United football club. In this research, we focus on question categories that contain a large number of named entities and entity name variations. We show that in these categories, entity linking can be used to identify relevant past resolved questions with shared needs as a given question by disambiguating named entities and matching these questions based on the disambiguated entities, identified entities, and knowledge base information related to these entities. We evaluated our algorithm on a new dataset constructed from Yahoo! Answers. The dataset contains annotated question pairs, (Qgiven, [Qpast, Answer]). We carried out experiments on several question categories and show that an entity-based approach gives good performance when searching for similar questions in entity rich categories.
Using aliases to refer to public figures is one way to make fun of people, to express sarcasm, or even to sidestep legal issues when expressing opinions on social media. However, linking an alias back to the real name is difficult, as it entails phonemic, graphemic, and semantic challenges. In this paper, we propose a phonemic-based approach and inject semantic information to align aliases with politicians{'} Chinese formal names. The proposed approach creates an HMM model for each name to model its phonemes and takes into account document-level pairwise mutual information to capture the semantic relations to the alias. In this work we also introduce two new datasets consisting of 167 phonemic pairs and 279 mixed pairs of aliases and formal names. Experimental results show that the proposed approach models both phonemic and semantic information and outperforms previous work on both the phonemic and mixed datasets with the best top-1 accuracies of 0.78 and 0.59 respectively.
Accurate event detection in social media is very challenging because user generated contents are extremely noisy and sparse in content. Event indicators are generally words or phrases that act as a trigger that help us understand the semantics of the context they occur in. We present a weakly supervised approach that relies on using a single strong event indicator phrase as a seed to acquire a variety of additional event cues. We propose to leverage various types of implicit event indicators, such as props, actors and precursor events, to achieve precise event detection. We experimented with civil unrest events and show that the automatically learnt event indicators are effective in identifying specific types of events.
Stemming is an essential processing step in a wide range of high level text processing applications such as information extraction, machine translation and sentiment analysis. It is used to reduce words to their stems. Many stemming algorithms have been developed for Modern Standard Arabic (MSA). Although Arabic tweets and MSA are closely related and share many characteristics, there are substantial differences between them in lexicon and syntax. In this paper, we introduce a light Arabic stemmer for Arabic tweets. Our results show improvements over the performance of a number of well-known stemmers for Arabic.
Topic modelling techniques such as LDA have recently been applied to speech transcripts and OCR output. These corpora may contain noisy or erroneous texts which may undermine topic stability. Therefore, it is important to know how well a topic modelling algorithm will perform when applied to noisy data. In this paper we show that different types of textual noise can have diverse effects on the stability of topic models. On the other hand, topic model stability is not consistent with the same type but different levels of noise. We introduce a dictionary filtering approach to address this challenge, with the result that a topic model with the correct number of topics is always identified across different levels of noise.
Postmarketing surveillance (PMS) has the vital aim to monitor effects of drugs after release for use by the general population, but suffers from under-reporting and limited coverage. Automatic methods for detecting drug effect reports, especially for social media, could vastly increase the scope of PMS. Very few automatic PMS methods are currently available, in particular for the messy text types encountered on Twitter. In this paper we describe first results for developing PMS methods specifically for tweets. We describe the corpus of 125,669 tweets we have created and annotated to train and test the tools. We find that generic tools perform well for tweet-level language identification and tweet-level sentiment analysis (both 0.94 F1-Score). For detection of effect mentions we are able to achieve 0.87 F1-Score, while effect-level adverse-vs.-beneficial analysis proves harder with an F1-Score of 0.64. Among other things, our results indicate that MetaMap semantic types provide a very promising basis for identifying drug effect mentions in tweets.
In social networks services like Twitter, users are overwhelmed with huge amount of social data, most of which are short, unstructured and highly noisy. Identifying accurate information from this huge amount of data is indeed a hard task. Classification of tweets into organized form will help the user to easily access these required information. Our first contribution relates to filtering parts of speech and preprocessing this kind of highly noisy and short data. Our second contribution concerns the named entity recognition (NER) in tweets. Thus, the adaptation of existing language tools for natural languages, noisy and not accurate language tweets, is necessary. Our third contribution involves segmentation of hashtags and a semantic enrichment using a combination of relations from WordNet, which helps the performance of our classification system, including disambiguation of named entities, abbreviations and acronyms. Graph theory is used to cluster the words extracted from WordNet and tweets, based on the idea of connected components. We test our automatic classification system with four categories: politics, economy, sports and the medical field. We evaluate and compare several automatic classification systems using part or all of the items described in our contributions and found that filtering by part of speech and named entity recognition dramatically increase the classification precision to 77.3 {\%}. Moreover, a classification system incorporating segmentation of hashtags and semantic enrichment by two relations from WordNet, synonymy and hyperonymy, increase classification precision up to 83.4 {\%}.
Text normalization techniques based on rules, lexicons or supervised training requiring large corpora are not scalable nor domain interchangeable, and this makes them unsuitable for normalizing user-generated content (UGC). Current tools available for Brazilian Portuguese make use of such techniques. In this work we propose a technique based on distributed representation of words (or word embeddings). It generates continuous numeric vectors of high-dimensionality to represent words. The vectors explicitly encode many linguistic regularities and patterns, as well as syntactic and semantic word relationships. Words that share semantic similarity are represented by similar vectors. Based on these features, we present a totally unsupervised, expandable and language and domain independent method for learning normalization lexicons from word embeddings. Our approach obtains high correction rate of orthographic errors and internet slang in product reviews, outperforming the current available tools for Brazilian Portuguese.
Text normalization is the task of transforming lexical variants to their canonical forms. We model the problem of text normalization as a character-level sequence to sequence learning problem and present a neural encoder-decoder model for solving it. To train the encoder-decoder model, many sentences pairs are generally required. However, Japanese non-standard canonical pairs are scarce in the form of parallel corpora. To address this issue, we propose a method of data augmentation to increase data size by converting existing resources into synthesized non-standard forms using handcrafted rules. We conducted an experiment to demonstrate that the synthesized corpus contributes to stably train an encoder-decoder model and improve the performance of Japanese text normalization.
This paper describes the ASU system submitted in the COLING W-NUT 2016 Twitter Named Entity Recognition (NER) task. We present an experimental study on applying deep learning to extracting named entities (NEs) from tweets. We built two Long Short-Term Memory (LSTM) models for the task. The first model was built to extract named entities without types while the second model was built to extract and then classify them into 10 fine-grained entity classes. In this effort, we show detailed experimentation results on the effectiveness of word embeddings, brown clusters, part-of-speech (POS) tags, shape features, gazetteers, and local context for the tweet input vector representation to the LSTM model. Also, we present a set of experiments, to better design the network parameters for the Twitter NER task. Our system was ranked the fifth out of ten participants with a final f1-score for the typed classes of 39{\%} and 55{\%} for the non typed ones.
The paper is a corpus study of the factors involved in disambiguating potential scope ambiguity in sentences with negation and universal quantifier, such as {``}I don{'}t want talk to all these people{''}, which can alternatively mean {`}I don{'}t want to talk to any of these people{'} and {`}I don{'}t want to talk to some of these people{'}. The relevant factors are demonstrated to be largely different from those involved in disambiguating lexical polysemy. They include the syntactic function of the constituent containing {``}all{''} quantifier (subject, direct complement, adjunct), as well as the deepness of its embedding; the status of the main predicate and {``}all{''} constituent with respect to the information structure of the 6utterance (topic vs. focus, given vs. new information); pragmatic implicatures pertaining to the situations described in the utterances.
In this talk, I will outline a range of challenges presented by multiword expressions in terms of (lexicalist) precision grammar engineering, and different strategies for accommodating those challenges, in an attempt to strike the right balance in terms of generalisation and over- and under-generation.
Microsyntactic linguistic units, such as syntactic idioms and non-standard syntactic constructions, are poorly represented in linguistic resources, mostly because the former are elements occupying an intermediate position between the lexicon and the grammar and the latter are too specific to be routinely tackled by general grammars. Consequently, many such units produce substantial gaps in systems intended to solve sophisticated computational linguistics tasks, such as parsing, deep semantic analysis, question answering, machine translation, or text generation. They also present obstacles for applying advanced techniques to these tasks, such as machine learning. The paper discusses an approach aimed at bridging such gaps, focusing on the development of monolingual and multilingual corpora where microsyntactic units are to be tagged.
An excellent example of a phenomenon bridging a lexicon and a grammar is provided by grammaticalized alternations (e.g., passivization, reflexivity, and reciprocity): these alternations represent productive grammatical processes which are, however, lexically determined. While grammaticalized alternations keep lexical meaning of verbs unchanged, they are usually characterized by various changes in their morphosyntactic structure. In this contribution, we demonstrate on the example of reciprocity and its representation in the valency lexicon of Czech verbs, VALLEX how a linguistic description of complex (and still systemic) changes characteristic of grammaticalized alternations can benefit from an integration of grammatical rules into a valency lexicon. In contrast to other types of grammaticalized alternations, reciprocity in Czech has received relatively little attention although it closely interacts with various linguistic phenomena (e.g., with light verbs, diatheses, and reflexivity).
Language-endowed intelligent agents benefit from leveraging lexical knowledge falling at different points along a spectrum of compositionality. This means that robust computational lexicons should include not only the compositional expectations of argument-taking words, but also non-compositional collocations (idioms), semi-compositional collocations that might be difficult for an agent to interpret (e.g., standard metaphors), and even collocations that could be compositionally analyzed but are so frequently encountered that recording their meaning increases the efficiency of interpretation. In this paper we argue that yet another type of string-to-meaning mapping can also be useful to intelligent agents: remembered semantic analyses of actual text inputs. These can be viewed as super-specific multi-word expressions whose recorded interpretations mimic a person{'}s memories of knowledge previously learned from language input. These differ from typical annotated corpora in two ways. First, they provide a full, context-sensitive semantic interpretation rather than select features. Second, they are are formulated in the ontologically-grounded metalanguage used in a particular agent environment, meaning that the interpretations contribute to the dynamically evolving cognitive capabilites of agents configured in that environment.
Universal Dependencies is an initiative to develop cross-linguistically consistent grammatical annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning and parsing research from a language typology perspective. It assumes a dependency-based approach to syntax and a lexicalist approach to morphology, which together entail that the fundamental units of grammatical annotation are words. Words have properties captured by morphological annotation and enter into relations captured by syntactic annotation. Moreover, priority is given to relations between lexical content words, as opposed to grammatical function words. In this position paper, I discuss how this approach allows us to capture similarities and differences across typologically diverse languages.
Human communication is a multimodal activity, involving not only speech and written expressions, but intonation, images, gestures, visual clues, and the interpretation of actions through perception. In this paper, we describe the design of a multimodal lexicon that is able to accommodate the diverse modalities that present themselves in NLP applications. We have been developing a multimodal semantic representation, VoxML, that integrates the encoding of semantic, visual, gestural, and action-based features associated with linguistic expressions.
Valency slot filling is a semantic glue, which brings together the meanings of words. As regards the position of an argument in the dependency structure with respect to its predicate, there exist three types of valency filling: active (canonical), passive, and discontinuous. Of these, the first type is studied much better than the other two. As a rule, canonical actants are unambiguously marked in the syntactic structure, and each actant corresponds to a unique syntactic position. Linguistic information on which syntactic function an actant might have (subject, direct or indirect object), what its morphological form should be and which prepositions or conjunctions it requires, can be given in the lexicon in the form of government patterns, subcategorization frames, or similar data structures. We concentrate on non-canonical cases of valency filling in Russian, which are characteristic of non-verbal parts of speech, such as adverbs, adjectives, and particles, in the first place. They are more difficult to handle than canonical ones, because the position of the actant in the tree is governed by more complicated rules. A valency may be filled by expressions occupying different syntactic positions, and a syntactic position may accept expressions filling different valencies of the same word. We show how these phenomena can be processed in a semantic analyzer.
Verbenet is a French lexicon developed by {``}translation{''} of its English counterpart {---} VerbNet (Kipper-Schuler, 2005){---}and treatment of the specificities of French syntax (Pradet et al., 2014; Danlos et al., 2016). One difficulty encountered in its development springs from the fact that the list of (potentially numerous) frames has no internal organization. This paper proposes a type system for frames that shows whether two frames are variants of a given alternation. Frame typing facilitates coherence checking of the resource in a {``}virtuous circle{''}. We present the principles underlying a program we developed and used to automatically type frames in VerbeNet. We also show that our system is portable to other languages.
We present an attempt to automatically identify Czech deverbative nouns using several methods that use large corpora as well as existing lexical resources. The motivation for the task is to extend a verbal valency (i.e., predicate-argument) lexicon by adding nouns that share the valency properties with the base verb, assuming their properties can be derived (even if not trivially) from the underlying verb by deterministic grammatical rules. At the same time, even in inflective languages, not all deverbatives are simply created from their underlying base verb by regular lexical derivation processes. We have thus developed hybrid techniques that use both large parallel corpora and several standard lexical resources. Thanks to the use of parallel corpora, the resulting sets contain also synonyms, which the lexical derivation rules cannot get. For evaluation, we have manually created a small, 100-verb gold data since no such dataset was initially available for Czech.
We present an interdisciplinary study on the interaction between the interpretation of noun-noun deverbal compounds (DCs; e.g., task assignment) and the morphosyntactic properties of their deverbal heads in English. Underlying hypotheses from theoretical linguistics are tested with tools and resources from computational linguistics. We start with Grimshaw{'}s (1990) insight that deverbal nouns are ambiguous between argument-supporting nominal (ASN) readings, which inherit verbal arguments (e.g., the assignment of the tasks), and the less verbal and more lexicalized Result Nominal and Simple Event readings (e.g., a two-page assignment). Following Grimshaw, our hypothesis is that the former will realize object arguments in DCs, while the latter will receive a wider range of interpretations like root compounds headed by non-derived nouns (e.g., chocolate box). Evidence from a large corpus assisted by machine learning techniques confirms this hypothesis, by showing that, besides other features, the realization of internal arguments by deverbal heads outside compounds (i.e., the most distinctive ASN-property in Grimshaw 1990) is a good predictor for an object interpretation of non-heads in DCs.
We show how to turn a large-scale syntactic dictionary into a dependency-based unification grammar where each piece of lexical information calls a separate rule, yielding a super granular grammar. Subcategorization, raising and control verbs, auxiliaries and copula, passivization, and tough-movement are discussed. We focus on the semantics-syntax interface and offer a new perspective on syntactic structure.
This paper presents our ongoing work on compilation of English multi-word expression (MWE) lexicon. We are especially interested in collecting flexible MWEs, in which some other components can intervene the expression such as {``}a number of{''} vs {``}a large number of{''} where a modifier of {``}number{''} can be placed in the expression and inherit the original meaning. We fiest collect possible candidates of flexible English MWEs from the web, and annotate all of their occurrences in the Wall Street Journal portion of Ontonotes corpus. We make use of word dependency strcuture information of the sentences converted from the phrase structure annotation. This process enables semi-automatic annotation of MWEs in the corpus and simultanaously produces the internal and external dependency representation of flexible MWEs.
The paper presents a contrastive description of reflexive possessive pronouns {``}sv{\r{u}}j{''} in Czech and {``}svoj{''} in Russian. The research concerns syntactic, semantic and pragmatic aspects. With our analysis, we shed a new light on the already investigated issue, which comes from a detailed comparison of the phenomenon of possessive reflexivization in two typologically and genetically similar languages. We show that whereas in Czech, the possessive reflexivization is mostly limited to syntactic functions and does not go beyond the grammar, in Russian it gets additional semantic meanings and moves substan-tially towards the lexicon. The obtained knowledge allows us to explain heretofore unclear marginal uses of reflexives in each language.
A specific language as used by different speakers and in different situations has a number of more or less distant varieties. Extending the notion of non-standard language to varieties that do not fit an explicitly or implicitly assumed norm or pattern, we look for methods and tools that could be applied to this domain. The needs start from the theoretical side: categories usable for the analysis of non-standard language are not readily available, and continue to methods and tools required for its detection and diagnostics. A general discussion of issues related to non-standard language is followed by two case studies. The first study presents a taxonomy of morphosyntactic categories as an attempt to analyse non-standard forms produced by non-native learners of Czech. The second study focusses on the role of a rule-based grammar and lexicon in the process of building and using a parsebank.
We propose a classification framework for semantic type identification of compounds in Sanskrit. We broadly classify the compounds into four different classes namely, \textit{Avyay{\=\i}bh{\=a}va}, \textit{Tatpuru·π£a}, \textit{Bahuvr{\=\i}hi} and \textit{Dvandva}. Our classification is based on the traditional classification system followed by the ancient grammar treatise \textit{Ad·π£·π≠{\=a}dhy{\=a}y{\=\i}}, proposed by P{\=a}·πáini 25 centuries back. We construct an elaborate features space for our system by combining conditional rules from the grammar \textit{Ad·π£·π≠{\=a}dhy{\=a}y{\=\i}}, semantic relations between the compound components from a lexical database \textit{Amarako·π£a} and linguistic structures from the data using Adaptor Grammars. Our in-depth analysis of the feature space highlight inadequacy of \textit{Ad·π£·π≠{\=a}dhy{\=a}y{\=\i}}, a generative grammar, in classifying the data samples. Our experimental results validate the effectiveness of using lexical databases as suggested by Amba Kulkarni and Anil Kumar, and put forward a new research direction by introducing linguistic patterns obtained from Adaptor grammars for effective identification of compound type. We utilise an ensemble based approach, specifically designed for handling skewed datasets and we {\%}and Experimenting with various classification methods, we achieve an overall accuracy of 0.77 using random forest classifiers.
Grapheme-to-Phoneme (G2P) conversion is the task of predicting the pronunciation of a word given its graphemic or written form. It is a highly important part of both automatic speech recognition (ASR) and text-to-speech (TTS) systems. In this paper, we evaluate seven G2P conversion approaches: Adaptive Regularization of Weight Vectors (AROW) based structured learning (S-AROW), Conditional Random Field (CRF), Joint-sequence models (JSM), phrase-based statistical machine translation (PBSMT), Recurrent Neural Network (RNN), Support Vector Machine (SVM) based point-wise classification, Weighted Finite-state Transducers (WFST) on a manually tagged Myanmar phoneme dictionary. The G2P bootstrapping experimental results were measured with both automatic phoneme error rate (PER) calculation and also manual checking in terms of voiced/unvoiced, tones, consonant and vowel errors. The result shows that CRF, PBSMT and WFST approaches are the best performing methods for G2P conversion on Myanmar language.
Named Entity Recognition (NER) is the task of classifying or labelling atomic elements in the text into categories such as Person, Location or Organisation. For Arabic language, recognizing named entities is a challenging task because of the complexity and the unique characteristics of this language. In addition, most of the previous work focuses on Modern Standard Arabic (MSA), however, recognizing named entities in social media is becoming more interesting these days. Dialectal Arabic (DA) and MSA are both used in social media, which is deemed as another challenging task. Most state-of-the-art Arabic NER systems count heavily on handcrafted engineering features and lexicons which is time consuming. In this paper, we introduce a novel neural network architecture which benefits both from character- and word-level representations automatically, by using combination of bidirectional LSTM and Conditional Random Field (CRF), eliminating the need for most feature engineering. Moreover, our model relies on unsupervised word representations learned from unannotated corpora. Experimental results demonstrate that our model achieves state-of-the-art performance on publicly available benchmark for Arabic NER for social media and surpassing the previous system by a large margin.
In recent years there has been a lot of interest in cross-lingual parsing for developing treebanks for languages with small or no annotated treebanks. In this paper, we explore the development of a cross-lingual transfer parser from Hindi to Bengali using a Hindi parser and a Hindi-Bengali parallel corpus. A parser is trained and applied to the Hindi sentences of the parallel corpus and the parse trees are projected to construct probable parse trees of the corresponding Bengali sentences. Only about 14{\%} of these trees are complete (transferred trees contain all the target sentence words) and they are used to construct a Bengali parser. We relax the criteria of completeness to consider well-formed trees (43{\%} of the trees) leading to an improvement. We note that the words often do not have a one-to-one mapping in the two languages but considering sentences at the chunk-level results in better correspondence between the two languages. Based on this we present a method to use chunking as a preprocessing step and do the transfer on the chunk trees. We find that about 72{\%} of the projected parse trees of Bengali are now well-formed. The resultant parser achieves significant improvement in both Unlabeled Attachment Score (UAS) as well as Labeled Attachment Score (LAS) over the baseline word-level transferred parser.
Currently, corpus based-similarity, string-based similarity, and knowledge-based similarity techniques are used to compare short phrases. However, no work has been conducted on the similarity of phrases in Sinhala language. In this paper, we present a hybrid methodology to compute the similarity between two Sinhala sentences using a Semantic Similarity Measurement technique (corpus-based similarity measurement plus knowledge-based similarity measurement) that makes use of word order information. Since Sinhala WordNet is still under construction, we used lexical resources in performing this semantic similarity calculation. Evaluation using 4000 sentence pairs yielded an average MSE of 0.145 and a Pearson correla-tion factor of 0.832.
This paper focuses on the generation of case markers for free word order languages that use case markers as phrasal clitics for marking the relationship between the dependent-noun and its head. The generation of such clitics becomes essential task especially when translating from fixed word order languages where syntactic relations are identified by the positions of the dependent-nouns. To address the problem of missing markers on source-side, artificial markers are added in source to improve alignments with its target counterparts. Up to 1 BLEU point increase is observed over the baseline on different test sets for English-to-Urdu.
Action verbs are one of the frequently occurring linguistic elements in any given natural language as the speakers use them during every linguistic intercourse. However, each language expresses action verbs in its own inherently unique manner by categorization. One verb can refer to several interpretations of actions and one action can be expressed by more than one verb. The inter-language and intra-language variations create ambiguity for the translation of languages from the source language to target language with respect to action verbs. IMAGACT is a corpus-based ontological platform of action verbs translated from prototypic animated images explained in English and Italian as meta-languages. In this paper, we are presenting the issues and challenges in translating action verbs of Indian languages as target and English as source language by observing the animated images. Among the ten Indian languages which have been annotated so far on the platform are Sanskrit, Hindi, Urdu, Odia (Oriya), Bengali, Manipuri, Tamil, Assamese, Magahi and Marathi. Out of them, Manipuri belongs to the Sino-Tibetan, Tamil comes off the Dravidian and the rest owe their genesis to the Indo-Aryan language family. One of the issues is that the one-word morphological English verbs are translated into most of the Indian languages as verbs having more than one-word form; for instance as in the case of conjunct, compound, serial verbs and so on. We are further presenting a cross-lingual comparison of action verbs among Indian languages. In addition, we are also dealing with the issues in disambiguating animated images by the L1 native speakers using competence-based judgements and the theoretical and machine translation implications they bear.
The automatic analysis of emotions conveyed in social media content, e.g., tweets, has many beneficial applications. In the Philippines, one of the most disaster-prone countries in the world, such methods could potentially enable first responders to make timely decisions despite the risk of data deluge. However, recognising emotions expressed in Philippine-generated tweets, which are mostly written in Filipino, English or a mix of both, is a non-trivial task. In order to facilitate the development of natural language processing (NLP) methods that will automate such type of analysis, we have built a corpus of tweets whose predominant emotions have been manually annotated by means of crowdsourcing. Defining measures ensuring that only high-quality annotations were retained, we have produced a gold standard corpus of 1,146 emotion-labelled Filipino and English tweets. We validate the value of this manually produced resource by demonstrating that an automatic emotion-prediction method based on the use of a publicly available word-emotion association lexicon was unable to reproduce the labels assigned via crowdsourcing. While we are planning to make a few extensions to the corpus in the near future, its current version has been made publicly available in order to foster the development of emotion analysis methods based on advanced Filipino and English NLP.
In this paper, we describe the results of sentiment analysis on tweets in three Indian languages {--} Bengali, Hindi, and Tamil. We used the recently released SAIL dataset (Patra et al., 2015), and obtained state-of-the-art results in all three languages. Our features are simple, robust, scalable, and language-independent. Further, we show that these simple features provide better results than more complex and language-specific features, in two separate classification tasks. Detailed feature analysis and error analysis have been reported, along with learning curves for Hindi and Bengali.
In Machine Translation, divergence is one of the major barriers which plays a deciding role in determining the efficiency of the system at hand. Translation divergences originate when there is structural discrepancies between the input and the output languages. It can be of various types based on the issues we are addressing to such as linguistic, cultural, communicative and so on. Owing to the fact that two languages owe their origin to different language families, linguistic divergences emerge. The present study attempts at categorizing different types of linguistic divergences: the lexical-semantic and syntactic. In addition, it also helps identify and resolve the divergent linguistic features between English as source language and Bhojpuri as target language pair. Dorr{'}s theoretical framework (1994, 1994a) has been followed in the classification and resolution procedure. Furthermore, so far as the methodology is concerned, we have adhered to the Dorr{'}s Lexical Conceptual Structure for the resolution of divergences. This research will prove to be beneficial for developing efficient MT systems if the mentioned factors are incorporated considering the inherent structural constraints between source and target languages.ated considering the inherent structural constraints between SL and TL pairs.
In this paper, we discuss our creation of a web corpus of spoken Hindi (COSH), one of the Indo-Aryan languages spoken mainly in the Indian subcontinent. We also point out notable problems we{'}ve encountered in the web corpus and the special concordancer. After observing the kind of technical problems we encountered, especially regarding annotation tagged by Shiva Reddy{'}s tagger, we argue how they can be solved when using COSH for linguistic studies. Finally, we mention the kinds of linguistic research that we non-native speakers of Hindi can do using the corpus, especially in pragmatics and semantics, and from a comparative viewpoint to Japanese.
A sentence aligned parallel corpus is an important prerequisite in statistical machine translation. However, manual creation of such a parallel corpus is time consuming, and requires experts fluent in both languages. Automatic creation of a sentence aligned parallel corpus using parallel text is the solution to this problem. In this paper, we present the first ever empirical evaluation carried out to identify the best method to automatically create a sentence aligned Sinhala-Tamil parallel corpus. Annual reports from Sri Lankan government institutions were used as the parallel text for aligning. Despite both Sinhala and Tamil being under-resourced languages, we were able to achieve an F-score value of 0.791 using a hybrid approach that makes use of a bilingual dictionary.
Acquiring labeled speech for low-resource languages is a difficult task in the absence of native speakers of the language. One solution to this problem involves collecting speech transcriptions from crowd workers who are foreign or non-native speakers of a given target language. From these mismatched transcriptions, one can derive probabilistic phone transcriptions that are defined over the set of all target language phones using a noisy channel model. This paper extends prior work on deriving probabilistic transcriptions (PTs) from mismatched transcriptions by 1) modelling multilingual channels and 2) introducing a clustering-based phonetic mapping technique to improve the quality of PTs. Mismatched crowdsourcing for multilingual channels has certain properties of projection mapping, e.g., it can be interpreted as a clustering based on singular value decomposition of the segment alignments. To this end, we explore the use of distinctive feature weights, lexical tone confusions, and a two-step clustering algorithm to learn projections of phoneme segments from mismatched multilingual transcriber languages to the target language. We evaluate our techniques using mismatched transcriptions for Cantonese speech acquired from native English and Mandarin speakers. We observe a 5-9{\%} relative reduction in phone error rate for the predicted Cantonese phone transcriptions using our proposed techniques compared with the previous PT method.
The paper describes a new tagset for the morphological disambiguation of Sanskrit, and compares the accuracy of two machine learning methods (Conditional Random Fields, deep recurrent neural networks) for this task, with a special focus on how to model the lexicographic information. It reports a significant improvement over previously published results.
In Cross-Language Information Retrieval, finding the appropriate translation of the source language query has always been a difficult problem to solve. We propose a technique towards solving this problem with the help of multilingual word clusters obtained from multilingual word embeddings. We use word embeddings of the languages projected to a common vector space on which a community-detection algorithm is applied to find clusters such that words that represent the same concept from different languages fall in the same group. We utilize these multilingual word clusters to perform query translation for Cross-Language Information Retrieval for three languages - English, Hindi and Bengali. We have experimented with the FIRE 2012 and Wikipedia datasets and have shown improvements over several standard methods like dictionary-based method, a transliteration-based model and Google Translate.
Neural machine translation (NMT) models have recently been shown to be very successful in machine translation (MT). The use of LSTMs in machine translation has significantly improved the translation performance for longer sentences by being able to capture the context and long range correlations of the sentences in their hidden layers. The attention model based NMT system (Bahdanau et al., 2014) has become the state-of-the-art, performing equal or better than other statistical MT approaches. In this paper, we wish to study the performance of the attention-model based NMT system (Bahdanau et al., 2014) on the Indian language pair, Hindi and Bengali, and do an analysis on the types or errors that occur in case when the languages are morphologically rich and there is a scarcity of large parallel training corpus. We then carry out certain post-processing heuristic steps to improve the quality of the translated statements and suggest further measures that can be carried out.
This paper presents a new comprehensive multi-level Part-Of-Speech tag set and a Support Vector Machine based Part-Of-Speech tagger for the Sinhala language. The currently available tag set for Sinhala has two limitations: the unavailability of tags to represent some word classes and the lack of tags to capture inflection based grammatical variations of words. The new tag set, presented in this paper overcomes both of these limitations. The accuracy of available Sinhala Part-Of-Speech taggers, which are based on Hidden Markov Models, still falls far behind state of the art. Our Support Vector Machine based tagger achieved an overall accuracy of 84.68{\%} with 59.86{\%} accuracy for unknown words and 87.12{\%} for known words, when the test set contains 10{\%} of unknown words.
Multilingual language processing tasks like statistical machine translation and cross language information retrieval rely mainly on availability of accurate parallel corpora. Manual construction of such corpus can be extremely expensive and time consuming. In this paper we present a simple yet efficient method to generate huge amount of reasonably accurate parallel corpus with minimal user efforts. We utilize the availability of large number of English books and their corresponding translations in other languages to build parallel corpus. Optical Character Recognizing systems are used to digitize such books. We propose a robust dictionary based parallel corpus generation system for alignment of multilingual text at different levels of granularity (sentence, paragraphs, etc). We show the performance of our proposed method on a manually aligned dataset of 300 Hindi-English sentences and 100 English-Malayalam sentences.
We present a research on learning Indonesian-Chinese bilingual lexicon using monolingual word embedding and bilingual seed lexicons to build shared bilingual word embedding space. We take the first attempt to examine the impact of different monolingual signals for the choice of seed lexicons on the model performance. We found that although monolingual signals alone do not seem to outperform signals coverings all words, the significant improvement for learning word translation of the same signal types may suggest that linguistic features possess value for further study in distinguishing the semantic margins of the shared word embedding space.
In this paper, we present how we generated two rich online bilingual dictionaries {---} Lao-French and French-Lao {---} from unstructured dictionaries in Microsoft Word files. Then we shortly discuss the possible reuse of the lexical data for Machine Translation projects.
This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent QNetworks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state. 
We consider the gap between user demands for seamless handling of complex interactions, and recent advances in dialog state tracking technologies. We propose a new statistical approach, Task Lineage-based Dialog State Tracking (TL-DST), aimed at seamlessly orchestrating multiple tasks with complex goals across multiple domains in continuous interaction. TL-DST consists of three components: (1) task frame parsing, (2) context fetching and (3) task state update (for which TL-DST takes advantage of previous work in dialog state tracking). There is at present very little publicly available multi-task, complex goal dialog data; however, as a proof of concept, we applied TL-DST to the Dialog State Tracking Challenge (DSTC) 2 data, resulting in state-of-the-art performance. TLDST also outperforms the DSTC baseline tracker on a set of pseudo-real datasets involving multiple tasks with complex goals which were synthesized using DSTC3 data. 
Speaker intent detection and semantic slot Ô¨Ålling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot Ô¨Ålling, and language modeling. The neural network model keeps updating the intent prediction as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot Ô¨Ålling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input. 
The use of irony and sarcasm in social media allows us to study them at scale for the Ô¨Årst time. However, their diversity has made it difÔ¨Åcult to construct a high-quality corpus of sarcasm in dialogue. Here, we describe the process of creating a largescale, highly-diverse corpus of online debate forums dialogue, and our novel methods for operationalizing classes of sarcasm in the form of rhetorical questions and hyperbole. We show that we can use lexico-syntactic cues to reliably retrieve sarcastic utterances with high accuracy. To demonstrate the properties and quality of our corpus, we conduct supervised learning experiments with simple features, and show that we achieve both higher precision and F than previous work on sarcasm in debate forums dialogue. We apply a weakly-supervised linguistic pattern learner and qualitatively analyze the linguistic differences in each class. 
Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, such as those found in reader comments in on-line news. To date, however, there has been little discussion of what these summaries should be like and a lack of humanauthored exemplars, quite likely because writing summaries of this kind of interchange is so difÔ¨Åcult. In this paper we propose one type of reader comment summary ‚Äì the conversation overview summary ‚Äì that aims to capture the key argumentative content of a reader comment conversation. We describe a method we have developed to support humans in authoring conversation overview summaries and present a publicly available corpus ‚Äì the Ô¨Årst of its kind ‚Äì of news articles plus comment sets, each multiply annotated, according to our method, with conversation overview summaries. 
Advisory Board ‚óè Justine Cassell, Associate Dean of the School of Computer Science for Technology Strategy and Impact, Carnegie Mellon University 
Proceedings of the SIGDIAL 2016 Conference, page 54, Los Angeles, USA, 13-15 September 2016. c 2016 Association for Computational Linguistics 
In this paper, we describe a system that reacts to both possible system breakdowns and low user engagement with a set of conversational strategies. These general strategies reduce the number of inappropriate responses and produce better user engagement. We also found that a system that reacts to both possible system breakdowns and low user engagement is rated by both experts and non-experts as having better overall user engagement compared to a system that only reacts to possible system breakdowns. We argue that for non-task-oriented systems we should optimize on both system response appropriateness and user engagement. We also found that apart from making the system response appropriate, funny and provocative responses can also lead to better user engagement. On the other hand, short appropriate responses, such as ‚ÄúYes‚Äù or ‚ÄúNo‚Äù can lead to decreased user engagement. We will use these Ô¨Åndings to further improve our system. 
Providing customer support through social media channels is gaining increasing popularity. In such a context, automatic detection and analysis of the emotions expressed by customers is important, as is identiÔ¨Åcation of the emotional techniques (e.g., apology, empathy, etc.) in the responses of customer service agents. Result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. In this paper, we show that, in addition to text based turn features, dialogue features can signiÔ¨Åcantly improve detection of emotions in social media customer service dialogues and help predict emotional techniques used by customer service agents. 
In this work, we investigate whether the cultural idiosyncrasies found in humanhuman interaction may be transferred to human-computer interaction. With the aim of designing a culture-sensitive dialogue system, we designed a user study creating a dialogue in a domain that has the potential capacity to reveal cultural differences. The dialogue contains different options for the system output according to cultural differences. We conducted a survey among Germans and Japanese to investigate whether the supposed differences may be applied in human-computer interaction. Our results show that there are indeed differences, but not all results are consistent with the cultural models. 
The generalisation of dialogue state tracking to unseen dialogue states can be very challenging. In a slot-based dialogue system, dialogue states lie in discrete space where distances between states cannot be computed. Therefore, the model parameters to track states unseen in the training data can only be estimated from more general statistics, under the assumption that every dialogue state will have the same underlying state tracking behaviour. However, this assumption is not valid. For example, two values, whose associated concepts have different ASR accuracy, may have different state tracking performance. Therefore, if the ASR performance of the concepts related to each value can be estimated, such estimates can be used as general features. The features will help to relate unseen dialogue states to states seen in the training data with similar ASR performance. Furthermore, if two phonetically similar concepts have similar ASR performance, the features extracted from the phonetic structure of the concepts can be used to improve generalisation. In this paper, ASR and phonetic structurerelated features are used to improve the dialogue state tracking generalisation to unseen states of an environmental control system developed for dysarthric speakers. 
This paper introduces a subtask of entity linking, called character identiÔ¨Åcation, that maps mentions in multiparty conversation to their referent characters. Transcripts of TV shows are collected as the sources of our corpus and automatically annotated with mentions by linguistically-motivated rules. These mentions are manually linked to their referents through crowdsourcing. Our corpus comprises 543 scenes from two TV shows, and shows the inter-annotator agreement of Œ∫ = 79.96. For statistical modeling, this task is reformulated as coreference resolution, and experimented with a state-of-the-art system on our corpus. Our best model gives a purity score of 69.21 on average, which is promising given the challenging nature of this task and our corpus. 
In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems. First, we show that, on summary state and action spaces, deep Reinforcement Learning (RL) outperforms Gaussian Processes methods. Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge, and domain expertise. In order to remove the need to deÔ¨Åne such summary spaces, we show that deep RL can also be trained efÔ¨Åciently on the original state and action spaces. Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment. We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efÔ¨Åciently. Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actorcritic deep learner is considerably bootstrapped from a combination of supervised and batch RL. In addition, convergence to an optimal policy is signiÔ¨Åcantly sped up compared to other deep RL methods initialized on the data with batch RL. All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2 (DSTC2) dataset. 
A training and test set for a dialogue system in the form of linked questions and responses is translated from English into Tamil. Accuracy of identifying an appropriate response in Tamil is 79%, compared to the English accuracy of 89%, suggesting that translation can be useful to start up a dialogue system. Machine translation of Tamil inputs into English also results in 79% accuracy. However, machine translation of the English training data into Tamil results in a drop in accuracy to 54% when tested on manually authored Tamil, indicating that there is still a large gap before machine translated dialogue systems can interact with human users. 
We investigate the manual and automatic annotation of PDTB discourse relations in student essays, a novel domain that is not only learning-based and argumentative, but also noisy with surface errors and deeper coherency issues. We discuss methodological complexities it poses for the task. We present descriptive statistics and compare relation distributions in related corpora. We compare automatic discourse parsing performance to prior work. 
Two heuristic rules that transform Rhetorical Structure Theory discourse trees into discourse dependency trees (DDTs) have recently been proposed (Hirao et al., 2013; Li et al., 2014), but these rules derive signiÔ¨Åcantly different DDTs because their conversion schemes on multinuclear relations are not identical. This paper reveals the difference among DDT formats with respect to the following questions: (1) How complex are the formats from a dependency graph theoretic point of view? (2) Which formats are analyzed more accurately by automatic parsers? (3) Which are more suitable for text summarization task? Experimental results showed that Hirao‚Äôs conversion rule produces DDTs that are more useful for text summarization, even though it derives more complex dependency structures. 
Although human-written summaries of documents tend to involve signiÔ¨Åcant edits to the source text, most automated summarizers are extractive and select sentences verbatim. In this work we examine how elementary discourse units (EDUs) from Rhetorical Structure Theory can be used to extend extractive summarizers to produce a wider range of human-like summaries. Our analysis demonstrates that EDU segmentation is effective in preserving human-labeled summarization concepts within sentences and also aligns with near-extractive summaries constructed by news editors. Finally, we show that using EDUs as units of content selection instead of sentences leads to stronger summarization performance in near-extractive scenarios, especially under tight budgets. 
Choosing an appropriate way for a spoken dialog system to initiate a conversation is a challenging problem, and, if done incorrectly, can negatively affect people‚Äôs performance on other important tasks. We describe the results of a study in which participants play a game and are interrupted by spoken notiÔ¨Åcations in different styles. We compare people‚Äôs perceptions of the notiÔ¨Åcation styles, as well as their effect on task performance. The different notiÔ¨Åcations include manipulations of pre-notiÔ¨Åcations and information about the urgency of the task. We Ô¨Ånd that prenotiÔ¨Åcations help people respond signiÔ¨Åcantly faster to urgent tasks, and that 43% of people, more than in any other category, prefer a notiÔ¨Åcation style in which the notiÔ¨Åcation begins by stating the urgency of the task. 
We have been studying methods to personalize system utterances for users in casual conversations. We know that personalization is important, but no wellestablished way to personalize system utterances for users has been proposed. In this paper, we report the results of our experiment that examined how humans personalize utterances when speaking to each other in casual conversations. In particular, we elicited post-dialogue comments from speakers and analyzed the comments to determine what they thought about the dialogues while they engaged in them. In addition, by analyzing the effectiveness of their thoughts, we found that dialogue strategies for personalization related to ‚Äútopic elaboration‚Äù, ‚Äútopic changing‚Äù and ‚Äútempo‚Äù signiÔ¨Åcantly increased the satisfaction with regard to the dialogues. 
This paper investigates the inÔ¨Çuence of discourse features on text complexity assessment. To do so, we created two data sets based on the Penn Discourse Treebank and the Simple English Wikipedia corpora and compared the inÔ¨Çuence of coherence, cohesion, surface, lexical and syntactic features to assess text complexity. Results show that with both data sets coherence features are more correlated to text complexity than the other types of features. In addition, feature selection revealed that with both data sets the top most discriminating feature is a coherence feature. 
 Chat language is often referred to as  Computer-mediated  communication  (CMC). Most of the previous studies  on chat language has been dedicated to  collecting ‚Äùchat room‚Äù data as it is the  kind of data which is the most accessible  on the WEB. This kind of data falls under  the informal register whereas we are  interested in this paper in understanding  the mechanisms of a more formal kind  of CMC: dialog chat in contact centers.  The particularities of this type of dialogs  and the type of language used by cus-  tomers and agents is the focus of this  paper towards understanding this new  kind of CMC data. The challenges for  processing chat data comes from the  fact that Natural Language Processing  tools such as syntactic parsers and part  of speech taggers are typically trained on  mismatched conditions, we describe in  this study the impact of such a mismatch  for a syntactic parsing task.  
We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users‚Äô way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test. 
We investigate automatic question detection from recordings of teacher speech collected in live classrooms. Our corpus contains audio recordings of 37 class sessions taught by 11 teachers. We automatically segment teacher speech into utterances using an amplitude envelope thresholding approach followed by filtering non-speech via automatic speech recognition (ASR). We manually code the segmented utterances as containing a teacher question or not based on an empirically-validated scheme for coding classroom discourse. We compute domain-independent natural language processing (NLP) features from transcripts generated by three ASR engines (AT&T, Bing Speech, and Azure Speech). Our teacher-independent supervised machine learning model detects questions with an overall weighted F1 score of 0.59, a 51% improvement over chance. Furthermore, the proportion of automatically-detected questions per class session strongly correlates (Pearson‚Äôs r = 0.85) with human-coded question rates. We consider our results to reflect a substantial (37%) improvement over the state-of-the-art in automatic question detection from naturalistic audio. We conclude by discussing applications of our work for teachers, researchers, and other stakeholders. 
Conversant-independent stochastic turntaking (STT) models generally beneÔ¨Åt from additional training data. However, conversants are patently not identical in turn-taking style: recent research has shown that conversant-speciÔ¨Åc models can be used to refractively detect some conversants in unseen conversations. The current work explores an unsupervised framework for studying turn-taking style variability. First, within a veriÔ¨Åcation framework using an information-theoretic model distance, sides cluster by conversant more often than not. Second, multi-dimensional scaling onto low-dimensional subspaces appears capable of preserving distance. These observations suggest that, for many speakers, turn-taking style as characterized by time-independent STT models is a stable attribute, which may be correlated with other stable speaker attributes such as personality. The exploratory techniques presented stand to beneÔ¨Åt speaker diarization technology, dialogue agent design, and automated psychological diagnosis. 
 We demonstrate dialogues with an autonomous android ERICA, who has an appearance like a human being. Currently, ERICA plays two social roles: a laboratory guide and a counselor. It is designed to follow the protocols of human dialogue to make the user comfortable: (1) having a chat before the main talk, (2) proactively asking questions, and (3) conveying proper feedbacks. The combination of the human-like appearance and the appropriate behaviors according to her social roles allows for symbiotic human-robot interaction. 
 Most human-machine communication for information access through speech, text and graphical interfaces are mediated by forms ‚Äì i.e. lists of named Ô¨Åelds. However, deploying form-Ô¨Ålling dialogue systems still remains a challenging task due to the effort and skill required to author such systems. We describe an extension to the OpenDial framework that enables the rapid creation of functional dialogue systems by non-experts. The dialogue designer speciÔ¨Åes the slots and their types as input and the tool generates a domain speciÔ¨Åcation that drives a slot-Ô¨Ålling dialogue system. The presented approach provides several beneÔ¨Åts compared to traditional techniques based on Ô¨Çowcharts, such as the use of probabilistic reasoning and Ô¨Çexible grounding strategies. 
We present the implementation of a largevocabulary continuous speech recognition (LVCSR) system on NVIDIA‚Äôs Tegra K1 hyprid GPU-CPU embedded platform. The system is trained on a standard 1000hour corpus, LibriSpeech, features a trigram WFST-based language model, and achieves state-of-the-art recognition accuracy. The fact that the system is realtime-able and consumes less than 7.5 watts peak makes the system perfectly suitable for fast, but precise, ofÔ¨Çine spoken dialog applications, such as in robotics, portable gaming devices, or in-car systems. 
SARA (Socially-Aware Robot Assistant) is an embodied intelligent personal assistant that analyses the user‚Äôs visual (head and face movement), vocal (acoustic features) and verbal (conversational strategies) behaviours to estimate its rapport level with the user, and uses its own appropriate visual, vocal and verbal behaviors to achieve task and social goals. The presented agent aids conference attendees by eliciting their preferences through building rapport, and then making informed personalized recommendations about sessions to attend and people to meet. 
Chat functionality is currently considered an important factor in spoken dialogue systems. In this paper, we explore the architecture of a chat-oriented dialogue system that can continue a long conversation with users and can be used for a long time. To achieve this goal, we propose a method combining various types of response generation modules, such as a statistical model-based module, a rule-based module, and a topic transition-oriented module. The core of this architecture is a method for selecting the most appropriate response based on a breakdown index and a willingness index. 
Real-world scenes typically have complex structure, and utterances about them consequently do as well. We devise and evaluate a model that processes descriptions of complex conÔ¨Ågurations of geometric shapes and can identify the described scenes among a set of candidates, including similar distractors. The model works with raw images of scenes, and by design can work word-by-word incrementally. Hence, it can be used in highly-responsive interactive and situated settings. Using a corpus of descriptions from game-play between human subjects (who found this to be a challenging task), we show that reconstruction of description structure in our system contributes to task success and supports the performance of the word-based model of grounded semantics that we use. 
Arguably, spoken dialogue systems are most often used not in hands/eyes-busy situations, but rather in settings where a graphical display is also available, such as a mobile phone. We explore the use of a graphical output modality for signalling incremental understanding and prediction state of the dialogue system. By visualising the current dialogue state and possible continuations of it as a simple tree, and allowing interaction with that visualisation (e.g., for conÔ¨Årmations or corrections), the system provides both feedback on past user actions and guidance on possible future ones, and it can span the continuum from slot Ô¨Ålling to full prediction of user intent (such as GoogleNow). We evaluate our system with real users and report that they found the system intuitive and easy to use, and that incremental and adaptive settings enable users to accomplish more tasks. 
In this paper, we present and evaluate an approach to incremental dialogue act (DA) segmentation and classiÔ¨Åcation. Our approach utilizes prosodic, lexico-syntactic and contextual features, and achieves an encouraging level of performance in ofÔ¨Çine corpus-based evaluation as well as in simulated human-agent dialogues. Our approach uses a pipeline of sequential processing steps, and we investigate the contribution of different processing steps to DA segmentation errors. We present our results using both existing and new metrics for DA segmentation. The incremental DA segmentation capability described here may help future systems to allow more natural speech from users and enable more natural patterns of interaction. 
An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed NextUtterance-ClassiÔ¨Åcation (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main Ô¨Åndings: (1) humans are able to correctly classify responses at a rate much better than chance, thus conÔ¨Årming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus conÔ¨Årming the utility of this class of tasks for driving further research in automated dialogue systems. 
Existing speaking tests only require nonnative speakers to engage in dialogue when the assessment is done by humans. This paper examines the viability of using off-the-shelf systems for spoken dialogue and for speech grading to automate the holistic scoring of the conversational speech of non-native speakers of English. 
When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. Debate websites produce curated summaries of arguments on such topics; these summaries typically consist of lists of sentences that represent frequently paraphrased propositions, or labels capturing the essence of one particular aspect of an argument, e.g. Morality or Second Amendment. We call these frequently paraphrased propositions ARGUMENT FACETS. Like these curated sites, our goal is to induce and identify argument facets across multiple conversations, and produce summaries. However, we aim to do this automatically. We frame the problem as consisting of two steps: we Ô¨Årst extract sentences that express an argument from raw social media dialogs, and then rank the extracted arguments in terms of their similarity to one another. Sets of similar arguments are used to represent argument facets. We show here that we can predict ARGUMENT FACET SIMILARITY with a correlation averaging 0.63 compared to a human topline averaging 0.68 over three debate topics, easily beating several reasonable baselines. 
We present a simple real-time, real-world grounding framework, and a system which implements it in a simple robot, allowing investigation into different grounding strategies. We put particular focus on the grounding effects of non-linguistic task-related actions. We experiment with a trade-off between the Ô¨Çuidity of the grounding mechanism with the ‚Äòsafety‚Äô of ensuring task success. The framework consists of a combination of interactive Harel statecharts and the Incremental Unit framework. We evaluate its in-robot implementation in a study with human users and Ô¨Ånd that in simple grounding situations, a model allowing greater Ô¨Çuidity is perceived to have better understanding of the user‚Äôs speech. 
Although word and character n-grams have been used as features in different NLP applications, no systematic comparison or analysis has shown the power of character-based features for detecting abusive language. In this study, we investigate the effectiveness of such features for abusive language detection in user-generated online comments, and show that such methods outperform previous state-of-theart approaches and other strong baselines. 
The goal of our research is to support fullÔ¨Çedged dialogue between a user and a system that transforms the user queries into visualizations. So far, we have collected a corpus where users explore data via visualizations; we have annotated the corpus for user intentions; and we have developed the core NL-to-visualization pipeline. 
Entrainment is a factor in dialogue that affects not only human-human but also human-machine interaction. While entrainment on the lexical level is well documented, less is known about how entrainment affects dialogue on a more abstract, structural level. In this paper, we investigate the effect of entrainment on dialogue acts and on lexical choice given dialogue acts, as well as how entrainment changes during a dialogue. We also deÔ¨Åne a novel measure of entrainment to measure these various types of entrainment. These results may serve as guidelines for dialogue systems that would like to entrain with users in a similar manner. 
One of the key ways of making dialogue agents more attractive as conversation partners is characterization, as it makes the agents more friendly, humanlike, and entertaining. To build such characters, utterances suitable for the characters are usually manually prepared. However, it is expensive to do this for a large number of utterances. To reduce this cost, we are developing a natural language generator that can express the linguistic styles of particular characters. To this end, we analyze the linguistic peculiarities of Japanese Ô¨Åctional characters (such as those in cartoons or comics and mascots), which have strong characteristics. The contributions of this study are that we (i) present comprehensive categories of linguistic peculiarities of Japanese Ô¨Åctional characters that cover around 90% of such characters‚Äô linguistic peculiarities and (ii) reveal the impact of each category on characterizing dialogue system utterances. 
Understanding situated dialogue requires identifying referents in the environment to which the dialogue participants refer. This reference resolution problem, often in a complex environment with high ambiguity, is very challenging. We propose an approach that addresses those challenges by combining learned semantic structure of referring expressions with dialogue history into a ranking-based model. We evaluate the new technique on a corpus of human-human tutorial dialogues for computer programming. The experimental results show a substantial performance improvement over two recent state-of-the-art approaches. The proposed work makes a stride toward automated dialogue in complex problem-solving environments. 
 We present a multi-modal dialogue system for interactive learning of perceptually grounded word meanings from a human tutor. The system integrates an incremental, semantic parsing/generation framework - Dynamic Syntax and Type Theory with Records (DS-TTR) - with a set of visual classiÔ¨Åers that are learned throughout the interaction and which ground the meaning representations that it produces. We use this system in interaction with a simulated human tutor to study the effects of diÔ¨Äerent dialogue policies and capabilities on accuracy of learned meanings, learning rates, and eÔ¨Äorts/costs to the tutor. We show that the overall performance of the learning agent is aÔ¨Äected by (1) who takes initiative in the dialogues; (2) the ability to express/use their conÔ¨Ådence level about visual attributes; and (3) the ability to process elliptical and incrementally constructed dialogue turns. Ultimately, we train an adaptive dialogue policy which optimises the trade-oÔ¨Ä between classiÔ¨Åer accuracy and tutoring costs. 
Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning Ô¨Åne-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topic-speciÔ¨Åc datasets enables learning Ô¨Åner-grained knowledge about events and results in signiÔ¨Åcant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topicsorted stories are judged as contingent. 
Studies on laughter in dialogue have proposed resolving what laughter is about by looking at what laughter follows. This paper investigates the sequential relation between the laughter and the laughable. We propose a semantic/pragmatic account treating laughter as a gestural event anaphor referring to a laughable. Data from a French and Chinese dialogue corpus suggest a rather free time alignment between laughter and laughable. Laughter can occur (long) before, during, or (long) after the laughable. Our results challenge the assumption that what laughter follows is what it is about, and thus question claims which rely on this assumption. 
This paper addresses the problem of how to build interview systems that users are willing to use. Existing interview dialogue systems are mainly focused on obtaining information from users, thus they just repeatedly ask questions. We propose a method for improving user impressions by engaging in small talk during interviews. The system performs framebased dialogue management for interviewing and generates small talk utterances after the user answers the system‚Äôs questions. Experimental results using a textbased interview dialogue system for diet recording showed the proposed method gives a better impression to users than interview dialogues without small talk. It is also found that generating too many small talk utterances makes user impressions worse because of the system‚Äôs low capability of continuously generating appropriate small talk utterances. 
In this work, we focus on automatically recognizing social conversational strategies that in human conversation contribute to building, maintaining or sometimes destroying a budding relationship. These conversational strategies include self-disclosure, reference to shared experience, praise and violation of social norms. By including rich contextual features drawn from verbal, visual and vocal modalities of the speaker and interlocutor in the current and previous turn, we can successfully recognize these dialog phenomena with an accuracy of over 80% and kappa ranging from 60-80%. Our Ô¨Åndings have been successfully integrated into an end-to-end socially aware dialog system, with implications for virtual agents that can use rapport between user and system to improve task-oriented assistance. 
In this study, we present our neural utterance ranking (NUR) model, an utterance selection model for conversational dialogue agents. The NUR model ranks candidate utterances with respect to their suitability in relation to a given context using neural networks; in addition, a dialogue system based on the model converses with humans using highly ranked utterances. SpeciÔ¨Åcally, the model processes word sequences in utterances and utterance sequences in context via recurrent neural networks. Experimental results show that the proposed model ranks utterances with higher precision relative to deep learning and other existing methods. Furthermore, we construct a conversational dialogue system based on the proposed method and conduct experiments on human subjects to evaluate performance. The experimental result indicates that our system can offer a response that does not provoke a critical dialogue breakdown with a probability of 92% and a very natural response with a probability of 58%. 
We propose a set of generic conversational strategies to handle possible system breakdowns in non-task-oriented dialog systems. We also design policies to select these strategies according to dialog context. We combine expert knowledge and the statistical Ô¨Åndings derived from data in designing these policies. The policy learned via reinforcement learning outperforms the random selection policy and the locally greedy policy in both simulated and real-world settings. In addition, we propose three metrics for conversation quality evaluation which consider both the local and global quality of the conversation. 
Elena Lloret Department of Software and Computing Systems University of Alicante Apdo. de Correos 99 E-03080, Alicante, Spain elloret@dlsi.ua.es  
Robust, statistical Natural Language Generation from Web knowledge bases is hindered by the lack of text-aligned resources. We aim to Ô¨Åll this gap by presenting a method for extracting knowledge from natural language text, and encode it in a format based on frame semantics and ready to be distributed in the Linked Open Data space. We run an implementation of such methodology on a collection of short documents and build a repository of frame instances equipped with Ô¨Åne-grained lexicalizations. Finally, we conduct a pilot stody to investigate the feasibility of an approach to NLG based on said resource. We perform error analysis to assess the quality of the resource and manually evaluate the output of the NLG prototype. 
We develop a system that operates on a document collection and represents the contained information to enable the intuitive and efÔ¨Åcient exploration of the collection. Using various NLP, IE and Semantic Web methods, we generate a semantic layer on top of the collection, from which we take the key con- cepts. We deÔ¨Åne templates for structured reorgan- isation and rearrange the information related to the key concepts to Ô¨Åt the respective template. The use case of the system is to support knowledge work- ers (journalists, editors, curators, etc.) in their task of processing large amounts of documents by sum- marising the information contained in these docu- ments and suggesting potential story paths that the knowledge worker can then process further. 
A sub-task of Natural Language Generation (NLG) is the generation of referring expressions (REG). REG algorithms are expected to select attributes that unambiguously identify an entity with respect to a set of distractors. In previous work we have deÔ¨Åned a methodology to evaluate REG algorithms using real life examples. In the present work, we evaluate REG algorithms using a dataset that contains alterations in the properties of referring entities. We found that naturally occurring ontological re-engineering can have a devastating impact in the performance of REG algorithms, with some more robust in the presence of these changes than others. The ultimate goal of this work is observing the behavior and estimating the performance of a series of REG algorithms as the entities in the data set evolve over time. 
Finding the natural language equivalent of structured data is both a challenging and promising task. In particular, an efÔ¨Åcient alignment of knowledge bases with texts would beneÔ¨Åt many applications, including natural language generation, information retrieval and text simpliÔ¨Åcation. In this paper, we present an approach to build a dataset of triples aligned with equivalent sentences written in natural language. Our approach consists of three main steps. First, target sentences are annotated automatically with knowledge base (KB) concepts and instances. The triples linking these elements in the KB are extracted as candidate facts to be aligned with the annotated sentence. Second, we use textual mentions referring to the subject and object of these facts to semantically simplify the target sentence via crowdsourcing. Third, the sentences provided by different contributors are post-processed to keep only the most relevant simpliÔ¨Åcations for the alignment with KB facts. We present different Ô¨Åltering methods, and share the constructed datasets in the public domain. These datasets contain 1,050 sentences aligned with 1,885 triples. They can be used to train natural language generators as well as semantic or contextual text simpliÔ¨Åers.  
In this paper we present an implementation of an NLG system that serves for stock news generation. The system has two modules: analysis module and NLG module. The Ô¨Årst one is intended for processing the data on stock index changes, the second one for the news texts generation using the template-based NLG approach. The evaluation shown that both modules give relatively accurate results and the system can be used as a newsbot for stock news generation. 
 CNRS/LORIA I.U.T. Blagnac  IIEST  Nancy (France)  Nancy (France)  Toulouse (France)  Shibpur (India)  laura.perez@loria.fr  claire.gardent@loria.fr anselme.revuz@gmail.com saptarashmicse@gmail.com  
The paper deals with the generation of ReadME files from an ontology-based description of NLP tool. ReadME files are structured and organised according to properties defined in the ontology. One of the problem is being able to deal with multilingual generation of texts. To do so, we propose to map the ontology elements to multilingual knowledge defined in a SKOS ontology. 
Ontologies are usually represented in OWL that is not easy to grasp by domain experts. A solution to bridge this gap is to use a controlled natural language or natural language generation (NLG), which allows the knowledge in the ontology to be rendered automatically into a natural language. Several approaches exist to realise this. We used both templates and the Grammatical Framework (GF) and examined the feasibility of each by developing NLG modules for a language that had none: Afrikaans. The template system requires manual translation of the ontology‚Äôs vocabulary into Afrikaans, if not already done so, while the GF system can translate the terms automatically. Yet, the template system is found to produce more grammatically correct sentences and verbalises the ontology slightly faster than the GF system. The template-based approach seems easier to extend for future development. 
Recent deep learning approaches to Natural Language Generation mostly rely on sequence-to-sequence models. In these approaches, the input is treated as a sequence whereas in most cases, input to generation usually is either a tree or a graph. In this paper, we describe an experiment showing how enriching a sequential input with structural information improves results and help support the generation of paraphrases. 
We examine the use of trafÔ¨Åc information with other knowledge sources to automatically generate natural language tweets similar to those created by humans. We consider how different forms of information can be combined to provide tweets customized to a particular location and/or speciÔ¨Åc user. Our approach is based on data-driven natural language generation (NLG) techniques using corpora containing examples of natural language tweets. It speciÔ¨Åcally draws upon semantic data and knowledge developed and used in the web based Connected Vehicles and Smart Transportation system. We introduce an alignment model, generation model and location-based user model which will together support location-relevant information delivery. We provide examples of our system output and discuss evaluation issues with generated tweets. 
Language is usually studied and analysed from different disciplines generally on the premise that it constitutes a form of communication which pursues a speciÔ¨Åc objective. The discourse, in that sense, can be understood as a text which is constructed to express such objective. When a discourse is created, its production is related to some textual genre, usually connected with some pragmatic features, like the intention of the writer or the audience to whom is addressed, both conditioning the use of language. But genres can be considered as well as compounds of different pieces of text with a certain degree of order, each one seeking for more concrete objectives. This paper presents a proposal to learn such features as a way to generate richer document plans, applying clustering techniques over annotated documents. 
Little is known, however, about how post-edited texts differ in their linguistic properties from human translations. For machine translated texts, Lapshinova-Koltunski (2013; 2015) provides insights on linguistic properties of texts translated by humans either manually or with the help of CAT tools, or by various machine translation systems. She analyses such features as verbal vs. nominal style and investigates whether typical translation properties such as shining-through (Teich, 2003) or explicitation (Blum-Kulka, 1986) can be observed not only in HT, but also in machine translated texts. Carl and Schaeffer (forthcoming) report that PE products exhibit less lexical  Patterns of variation in post-editing and MT in contrast to human translation  107  variation in translation than HT products, possibly due to the priming of the machine translation output. In the following, we present a pilot study analysing how texts may differ in terms of their lexical profiles between purely human translations and when machine translation (MT) is added to the process. We focused on two specific kinds of lexemes: terms and cognates. In the first experiment, we contrasted post-editing and human translation along the dimension of term translation within the domain of Languages for Specific Purposes (LSP). In the second experiment we compared translation choices made by humans and machine translation systems in terms of cognates, in order to understand in which other ways MT may change the lexical profiles of texts. Cognates are lexical items that have a similar form and meaning in two languages, like German System and English system, but are not always the best or preferred translation equivalents.  2. Studies 2.1. Patterns of terminological variation The data used for the analysis in this subsection was collected in an experiment in which participants were asked to translate (HT), fully post-edit (FPE) and light-post-edit (LPE) texts from either the technical or the medical domain. The texts were of low formality level and originated from LSP texts freely available on the internet. The three technical texts selected for the experiment are parts taken from a dish washer manual, the three medical texts were taken from package leaflets ranging from a vaccine against measles to human insulin for diabetes patients and medication for treatment of cancer. All texts were about 150 words long. The data collection is a generic collection in the sense that we did not aim at studying one specific phenomenon (e.g. term translation), but it is generally aimed at contrasting the PE and HT processes and products along various dimensions. The texts were automatically pre-translated by Google Translate for the PE tasks. A permutation scheme was set for the three sessions for each domain so that each text would be translated, fully and lightly post-edited the same amount of times, but by different participants (cf. Table 1). Participants had access to internet search facilities, including online dictionaries and term databases such as IATE1. The participant groups consisted of 12 advanced translation students for the technical and 9 for the medical texts, all students at FTSK Germersheim. They had at least two years of training and had passed at least one course on translating in the domain they would translate and post-edit in in the experiment. Some had minor post-editing experience, but given that PE is not part of the regular course offering at the institute, it can be assumed that all participants were better trained in HT than in PE. The technical texts were thus translated, lightly and fully post-edited each four times, the medical texts each three times. The participants used the Translog-II editor for all three tasks, which logged the participant‚Äôs key stroke activities, and eye movements were recorded using a Tobii TX 300. The eye-tracking and keylogging data were not used for the present study. The experiment data will be made available through the translation process research database (TPR-DB)2.  
 116  Babych  and syntactic taggers exploiting similarities between closely related languages (Hana et al., 2006), statistical learning of preferred edits for detecting regular orthographic correspondences in closely related languages (Ciobanu and Dinu, 2014). Applications of Levenshtein‚Äôs metric for the translation technologies and specifically for Machine Translation include automated identification of cognates for the tasks of creating bilingual resources such as electronic dictionaries (e.g., Koehn and Knight, 2002; Mulloni and Pekar, 2006; Bergsma and Kondrak, G. 2007), improving document alignment by using cognate translation equivalents as a seed lexicon (Enright, J and Kondrak, G., 2007), automated MT evaluation (e.g., Niessen et al., 2000; Leusch et al., 2003). Levenshtein distance metrics has been modified and extended for applications in different areas; certain ideas have yet not been tested in MT context, but have a clear potential for benefiting MT-related tasks. This paper develops and evaluates one of such ideas for a linguistic extension of the metric proposed in the area of computational modelling of dialectological variation and measuring ‚Äòcognate‚Äô lexical distance between languages, dialects and different historical periods in development of languages, e.g., using cognates from the slow-changing part of the lexicon ‚Äì the Swadesh list (Swadesh, 1952; Serva and Petroni, 2008; Schepens et al., 2012). In this paper the suggestion is explored of calculating the so called Levenshtein‚Äôs ‚Äòphonological edit distance‚Äô between phonemic transcriptions of cognates, rather than the traditional string edit distance (Nerbonne and Heeringa 1997; Sanders and Chin, 2009). This idea is based on the earlier linguistic paradigm of describing phonemes as systems of their phonological features, formulated in its modern form by Roman Jacobson ‚Äì see (Anderson, 1985) for the development of the theory; later it was introduced into generative and computational linguistic paradigms by Chomsky and Halle (1968). The idea is that each phoneme in a transcription of a cognate is represented as a structure of phonological distinctive features, such as: [a] = [+vowel, +back; +open; ‚Äìlabialised]  1.1. Distinctive phonological features: the background  In phonology, sounds of a language form a system of phonemes (i.e., minimal segments of speech that can be used in the same context and distinguish meanings in minimal word pairs, which differ only by one such segment (i.e., a phoneme). For example, English phonemes /p/ and /b/ distinguish meaning in pull vs. bull; pill vs. bill; phonemes /v/ and /w/ distinguish meanings of vary vs. wary. However, Ukrainian sounds /v/ and /w/ are positional variants, or allophones, of the same phoneme, since they are never used in the same position or distinguish meanings: /w/ is restricted to a word-final position after a vowel: –≤–∏–π—à–æ–≤ /vyj≈°ow/ ‚Äòentered‚Äô). There is evidence that phonemes are not simply linguistic constructs, but have a psychological reality, e.g., for native speakers they form cognitive pronunciation targets; non-native speakers often confuse phonemes that are not separated in their first language (e.g., native Ukrainian speakers would confuse /v/ and /w/ when speaking English). In languages where writing systems and pronunciation are close to each other, e.g., Ukrainian or Georgian, the written characters usually correspond to phonemes (much less often ‚Äì to allophones). Phonemes and allophones are characterised by a further internal structure, which consists of a system of distinctive phonological features (Jakobson et al., 1958). These features are typically based on differences in their acoustic properties and the way of how they are pronounced (their articulation). For example, /v/ and /w/ are both  Graphonological Levenshtein Edit Distance: Application for Automated Cognate Identification 117 consonants, i.e., they are formed with a participation of noise (unlike vowels /u, o, a/, etc., which are formed with an unobstructed sound); both are fricative consonants, i.e., they are formed with a constant air friction against an obstacle in the vocal tract (unlike plosive consonants, such as /b, p, d, t, g, k/ that include a build up of air behind some obstacle during an initial silence, followed by its instant release); the difference between /v/ and /w/ is that /v/ is labio-dental, i.e., the air friction is created with the teeth and the lower lip, while /w/ is bilabial, i.e., the source of friction is the upper and lower lip, while the teeth are not involved. However, not all acoustic or articulatory differences become distinctive phonological features. The necessary condition is that these features should capture phonological distinctions, i.e., those needed for differentiation between phonemes: e.g., long vs. short pairs of vowels in Dutch differ primarily by their length; however, they have further qualitative differences as well, which are visible on their spectrograms, but are not perceived by speakers as features that make phonemic distinctions; therefore, these qualitative differences are not part of their distinctive phonological features. Similarly, the same Ukrainian vowels in stressed and unstressed positions are very different qualitatively, but these differences are not perceived as phonological, i.e., the ones that distinguish different phonemes, so both stressed and unstressed variants have the same set of distinctive features. Some distinctive phonological features are in correlated oppositions, i.e., they distinguish sets of phonemes that only differ by a single feature, e.g., +voiced vs ‚Äìvoiced (i.e., formed with or without the vocal cords) distinguishes /d/~/t/; /z/~/s/; /b/~/p/; /v/~/f/, /g/~/k/. These correlated features often switch their value in positional or historical alternations, and as a result, may distinguish cognates in closely related languages. Nowadays there are standard description of phonemes and phonological features for most languages of the world, illustrated with sound charts, e.g., by the International Phonetic Association (IPA) (Ladefoged and Halle, 1988). These charts group sounds along several dimensions of their distinctive phonological features, such as place, manner of articulation, voiced/voiceless for consonants; high/low, back/front, roundness for vowels, with finer-grained sub-divisions. Sound charts for individual languages can be found in standard language references. For the experiments described in this paper the systems of phonological distinctive features for Ukrainian and Russian has been adapted from (Comrie and Corbett, Eds., 1993: 949, 951, 829). 1.2. Application of phonological features for calculating the edit distance For using phonological distinctive features in calculation of the Levenshtein edit distance, the idea is to replace the operation of substitution of a whole character by the substitution of its constituent phonological feature representations, which would be sufficient to convert it into another character: so rewriting [o] into [a] (which, e.g., is a typical vowel alternation pattern in Russian and distinguishes some of its major dialects) would incur a smaller cost compared to the substitution of the whole character, since only two of its distinctive phonological features need to be rewritten: [o] = [+vowel, +back; +mid; +labialised] On the other hand, the cost of rewriting the vowel [a] into the consonant [t] (the change which normally does not happen as part of the historical language development or dialectological variation) would involve rewriting all the phonological features in the representation, so the edit cost will be the same as for the substitution of the entire character:  118  Babych  [t] = [+consonant; ‚Äìvoiced; +plosive; +fronttongue; +alveolar] According to Nerbonne and Heeringa (1997:2) the feature-based Levenshtein distance makes it ‚Äú‚Ä¶possible to take into account the affinity between sounds that are not equal, but are still related‚Äù; and to ‚Äú‚Ä¶show that 'pater' and 'vader' are more kindred then 'pater' and 'maler'.‚Äù This is modelled by the fact that phonological feature representations for pairs such as [t] and [d] (both front-tongue alveolar plosive consonants, which only differ by ‚Äòvoiced‚Äô feature), as well as [p] and [v] (both labial consonants), share greater number of phonological features compared to the pairs [p] and [m] (which differ in sonority, manner and passive organ of articulation) or [t] and [l] (which differ in sonority and the manner of articulation). However, the authors point out to a number of open questions and problems related to their modified metric, e.g., how to represent phonetic features of complex phonemes, such as diphthongs; what should be the structure of feature representations: Nerbonne and Heeringa use feature vectors, but are these vectors sufficient or more complex feature representations are needed; how to integrate edits of individual features into the calculation of a coherent distance measure (certain settings are not used, whether to use Euclidian or Manhattan distance, etc.). Linguistic ideas behind the suggestion to use Levenshtein phonological edit distance are intuitively appealing and potentially useful for applications beyond dialectological modelling. However, to understand their value for other areas, such as MT, there is a need to develop a clear evaluation framework for testing the impact of different possible settings of the modified metric and different types of feature representations, to compare specific settings of the metric to alternatives and the classical Levenshtein‚Äôs baseline. Without a systematic evaluation framework the usefulness of metrics remain unknown. This paper proposes an evaluation framework for testing alternative settings of the modified Levenshtein‚Äôs metric. This framework is task-based: it evaluates the metric‚Äôs alternative settings and feature representations in relation to its success on the task of automated identification of cognates from non-parallel (comparable) corpora. The scripts for calculating the modified feature-based Levenshtein distance, and the corresponding graphonological feature representations (vectors and the hierarchies of features) are released on the author‚Äôs webpage1. Features are currently available for Latin and Cyrillic alphabets, new alphabets will be added in future. Graphonological Levenshtein distance can also be applied, calibrated and evaluated for other tasks, beyond the task of cognate identification, e.g., to robust transliteration, reconstruction of diacritics or recognition of words with distorted, non-standard or variable spelling, e.g.: the names Osama/ Usama/ Ousamma /–û—Å–∞–º–∞/ –£—Å–∞–º–∞/ –£—Å–∞–º–º–∞ are closer to each other in terms of their underlying phonological feature sequences than their plain character-based distances. Evaluation on these tasks may lead to alternative preferred settings and feature representations for the graphonological Levenshtein metric, compared to evaluation on the cognate identification task described here. The paper is organised as follows: Section 2 presents the set-up of the experiment, the application of automated cognate identification; the design and feature representations for the metric and the evaluation framework. Section 3 presents evaluation results of different metric settings and comparison with the classical Levenshtein distance; Section 4 presents conclusion and future work.  
For those of us with longer memories, it was not always this way. When commercial TM tools were first introduced, many translators resented the imposition of this new technology. However, early adopters found that, once past the initial learning curve, they could achieve perceptible productivity gains, although the financial benefit of these gains was mitigated to an extent when discounts based on TM matches became common (Garc√≠a, 2006). As regards the second claim above, a disadvantage for MT is that in exactly the same way as with the introduction of TM, translators further resent the imposition of the newer technology, especially when associated discounts are expected immediately. Translators have complained about having to make tedious repetitive corrections to MT output, lack of creativity, and ‚Äúlimited opportunity to create quality‚Äù when post-editing  142  Moorkens and Way  (Moorkens and O‚ÄôBrien, 2015). These complaints are exacerbated by the perishable and often poorly-written source content that is pushed towards MT in localisation workflows (Way, 2013; Moorkens and O‚ÄôBrien, 2015). Despite many studies having shown that MT post-editing increases productivity, users do not always perceive this increase (Koehn, 2009; Gaspari et al., 2014). Despite the increasing incorporation of MT into translation workflows via post-editing (PEMT) or sub-segment auto-suggestion, MT does not yet appear to be widely accepted by translators (cf. Penkale and Way, 2013; Way, 2013). While this is obvious to many, we consider it worth pointing out the main difference between TM and MT: namely that while MT attempts to translate all sentences in an input document, TM does not (except in the case of 100% matches, for which translators receive little or no remuneration in any case); TM systems merely search the source side of a set of translation pairs for the closest-matching instances above some predetermined threshold imposed by the translator (so-called ‚Äòfuzzy matches‚Äô; Sikes (2007)). A ranked list of the said translation pairs is then presented to the translator with user-friendly colour-coding to help the user decide which parts are useful in the composition of the target translation, and which should be ignored and discarded. The addition of project-specific or historical information from the suggested TM segment metadata may help the translator with this decision (Teixeira, 2014). Accordingly, we note the different roles played by the human-in-the-loop here: when using TM, the human still translates, whereas with MT, the MT output is usually post-edited. There are exceptions here as the delineation between TM and MT has become somewhat blurred, with some tools incorporating both technologies and others adding sub-segment autosuggestions from MT output (Green et al. 2014; O‚ÄôBrien and Moorkens, 2014). Given that today‚Äôs statistical MT (SMT) engines have greatly improved in terms of the quality of their output (cf. Way (2013) for a list of use-cases where MT demonstrably plays an invaluable role), it is disappointing for MT developers to learn that human translators still appear to draw greater satisfaction from slow, interactive TM tools as opposed to fully automatic, fast MT systems. For example, 75% of respondents to Moorkens and O‚ÄôBrien‚Äôs (2016) survey of translators agreed that TM helps with their work, whereas only 30% said the same of MT; what‚Äôs more, 56% indicated that they considered MT a problematic technology. Participants in another study by Moorkens and O‚ÄôBrien (2015) said that they found post-editing tiring as they are required to be ‚Äúconstantly vigilant ‚Ä¶ due to the absence of any confidence indication‚Äù. More positively, Koskinen and Ruokonen (2016) suggest that translators are ‚Äúquite willing to adopt new technology as long as it makes their work more efficient‚Äù. Consequently, we feel that some of the problems with MT reside in how it is presented. In particular, if making productivity improvements could be made demonstrable to and perceptible for users, there would be far fewer objections to MT as a technology in its own right than we have seen heretofore. Accordingly, this paper reports on an initial study that seeks to answer the following question: Is comparative acceptability of TM over MT predicated on the user‚Äôs ability to optimise the precision and usefulness of match suggestions by setting a minimum match threshold? We contend that the answer to this question is yes, and that: 1. MT would be considered more acceptable to users if only those matches that required relatively small amounts of editing were presented to post-editors. 2. TM would be less acceptable to users if matches that required large amounts of editing were presented to translators.  Comparing Translator Acceptability of TM and SMT Outputs  143  In other words, we suggest that translators‚Äô comparative preference of TM over MT demonstrates their preference for precision over maximum recall. Guerberof (2012) found that the average post-editing time for English-to-Spanish MT trained on a source text-appropriate technical domain was roughly equivalent to the time required to edit an 85-94% fuzzy TM match. Not all MT output will be of this quality of course, and the translator‚Äôs bugbear of repetitive mistakes to correct in MT output remains a problem, although great strides are being made to incorporate translator feedback into iterative retraining of SMT systems (cf. Du et al., 2015). However, if the ability to set an accurate threshold is one of the things that makes TM useful and acceptable to translators, this highlights the need for accurate confidence prediction of MT quality that correlates with human judgement (e.g. Specia et al. (2009); Specia (2011); Turchi et al. (2013)), the absence of which we believe to be a major stumbling block for acceptability. This study is reported with the caveat that the research was carried out with a small number of translators for a single domain and language pair, and is intended to preface a larger-scale study that will include measures of actual post-editing effort. However, all participants have substantial translation experience (on average 11.4 years of professional translation experience and 4.5 years of professional post-editing experience) and the chosen language pair of English-German is acknowledged to be a difficult one for MT systems. The remainder of this paper is organised as follows. In Section 2, we describe the methodology chosen to test the central hypothesis in this paper. In Section 3, we present the results of the experiments conducted, which are discussed further in Section 4. In Section 5, we conclude, and list a number of avenues for further work in this area.  2. Methodology  In this study, seven translators were asked to rate the usefulness of 60 match suggestions in German for 60 English source text segments. Source segments were taken from the documentation for the open-source computer-aided design (CAD) program FreeCAD and from the Wikipedia page for CAD.1 Table 1 shows the homogeneity of all segments, the segments used for TM matching, and those translated using MT, which all exhibited similar characteristics (well within the standard deviation for each text) using common corpora analyses (such as the type/token ratio of lexical variation) in the WordSmith WordList tool.2 Note that some types appear in both TM and MT corpora.  Table 1. Wordsmith statistics for source data.  Overall  Types (distinct words)  447  Type/token ratio (TTR)  42.21  Mean word length (chars.)  4.82  Segments for MT 260 46.93 4.87  Segments for TM 268 53.07 4.76  
Despite the continuous progress that has been made over the last decades, Machine Translation (MT) systems are far from perfect. Moreover, MT quality not only varies between different domains or language pairs but also from sentence to sentence. Even though it has been shown that using MT leads to productivity gains in computer-assisted translation (CAT) workflows (Guerberof, 2009; Depraetere et al., 2014), to produce high-quality translations, humans still need to intervene in the translation process and do this usually by post-editing (correcting) the MT output. Post-editing MT output requires post-editors to detect translation errors prior to correcting them. Hence, automatic quality estimation (QE) systems not only aim to estimate the post-editing effort at segment level to filter low quality translations (Specia et al, 2009), but also to detect the location and the nature of errors at word level (Ueffing and Ney, 2007; Bach et al., 2011). MT errors can be analysed as adequacy and fluency errors. While adequacy is concerned with how much of the source content and meaning is also expressed in the target text, fluency is concerned with to what extent the translation is well formed and adheres to the norms of the target language. The distinction between adequacy and  204  Tezkan et al.  fluency has been used in different translation error taxonomies (Lommel et al., 2014; Daems, Macken and Vandepitte, 2014). Besides the difficulties of transferring source content and meaning to a target sentence, the task of producing grammatically correct sentences remains to be challenging for MT systems, independent of the domain of text to be translated and the type of MT system (Costa et al., 2015; Daems et al., 2015). This motivates us to examine the use of dependency structures, which represent the abstract grammatical relations that hold between constituents, for detecting grammatical errors in machine-translated text. A dependency tree is a rooted, directed acyclic graph, which represents all words in a sentence as nodes and grammatical relations between the words as edges. A labelled dependency tree, additionally, incorporates the nature of the grammatical relationships between the words as annotations of relation names on the edges of the tree. Dependency trees are interesting for the QE task due to the fact that the dependents may span discontinuous parts of the input sentence and are suited for representing languages with word order variations and discontinuous constituencies such as Dutch. In this paper, we present two new approaches for QE on sub-segment level, and more specifically for detecting grammatical errors in Dutch MT output. In the first approach, we use the partial dependency parses as an indicator of problematic text fragments when no parse covers the complete input. In the second approach, we query the sub-trees of the dependency tree of an MT sentence against a treebank of correct Dutch sentences by using dependency relation and syntactic category information on phrase and lexical level. The number of matching constructions is then considered to be an indicator of possible translation errors for a given sub-tree. In addition to using these two approaches separately, we combine them together and evaluate the three approaches on the output of an English-to-Dutch statistical and rule-based MT system. We evaluate the performance on sentence and word-level by comparing the detected errors to manually annotated errors. The remainder of this work is as follows: In Section 2, we describe related research. In Section 3, our approach is outlined in detail and in Section 4, we describe the data sets we used. In Section 5, we give the results of our experiments. Finally, in Section 6, we conclude by discussing the results and future work.  2. Related work QE is the task of providing a quality indicator for machine-translated text without relying on reference translations (Gandrabur and Foster, 2003). Most work on QE has focused on segment level, which aims to provide a binary or continuous quality score for the whole machine-translated sentence that reflects the post-editing time or the number of edits that are required to correct the MT output (Blatz et al., 2004; Specia et al., 2009; Hardmeier et al., 2011). QE on word or sub-segment level, on the other hand, has received less attention. Estimating the quality of MT output on word or sub-segment level has a number of advantages compared to sentence-level QE. First of all, word-level QE systems can highlight problematic text fragments in machine-translated text to guide the post-editors. Furthermore, since the overall quality of an MT system depends on the individual errors it makes, word-level QE systems can easily be extended to estimate segment-level quality (de Souza et al., 2014; Tezcan et al., 2015). Word-level QE systems can additionally be used for improving MT quality by providing valuable information about  Detecting Grammatical Errors in Machine Translation  205  the location, the frequency and the type of errors MT systems make (Popovic and Ney, 2011) or by combining correct text fragments from different MT systems (Ueffing and Ney, 2007). In one of the early works on word-level QE, Blatz et al. (2004) used a collection of features to build a binary classifier that provides confidence scores for each word in machine-translated text. Besides using features that capture the relationships between source and target words, such as the word posterior probabilities and semantic similarities, they used additional target-language features that were based on basic syntax checking and word frequency. A recent work on word-level QE is QuEst++ (Specia et al., 2015), which is an open-source toolkit for QE on word, sentence and document level. QuEst++ consists of two main modules: a feature extraction and a Machine Learning (ML) module. For word-level QE, besides using features that explore the word alignments and POS-similarities between source and target texts, QuEst++ uses target-language features that are derived from n-gram language models. As n-gram language models rely primarily on local context, they can capture short-distance dependencies (e.g. article-noun agreements), but they fail to capture long-distance dependencies such as non-adjacent subject-verb agreements. The idea of using dependency structures in QE is not new. Bach et al. (2011) compared the dependency relations of Arabic source sentences and English MT translations and incorporated childfather and children correspondences as features in their ML system. They used source and target dependency structure features together with source-side and alignment content features to train a classifier for predicting word-level quality. Hardmeier et al. (2011) used tree kernels over constituency and dependency parses of MT input and output in conjunction with Support Vector Machine (SVM) classification for QE. A number of studies focused on detecting grammatical errors. Stymne and Ahrenberg (2010) used a mainly rule-based Swedish grammar checker not only to assess the grammaticality of their English-Swedish SMT system, but also for post-processing the MT output by applying the grammar checker suggestions. Ma and McKeown (2011), on the other hand, used feature-based lexicalized tree adjoining grammars (FB-LTAG) to detect and filter ungrammatical translations generated by their MT system. A Tree Adjoining Grammar (TAG) consists of a number of elementary trees, which can be combined with substitution and adjunction operations and while the derivation trees in TAG resemble dependency structures, the derived trees are phrase-structure trees (Joshi and Rambow, 2013).  The approaches we propose differ from previous work in several ways. First of all, we use only dependency tree information of the target language to detect grammatical errors in the MT output and by doing so we make a clear distinction on the type of MT errors we target. Second, in this exploratory study, we do not consider the QE task as a ML problem. Instead we try to gain insights in the strengths and weaknesses of the information the dependency structures provide for the QE task on sub-segment level, so that we can incorporate informative features in a ML system in the future. From this perspective, this method shows similarities to the GrETEL tool described by Augustinus et al. (2012), which allows users to query Dutch strings against a treebank and search for similar syntactic constructions. This application uses XPath1 for treebank querying and  
This paper describes how coordination has been integrated into a broad coverage statistical Minimalist Grammar parser currently under development, and presents a uniÔ¨Åed analysis for a number of coordinate (and related) constructions sometimes considered problematic for transformational syntax; these include across-theboard (ATB) head and phrasal movements, argument cluster coordination, right node raising and parasitic gaps. To accommodate all these structures, a number of novel extensions are introduced into the formalism, including a mechanism for excorporation which enables ATB head movement; this supplements a variant of Kobele‚Äôs (2008) mechanism for ATB phrasal movement. The weak expressive power of the formalism is shown to be unaffected by these extensions. 
In this paper, we present the redesign of an existing TAG for Arabic using a description language (so-called metagrammatical language). The use of such a language makes it easier for the linguist to share information among grammatical structures while ensuring a high degree of modularity within the target grammar. Additionally, this redesign beneÔ¨Åts from a grammar testing environment which is used to check both grammar coverage and overgeneration. 
Tree-Adjoining Grammars (TAG) have been used both for syntactic parsing, with sentential grammars, and for discourse parsing, with discourse grammars. But the modeling of discourse connectives (coordinate conjunctions, subordinate conjunctions, adverbs, etc.) in TAG-based formalisms for discourse differ from their modeling in sentential grammars. Because of this mismatch, an intermediate, not TAG-related, processing step is required between the sentential and the discourse processes, both in parsing and in generation. We present a method to smoothly interface sentential and discourse TAG grammars, without using such an intermediate processing step. This method is based on Abstract Categorial Grammars (ACG) and relies on the modularity of the latter. It also provides the possibility, as in D-STAG, to build discourse structures that are direct acyclic graphs (DAG) and not only trees. All the examples may be run and tested with the appropriate software. 
 from (1b) to produce (1c).1  We propose a new model in STAG syntax and semantics for subordinate conjunctions (SubConjs) and attributing phrases ‚Äì attitude/reporting verbs (AVs; believe, say) and attributing prepositional phrase (APPs; according to). This model is discourse-oriented, and is based on the observation that SubConjs and AVs are not homogeneous categories. Indeed, previous work has shown that SubConjs can be divided into two classes according to their syntactic and semantic properties. Similarly, AVs have two different uses in discourse: evidential and intentional. While evidential AVs and APPs have strong semantic similarities, they do not appear in the same contexts when SubConjs are at play. Our proposition aims at representing these distinctions and capturing these various discourse-related interactions. 
This paper develops a Ô¨Årst systematic approach to argument linking in LTAG, building on typologically oriented work in Van Valin (2005). While Van Valin‚Äôs argument linking mechanism is procedurally deÔ¨Åned, we propose a constraint-based implementation. The advantage is that we can separate between the linguistic generalizations to be captured and algorithmic considerations. The implementation is couched into the metagrammar framework eXtensible MetaGrammar (XMG). 
MC-TAG derivations using delayed locality (i.e. non set-local composition) have been used to model long distance binding in natural language. In this paper, I explore the possibility within Synchronous TAG that tree sets composing via delayed locality may have many possible syntactic derivations for a single semantic form. Using the Mandarin (Chinese) anaphor ziji as a test case, I show that the blocking effect which is described in other generative frameworks as a constraint on covert movement can be modelled as arising from constraints across possible derivation families for a given tree set. Further observing the interaction of ziji with other bound variables in the language, I propose that anti-locality effects in variable binding arise when a more specialized bound variable is available. This has consequences for the deÔ¨Ånition of a typology of bound variables. 
Because PCFGs are, as their name suggests, context-free, they cannot encode many dependencies that occur in natural language, such as the dependencies between determiners and nouns, allowing them to overgenerate phrases like those cat. One formalism that is able to capture many dependencies that PCFGs cannot is that of probabilistic tree-substitution grammars (PTSGs). Because PTSGs allow larger subtrees to be used as grammar rules, they can better model natural language but are also more difÔ¨Åcult to induce from a corpus. In this paper, I will show how PTSGs can be used to represent dependencies between determiners and nouns and present a novel method for inducing a PTSG from a parsed corpus. 
We discuss the use of supertags derived from a TAG in transition-based parsing. We show some initial experimental results which suggest that using a representation of a supertag in terms of its structural and linguistic dimensions outperforms the use of atomic supertags. 
TAGs were recently shown not to be closed under strong lexicalization but to be strongly lexicalizable by context-free tree grammars of rank 2. This paper presents an alternative lexicalization procedure that builds on an earlier generalization of TAGs to multi-dimensional trees. A previous theorem that every Tree Substitution grammar is strongly lexicalized by a corresponding TAG is lifted to higher dimensions to show that for every d-dimensional TAG there exists a (d + 1)-dimensional TAG that strongly lexicalizes it. A similar lifting reveals that d-dimensional TAGs are not closed under strong lexicalization, so for arbitrary TAGs an increase in dimensionality is an unavoidable consequence of strong lexicalization. 
Synchronous Hyperedge Replacement Graph Grammars (SHRG) can be used to translate between strings and graphs. In this paper, we study the capacity of these grammars to create non-projective dependency graphs. As an example, we use languages that contain cross serial dependencies. Lexicalized hyperedge replacement grammars can derive string languages (as path graphs) that contain an arbitrary number of these dependencies so that their derivation trees reÔ¨Çect the correct dependency graphs. We Ô¨Ånd that, in contrast, string-to-graph SHRG that derive dependency structures on the graph side are limited to derivations permitted by the string side. We show that, as a result, string-to-graph SHRG cannot capture languages with an unlimited degree of crossing dependencies. This observation has practical implications for the use of SHRG in semantic parsing. 
This paper presents an account of parasitic gaps in Synchronous TAG, making use of the more Ô¨Çexible semantic derivations that derive from the proposal in Frank and Storoshenko (2012) to add separate scope components to all predicates. We model parasitic gaps as deriving from a TAG analog of sidewards movement (Nunes, 2004), where the licensing wh-phrase combines Ô¨Årst with the domain containing the parasitic gap, which then combines with the main clause domain via tree-local multi-component combination. Such tree-local derivations are possible only because of the manipulations of scope available in the semantics. The phenomenon explored here not only shows the continued role of the syntax in constraining syntactic dependencies, but also demonstrates the potential for derivations which are syntactically well-formed, but are rendered impossible due to the improper binding of the parasitic gap variable. 
Motivated by the application of fact-level image understanding, we present an automatic method for data collection of structured visual facts from images with captions. Example structured facts include attributed objects (e.g., <Ô¨Çower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man, walking, dog>), and positional information (e.g., <vase, on, table>). The collected annotations are in the form of fact-image pairs (e.g.,<man, walking, dog> and an image region containing this fact). With a language approach, the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83% according to human judgment. Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms. We will make the data publically available. 
Explicit representations of images are useful for linguistic applications related to images. We design a representation based on Ô¨Årst-order models that capture the objects present in an image as well as their spatial relations. We take a supervised learning approach to the spatial relation classiÔ¨Åcation problem and study the effects of spatial and lexical information on prediction performance. We Ô¨Ånd that lexical information is required to accurately predict spatial relations when combined with location information, achieving an F-score of 0.80, compared to a most-frequent-class baseline of 0.62. 
Current evaluation metrics for image description may be too coarse. We therefore propose a series of binary forced-choice tasks that each focus on a different aspect of the captions. We evaluate a number of different off-the-shelf image description systems. Our results indicate strengths and shortcomings of both generation and ranking based approaches. 
In this study, we explore whether the captions in the wild can boost the performance of object detection in images. Captions that accompany images usually provide signiÔ¨Åcant information about the visual content of the image, making them an important resource for image understanding. However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation. In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection. We show how a pre-trained state-of-theart object detector can take advantage of noisy captions. Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection. 
There has been continuous growth in the volume and ubiquity of video material. It has become essential to deÔ¨Åne video semantics in order to aid the searchability and retrieval of this data. Although the method of annotating this data with keywords is relatively well researched, the quality can be improved through describing videos with natural language. We are exploring approaches to generating natural language descriptions of interrelations between human activities in a video stream. This paper focuses on creation of a dataset that can be used for development and evaluation. To this end a corpus of video clips, manually selected from the Hollywood2 dataset, and their natural language descriptions has been generated. Analysis of the hand annotation presents insights into human interests and thoughts. Such resource can be used to evaluate automatic natural language generation systems for video. 
We present a multi-modal dialogue system for interactive learning of perceptually grounded word meanings from a human tutor. The system integrates an incremental, semantic parsing/generation framework - Dynamic Syntax and Type Theory with Records (DS-TTR) - with a set of visual classiÔ¨Åers that are learned throughout the interaction and which ground the meaning representations that it produces. We use this system in interaction with a simulated human tutor to study the eÔ¨Äect of diÔ¨Äerent dialogue policies and capabilities on accuracy of learned meanings, learning rates, and eÔ¨Äorts/costs to the tutor. We show that the overall performance of the learning agent is aÔ¨Äected by (1) who takes initiative in the dialogues; (2) the ability to express/use their conÔ¨Ådence level about visual attributes; and (3) the ability to process elliptical as well as incrementally constructed dialogue turns. 
We provide a qualitative analysis of the descriptions containing negations (no, not, n‚Äôt, nobody, etc) in the Flickr30K corpus, and a categorization of negation uses. Based on this analysis, we provide a set of requirements that an image description system should have in order to generate negation sentences. As a pilot experiment, we used our categorization to manually annotate sentences containing negations in the Flickr30k corpus, with an agreement score of Œ∫=0.67. With this paper, we hope to open up a broader discussion of subjective language in image descriptions. 
 This preliminary study investigates whether, and to what extent, conceptual combination is conveyed by vision. Working with noun-noun compounds we show that, for some cases, the composed visual vector built with a simple additive model is effective in approximating the visual vector representing the complex concept. 
In this paper we look at the question of how to create good automatic methods for generating descriptions of spatial relationships between objects in images. In particular, we investigate the impact of varying different aspects of automatic method development, including using different preposition sets, models and feature sets. We Ô¨Ånd that optimising the preposition set improves previous best Accuracy from 46.2 to 50.2. Feature set optimisation further improves best Accuracy from 50.2 to 53.25. Naive Bayes models outperform SVMs and decision trees under all conditions tested. The utility of individual features depends on the model used, but the most useful features tend to capture a property pertaining to both objects jointly. 
We introduce the Multi30K dataset to stimulate multilingual multimodal research. Recent advances in image description have been demonstrated on Englishlanguage datasets almost exclusively, but image description should not be limited to English. This dataset extends the Flickr30K dataset with i) German translations created by professional translators over a subset of the English descriptions, and ii) German descriptions crowdsourced independently of the original English descriptions. We describe the data and outline how it can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks. 
In this paper, we investigate whether a neural network model can learn the meaning of natural language quantiÔ¨Åers (no, some and all) from their use in visual contexts. We show that memory networks perform well in this task, and that explicit counting is not necessary to the system‚Äôs performance, supporting psycholinguistic evidence on the acquisition of quantiÔ¨Åers. 
Differential diagnosis aims at distinguishing between diseases causing similar symptoms. This is exempliÔ¨Åed by epilepsies and dissociative disorders. Recently, it has been shown that linguistic features of physician-patient talks allow for differentiating between these two diseases. Since this method relies on trained linguists, it is not suitable for daily use. In this paper, we introduce a novel approach, called text2voronoi, for utilizing the paradigm of text visualization to reconstruct differential diagnosis as a task of text categorization. In line with current research on linguistic differential diagnosis, we explore linguistic characteristics of physician-patient talks to span our feature space. However, unlike standard approaches to categorization, we do not use linguistic feature spaces directly, but explore visual features derived from the talks‚Äô pictorial representations. That is, we provide an approach to image-driven differential diagnosis. By example of 24 talks of epileptics and dissociatively disordered patients, we show that our approach outperforms its counterpart based on the bag-of-words model. 
 Detecting discriminative semantic attributes from text which correlate with image features is one of the main challenges of zero-shot learning for Ô¨Åne-grained image classiÔ¨Åcation. Particularly, using full-length encyclopedic articles as textual descriptions has had limited success, one reason being that such documents contain many non-visual or unrelated sentences. We propose a method to automatically extract visually relevant sentences from Wikipedia documents. Our model, based on a convolutional neural network, is robustly tested through ground truth labeling obtained via Amazon Mechanical Turk, achieving 81.73% F1 measure. 
The goal of this task is to push the research frontier towards hybrid information systems. We aim to promote systems and approaches that are able to deal with the whole diversity of the Web, especially for, but not restricted to, the context of biomedicine. This goal is pursued by the organization of challenges. The fourth challenge, as the previous challenges, consisted of two tasks: semantic indexing and question answering. 16 systems participated by 7 different participating teams for the semantic indexing task. The question answering task was tackled by 37 different systems, developed by 11 different teams. 25 of the systems participated in the phase A of the task, while 12 participated in phase B. 3 of the teams participated in both phases of the question answering task. Overall, as in previous years, the best systems were able to outperform the strong baselines. This suggests that advances over the state of the art were achieved through the BIOASQ challenge but also that the benchmark in itself is very challenging. In this paper, we present the data used during the challenge as well as the technologies which were at the core of the participants‚Äô frameworks. 
For almost 15 years, the NLM Medical Text Indexer (MTI) system has been providing assistance to NLM Indexers, Catalogers, and the History of Medicine Division (HMD) in the task of indexing the ever increasing number of MEDLINE citations, with MTI‚Äôs role continuously expanding by providing more extensive and specialized coverage of the MEDLINE collection. The BioASQ Challenge has been a tremendous beneÔ¨Åt by expanding the knowledge of leading-edge indexing research. In this paper we present an indexing approach based on the Learning to Rank methodology which was successfully applied to the indexing task by several participants of recent Challenges. The proposed solution is designed to enhance the results that come from MTI by combining strengths of MTI with additional sources of evidence to produce a more accurate list of top MeSH Heading candidates for a MEDLINE citation being indexed. It incorporates novel Learning to Rank features and other enhancements to produce performance superior to that of MTI, both overall and for two speciÔ¨Åc classes of MeSH Headings for which MTI has shown poor performance. 
This paper describes the participation of LABDA team in the 2016 BioASQ Task 4a on large-scale online biomedical semantic indexing. Our approach is based on the use of the open source search engine ElasticSearch. Experimental results show that our approach achieves high recall while keeping processing time low. Although more work needs to be done to improve our results, we can conclude that ElasticSearch is a competitive and scalable system for indexing biomedical literature. 
This paper describes the OAQA system evaluated in the BioASQ 4B Question Answering track. The system extends the Yang et al. (2015) system and integrates additional biomedical and generalpurpose NLP annotators, machine learning modules for search result scoring, collective answer reranking, and yes/no answer prediction. We Ô¨Årst present the overall architecture of the system, and then focus on describing the main extensions to the Yang et al. (2015) approach. Before the ofÔ¨Åcial evaluation, we used the development dataset (excluding the 3B Batch 5 subset) for training. We present initial evaluation results on a subset of the development data set to demonstrate the effectiveness of the proposed new methods, and focus on performance analysis of yes/no question answering. 
Question answering (QA) systems are crucial when searching for exact answers for natural language questions in the biomedical domain. Answers to many of such questions can be extracted from the 26 millions biomedical publications currently included in MEDLINE when relying on appropriate natural language processing (NLP) tools. In this work we describe our participation in the task 4b of the BioASQ challenge using two QA systems that we developed for biomedicine. Preliminary results show that our systems achieved Ô¨Årst and second positions in the snippet retrieval sub-task and for the generation of ideal answers. 
This paper describes a question‚Äì answering system that returns relevant documents and snippets (with particular emphasis on snippets) from a large medical document collection. The system is implemented as part of our participation to Phase A of Task 4b in the 2016 BioASQ Challenge. The proposed system retrieves candidate answer sentences using a cluster‚Äìbased language model. Then, it re‚Äìranks the retrieved top-n sentences using Ô¨Åve independent similarity models based on shallow semantic analysis. The experimental results show that the proposed system is the Ô¨Årst to Ô¨Ånd snippets in batches 2 (MAP 0.0604), 3 (MAP 0.0728), 4 (MAP 0.1182), and 5 (MAP 0.1187). 
In this paper we present the methods and approaches employed in terms of our participation in the 2016 version of the BioASQ challenge. For the semantic indexing task, we extended our successful ensemble approach of last year with additional models. The ofÔ¨Åcial results obtained so-far demonstrate a continuing consistent advantage of our approaches against the National Library of Medicine (NLM) baselines. For the question answering task, we extended our approach on factoid questions, while we also developed approaches for the document, concept and snippet retrieval sub-tasks. 
Even a simple biological phenomenon may introduce a complex network of molecular interactions. ScientiÔ¨Åc literature is one of the trustful resources delivering knowledge of these networks. We propose LitWay, a system for extracting semantic relations from texts. LitWay utilizes a hybrid method that combines both a rule-based method and a machine learning-based method. It is tested on the SeeDev task of BioNLP-ST 2016, achieves the state-of-the-art performance with the F-score of 43.2%, ranking Ô¨Årst of all participating teams. To further reveal the linguistic characteristics of each event, we test the system solely with syntactic rules or machine learning, and different combinations of two methods. We Ô¨Ånd that it is difÔ¨Åcult for one method to achieve good performance for all semantic relation types due to the complication of bio-events in the literatures. 
The number of scientiÔ¨Åc papers published each year is growing exponentially and given the rate of this growth, automated information extraction is needed to efÔ¨Åciently extract information from this corpus. A critical Ô¨Årst step in this process is to accurately recognize the names of entities in text. Previous efforts, such as SPECIES, have identiÔ¨Åed bacteria strain names, among other taxonomic groups, but have been limited to those names present in NCBI taxonomy. We have implemented a dictionary-based named entity tagger, TagIt, that is followed by a rule based expansion system to identify bacteria strain names and habitats and resolve them to the closest match possible in the NCBI taxonomy and the OntoBiotope ontology respectively. The rule based post processing steps expand acronyms, and extend strain names according to a set of rules, which captures additional aliases and strains that are not present in the dictionary. TagIt has the best performance out of three entries to BioNLP-ST BB3 cat+ner, with an overall SER of 0.628 on the independent test set. 
This paper presents our participation in the Bacteria/Biotope track from the 2016 BioNLP Shared-Task. Our methods rely on a combination of distinct machinelearning and rule-based systems. We used CRF and post-processing rules to identify mentions of bacteria and biotopes, a rulebased approach to normalize the concepts in the ontology and the taxonomy, and SVM to identify relations between bacteria and biotopes. On the test datasets, we achieved similar results to those obtained on the development datasets: on the categorization task, precision of 0.503 (gold standard entities) and SER of 0.827 (both NER and categorization); on the event relation task, F-measure of 0.485 (gold standard entities, ranking third out of 11) and of 0.192 (both NER and event relation, ranking Ô¨Årst); on the knowledgebased task, mean references of 0.771 (gold standard entities) and of 0.202 (both NER, categorization and event relation). 
This paper describes our system to extract binary regulatory relations from text, used to participate in the SeeDev task of BioNLP-ST 2016. Our system was based on machine learning, using support vector machines with a shallow linguistic kernel to identify each type of relation. Additionally, we employed a distant supervised approach to increase the size of the training data. Our submission obtained the third best precision of the SeeDev-binary task. Although the distant supervised approach did not signiÔ¨Åcantly improve the results, we expect that by exploring other techniques to use unlabeled data should lead to better results. 
 We participate in the BB3 and GE4 tasks of BioNLPST 2016. In the BB3 task, we adopt word representation methods to improve the feature-based Biomedical Event Extraction System, and take the 4th place. In the GE4 task, based on the Uturku system, a two-stage method is proposed for trigger detection, which divides trigger detection into recognition stage and classification stage, using different features in each stage. In the edge detection, we adopt Passiveaggressive (PA) online algorithm, then we constitute events by post-processing of TEES.  
We propose a machine learning approach for semantic recognition and normalization of clinical term descriptions. Clinical terms considered here are noisy descriptions in Spanish language written by health care professionals in our electronic health record system. These description terms contain clinical Ô¨Åndings, family history, suspected disease, among other categories of concepts. Descriptions are usually very short texts presenting high lexical variability containing synonymy, acronyms, abbreviations and typographical errors. Mapping description terms to normalized descriptions requires medical expertise which makes it difÔ¨Åcult to develop a rule-based knowledge engineering approach. In order to build a training dataset we use those descriptions that have been previously matched by terminologists to the hospital thesaurus database. We generate a set of feature vectors based on pairs of descriptions involving their individual and joint characteristics. We propose an unsupervised learning approach to discover term equivalence classes including synonyms, abbreviations, acronyms and frequent typographical errors. We evaluate different combinations of features to train MaxEnt and XGBoost models. Our system achieves an F1 score of 89% on the Hospital Italiano de Buenos Aires (HIBA) problem list. 
Most existing corpus-based approaches to semantic representation suffer from inaccurate modeling of domain-speciÔ¨Åc lexical items which either have low frequencies or are non-existent in open-domain corpora. We put forward a technique that improves word embeddings in speciÔ¨Åc domains by Ô¨Årst transforming a given lexical item to a sorted list of representative words and then modeling the item by combining the embeddings of these words. Our experiments show that the proposed technique can signiÔ¨Åcantly improve some of the recent word embedding techniques while modeling a set of lexical items in the biomedical domain, i.e., phenotypes. 
Gradable adjectives are inherently vague and are used by clinicians to document medical interpretations (e.g., severe reaction, mild symptoms). We present a comprehensive study of gradable adjectives used in the clinical domain. We automatically identify gradable adjectives and demonstrate that they have a substantial presence in clinical text. Further, we show that there is a speciÔ¨Åc pattern associated with their usage, where certain medical concepts are more likely to be described using these adjectives than others. Interpretation of statements using such adjectives is a barrier in medical decision making. Therefore, we use a simple probabilistic model to ground their meaning based on their usage in context. 
We consider the use of distant supervision for biological information extraction, and introduce two understudied corpora of this form, the Biological Expression Language (BEL) Large Corpus and the Pathway Logic (PL) Datum Corpus. Each resource eschews annotation at the sentence constituent level, and the PL corpus requires synthesis of information across multiple sentences to construct composite knowledge frames. Decomposing this problem into feature induction for slotlevel attributes, followed by event assembly over this space of features, we introduce a novel, general-purpose pattern induction procedure, evaluating it against these two corpora, demonstrating its ability to induce effective detection against dependency parses. 
Biomedical relations are often expressed between entities occurring within the same sentence through syntactic means. However, a signiÔ¨Åcant portion of such relations (in particular, causal relations) are expressed implicitly across sentence boundaries. Inferring these discourse-level relations can be challenging in the absence of syntactic clues. In this paper, we present a study of textual characteristics that contribute to expression of implicit causal relations across sentence boundaries. Focusing on a chemical-disease relationship corpus, we identify and investigate the contribution of various features that can assist in identifying such inter-sentential relations. Using these features for supervised learning, we were able to improve previously reported best results by more than 13%. Our results demonstrate the usefulness of the proposed features and the importance of using a balanced dataset for this task. 
We propose an approach for biomedical information extraction that marries the advantages of machine learning models, e.g., learning directly from data, with the beneÔ¨Åts of rule-based approaches, e.g., interpretability. Our approach starts by training a feature-based statistical model, then converts this model to a rule-based variant by converting its features to rules, and ‚Äúsnapping to grid‚Äù the feature weights to discrete votes. In doing so, our proposal takes advantage of the large body of work in machine learning, but it produces an interpretable model, which can be directly edited by experts. We evaluate our approach on the BioNLP 2009 event extraction task. Our results show that there is a small performance penalty when converting the statistical model to rules, but the gain in interpretability compensates for that: with minimal effort, human experts improve this model to have similar performance to the statistical model that served as starting point. 
Extracting bio-entity relations has emerged as an important task due to the ever-growing number of bio-medical documents. In this paper, we present a simple and novel representation for extracting bio-entity relationships. The state-of-theart systems for such tasks rely on word based representations and variations of linguistic driven features. In contrast, we model bio-text by the most basic character based string representation with a family of string kernels. This eliminates time consuming parsing, issue of rare words and domain speciÔ¨Åc pre-processing. This simple representation makes our approach fast and Ô¨Çexible for any bio-NLP dataset. We demonstrate comparable performance and faster computation time of our approach versus previous state-of-the-art kernel methods. 
Entity disambiguation in the biomedical domain is an essential task in any text mining pipeline. Much existing work shares one limitation, in that their model training prerequisite and/or runtime computation are too expensive to be applied to all ambiguous entities in real-time. We propose an automatic, light-weight method that processes MEDLINE abstracts at largescale and with high-quality output. Our method exploits MeSH terms and knowledge in UMLS to Ô¨Årst identify unambiguous anchor entities, and then disambiguate remaining entities via heuristics. Experiments showed that our method is 79.6% and 87.7% accurate under strict and relaxed rating schemes, respectively. When compared to MetaMap‚Äôs disambiguation, our method is one order of magnitude faster with a slight advantage in accuracy. 
In this paper, we report a knowledge-based method for Word Sense Disambiguation in the domains of biomedical and clinical text. We combine word representations created on large corpora with a small number of deÔ¨Ånitions from the UMLS to create concept representations, which we then compare to representations of the context of ambiguous terms. Using no relational information, we obtain comparable performance to previous approaches on the MSH-WSD dataset, which is a well-known dataset in the biomedical domain. Additionally, our method is fast and easy to set up and extend to other domains. Supplementary materials, including source code, can be found at https: //github.com/clips/yarn 
Document classiÔ¨Åcation is an important and common application in natural language processing. Scaling classiÔ¨Åcation approaches to many targets faces a bottleneck in acquiring gold standard labels. In this work, we develop and evaluate a method for using informed topic models to noisily label documents, creating a noisy but usable set of labels for training discriminative classiÔ¨Åers. We investigate multiple ways to train this noisy classiÔ¨Åer, and the best performing method uses Wikipedia-seeded topic models to approximately label training instances without any supervision. We evaluate these methods on the classiÔ¨Åcation task as well as in an active learning setting, in which they are shown to improve learning rates over traditional active learning. 
Extracting information from mental health records can be useful for large-scale clinical studies (e.g., to predict medication adherence or to understand medication effects) in this clinical specialty largely underserved by the Natural Language Processing (NLP) community. Vocabularies that contain medical terms for speciÔ¨Åc clinical use-cases, such as signs, symptoms, histories, social risk factors, are valuable resources for the development of NLP systems that aid clinicians in extracting information from text. Substance abuse is an important variable for many clinical use-cases, but, to our knowledge, there are no publicly available vocabularies that cover these types of terms. In this study, we apply and combine three methods for generating vocabularies related to substance abuse. We propose a simple and systematic method to generate highly relevant vocabularies and evaluate these vocabularies with respect to size and content, as well as coverage and relevance when applied to authentic psychiatric notes. 
Although advanced text mining methods speciÔ¨Åcally adapted to the biomedical domain are continuously being developed, their applications on large scale have been scarce. One of the main reasons for this is the lack of computational resources and workforce required for processing large text corpora. In this paper we present a publicly available resource distributing preprocessed biomedical literature including sentence splitting, tokenization, part-of-speech tagging, syntactic parses and named entity recognition. The aim of this work is to support the future development of largescale text mining resources by eliminating the time consuming but necessary preprocessing steps. This resource covers the whole of PubMed and PubMed Central Open Access section, currently containing 26M abstracts and 1.4M full articles, constituting over 388M analyzed sentences. The resource is based on a fully automated pipeline, guaranteeing that the distributed data is always up-to-date. The resource is available at https://turkunlp. github.io/pubmed_parses/. 
Temporal relation extraction is important for understanding the ordering of events in narrative text. We describe a method for increasing the number of high-quality training instances available to a temporal relation extraction task, with an adaptation to different annotation styles in the clinical domain by taking advantage of the UniÔ¨Åed Medical Language System (UMLS). This method notably improves clinical temporal relation extraction, works beyond featurizing or duplicating the same information, can generalize between-argument signals in a more effective and robust fashion. We also report a new state-of-the-art result, which is a two point improvement over the best Clinical TempEval 2016 system.  sions. For example, the THYME corpus, consisting of oncology, pathology and radiology notes, annotated only event headwords (Styler et al., 2014), while the i2b2 corpus, consisting of discharge summaries, annotated entire noun phrases as events (Sun et al., 2013). As a result, it is necessary to account for these differences when implementing a generalizable relation extraction system. However, the annotations of the temporal relations between the events remain unaffected by the choice of headwords or phrases for the event annotation. For example, in a relation between the temporal expression yesterday and the event severe lower abdominal pain, if the argument had been the head word pain it still would have been an instance of the same temporal relation. Thus, we can automatically create additional training examples by varying the extent of headword expansion. For example, the relation between yesterday and severe  
We propose a document retrieval method for question answering that represents documents and questions as weighted centroids of word embeddings and reranks the retrieved documents with a relaxation of Word Mover‚Äôs Distance. Using biomedical questions and documents from BIOASQ, we show that our method is competitive with PUBMED. With a top-k approximation, our method is fast, and easily portable to other domains and languages. 
This paper evaluates the difference between human pathway curation and current NLP systems. We propose graph analysis methods for quantifying the gap between human curated pathway maps and the output of state-of-the-art automatic NLP systems. Evaluation is performed on the popular mTOR pathway. Based on analyzing where current systems perform well and where they fail, we identify possible avenues for progress. 
Studies have shown that Twitter can be used for health surveillance, and personal experience tweets (PETs) are an important source of information for health surveillance. To mine Twitter data requires a relatively balanced corpus and it is challenging to construct such a corpus due to the labor-intensive annotation tasks of large data sets. We developed a bootstrap method of Ô¨Ånding PETs with the use of the machine learning-based Ô¨Ålter. Through a few iterations, our approach can efÔ¨Åciently improve the balance of two class dataset with a reduced amount of annotation work. To demonstrate the usefulness of our method, a PET corpus related to effects caused by 4 dietary supplements was constructed. In 3 iterations, a corpus of 8,770 tweets was obtained from 108,528 tweets collected, and the imbalance of two classes was signiÔ¨Åcantly reduced from 1:31 to 1:3. In addition, two out of three classiÔ¨Åers used showed improved performance over iterations. It is conceivable that our approach can be applied to various other health surveillance studies that use machine learning-based classiÔ¨Åcations of imbalanced Twitter data. 
 et al., 2015). For example, Turian et al. (2010)  Word embeddings have been successfully exploited in systems for NLP tasks, such as parsing and text classiÔ¨Åcation. It is intuitive that word embeddings created from a larger corpus would provide a better coverage of vocabulary. Meanwhile, word embeddings trained on a corpus related to the given task or target domain would more effectively represent the semantics of terms. However, in some emerging domains (e.g. bio-surveillance using social media data), it may be difÔ¨Åcult to Ô¨Ånd a domain corpus that is large enough for creating effective word embeddings. To deal with this problem, we propose novel approaches that use both word embeddings created from generic and target domain corpora. Our experimental results on sentence classiÔ¨Åcation tasks show that our approaches signiÔ¨Åcantly improve the performance of an existing convolutional neural network that achieved state-of-the-art performances on several text classiÔ¨Åcation tasks.  used word embeddings as input features for several NLP systems, including a traditional chunking system based on conditional random Ô¨Åelds (CRFs) (Lafferty et al., 2001). Collobert et al. (2011) used word embeddings as inputs of a multilayer neural network for part-of-speech tagging, chunking, named entity recognition and semantic role labelling. Limsopatham and Collier (2016) leveraged semantics from word embeddings when identifying medical concepts mentioned in social media messages. Kim (2014) showed that using pre-built word embeddings, induced from 100 billion words of Google News using word2vec (Mikolov et al., 2013), as inputs of a simple convolutional neural network (CNN) could achieve state-of-the-art performances on several sentence classiÔ¨Åcation tasks, such as classiÔ¨Åcation of positive and negative reviews of movies (Pang and Lee, 2005) and consumer products, e.g. cameras (Hu and Liu, 2004). The quality of word embeddings (e.g. the ability to capture semantics of words) highly depends on the corpus from which they are induced (Pennington et al., 2014). For instance, when induced  
Term normalization is frequently used in information retrieval task to reduce variant word forms to a common form. The most general term normalization technique used in practice is stemming, however it has been found to not be completely reliable. Here we present PubTermVariants, a high-quality data-driven resource of term variant pairs that can improve search results in PubMed. For a given pair, we consider two terms to be variants if they stem to the same form, pass the hypergeometric test, and pass the morpho-semantic test. We perform manual evaluation of a subset of PubTermVariants that confirms the high quality of the candidate pairs. We further present experiments that demonstrate their usefulness for PubMed search. 
Causal precedence between biochemical interactions is crucial in the biomedical domain, because it transforms collections of individual interactions, e.g., bindings and phosphorylations, into the causal mechanisms needed to inform meaningful search and inference. Here, we analyze causal precedence in the biomedical domain as distinct from open-domain, temporal precedence. First, we describe a novel, hand-annotated text corpus of causal precedence in the biomedical domain. Second, we use this corpus to investigate a battery of models of precedence, covering rule-based, feature-based, and latent representation models. The highestperforming individual model achieved a micro F1 of 43 points, approaching the best performers on the simpler temporalonly precedence tasks. Feature-based and latent representation models each outperform the rule-based models, but their performance is complementary to one another. We apply a sieve-based architecture to capitalize on this lack of overlap, achieving a micro F1 score of 46 points. 
IdentiÔ¨Åcation of the certainty of events is an important text mining problem. In particular, biomedical texts report medical conditions or Ô¨Åndings that might be factual, hedged or negated. IdentiÔ¨Åcation of negation and its scope over a term of interest determines whether a Ô¨Ånding is reported and is a challenging task. Not much work has been performed for Spanish in this domain. In this work we introduce different algorithms developed to determine if a term of interest is under the scope of negation in radiology reports written in Spanish. The methods include syntactic techniques based in rules derived from PoS tagging patterns, constituent tree patterns and dependency tree patterns, and an adaption of NegEx, a well known rule-based negation detection algorithm (Chapman et al., 2001a). All methods outperform a simple dictionary lookup algorithm developed as baseline. NegEx and the PoS tagging pattern method obtain the best results with 0.92 F1. 
The quality of word embeddings depends on the input corpora, model architectures, and hyper-parameter settings. Using the state-of-the-art neural embedding tool word2vec and both intrinsic and extrinsic evaluations, we present a comprehensive study of how the quality of embeddings changes according to these features. Apart from identifying the most inÔ¨Çuential hyper-parameters, we also observe one that creates contradictory results between intrinsic and extrinsic evaluations. Furthermore, we Ô¨Ånd that bigger corpora do not necessarily produce better biomedical domain word embeddings. We make our evaluation tools and resources as well as the created state-of-the-art word embeddings available under open licenses from https://github.com/ cambridgeltl/BioNLP-2016. 
For many types of high-dimensional data, such as natural language corpora, the vast majority of extracted variables or features are essentially noise. Culling such features can not only reveal important patterns, but also improve the performance of supervised and unsupervised machine algorithms. Most research on feature selection has focused on the statistical measures used to rank features. Meanwhile, little work has been done developing techniques for identifying the optimal subset of features without repeatedly training models. However, developing such techniques is important, as they can signiÔ¨Åcantly decrease computation time while providing a way to determine the features that characterize the classes within a data set, independent of how the data may be classiÔ¨Åed in the future. Here we introduce a novel method based on information foraging that works in conjunction with existing feature ranking methods to automatically determine a subset of important features. The method is demonstrated on simulated and linguistic data from psychiatric interviews. We show that the method is able to accurately determine the features that characterize the classes within both data sets. The method is fast, simple, and independent of any method of classifying the data, and can be extended to any highdimensional data set. 
Detecting phenotype descriptors in text and linking them to ontology concepts is a challenging task. Current state-of-the art concept recognizers struggle with several issues due the variety of human expressiveness. Here we present initial results of creating a dictionary of lexical variants for the Human Phenotype Ontology. This work is a smaller but important part of a larger project with a goal to improve recall in phenotype concept recognizers. 
Detecting healthcare-associated infections pose a major challenge in healthcare. Using natural language processing and machine learning applied on electronic patient records is one approach that has been shown to work. However the results indicate that there was room for improvement and therefore we have applied deep learning methods. SpeciÔ¨Åcally we implemented a network of stacked sparse auto encoders and a network of stacked restricted Boltzmann machines. Our best results were obtained using the stacked restricted Boltzmann machines with a precision of 0.79 and a recall of 0.88. 
In recent years extracting relevant information from biomedical and clinical texts such as research articles, discharge summaries, or electronic health records have been a subject of many research efforts and shared challenges. Relation extraction is the process of detecting and classifying the semantic relation among entities in a given piece of texts. Existing models for this task in biomedical domain use either manually engineered features or kernel methods to create feature vector. These features are then fed to classiÔ¨Åer for the prediction of the correct class. It turns out that the results of these methods are highly dependent on quality of user designed features and also suffer from curse of dimensionality. In this work we focus on extracting relations from clinical discharge summaries. Our main objective is to exploit the power of convolution neural network (CNN) to learn features automatically and thus reduce the dependency on manual feature engineering. We evaluate performance of the proposed model on i2b2-2010 clinical relation extraction challenge dataset. Our results indicate that convolution neural network can be a good model for relation exaction in clinical text without being dependent on expert‚Äôs knowledge on deÔ¨Åning quality features. 
In recent years, social media has revolutionized how people communicate and share information. One function of social media, besides connecting with friends, is sharing opinions with others. Micro blogging sites, like Twitter, have often provided an online forum for social activism. When users debate about controversial topics on social media, they typically share different types of evidence to support their claims. Classifying these types of evidence can provide an estimate for how adequately the arguments have been supported. We first introduce a manually built gold standard dataset of 3000 tweets related to the recent FBI and Apple encryption debate. We develop a framework for automatically classifying six evidence types typically used on Twitter to discuss the debate. Our findings show that a Support Vector Machine (SVM) classifier trained with n-gram and additional features is capable of capturing the different forms of representing evidence on Twitter, and exhibits significant improvements over the unigram baseline, achieving a F1 macroaveraged of 82.8%. 
Existing approaches to summarizing multi-party argumentative conversations in reader comment are extractive and fail to capture the argumentative nature of these conversations. Work on argument mining proposes schemes for identifying argument elements and relations in text but has not yet addressed how summaries might be generated from a global analysis of a conversation based on these schemes. In this paper we: (1) propose an issue-centred scheme for analysing and graphically representing argument in reader comment discussion in on-line news, and (2) show how summaries capturing the argumentative nature of reader comment can be generated from our graphical representation. 
Argumentative texts have been thoroughly analyzed for their argumentative structure, and recent efforts aim at their automatic classiÔ¨Åcation. This work investigates linguistic properties of argumentative texts and text passages in terms of their semantic clause types. We annotate argumentative texts with Situation Entity (SE) classes, which combine notions from lexical aspect (states, events) with genericity and habituality of clauses. We analyse the correlation of SE classes with argumentative text genres, components of argument structures, and some functions of those components. Our analysis reveals interesting relations between the distribution of SE types and the argumentative text genre, compared to other genres like Ô¨Åction or report. We also see tendencies in the correlations between argument components (such as premises and conclusions) and SE types, as well as between argumentative functions (such as support and rebuttal) and SE types. The observed tendencies can be deployed for automatic recognition and Ô¨Åne-grained classiÔ¨Åcation of argumentative text passages. 
Enthymemes, that are arguments with missing premises, are common in natural language text. They pose a challenge for the Ô¨Åeld of argument mining, which aims to extract arguments from such text. If we can detect whether a premise is missing in an argument, then we can either Ô¨Åll the missing premise from similar/related arguments, or discard such enthymemes altogether and focus on complete arguments. In this paper, we draw a connection between explicit vs. implicit opinion classiÔ¨Åcation in reviews, and detecting arguments from enthymemes. For this purpose, we train a binary classiÔ¨Åer to detect explicit vs. implicit opinions using a manually labelled dataset. Experimental results show that the proposed method can discriminate explicit opinions from implicit ones, thereby providing encouraging Ô¨Årst step towards enthymeme detection in natural language texts. 
Argument mining integrates many distinct computational linguistics tasks, and as a result, reporting agreement between annotators or between automated output and gold standard is particularly challenging. More worrying for the Ô¨Åeld, agreement and performance are also reported in a wide variety of different ways, making comparison between approaches difÔ¨Åcult. To solve this problem, we propose the CASS technique for combining metrics covering different parts of the argument mining task. CASS delivers a justiÔ¨Åed method of integrating results yielding confusion matrices from which CASS-Œ∫ and CASS-F1 scores can be calculated. 
Legal argumentation often centers on the interpretation and understanding of terminology. Statutory texts are known for a frequent use of vague terms that are difÔ¨Åcult to understand. Arguments about the meaning of statutory terms are an inseparable part of applying statutory law to a speciÔ¨Åc factual context. In this work we investigate the possibility of supporting this type of argumentation by automatic extraction of sentences that deal with the meaning of a term. We focus on case law because court decisions often contain sentences elaborating on the meaning of one or more terms. We show that human annotators can reasonably agree on the usefulness of a sentence for an argument about the meaning (interpretive usefulness) of a speciÔ¨Åc statutory term (kappa>0.66). We specify a list of features that could be used to predict the interpretive usefulness of a sentence automatically. We work with offthe-shelf classiÔ¨Åcation algorithms to conÔ¨Årm the hypothesis (accuracy>0.69). 
This paper describes and evaluates a novel feature set for stance classiÔ¨Åcation of argumentative texts; i.e. deciding whether a post by a user is for or against the issue being debated. We model the debate both as attitude bearing features, including a set of automatically acquired ‚Äòtopic terms‚Äô associated with a Distributional Lexical Model (DLM) that captures the writer‚Äôs attitude towards the topic term, and as dependency features that represent the points being made in the debate. The stance of the text towards the issue being debated is then learnt in a supervised framework as a function of these features. The main advantage of our feature set is that it is scrutable: The reasons for a classiÔ¨Åcation can be explained to a human user in natural language. We also report that our method outperforms previous approaches to stance classiÔ¨Åcation as well as a range of baselines based on sentiment analysis and topic-sentiment analysis. 
In this paper, we investigate the relationship between argumentation structures and (a) argument content, and (b) the holistic quality of an argumentative essay. Our results suggest that structure-based approaches hold promise for automated evaluation of argumentative writing. 
Evidences that support a claim ‚Äúa subject phrase promotes or suppresses a value‚Äù help in making a rational decision. We aim to construct a model that can classify if a particular evidence supports a claim of a promoting/suppressing relationship given an arbitrary subject-value pair. In this paper, we propose a recurrent neural network (RNN) with an attention model to classify such evidences. We incorporated a word embedding technique in an attention model such that our method generalizes for never-encountered subjects and value phrases. Benchmarks showed that the method outperforms conventional methods in evidence classiÔ¨Åcation tasks. 
The annotation of argument schemes represents an important step for argumentation mining. General guidelines for the annotation of argument schemes, applicable to any topic, are still missing due to the lack of a suitable taxonomy in Argumentation Theory and the need for highly trained expert annotators. We present a set of guidelines for the annotation of argument schemes, taking as a framework the Argumentum Model of Topics (Rigotti and Morasso, 2010; Rigotti, 2009). We show that this approach can contribute to solving the theoretical problems, since it offers a hierarchical and Ô¨Ånite taxonomy of argument schemes as well as systematic, linguistically-informed criteria to distinguish various types of argument schemes. We describe a pilot annotation study of 30 persuasive essays using multiple minimally trained non-expert annotators .Our Ô¨Åndings from the confusion matrixes pinpoint problematic parts of the guidelines and the underlying annotation of claims and premises. We conduct a second annotation with reÔ¨Åned guidelines and trained annotators on the 10 essays which received the lowest agreement initially. A significant improvement of the inter-annotator agreement shows that the annotation of argument schemes requires highly trained annotators and an accurate annotation of argumentative components (premises and claims). 
In this paper we examine the application of an unsupervised extractive summarisation algorithm, TextRank, on a different task, the identiÔ¨Åcation of argumentative components. Our main motivation is to examine whether there is any potential overlap between extractive summarisation and argument mining, and whether approaches used in summarisation (which typically model a document as a whole) can have a positive effect on tasks of argument mining. Evaluation has been performed on two corpora containing user posts from an on-line debating forum and persuasive essays. Evaluation results suggest that graph-based approaches and approaches targeting extractive summarisation can have a positive effect on tasks related to argument mining. 
On the basis of a new corpus of short ‚Äúmicrotexts‚Äù with parallel manual annotations, we study the mapping from discourse structure (in terms of Rhetorical Structure Theory, RST) to argumentation structure. We Ô¨Årst perform a qualitative analysis and discuss our Ô¨Åndings on correspondence patterns. Then we report on experiments with deriving argumentation structure from the (gold) RST trees, where we compare a tree transformation model, an aligner based on subgraph matching, and a more complex ‚Äúevidence graph‚Äù model. 
In this paper, we introduce an approach for recognizing the absence of opposing arguments in persuasive essays. We model this task as a binary document classiÔ¨Åcation and show that adversative transitions in combination with unigrams and syntactic production rules signiÔ¨Åcantly outperform a challenging heuristic baseline. Our approach yields an accuracy of 75.6% and 84% of human performance in a persuasive essay corpus with various topics. 
We describe the construction of an Expert Stance Graph, a novel, large-scale knowledge resource that encodes the stance of more than 100,000 experts towards a variety of controversial topics. We suggest that this graph may be valuable for various fundamental tasks in computational argumentation. Experts and topics in our graph are Wikipedia entries. Both automatic and semi-automatic methods for building the graph are explored, and manual assessment validates the high accuracy of the resulting graph. 
Identifying the main claims occurring across texts is important for large-scale argumentation mining from social media. However, the claims that users make are often unclear and build on implicit knowledge, effectively introducing a gap between the claims. In this work, we study the problem of matching user claims to predeÔ¨Åned main claims, using implicit premises to Ô¨Åll the gap. We build a dataset with implicit premises and analyze how human annotators Ô¨Åll the gaps. We then experiment with computational claim matching models that utilize these premises. We show that using manually-compiled premises improves similarity-based claim matching and that premises generalize to unseen user claims. 
Online communities host growing numbers of discussions amongst large groups of participants on all manner of topics. This user-generated content contains millions of statements of opinions and ideas. We propose an abstractive approach to summarize such argumentative discussions, making key content accessible through ‚Äòpoint‚Äô extraction, where a point is a verb and its syntactic arguments. Our approach uses both dependency parse information and verb case frames to identify and extract valid points, and generates an abstractive summary that discusses the key points being made in the debate. We performed a human evaluation of our approach using a corpus of online political debates and report signiÔ¨Åcant improvements over a highperforming extractive summarizer. 
 2 Previous research  Topic-independent expressions for conveying agreement and disagreement were annotated in a corpus of web forum debates, in order to evaluate a classiÔ¨Åer trained to detect these two categories. Among the 175 expressions annotated in the evaluation set, 163 were unique, which shows that there is large variation in expressions used. This variation might be one of the reasons why the task of automatically detecting the categories was difÔ¨Åcult. F-scores of 0.44 and 0.37 were achieved by a classiÔ¨Åer trained on 2,000 debate sentences for detecting sentencelevel agreement and disagreement. 
This paper proposes a new task in argument mining in online debates. The task includes three annotations steps that result in Ô¨Åne-grained annotations of agreement and disagreement at a propositional level. We report on the results of a pilot annotation task on identifying sentences that are directly addressed in the comment. 
In this paper, we propose a task for quality evaluation of disputing argument. In order to understand the disputation behavior, we propose three sub-tasks, detecting disagreement hierarchy, refutation method and argumentation strategy respectively. We Ô¨Årst manually labeled a real dataset collected from an online debating forum. The dataset includes 45 disputing argument pairs. The annotation scheme is developed by three NLP researchers via annotating all the argument pairs in the dataset. Two under-graduate students are then trained to annotate the same dataset. We report annotation results from both groups. Then, another larger dataset was annotated and we show analysis of the correlation between disputing quality and different disputation behaviors. 
Traditional name transliteration methods largely ignore source context information and inter-dependency among entities for entity disambiguation. We propose a novel approach to leverage state-of-the-art Entity Linking (EL) techniques to automatically correct name transliteration results, using collective inference from source contexts and additional evidence from knowledge base. Experiments on transliterating names from seven languages to English demonstrate that our approach achieves 2.6% to 15.7% absolute gain over the baseline model, and significantly advances state-of-the-art. When contextual information exists, our approach can achieve further gains (24.2%) by collectively transliterating and disambiguating multiple related entities. We also prove that combining Entity Linking and projecting resources from related languages obtained comparable performance as the method using the same amount of training pairs in the original languages without Entity Linking.1 
Typed lexicons that encode knowledge about the semantic types of an entity name, e.g., that ‚ÄòParis‚Äô denotes a geolocation, product, or person, have proven useful for many text processing tasks. While lexicons may be derived from large-scale knowledge bases (KBs), KBs are inherently imperfect, in particular they lack coverage with respect to long tail entity names. We infer the types of a given entity name using multi-source learning, considering information obtained by alignment to the Freebase knowledge base, Web-scale distributional patterns, and global semi-structured contexts retrieved by means of Web search. Evaluation in the challenging domain of social media shows that multi-source learning improves performance compared with rule-based KB lookups, boosting typing results for some semantic categories. 
Name entity recognition (NER) is an important subtask in natural language processing. Various NER systems have been developed in the last decade. They may target for different domains, employ different methodologies, work on different languages, detect different types of entities, and support different inputs and output formats. These conditions make it difficult for a user to select the right NER tools for a specific task. Motivated by the need of NER tools in our research work, we select several publicly available and well-established NER tools to validate their outputs against both Wikipedia gold standard corpus and a small set of manually annotated documents. All the evaluations show consistent results on the selected tools. Finally, we constructed a hybrid NER tool by combining the best performing tools for the domains of our interest. 
Word Representations such as word embeddings have been shown to signiÔ¨Åcantly improve (semi-)supervised NER for the English language. In this work we investigate whether word representations can also boost (semi-)supervised NER in Spanish. To do so, we use word representations as additional features in a linear chain Conditional Random Field (CRF) classiÔ¨Åer. Experimental results (82.44 Fscore on the CoNLL-2002 corpus) show that our approach is comparable to some state-of-the-art Deep Learning approaches for Spanish, in particular when using cross-lingual Word Representations. Keywords. NER for Spanish, Word Representations, Conditional Random Fields. 
This paper introduces a Japanese Named Entity (NE) corpus of various genres. We annotated 136 documents in the Balanced Corpus of Contemporary Written Japanese (BCCWJ) with the eight types of NE tags deÔ¨Åned by Information Retrieval and Extraction Exercise. The NE corpus consists of six types of genres of documents such as blogs, magazines, white papers, and so on, and the corpus contains 2,464 NE tags in total. The corpus can be reproduced with BCCWJ corpus and the tagging information obtained from https://sites.google.com/ site/projectnextnlpne/en/ . 
Our purely neural network-based system represents a paradigm shift away from the techniques based on phrase-based statistical machine translation we have used in the past. The approach exploits the agreement between a pair of target-bidirectional LSTMs, in order to generate balanced targets with both good suffixes and good prefixes. The evaluation results show that the method is able to match and even surpass the current state-of-the-art on most language pairs, but also exposes weaknesses on some tasks motivating further study. The Janus toolkit that was used to build the systems used in the evaluation is publicly available at https://github.com/lemaoliu/Agtarbidir. 
In this paper, we describe preliminary results from an ongoing experiment wherein we classify two large unstructured text corpora‚Äîa web corpus and a newspaper corpus‚Äîby topic domain (or subject area). Our primary goal is to develop a method that allows for the reliable annotation of large crawled web corpora with meta data required by many corpus linguists. We are especially interested in designing an annotation scheme whose categories are both intuitively interpretable by linguists and Ô¨Årmly rooted in the distribution of lexical material in the documents. Since we use data from a web corpus and a more traditional corpus, we also contribute to the important Ô¨Åeld of corpus comparison and corpus evaluation. Technically, we use (unsupervised) topic modeling to automatically induce topic distributions over gold standard corpora that were manually annotated for 13 coarse-grained topic domains. In a second step, we apply supervised machine learning to learn the manually annotated topic domains using the previously induced topics as features. We achieve around 70% accuracy in 10-fold cross validations. An analysis of the errors clearly indicates, however, that a revised classiÔ¨Åcation scheme and larger gold standard corpora will likely lead to a substantial increase in accuracy. 
Metadata extraction is known to be a problem in general-purpose Web corpora, and so is extensive crawling with little yield. The contributions of this paper are threefold: a method to Ô¨Ånd and download large numbers of WordPress pages; a targeted extraction of content featuring much needed metadata; and an analysis of the documents in the corpus with insights of actual blog uses. The study focuses on a publishing software (WordPress), which allows for reliable extraction of structural elements such as metadata, posts, and comments. The download of about 9 million documents in the course of two experiments leads after processing to 2.7 billion tokens with usable metadata. This comparatively high yield is a step towards more efÔ¨Åciency with respect to machine power and ‚ÄúHi-Fi‚Äù web corpora. The resulting corpus complies with formal requirements on metadata-enhanced corpora and on weblogs considered as a series of dated entries. However, existing typologies on Web texts have to be revised in the light of this hybrid genre. 
This paper describes the construction of three corpora, intended for use in social science research, comprising English-language, Frenchlanguage and Norwegian-language blogs related to the topic of climate change. The approach, techniques and lessons learnt should be applicable for creating other topically-focused blog corpora. 
Researchers of language variation and change often need to go to great lengths to Ô¨Ånd sufÔ¨Åcient data, particularly when they shall be used for a sound statistical analysis of the phenomenon in question. The recent analogical change in the formation of the imperative singular of German strong verbs with vowel gradation is a case in point, as it could not have been studied without the compilation of a webbased corpus. On the one hand, the investigation was faced with a number of challenges during the compilation of the corpus, the search for relevant hits and their annotation for a number of variables. On the other hand, results which would otherwise not have been obtained balance out this increased amount of manual labour. The present paper elaborates on some of these challenges and provides suggestions how they might be avoided in similar investigations in future. It concludes by presenting invaluable insights which would not have been gained without the present corpus study. 
This paper employs both a web-ascorpus and a Twitter-as-corpus approach to present a longitudinal case study of the establishment of three recently coined, synonymous neologisms: rapefugee, rapeugee and rapugee. We describe the retrieval and processing of the web and Twitter data and discuss the dynamics of the competition between the three forms within and across both datasets based on quantitative summaries of the results. The results show that various languageexternal events boost the usage of the terms both on the web and on Twitter, with the latter typically ahead of the former by some days. Beside absolute frequencies, we distinguish between several special usages of the target words and their effects on the establishment process. For the web corpus, we examine target words appearing in the title of websites and metalinguistic usages; for the Twitter corpus, we examine hashtag uses and retweets. We Ô¨Ånd that the use of hashtags and retweets signiÔ¨Åcantly affects the spread of the neologisms both on Twitter and on the web. 
We describe a system to collect web data for Low Resource Languages, to augment language model training data for Automatic Speech Recognition (ASR) and keyword search by reducing the Out-ofVocabulary (OOV) rates ‚Äì words in the test set that did not appear in the training set for ASR. We test this system on seven Low Resource Languages from the IARPA Babel Program: Paraguayan Guarani, Igbo, Amharic, Halh Mongolian, Javanese, Pashto, and Dholuo. The success of our system compared with other web collection systems is due to the targeted collection sources (blogs, twitter, forums) and the inclusion of a separate language identiÔ¨Åcation component in its pipeline, which Ô¨Ålters the data initially collected before Ô¨Ånally saving it. Our results show a major reduction of OOV rates relative to those calculated from training corpora alone and major reductions in OOV rates calculated in terms of keywords in the training development set. We also describe differences among genres in this reduction, which vary by language but show a pronounced inÔ¨Çuence for augmentation from Twitter data for most languages. 
Emojis are a quickly spreading and rather unknown communication phenomenon which occasionally receives attention in the mainstream press, but lacks the scientiÔ¨Åc exploration it deserves. This paper is a Ô¨Årst attempt at investigating the global distribution of emojis. We perform our analysis of the spatial distribution of emojis on a dataset of ‚àº17 million (and growing) geo-encoded tweets containing emojis by running a cluster analysis over countries represented as emoji distributions and performing correlation analysis of emoji distributions and World Development Indicators. We show that emoji usage tends to draw quite a realistic picture of the living conditions in various parts of our world. 
In this paper we report our analysis of the similarities between webpages that are crawled from European academic websites, and comparison of their distribution in terms of the English language variety (native English vs English as a lingua franca) and their language family (based on the country‚Äôs ofÔ¨Åcial language). After building a corpus of university webpages, we selected a set of relevant descriptors that can represent their text types using the framework of the Functional Text Dimensions. Manual annotation of a random sample of academic pages provides the basis for classifying the remaining texts on each dimension. Reliable thresholds are then determined in order to evaluate precision and assess the distribution of text types by each dimension, with the ultimate goal of analysing language features over English varieties and language families. 
In this paper, I present a specialized opensource crawler that can be used to obtain bias-reduced samples from the web. First, I brieÔ¨Çy discuss the relevance of bias-reduced web corpus sampling for corpus linguistics. Then, I summarize theoretical results that show how commonly used crawling methods obtain highly biased samples from the web. The theoretical part of the paper is followed by a description my feature-complete and stable ClaraX crawler which performs so-called Random Walks, a form of crawling that allows for bias-reduced sampling if combined with methods of post-crawl rejection sampling. Finally, results from two large crawling experiments in the German web are reported. I show that bias reduction is feasible if certain technical and practical hurdles are overcome. 
The quality of word representations is frequently assessed using correlation with human judgements of word similarity. Here, we question whether such intrinsic evaluation can predict the merits of the representations for downstream tasks. We study the correlation between results on ten word similarity benchmarks and tagger performance on three standard sequence labeling tasks using a variety of word vectors induced from an unannotated corpus of 3.8 billion words, and demonstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish speciÔ¨Åc similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study. 
This paper aims to re-think the role of the word similarity task in distributional semantics research. We argue while it is a valuable tool, it should be used with care because it provides only an approximate measure of the quality of a distributional model. Word similarity evaluations assume there exists a single notion of similarity that is independent of a particular application. Further, the small size and low inter-annotator agreement of existing data sets makes it challenging to Ô¨Ånd signiÔ¨Åcant differences between models. 
The offset method for solving word analogies has become a standard evaluation tool for vector-space semantic models: it is considered desirable for a space to represent semantic relations as consistent vector offsets. We show that the method‚Äôs reliance on cosine similarity conÔ¨Çates offset consistency with largely irrelevant neighborhood structure, and propose simple baselines that should be used to improve the utility of the method in vector space evaluation. 
Word embeddings are increasingly used in natural language understanding tasks requiring sophisticated semantic information. However, the quality of new embedding methods is usually evaluated based on simple word similarity benchmarks. We propose evaluating word embeddings in vivo by evaluating them on a suite of popular downstream tasks. To ensure the ease of use of the evaluation, we take care to Ô¨Ånd a good point in the tradeoff space between (1) creating a thorough evaluation ‚Äì i.e., we evaluate on a diverse set of tasks; and (2) ensuring an easy and fast evaluation ‚Äì by using simple models with few tuned hyperparameters. This allows us to release this evaluation as a standardized script and online evaluation, available at http://veceval.com/. 
Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of ‚Äúsemantic similarity‚Äù is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods. 
This paper presents an analysis of existing methods for the intrinsic evaluation of word embeddings. We show that the main methodological premise of such evaluations is ‚Äúinterpretability‚Äù of word embeddings: a ‚Äúgood‚Äù embedding produces results that make sense in terms of traditional linguistic categories. This approach is not only of limited practical use, but also fails to do justice to the strengths of distributional meaning representations. We argue for a shift from abstract ratings of word embedding ‚Äúquality‚Äù to exploration of their strengths and weaknesses. 
We present a new framework for an intrinsic evaluation of word vector representations based on the outlier detection task. This task is intended to test the capability of vector space models to create semantic clusters in the space. We carried out a pilot study building a gold standard dataset and the results revealed two important features: human performance on the task is extremely high compared to the standard word similarity task, and stateof-the-art word embedding models, whose current shortcomings were highlighted as part of the evaluation, still have considerable room for improvement. 
If lexical similarity is not enough to reliably assess how word vectors would perform on various speciÔ¨Åc tasks, we need other ways of evaluating semantic representations. We propose a new task, which consists in extracting semantic differences using distributional models: given two words, what is the difference between their meanings? We present two proof of concept datasets for this task and outline how it may be performed. 
In this proposal track paper, we have presented a crowdsourcing-based word embedding evaluation technique that will be more reliable and linguistically justiÔ¨Åed. The method is designed for intrinsic evaluation and extends the approach proposed in (Schnabel et al., 2015). Our improved evaluation technique captures word relatedness based on the word context. 
Recently, researchers in speech recognition have started to reconsider using whole words as the basic modeling unit, instead of phonetic units. These systems rely on a function that embeds an arbitrary or Ô¨Åxed dimensional speech segments to a vector in a Ô¨Åxed-dimensional space, named acoustic word embedding. Thus, speech segments of words that sound similarly will be projected in a close area in a continuous space. This paper focuses on the evaluation of acoustic word embeddings. We propose two approaches to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations in order to evaluate whether they capture discriminative phonetic information. Since French language is targeted in experiments, a particular focus is made on homophone words. 
Most evaluations for vector space models are semantically motivated, e.g. by measuring how well they capture word similarity. If one is interested in syntax-related downstream applications such as dependency parsing, a syntactically motivated evaluation seems preferable. As we show, the choice of embeddings has a noticeable impact on parser performance. Since evaluating embeddings directly in a parser is costly, we analyze the correlation between the full parsing task and a simple linear classiÔ¨Åcation task as a potential proxy. 
Vector space models of word representation are often evaluated using human similarity ratings. Those ratings are elicited in explicit tasks and have well-known subjective biases. As an alternative, we propose evaluating vector spaces using implicit cognitive measures. We focus in particular on semantic priming, exploring the strengths and limitations of existing datasets, and propose ways in which those datasets can be improved. 
We propose a method for evaluating embeddings against dictionaries with tens or hundreds of thousands of entries, covering the entire gamut of the vocabulary. 
Multi-sense word embeddings (MSEs) model different meanings of word forms with different vectors. We propose two new methods for evaluating MSEs, one based on monolingual dictionaries, and the other exploiting the principle that words may be ambiguous as far as the postulated senses translate to different words in some other language. 
While there has been a growing body of work on word embeddings, and recent directions better reÔ¨Çect sense-level representations, evaluation remains a challenge. We propose a method of query inventory generation for embedding evaluation that recasts the principle of subsumption preservation, a desirable property of semantic graph-based similarity measures, as a comparative similarity measure as applied to existing lexical resources. We aim that this method is immediately applied to populate query inventories and perform evaluation with the ordered triple-based approach set forth, and inspires future reÔ¨Ånements to existing notions of evaluating sense-directed embeddings. 
Existing corpora for intrinsic evaluation are not targeted towards tasks in informal domains such as Twitter or news comment forums. We want to test whether a representation of informal words fulÔ¨Ålls the promise of eliding explicit text normalization as a preprocessing step. One possible evaluation metric for such domains is the proximity of spelling variants. We propose how such a metric might be computed and how a spelling variant dataset can be collected using UrbanDictionary. 
In this paper, we discuss the human thematic Ô¨Åt judgement correlation task in the context of real-valued vector space word representations. Thematic Ô¨Åt is the extent to which an argument fulÔ¨Åls the selectional preference of a verb given a role: for example, how well ‚Äúcake‚Äù fulÔ¨Åls the patient role of ‚Äúcut‚Äù. In recent work, systems have been evaluated on this task by Ô¨Ånding the correlations of their output judgements with human-collected judgement data. This task is a representationindependent way of evaluating models that can be applied whenever a system score can be generated, and it is applicable wherever predicate-argument relations are signiÔ¨Åcant to performance in end-user tasks. SigniÔ¨Åcant progress has been made on this cognitive modeling task, leaving considerable space for future, more comprehensive types of evaluation. 
We suggest a new method for creating and using gold-standard datasets for word similarity evaluation. Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by deÔ¨Åning a performance measure which takes the reliability of each annotation decision in the dataset into account. 
We introduce QVEC-CCA‚Äîan intrinsic evaluation metric for word vector representations based on correlations of learned vectors with features extracted from linguistic resources. We show that QVECCCA scores are an effective proxy for a range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 
The way humans deÔ¨Åne words is a powerful way of representing them. In this work, we propose to measure word similarity by comparing the overlap in their deÔ¨Ånition. This highlights linguistic phenomena that are complementary to the information extracted from standard context-based representation learning techniques. To acquire a large amount of word deÔ¨Ånitions in a cost-efÔ¨Åcient manner, we designed a simple interactive word game, Word Sheriff. As a byproduct of game play, it generates short word sequences that can be used to uniquely identify words. These sequences can not only be used to evaluate the quality of word representations, but it could ultimately give an alternative way of learning them, as it overcomes some of the limitations of the distributional hypothesis. Moreover, inspecting player behaviour reveals interesting aspects about human strategies and knowledge acquisition beyond those of simple word association games, due to the conversational nature of the game. Lastly, we outline a vision of a communicative evaluation setting, where systems are evaluated based on how well a given representation allows a system to communicate with human and computer players. 
Similarity is a core notion that is used in psychology and two branches of linguistics: theoretical and computational. The similarity datasets that come from the two Ô¨Åelds differ in design: psychological datasets are focused around a certain topic such as fruit names, while linguistic datasets contain words from various categories. The later makes humans assign low similarity scores to the words that have nothing in common and to the words that have contrast in meaning, making similarity scores ambiguous. In this work we discuss the similarity collection procedure for a multi-category dataset that avoids score ambiguity and suggest changes to the evaluation procedure to reÔ¨Çect the insights of psychological literature for word, phrase and sentence similarity. We suggest to ask humans to provide a list of commonalities and differences instead of numerical similarity scores and employ the structure of human judgements beyond pairwise similarity for model evaluation. We believe that the proposed approach will give rise to datasets that test meaning representation models more thoroughly with respect to the human treatment of similarity. 
We propose a diagnostic method for probing speciÔ¨Åc information captured in vector representations of sentence meaning, via simple classiÔ¨Åcation tasks with strategically constructed sentence sets. We identify some key types of semantic information that we might expect to be captured in sentence composition, and illustrate example classiÔ¨Åcation tasks for targeting this information. 
Measuring the semantic relatedness of phrase pairs is important for evaluating compositional distributional semantic representations. Many existing phrase relatedness datasets are limited to either lexical or syntactic alternations between phrase pairs, which limits the power of the evaluation. We propose SLEDDED (Syntactically and LExically Divergent Dataset of Event Descriptions), a dataset of event descriptions in which related phrase pairs are designed to exhibit minimal lexical and syntactic overlap; for example, a decisive victory ‚Äî won the match clearly. We also propose a subset of the data aimed at distinguishing event descriptions from related but dissimilar phrases; for example, vowing to Ô¨Åght to the death ‚Äî a new training regime for soldiers, which serves as a proxy for the tasks of narrative generation, event sequencing, and summarization. We describe a method for extracting candidate pairs from a corpus based on occurrences of event nouns (e.g. war) and a two-step annotation process consisting of expert annotation followed by crowdsourcing. We present examples from a pilot of the expert annotation step. 
Word embedding vectors are used as input for a variety of tasks. Choosing the right model and features for producing such vectors is not a trivial task and different embedding methods can greatly affect results. In this paper we repurpose the "Pyramid Method" annotations used for evaluating automatic summarization to create a benchmark for comparing embedding models when identifying paraphrases of text snippets containing a single clause. We present a method of converting pyramid annotation files into two distinct sentence embedding tests. We show that our method can produce a good amount of testing data, analyze the quality of the testing data, perform test on several leading embedding methods, and finally explain the downstream usages of our task and its significance. 
Encoding unranked trees to binary trees, henceforth called binarization, is an important method to deal with unranked trees. For each of three binarizations we show that weighted (ranked) tree automata together with the binarization are equivalent to weighted unranked tree automata; even in the probabilistic case. This allows to easily adapt training methods for weighted (ranked) tree automata to weighted unranked tree automata. 
We present a general importance sampling technique for approximating expected values based on samples from probabilistic Ô¨Ånite tree automata. The algorithm uses the samples it produces to adapt rule probabilities for the automaton in order to improve sample quality. 
We provide a theoretical argument that a common form of projective transitionbased dependency parsing is less powerful than constituent parsing using latent variables. The argument is a proof that, under reasonable assumptions, a transition-based dependency parser can be converted to a latent-variable context-free grammar producing equivalent structures. 
We present methods for partitioning a weighted Ô¨Ånite-state transducer (WFST) representation of an n-gram language model into multiple blocks or shards, each of which is a stand-alone WFST n-gram model in its own right, allowing processing with existing algorithms. After independent estimation, including normalization, smoothing and pruning on each shard, the shards can be reassembled into a single WFST that is identical to the model that would have resulted from estimation without sharding. We then present an approach that uses data partitions in conjunction with WFST sharding to estimate models on orders-of-magnitude more data than would have otherwise been feasible with a single process. We present some numbers on shard characteristics when large models are trained from a very large data set. Functionality to support distributed n-gram modeling has been added to the open-source OpenGrm library. 
In this paper, we present a method to convert morphological inÔ¨Çection tables into unweighted and weighted Ô¨Ånite transducers that perform parsing and generation. These transducers model the inÔ¨Çectional behavior of morphological paradigms induced from examples and can map inÔ¨Çected forms of previously unseen word forms into their lemmas and give morphosyntactic descriptions of them. The system is evaluated on several languages with data collected from the Wiktionary. 
This paper presents two systems for spelling correction formulated as a sequence labeling task. One of the systems is an unstructured classiÔ¨Åer and the other one is structured. Both systems are implemented using weighted Ô¨Ånite-state methods. The structured system delivers stateof-the-art results on the task of tweet normalization when compared with the recent AliSeTra system introduced by Eger et al. (2016) even though the system presented in the paper is simpler than AliSeTra because it does not include a model for input segmentation. In addition to experiments on tweet normalization, we present experiments on OCR post-processing using an Early Modern Finnish corpus of OCR processed newspaper text. 
We develop the concept of weighted aligned hypergraph bimorphism where the weights may, in particular, represent probabilities. Such a bimorphism consists of an R‚â•0-weighted regular tree grammar, two hypergraph algebras that interpret the generated trees, and a family of alignments between the two interpretations. Semantically, this yields a set of bihypergraphs each consisting of two hypergraphs and an explicit alignment between them; e.g., discontinuous phrase structures and nonprojective dependency structures are bihypergraphs. We present an EM-training algorithm which takes a corpus of bihypergraphs and an aligned hypergraph bimorphism as input and generates a sequence of weight assignments which converges to a local maximum or saddle point of the likelihood function of the corpus. 
Compositional matrix-space models of language were recently proposed for the task of meaning representation of complex text structures in natural language processing. These models have been shown to be a theoretically elegant way to model compositionality in natural language. However, in practical cases, appropriate methods are required to learn such models by automatically acquiring the necessary token-to-matrix assignments. In this paper, we introduce graded matrix grammars of natural language, a variant of the matrix grammars proposed by Rudolph and Giesbrecht (2010), and show a close correspondence between this matrix-space model and weighted Ô¨Ånite automata. We conclude that the problem of learning compositional matrix-space models can be mapped to the problem of learning weighted Ô¨Ånite automata over the real numbers. 
 2 Existing WFST libraries  We present Pynini, an open-source library for the compilation of weighted finitestate transducers (WFSTs) and pushdown transducers (PDTs) from strings, contextdependent rewrite rules, and recursive transition networks. Pynini uses the OpenFst library for encoding, modifying, and applying WFSTs and PDTs. We describe the design of this library and the algorithms and interfaces used for compilation, optimization, and application, and provide two illustrations of the library in use. 
This paper describes LIMSI‚Äôs submissions to the shared WMT‚Äô16 task ‚ÄúTranslation of News‚Äù. We report results for Romanian-English in both directions, for English to Russian, as well as preliminary experiments on reordering to translate from English into German. Our submissions use mainly NCODE and MOSES along with continuous space models in a post-processing step. The main novelties of this year‚Äôs participation are the following: for the translation into Russian and Romanian, we have attempted to extend the output of the decoder with morphological variations and to use a CRF model to rescore this new search space; as for the translation into German, we have been experimenting with source-side pre-ordering based on a dependency structure allowing permutations in order to reproduce the target word order. 
We describe the TU¬® BÀôITAK TurkishEnglish machine translation systems submissions in both directions for the WMT 2016: News Translation Task. We experiment with phrase-based and hierarchical phrase-based systems for both directions using word-level and morpheme-level representations for the Turkish side. Finally we perform system combination which results in 0.5 BLEU increase for Turkishto-English and 0.3 BLEU increase for English-to-Turkish. 
We build parallel feature decay algorithms (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the translation task at the Ô¨Årst conference on statistical machine translation (Bojar et al., 2016a) (WMT16). ParFDA obtains results close to the top constrained phrase-based SMT with an average of 2.52 BLEU points difference using signiÔ¨Åcantly less computation for building SMT systems than the computation that would be spent using all available corpora. We obtain BLEU bounds based on target coverage and show that ParFDA results can be improved by 12.6 BLEU points on average. Similar bounds show that top constrained SMT results at WMT16 can be improved by 8 BLEU points on average while German to English and Romanian to English translations results are already close to the bounds. 
This paper provides an overview of the submissions the University of ShefÔ¨Åeld for the English-Romanian Translation Task of the ACL 2016 First Conference on Machine Translation (WMT16). The submitted translations were produced with a phrase-based system trained using the Moses toolkit, in two variants: (i) n-best rescoring using additional features from Quality Estimation (primary submission), and (ii) a novel weighted ranking optimisation approach (secondary submission). 
Neural Machine Translation (NMT) systems, introduced only in 2013, have achieved state of the art results in many MT tasks. MetaMind‚Äôs submissions to WMT ‚Äô16 seek to push the state of the art in one such task, English‚ÜíGerman newsdomain translation. We integrate promising recent developments in NMT, including subword splitting and back-translation for monolingual data augmentation, and introduce the Y-LSTM, a novel neural translation architecture. 
We describe the neural machine translation system of New York University (NYU) and University of Montreal (MILA) for the translation tasks of WMT‚Äô16. The main goal of NYU-MILA submission to WMT‚Äô16 is to evaluate a new character-level decoding approach in neural machine translation on various language pairs. The proposed neural machine translation system is an attention-based encoder‚Äìdecoder with a subword-level encoder and a character-level decoder. The decoder of the neural machine translation system does not require explicit segmentation, when characters are used as tokens. The character-level decoding approach provides beneÔ¨Åts especially when translating a source language into other morphologically rich languages. 
This paper describes the submission of Johns Hopkins University for the shared translation task of ACL 2016 First Conference on Machine Translation (WMT 2016). We set up phrase-based, hierarchical phrase-based and syntax-based systems for all 12 language pairs of this year‚Äôs evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 
We describe the English-Turkish and Turkish-English translation systems submitted by Yandex School of Data Analysis team to WMT16 news translation task. We successfully applied hand-crafted morphological (de-)segmentation of Turkish, syntax-based pre-ordering of English in English-Turkish and post-ordering of English in Turkish-English. We perform desegmentation using SMT and propose a simple yet efÔ¨Åcient modiÔ¨Åcation of postordering. We also show that Turkish morphology and word order can be handled in a fully-automatic manner with only a small loss of BLEU. 
This paper describes the AFRL-MITLL statistical machine translation systems and the improvements that were developed during the WMT16 evaluation campaign. New techniques applied this year include Neural Machine Translation, a unique selection process for language modelling data, additional out-of-vocabulary transliteration techniques, and morphology generation. 
We describe the statistical machine translation system developed at the National Research Council of Canada (NRC) for the Russian-English news translation task of the First Conference on Machine Translation (WMT 2016). Our submission is a phrase-based SMT system that tackles the morphological complexity of Russian through comprehensive use of lemmatization. The core of our lemmatization strategy is to use different views of Russian for different SMT components: word alignment and bilingual neural network language models use lemmas, while sparse features and reordering models use fully inÔ¨Çected forms. Some components, such as the phrase table, use both views of the source. Russian words that remain out-ofvocabulary (OOV) after lemmatization are transliterated into English using a statistical model trained on examples mined from the parallel training corpus. The NRC Russian-English MT system achieved the highest uncased BLEU and the lowest TER scores among the eight participants in WMT 2016. 
In this paper, we present our new experimental system of merging dependency representations of two parallel sentences into one dependency tree. All the inner nodes in dependency tree represent source-target pairs of words, the extra words are in form of leaf nodes. We use Universal Dependencies annotation style, in which the function words, whose usage often differs between languages, are annotated as leaves. The parallel treebank is parsed in minimally supervised way. Unaligned words are there automatically pushed to leaves. We present a simple translation system trained on such merged trees and evaluate it in WMT 2016 English-to-Czech and Czechto-English translation task. Even though the model is so far very simple and no language model and word-reordering model were used, the Czech-to-English variant reached similar BLEU score as another established tree-based system. 
This paper provides an overview of the PROMT submissions for the WMT16 Shared Translation Tasks. We participated in seven language pairs with three different system conÔ¨Ågurations (rule-based, statistical and hybrid). We describe the architecture of the three conÔ¨Ågurations. We show that fast and accurate customization of the rule-based system can increase the BLEU scores signiÔ¨Åcantly. 
This paper describes the joint submission of the QT21 and HimL projects for the English‚ÜíRomanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of ShefÔ¨Åeld, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTH‚Äôs system combination approach. The Ô¨Ånal submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016. 
This paper describes the statistical machine translation system developed at RWTH Aachen University for the English‚ÜíRomanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). We combined three different state-ofthe-art systems in a system combination: A phrase-based system, a hierarchical phrase-based system and an attentionbased neural machine translation system. The phrase-based and the hierarchical phrase-based systems make use of a language model trained on all available data, a language model trained on the bilingual data and a word class language model. In addition, we utilized a recurrent neural network language model and a bidirectional recurrent neural network translation model for reranking the output of both systems. The attention-based neural machine translation system was trained using all bilingual data together with the backtranslated data from the News Crawl 2015 corpora. 
This paper presents the systems submitted by the Abu-MaTran project to the Englishto-Finnish language pair at the WMT 2016 news translation task. We applied morphological segmentation and deep learning in order to address (i) the data scarcity problem caused by the lack of in-domain parallel data in the constrained task and (ii) the complex morphology of Finnish. We submitted a neural machine translation system, a statistical machine translation system reranked with a neural language model and the combination of their outputs tuned on character sequences. The combination and the neural system were ranked Ô¨Årst and second respectively according to automatic evaluation metrics and tied for the Ô¨Årst place in the human evaluation. 
We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English‚ÜîCzech, English‚ÜîGerman, English‚ÜîRomanian and English‚ÜîRussian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a Ô¨Åxed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3‚Äì11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.12 
This paper presents the University of Cambridge submission to WMT16. Motivated by the complementary nature of syntactical machine translation and neural machine translation (NMT), we exploit the synergies of Hiero and NMT in different combination schemes. Starting out with a simple neural lattice rescoring approach, we show that the Hiero lattices are often too narrow for NMT ensembles. Therefore, instead of a hard restriction of the NMT search space to the lattice, we propose to loosely couple NMT and Hiero by composition with a modiÔ¨Åed version of the edit distance transducer. The loose combination outperforms lattice rescoring, especially when using multiple NMT systems in an ensemble. 
This paper describes the phrase-based systems jointly submitted by CUNI and LMU to English-Czech and English-Romanian News translation tasks of WMT16. In contrast to previous years, we strictly limited our training data to the constraint datasets, to allow for a reliable comparison with other research systems. We experiment with using several additional models in our system, including a feature-rich discriminative model of phrasal translation. 
This paper summarises the contributions of the teams at the University of Helsinki, Uppsala University and the University of Turku to the news translation tasks for translating from and to Finnish. Our models address the problem of treating morphology and data coverage in various ways. We introduce a new efÔ¨Åcient tool for word alignment and discuss factorisations, gappy language models and reinÔ¨Çection techniques for generating proper Finnish output. The results demonstrate once again that training data is the most effective way to increase translation performance. 
This paper describes the University of Edinburgh‚Äôs phrase-based and syntax-based submissions to the shared translation tasks of the ACL 2016 First Conference on Machine Translation (WMT16). We submitted Ô¨Åve phrase-based and Ô¨Åve syntaxbased systems for the news task, plus one phrase-based system for the biomedical task. 
In this paper, we attempt to improve Statistical Machine Translation (SMT) systems between Czech and English. To accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our SMT systems. Innovative tools and data adaptation techniques were employed. Only the official parallel text corpora and monolingual models for the WMT 2016 evaluation campaign were used to train language models, and to develop, tune, and test the system. We explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. To evaluate the effects of different preparations on translation results, we conducted experiments and used the BLEU, NIST and TER metrics. Our results indicate that our approach produced a positive impact on SMT quality. 
We are presenting a hybrid MT approach in the WMT2016 Shared Translation Task for the IT-Domain. Our work consists of several translation components based on rule-based and statistical approaches that feed into an informed selection mechanism. Additions to last year‚Äôs submission include a WSD component, a syntactically-enhanced component and several improvements to the rule-based component, relevant to the particular domain. We also present detailed human evaluation on the output of all translation components, focusing on particular systematic errors. 
This paper describes Scorpio, the ILLCUvA Adaptation System submitted to the IT-DOMAIN translation task at WMT 2016, which participated with the language pair of English-Dutch. This system consolidates the ideas in our previous work on latent variable models for adaptation, and demonstrates their effectiveness in a competitive setting. 
This paper presents an overview of the system submitted by the University of Hamburg to the IT domain shared translation task as part of the ACL 2016 First Conference of Machine Translation (WMT 2016). We have chosen data selection as a domain adaptation method. The Ô¨Åltering of the general domain data makes use of paragraph vectors as a novel approach for scoring the sentences. Experiments were conducted for English-German under the constrained condition. 
This paper presents the description of 12 systems submitted to the WMT16 IT-task, covering six different languages, namely Basque, Bulgarian, Dutch, Czech, Portuguese and Spanish. All these systems were developed under the scope of the QTLeap project, presenting a common strategy. For each language two different systems were submitted, namely a phrasebased MT system built using Moses, and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was implemented using TectoMT. For 4 of the 6 languages, the TectoMT-based system performs better than the Moses-based one. 
We describe our submission to the ITdomain translation task of WMT 2016. We perform domain adaptation with dictionary data on already trained MT systems with no further retraining. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one. 
The article describes LIMSI‚Äôs submission to the Ô¨Årst WMT‚Äô16 shared biomedical translation task, focusing on the sole English-French translation direction. Our main submission is the output of a MOSES-based statistical machine translation (SMT) system, rescored with Structured OUtput Layer (SOUL) neural network models. We also present an attempt to circumvent syntactic complexity: our proposal combines the outputs of PBSMT systems trained either to translate entire source sentences or speciÔ¨Åc syntactic constructs extracted from those sentences. The approach is implemented using Confusion Network (CN) decoding. The quality of the combined output is comparable to the quality of our main system. 
In this paper we present the system developed at the IXA NLP Group of the University of the Basque Country for the Biomedical Translation Task in the First Conference on Machine Translation (WMT16). For the adaptation of a statistical machine translation system to the biomedical domain, we developed three approaches based on a baseline system for English-Spanish and Spanish-English language pairs. The lack of terminology and the variation of the prominent sense of the words are the issues we have addressed on these approaches. The best of our systems reached the average of all the systems submitted in the challenge in most of the evaluation sets. 
The vast majority of Machine Translation (MT) evaluation approaches are based on the idea that the closer the MT output is to a human reference translation, the higher its quality. While translation quality has two important aspects, adequacy and Ô¨Çuency, the existing referencebased metrics are largely focused on the former. In this work we combine our metric UPF-Cobalt, originally presented at the WMT15 Metrics Task, with a number of features intended to capture translation Ô¨Çuency. Experiments show that the integration of Ô¨Çuency-oriented features significantly improves the results, rivalling the best-performing evaluation metrics on the WMT15 data. 
We present DTED, a submission to the WMT 2016 Metrics Task using structural information generated by dependency parsing and evaluated using tree edit distances. In this paper we apply this system to translations produced during WMT 2015, and compare our scores with human rankings from that year. We Ô¨Ånd moderate correlations, despite the human judgements being based on all aspects of the sentences while our metric is based only on word order. 
Character n-gram F-score (CHRF) is shown to correlate very well with human rankings of different machine translation outputs, especially for morphologically rich target languages. However, only two versions have been explored so far, namely CHRF1 (standard F-score, Œ≤ = 1) and CHRF3 (Œ≤ = 3), both with uniform n-gram weights. In this work, we investigated CHRF in more details, namely Œ≤ parameters in range from 1/6 to 6, and we found out that CHRF2 is the most promising version. Then we investigated different n-gram weights for CHRF2 and found out that the uniform weights are the best option. Apart from this, CHRF scores were systematically compared with WORDF scores, and a preliminary experiment carried out on small amount of data with direct human scores indicates that the main advantage of CHRF is that it does not penalise too hard acceptable variations in high quality translations. 
Recently, the capability of character-level evaluation measures for machine translation output has been conÔ¨Årmed by several metrics. This work proposes translation edit rate on character level (CharacTER), which calculates the character level edit distance while performing the shift edit on word level. The novel metric shows high system-level correlation with human rankings, especially for morphologically rich languages. It outperforms the strong CHRF by up to 7% correlation on different metric tasks. In addition, we apply the hypothesis sentence length for normalizing the edit distance in CharacTER, which also provides signiÔ¨Åcant improvements compared to using the reference sentence length. 
Paraphrase can help match synonyms or match phrases with the same or similar meaning, thus it plays an important role in automatic evaluation of machine translation. The traditional approaches extract paraphrase in general domain from bilingual corpus. Because the WMT16 metrics task consists of three subtasks, namely news domain, medical domain, and IT domain, we propose to extract domainspecific paraphrase tables from monolingual corpus to replace the general paraphrase table. We utilize the M-L approach to filter the large scale general monolingual corpus into a domain-specific sub-corpus, and exploit Markov Network model to extract paraphrase tables from the sub-corpus. The experimental results on WMT15 Metrics task show that METEOR metric using the domain-specific paraphrase tables outperforms that using the paraphrase table in general domain extracted from the bilingual corpus. 
This paper describes our submission to the Tuning Task of WMT16. We replace the grid search implemented as part of standard minimum-error rate training (MERT) in the Moses toolkit with a search based on particle swarm optimization (PSO). An older variant of PSO has been previously successfully applied and we now test it in optimizing the Tuning Task model for English-to-Czech translation. We also adapt the method in some aspects to allow for even easier parallelization of the search. 
In this paper we describe our system we designed and implemented for the crosslingual pronoun prediction task as a part of WMT 2016. The majority of the paper will be dedicated to the system whose outputs we submitted wherein we describe the simpliÔ¨Åed mathematical model, the details of the components and the working by means of an architecture diagram which also serves as a Ô¨Çowchart. We then discuss the results of the ofÔ¨Åcial scores and our observations on the same. 
The cross-lingual pronoun prediction task at WMT 2016 requires to restore the missing target pronouns from source text and target lemmatized and POS-tagged translations. We study the beneÔ¨Åts for this task of a speciÔ¨Åc Pronoun Language Model (PLM), which captures the likelihood of a pronoun given the gender and number of the nouns or pronouns preceding it, on the target-side only. Experimenting with the English-to-French subtask, we select the best candidate pronoun by applying the PLM and additional heuristics based on French grammar rules to the target-side texts provided in the subtask. Although the PLM helps to outperform a random baseline, it still scores far lower than system using both source and target texts. 
We explore a large number of features for cross-lingual pronoun prediction for translation between English and German/French. We Ô¨Ånd that features related to German/French are more informative than features related to English, regardless of the translation direction. Our most useful features are local context, dependency head features, and source pronouns. We also Ô¨Ånd that it is sometimes more successful to employ a 2-step procedure that Ô¨Årst makes a binary choice between pronouns and other, then classiÔ¨Åes pronouns. For the pronoun/other distinction POS ngrams were very useful. 
This paper presents baseline models using linear classiÔ¨Åers for the pronoun translation task at WMT 2016. We explore various local context features and include history features of potential antecedents extracted by means of a simple PoSmatching strategy. The results show the difÔ¨Åculties of the task in general but also represent valuable baselines to compare other more-informed systems with. Our experiments reveal that the predictions of English correspondences for given ambiguous pronouns in French and German is easier than the other way around. This seems to verify that predictions, which need to follow more complex agreement constraints, require more reliable information about the referential links of the tokens to be inserted. 
We present a doubly-attentive multimodal machine translation model. Our model learns to attend to source language and spatial-preserving CONV5,4 visual features as separate attention mechanisms in a neural translation model. In image description translation experiments (Task 1), we Ô¨Ånd an improvement of 2.3 Meteor points compared to initialising the hidden state of the decoder with only the FC7 features and 2.9 Meteor points compared to a text-only neural machine translation baseline, conÔ¨Årming the useful nature of attending to the CONV5,4 features. 
We present a novel neural machine translation (NMT) architecture associating visual and textual features for translation tasks with multiple modalities. Transformed global and regional visual features are concatenated with text to form attendable sequences which are dissipated over parallel long short-term memory (LSTM) threads to assist the encoder generating a representation for attention-based decoding. Experiments show that the proposed NMT outperform the text-only baseline. 
Bidirectional Recurrent Neural Networks (BiRNNs) have shown outstanding results on sequence-to-sequence learning tasks. This architecture becomes specially interesting for multimodal machine translation task, since BiRNNs can deal with images and text. On most translation systems the same word embedding is fed to both BiRNN units. In this paper, we present several experiments to enhance a baseline sequence-to-sequence system (Elliott et al., 2015), for example, by using double embeddings. These embeddings are trained on the forward and backward direction of the input sequence. Our system is trained, validated and tested on the Multi30K dataset (Elliott et al., 2016) in the context of the WMT 2016 Multimodal Translation Task. The obtained results show that the double-embedding approach performs signiÔ¨Åcantly better than the traditional single-embedding one. 
This work describes our submission to the WMT16 Bilingual Document Alignment task. We show that a very simple distance metric, namely Cosine distance of tf/idf weighted document vectors provides a quick and reliable way to align documents. We compare many possible variants for constructing the document vectors. We also introduce a greedy algorithm that runs quicker and performs better in practice than the optimal solution to bipartite graph matching. Our approach shows competitive performance and can be improved even further through combination with URL based pair matching. 
We apply cross-lingual Latent Semantic Indexing to the Bilingual Document Alignment Task at WMT16. Reduced-rank singular value decomposition of a bilingual term-document matrix derived from known English/French page pairs in the training data allows us to map monolingual documents into a joint semantic space. Two variants of cosine similarity between the vectors that place each document into the joint semantic space are combined with a measure of string similarity between corresponding URLs to produce 1:1 alignments of English/French web pages in a variety of domains. The system achieves a recall of ca. 88% if no in-domain data is used for building the latent semantic model, and 93% if such data is included. Analysing the system‚Äôs errors on the training data, we argue that evaluating aligner performance based on exact URL matches under-estimates their true performance and propose an alternative that is able to account for duplicates and near-duplicates in the underlying data. 
In this paper we describe a method for selecting pairs of parallel documents (documents that are a translation of each other) from a large collection of documents obtained from the web. Our approach is based on a coverage score that reÔ¨Çects the number of distinct bilingual phrase pairs found in each pair of documents, normalized by the total number of unique phrases found in them. Since parallel documents tend to share more bilingual phrase pairs than non-parallel documents, our alignment algorithm selects pairs of documents with the maximum coverage score from all possible pairings involving either one of the two documents. 
The WMT Bilingual Document Alignment Task requires systems to assign source pages to their ‚Äútranslations‚Äù, in a big space of possible pairs. We present four methods: The Ô¨Årst one uses the term position similarity between candidate document pairs. The second method requires automatically translated versions of the target text, and matches them with the candidates. The third and fourth methods try to overcome some of the challenges presented by the nature of the corpus, by considering the string similarity of source URL and candidate URL, and combining the Ô¨Årst two approaches. 
In this paper we present our approach to the Bilingual Document Alignment Task (WMT16), where the main goal was to reach the best recall on extracting aligned pages within the provided data. Our approach consists of tree main parts: data preprocessing, keyword extraction and text pairs scoring based on keyword matching. For text preprocessing we use the TreeTagger pipeline that contains the Unitok tool (Michelfeit et al., 2014) for tokenization and the TreeTagger morphological analyzer (Schmid, 1994). After keywords extraction from the texts according TF-IDF scoring our system searches for comparable English-French pairs. Using a statistical dictionary created from a large English-French parallel corpus, the system is able to Ô¨Ånd comaparable documents. At the end this procedure is combined with the baseline algorithm and best one-to-one pairing is selected. The result reaches 91.6% recall on provided training data. After a deep error analysis (see section 5) the recall reached 97.4%. 
This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output. To overcome the problem of too little training data, we generate large amounts of artiÔ¨Åcial data. Our submission improves over the uncorrected baseline on the unseen test set by -3.2% TER and +5.5% BLEU and outperforms any other system submitted to the shared-task by a large margin. 
This paper presents an automatic postediting (APE) method to improve the translation quality produced by an English‚ÄìGerman (EN‚ÄìDE) statistical machine translation (SMT) system. Our system is based on Operation Sequential Model (OSM) combined with phrasedbased statistical MT (PB-SMT) system. The system is trained on monolingual settings between MT outputs (T LMT ) produced by a black-box MT system and their corresponding post-edited version (T LP E). Our system achieves considerable improvement over T LMT on a held-out development set. The reported system achieves 64.10 BLEU (1.99 absolute points and 3.2% relative improvement in BLEU over raw MT output) and 24.14 TER and a TER score of 24.14 (0.66 absolute points and 0.25% relative improvement in TER over raw MT output) in the ofÔ¨Åcial test set. 
Referential translation machines (RTMs) pioneer a language independent approach for predicting translation performance and to all similarity tasks with top performance in both bilingual and monolingual settings and remove the need to access any task or domain speciÔ¨Åc information or resource. RTMs achieve to become 1st in documentlevel, 4th system at sentence-level according to mean absolute error, and 4th in phrase-level prediction of translation quality in quality estimation task. 
We introduce SimpleNets: a resource-light solution to the sentence-level Quality Estimation task of WMT16 that combines Recurrent Neural Networks, word embedding models, and the principle of compositionality. The SimpleNets systems explore the idea that the quality of a translation can be derived from the quality of its n-grams. This approach has been successfully employed in Text SimpliÔ¨Åcation quality assessment in the past. Our experiments show that, surprisingly, our models can learn more about a translation‚Äôs quality by focusing on the original sentence, rather than on the translation itself. 
Tree-to-tree machine translation (MT) that utilizes syntactic parse trees on both source and target sides suffers from the non-isomorphism of the parse trees due to parsing errors and the difference of annotation criterion between the two languages. In this paper, we present a method that projects dependency parse trees from the language side that has a high quality parser, to the side that has a low quality parser, to improve the isomorphism of the parse trees. We Ô¨Årst project a part of the dependencies with high conÔ¨Ådence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 
Information about the antecedents of pronouns is considered essential to solve certain translation divergencies, such as those concerning the English pronoun it when translated into gendered languages, e.g. for French into il, elle, or several other options. However, no machine translation system using anaphora resolution has so far been able to outperform a phrase-based statistical MT baseline. We address here one of the reasons for this failure: the imperfection of automatic anaphora resolution algorithms. Using parallel data, we learn probabilistic correlations between target-side pronouns and the gender and number features of their (uncertain) antecedents, as hypothesized by the Stanford Coreference Resolution system on the source side. We embody these correlations into a secondary translation model, which we invoke upon decoding with the Moses statistical phrase-based MT system. This solution outperforms a deterministic pronoun post-editing system, as well as a statistical MT baseline, on automatic and human evaluation metrics. 
German verbal inÔ¨Çection is frequently wrong in standard statistical machine translation approaches. German verbs agree with subjects in person and number, and they bear information about mood and tense. For subject‚Äìverb agreement, we parse German MT output to identify subject‚Äìverb pairs and ensure that the verb agrees with the subject. We show that this approach improves subject-verb agreement. We model tense/mood translation from English to German by means of a statistical classiÔ¨Åcation model. Although our model shows good results on wellformed data, it does not systematically improve tense and mood in MT output. Reasons include the need for discourse knowledge, dependency on the domain, and stylistic variety in how tense/mood is translated. We present a thorough analysis of these problems. 
We address the problem of mistranslated predicate-argument structures in syntaxbased machine translation. This paper explores whether knowledge about semantic afÔ¨Ånities between the target predicates and their argument Ô¨Ållers is useful for translating ambiguous predicates and arguments. We propose a selectional preference feature based on the selectional association measure of Resnik (1996) and integrate it in a string-to-tree decoder. The feature models selectional preferences of verbs for their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. We compare our features with a variant of the neural relational dependency language model (RDLM) (Sennrich, 2015) and Ô¨Ånd that neither of the features improves automatic evaluation metrics. We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspeciÔ¨Åed syntactic relations are negatively impacting these features. 
We explore two approaches to model complement types (NPs and PPs) in an Englishto-German SMT system: A simple abstract representation inserts pseudo-prepositions that mark the beginning of noun phrases, to improve the symmetry of source and target complement types, and to provide a Ô¨Çat structural information on phrase boundaries. An extension of this representation generates context-aware synthetic phrasetable entries conditioned on the source side, to model complement types in terms of grammatical case and preposition choice. Both the simple preposition-informed system and the context-aware system signiÔ¨Åcantly improve over the baseline; and the context-aware system is slightly better than the system without context information. 
Neural machine translation (NMT) has emerged recently as a promising statistical machine translation approach. In NMT, neural networks (NN) are directly used to produce translations, without relying on a pre-existing translation framework. In this work, we take a step towards bridging the gap between conventional word alignment models and NMT. We follow the hidden Markov model (HMM) approach that separates the alignment and lexical models. We propose a neural alignment model and combine it with a lexical neural model in a loglinear framework. The models are used in a standalone word-based decoder that explicitly hypothesizes alignments during search. We demonstrate that our system outperforms attention-based NMT on two tasks: IWSLT 2013 German‚ÜíEnglish and BOLT Chinese‚ÜíEnglish. We also show promising results for re-aligning the training data using neural models. 
We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on EnglishCzech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment. 
Neural network language and translation models have recently shown their great potentials in improving the performance of phrase-based machine translation. At the same time, word representations using different word factors have been translation quality and are part of many state-of-theart machine translation systems. used in many state-of-the-art machine translation systems, in order to support better translation quality. In this work, we combined these two ideas by investigating the combination of both techniques. By representing words in neural network language models using different factors, we were able to improve the models themselves as well as their impact on the overall machine translation performance. This is especially helpful for morphologically rich languages due to their large vocabulary size. Furthermore, it is easy to add additional knowledge, such as source side information, to the model. Using this model we improved the translation quality of a state-of-the-art phrasebased machine translation system by 0.7 BLEU points. We performed experiments on three language pairs for the news translation task of the WMT 2016 evaluation. 
Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder‚Äìdecoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-ofspeech tags, and syntactic dependency labels as input features to English‚ÜîGerman and English‚ÜíRomanian neural machine translation systems. In experiments on WMT16 training and test sets, we Ô¨Ånd that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An opensource implementation of our neural MT system is available1, as are sample Ô¨Åles and conÔ¨Ågurations2. 
Training discriminative rule selection models is usually expensive because of the very large size of the hierarchical grammar. Previous approaches reduced the training costs either by (i) using models that are local to the source side of the rules or (ii) by heavily pruning out negative samples. Moreover, all previous evaluations were performed on small scale translation tasks, containing at most 250,000 sentence pairs. We propose two contributions to discriminative rule selection. First, we test previous approaches on two French-English translation tasks in domains for which only limited resources are available and show that they fail to improve translation quality. To improve on such tasks, we propose a rule selection model that is (i) global with rich label-dependent features (ii) trained with all available negative samples. Our global model yields signiÔ¨Åcant improvements, up to 1 BLEU point, over previously proposed rule selection models. Second, we successfully scale rule selection models to large translation tasks but have so far failed to produce signiÔ¨Åcant improvements in BLEU on these tasks. 
Speed of access is a very important property for phrase tables in phrase based statistical machine translation as they are queried many times per sentence. In this paper we present a new standalone phrase table, optimized for query speed and memory locality. The phrase table is cache free and can optionally incorporate a reordering table within. We are able to achieve two times faster decoding by using our phrase table in the Moses decoder in place of the current state-of-the-art phrase table solution without sacriÔ¨Åcing translation quality. Using a new, experimental version of Moses we are able to achieve 10 times faster decoding using our novel phrase table. 
This work systematically analyzes the smoothing effect of vocabulary reduction for phrase translation models. We extensively compare various word-level vocabularies to show that the performance of smoothing is not signiÔ¨Åcantly affected by the choice of vocabulary. This result provides empirical evidence that the standard phrase translation model is extremely sparse. Our experiments also reveal that vocabulary reduction is more effective for smoothing large-scale phrase tables. 
We study the relationship between word order freedom and preordering in statistical machine translation. To assess word order freedom, we Ô¨Årst introduce a novel entropy measure which quantiÔ¨Åes how difÔ¨Åcult it is to predict word order given a source sentence and its syntactic analysis. We then address preordering for two target languages at the far ends of the word order freedom spectrum, German and Japanese, and argue that for languages with more word order freedom, attempting to predict a unique word order given source clues only is less justiÔ¨Åed. Subsequently, we examine lattices of n-best word order predictions as a uniÔ¨Åed representation for languages from across this broad spectrum and present an effective solution to a resulting technical issue, namely how to select a suitable source word order from the lattice during training. Our experiments show that lattices are crucial for good empirical performance for languages with freer word order (English‚ÄìGerman) and can provide additional improvements for Ô¨Åxed word order languages (English‚Äì Japanese). 
One of the main interests in the analysis of large document collections is to discover domains of discourse that are still actively developing, growing in interest and relevance, at a given point in time, and to distinguish them from those topics that are in stagnation or decline. The present paper describes a terminologically inspired approach to this kind of task. The inputs to the method are a corpus spanning several decades of research in computational linguistics and a set of single-word terms that frequently occur in that corpus. The diachronic development of these terms is modelled by means of term life cycle information, namely the parameters relative frequency and productivity. In a second step, k-means clustering is used to identify groups of terms with similar development patterns. The paper describes a mathematical approach to modelling term productivity and discusses what kind of information can be obtained from this measure. The results of the clustering experiment are promising and well motivate future research. 
Policy agenda research is concerned with measuring the policymaker activities. Topic classiÔ¨Åcation has proven a valuable tool for policy agenda research. However, manual topic coding is extremely costly and time-consuming. Supervised topic classiÔ¨Åcation offers a cost-effective and reliable alternative, yet it introduces new challenges, the most signiÔ¨Åcant of which are the training set coding, classiÔ¨Åer design, and accuracy-efÔ¨Åciency trade-off. In this work, we address these challenges in the context of the recently launched Croatian Policy Agendas project. We describe a new policy agenda dataset, explore the many system design choices, and report on the insights gained. Our best-performing model reaches 77% and 68% of F1-score for major topics and subtopics, respectively. 
Progress made in recent years has led to a growing interest in Digital Heritage. This article focuses on Egyptology and, more speciÔ¨Åcally, the study and preservation of ancient Egyptian scripts. We present a Text Retrieval system developed speciÔ¨Åcally to work with hieroglyphic texts. We intend to make it freely available to the research community. To the best of our knowledge this is the Ô¨Årst tool of its kind. 
We present results on part-of-speech and morphological tagging for Old Swedish (1225‚Äì1526). In a set of experiments we look at the difference between withincorpus and across-corpus accuracy, and explore ways of mitigating the effects of variation and data sparseness by adding different types of dictionary information. Combining several methods, together with a simple approach to handle spelling variation, we achieve a major boost in tagger performance on a modest test collection. 
In this paper, we describe the development of a language identiÔ¨Åcation system and a part-of-speech tagger for Latin-Middle English mixed text. To this end, we annotate data with language IDs and Universal POS tags (Petrov et al., 2012). As a classiÔ¨Åer, we train a conditional random Ô¨Åeld classiÔ¨Åer for both sub-tasks, including features generated by the TreeTagger models of both languages. The focus lies on both a general and a task-speciÔ¨Åc evaluation. Moreover, we describe our effort concerning beyond proof-of-concept implementation of tools and towards a more task-oriented approach, showing how to apply our techniques in the context of Humanities research. 
This paper describes our contribution to two challenges in data-driven lemmatization. We approach lemmatization in the framework of a two-stage process, where Ô¨Årst lemma candidates are generated and afterwards a ranker chooses the most probable lemma from these candidates. The Ô¨Årst challenge is that languages with rich morphology like Modern German can feature morphological changes of different kinds, in particular word-internal modiÔ¨Åcation. This makes the generation of the correct lemma a harder task than just removing sufÔ¨Åxes (stemming). The second challenge that we address is spelling variation as it appears in non-standard texts. We experiment with different generators that are speciÔ¨Åcally tailored to deal with these two challenges. We show in an oracle setting that there is a possible increase in lemmatization accuracy of 14% with our methods to generate lemma candidates on Middle Low German, a group of historical dialects of German (1200‚Äì1650 AD). Using a log-linear model to choose the correct lemma from the set, we obtain an actual increase of 5.56%. 
The growth of digitization in the cultural heritage domain offers great possibilities to broaden the boundaries of historical research. With the ultimate aim of creating social networks of person names from news articles, we introduce a person name disambiguation method that exploits the relation between the ambiguity of a person name and the number of entities referred to by it. Modeled as a clustering problem with a strong focus on social relations, our system dynamically adapts its clustering strategy to the most suitable conÔ¨Åguration for each name depending on how common this name is. Our method‚Äôs performance is on par with the state-of-the-art reported for the CRIPCO dataset, while using less speciÔ¨Åc resources. 
The Regesta Imperii (RI) are an important source for research in European-medieval history. Sources spread over many centuries of medieval history ‚Äì mainly charters of German-Roman Emperors ‚Äì are summarized as ‚ÄúRegests‚Äù and pooled in the RI. Interesting medieval demographic groups and players are i.a. cities, citizens or spiritual institutions (e.g. bishops or monasteries). Themes of historical interest are i.a. peace and war or the endowment of new privileges. We investigate the RI for important players and themes, applying state-of-the-art text classiÔ¨Åcation methods from computational linguistics. We examine the performance of different classiÔ¨Åcation methods in view of the linguistically very heterogeneous RI, including a Neural Network approach that is designed to capture complex interactions between players and themes. 
This paper describes ongoing work on a tool developed for annotating document images for their multimodal features and compiling this information into a corpus. The tool leverages open source computer vision and natural language processing libraries to describe the content and structure of multimodal documents and to generate multiple layers of XML annotation. The paper introduces the annotation schema, describes the document processing pipeline and concludes with a brief description of future work. 
 Lemlat is a morphological analyser for Latin, which shows a remarkably wide coverage of the Latin lexicon. However, the performance of the tool is limited by the absence of proper names in its lexical basis. In this paper we present the extension of Lemlat with a large Onomasticon for Latin. First, we describe and motivate the automatic and manual procedures for including the proper names in Lemlat. Then, we compare the new version of Lemlat with the previous one, by evaluating their lexical coverage of four Latin texts of different era and genre.  
Sarcasm annotation extends beyond linguistic expertise, and often involves cultural context. This paper presents our Ô¨Årst-of-its-kind study that deals with impact of cultural differences on the quality of sarcasm annotation. For this study, we consider the case of American text and Indian annotators. For two sarcasmlabeled datasets of American tweets and discussion forum posts that have been annotated by American annotators, we obtain annotations from Indian annotators. Our Indian annotators agree with each other more than their American counterparts, and face difÔ¨Åculties in case of unfamiliar situations and named entities. However, these difÔ¨Åculties in sarcasm annotation result in statistically insigniÔ¨Åcant degradation in sarcasm classiÔ¨Åcation. We also show that these disagreements between annotators can be predicted using textual properties. Although the current study is limited to two annotators and one culture pair, our paper opens up a novel direction in evaluation of the quality of sarcasm annotation, and the impact of this quality on sarcasm classiÔ¨Åcation. This study forms a stepping stone towards systematic evaluation of quality of these datasets annotated by non-native annotators, and can be extended to other culture combinations. 
This paper presents a proposal for the normalization of word-forms in historical texts. To perform this task, we extend our previous research on induction of phonology and adapt it to the task of normalization. In particular, we combine our earlier models with models for learning morphology (without additional supervision). The results are mixed: induction of the segmentation of morphemes fails to directly offer signiÔ¨Åcant improvements while including known morpheme boundaries in standard texts do improve results. 
Despite considerable theoretical work in social sciences, ready to use resources are very limited compared to digitally available mass media resources. Thus, this project creates a political protest database from online news resources in Brazil that will be used to explain Brazilian welfare state policy changes. In this paper we present the preliminary results of a system that automatically crawls digital resources and produces a protest database, which includes events such as strikes, rallies, boycotts, protests, and riots, as well as their attributes such as location, participants, and ideology. 
Our research aims at tracking the semantic evolution of the lexicon over time. For this purpose, we investigated two wellknown training protocols for neural language models in a synchronic experiment and encountered several problems relating to accuracy and reliability. We were able to identify critical parameters for improving the underlying protocols in order to generate more adequate diachronic language models. 
This paper provides a description of the automatic conversion of the morphologically annotated part of the Old Hungarian Corpus. These texts are in the format of the Humor analyzer, which does not follow any international standards. Since standardization always facilitates future research, even for researchers who do not know the Old Hungarian language, we opted for mapping the Humor formalism to a widely used universal tagset, namely the Universal Dependencies framework. The beneÔ¨Åts of using a shared tagset across languages enable interlingual comparisons from a theoretical point of view and also multilingual NLP applications can proÔ¨Åt from a uniÔ¨Åed annotation scheme. In this paper, we report the adaptation of the Universal Dependencies morphological annotation scheme to Old Hungarian, and we discuss the most important theoretical linguistic issues that had to be resolved during the process. We focus on the linguistic phenomena typical of Old Hungarian that required special treatment and we offer solutions to them. 
Psychological studies have shown that our state of mind can manifest itself in the linguistic features we use to communicate. Recent statistics in suicide prevention show that young people are increasingly posting their last words online. In this paper, we investigate whether it is possible to automatically identify suicide notes and discern them from other types of online discourse based on analysis of sentiments and linguistic features. Using supervised learning, we show that our model achieves an accuracy of 86.6%, outperforming previous work on a similar task by over 4%. 
Social scientists and journalists nowadays have to deal with an increasingly large amount of data. It usually requires expensive searching and annotation effort to Ô¨Ånd insight in a sea of information. Our goal is to build a discourse analysis system which can be applied to large text collections. This system can help social scientists and journalists to analyze data and validate their research theories by providing them with tailored machine learning methods to alleviate the annotation effort and exploratory facilities and visualization tools. We report initial experimental results in a case study related to discourse analysis in political debates. 
This paper investigates whether sentence structure analysis‚Äîexamining who appears in subject versus object position‚Äî can illuminate who academic articles portray as having agency in labor relations. We extract subjects and objects from a corpus of 3,800 academic articles, and compare both the relative occurrence of different groups (workers, women, employers) in each position and the verbs that most commonly attach to each group. We conclude that agency, while elusive, can potentially be modeled by sentence structure analysis. 
The Coptic language of Hellenistic era Egypt in the first millennium C.E. is a treasure trove of information for History, Religious Studies, Classics, Linguistics and many other Humanities disciplines. Despite the existence of large amounts of text in the language, comparatively few digital resources have been available, and almost no tools for Natural Language Processing. This paper presents an endto-end, freely available open source tool chain starting with Unicode plain text or XML transcriptions of Coptic manuscript data, which adds fully automatic word and morpheme segmentation, normalization, language of origin recognition, part of speech tagging, lemmatization, and dependency parsing at the click of a button. We evaluate each component of the pipeline, which is accessible as a Web interface and machine readable API online. 
Syntactic change tends to affect constructions, but treebanks annotate lower-level structure: PCFG rules or dependency arcs. This paper extends prior work in native language identiÔ¨Åcation, using Tree Substitution Grammars to discover constructions which can be tested for historical variability. In a case study comparing Classical and Medieval Latin, the system discovers several constructions corresponding to known historical differences, and learns to distinguish the two varieties with high accuracy. Applied to an intermediate text (the Vulgate Bible), it indicates which changes between the eras were already occurring at this earlier stage. 
We present a new approach for modeling diachronic linguistic change in grammatical usage. We illustrate the approach on English scientiÔ¨Åc writing in Late Modern English, focusing on grammatical patterns that are potentially indicative of shifts in register, genre and/or style. Commonly, diachronic change is characterized by the relative frequency of typical linguistic features over time. However, to fully capture changing linguistic usage, feature productivity needs to be taken into account as well. We introduce a data-driven approach for systematically detecting typical features and assessing their productivity over time, using information-theoretic measures of entropy and surprisal. 
This paper conceptualizes speech prosody data mining and its potential application in data-driven phonology/phonetics research. We Ô¨Årst conceptualize Speech Prosody Mining (SPM) in a time-series data mining framework. SpeciÔ¨Åcally, we propose using efÔ¨Åcient symbolic representations for speech prosody time-series similarity computation. We experiment with both symbolic and numeric representations and distance measures in a series of time-series classiÔ¨Åcation and clustering experiments on a dataset of Mandarin tones. Evaluation results show that symbolic representation performs comparably with other representations at a reduced cost, which enables us to efÔ¨Åciently mine large speech prosody corpora while opening up to possibilities of using a wide range of algorithms that require discrete valued data. We discuss the potential of SPM using time-series mining techniques in future works. 
This paper presents a proposal for learning morphological inÔ¨Çections by a graphemeto-phoneme learning model. No special processing is used for speciÔ¨Åc languages. The starting point has been our previous research on induction of phonology and morphology for normalization of historical texts. The results show that a very simple method can indeed improve upon some baselines, but does not reach the accuracies of the best systems in the task. 
For languages such as German where compounds occur frequently and are written as single tokens, a wide variety of NLP applications beneÔ¨Åts from recognizing and splitting compounds. As the traditional word frequency-based approach to compound splitting has several drawbacks, this paper introduces a letter sequence labeling approach, which can utilize rich word form features to build discriminative learning models that are optimized for splitting. Experiments show that the proposed method signiÔ¨Åcantly outperforms state-ofthe-art compound splitters. 
Many people are multilingual and they may draw from multiple language varieties when writing their messages. This paper is a Ô¨Årst step towards analyzing and detecting code-switching within words. We Ô¨Årst segment words into smaller units. Then, words are identiÔ¨Åed that are composed of sequences of subunits associated with different languages. We demonstrate our method on Twitter data in which both Dutch and dialect varieties labeled as Limburgish, a minority language, are used. 
Three popular vocal-tract animation paradigms were tested for intelligibility when displaying videos of pre-recorded Electromagnetic Articulography (EMA) data in an online experiment. EMA tracks the position of sensors attached to the tongue. The conditions were dots with tails (where only the coil location is presented), 2D animation (where the dots are connected to form 2D representations of the lips, tongue surface and chin), and a 3D model with coil locations driving facial and tongue rigs. The 2D animation (recorded in VisArtico) showed the highest identiÔ¨Åcation of the prompts. 
Conversion is a word formation operation that changes the grammatical category of a word in the absence of overt morphology. Conversion is extremely productive in English (e.g., tunnel, talk). This paper investigates whether distributional information can be used to predict the diachronic direction of conversion for homophonous noun‚Äìverb pairs. We aim to predict, for example, that tunnel was used as a noun prior to its use as a verb. We test two hypotheses: (1) that derived forms are less frequent than their bases, and (2) that derived forms are more semantically speciÔ¨Åc than their bases, as approximated by information theoretic measures. We Ô¨Ånd that hypothesis (1) holds for N-to-V conversion, while hypothesis (2) holds for V-to-N conversion. We achieve the best overall account of the historical data by taking both frequency and semantic speciÔ¨Åcity into account. These results provide a new perspective on linguistic theories regarding the semantic speciÔ¨Åcity of derivational morphemes, and on the morphosyntactic status of conversion. 
SyllabiÔ¨Åcation is sometimes inÔ¨Çuenced by morphological boundaries. We show that incorporating morphological information can improve the accuracy of orthographic syllabiÔ¨Åcation in English and German. Surprisingly, unsupervised segmenters, such as Morfessor, can be more useful for this purpose than the supervised ones. 
This paper presents an approach for the formal representation of components in German compounds. We assume that such a formal representation will support the segmentation and analysis of unseen compounds that feature components already seen in other compounds. An extensive language resource that explicitly codes components of compounds is GermaNet, a lexical semantic network for German. We summarize the GermaNet approach to the description of compounds, discussing some of its shortcomings. Our proposed extension of this representation builds on the lemon lexicon model for ontologies, established by the W3C Ontology Lexicon Community Group. 
Recent work has proposed using network science to analyse the structure of the mental lexicon by viewing words as nodes in a phonological network, with edges connecting words that differ by a single phoneme. Comparing the structure of phonological networks across different languages could provide insights into linguistic typology and the cognitive pressures that shape language acquisition, evolution, and processing. However, previous studies have not considered how statistics gathered from these networks are affected by factors such as lexicon size and the distribution of word lengths. We show that these factors can substantially affect the statistics of a phonological network and propose a new method for making more robust comparisons. We then analyse eight languages, Ô¨Ånding many commonalities but also some qualitative differences in their lexicon structure. 
It is commonly accepted that morphological dependencies are Ô¨Ånite-state in nature. We argue that the upper bound on morphological expressivity is much lower. Drawing on technical results from computational phonology, we show that a variety of morphotactic phenomena are tierbased strictly local and do not fall into weaker subclasses such as the strictly local or strictly piecewise languages. Since the tier-based strictly local languages are learnable in the limit from positive texts, this marks a Ô¨Årst important step towards general machine learning algorithms for morphology. Furthermore, the limitation to tier-based strictly local languages explains typological gaps that are puzzling from a purely linguistic perspective. 
We present a novel approach to the unsupervised learning of morphology. In particular, we use a Multiple Cause Mixture Model (MCMM), a type of autoencoder network consisting of two node layers‚Äîhidden and surface‚Äîand a matrix of weights connecting hidden nodes to surface nodes. We show that an MCMM shares crucial graphical properties with autosegmental morphology. We argue on the basis of this graphical similarity that our approach is theoretically sound. Experiment results on Hebrew data show that this theoretical soundness bears out in practice. 
In this paper I present a k-means clustering approach to inferring morphological position classes (morphotactics) from Interlinear Glossed Text (IGT), data collections available for some endangered and low-resource languages. While the experiment is not restricted to low-resource languages, they are meant to be the targeted domain. SpeciÔ¨Åcally my approach is meant to be for Ô¨Åeld linguists who do not necessarily know how many position classes there are in the language they work with and what the position classes are, but have the expertise to evaluate different hypotheses. It builds on an existing approach (Wax, 2014), but replaces the core heuristic with a clustering algorithm. The results mainly illustrate two points. First, they are largely negative, which shows that the baseline algorithm (summarized in the paper) uses a very predictive feature to determine whether afÔ¨Åxes belong to the same position class, namely edge overlap in the afÔ¨Åx graph. At the same time, unlike the baseline method that relies entirely on a single feature, kmeans clustering can account for different features and helps discover more morphological phenomena, e.g. circumÔ¨Åxation. I conclude that unsupervised learning algorithms such as k-means clustering can in principle be used for morphotactics inference, though the algorithm should probably weigh certain features more than others. Most importantly, I conclude that clustering is a promising approach for diverse morphotactics and as such it can facilitate linguistic analysis of Ô¨Åeld languages. 
We present a study about automated discourse analysis of oral narrative language in adolescents with autistic spectrum disorder (ASD). The basis of this evaluation is an existing dataset of Ô¨Åctional narrations of individuals with ASD and two matched comparison groups. We use three robust measures for quantifying different aspects of text cohesion on this corpus. These measures and several combinations of them correlate strongly with human cohesion annotations. Our evaluation will show which of these also distinguish the ASD group from the two comparison groups, which do not, and which differences are related to language competence rather than to factors speciÔ¨Åc to ASD. 
Many studies have been made on the language alterations that take place over the course of Alzheimer‚Äôs disease (AD). As a consequence, it is now admitted that it is possible to discriminate between healthy and ailing patients solely based on the analysis of language production. Most of these studies, however, were made on very small samples‚Äî30 participants per study, on an average‚Äî, or involved a great deal of manual work in their analysis. In this paper, we present an automatic analysis of transcripts of elderly participants describing six common objects. We used partof-speech and lexical richness as linguistic features to train an SVM classiÔ¨Åer to automatically discriminate between healthy and AD patients in the early and moderate stages. The participants, in the corpus used for this study, were 63 Spanish adults over 55 years old (29 controls and 34 AD patients). With an accuracy of 88%, our experimental results compare favorably to those relying on the manual extraction of attributes, providing evidence that the need for manual analysis can be overcome without sacriÔ¨Åcing in performance. 
Many studies have found that language alterations can aid in the detection of certain medical afÔ¨Çictions. In this work, we present an ongoing project for recollecting multilingual conversations with the elderly in Latin America. This project, so far, involves the combined efforts of psychogeriatricians, linguists, computer scientists, research nurses and geriatric caregivers from six institutions across USA, Canada, Mexico and Ecuador. The recollections are being made available to the international research community. They consist of conversations with adults aged sixty and over, with different nationalities and socio-economic backgrounds. Conversations are recorded on video, transcribed and time-aligned. Additionally, we are in the process of receiving written texts‚Äîrecent or old‚Äîauthored by the participants, provided voluntarily. Each participant is recorded at least twice a year to allow longitudinal studies. Furthermore, information such as medical history, educational background, economic level, occupation, medications and treatments is being registered to aid conducting research on treatment progress and pharmacological effects. Potential studies derived from this work include speech, voice, writing, discourse, and facial and corporal expression analysis. We believe that our recollections incorporate complementary data that can aid researchers in further understanding the progression of cognitive degenerative diseases of the elderly.  
This paper aims at utilizing cognitive information obtained from the eye movements behavior of annotators for automatic coreference resolution. We Ô¨Årst record eye-movement behavior of multiple annotators resolving coreferences in 22 documents selected from MUC dataset. By inspecting the gaze-regression proÔ¨Åles of our participants, we observe how regressive saccades account for selection of potential antecedents for a certain anaphoric mention. Based on this observation, we then propose a heuristic to utilize gaze data to prune mention pairs in mention-pair model, a popular paradigm for automatic coreference resolution. Consistent improvement in accuracy across several classiÔ¨Åers is observed with our heuristic, demonstrating why cognitive data can be useful for a difÔ¨Åcult task like coreference resolution. 
This paper presents a method for linking models for aligning linguistic etymological data with models for phylogenetic inference from population genetics. We begin with a large database of genetically related words‚Äîsets of cognates‚Äîfrom languages in a language family. We process the cognate sets to obtain a complete alignment of the data. We use the alignments as input to a model developed for phylogenetic reconstruction in population genetics. This is achieved via a natural novel projection of the linguistic data onto genetic primitives. As a result, we induce phylogenies based on aligned linguistic data. We place the method in the context of those reported in the literature, and illustrate its operation on data from the Uralic language family, which results in family trees that are very close to the ‚Äútrue‚Äù (expected) phylogenies. 
Syntactic bootstrapping is the hypothesis that learners can use the preliminary syntactic structure of a sentence to identify and characterise the meanings of novel verbs. Previous work has shown that syntactic bootstrapping can begin using only a few seed nouns (Connor et al., 2010; Connor et al., 2012). Here, we relax their key assumption: rather than training the model over the entire corpus at once (batch mode), we train the model incrementally, thus more realistically simulating a human learner. We also improve on the verb prediction method by incorporating the assumption that verb assignments are stable over time. We show that, given a high enough number of seed nouns (around 30), an incremental model achieves similar performance to the batch model. We also Ô¨Ånd that the number of seed nouns shown to be sufÔ¨Åcient in the previous work is not sufÔ¨Åcient under the more realistic incremental model. The results demonstrate that adopting more realistic assumptions about the early stages of language acquisition can provide new insights without undermining performance. 
One of the characteristics of child-directed speech is its high degree of repetitiousness. Sequences of repetitious utterances with a constant intention, variation sets, have been shown to be correlated with children‚Äôs language acquisition. To obtain a baseline for the occurrences of variation sets in Swedish, we annotate 18 parent‚Äì child dyads using a generalised deÔ¨Ånition according to which the varying form may pertain not just to the wording but also to prosody and/or non-verbal cues. To facilitate further empirical investigation, we introduce a surface algorithm for automatic extraction of variation sets which is easily replicable and language-independent. We evaluate the algorithm on the Swedish gold standard, and use it for extracting variation sets in Croatian, English and Russian. We show that the proportion of variation sets in child-directed speech decreases consistently as a function of children‚Äôs age across Swedish, Croatian, English and Russian. 
This paper presents a novel model that learns and exploits embeddings of phone ngrams for word segmentation in child language acquisition. Embedding-based models are evaluated on a phonemically transcribed corpus of child-directed speech, in comparison with their symbolic counterparts using the common learning framework and features. Results show that learning embeddings signiÔ¨Åcantly improves performance. We make use of extensive visualization to understand what the model has learned. We show that the learned embeddings are informative for both word segmentation and phonology in general. 
Experiments in ArtiÔ¨Åcial Language Learning have revealed much about the cognitive mechanisms underlying sequence and language learning in human adults, in infants and in non-human animals. This paper focuses on their ability to generalize to novel grammatical instances (i.e., instances consistent with a familiarization pattern). Notably, the propensity to generalize appears to be negatively correlated with the amount of exposure to the artiÔ¨Åcial language, a fact that has been claimed to be contrary to the predictions of statistical models (PenÀúa et al. (2002); Endress and Bonatti (2007)). In this paper, we propose to model generalization as a threestep process, and we demonstrate that the use of statistical models for the Ô¨Årst two steps, contrary to widespread intuitions in the ALL-Ô¨Åeld, can explain the observed decrease of the propensity to generalize with exposure time. 
In this study, we model the causal links between the complexities of different macroscopic aspects of child language. We consider pairs of sequences of measurements of the quantity and diversity of the lexical and grammatical properties. Each pair of sequences is taken as the trajectory of a high-dimensional dynamical system, some of whose dimensions are unknown. We use Multispatial Convergent Cross Mapping to ascertain the directions of causality between the pairs of sequences. Our results provide support for the hypothesis that children learn grammar through distributional learning, where the generalization of smaller structures enables generalizations at higher levels, consistent with the proposals of construction-based approaches to language. 
How do infants learn the meanings of their Ô¨Årst words? This study investigates the informativeness and temporal dynamics of non-verbal cues that signal the speaker‚Äôs referent in a model of early word‚Äìreferent mapping. To measure the information provided by such cues, a supervised classiÔ¨Åer is trained on information extracted from a multimodally annotated corpus of 18 videos of parent‚Äìchild interaction with three children aged 7 to 33 months. Contradicting previous research, we Ô¨Ånd that gaze is the single most informative cue, and we show that this Ô¨Ånding can be attributed to our Ô¨Åne-grained temporal annotation. We also Ô¨Ånd that offsetting the timing of the non-verbal cues reduces accuracy, especially if the offset is negative. This is in line with previous research, and suggests that synchrony between verbal and non-verbal cues is important if they are to be perceived as causally related. 
In this paper, we investigate the impact of context for the paraphrase ranking task, comparing and quantifying results for multi-word expressions and single words. We focus on systematic integration of existing paraphrase resources to produce paraphrase candidates and later ask human annotators to judge paraphrasability in context. We Ô¨Årst conduct a paraphrase-scoring annotation task with and without context for targets that are i) single- and multi-word expressions ii) verbs and nouns. We quantify how differently annotators score paraphrases when context information is provided. Furthermore, we report on experiments with automatic paraphrase ranking. If we regard the problem as a binary classiÔ¨Åcation task, we obtain an F1‚Äìscore of 81.56% and 79.87% for multi-word expressions and single words resp. using kNN classiÔ¨Åer. Approaching the problem as a learning-to-rank task, we attain MAP scores up to 87.14% and 91.58% for multiword expressions and single words resp. using LambdaMART, thus yielding highquality contextualized paraphrased selection. Further, we provide the Ô¨Årst dataset with paraphrase judgments for multi-word targets in context. 
Differentiating between outdated expressions and current expressions is not a trivial task for foreign language learners, and could be beneÔ¨Åcial for lexicographers, as they examine expressions. Assuming that the usage of expressions over time can be represented by a time-series of their periodic frequencies over a large lexicographic corpus, we test the hypothesis that there exists an old‚Äìnew relationship between the time-series of some synonymous expressions, a hint that a later expression has replaced an earlier one. Another hypothesis we test is that Multiword Expressions (MWEs) can be characterized by sparsity & frequency thresholds. Using a dataset of 1 million English books, we choose MWEs having the most positive or the most negative usage trends from a ready-made list of known MWEs. We identify synonyms of those expressions in a historical thesaurus and visualize the temporal relationships between the resulting expression pairs. Our empirical results indicate that old‚Äìnew usage relationships do exist between some synonymous expressions, and that new candidate expressions, not found in dictionaries, can be found by analyzing usage trends. 
In this work we carried out an idiom type identiÔ¨Åcation task on a set of 90 Italian V-NP and V-PP constructions comprising both idioms and non-idioms. Lexical variants were generated from these expressions by replacing their components with semantically related words extracted distributionally and from the Italian section of MultiWordNet. Idiomatic phrases turned out to be less similar to their lexical variants with respect to non-idiomatic ones in distributional semantic spaces. Different variant-based distributional measures of idiomaticity were tested. Our indices proved reliable in identifying also those idioms whose lexical variants are poorly or not at all attested in our corpus. 
This paper analyzes datasets with numerical scores that quantify the semantic compositionality of MWEs. We present the results of our analysis of crowdsourced compositionality judgments for noun compounds in three languages. Our goals are to look at the characteristics of the annotations in diÔ¨Äerent languages; to examine intrinsic quality measures for such data; and to measure the impact of Ô¨Ålters proposed in the literature on these measures. The cross-lingual results suggest that greater agreement is found for the extremes in the compositionality scale, and that outlier annotation removal is more eÔ¨Äective than outlier annotator removal. 
In this paper, we address the automatic induction of synonym paraphrases for the empirically challenging class of German particle verbs. Similarly to Cocos and Callison-Burch (2016), we incorporate a graph-based clustering approach for word sense discrimination into an existing paraphrase extraction system, (i) to improve the precision of synonym identiÔ¨Åcation and ranking, and (ii) to enlarge the diversity of synonym senses. Our approach signiÔ¨Åcantly improves over the standard system, but does not outperform an extended baseline integrating a simple distributional similarity measure. 
The paper presents an empirical study of integrating ngrams and multi-word terms into topic models, while maintaining similarities between them and words based on their component structure. First, we adapt the PLSA-SIM algorithm to the more widespread LDA model and ngrams. Then we propose a novel algorithm LDA-ITER that allows the incorporation of the most suitable ngrams into topic models. The experiments of integrating ngrams and multiword terms conducted on Ô¨Åve text collections in different languages and domains demonstrate a signiÔ¨Åcant improvement in all the metrics under consideration. 
We present a Ô¨Çexible method that rearranges the ranked output of compound splitters (i.e., decomposers of one-word compounds such as the German Kinderlied ‚Äòchildren‚Äôs song‚Äô) using a distributional semantics model. In an experiment, we show that our re-ranker improves the quality of various compound splitters. 
We examine the employment of word embeddings for machine translation (MT) of phrasal verbs (PVs), a linguistic phenomenon with challenging semantics. Using word embeddings, we augment the translation model with two features: one modelling distributional semantic properties of the source and target phrase and another modelling the degree of compositionality of PVs. We also obtain paraphrases to increase the amount of relevant training data. Our method leads to improved translation quality for PVs in a case study with English to Bulgarian MT system. 
Non-substitutability is a property of Multiword Expressions (MWEs) that often causes lexical rigidity and is relevant for most types of MWEs. EfÔ¨Åcient identiÔ¨Åcation of this property can result in the efÔ¨Åcient identiÔ¨Åcation of MWEs. In this work we propose using distributional semantics, in the form of word embeddings, to identify candidate substitutions for a candidate MWE and model its substitutability. We use our models to rank MWEs based on their lexical rigidity and study their performance in comparison with association measures. We also study the interaction between our models and association measures. We show that one of our models can signiÔ¨Åcantly improve over the association measure baselines, identifying collocations. 
Recent works in Natural Language Processing (NLP) using neural networks have focused on learning dense word representations to perform classiÔ¨Åcation tasks. When dealing with phrase prediction problems, is is common practice to use special tagging schemes to identify segments boundaries. This allows these tasks to be expressed as common word tagging problems. In this paper, we propose to learn Ô¨Åxed-size representations for arbitrarily sized chunks. We introduce a model that takes advantage of such representations to perform phrase tagging by directly identifying and classifying phrases. We evaluate our approach on the task of multiword expression (MWE) tagging and show that our model outperforms the stateof-the-art model for this task. 
This paper presents FrameNet‚Äôs approach to the representation of Support Verbs, as but one type of multiword expression (MWE) included in the database. In addition to motivating and illustrating FrameNet‚Äôs newly consistent annotation practice for Support Verb constructions, the present work advocates adopting a broad view of what constitutes a multiword expression. 
This study aims at determining whether collocational features automatically extracted from EFL (English as a foreign language) texts are useful for quality scoring, and allow the improvement of a competitive baseline based on, amongst other factors, bigram frequencies. The collocational features were gathered by assigning to each bigram in an EFL text eight association scores computed on the basis of a native reference corpus. The distribution of the association scores were then summarized by a few global statistical features and by a discretizing procedure. An experiment conducted on a publicly available dataset conÔ¨Årmed the effectiveness of these features and the beneÔ¨Åt brought by using several discretized association scores. 
In this paper we present a study on the production of collocations by students of European Portuguese as a foreign language. We start by gathering several corpora written by students, and identify the correct and incorrect collocations. We annotate the latter considering several different aspects, such as the error location, description and explanation. Then, taking these elements into consideration, we compare the performance of students considering their levels of proÔ¨Åciency, their mother tongue and, also, other languages they know. Finally, we correct all the students productions and contribute with a corpus of everyday language collocations that can be helpful in Portuguese classes. 
Linguistic resources for Polish are often missing multiword expressions (MWEs) ‚Äì idioms, compound nouns and other expressions which have their own distinct meaning as a whole. This paper describes an effort to extract and recognize nominal MWEs in Polish text using Wikipedia, inÔ¨Çection dictionaries and Ô¨Ånite-state automata. Wikipedia is used as a lexicon of MWEs and as a corpus annotated with links to articles. Incoming links for each article are used to determine the inÔ¨Çection pattern of the headword ‚Äì this approach helps eliminate invalid inÔ¨Çected forms. The goal is to recognize known MWEs as well as to Ô¨Ånd more expressions sharing similar grammatical structure and occurring in similar context. 
In this paper, we demonstrate the impact of Multiword Expression (MWE) resources in the task of MWE recognition in text. We present results based on the Wiki50 corpus for MWE resources, generated using unsupervised methods from raw text and resources that are extracted using manual text markup and lexical resources. We show that resources acquired from manual annotation yield the best MWE tagging performance. However, a more Ô¨Ånegrained analysis that differentiates MWEs according to their part of speech (POS) reveals that automatically acquired MWE lists outperform the resources generated from human knowledge for three out of four classes. 
Verb‚Äìnoun idiomatic combinations (VNICs) are idioms consisting of a verb with a noun in its direct object position. Usages of these expressions can be ambiguous between an idiomatic usage and a literal combination. In this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al. (2009), respectively. 
We propose a new task of extracting eventevent relations across documents. We present our efforts at designing an annotation schema and building a corpus for this task. Our schema includes Ô¨Åve main types of relations: Inheritance, Expansion, Contingency, Comparison and Temporality, along with 21 subtypes. We also lay out the main challenges based on detailed inter-annotator disagreement and error analysis. We hope these resources can serve as a benchmark to encourage research on this new problem. 
Abstract Meaning Representation (AMR) is an annotation framework in which the meaning of a full sentence is represented as a rooted, acyclic, directed graph. In this paper, we describe a pilot project in which we develop speciÔ¨Åcations for the annotation of a Chinese AMR corpus: the Chinese translation of the Little Prince. The interagreement smatch score between the two annotators is 0.83. We also propose to integrate alignment into Chinese AMR annotation. 
This paper presents the conversion of SynTagRus dependency structures into Penn Treebank style phrase structures, whose resulting data will be used to train a statistical constituency parser for Russian and create a large-scale constituency-parsed corpus. The implemented conversion includes various innovative features in order to create phrase structure trees that are closest to Penn Treebank style while optimally preserving information of the original dependency structure annotations. We believe the newly converted phrase structure treebank will be not only an adequate training dataset for our ongoing project but also a valuable resource for traditional and computational linguistic research. 
English grammars indicate a variety of relations holding between conjoined VPs. VPs conjoined by and evince such senses as Result, Temporal Sequence and Concession. Although all these senses are ones associated with discourse relations, conjoined VPs have not been fully included in discourse annotation. Because of the value of discourse-annotated corpora for developing approaches to automated sense recognition, we have added their annotation to the Penn Discourse TreeBank. This paper describes how tokens were identiÔ¨Åed; how the process of span and sense annotation was modiÔ¨Åed and extended in order to keep the annotation of intra-sentential multi-clausal structures consistent with the rest of the corpus; and what the resulting corpus looks like, in terms of token frequency and common sense patterns. 
We present a new multi-layered annotation scheme for orthographic errors in freely written German texts produced by primary school children. The scheme is closely linked to the German graphematic system and deÔ¨Ånes categories for both general structural word properties and errorrelated properties. Furthermore, it features multiple layers of information which can be used to evaluate an error. The categories can also be used to investigate properties of correctly-spelled words, and to compare them to the erroneous spellings. For data representation, we propose the XML-format LearnerXML. 
Linguistic annotation underlies many successful approaches in Natural Language Processing (NLP), where the annotated corpora are used for training and evaluating supervised learners. The consistency of annotation limits the performance of supervised models, and thus a lot of effort is put into obtaining high-agreement annotated datasets. Recent research has shown that annotation disagreement is not random noise, but carries a systematic signal that can be used for improving the supervised learner. However, prior work was limited in scope, focusing only on part-ofspeech tagging in a single language. In this paper we broaden the experiments to a semantic task (supersense tagging) using multiple languages. In particular, we analyse how systematic disagreement is for sense annotation, and we present a preliminary study of whether patterns of disagreements transfer across languages. 
The semantic relationship between a sentence and its context may be marked explicitly, or left to inference. Rohde et al. (2015) showed that, contrary to common assumptions, this isn‚Äôt exclusive or: a conjunction can often be inferred alongside an explicit discourse adverbial. Here we broaden the investigation to a larger set of 20 discourse adverbials by eliciting ‚âà28K conjunction completions via crowdsourcing. Our data replicate and extend Rohde et al.‚Äôs Ô¨Åndings that discourse adverbials do indeed license inferred conjunctions. Further, the diverse patterns observed for the adverbials include cases in which more than one valid connection can be inferred, each one endorsed by a substantial number of participants; such differences in annotation might otherwise be written off as annotator error or bias, or just a low level of inter-annotator agreement. These results will inform future discourse annotation endeavors by revealing where it is necessary to entertain implicit relations and elicit several judgments to fully characterize discourse relationships. 
We compared two methods to annotate a corpus via non-expert annotators for named entity (NE) recognition task, which are (1) revising the results of the existing NE recognizer and (2) annotating NEs only by hand. We investigated the annotation time, the degrees of agreement, and the performances based on the gold standard. As we have two annotators for one Ô¨Åle of each method, we evaluated the two performances, which are the averaged performances over the two annotators and the performances deeming the annotations correct when either of them is correct. The experiments revealed that the semi-automatic annotation was faster and showed better agreements and higher performances on average. However they also indicated that sometimes fully manual annotation should be used for some texts whose genres are far from its training data. In addition, the experiments using the annotated corpora via semi-automatic and fully manual annotation as training data for machine learning indicated that the F-measures sometimes could be better for some texts when we used manual annotation than when we used semi-automatic annotation. 
Genre and domain are well known covariates of both manual and automatic annotation quality. Comparatively less is known about the effect of sentence types, such as imperatives, questions or fragments, and how they interact with text type effects. Using mixed effects models, we evaluate the relative influence of genre and sentence types on automatic and manual annotation quality for three related tasks in English data: POS tagging, dependency parsing and coreference resolution. For the latter task, we also develop a new metric for the evaluation of individual regions of coreference annotation. Our results show that while there are substantial differences between manual and automatic annotation in each task, sentence type is generally more important than genre in predicting errors within our data. 
Automatically detecting the stance of people toward political and ideological topics ‚Äìnamely their ‚ÄúIdeological Perspective‚Äù‚Äì from social media is a rapidly growing research area with a wide range of applications. Research in such a Ô¨Åeld faces several challenges among which is the lack of annotated corpora and associated guidelines for collecting annotations. The problem is even more pronounced in situations where there is no clear taxonomy for the common community perspectives and ideologies. The challenges are exacerbated when the communities where we need to gather these annotations are in a state of turmoil causing subjectivity and intimidation to be factors in the annotation process. Accordingly, we present the process for creating a robust and succinct set of guidelines for annotating ‚ÄúEgyptian Ideological Perspectives‚Äù. We collect social media data discussing Egyptian politics and develop an iterative feedback annotation framework reÔ¨Åning the annotation task and associated guidelines attempting to circumvent both weaknesses. Our efforts lead to a signiÔ¨Åcant increase in inter-annotator agreement measures from 75.7% to 92% overall agreement. 
This paper deals with means of evaluating inter-annotator agreement for a normalization task. This task differs from common annotation tasks in two important aspects: (i) the class of labels (the normalized wordforms) is open, and (ii) annotations can match to different degrees. We propose a new method to measure inter-annotator agreement for the normalization task. It integrates common chancecorrected agreement measures, such as Fleiss‚Äôs Œ∫ or Krippendorff‚Äôs Œ±. The novelty of our proposed method lies in the way the annotated word forms are treated. First, they are evaluated character-wise; second, certain characters are mapped to more general categories. 
We present the Ô¨Årst corpus annotated with preposition supersenses, unlexicalized categories for semantic functions that can be marked by English prepositions (Schneider et al., 2015). The preposition supersenses are organized hierarchically and designed to facilitate comprehensive manual annotation. Our dataset is publicly released on the web.1 
We explore the annotation of information structure in German and compare the quality of expert annotation with crowdsourced annotation taking into account the cost of reaching crowd consensus. Concretely, we discuss a crowd-sourcing effort annotating focus in a task-based corpus of German containing reading comprehension questions and answers. Against the backdrop of a gold standard reference resulting from adjudicated expert annotation, we evaluate a crowd sourcing experiment using majority voting to determine a baseline performance. To reÔ¨Åne the crowd-sourcing setup, we introduce the Consensus Cost as a measure of agreement within the crowd. We investigate the usefulness of Consensus Cost as a measure of crowd annotation quality both intrinsically, in relation to the expert gold standard, and extrinsically, by integrating focus annotation information into a system performing Short Answer Assessment taking into account the Consensus Cost. We Ô¨Ånd that low Consensus Cost in crowd sourcing indicates high quality, though high cost does not necessarily indicate low accuracy but increased variability. Overall, taking Consensus Cost into account improves both intrinsic and extrinsic evaluation measures. 
In this paper we describe our efforts on POS annotation of a code-switching corpus created from Turkish-German tweets. We use Universal Dependencies (UD) POS tags as our tag set. While the German parts of the corpus employ UD speciÔ¨Åcations, for the Turkish parts we propose annotation guidelines that adopt UD‚Äôs language-general rules when it is applicable and adapt its principles to TurkishspeciÔ¨Åc phenomena when it is not. The resulting corpus has POS annotation of 1029 tweets, which is aligned with existing language identiÔ¨Åcation annotation. 
This article attempts to place dependency annotation options on a solid theoretical and applied footing. By verifying the validity of some basic choices of the current dependency reference framework, Universal Dependencies (UD), in a perspective of general annotation principles, we show how some choices can lead to inconsistencies and discontinuities, partly due to UD‚Äôs alternation between syntax and semantics. For some constructions, we propose better suited alternative structures with a clear-cut distinction of syntax and semantics. We propose a classiÔ¨Åcation of conception-oriented, annotatororiented, and Ô¨Ånally, treebank end-useroriented considerations to be used in the creation of new annotation schemes. 
Universal Dependencies (UD) are gaining much attention of late for systematic evaluation of cross-lingual techniques for crosslingual dependency parsing. In this paper we present our work in line with UD. Our contribution to this is manifold. We extend UD to Indian languages through conversion of PƒÅniÃ£ nian Dependencies to UD for the Hindi Dependency Treebank (HDTB). We discuss the differences in annotation in both the schemes, present parsing experiments for both the formalisms and empirically evaluate their weaknesses and strengths for Hindi. We produce an automatically converted Hindi Treebank conforming to the international standard UD scheme, making it useful as a resource for multilingual language technology. 
Content can be expressed at different levels of speciÔ¨Åcity, varying the amount of detail presented to the reader. The need to transform speciÔ¨Åc content into more general form naturally arises in summarization, where people and machines need to convey the gist of a text within imposed space constraints. Completely removing sentences and phrases is one way to reduce the level of detail. The bulk of work on summarization content selection and compression deal with these tasks. In this paper, we present a corpus study on a more subtle and understudied phenomenon: noun phrase generalization. Based on multi-document news and abstract alignments at the phrase level, we arrive at a Ô¨Åve category classiÔ¨Åcation scheme and Ô¨Ånd that the most common category requires semantic interpretation and inference. The others rely on lexical substitution or deletion of details from the original expression. We provide a systematic analysis, elucidating the capabilities needed for automating the generation of more general or more speciÔ¨Åc references. 
We present a method that, for the Ô¨Årst time in a broad coverage setting, uses natural language generation to automatically construct disambiguating paraphrases for structurally ambiguous sentences. By simply asking naive annotators to clarify which paraphrase is closer in meaning to the original sentence, the resulting paraphrases can potentially enable meaning judgments for parser training and domain adaptation to be crowd-sourced on a massive scale. To validate the method, we demonstrate that meaning judgments crowd-sourced in this way via Amazon Mechanical Turk have reasonably high accuracy‚Äîe.g. 80%, given a strong majority choice between two paraphrases‚Äîwith accuracy increasing as the level of agreement among annotators increases. We also show that even with just the limited validation data gathered to date, the crowd-sourced judgments make it possible to retrain a parser to achieve signiÔ¨Åcantly higher accuracy in a novel domain. We conclude with lessons learned for gathering such judgments on a much larger scale. 
This paper discusses the use of Universal Dependency for annotations of a Native North American language Arapaho (Algonquian). While some relations of the universal dependency perfectly correspond with those in Arapaho, language speciÔ¨Åc annotations of verbal arguments elucidate problems of assuming certain syntactic categories across languages. By critiquing the inÔ¨Çuence of grammatical structures of major European and Asian languages in establishing the UD framework, this paper develops guidelines for annotating a polysynthetic agglutinating language and sets a path to developing a more comprehensive cross-linguistic approach to syntactic annotations of language data. 
In this paper we present a framework for annotating the discourse and dialogue structure of SMS message conversations. The annotation speciÔ¨Åcations integrate elements of coherence-based discourse relations and communicative acts in conversational speech. We present annotation experiments that show reliable annotation can be achieved with this annotation framework. 
This paper describes the process of annotating a historical US civil war corpus with geographic reference. Reference annotations are given at two different textual scales: individual place names and documents. This is the Ô¨Årst published corpus of its kind in document-level geolocation, and it has over 10,000 disambiguated toponyms, double the amount of any prior toponym corpus. We outline many challenges and considerations in creating such a corpus, and we evaluate baseline and benchmark toponym resolution and document geolocation systems on it. Aspects of the corpus suggest several recommendations for proper annotation procedure for the tasks. 
Layer-wise relevance propagation (LRP) is a recently proposed technique for explaining predictions of complex non-linear classiÔ¨Åers in terms of input variables. In this paper, we apply LRP for the Ô¨Årst time to natural language processing (NLP). More precisely, we use it to explain the predictions of a convolutional neural network (CNN) trained on a topic categorization task. Our analysis highlights which words are relevant for a speciÔ¨Åc prediction of the CNN. We compare our technique to standard sensitivity analysis, both qualitatively and quantitatively, using a ‚Äúword deleting‚Äù perturbation experiment, a PCA analysis, and various visualizations. All experiments validate the suitability of LRP for explaining the CNN predictions, which is also in line with results reported in recent image classiÔ¨Åcation studies. 
We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question. We compare several variants of neural networks for sentence embeddings in a setting of decision-making based on evidence of varying relevance. We propose a basic model to integrate evidence for entailment, show that joint training of the sentence embeddings to model relevance and entailment is feasible even with no explicit perevidence supervision, and show the importance of evaluating strong baselines. We also demonstrate the beneÔ¨Åt of carrying over text comprehension model trained on an unrelated task for our small datasets. Our research is motivated primarily by a new open dataset we introduce, consisting of binary questions and news-based evidence snippets. We also apply the proposed relevance-entailment model on a similar task of ranking multiple-choice test answers, evaluating it on a preliminary dataset of school test questions as well as the standard MCTest dataset, where we improve the neural model state-of-art. 
This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings. Our model splits individual words into segments, and weights each segment according to its ability to predict context words. Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering. Finally, we show that incorporating morphology explicitly into character-level models helps them produce embeddings for unseen words which correlate better with human judgments. 
In this paper we present a study covering the creation of compositional distributional representations for English noun compounds (e.g. computer science) using two compositional models proposed in the literature. The compositional representations are Ô¨Årst evaluated based on their similarity to the corresponding corpus-learned representations and then on the task of automatic classiÔ¨Åcation of semantic relations for English noun compounds. Our experiments show that compositional models are able to build meaningful representations for more than half of the test set compounds. However, using pre-trained compositional models does not lead to the expected performance gains for the semantic relation classiÔ¨Åcation task. Models using compositional representations have a similar performance as a basic classiÔ¨Åcation model, despite the advantage of being pretrained on a large set of compounds. 
Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning. In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms. We describe an implementation of this framework using a combination of Restricted Boltzmann Machines and feedforward neural networks. Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets. 
We present a discussion forum assistant based on deep recurrent neural networks (RNNs). The assistant is trained to perform three different tasks when faced with a question from a user. Firstly, to recommend related posts. Secondly, to recommend other users that might be able to help. Thirdly, it recommends other channels in the forum where people may discuss related topics. Our recurrent forum assistant is evaluated experimentally by prediction accuracy for the end‚Äìto‚Äìend trainable parts, as well as by performing an end-user study. We conclude that the model generalizes well, and is helpful for the users. 
Semantic lexicons such as WordNet and PPDB have been used to improve the vector-based semantic representations of words by adjusting the word vectors. However, such lexicons lack semantic intensity information, inhibiting adjustment of vector spaces to better represent semantic intensity scales. In this work, we adjust word vectors using the semantic intensity information in addition to synonyms and antonyms from WordNet and PPDB, and show improved performance on judging semantic intensity orders of adjective pairs on three different human annotated datasets. 
In this work, we introduce temporal hierarchies to the sequence to sequence (seq2seq) model to tackle the problem of abstractive summarization of scientiÔ¨Åc articles. The proposed Multiple Timescale model of the Gated Recurrent Unit (MTGRU) is implemented in the encoderdecoder setting to better deal with the presence of multiple compositionalities in larger texts. The proposed model is compared to the conventional RNN encoderdecoder, and the results demonstrate that our model trains faster and shows signiÔ¨Åcant performance gains. The results also show that the temporal hierarchies help improve the ability of seq2seq models to capture compositionalities better without the presence of highly complex architectural hierarchies. 
Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec (Mikolov et al., 2013a) to learn document-level embeddings. Despite promising results in the original paper, others have struggled to reproduce those results. This paper presents a rigorous empirical evaluation of doc2vec over two tasks. We compare doc2vec to two baselines and two state-of-the-art document embedding methodologies. We found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained word embeddings. We also provide recommendations on hyper-parameter settings for generalpurpose applications, and release source code to induce document embeddings using our trained doc2vec models. 
Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN‚Äôs on these problems. We present an artiÔ¨Åcial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the beneÔ¨Åts of including hierarchical structure and of including LSTM-style gating are complementary. 
We introduce an LSTM-based method for dynamically integrating several wordprediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the beneÔ¨Åts of using such mixtures of experts in NLP and dialogue systems speciÔ¨Åcally. 
We consider the supervised training setting in which we learn task-speciÔ¨Åc word embeddings. We assume that we start with initial embeddings learned from unlabelled data and update them to learn taskspeciÔ¨Åc embeddings for words in the supervised training data. However, for new words in the test set, we must use either their initial embeddings or a single unknown embedding, which often leads to errors. We address this by learning a neural network to map from initial embeddings to the task-speciÔ¨Åc embedding space, via a multi-loss objective function. The technique is general, but here we demonstrate its use for improved dependency parsing (especially for sentences with out-of-vocabulary words), as well as for downstream improvements on sentiment analysis. 
Modal sense classiÔ¨Åcation (MSC) is a special WSD task that depends on the meaning of the proposition in the modal‚Äôs scope. We explore a CNN architecture for classifying modal sense in English and German. We show that CNNs are superior to manually designed feature-based classiÔ¨Åers and a standard NN classiÔ¨Åer. We analyze the feature maps learned by the CNN and identify known and previously unattested linguistic features. We benchmark the CNN on a standard WSD task, where it compares favorably to models using sense-disambiguated target vectors. 
Current approaches to learning vector representations of text that are compatible between different languages usually require some amount of parallel text, aligned at word, sentence or at least document level. We hypothesize however, that different natural languages share enough semantic structure that it should be possible, in principle, to learn compatible vector representations just by analyzing the monolingual distribution of words. In order to evaluate this hypothesis, we propose a scheme to map word vectors trained on a source language to vectors semantically compatible with word vectors trained on a target language using an adversarial autoencoder. We present preliminary qualitative results and discuss possible future developments of this technique, such as applications to cross-lingual sentence representations. 
Bilexical dependencies have been commonly used to help identify the most likely parses of a sentence. The probability of a word occurring as the dependent of a given head within a particular structure provides a measure of semantic plausibility that complements the purely syntactic part of the parsing model. Here, we attempt to use the distributional information within these bilexical dependencies to construct representations that decompose into semantic and syntactic components. In particular, we compare two different approaches to composing vectors to explore how syntactic and semantic representations should interact within such a model. Our results suggest a tensor product approach has advantages, which we believe could be exploited in making more effective use of the information captured in these bilexical dependencies. 
Community Question Answering forums, such as Quora and StackoverÔ¨Çow contain millions of questions and answers. Automatically Ô¨Ånding the relevant questions from the existing questions and Ô¨Ånding the relevant answers to a new question are Natural Language Processing tasks. In this paper, we aim to address these tasks, which we refer to as similar-Question Retrieval and Answer Selection. We present a neural-based model with stacked bidirectional LSTMs and MLP to address these tasks. The model generates the vector representations of the question-question or question-answer pairs and computes their semantic similarity scores, which are then employed to rank and predict relevancies. Extensive experiments demonstrate our results outperform the baselines. 
This paper presents a deep architecture for learning a similarity metric on variablelength character sequences. The model combines a stack of character-level bidirectional LSTM‚Äôs with a Siamese architecture. It learns to project variablelength strings into a Ô¨Åxed-dimensional embedding space by using only information about the similarity between pairs of strings. This model is applied to the task of job title normalization based on a manually annotated taxonomy. A small data set is incrementally expanded and augmented with new sources of variance. The model learns a representation that is selective to differences in the input that reÔ¨Çect semantic differences (e.g., ‚ÄúJava developer‚Äù vs. ‚ÄúHR manager‚Äù) but also invariant to nonsemantic string differences (e.g., ‚ÄúJava developer‚Äù vs. ‚ÄúJava programmer‚Äù). 
We study the event detection problem in the new type extension setting. In particular, our task involves identifying the event instances of a target type that is only speciÔ¨Åed by a small set of seed instances in text. We want to exploit the large amount of training data available for the other event types to improve the performance of this task. We compare the convolutional neural network model and the feature-based method in this type extension setting to investigate their effectiveness. In addition, we propose a two-stage training algorithm for neural networks that effectively transfers knowledge from the other event types to the target type. The experimental results show that the proposed algorithm outperforms strong baselines for this task. 
This paper introduces a parameterization for word embeddings produced by the Random Indexing framework. The parameterization introduces position speciÔ¨Åc weights in the context windows, and the approach is shown to improve the performance in both word similarity and sentiment classiÔ¨Åcation tasks. We also demonstrate the relation between Random Indexing and Convolutional Neural Networks. 
We present a simple yet effective approach for learning word sense embeddings. In contrast to existing techniques, which either directly learn sense representations from corpora or rely on sense inventories from lexical resources, our approach can induce a sense inventory from existing word embeddings via clustering of ego-networks of related words. An integrated WSD mechanism enables labeling of words in context with learned sense vectors, which gives rise to downstream applications. Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems. 
We introduce PDD (Pair Distance Distribution), a novel corpus-based model of semantic representation. Most corpus-based models are VSMs (Vector Space Models), which while being successful, suffer from both practical and theoretical shortcomings. VSM models produce very large, sparse matrices, and dimensionality reduction is usually performed, leading to high computational complexity, and obscuring the meaning of the dimensions. Similarity in VSMs is constrained to be both symmetric and transitive, contrary to evidence from human subject tests. PDD is featurebased, created automatically from corpora without producing large, sparse matrices. The dimensions along which words are compared are meaningful, enabling better understanding of the model and providing an explanation as to how any two words are similar. Similarity is neither symmetric nor transitive. The model achieved accuracy of 97.6% on a published semantic similarity test. 
We present a state-of-the-art algorithm for measuring the semantic similarity of word pairs using novel combinations of word embeddings, WordNet, and the concept dictionary 4lang. We evaluate our system on the SimLex-999 benchmark data. Our top score of 0.76 is higher than any published system that we are aware of, well beyond the average inter-annotator agreement of 0.67, and close to the 0.78 average correlation between a human rater and the average of all other ratings, suggesting that our system has achieved nearhuman performance on this benchmark. 0 Introduction We present a hybrid system for measuring the semantic similarity of word pairs. The system relies both on standard word embeddings, the WordNet database, and features derived from the 4lang concept dictionary, a set of concept graphs built from entries in monolingual dictionaries of English. 4lang-based features improve the performance of systems using only word embeddings and/or WordNet, our top conÔ¨Ågurations achieve state-of-the-art results on the SimLex-999 data, which has recently become a popular benchmark of word similarity metrics. In Section 1 we summarize earlier work on measuring word similarity and review the latest results achieved on the SimLex-999 data. Section 2 describes our experimental setup, Sections 2.1 and 2.2 documents the features obtained  using word embeddings and WordNet. In Section 3 we brieÔ¨Çy introduce the 4lang resources and the formalism it uses for encoding the meaning of words as directed graphs of concepts, then document our efforts to develop novel 4langbased similarity features. Besides improving the performance of existing systems for measuring word similarity, the goal of the present project is to examine the potential of 4lang representations in representing non-trivial lexical relationships that are beyond the scope of word embeddings and standard linguistic ontologies. Section 4 presents our results and provides rough error analysis. Section 5 offers some conclusions and plans for future work. All software presented in this paper is available for download under an MIT license at http://github.com/recski/wordsim. 
Word embeddings are widely used nowadays for many NLP tasks. They reduce the dimensionality of the vocabulary space, but most importantly they should capture (part of) the meaning of words. The new vector space used by the embeddings allows computation of semantic distances between words, while some word embeddings also permit simple vector operations (e.g. summation, difference) resembling analogical reasoning. This paper proposes a new operation on word embeddings aimed to capturing categorical information by Ô¨Årst learning and then applying an embedding mask for each analyzed category. Thus, we conducted a series of experiments related to categorization of words based on their embeddings. Several classical approaches were compared together with the one introduced in the paper which uses different embedding masks learnt for each category. 
Sparsity often leads to efÔ¨Åcient and interpretable representations for data. In this paper, we introduce an architecture to infer the appropriate sparsity pattern for the word embeddings while learning the sentence composition in a deep network. The proposed approach produces competitive results in sentiment and topic classiÔ¨Åcation tasks with high degree of sparsity. It is computationally cheaper to compute sparse word representations than existing approaches. The imposed sparsity is directly controlled by the task considered and leads to more interpretability. 
A long-term goal of machine learning is to build an intelligent dialogue agent that is capable of learning associations within data and using them to understand and answer questions and make relevant recommendations. The Facebook Movie Dialog Dataset (MDD) was recently proposed in Dodge et al. (2016) to evaluate the comparative performance of dialogue agent systems. However, a structural analysis of the data for the recommendation tasks suggests that there may be some Ô¨Çaws in the design of the dataset. 
The Neural Bag-of-Words (NBOW) model performs classiÔ¨Åcation with an average of the input word vectors and achieves an impressive performance. While the NBOW model learns word vectors targeted for the classiÔ¨Åcation task it does not explicitly model which words are important for given task. In this paper we propose an improved NBOW model with this ability to learn task speciÔ¨Åc word importance weights. The word importance weights are learned by introducing a new weighted sum composition of the word vectors. With experiments on standard topic and sentiment classiÔ¨Åcation tasks, we show that (a) our proposed model learns meaningful word importance for a given task (b) our model gives best accuracies among the BOW approaches. We also show that the learned word importance weights are comparable to tf-idf based word weights when used as features in a BOW SVM classiÔ¨Åer. 
Vector models of distributional semantics can be viewed as a geometric interpretation of a fragment of dependent type theory. By extending to a bigger fragment to include the dependent product we achieve a signiÔ¨Åcant increase in expressive power of vector models, which allows for an implementation of contextual adaptation of word meanings in the compositional setting. 
In this work, we evaluate different sentence encoders with emphasis on examining their embedding spaces. SpeciÔ¨Åcally, we hypothesize that a ‚Äúhigh-quality‚Äù embedding aids in generalization, promoting transfer learning as well as zero-shot and one-shot learning. To investigate this, we modify Skipthought vectors to learn a more generalizable space by exploiting a small amount of supervision. The aim is to introduce an additional notion of similarity in the embeddings, rendering the vectors informative for different tasks requiring less adaptation. Our embeddings capture human intuition on similarity favorably than competing models, while we also show positive indications of transfer from the task of natural language inference to paraphrase detection and paraphrase ranking. Further, our model‚Äôs behaviour on paraphrase detection when trained with an increasing amount of labelled data is indicative of a generalizable model. Finally, we support our hypothesis on generalizability of our embeddings through inspection of their statistics. 
We propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation; however, the existing domain adaptation techniques are limited to (1) tune the model parameters by the target dataset after the training by the source dataset, or (2) design the network to have dual output, one for the source domain and the other for the target domain. Reformulating the idea of the domain adaptation technique proposed by Daume¬¥ (2007), we propose a simple domain adaptation method, which can be applied to neural networks trained with a cross-entropy loss. On captioning datasets, we show performance improvements over other domain adaptation methods. 
Many important NLP problems can be posed as dual-sequence or sequence-tosequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on autoencoding reveals that these beneÔ¨Åts are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed. 
Abstract Historically, researchers have not fully capitalized on the potential synergies that exist between bibliometrics and information retrieval (IR). Knowledge of regularities in information production and use, as well as citation relationships in bibliographic databases that are studied in bibliometrics, can benefit IR system design and evaluation. Similarly, techniques developed for IR and database technology have made the investigation of large-scale bibliometric phenomena feasible. Both fields of study have also benefitted directly from developments in natural language processing (NLP), which have provided new tools and techniques to explore research problems in bibliometrics and IR. Digital libraries, with their full text, multimedia content, along with searching and browsing capabilities, represent ideal environments in which to investigate the mutually beneficial relationships that can be forged among bibliometrics, IR and NLP. This brief presentation highlights the symbiotic relationship that exists among bibliometrics, IR and NLP. Keywords: bibliometrics, information retrieval, digital libraries, natural language processing 
In this paper we consider sentences that contain Multiple Abstract. In-text References (MIR) and their position in the rhetorical structure of articles. We carry out the analysis of MIR in a large scale dataset of about 80,000 research articles published by the Public Library of Science in 7 journals. We analyze two major characteristics of MIR: their positions in the IMRaD structure of articles, and the number of in-text references that make up a MIR in the di‚Üµerent journals. We show that MIR are rather frequent in all sections of the rhetorical structure. In the Introduction section, sentences containing MIR account for more than half of the sentences with references. Multiple In-text References, Bibliometrics, Citation AnalKeywords: ysis, In-text References, Content Citation Analysis, IMRaD structure 
Abstract. In this paper we explore post retraction citations to retracted papers. The reasons for retractions in our sample were data manipulation, small sample size, scientific misconduct, and duplicate publication by the authors. We found, that the huge majority of the citations are positive, and the citing papers usually fail to mention that the cited article was retracted. Retracted articles are freely available at the publishers‚Äô site, which is probably a catalyst for receiving more citations. 
Abstract. The World Wide Web has become the hugest repository ever for scientiÔ¨Åc publications and it continues to increase at an unprecedented rate. Nevertheless, this information overload makes the exploration of this content a very time-consuming task. In this landscape, the availability of text mining tools to characterize and explore distinctive features of the scientiÔ¨Åc literature is mandatory. We present the ScientiÔ¨Åc Knowledge Miner (SKM) Project, that aims to investigate new approaches and frameworks to facilitate the extraction of knowledge from scientiÔ¨Åc publications across different disciplines. More speciÔ¨Åcally, we will focus on citation characterization, recommendation and scientiÔ¨Åc document summarization. Keywords: text mining, information extraction, recommender systems, indexing, crawling, online resources. 
Introduction There has been considerable e‚Üµort building and designing new recommendation algorithms to help scholars Ô¨Ånd relevant papers. Most of these methods depend on citations [14], full text [5] or usage data [3]. One feature that has been largely ignored are equations and the mathematical language surrounding these equations. These formal languages can tie together papers and ideas across Ô¨Åelds and time periods. Shannon‚Äôs famous entropy equation (also used in this paper) is an example of this kind of trace [7]. Unlike natural languages, formal languages such as mathematics are exempt from plagiarism rules. The norm is for an author to copy an equation from the original source. This provides a unique opportunity for tracing ideas back to their origins and for tracking them forward in time. There have been attempts at utilizing the equations as a search facet. For example, Springer‚Äôs LaTeX Search tool1 allows authors to search formulae (in LATEX format) from more than 8 million documents in Springer journals and articles. This is used both as a tool for searching similar formulae and for translating formulae to existing documents. But what if a researcher wants to Ô¨Ånd not just individual manuscripts with the same equations, but Ô¨Åelds of study and groups of papers using similar language? What kind of formalism can be used to map jargon di‚Üµerences across the quantitative sciences? 
Recommendation services, bibliometric-enhanced IR, coKeywords: word analysis, author centrality, journal productivity, relevance assessment 
Keywords: Natural Language Processing, Syntactic Analysis, Scientific Document Summarisation, Bag of Words 
Abstract. This document demonstrates our participant system PolyU on CLSciSumm 2016. There are three tasks in CL-SciSumm 2016. In Task 1A, we apply SVM Rank to identify the spans of text in the reference paper reÔ¨Çecting the citance. In Task 1B, we use the decision tree to classify the facet that a citance belongs to. Finally, in Task 2, we develop an enhanced Manifold Ranking summarization model. 
Abstract. As a way to tackle Task 1A in CL-SciSumm 2016, we introduce a composite model consisting of TFIDF and Neural Network (NN), the latter being a adaptation of the embedding model originally proposed for the Q/A domain [2, 7]. We discuss an experiment using a development data, results thereof, and some remaining issues. 
We propose a simple graph-based method for word sense disambiguation (WSD) where sense and context embeddings are constructed by applying the Skip-gram method to random walks over the sense graph. We used this method to build a WSD system for Swedish using the SALDO lexicon, and evaluated it on six different annotated test sets. In all cases, our system was several orders of magnitude faster than a state-of-the-art PageRank-based system, while outperforming a random baseline soundly. 
Language processing tools suffer from significant performance drops in social media domain due to its continuously evolving language. Transforming non-standard words into their standard forms has been studied as a step towards proper processing of ill-formed texts. This work describes a normalization system that considers contextual and lexical similarities between standard and non-standard words for removing noise in texts. A bipartite graph that represents contexts shared by words in a large unlabeled text corpus is utilized for exploring normalization candidates via random walks. Input context of a non-standard word in a given sentence is tailored in cases where a direct match to shared contexts is not possible. The performance of the system was evaluated on Turkish social media texts. 
With the advent of Big Data concept, a lot of attention has been paid to structuring and giving semantic to this data. Knowledge bases like DBPedia play an important role to achieve this goal. Question answering systems are common approach to address expressivity and usability of information extraction from knowledge bases. Recent researches focused only on monolingual QA systems while cross-lingual setting has still so many barriers. In this paper we introduce a new cross-lingual approach using a uniÔ¨Åed semantic space among languages. After keyword extraction, entity linking and answer type detection, we use cross lingual semantic similarity to extract the answer from knowledge base via relation selection and type matching. We have evaluated our approach on Persian and Spanish which are typologically different languages. Our experiments are on DBPedia. The results are promising for both languages. 
Motif analysis counts the number of small building blocks (the motifs) in a network and relates these statistical numbers to the inherent semantics of the network. In the realm of natural language processing, the networks are induced by texts. We demonstrate that motif analysis may help assess the quality of a document. More speciÔ¨Åcally, we consider the German Wikipedia and use the label ‚Äúfeatured‚Äù as the (binary) quality criterion. The length (number of words) of an article is a comparatively good predictor for this label. We show that a well-designed combination of this criterion and motif statistics yields a signiÔ¨Åcant improvement. We also found that a deeper look into the most relevant motifs may improve our understanding of quality. 
Despite the clear inter-dependency between analyzing the interactions in social networks, and analyzing the natural language content of these interactions, these aspects are typically studied independently. In this paper we present a Ô¨Årst step towards Ô¨Ånding a joint representation, by embedding the two aspects into a single vector space. We show that the new representation can help improve performance in two social relations prediction tasks. 
We present a tool for dynamic reference graph visualization. A reference graph is a graph based on key phrases retrieved from a timeindexed natural language text corpus. This tool may be useful for the analysis of connected pairs of latent topics, changes in the signiÔ¨Åcance of these topics as well as in the relationship between them over various time periods. 
We propose a general approach to modeling semi-supervised learning constraints on unlabeled data. Both traditional supervised classiÔ¨Åcation tasks and many natural semisupervised learning heuristics can be approximated by specifying the desired outcome of walks through a graph of classiÔ¨Åers. We demonstrate the modeling capability of this approach in the task of relation extraction, and experimental results show that the modeled constraints achieve better performance as expected. 
Recognition and disambiguation of named entities in text is a knowledge-intensive task. Systems are typically bound by the resources and coverage of a single target knowledge base (KB). In place of a Ô¨Åxed knowledge base, we attempt to infer a set of endpoints which reliably disambiguate entity mentions on the web. We propose a method for discovering web KBs and our preliminary results suggest that web KBs allow linking to entities that can be found on the web, but may not merit a major KB entry. 
Recent work on information extraction has suggested that fast, interactive tools can be highly effective; however, creating a usable system is challenging, and few publically available tools exist. In this paper we present IKE, a new extraction tool that performs fast, interactive bootstrapping to develop high-quality extraction patterns for targeted relations. Central to IKE is the notion that an extraction pattern can be treated as a search query over a corpus. To operationalize this, IKE uses a novel query language that is expressive, easy to understand, and fast to execute - essential requirements for a practical system. It is also the Ô¨Årst interactive extraction tool to seamlessly integrate symbolic (boolean) and distributional (similarity-based) methods for search. An initial evaluation suggests that relation tables can be populated substantially faster than by manual pattern authoring while retaining accuracy, and more reliably than fully automated tools, an important step towards practical KB construction. We are making IKE publically available (http://allenai.org/ software/interactive-knowledge-extraction). 
Relation extraction is one of the core challenges in automated knowledge base construction. One line of approach for relation extraction is to perform multi-hop reasoning on the paths connecting an entity pair to infer new relations. While these methods have been successfully applied for knowledge base completion, they do not utilize the entity or the entity type information to make predictions. In this work, we incorporate selectional preferences, i.e., relations enforce constraints on the allowed entity types for the candidate entities, to multi-hop relation extraction by including entity type information. We achieve a 17.67% (relative) improvement in MAP score in a relation extraction task when compared to a method that does not use entity type information.  Figure 1: The two paths above consist of the same relations (locatedIn ‚Üí locatedIn) and, hence, the model of Neelakantan (2015) will assign them the same score for the relation AirportServesPlace without considering the fact that Yankee Stadium is not an airport.  
A prior study found that on average there are 6.3 named mentions of organizations found in email messages from the Enron collection, only about half of which could be linked to known entities in Wikipedia (Gao et al., 2014). That suggests a need for collection-speciÔ¨Åc approaches to entity linking, similar to those have proven successful for person mentions. This paper describes a process for automatically constructing such a collection-speciÔ¨Åc knowledge base of organization entities for named mentions in Enron. A new public test collection for linking 130 mentions of organizations found in Enron email to either Wikipedia or to this new collection-speciÔ¨Åc knowledge base is also described. Together, Wikipedia entities plus the new collectionspeciÔ¨Åc knowledge base cover 83% of the 130 organization mentions, a 14% (absolute) improvement over the 69% that could be linked to Wikipedia alone.  tioned in the course of informal interactions (speciÔ¨Åcally, in Enron email) exist in such general-coverage KB‚Äôs (Gao et al., 2014). That fact resulted in renewed interest in constructing collection-speciÔ¨Åc KBs, which had been investigated a decade earlier in other contexts (Elsayed and Oard, 2006). That approach proved productive, covering about 80% of all person named mentions found in the Enron collection (Elsayed et al., 2008). Several entity linking systems (some referred to in earlier work as ‚Äúidentity resolution‚Äù) have been proposed for resolving named mentions of people in email messages to a collection-speciÔ¨Åc person KB (Minkov et al., 2006; Elsayed et al., 2008; Xu and Oard, 2012; Diehl et al., 2006). The next natural question to explore is whether similar techniques might be used to create collection-speciÔ¨Åc organization (ORG) KBs, since that same earlier study reported that only about half (53%) of ORG mentions in the Enron collection could be found in Wikipedia (Gao et al., 2014). That is our focus in this paper.  
Wikidata is a large-scale, multilingual and freely available knowledge base. It contains more than 14 million facts, however, it is still missing linguistic information. In this paper, we aim to bridge this gap by aligning Wikidata with FrameNet lexicon. We propose an approach based on word embedding to identify a mapping between Wikidata relations, called properties, and FrameNet frames and to annotate the arguments of each relation with the semantic roles of the matching frames. Early empirical results show the advantage of our approach compared to other baseline methods. 
Extracting open relational tuples that are mediated by nouns (instead of verbs) is important since titles and entity attributes are often expressed nominally. While appositives and possessives are easy to handle, a difÔ¨Åcult and important class of nominal extractions requires interpreting compound noun phrases (e.g., ‚ÄúGoogle CEO Larry Page‚Äù). We substantially improve the quality of Open IE from compound noun phrases by focusing on phenomena like demonyms and compound relational nouns. We release RELNOUN 2.2, which obtains 3.5 times yield with over 15 point improvement in precision compared to RELNOUN 1.1, a publicly available nominal Open IE system. 
Knowledge bases such as Wikidata, DBpedia, YAGO, or the Google Knowledge Vault collect a vast number of facts about the world. But while quite some facts are known about the world, little is known about how much is unknown. For example, while the knowledge base may tell us that Barack Obama is the father of Malia Obama and Sasha Obama, it does not tell us whether these are all of his children. This is not just an epistemic challenge, but also a practical problem for data producers and consumers. We envision that KBs become annotated with information about their recall on speciÔ¨Åc topics. We show what such annotations could look like, how they could be obtained, and survey related work. 
In this paper we present a proof-of-concept implementation of Neural Theorem Provers (NTPs), end-to-end differentiable counterparts of discrete theorem provers that perform Ô¨Årst-order inference on vector representations of symbols using function-free, possibly parameterized, rules. As such, NTPs follow a long tradition of neural-symbolic approaches to automated knowledge base inference, but differ in that they are differentiable with respect to representations of symbols in a knowledge base and can thus learn representations of predicates, constants, as well as rules of predeÔ¨Åned structure. Furthermore, they still allow us to incorporate domainknowledge provided as rules. The NTP presented here is realized via a differentiable version of the backward chaining algorithm. It operates on substitution representations and is able to learn complex logical dependencies from training facts of small knowledge bases. 
We propose an approach to extracting information from text based on the hypothesis that text sometimes describes the world. The hypothesis is embodied in a generative probability model that describes (1) possible worlds and the facts they might contain, (2) how an author chooses facts to express, and (3) how those facts are expressed in text. Given text, information extraction is done by computing a posterior over the worlds that might have generated it. As a by-product, this unsupervised learning process discovers new relations and their textual expressions, extracts new facts, disambiguates instances of polysemous expressions, and resolves entity references. The probability model also explains and improves on Brin‚Äôs bootstrapping heuristic, which underlies many open information extraction systems. Preliminary results on a small corpus of New York Times text suggest that the approach is effective. 
With the rise in popularity of social media, images accompanied by contextual text form a huge section of the web. However, search and retrieval of documents are still largely dependent on solely textual cues. Although visual cues have started to gain focus, the imperfection in object/scene detection do not lead to signiÔ¨Åcantly improved results. We hypothesize that the use of background commonsense knowledge on query terms can signiÔ¨Åcantly aid in retrieval of documents with associated images. To this end we deploy three different modalities - text, visual cues, and commonsense knowledge pertaining to the query - as a recipe for efÔ¨Åcient search and retrieval. 
Universal schema jointly embeds knowledge bases and textual patterns to reason about entities and relations for automatic knowledge base construction and information extraction. In the past, entity pairs and relations were represented as learned vectors with compatibility determined by a scoring function, limiting generalization to unseen text patterns and entities. Recently, ‚Äòcolumn-less‚Äô versions of Universal Schema have used compositional pattern encoders to generalize to all text patterns. In this work we take the next step and propose a ‚Äòrow-less‚Äô model of universal schema, removing explicit entity pair representations. Instead of learning vector representations for each entity pair in our training set, we treat an entity pair as a function of its relation types. In experimental results on the FB15k-237 benchmark we demonstrate that we can match the performance of a comparable model with explicit entity pair representations using a model of attention over relation types. We further demonstrate that the model performs with nearly the same accuracy on entity pairs never seen during training. 
In this work we propose a novel attentionbased neural network model for the task of Ô¨Åne-grained entity type classiÔ¨Åcation that unlike previously proposed models recursively composes representations of entity mention contexts. Our model achieves state-of-theart performance with 74.94% loose micro F1score on the well-established FIGER dataset, a relative improvement of 2.59% . We also investigate the behavior of the attention mechanism of our model and observe that it can learn contextual linguistic expressions that indicate the Ô¨Åne-grained category memberships of an entity.  Output layer Context Representation  Mention Representation  Attention Layers LSTM Layers  New York got a Ph.D. from  Word Embeddings  in Feb. 1995 .  Figure 1: An illustration of our proposed model predicting Ô¨Ånegrained semantic types for the mention ‚ÄúNew York‚Äù in the sentence ‚ÄúShe got a Ph.D from New York in Feb. 1995.‚Äù.  
Methods for automated knowledge base construction often rely on trained Ô¨Åxed-length vector representations of relations and entities to predict facts. Recent work showed that such representations can be regularized to inject Ô¨Årst-order logic formulae. This enables to incorporate domain-knowledge for improved prediction of facts, especially for uncommon relations. However, current approaches rely on propositionalization of formulae and thus do not scale to large sets of formulae or knowledge bases with many facts. Here we propose a method that imposes Ô¨Årst-order constraints directly on relation representations, avoiding costly grounding of formulae. We show that our approach works well for implications between pairs of relations on artiÔ¨Åcial datasets. 
Manually created large scale ontologies are useful for organizing, searching, and repurposing content ranging from scientiÔ¨Åc papers and medical guidelines to images. However, maintenance of such ontologies is expensive. In this paper, we investigate the use of universal schemas (Riedel et al., 2013) as a mechanism for ontology maintenance. We apply this approach on top of two unique data sources: 14 million full-text scientiÔ¨Åc articles and chapters, plus a 1 million concept handcurated medical ontology. We show that using a straightforward matrix factorization algorithm one can achieve 0.7 F1 measure on a link prediction task in this environment. Link prediction results can be used to suggest new relation types and relation type synonyms coming from the literature as well as predict speciÔ¨Åc new relation instances in the ontology. 
We present a design for acquiring word association knowledge of high quality on the basis of a game with a purpose (GWAP). We evaluate automatically acquired word associations using a word association game as a GWAP. In the word association game, a player is given a set of associated words as a hint and is asked to answer a word that can be associated with the hint. If many players can answer the correct keyword, we judge the set of associated words to be of high quality. This word association game was implemented in a smartphonebased dialog system, which has been installed into more than one million smartphones. Our analysis of numerous game logs demonstrated that our framework can effectively select word associations of high quality. 
This paper is an attempt to raise pertinent questions and act as platform to generate fruitful discussions within the AKBC community about the need for a large scale dataset for relation extraction. For proper training and evaluation of relation extraction tasks, the weaknesses of datasets used so far need to be tackled: mainly the size (too small) and the amount of data that is actually labelled (unlabelled data leading to recall problems). We have the vision of building a new large and fully labelled dataset for entity pairs connected via binary relations from both Freebase as well as other datasets, such as Clueweb. Concerning the process of building, we present pioneering work on a roadmap which will serve as the foundation for the intended discussion within the community. Points to discuss arise within the following steps: Ô¨Årst, the source data has to be preprocessed in order to ensure that the set of relations consists of valid relations only; second, we suggest a method to Ô¨Ånd the most relevant relations for an entity pair; and third, we outline approaches on how to actually label the data. It is necessary to discuss several key issues in the process of generating this dataset. This will enable us to thoroughly create a dataset that will have the potential to serve as a standard to the community. 
We present a comparison of weak and distant supervision methods for producing proxy examples for supervised relation extraction. We Ô¨Ånd that knowledge-based weak supervision tends to outperform popular distance supervision techniques, providing a higher yield of positive examples and more accurate models. 
Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. Facts can however also be represented using pairwise embeddings, i.e. embeddings for pairs of entities and relations. In this paper we explore such bigram embeddings with a Ô¨Çexible Factorization Machine model and several ablations from it. We investigate the relevance of various bigram types on the fb15k237 dataset and Ô¨Ånd relative improvements compared to a compositional model. 
We present a matrix factorization model for learning cross-lingual representations. Using sentence-aligned corpora, the proposed model learns distributed representations by factoring the given data into language-dependent factors and one shared factor. Moreover, the model can quickly learn shared representations for more than two languages without undermining the quality of the monolingual components. The model achieves an accuracy of 88% on English to German cross-lingual document classiÔ¨Åcation, and 0.8 Pearson correlation on Spanish-English cross-lingual semantic textual similarity. While the results do not beat state-of-the-art performance in these tasks, we show that the crosslingual models are at least as good as their monolingual counterparts. 
Treebanks have recently been released for a number of languages with the harmonized annotation created by the Universal Dependencies project. The representation of certain constructions in UD are known to be suboptimal for parsing and may be worth transforming for the purpose of parsing. In this paper, we focus on the representation of verb groups. Several studies have shown that parsing works better when auxiliaries are the head of auxiliary dependency relations which is not the case in UD. We therefore transformed verb groups in UD treebanks, parsed the test set and transformed it back, and contrary to expectations, observed signiÔ¨Åcant decreases in accuracy. We provide suggestive evidence that improvements in previous studies were obtained because the transformation helps disambiguating POS tags of main verbs and auxiliaries. The question of why parsing accuracy decreases with this approach in the case of UD is left open.  than their alternatives for parsing, for example in Schwartz et al. (2012). The UD guidelines however have been written with the intent to maximize crosslinguistic parallelism and this constraint has forced the guidelines developers to sometimes choose representations that are known to be worse for parsing (de Marneffe et al., 2014). For that reason, de Marneffe et al. (2014) suggest that those representations could be modiÔ¨Åed for the purpose of parsing, thus creating a parsing representation. Transforming tree representations for the purpose of parsing is not a new idea. It has been done for constituency parsing for example by Collins (1999) but also for dependency parsing for example by Nilsson et al. (2007). Nilsson et al. (2007) modiÔ¨Åed the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy. In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by Nilsson et al. (2007) on UD treebanks to Ô¨Ånd out whether or not the alternative representation is useful for parsing with UD.  
In this paper, we propose to analyze the preand post-processing steps applied in the context of cross-lingual dependency transfer. To this aim, we employ a simple transfer strategy that operates on partially annotated projected data. We show that a good data selection strategy is a key point in successfully transferring dependencies and that better data selection techniques need to be developed in order to achieve the performance of fully supervised methods.  The second approach (transfer of annotations), relies on parallel corpora to project, through alignment links, the dependencies automatically predicted from a resource-rich language to a resourcepoor language. This approach pioneered by Hwa et al. (2005) requires various heuristic transformation rules to cope with the non-isomorphism between the source and target structures as well as with the noise in source annotations and in alignments. It has since enjoyed a great popularity and been improved by many works (see the overview in Section 2).  
Researchers have shown that a wordnet for a new language, possibly resource-poor, can be constructed automatically by translating wordnets of resource-rich languages. The quality of these constructed wordnets is affected by the quality of the resources used such as dictionaries and translation methods in the construction process. Recent work shows that vector representation of words (word embeddings) can be used to discover related words in text. In this paper, we propose a method that performs such similarity computation using word embeddings to improve the quality of automatically constructed wordnets. 
In this paper, we challenge a basic assumption of many cross-lingual transfer techniques: the availability of word aligned parallel corpora, and consider ways to accommodate situations in which such resources do not exist. We show experimentally that, here again, weakly supervised cross-lingual learning techniques can prove useful, once adapted to transfer knowledge across pairs of languages. 
This paper presents a data-driven, simple cluster-and-label approach using optimized count-based methods for word-level language identiÔ¨Åcation for a large domain-speciÔ¨Åc multilingual diachronic corpus of periodicals published at least yearly between 1864 and 2014 in Switzerland. Our system requires no annotated data or training, only minimal human effort in evaluating and labeling 50 clusters for a corpus of almost 40 million tokens. Despite being unsupervised, our results show an accuracy that is comparable to the corpus annotations which result from an existing code switching algorithm and the combined usage of two supervised systems using character and byte n-gram models (Volk and Clematide, 2014). 
We tackle the challenge of learning part-ofspeech classified translations as part of an inversion transduction grammar, by learning translations for English words with known part-of-speech tags, both from existing translation lexica and from parallel corpora. When translating from a low resource language into English, we can expect to have rich resources for English, such as treebanks, and small amounts of bilingual resources, such as translation lexica and parallel corpora. We solve the problem of integrating these heterogeneous resources into a single model using stochastic Inversion Transduction Grammars, which we augment with wildcards to handle unknown translations. 
We introduce a new measure of distance between languages based on word embedding, called word embedding language divergence (WELD). WELD is deÔ¨Åned as divergence between uniÔ¨Åed similarity distribution of words between languages. Using such a measure, we perform language comparison for Ô¨Åfty natural languages and twelve genetic languages. Our natural language dataset is a collection of sentence-aligned parallel corpora from bible translations for Ô¨Åfty languages spanning a variety of language families. Although we use parallel corpora, which guarantees having the same content in all languages, interestingly in many cases languages within the same family cluster together. In addition to natural languages, we perform language comparison for the coding regions in the genomes of 12 different organisms (4 plants, 6 animals, and two human subjects). Our result conÔ¨Årms a signiÔ¨Åcant high-level difference in the genetic language model of humans/animals versus plants. The proposed method is a step toward deÔ¨Åning a quantitative measure of similarity between languages, with applications in languages classiÔ¨Åcation, genre identiÔ¨Åcation, dialect identiÔ¨Åcation, and evaluation of translations. 
Some metaphorical mappings between source and target are obvious and appear in collocation patterns in natural language data. However, other metaphors that structure abstract processes or complex topics are trickier to investigate because the target domain is lexically divorced from the source. Using metaphors for the economy as a case study, this paper introduces new techniques to find metaphorical tokens when target and source relationships are nonobvious. Through novel methods, constellations of source-domain triggers are identified in the data and evaluated for metaphoricity and keyness and then grouped according to trigger potency. 
Recent work on metaphor processing often employs selectional preference information. We present a comparison of different approaches to the modelling of selectional preferences, based on various ways of generalizing over corpus frequencies. We evaluate on the VU Amsterdam Metaphor corpus, a broad corpus of metaphor. We Ô¨Ånd that using only selectional preference information is enough to outperform an all-metaphor baseline classiÔ¨Åcation, but that generalization through prediction or clustering is not beneÔ¨Åcial. A possible explanation for this lies in the nature of the evaluation data, and lack of power of selectional preference information on its own for non-novel metaphor detection. To better investigate the role of metaphor type in metaphor detection, we suggest a resource with annotation of novel metaphor should be created. 
In this paper, we propose a novel approach for supervised classification of linguistic metaphors in an open domain text using Conditional Random Fields (CRF). We analyze CRF based classification model for metaphor detection using syntactic, conceptual, affective, and word embeddings based features which are extracted from MRC Psycholinguistic Database (MRCPD) and WordNet-Affect. We use word embeddings given by Huang et al. to capture information such as coherence and analogy between words. To tackle the bottleneck of limited coverage of psychological features in MRCPD, we employ synonymy relations from WordNet¬Æ. A comparison of our approach with previous approaches shows the efficacy of CRF classifier in detecting metaphors. The experiments conducted on VU Amsterdam metaphor corpus provides an accuracy of more than 92% and Fmeasure of approximately 78%. Results shows that inclusion of conceptual features improves the recall by 5% whereas affective features do not have any major impact on metaphor detection in open text. 
Automatic metaphor detection usually relies on various features, incorporating e.g. selectional preference violations or concreteness ratings to detect metaphors in text. These features rely on background corpora, hand-coded rules or additional, manually created resources, all speciÔ¨Åc to the language the system is being used on. We present a novel approach to metaphor detection using a neural network in combination with word embeddings, a method that has already proven to yield promising results for other natural language processing tasks. We show that foregoing manual feature engineering by solely relying on word embeddings trained on large corpora produces comparable results to other systems, while removing the need for additional resources. 
The elasticity of metaphor as a communication strategy has spurred philosophers to question its ability to mean anything at all. If a metaphor can elicit different responses from different people in varying contexts, how can one say it has a single meaning? Davidson has argued that metaphors have no special or secondary meaning, and must thus mean exactly what they seem to mean on the surface. It is this literally anomalous meaning that directs us to the pragmatic inferences that a speaker actually wishes us to explore. Conveniently, this laissez faire strategy assumes that metaphors are crafted from apt knowledge by speakers with real communicative intent, allowing useful inference to be extracted from their words. But this assumption is not valid in the case of many machine-generated metaphors that merely echo the linguistic form ‚Äì but lack the actual substance ‚Äì of real metaphors. We present here an open public resource with which a metaphor-generation system can give its figurative efforts real meaning. 
Most events described in a news article are background events ‚Äì only a small number are noteworthy, and a even smaller number serve as the trigger for writing of that article. Although these events are difÔ¨Åcult to identify, they are crucial to NLP tasks such as Ô¨Årst story detection, document summarization and event coreference, and to many applications of event analysis that depend on event counting and identifying trends. In this work, we introduce the notion of news-peg, a concept borrowed from the political science literature, in an attempt to remedy this problem. A news-peg is an event which prompted the author to write the article, and it serves as a more Ô¨Åne-grained measure of noteworthiness than a summary. We describe a new task of news-peg identiÔ¨Åcation and release an annotated dataset for its evaluation. We formalize an operational definition of a news-peg, on which human annotators achieve high inter-annotator agreement (over 80%), and present a rule-based system for this task, which exploits syntactic features derived from established journalistic devices. 
Causal and temporal relations among events are typically analyzed in terms of interclausal relations. Yet participants in a monoclausal event interact causally as well, and monoclausal events unfold in temporal phases. We propose an annotation scheme for the resulting analysis of event structure types. The annotation scheme is based on a Ô¨Åne-grained analysis of aspectual structure combined with a novel analysis of physical event types based on proposals in the theoretical linguistics literature. By decomposing complex events in a clause, we will ultimately model the overall dynamic causal network of entities interacting over time described in a text. 
We describe the ongoing development of a lexically-informed, upper-level event ontology and explore use cases of the ontology. This ontology draws its lexical sense distinctions from VerbNet, FrameNet and the Rich Entities, Relations and Events Project. As a result, the ontology facilitates interoperability and the combination of annotations done for each independent resource. While this ontology is intended to be practical for a variety of applications, here we take the initial steps in determining whether or not the event ontology could be utilized in multimodal applications, specifically to recognize and reason about events in both text and video. We find that the ontology facilitates the generalization of potentially noisy or sparse individual realizations of events into larger categories of events and enables reasoning about event relations and participants, both of which are useful in event recognition and interpretation regardless of modality. 
This paper will discuss and compare event representations across a variety of types of event annotation: Rich Entities, Relations, and Events (Rich ERE), Light Entities, Relations, and Events (Light ERE), Event Nugget (EN), Event Argument Extraction (EAE), Richer Event Descriptions (RED), and Event-Event Relations (EER). Comparisons of event representations are presented, along with a comparison of data annotated according to each event representation. An event annotation experiment is also discussed, including annotation for all of these representations on the same set of sample data, with the purpose of being able to compare actual annotation across all of these approaches as directly as possible. We walk through a brief example to illustrate the various annotation approaches, and to show the intersections among the various annotated data sets.  
In this paper, we describe the event nugget annotation created in support of the pilot Event Nugget Detection evaluation in 2014 and in support of the Event Nugget Detection and Coreference open evaluation in 2015, which was one of the Knowledge Base Population tracks within the NIST Text Analysis Conference. We present the data volume annotated for both training and evaluation data for the 2015 evaluation as well as changes to annotation in 2015 as compared to that of 2014. We also analyze the annotation for the 2015 evaluation as an example to show the annotation challenges and consistency, and identify the event types and subtypes that are most difficult for human annotators. Finally, we discuss annotation issues that we need to take into consideration in the future. 
Common sense knowledge plays an essential role for natural language understanding, human-machine communication and so forth. In this paper, we acquire knowledge of events as common sense knowledge because there is a possibility that dictionaries of such knowledge are useful for recognition of implication relations in texts, inference of human activities and their planning, and so on. As for event knowledge, we focus on feature changes of arguments (hereafter, FCAs) in event sentences as knowledge of events. To construct a dictionary of FCAs, we propose a framework for acquiring such knowledge based on both of the automatic approach and the collective intelligence approach to exploit merits of both approaches. We acquired FCAs in event sentences through crowdsourcing and conducted the subjective evaluation to validate whether the FCAs are adequately acquired. As a result of the evaluation, it was shown that we were able to reasonably well capture FCAs in event sentences. 
This paper presents an overview of an LFG treatment of discontinuous nominal expressions involving modiÔ¨Åcation, making the claim that cross-linguistically different types of discontinuity (i.e. in Warlpiri and English) should be captured by the same overall analysis, despite being licensed in different ways. LFG‚Äôs separation of grammatical functions from phrase structural positions intuitively accounts for discontinuous expressions, and its use of glue semantics ensures that discontinuous and contiguous expressions receive the same semantic analysis.  (1) Kurdu-ngku ka wajilipi-nyi child-ERG PRES chase-NONPAST wita-ngku small-ERG ‚ÄòThe small child is chasing it.‚Äô In (1) a head noun is separated from a modiÔ¨Åer, but both parts map to the same grammatical function (subject). The two parts of the discontinuous expression share the same case-marking. A similar type of discontinuity involving modiÔ¨Åcation is attested in English, in the cases of relative clause extraposition in (2a) and NP-PP split in (2b) (Kirkwood, 1977, p. 55):2  
We describe results of investigation of a speciÔ¨Åc type of discontinuous constructions, namely non-projective constructions concerning verbs and their arguments. This topic is especially important for languages with a relatively free word order, such as Czech, which is the language we have primarily worked with. For comparison, we have included some results for English. The corpora used for both languages are the Prague Czech-English Dependency Treebank and the Prague Dependency Treebank, which are both annotated at a dependency syntax level as well as a deep (semantic) level, including verbs and their valency (arguments). We are using traditionally deÔ¨Åned non-projectivity on trees with full linear ordering, but the two levels of annotation are innovatively combined to determine if a particular (deep) verb -argument structure is non-projective. As a result, we have identiÔ¨Åed several types of discontinuities, which we classify either by the verb class or structurally in terms of the verb, its arguments and their dependents. In addition, we have quantitatively compared selected phenomena found in Czech translated texts (in the PCEDT) to the native Czech as found in the original Prague Dependency Treebank. 
Non-adjacent linguistic phenomena such as non-contiguous multiwords and other phrasal units containing insertions, i.e., words that are not part of the unit, are difÔ¨Åcult to process and remain a problem for NLP applications. Non-contiguous multiword units are common across languages and constitute some of the most important challenges to high quality machine translation. This paper presents an empirical analysis of non-contiguous multiwords, and highlights our use of the Logos Model and the Semtab function to deploy semantic knowledge to align non-contiguous multiword units with the goal to translate these units with high Ô¨Ådelity. The phrase level manual alignments illustrated in the paper were produced with the CLUE-Aligner, a CrossLanguage Unit Elicitation alignment tool. 
This paper presents Bulgarian discontinuous constituents.1 Bulgarian is claimed to be a language of relatively free word order. As a typical manifestation of free word order discontinuous constituents in Bulgarian have not been studied so far. The paper discusses and analyzes the freedom in Bulgarian word order and points out the way discontinuity has been treated within BulTreeBank. We show the results of our linguistic analysis of discontinuous VPs and summarize the extent of word order freedom and word order constraints within VP. 
This paper discusses genitive phrases in Hindi/Urdu in general and puts a particular focus on genitive scrambling, a process whereby the basic order of constituents is changed. In Hindi/Urdu, genitive phrases may not only occur at different structural positions within the NP that they modify; under the right circumstances, they can also be found outside of the NP, yielding discontinuous structures. The theoretical challenge is to identify and formalize the linguistic constraints that govern genitive scrambling. Further, a successful computational treatment correctly attaches the genitive phrase to its head NP. I use a Lexical-Functional Grammar to solve both challenges, demonstrating that the constraints can be aptly formulated using a functional uncertainty path. Successful attachment further depends on the morphological agreement of the genitive phrase with its head. On a theoretical level, the present contribution sheds light on the possibilities of NP discontinuities in a morphologically rich language like Hindi/Urdu. 
We introduce a new method for incremental shift-reduce parsing of discontinuous constituency trees, based on the fact that discontinuous trees can be transformed into continuous trees by changing the order of the terminal nodes. It allows for a clean formulation of different oracles, leads to faster parsers and provides better results. Our best system achieves an F1 of 80.02 on TIGER. 
In this paper, we use insights from Minimalist Grammars (Keenan and Stabler, 2003) to argue for a context-free approximation of discontinuous structures that is both easy to parse for state-of-the-art dynamic programming constituent parsers and has a simple and effective method for the reconstruction of discontinuous tree structures. The results achieved on the Tiger treebank ‚Äì paired with state-of-the-art constituent parsers such as the BLLIP and Berkeley parsers ‚Äì both improve on existing transformationbased approaches for representing discontinuous structures and the state-of-the-art results of Ferna¬¥ndez-Gonza¬¥lez and Martins‚Äô (2015) parsing-as-reduction approach. 
Social networks are dynamically changing over time e.g., some accounts are being created and some are being deleted or become private. This ephemerality at both an account level and content level results from a combination of privacy concerns, spam, and deceptive behaviors. In this study we analyze a large dataset of 180,340 accounts active during the Russian-Ukrainian crisis to discover a series of predictive features for the removal or shutdown of a suspicious account. We Ô¨Ånd that unlike previously reported proÔ¨Åle and network features, lexical features form the basis for highly accurate prediction of the deletion of an account. 
Satire is an attractive subject in deception detection research: it is a type of deception that intentionally incorporates cues revealing its own deceptiveness. Whereas other types of fabrications aim to instill a false sense of truth in the reader, a successful satirical hoax must eventually be exposed as a jest. This paper provides a conceptual overview of satire and humor, elaborating and illustrating the unique features of satirical news, which mimics the format and style of journalistic reporting. Satirical news stories were carefully matched and examined in contrast with their legitimate news counterparts in 12 contemporary news topics in 4 domains (civics, science, business, and ‚Äúsoft‚Äù news). Building on previous work in satire detection, we proposed an SVMbased algorithm, enriched with 5 predictive features (Absurdity, Humor, Grammar, Negative Affect, and Punctuation) and tested their combinations on 360 news articles. Our best predicting feature combination (Absurdity, Grammar and Punctuation) detects satirical news with a 90% precision and 84% recall (F-score=87%). Our work in algorithmically identifying satirical news pieces can aid in minimizing the potential deceptive impact of satire. 1. Introduction In the course of news production, dissemination, and consumption, there are ample opportunities to deceive and be deceived. Direct falsifications such as journalistic fraud or social media hoaxes pose obvious predicaments. While fake or satirical news may be less malicious, they may still mislead inattentive readers. Taken at face value, satirical news can intentionally create a false belief in the readers‚Äô minds, per classical definitions of deception 7  (Buller & Burgoon, 1996; Zhou, Burgoon, Nunamaker, & Twitchell, 2004). The falsehoods are intentionally poorly concealed, and beg to be unveiled. Yet some readers simply miss the joke, and the fake news is further propagated, with often costly consequences (Rubin, Chen, & Conroy, 2015). 1.1. The News Context In recent years, there has been a trend towards decreasing confidence in the mainstream media. According to Gallup polls, only 40% of Americans trust their mass media sources to report the news ‚Äúfully, accurately and fairly‚Äù (Riffkin, 2015) and a similar survey in the UK has shown that the mostread newspapers were also the least-trusted (Reilly & Nye, 2012). One effect of this trend has been to drive news readers to rely more heavily on alternative information sources, including blogs and social media, as a means to escape the perceived bias and unreliability of mainstream news (Tsfati, 2010). Ironically, this may leave the readers even more susceptible to incomplete, false, or misleading information (Mocanu, Rossi, Zhang, Karsai, & Quattrociocchi, 2015). In general, humans are fairly ineffective at recognizing deception (DePaulo, Charlton, Cooper, Lindsay, & Muhlenbruck, 1997; Rubin & Conroy, 2012; Vrij, Mann, & Leal, 2012). A number of factors may explain why. First, most people show an inherent truth-bias (Van Swol, 2014): they tend to assume that the information they receive is true and reliable. Second, some people seem to exhibit a ‚Äúgeneral gullibility‚Äù (Pennycook, Cheyne, Barr, Koehler, & Fugelsang, 2015) and are inordinately receptive to ideas that they do not fully understand. Third, confirmation bias can cause people to simply see only what they want to see ‚Äì conservative  Proceedings of NAACL-HLT 2016, pages 7‚Äì17, San Diego, California, June 12-17, 2016. c 2016 Association for Computational Linguistics  viewers of the news satire program the Colbert Report, for example, tend to believe that the comedian‚Äôs statements are sincere, while liberal viewers tend to recognize the satirical elements (LaMarre, Landreville, & Beam, 2009). 1.2. Problem Statement High rates of media consumption and low trust in news institutions create an optimal environment for the ‚Äúrapid viral spread of information that is either intentionally or unintentionally misleading or provocative‚Äù (Howell, 2013). Journalists and other content producers are incentivized towards speed and spectacle over accuracy (Chen, Conroy, & Rubin, 2015) and content consumers often lack the literacy skills required to interpret news critically (Hango, 2014). What is needed for both content producers and consumers is an automated assistive tool that can save time and cognitive effort by flagging/filtering inaccurate or false information. In developing such a tool, we have chosen news satire as a starting point for the investigation of deliberate deception in news. Unlike subtler forms of deception, satire may feature more obvious cues that reveal its disassociation from truth because the objective of satire is for at least some subset of readers to recognize the joke (Pfaff & Gibbs, 1997). And yet, articles from The Onion and other satirical news sources are often shared and even reprinted in newspapers as if the stories were true1. In other words, satirical news may mislead readers who are unaware of the satirical nature of news, or lacking in the contextual or cultural background to interpret the fake as such. In this paper, we examine the textual features of satire and test for the most reliable cues in differentiating satirical news from legitimate news. 2. Literature Review 2.1. Satire As a concept, satire has been remarkably hard to pin down in the scholarly literature (Condren, 2012). One framework for humor, proposed by Ziv (1988), suggests five discrete categories of humor: aggressive, sexual, social, defensive, and intellectual. Satire, according to Simpson (2003), is com- 
The Verifiability Approach (VA) is a promising new approach for deception detection. It extends existing verbal credibility assessment tools by asking interviewees to provide statements rich in verifiable detail. Details that i) have been experienced with an identifiable person, ii) have been witnessed by an identifiable person, or iii) have been recorded through technology, are labelled as verifiable. With only minimal modifications of information-gathering interviews this approach has yielded remarkable classification accuracies. Currently, the VA relies on extensive manual annotation by human coders. Aiming to extend the VA‚Äôs applicability, we present a work in progress on automated VA scoring. We provide a conceptual outline of two automation approaches: one being based on the Linguistic Inquiry and Word Count software and the other on rule-based shallow parsing and named entity recognition. Differences between both approaches and possible future steps for an automated VA are discussed. 
In this paper we present an initial experiment in the estimation of the amenability of new domains to true/false classification. We choose four domains, two of which have been classified for deception, and use the out-ofrank distance measure on n-grams to aid in deciding whether the third and fourth domains are amenable to T/F classification. We then use a classifier covered in the literature to train on the verified domains and test on the new domains to determine whether the relative distance measure can be a predictor of classification accuracy. 
Detecting deception in natural language is a problem amenable to economic analysis. Economics typically assumes that individuals are self-interested, which leads them to perform actions in accord with their own goals. The Ô¨Åeld of experimental economics emerged to construct environments wherein human subjects make decisions so as to test economic hypotheses. Experimental economists recently have developed virtual worlds to better situate experiment subjects in more realistic environments. Virtual word experiments represent an exciting new area for deception research as they offer insight into individuals both acting out and communicating in accord with their intentions. This paper describes the use of virtual world experiments for economic research incorporating the detection of deceptive individuals. 
When automatically detecting deception, it is important to model individual differences across speakers. We explore the automatic identiÔ¨Åcation of individual traits such as gender, native language, and personality, using acoustic-prosodic and lexical features from an initial non-deceptive dialogue. We also explore predicting success at deception and at deception detection, using the same features. 
While text-based deception in computer mediated communication has been studied, e.g., Zhou (2005) and Duran et al. (2010), there has been less focus on the differentiation of strategies for deception, especially those which may manifest in modern communication, such as found in social media. In this paper, we extend our previous work on the evaluation of linguistic indicators to strategic deception (Appling et al., 2015), by evaluating the relationship between personality and deceptive strategy use and the utilization of linguistic features for inferring both (personality and deception). We Ô¨Ånd that even with a relatively small corpus, there is evidence that personality is related to particular deception strategies, though in short social media communications, these personality traits are difÔ¨Åcult to infer using standard linguistic measures (e.g., LIWC). We also describe the corpus we collected from an experiment in which subjects engaged in deception through a social media platform. 
This paper focuses on identity-of-sense anaphoric relations, in which the sense is shared but not the referent. We are not restricted to the pronoun one, the focus of the small body of previous NLP work on this phenomenon, but look at a wider range of pronouns (that, some, another, etc.). We develop annotation guidelines, enrich a third of English OntoNotes with sense anaphora annotations, and shed light onto this phenomenon from a corpus-based perspective. We release the annotated data as part of the SAnaNotes corpus. We also use this corpus to develop a learning-based classiÔ¨Åer to identify sense anaphoric uses, showing both the power and limitations of local features. 
In this paper, we introduce a typology of bridging relations applicable to multiple languages and genres. After discussing our annotation guidelines, we describe annotation experiments on the German part of our parallel coreference corpus and show that our interannotator agreement results are reliable, considering both antecedent selection and relation assignment. In order to validate our theoretical model on other languages, we manually transfer German annotations to the English and Russian sides of the corpus and brieÔ¨Çy discuss Ô¨Årst results that suggest the promise of our approach. Furthermore, for the complete exploration of extended coreference relations, we exploit an existing near-identity scheme to augment our annotations with near-identity links, and we report on the results. 
The paper attempts at presenting initial veriÔ¨Åcation of existing approaches to annotation of bridging relations by proposing a compiled model based on schemata used in previous annotation projects and testing its validity on a corpus of Polish. The categorization features structural relations, dissimilation, analogy, reference to label, class, entailment and attribution. Multiple categories can be assigned to model situations where several aspects of the relation play a part. The relations are organized hierarchically which allows varied granularity of processing depending on computational needs. The classiÔ¨Åcation is confronted with existing annotation of other-than-identity relations in a portion of Polish Coreference Corpus. Results of manual annotation involving two annotators and adjudicator are presented. Findings from the process are intended to facilitate development of annotation guidelines of a new referencerelated project. 
This paper focuses on the interaction of chains of coreference identity with other types of relations, comparing English and German data sets in terms of language, mode (written vs. spoken) and register. We Ô¨Årst describe the types of coreference and the chain features analysed as indicators of textual coherence and topic continuity. After sketching the feature categories under analysis and the methods used for statistical evaluation, we present the Ô¨Åndings from our analysis and interpret them in terms of the contrasts mentioned above. We will also show that for some registers, coreference types other than identity are of great importance. 
Verb Phrase Ellipsis is a well-studied topic in theoretical linguistics but has received little attention as a computational problem. Here we propose a decomposition of the overall resolution problem into three tasks‚Äîtarget detection, antecedent head resolution, and antecedent boundary detection‚Äîand implement a number of computational approaches for each one. We also explore the relationships among these tasks by attempting joint learning over different combinations. Our new decomposition of the problem yields signiÔ¨Åcantly improved performance on publicly available datasets, including a newly contributed one. 
Anaphoric connectives are event anaphors (or abstract anaphors) that in addition convey a coherence relation holding between the antecedent and the host clause of the connective. Some of them carry an explicitly-anaphoric morpheme, others do not. We analysed the set of German connectives for this property and found that many have an additional nonconnective reading, where they serve as nominal anaphors. Furthermore, many connectives can have multiple senses, so altogether the processing of these words can involve substantial disambiguation. We study the problem for one speciÔ¨Åc German word, demzufolge, which can be taken as representative for a large group of similar words. 
This paper aims at a cross-lingual analysis of coreference to abstract entities in Czech and German, two languages that are typologically not very close, since they belong to two different language groups ‚Äì Slavic and Germanic. We will speciÔ¨Åcally focus on coreference chains to abstract entities, i.e. verbal phrases, clauses, sentences or even longer text passages. To our knowledge, this type of relation is underinvestigated in the current stateof-the-art literature. 
We consider several antecedent prediction models that use no pipelined features generated by upstream systems. Models trained in this way are interesting because they allow for side-stepping the intricacies of upstream models, and because we might expect them to generalize better to situations in which upstream features are unavailable or unreliable. Through quantitative and qualitative error analysis we identify what sorts of cases are particularly difÔ¨Åcult for such models, and suggest some directions for further improvement. 
In this paper, we present a syntactic approach to the annotation of bridging relations, socalled genitive bridging. We introduce the RuGenBridge corpus for Russian annotated with genitive bridging and compare it to the semantic approach that was applied in the Prague Dependency Treebank for Czech. We discuss some special aspects of bridging resolution for Russian and speciÔ¨Åcs of bridging annotation for languages where deÔ¨Ånite nominal groups are not as frequent as e.g. in Romance and Germanic languages. To verify the consistency of our method, we carry out two comparative experiments: the annotation of a small portion of our corpus with bridging relations according to both approaches and Ô¨Ånding for all relations from the RuGenBridge their semantic interpretation that would be annotated for Czech. 
In this paper we present our work on Coreference Resolution in Basque, a unique language which poses interesting challenges for the problem of coreference. We explain how we extend the coreference resolution toolkit, BART, in order to enable it to process Basque. Then we run four different experiments showing both a signiÔ¨Åcant improvement by extending a baseline feature set and the effect of calculating performance of hand-parsed mentions vs. automatically parsed mentions. Finally, we discuss some key characteristics of Basque which make it particularly challenging for coreference and draw a road map for future work. 
Resolution of the anaphoric entities in natural language text is very much essential to extract the complete information from the text. In this paper, we present a methodology to resolve one of the difficult pronouns, plural pronouns with split antecedents in Tamil. We have used a salience measure based approach with salience factors obtained from sub-categorization information of nouns and selectional restriction rules of the verbs. We have evaluated our approach with Tamil novel corpus and the results are encouraging. 
This paper approaches the challenge of adapting coreference resolution to different coreference phenomena and mention-border definitions when there is no access to large training data in the desired target scheme. We take a configurable, rule-based approach centered on dependency syntax input, which we test by examining coreference types not covered in benchmark corpora such as OntoNotes. These include cataphora, compound modifier coreference, generic anaphors, predicate markables, i-within-i, and metonymy. We test our system, called xrenner, using different configurations on two very different datasets: Wall Street Journal material from OntoNotes and four types Wiki data from the GUM corpus. Our system compares favorably with two leading rule based and stochastic approaches in handling the different annotation formats. 
This paper describes a deterministic method for generating natural language suited to being part of a machine translation system with meaning representations as the level for language transfer. Starting from Davidsonian/Penman meaning representations, syntactic trees are built following the Penn Parsed Corpus of Modern British English, from which the yield (i.e., the words) can be taken. The novel contribution is to highlight exploiting the presentation of meaning content to inform decisions regarding the selection of language constructions: active vs. passive, argument subject vs. expletive it vs. existential there, discourse vs. intra-sentential coordination vs. adverbial clause vs. participial clause vs. purpose clause, and inÔ¨Ånitive clause vs. Ô¨Ånite clause vs. small clause vs. relative clause vs. it cleft. 
In this paper, we propose a graph-based translation model which takes advantage of discontinuous phrases. The model segments a graph which combines bigram and dependency relations into subgraphs and produces translations by combining translations of these subgraphs. Experiments on Chinese‚ÄìEnglish and German‚ÄìEnglish tasks show that our system is signiÔ¨Åcantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement.  held Juxing  FIFA World Cup in successfully FIFA Shijiebei Zai Chenggong  2010 2010Nian  South Africa Nanfei  Figure 1: An example of constructing a graph for a Chinese sentence. Each node includes a Chinese word and its English meaning. Dashed red lines are bigram relations. Dark lines are dependency relations. Dotted blue lines are shared by bigram and dependency relations.  
This research puts forward two concepts, the Naming Sharing Structure (NSS) and the Naming-Telling clause (NT clause). By transforming linear text into a plane structure, we discovered five construction patterns of NT clause for the NSS. The constructions of NT clauses are order-preserving, nontransgressive, and syntactically well-formed. These features of NT clause can be explained from the point of view of cognition. 
The paper focuses on the creation of a semantic-based hybrid Machine Translation system between Bulgarian and English in the domain of Information Technology. The preprocessing strategies are presented. A method for the substitution of English word forms with the synsets or Bulgarian representative lemmas is discussed. Finally, the creation of a factored model in the Moses system is described. 
In this work, we estimate the deterioration of NLP processing given an estimate of the amount and nature of grammatical errors in a text. From a corpus of essays written by English-language learners, we extract ungrammatical sentences, controlling the number and types of errors in each sentence. We focus on six categories of errors that are commonly made by English-language learners, and consider sentences containing one or more of these errors. To evaluate the effect of grammatical errors, we measure the deterioration of ungrammatical dependency parses using the labeled F-score, an adaptation of the labeled attachment score. We Ô¨Ånd notable differences between the inÔ¨Çuence of individual error types on the dependency parse, as well as interactions between multiple errors. 
This paper addresses the task of readability assessment for the texts aimed at second language (L2) learners. One of the major challenges in this task is the lack of signiÔ¨Åcantly sized level-annotated data. For the present work, we collected a dataset of CEFR-graded texts tailored for learners of English as an L2 and investigated text readability assessment for both native and L2 learners. We applied a generalization method to adapt models trained on larger native corpora to estimate text readability for learners, and explored domain adaptation and self-learning techniques to make use of the native data to improve system performance on the limited L2 data. In our experiments, the best performing model for readability on learner texts achieves an accuracy of 0.797 and P CC of 0.938. 
In this paper, we propose a method of automatically generating multiple-choice Ô¨Åll-inthe-blank exercises from existing text passages that challenge a reader‚Äôs comprehension skills and contextual awareness. We use a unique application of word co-occurrence likelihoods and the Google n-grams corpus to select words with strong contextual links to their surrounding text, and to generate distractors that make sense only in an isolated narrow context and not in the full context of the passage. Results show that our method is successful at generating questions with distractors that are semantically consistent in a narrow context but inconsistent given the full text, with larger n-grams yielding signiÔ¨Åcantly better results. 
We present a novel situational task that integrates collaborative problem solving behavior with testing in a science domain. Participants engage in discourse, which is used to evaluate their collaborative skills. We present initial experiments for automatic classification of such discourse, using a novel classification schema. Considerable accuracy is achieved with just lexical features. A speech-act classifier, trained on out-of-domain data, can also be helpful. 
We investigate how users of intelligent writing assistance tools deal with correct, incorrect, and incomplete feedback. To this end, we conduct an empirical user study around an L1 text revision task for German. Our participants should revise stylistic issues in two given texts using a novel web-based writing environment that highlights potential issues and provides corresponding feedback messages. In comparison to a control group, we Ô¨Ånd that precision plays a more important role than recall, which conÔ¨Årms previous Ô¨Åndings for other languages, issue types, user groups, and experimental setups. 
In this paper, we address the problem of quantifying the overall extent to which a testtaker‚Äôs essay deals with the topic it is assigned (prompt). We experiment with a number of models for word topicality, and a number of approaches for aggregating word-level indices into text-level ones. All models are evaluated for their ability to predict the holistic quality of essays. We show that the best texttopicality model provides a signiÔ¨Åcant improvement in a state-of-art essay scoring system. We also show that the Ô¨Åndings of the relative merits of different models generalize well across three different datasets. 
In many language learning scenarios, it is important to anticipate spelling errors. We model the spelling diÔ¨Éculty of words with new features that capture phonetic phenomena and are based on psycholinguistic Ô¨Åndings. To train our model, we extract more than 140,000 spelling errors from three learner corpora covering English, German and Italian essays. The evaluation shows that our model predicts spelling diÔ¨Éculty with an accuracy of over 80% and yields a stable quality across corpora and languages. In addition, we provide a thorough error analysis that takes the native language of the learners into account and provides insights into crosslingual transfer eÔ¨Äects. 
Natural language processing (NLP) methodologies have been widely adopted for readability assessment and greatly enhanced predictive accuracy. In the present study, we study a well-established feature, the frequency of a word in common language use, and systematically explore how such a word-level feature is best used to characterize the reading levels of texts, a text-level classiÔ¨Åcation problem. While traditionally such word-level features are simply averaged for all words of given text, we show that a richer representation leads to signiÔ¨Åcantly better predictive models. A basic approach adding a feature for the standard deviation already shows clear gains, and two more complex options systematically integrating more frequency information are explored: (i) encoding separate means for the words of a text according to which frequency band of the language they occur in, and (ii) encoding the mean of each cluster of words obtained by agglomerative hierarchical clustering of the words in the text based on their frequency. The former organizes frequency around general language characteristics, whereas the latter aims to lose as little information as possible about the distribution of word frequencies in a given text. To investigate the generalizability of the results, we compare cross-validation experiments within a corpus with cross-corpus experiments testing on the Common Core State Standards reference texts. We also contrast two different frequency norms and compare frequency with a measure of contextual diversity. ‚àóXiaobin Chen is also afÔ¨Åliated with the South China University of Technology, where he holds a lecturer position.  
The automated scoring of second-language (L2) learner text along various writing dimensions is an increasingly active research area. In this paper, we focus on determining the topical relevance of an essay to the prompt that elicited it. Given the burden involved in manually assigning scores for use in training supervised prompt-relevance models, we develop unsupervised models and show that they correlate well with human judgements. We show that expanding prompts using topically-related words, via pseudo-relevance modelling, is beneÔ¨Åcial and outperforms other distributional techniques. Finally, we incorporate our prompt-relevance models into a supervised essay scoring system that predicts a holistic score and show that it improves its performance. 
We investigate questions of how to reason about learner meaning in cases where the set of correct meanings is never entirely complete, speciÔ¨Åcally for the case of picture description tasks (PDTs). To operationalize this, we explore different models of representing and scoring non-native speaker (NNS) responses to a picture, including bags of dependencies, automatically determining the relevant parts of an image from a set of native speaker (NS) responses. In more exploratory work, we examine the variability in both NS and NNS responses, and how different system parameters correlate with the variability. In this way, we hope to provide insight for future system development, data collection, and investigations into learner language. 
In this paper we investigate how well the systems developed for automated evaluation of written responses perform when applied to spoken responses. We compare two state of the art systems for automated writing evaluation and a state of the art system for evaluating spoken responses. We Ô¨Ånd that the systems for writing evaluation achieve very good performance when applied to transcriptions of spoken responses but show degradation when applied to ASR output. The system based on sparse n-gram features appears to be more robust to such degradation. We further explore the role of ASR accuracy and the performance and construct coverage of the combined model which includes all three engines. 
Many grammatical error correction approaches use classiÔ¨Åers with specially-engineered features to predict corrections. A simpler alternative is to use n-gram language model scores. Rozovskaya and Roth (2011) reported that classiÔ¨Åers outperformed a language modeling approach. Here, we report a more nuanced result: a classiÔ¨Åer approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classiÔ¨Åer and a language modeling approach. 
As many as two-thirds of individuals with an Autism Spectrum Disorder (ASD) also have language impairments, which can range from mild limitations to complete non-verbal behavior. For such cases, there are several Augmentative and Alternative Communication (AAC) devices available. These are computer-designed tools in order to help people with ASD to palliate or overcome such limitations, at least partially. Some of the most popular AAC devices are based on pictograms, so that a pictogram is the graphical representation of a simple concept and sentences are composed by concatenating a number of such pictograms. Usually, these tools have to manage a vocabulary made up of hundreds of pictograms/concepts, with no or very poor knowledge of the language at semantic and pragmatic level. In this paper we present Pictogrammar, an AAC system which takes advantage of SUpO and PictOntology. SUpO (Simple Upper Ontology) is a formal semantic ontology which is made up of detailed knowledge of facts of everyday life such as simple words, with special interest in linguistic issues, allowing automated grammatical supervision. PictOntology is an ontology developed to manage sets of pictograms, linked to SUpO. Both ontologies make possible the development of tools which are able to take advantage of a formal semantics. 
We explore the factors inÔ¨Çuencing the dependence of single sentences on their larger textual context in order to automatically identify candidate sentences for language learning exercises from corpora which are presentable in isolation. An in-depth investigation of this question has not been previously carried out. Understanding this aspect can contribute to a more efÔ¨Åcient selection of candidate sentences which, besides reducing the time required for item writing, can also ensure a higher degree of variability and authenticity. We present a set of relevant aspects collected based on the qualitative analysis of a smaller set of contextdependent corpus example sentences. Furthermore, we implemented a rule-based algorithm using these criteria which achieved an average precision of 0.76 for the identiÔ¨Åcation of different issues related to context dependence. The method has also been evaluated empirically where 80% of the sentences in which our system did not detect context-dependent elements were also considered context-independent by human raters. 
Cloze tests, also known as gap-Ô¨Åll exercises, are a popular tool for acquiring and evaluating language proÔ¨Åciency. A major challenge in the way of automating scoring of cloze tests is the yet unsolved problem of gap Ô¨Åller ambiguity. To address this challenge, we present the concept of bundled gap Ô¨Ålling, along with (1) an efÔ¨Åcient computational model for automatically generating unambiguous gap bundle exercises, and (2) a disambiguation measure for guiding the construction of the exercises and validating their level of ambiguity. Our evaluation shows that our proposed exercises achieve a dramatic reduction in gap Ô¨Åller ambiguity, while our disambiguation measure can be effectively used to discard exercises that are nevertheless too ambiguous. 
Evaluating student answers often requires contextual information, such as previous utterances in conversational tutoring systems. For example, students use coreferences and write elliptical responses, i.e. incomplete but can be interpreted in context. The DT-Grade corpus which we present in this paper consists of short constructed answers extracted from tutorial dialogues between students and an Intelligent Tutoring System and annotated for their correctness in the given context and whether the contextual information was useful. The dataset contains 900 answers (of which about 25% required contextual information to properly interpret them). We also present a baseline system developed to predict the correctness label (such as correct, correct but incomplete) in which weights for the words are assigned based on context. 
How can second language teachers retrieve texts that are rich in terms of the grammatical constructions to be taught, but also address the content of interest to the learners? We developed an Information Retrieval system that identiÔ¨Åes the 87 grammatical constructions spelled out in the ofÔ¨Åcial English language curriculum of schools in Baden-Wu¬®rttemberg (Germany) and reranks the search results based on the selected (de)prioritization of grammatical forms. In combination with a visualization of the characteristics of the search results, the approach effectively supports teachers in prioritizing those texts that provide the targeted forms. The approach facilitates systematic input enrichment for language learners as a complement to the established notion of input enhancement: while input enrichment aims at richly representing the selected forms and categories in a text, input enhancement targets their presentation to make them more salient and support noticing. 
We present, to our knowledge, the Ô¨Årst experiments on using NLP to measure the extent to which a writing sample expresses the writer‚Äôs utility value from studying a STEM subject. Studies in social psychology have shown that a writing intervention where a STEM student is asked to reÔ¨Çect on the value of the STEM subject in their personal and social life is effective for improving motivation and retention of students in STEM in college. Automated assessment of UV in student writing would allow scaling the intervention up, opening access to its beneÔ¨Åts to multitudes of college students. Our results on biology data suggest that expression of utility value can be measured with reasonable accuracy using automated means, especially in personal essays. 
In this paper, we describe the development of a morphological analyzer for learner Hungarian, outlining extensions to a resourcelight system that can be developed by different types of experts. SpeciÔ¨Åcally, we discuss linguistic rule writing, resource creation, and different system settings, and our evaluation showcases the amount of improvement one gets for differing levels and kinds of effort, enabling other researchers to spend their time and energy as effectively as possible. 
We present preliminary work on automatically scoring constructed responses elicited as part of a certiÔ¨Åcation test designed to measure the effectiveness of the test-taker as a K-12 music teacher. This content scoring differs from most previous work in that the responses are relatively long and are written by an adult population of generally proÔ¨Åcient English writers. We obtain reasonably good scoring performance for all the test questions using simple features. We carry out some initial error analysis and show that there is still room for improvement. 
Online, open access, high-quality textbooks are an exciting new resource for improving the online learning experience. Because textbooks contain carefully crafted material written in a logical order, with terms deÔ¨Åned before use and discussed in detail, they can provide foundational material with which to buttress other resources. As a Ô¨Årst step towards this goal, we explore the automated augmentation of a popular online learning resource ‚Äì Khan Academy video modules ‚Äì with relevant reference chapters from open access textbooks. We show results from standard information retrieval weighting and ranking methods as well as an NLP-inspired approach, achieving F1 scores ranging from 0.63, to 0.83 on science topics. Future work includes taking into account the difÔ¨Åculty level and prerequisites of a textbook to select sections that are both relevant and reÔ¨Çect the concepts that the reader has already encountered. 
In spite of methodological and conceptual parallels, the computational linguistic applications short answer scoring (Burrows et al., 2015), authorship attribution (Stamatatos, 2009), and plagiarism detection (Zesch and Gurevych, 2012) have not been linked in practice. This work explores the practical usefulness of the combination of features from each of these Ô¨Åelds for two tasks: short answer assessment, and plagiarism detection. The experiments show that incorporating features from the other domain yields signiÔ¨Åcant improvements. A feature analysis reveals that robust lexical and semantic features are most informative for these tasks. 
We present an automated method for estimating the difÔ¨Åculty of spoken texts for use in generating items that assess non-native learners‚Äô listening proÔ¨Åciency. We collected information on the perceived difÔ¨Åculty of listening to various English monologue speech samples using a Likert-scale questionnaire distributed to 15 non-native English learners. We averaged the overall rating provided by three nonnative learners at different proÔ¨Åciency levels into an overall score of listenability. We then trained a multiple linear regression model with the listenability score as the dependent variable and features from both natural language and speech processing as the independent variables. Our method demonstrated a correlation of 0.76 with the listenability score, comparable to the agreement between the nonnative learners‚Äô ratings and the listenability score. 
We investigate automatically extracting multiword topical components to replace information currently provided by experts that is used to score the Evidence dimension of a writing in response to text assessment. Our goal is to reduce the amount of expert effort and improve the scalability of an automatic scoring system. Experimental results show that scoring performance using automatically extracted data-driven topical components is promising. 
We investigate the task of assessing sentencelevel prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentencelevel similarity calculation, which learns to adjust the weights of pre-trained word embeddings for a speciÔ¨Åc task, achieving substantially higher accuracy compared to other relevant baselines. 
I investigate Russian second language readability assessment using a machine-learning approach with a range of lexical, morphological, syntactic, and discourse features. Testing the model with a new collection of Russian L2 readability corpora achieves an F-score of 0.671 and adjacent accuracy 0.919 on a 6-level classiÔ¨Åcation task. Information gain and feature subset evaluation shows that morphological features are collectively the most informative. Learning curves for binary classiÔ¨Åers reveal that fewer training data are needed to distinguish between beginning reading levels than are needed to distinguish between intermediate reading levels. 
Active learning has been shown to be effective for reducing human labeling effort in supervised learning tasks, and in this work we explore its suitability for automatic short answer assessment on the ASAP corpus. We systematically investigate a wide range of AL settings, varying not only the item selection method but also size and selection of seed set items and batch size. Comparing to a random baseline and a recently-proposed diversitybased baseline which uses cluster centroids as training data, we Ô¨Ånd that uncertainty-based sampling methods can be beneÔ¨Åcial, especially for data sets with particular properties. The performance of AL, however, varies considerably across individual prompts. 
Social media users spend several hours a day to read, post and search for news on microblogging platforms. Social media is becoming a key means for discovering news. However, verifying the trustworthiness of this information is becoming even more challenging. In this study, we attempt to address the problem of rumor detection and belief investigation on Twitter. Our deÔ¨Ånition of rumor is an unveriÔ¨Åable statement, which spreads misinformation or disinformation. We adopt a supervised rumors classiÔ¨Åcation task using the standard dataset. By employing the Tweet Latent Vector (TLV) feature, which creates a 100-d vector representative of each tweet, we increased the rumor retrieval task precision up to 0.972. We also introduce the belief score and study the belief change among the rumor posters between 2010 and 2016. 
Access to expressions of subjective personal posts increased with the popularity of Social Media. However, most of the work in sentiment analysis focuses on predicting only valence from text and usually targeted at a product, rather than affective states. In this paper, we introduce a new data set of 2895 Social Media posts rated by two psychologicallytrained annotators on two separate ordinal nine-point scales. These scales represent valence (or sentiment) and arousal (or intensity), which deÔ¨Ånes each post‚Äôs position on the circumplex model of affect, a well-established system for describing emotional states (Russell, 1980; Posner et al., 2005). The data set is used to train prediction models for each of the two dimensions from text which achieve high predictive accuracy ‚Äì correlated at r = .65 with valence and r = .85 with arousal annotations. Our data set offers a building block to a deeper study of personal affect as expressed in social media. This can be used in applications such as mental illness detection or in automated large-scale psychological studies. 
This work explores the relationship between the sentiment of lyrics in Billboard Top 100 songs, stocks, and a consumer confidence index. We hypothesized that sentiment of Top 100 songs could be representative of public mood and correlate to stock market changes as well. We analyzed the sentiment for polarity and mood in terms of seven dimensions. We gathered data from 2008 to 2013 and found statistically significant correlations between lyrical sentiment polarity and DJIA closing values and between anxiety in lyrics and consumer confidence. We also found strong Granger-causal relationships involving anxiety, hope, anger, and both societal indicators. Finally, we introduced a vector autoregression model with time lag which is able to capture stock and consumer confidence indices (R2=.97, p<.001 and R2=0.72, p<.01 respectively). 
In this paper, we study the performance of N-gram language models on classification tasks such as sentiment analysis and spam detection and evaluate the effect of prior probability estimates on the results. Our data is in the form of public online posts pertaining to fast fashion brands, from different social media channels (Twitter and Facebook). We propose a novel ensemble model based on the combination of different N-grams in order to deal with the heteroskedastic nature of data collected from these social media channels. This has been further extended to increase the efficacy of the classification results. 
This work presents a novel approach for automatic creation of sentiment word lists. In this approach, words are Ô¨Årst mapped into a continuous latent space, which serves as input to a multilayer perceptron (MLP) trained using sentiment-annotated words. When evaluated using manually annotated EmoLex corpus, our approach compares favourably with SentiWordNet 3.0, another automatically generated word list. 
Negators, modals, and degree adverbs can signiÔ¨Åcantly affect the sentiment of the words they modify. Often, their impact is modeled with simple heuristics; although, recent work has shown that such heuristics do not capture the true sentiment of multi-word phrases. We created a dataset of phrases that include various negators, modals, and degree adverbs, as well as their combinations. Both the phrases and their constituent content words were annotated with real-valued scores of sentiment association. Using phrasal terms in the created dataset, we analyze the impact of individual modiÔ¨Åers and the average effect of the groups of modiÔ¨Åers on overall sentiment. We Ô¨Ånd that the effect of modiÔ¨Åers varies substantially among the members of the same group. Furthermore, each individual modiÔ¨Åer can affect sentiment words in different ways. Therefore, solutions based on statistical learning seem more promising than Ô¨Åxed hand-crafted rules on the task of automatic sentiment prediction. 
Existing opinion analysis techniques rely on the clues within the sentence that focus on the sentiment analysis task itself. However, the sentiment analysis task is not isolated from other NLP tasks (co-reference resolution, entity linking, etc) but they can beneÔ¨Åt each other. In this paper, we deÔ¨Åne dependencies between sentiment analysis and other tasks, and express the dependencies in Ô¨Årst order logic rules regardless of the representations of different tasks. The conceptual framework proposed in this paper using such dependency rules as constraints aims at exploiting information outside the sentence and outside the document to improve sentiment analysis. Further, the framework allows exception to the rules. 
Social media provides a wealth of information regarding users‚Äô perspectives on issues, public Ô¨Ågures and brands, but it can be a timeconsuming and labor-intensive process to develop data pipelines in which those perspectives are encoded, and to build visualizations that illuminate important developments. This paper describes a system for quickly developing a model of the conversation around an issue on Twitter, and a Ô¨Çexible visualization system that allows analysts to interactively explore key facets of the analysis. 
This paper investigates the effect of various types of linguistic features (lexical, syntactic and semantic) for training classiÔ¨Åers to detect threats of violence in a corpus of YouTube comments. Our results show that combinations of lexical features outperform the use of more complex syntactic and semantic features for this task. 
Clinical research article summaries called infoPOEMs (Patient-Oriented Evidence that Matters) are emailed by the Canadian Medical Association to family physicians who read them and answer the online Information Assessment Method (IAM) questionnaire which a free form textual opinion Ô¨Åelds to comment on the value or content of the infoPOEM. This article presents results of a relevance evaluation study applied on these comments to automatically determine their helpfulness and consequently the interest of sharing them among the medical community. A dataset of 3,470 manually annotated comments provides a gold standard, containing structural, syntactic, and semantic features taken from the UniÔ¨Åed Medical Language System and IAM questionnaire. Applied machine learning algorithms show a global f-measure improvement of 9.1% when compared to a binary occurrence bag-of-word baseline. 
People often use social media to discuss opinions, including political ones. We refer to relevant topics in these discussions as political issues, and the alternate stands towards these topics as political positions. We present a Political Issue Extraction (PIE) model that is capable of discovering political issues and positions from an unlabeled dataset of tweets. A strength of this model is that it uses twitter timelines of political and non-political authors, and afÔ¨Åliation information of only political authors. The model estimates word-speciÔ¨Åc distributions (that denote political issues and positions) and hierarchical author/group-speciÔ¨Åc distributions (that show how these issues divide people). Our experiments using a dataset of 2.4 million tweets from the US show that this model effectively captures the desired properties (with respect to words and groups) of political discussions. We also evaluate the two components of the model by experimenting with: (a) Use to alternate strategies to classify words, and (b) Value addition due to incorporation of group membership information. Estimated distributions are then used to predict political afÔ¨Åliation with 68% accuracy. 
Web discussion forums typically contain posts that fall into different categories such as question, solution, feedback, spam, etc. Automatic identiÔ¨Åcation of these categories can aid information retrieval that is tailored for speciÔ¨Åc user requirements. Previously, a number of supervised methods have attempted to solve this problem; however, these depend on the availability of abundant training data. A few existing unsupervised and semi-supervised approaches are either focused on identifying only one or two categories, or do not discuss category-speciÔ¨Åc performance. In contrast, this work proposes methods for identifying multiple categories, and also analyzes the category-speciÔ¨Åc performance. These methods are based on sequence models (speciÔ¨Åcally, hidden Markov Models) that can model language for each category using both probabilistic word and part-of-speech information, and minimal manually speciÔ¨Åed features. The unsupervised version initializes the models using clustering, whereas the semi-supervised version uses few manually labeled forum posts. Empirical evaluations demonstrate that these methods are more accurate than previous ones. 
Traditional sentiment analysis has been focused on predicting the polarity of texts as positive or negative at different granularity. This broad categorization does not account for informativeness of the underlying text. For many real-world applications such as social listening, brand monitoring and e-commerce platforms, the opinions that really matter are the informative opinions describing why something is good or bad. In this paper, we try to understand the properties of complaints and praises which is an informative subset of the negative and positive categories. Our analysis in the context of user reviews shows that complaints and praises have distinct properties that differentiate it from positive only or negative only sentences. 
A reputation system assists people selecting whom to trust. The ‚Äúall good reputation‚Äù problem is common in e-commerce domain, making it difÔ¨Åcult for buyers to choose credible sellers. Observing high growth of online data in Hindi language, in this paper, we propose a reputation system in this language. The functions of this system include 1) review mining for different criteria of online transactions 2) calculation of reputation rating and reputation weight for each criteria from user reviews and 3) ranking sellers based on computed reputation score. Extensive simulations conducted on eBay dataset show its effectiveness in solving ‚Äúall good reputation‚Äù problem. So far as our knowledge is concerned, this is the Ô¨Årst work in Hindi language on reputation system. 
For aspect-level sentiment analysis, the important first step is to identify the aspects and their associated entities present in customer reviews. Aspects can be either explicit or implicit, where the identification of the latter is more difficult. For restaurant reviews, this difficulty is escalated due to the vast number of entities and aspects present in reviews. The problem of implicit aspect identification has been studied for customer reviews in different domains, including restaurant reviews. However, the existing work for implicit aspect identification in customer reviews has the limitation of choosing at most one implicit aspect for each sentence. Furthermore, they deal only with a limited set of aspects related to a particular domain, thus have not faced the problem of ambiguity that arises when an opinion word is used to describe different aspects. This paper presents a novel approach for implicit aspect detection, which overcomes these two limitations. Our approach yields an F1measure of 0.842 when applied for a set of restaurant reviews collected from Yelp. 
In this paper we study several approaches to adapting a polarity lexicon to a speciÔ¨Åc domain. On the one hand, the domain adaptation using Term Frequency (TF) and on the other hand, the domain adaptation using pattern matching with a BootStrapping algorithm (BS). Both methods are corpus based and start with the same polarity lexicon, but the Ô¨Årst one requires an annotated collection of documents while the second one only needs a corpus where it looks for linguistic patterns. The performance of both methods overcomes the baseline system using the general polarity lexicon iSOL. However, although the TF approach achieves very promising results, the BS strategy does not give as much improvement as we expected. For this reason, we have combined both methods in order to take advantage of the positive aspects of each one. With this new approach the results obtained are even better that those with the systems applied individually. Actually, we have achieved a signiÔ¨Åcant improvement of 11.50% (in terms of accuracy) in the polarity classiÔ¨Åcation of the movie reviews with respect to the results achieved with the general purpose lexicon iSOL. 
Emotional language of human individuals has been studied for quite a while dealing with opinions and value judgments people have and share with others. In our work, we take a different stance and investigate whether large organizations, such as major industrial players, have and communicate emotions, as well. Such an anthropomorphic perspective has recently been advocated in management and organization studies which consider organizations as social actors. We studied this assumption by analyzing 1,676 annual business and sustainability reports from 90 top-performing enterprises in the United States, Great Britain and Germany. We compared the measurements of emotions in this homogeneous corporate text corpus with those from RCV1, a heterogeneous Reuters newswire corpus. From this, we gathered empirical evidence that business reports compare well with typical emotion-neutral economic news, whereas sustainability reports are much more emotionally loaded, similar to emotion-heavy sports and fashion news from Reuters. Furthermore, our data suggest that these emotions are distinctive and relatively stable over time per organization, thus constituting an emotional proÔ¨Åle for enterprises. 
Precise semantic representation of a sentence and deÔ¨Ånitive information extraction are key steps in the accurate processing of sentence meaning, especially for Ô¨Ågurative phenomena such as sarcasm, Irony, and metaphor cause literal meanings to be discounted and secondary or extended meanings to be intentionally proÔ¨Åled. Semantic modelling faces a new challenge in social media, because grammatical inaccuracy is commonplace yet many previous state-of-the-art methods exploit grammatical structure. For sarcasm detection over social media content, researchers so far have counted on Bag-of-Words(BOW), N-grams etc. In this paper, we propose a neural network semantic model for the task of sarcasm detection. We also review semantic modelling using Support Vector Machine (SVM) that employs constituency parsetrees fed and labeled with syntactic and semantic information. The proposed neural network model composed of Convolution Neural Network(CNN) and followed by a Long short term memory (LSTM) network and Ô¨Ånally a Deep neural network(DNN). The proposed model outperforms state-of-the-art textbased methods for sarcasm detection, yielding an F-score of .92. 
‚àóThe author is on leave from Consiglio Nazionale delle Ricerche, Italy 
 On the one hand, this characterization of the task is  Sentences and tweets are often annotated for sentiment simply by asking respondents to label them as positive, negative, or neutral. This works well for simple expressions of sentiment; however, for many other types of sen-  simple, terse, and reliant on the intuitions of native speakers of a language (rather than biasing the annotators by providing deÔ¨Ånitions of what it means to be positive, negative, and neutral). On the other hand, the lack of speciÔ¨Åcation leaves the annotator in  tences, respondents are unsure of how to annotate, and produce inconsistent labels. In this paper, we outline several types of sentences that are particularly challenging for manual sentiment annotation. Next we propose two  doubt over how to label certain kinds of instances‚Äî for example, sentences where one side wins against another, sarcastic sentences, or retweets. A different approach to sentiment annotation is  annotation schemes that address these challenges, and list beneÔ¨Åts and limitations for both.  to ask respondents to identify the target of opinion, and the sentiment towards this target of opinion (Pontiki et al., 2014; Mohammad et al., 2015; Deng  
Alzheimer‚Äôs disease (AD) and depression share a number of symptoms, and commonly occur together. Being able to differentiate between these two conditions is critical, as depression is generally treatable. We use linguistic analysis and machine learning to determine whether automated screening algorithms for AD are affected by depression, and to detect when individuals diagnosed with AD are also showing signs of depression. In the Ô¨Årst case, we Ô¨Ånd that our automated AD screening procedure does not show false positives for individuals who have depression but are otherwise healthy. In the second case, we have moderate success in detecting signs of depression in AD (accuracy = 0.658), but we are not able to draw a strong conclusion about the features that are most informative to the classiÔ¨Åcation. 
Dementia is an increasing problem for an aging population, with a lack of available treatment options, as well as expensive patient care. Early detection is critical to eventually postpone symptoms and to prepare health care providers and families for managing a patient‚Äôs needs. IdentiÔ¨Åcation of diagnostic markers may be possible with patients‚Äô clinical records. Text portions of clinical records are integrated into predictive models of dementia development in order to gain insights towards automated identiÔ¨Åcation of patients who may beneÔ¨Åt from providers‚Äô early assessment. Results support the potential power of linguistic records for predicting dementia status, both in the absence of, and in complement to, corresponding structured nonlinguistic data. 
As self-directed online anxiety treatment and e-mental health programs become more prevalent and begin to rapidly scale to a large number of users, the need to develop automated techniques for monitoring patient progress and detecting early warning signs is at an alltime high. While current online therapy systems work based on explicit quantitative feedback from various survey measures, little attention has been paid thus far to the large amount of unstructured free text present in the monitoring logs and journals submitted by patients as part of the treatment process. In this paper, we automatically categorize patients‚Äô internal sentiment and emotions using machine learning classiÔ¨Åers based on n-grams, syntactic patterns, sentiment lexicon features, and distributed word embeddings. We report classiÔ¨Åcation metrics on a novel mental health dataset. 
The sharing of emotional material is central to the process of psychotherapy and emotional problems are a primary reason for seeking treatment. Surprisingly, very little systematic research has been done on patterns of emotional exchange during psychotherapy. It is likely that a major reason for this void in the research is the enormous cost of annotating sessions for affective content. In the Ô¨Åeld of NLP, there have been major strides in the creation of algorithms for sentiment analysis, but most of this work has focused on written reviews of movies and twitter feeds with little work on spoken dialogue. We have created a new database of 97,497 utterances from psychotherapy transcripts labeled by humans for sentiment. We describe this dataset and present initial results for models identifying sentiment. We also show that one of the best models from the literature, trained on movie reviews, performed below many of our baseline models that trained on the psychotherapy corpus. 
This paper contributes a novel psychological dataset consisting of counselors‚Äô behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation CoefÔ¨Åcient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen‚Äôs Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efÔ¨Åcacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential beneÔ¨Åts in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger  scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 
Many people with mental illnesses face challenges posed by stigma perpetuated by fear and misconception in society at large. This societal stigma against mental health conditions is present in everyday language. In this study we take a set of 14 words with the potential to stigmatize mental health and sample Twitter as an approximation of contemporary discourse. Annotation reveals that these words are used with different senses, from expressive to stigmatizing to clinical.We use these wordsense annotations to extract a set of mental health‚Äìaware Twitter users, and compare their language use to that of an age- and gendermatched comparison set of users, discovering a difference in frequency of stigmatizing senses as well as a change in the target of pejorative senses. Such analysis may provide a Ô¨Årst step towards a tool with the potential to help everyday people to increase awareness of their own stigmatizing language, and to measure the effectiveness of anti-stigma campaigns to change our discourse. 
Online social media, such as Reddit, has become an important resource to share personal experiences and communicate with others. Among other personal information, some social media users communicate about mental health problems they are experiencing, with the intention of getting advice, support or empathy from other users. Here, we investigate the language of Reddit posts speciÔ¨Åc to mental health, to deÔ¨Åne linguistic characteristics that could be helpful for further applications. The latter include attempting to identify posts that need urgent attention due to their nature, e.g. when someone announces their intentions of ending their life by suicide or harming others. Our results show that there are a variety of linguistic features that are discriminative across mental health user communities and that can be further exploited in subsequent classiÔ¨Åcation tasks. Furthermore, while negative sentiment is almost uniformly expressed across the entire data set, we demonstrate that there are also condition-speciÔ¨Åc vocabularies used in social media to communicate about particular disorders. Source code and related materials are available from: https: //github.com/gkotsis/ reddit-mental-health.  
The phenotypic complexity of Autism Spectrum Disorder motivates the application of modern computational methods to large collections of observational data, both for improved clinical diagnosis and for better scientific understanding. We have begun to create a corpus of annotated language samples relevant to this research, and we plan to join with other researchers in pooling and publishing such resources on a large scale. The goal of this paper is to present some initial explorations to illustrate the opportunities that such datasets will afford. 
The need to protect privacy poses unique challenges to behavioral research. For instance, researchers often can not use examples drawn directly from such data to explain or illustrate key Ô¨Åndings. In this research, we use data-driven models to synthesize realistic-looking data, focusing on discourse produced by social-media participants announcing life-changing events. We comparatively explore the performance of distinct techniques for generating synthetic linguistic data across different linguistic units and topics. Our approach offers utility not only for reporting on qualitative behavioral research on such data, where directly quoting a participant‚Äôs content can unintentionally reveal sensitive information about the participant, but also for clinical computational system developers, for whom access to realistic synthetic data may be sufÔ¨Åcient for the software development process. Accordingly, the work also has implications for computational linguistics at large. 
Mental Health Records (MHRs) contain freetext documentation about patients‚Äô suicide and suicidality. In this paper, we address the problem of determining whether grammatic variants (inÔ¨Çections) of the word ‚Äúsuicide‚Äù are afÔ¨Årmed or negated. To achieve this, we populate and annotate a dataset with over 6,000 sentences originating from a large repository of MHRs. The resulting dataset has high InterAnnotator Agreement (Œ∫ 0.93). Furthermore, we develop and propose a negation detection method that leverages syntactic features of text1. Using parse trees, we build a set of basic rules that rely on minimum domain knowledge and render the problem as binary classiÔ¨Åcation (afÔ¨Årmed vs. negated). Since the overall goal is to identify patients who are expected to be at high risk of suicide, we focus on the evaluation of positive (afÔ¨Årmed) cases as determined by our classiÔ¨Åer. Our negation detection approach yields a recall (sensitivity) value of 94.6% for the positive cases and an overall accuracy value of 91.9%. We believe that our approach can be integrated with other clinical Natural Language Processing tools in order to further advance information extraction capabilities. 
Tragically, an estimated 42,000 Americans died by suicide in 2015, each one deeply affecting friends and family. Very little data and information is available about people who attempt to take their life, and thus scientiÔ¨Åc exploration has been hampered. We examine data from Twitter users who have attempted to take their life and provide an exploratory analysis of patterns in language and emotions around their attempt. We also show differences between those who have attempted to take their life and matched controls. We Ô¨Ånd quantiÔ¨Åable signals of suicide attempts in the language of social media data and estimate performance of a simple machine learning classiÔ¨Åer with these signals as a non-invasive analysis in a screening process. 
Online mental health forums provide users with an anonymous support platform that is facilitated by moderators responsible for Ô¨Ånding and addressing critical posts, especially those related to self-harm. Given the seriousness of these posts, it is important that the moderators are able to locate these critical posts quickly in order to respond with timely support. We approached the task of automatically triaging forum posts as a multiclass classiÔ¨Åcation problem. Our model uses a supervised classiÔ¨Åer with various features including lexical, psycholinguistic, and topic modeling features. On a dataset of mental forum posts from ReachOut.com1, our approach identiÔ¨Åed critical cases with a F-score of over 80%, showing the effectiveness of the model. Among 16 participating teams and 60 total runs, our best run achieved macro-average F1-score of 41% for the critical categories (The best score among all the runs was 42%). 
Varun Kumar University of Maryland College Park, MD varunk@cs.umd.edu  Philip Resnik University of Maryland College Park, MD resnik@umd.edu  
This paper presents a system capable of performing automatic triage of forum posts from ReachOut.com, a mental health online forum. The system assigns to each post a tag that indicates how urgently moderator attention is needed. The evaluation is based on experiments conducted on the CLPsych 2016 task, and the system is released as an open-source software. 
Following classical antiquity, European poetic meter was complicated by traditions negotiating between the prosodic stress of vernacular dialects and a classical system based on syllable length. Middle High German (MHG) epic poetry found a solution in a hybrid qualitative and quantitative meter. We develop a CRF model to predict the metrical values of syllables in MHG epic verse, achieving an Fscore of .894 on 10-fold cross-validated development data (outperforming several baselines) and .904 on held-out testing data. The method used in this paper presents itself as a viable option for other literary traditions, and as a tool for subsequent genre or author analysis. 
Most of the work dealing with automatic story production is based on a generic architecture for text generation; however, the resulting stories still lack a style that can be called literary. We believe that in order to generate automatically stories that could be compared with those by human authors, a speciÔ¨Åc methodology for Ô¨Åction text generation should be deÔ¨Åned. We also believe that it is essential for a story to convey the effect of originality to the person who is reading it. Our methodology proposes corpus-based generation of stories that could be called creative and also have a style similar to human Ô¨Åction texts. We also show how these stories have plausible syntax and coherence, and are perceived as interesting by human evaluators. 
 This paper proposes a technique to create Ô¨Ågurative relationships using Mikolov et al.‚Äôs word vectors. Drawing on existing work on Ô¨Ågurative language, we start with a pair of words and use the intersection of word vector similarity sets to blend the distinct semantic spaces of the two words. We conduct preliminary quantitative and qualitative observations to compare the use of this novel intersection method with the standard word vector addition method for the purpose of supporting the generation of Ô¨Ågurative language. 
Film scripts provide a means of examining generalized western social perceptions of accepted human behavior. In particular, we focus on how dialogue in Ô¨Ålms describes gender, identifying linguistic and structural differences in speech for men and women and in same and different-gendered pairs. Using the Cornell Movie-Dialogs Corpus (DanescuNiculescu-Mizil et al., 2012a), we identify signiÔ¨Åcant linguistic and structural features of dialogue that differentiate genders in conversation and analyze how those effects relate to existing literature on gender in Ô¨Ålm. Author‚Äôs Note (July 2020) The subsequent work below makes gender determinations based on a binary assignment assessed using statistics from most common baby names. We regret and recommend against this heuristic for several reasons: 1. Acceptance of a 6% error rate promotes misgendering as an acceptable consequence of analysis instead of an act of systemic violence against those who would be misgendered by this system (Hamidi et al., 2018; Keyes, 2018; Cao and Daume¬¥ III, 2020). 2. Even for characters in this dataset with binary gender, this particular labeling strategy is systematically biased against individuals with non-Western or traditionally genderneutral names (Larson, 2017).  task still performs erasure by not considering gender as something that can be nonbinary, Ô¨Çuid, and private (Keyes, 2018). We afÔ¨Årm that it can be all these things. Though we considered two ideas for recreating this work ‚Äî using database entries for the actors and actresses playing these roles, and automatically extracting pronouns from plot synopses ‚Äî we have decided that neither option is sufÔ¨Åcient. First, casting choices in Ô¨Ålms have often used actors of a gender other than their character (including the damaging practice of casting cis actors in trans roles, e.g., Eddie Redmayne in The Danish Girl and Jared Leto in Dallas Buyers Club (Ford, 2016; Reitz, 2017)). Second, the proxy of the labels ‚Äúactor‚Äù and ‚Äúactress‚Äù would still cause nonbinary erasure: at the time of writing, IMDb apparently distinguishes ‚Äúactresses‚Äù as those who use she/her pronouns and ‚Äúactors‚Äù as those who do not or for whom it is unknown (imd, ). Third, the trope of trans identity as a plot device for the narrative of a cis character (Ford, 2016) can mean that pronouns from synopses may reÔ¨Çect either a character‚Äôs or writer‚Äôs transphobia. The authors sincerely apologize for the harm this paper may have caused. We have left this note instead of retracting the paper in the hope that those interested in using computational methods to understand Ô¨Ålm dialogue will consider these concerns and put forth a more inclusive theory of gender in their own analyses.. 
Given multiple corrupted versions of the same text, as is common with ancient manuscripts, we wish to reconstruct the original text from which the extant corrupted versions were copied (typically via latent intermediary versions). This is a challenge of cardinal importance in the humanities. We use a variant of expectation-maximization (EM), to solve this problem. We prove the efficacy of our method on both synthetic and real-world data.  tomated using a variant of the expectationmaximization (EM) algorithm (Dempster et al 1977). The structure of the paper is as follows. In the next section, we consider previous work on the urtext problem (mostly dealing with a different version of the problem). In Section 3, we define the synoptic form in which we assume the texts are presented. In Sections 4 and 5, we formalize the problem and present our solution. In the three subsequent sections, we consider synthetic, artificial and real-world testbeds, respectively.  
 The chiasmus is a rhetorical Ô¨Ågure involving the repetition of a pair of words in reverse order, as in ‚Äúall for one, one for all‚Äù. Previous work on detecting chiasmus in running text has only considered superÔ¨Åcial features like words and punctuation. In this paper, we explore the use of syntactic features as a means to improve the quality of chiasmus detection. Our results show that taking syntactic structure into account may increase average precision from about 40 to 65% on texts taken from European Parliament proceedings. To show the generality of the approach, we also evaluate it on literary text and observe a similar improvement and a slightly better overall result. 
We present a novel task: the chronological classiÔ¨Åcation of Hafez‚Äôs poems (ghazals). We compiled a bilingual corpus in digital form, with consistent idiosyncratic properties. We have used Hooman‚Äôs labeled ghazals in order to train automatic classiÔ¨Åers to classify the remaining ghazals. Our classiÔ¨Åcation framework uses a Support Vector Machine (SVM) classiÔ¨Åer with similarity features based on Latent Dirichlet Allocation (LDA). In our analysis of the results we use the LDA topics‚Äô main terms that are passed on to a Principal Component Analysis (PCA) module. 
This work discusses a mix of challenges arising from Watson Discovery Advisor (WDA), an industrial strength descendant of the Watson Jeopardy! Question Answering system currently used in production in industry settings. Typical challenges include generation of appropriate training questions, adaptation to new industry domains, and iterative improvement of the system through manual error analyses. 
 Understanding open-domain text is one of the primary challenges in NLP. Machine comprehension benchmarks evaluate a system‚Äôs ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. SpeciÔ¨Åcally, we explore HABCNN for this task by two routes, one is through traditional joint modeling of document, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin. 
We introduce a highly scalable approach for open-domain question answering with no dependence on any data set for surface form to logical form mapping or any linguistic analytic tool such as POS tagger or named entity recognizer. We deÔ¨Åne our approach under the Constrained Conditional Models framework which lets us scale up to a full knowledge graph with no limitation on the size. On a standard benchmark, we obtained near 4 percent improvement over the state-of-the-art in open-domain question answering task. 
We propose NEURAL ENQUIRER ‚Äî a neural network architecture for answering natural language (NL) questions given a knowledge base (KB) table. Unlike previous work on end-to-end training of semantic parsers, NEURAL ENQUIRER is fully ‚Äúneuralized‚Äù: it gives distributed representations of queries and KB tables, and executes queries through a series of differentiable operations. The model can be trained with gradient descent using both endto-end and step-by-step supervision. During training the representations of queries and the KB table are jointly optimized with the query execution logic. Our experiments show that the model can learn to execute complex NL queries on KB tables with rich structures. 
This paper presents an end-to-end neural network model, named Neural Generative Question Answering (GENQA), that can generate answers to simple factoid questions, based on the facts in a knowledge-base. More specifically, the model is built on the encoderdecoder framework for sequence-to-sequence learning, while equipped with the ability to enquire the knowledge-base, and is trained on a corpus of question-answer pairs, with their associated triples in the knowledge-base. Empirical study shows the proposed model can effectively deal with the variations of questions and answers, and generate right and natural answers by referring to the facts in the knowledge-base. The experiment on question answering demonstrates that the proposed model can outperform an embedding-based QA model as well as a neural dialogue model trained on the same data. 
Most question answering systems use symbolic or text information. We present a dataset for a task that requires understanding descriptions of visual themes and their layout: identifying paintings from their descriptions. We annotate paintings with contour data, align regions with entity mentions from an ontology, and associate image regions with text spans from descriptions. A simple embedding-based method applied to text-to-image coreferences achieves state-of-the-art results on our task when paired with bipartite matching. The task is made all the more difÔ¨Åcult by scarcity of training data.  of countries. Since these images are of cultural importance, we have access to many redundant descriptions of the same works, allowing us to create a naturalistic but inexpensive dataset. Due to the complex and oblique nature of questions about paintings, their visual complexity, and the relatively small data size, prior approaches used for VQA over natural images are infeasible for our task. We formalise the task in Section 3, where we also present a preliminary system (ARTMATCH) and compare with it a data-driven text baseline to illustrate the usefulness and versatility of our method (Section 4). Finally, in Section 5 we compare our task and system to previous work that combines NLP and vision.  
Understanding the nature of the errors of a machine learning system is often difÔ¨Åcult for multiclass classiÔ¨Åcation problems with a large number of classes. This is true even more so if the number of examples for each class is low. To interpret the performance of a multiclass classiÔ¨Åer, we form a graph representing the errors, and use average-link clustering to Ô¨Ånd groups of classes which are confused with each other. We apply this idea to the QANTA question answering system (Iyyer et al., 2014), and provide a method of analysis of the clusters. 
We propose a new open question answering framework for question answering over a knowledge base (KB). Our system uses both a curated KB, Freebase, and one that is extracted automatically by an open information extraction model, IE KB. Our system consists of only one layer of paraphrase, compared to the three layers used in a previous open question answering system (Fader et al., 2014). However, because of the more accurately extracted relation triples in IE KB, combined with linked entities from IE KB to Freebase, our system achieves a 7% absolute gain in F1 score over the previous system. 
This paper presents an extension of neural machine translation (NMT) model to incorporate additional word-level linguistic factors. Adding such linguistic factors may be of great beneÔ¨Åts to learning of NMT models, potentially reducing language ambiguity or alleviating data sparseness problem (Koehn and Hoang, 2007). We explore different linguistic annotations at the word level, including: lemmatization, word clusters, Part-ofSpeech tags, and labeled dependency relations. We then propose different neural attention architectures to integrate these additional factors into the NMT framework. Evaluating on translating between English and German in two directions with a low resource setting in the domain of TED talks, we obtain promising results in terms of both perplexity reductions and improved BLEU scores over baseline methods. 
Large volumes of electronic health records, including free-text documents, are extensively generated within various sectors of healthcare. Medical concept annotation systems are designed to enrich these documents with key concepts in the domain using reference terminologies. Although there is a wide range of annotation systems, there is a lack of comparative analysis that enables thorough understanding of the effectiveness of both the concept extraction and concept recognition components of these systems, especially within the clinical domain. This paper analyses and evaluates four annotation systems (i.e., MetaMap, NCBO annotator, Ontoserver, and QuickUMLS) for the task of extracting medical concepts from clinical free-text documents. Empirical Ô¨Åndings have shown that each annotator exhibits various levels of strengths in terms of overall precision or recall. The concept recognition component of each system, however, was found to be highly sensitive to the quality of the text spans output by the concept extraction component of the annotation system. The effects of these components on each other are quantiÔ¨Åed in such way as to provide evidence for an informed choice of an annotation system as well as avenues for future research. 
This study investigates the use of unsupervised word embeddings and sequence features for sample representation in an active learning framework built to extract clinical concepts from clinical free text. The objective is to further reduce the manual annotation effort while achieving higher effectiveness compared to a set of baseline features. Unsupervised features are derived from skip-gram word embeddings and a sequence representation approach. The comparative performance of unsupervised features and baseline handcrafted features in an active learning framework are investigated using a wide range of selection criteria including least confidence, information diversity, information density and diversity, and domain knowledge informativeness. Two clinical datasets are used for evaluation: the i2b2/VA 2010 NLP challenge and the ShARe/CLEF 2013 eHealth Evaluation Lab. Our results demonstrate significant improvements in terms of effectiveness as well as annotation effort savings across both datasets. Using unsupervised features along with baseline features for sample representation lead to further savings of up to 9% and 10% of the token and concept annotation rates, respectively. 
Public health surveillance is challenging due to difÔ¨Åculties accessing medical data in real-time. We present a novel, effective and computationally inexpensive method for syndromic surveillance using Twitter data. The proposed method uses a regression model on a database previously built using named entity recognition to identify mentions of symptoms, disorders and pharmacological substances over GNIP Decahose Twitter data. The result of our method is compared to the reported weekly Ô¨Çu and Lyme disease rates from the US Center of Disease Control and Prevention (CDC) website. Our method predicts the 2014 CDC reported Ô¨Çu prevalence with 94.9% Spearman correlation using 2012 and 2013 CDC Ô¨Çu statistics as training data, and the CDC Lyme disease rate for July to December 2014 with 89.6% Spearman correlation. It also predicts the prevalences for the same diseases and time periods using the Twitter data from the previous week with 93.31% and 86.9% Spearman correlations respectively. 
Syndromic Surveillance has been performed using machine learning and other statistical methods to detect disease outbreaks. These methods are largely dependent on the availability of historical data to train the machine learning-based surveillance system. However, relevant training data may differ from region to region due to geographical and seasonal trends, meaning that the syndromic surveillance designed for one area may not be effective for another. We proposed and analyse a semi-supervised method for syndromic surveillance from emergency department chief complaint textual notes that avoids the need for large training data. Our new method is based on identiÔ¨Åcation of lexical shifts in the language of Chief Complaints of patients, as recorded by triage nurses, that we believe can be used to monitor disease distributions and possible outbreaks over time. The results we obtained demonstrate that effective lexical syndromic surveillance can be approached when distinctive lexical items are available to describe speciÔ¨Åc syndromes. 
Relation extraction models based on deep learning have been attracting a lot of attention recently. Little research is carried out to reduce their need of labeled training data. In this work, we propose an unsupervised pre-training method based on the sequence-to-sequence model for deep relation extraction models. The pre-trained models need only half or even less training data to achieve equivalent performance as the same models without pre-training. 
Kernel methods have been widely studied in several natural language processing tasks such as relation extraction and sentence classiÔ¨Åcation. In this work, we present a new graph kernel that is derived from a distance measure described in prior work as Approximate Subgraph Matching (ASM). The classical ASM distance, shown to be effective for event extraction, is not a valid kernel and was primarily designed to work with rule based systems. We modify this distance suitably to render it a valid kernel (ASM kernel) and enable its use in powerful learning algorithms such as Support Vector Machine (SVM). We compare the ASM kernel with SVMs to the classical ASM with a rule based approach, for two relation extraction tasks and show an improved performance with the kernel based approach. Compared to other kernels such as the Subset tree kernel and the Partial tree kernel, ASM kernel outperforms in relation extraction tasks and is of comparable performance in a general sentence classiÔ¨Åcation task. We describe the advantages of the ASM kernel such as its Ô¨Çexibility and ease of modiÔ¨Åcation, which offers further directions for improvement. 
Modern question answering and summarizing systems have motivated the need for complex n-ary relation extraction systems where the number of related entities (n) can be more than two. Shortest path dependency kernels have been proven to be effective in extracting binary relations. In this work, we propose a method that employs shortest path dependency based rules to extract complex n-ary relations without decomposing a sentence into constituent binary relations. With an aim of extracting biographical entities and relations from manually annotated datasets of Australian researchers and department seminar mails, we train an information extraction system which Ô¨Årst extracts entities using conditional random Ô¨Åelds and then employs the shortest path dependency based rules along with semantic and syntactic features to extract n-ary afÔ¨Åliation relations using support vector machine. Cross validation of this method on the two datasets provides evidence that it outperforms the state-of-the-art n-ary relation extraction system by a margin of 8% F-score. 
This paper presents a novel approach to low resource language modeling. Here we propose a model for word prediction which is based on multi-variant ngram abstraction with weighted conÔ¨Ådence level. We demonstrate a signiÔ¨Åcant improvement in word recall over ‚Äùtraditional‚Äù KneserNey back-off model for most of the examined low resource languages. 
There is a growing body of work studying suicide ideation, expressions of intentions to kill oneself, on social media. We explore the problem of detecting such ideation on Twitter, focusing on the impact of a set of features drawn from the literature and on the role of discussion context for this task. Our experiments show a signiÔ¨Åcant improvement upon the previously published results for the O‚ÄôDea et al. (2015) dataset on suicide ideation. Interestingly, we found that stylistic features helped while social media metadata features did not. Furthermore, discussion context was useful. To further understand the contributions of these different features and of discussion context, we present a discussion of our experiments in varying the feature representations, and examining their effects on suicide ideation detection on Twitter. 
Supervised domain-speciÔ¨Åc term extraction often suffers from two common problems, namely labourious manual feature selection, and the lack of labelled data. In this paper, we introduce a weakly supervised bootstrapping approach using two deep learning classiÔ¨Åers. Each classiÔ¨Åer learns the representations of terms separately by taking word embedding vectors as inputs, thus no manually selected feature is required. The two classiÔ¨Åers are Ô¨Årstly trained on a small set of labelled data, then independently make predictions on a subset of the unlabeled data. The most conÔ¨Ådent predictions are subsequently added to the training set to retrain the classiÔ¨Åers. This co-training process minimises the reliance on labelled data. Evaluations on two datasets demonstrate that the proposed co-training approach achieves a competitive performance with limited training data as compared to standard supervised learning baseline. 
This is a Monte Carlo simulation-based study that explores the effect of the sample size of the background database on a likelihood ratio (LR)-based forensic text comparison (FTC) system built on multivariate authorship attribution features. The text messages written by 240 authors who were randomly selected from an archive of chatlog messages were used in this study. The strength of evidence (= LR) was estimated using the multivariate kernel density likelihood ratio (MVKD) formula with a logistic-regression calibration. The results are reported along two points: the system performance (= accuracy) and the stability of performance based on the standard metric for LR-based systems; namely the log-likelihoodratio cost (Cllr). It was found in this study that the system performance and its stability improve as a function of the sample size (= author count) in the background database in a non-linear manner, and that the more features used for modelling, the more background data the system generally requires for optimal results. The implications of the findings to the real casework are also discussed. 
Sarcasm and irony, although similar, differ in that sarcasm has an impact on sentiment (because it is used to ridicule a target) while irony does not. Past work treats the two interchangeably. In this paper, we wish to validate if sarcasm versus irony classiÔ¨Åcation is indeed a challenging task. To this end, we use a dataset of quotes from English literature, and conduct experiments from two perspectives: the human perspective and the computational perspective. For the former, we show that three human annotators have lower agreement for sarcasm versus irony as compared to sarcasm versus philosophy. Similarly, sarcasm versus irony classiÔ¨Åcation performs with a lower F-score as compared to another classiÔ¨Åcation task where labels are not related: sarcasm versus philosophy classiÔ¨Åcation. Another key point that our paper makes is that features designed for sarcasm versus non-sarcasm classiÔ¨Åcation do not work well for sarcasm versus irony classiÔ¨Åcation. 
In this paper, we develop a weakly supervised version of logistic regression to help to improve biomedical text classiÔ¨Åcation performance when there is limited annotated data. We learn cascaded latent variable models for the classiÔ¨Åcation tasks. First, with a large number of unlabelled but limited amount of labelled biomedical text, we will bootstrap and semi-automate the annotation task with partially and weakly annotated data. Second, both coarse-grained (document) and Ô¨Åne-grained (sentence) levels of each individual biomedical report will be taken into consideration. Our experimental work shows this achieves higher classiÔ¨Åcation results. 
Twitter text-based geotagging often uses geospatial words to determine locations. While much work has been done in word geospatiality analysis, there has been little work on temporal variations in the geospatial spread of word usage. In this paper, we investigate geospatial words relative to their temporal locality patterns by Ô¨Åtting periodical models over time. The model jointly captures inherent geospatial locality and periodical factor for a word. The resultant factorisation enables better understanding of word temporal trends and improves geotagging accuracy by only using inherent geospatial local words. 
Social media sites such as Twitter are attractive sources of information due to their combination of accessibility, timeliness and large data volumes. IdentiÔ¨Åcation of medical entities in Twitter can support tasks such public health surveillance. We propose an approach to perform annotation of medical entities using a sequence to sequence neural network. Results show that our approach improves over previous work based on CRF in the annotation of two medical entities types in Twitter. 
This paper presents an empirical comparison of different dependency parsers for Vietnamese, which has some unusual characteristics such as copula drop and verb serialization. Experimental results show that the neural network-based parsers perform significantly better than the traditional parsers. We report the highest parsing scores published to date for Vietnamese with the labeled attachment score (LAS) at 73.53% and the unlabeled attachment score (UAS) at 80.66%. 
We report on an exploratory analysis of Emoji Dick, a project that leverages crowdsourcing to translate Melville‚Äôs Moby Dick into emoji. This distinctive use of emoji removes textual context, and leads to a varying translation quality. In this paper, we use statistical word alignment and part-of-speech tagging to explore how people use emoji. Despite these simple methods, we observed differences in token and part-of-speech distributions. Experiments also suggest that semantics are preserved in the translation, and repetition is more common in emoji. 
The timeline generation task summarises an entity‚Äôs biography by selecting stories representing key events from a large pool of relevant documents. This paper addresses the lack of a standard dataset and evaluative methodology for the problem. We present and make publicly available a new dataset of 18,793 news articles covering 39 entities. For each entity, we provide a gold standard timeline and a set of entityrelated articles. We propose ROUGE as an evaluation metric and validate our dataset by showing that top Google results outperform straw-man baselines. 
Do distributional word representations encode the linguistic regularities that theories of meaning argue they should encode? We address this question in the case of the logical properties (monotonicity, force) of quantiÔ¨Åcational words such as everything (in the object domain) and always (in the time domain). Using the vector offset approach to solving word analogies, we Ô¨Ånd that the skip-gram model of distributional semantics behaves in a way that is remarkably consistent with encoding these features in some domains, with accuracy approaching 100%, especially with mediumsized context windows. Accuracy in others domains was less impressive. We compare the performance of the model to the behavior of human participants, and Ô¨Ånd that humans performed well even where the models struggled. 
The automatic prediction of aspectual classes is very challenging for verbs whose aspectual value varies across readings, which are the rule rather than the exception. This paper sheds a new perspective on this problem by using a machine learning approach and a rich morpho-syntactic and semantic valency lexicon. In contrast to previous work, where the aspectual value of corpus clauses is determined on the basis of features retrieved from the corpus, we use features extracted from the lexicon, and aim to predict the aspectual value of verbal readings rather than verbs. Studying the performance of the classiÔ¨Åers on a set of manually annotated verbal readings, we found that our lexicon provided enough information to reliably predict the aspectual value of verbs across their readings. We additionally tested our predictions for unseen predicates through a task based evaluation, by using them in the automatic detection of temporal relation types in TempEval 2007 tasks for French. These experiments also conÔ¨Årmed the reliability of our aspectual predictions, even for unseen verbs. 
It is generally believed that a metaphor tends to have a stronger emotional impact than a literal statement; however, there is no quantitative study establishing the extent to which this is true. Further, the mechanisms through which metaphors convey emotions are not well understood. We present the Ô¨Årst data-driven study comparing the emotionality of metaphorical expressions with that of their literal counterparts. Our results indicate that metaphorical usages are, on average, signiÔ¨Åcantly more emotional than literal usages. We also show that this emotional content is not simply transferred from the source domain into the target, but rather is a result of meaning composition and interaction of the two domains in the metaphor. 
This paper presents a rule-based approach to constructing lexical axioms from WordNet verb entries in an expressive semantic representation, Episodic Logic (EL). EL differs from other representations in being syntactically close to natural language and covering phenomena such as generalized quantiÔ¨Åcation, modiÔ¨Åcation, and intensionality while still allowing highly effective inference. The presented approach uses a novel preprocessing technique to improve parsing precision of coordinators and incorporates frames, hand-tagged word senses, and examples from WordNet to achieve highly consistent semantic interpretations. EL allows the full content of glosses to be incorporated into the formal lexical axioms, without sacriÔ¨Åcing interpretive accuracy, or verb-to-verb inference accuracy on a standard test set. Evaluation of semantic parser performance is based on EL-match, introduced here as a generalization of the smatch metric for semantic structure accuracy. On gloss parses, the approach achieves an ELmatch F1 score of 0.83, and a wholeaxiom F1 score of 0.45; verb entailment identiÔ¨Åcation based on extracted axioms is competitive with the state-of-the-art. 
Extending semantic role labeling (SRL) to detect and recover non-local arguments continues to be a challenge. Our work is the Ô¨Årst to address the detection of implicit roles from a multilingual perspective. We map predicate-argument structures across English and German sentences, and we develop a classiÔ¨Åer that distinguishes implicit arguments from other translation shifts. Using a combination of alignment statistics and linguistic features, we achieve a precision of 0.68 despite a limited training set, which is a signiÔ¨Åcant gain over the majority baseline. Our approach does not rely on pre-existing knowledge bases and is extendible to any language pair with parallel data and dependency parses. 
We describe a new technique for improving statistical machine translation training by adopting scores from a recent crosslingual semantic frame based evaluation metric, XMEANT, as outside probabilities in expectation-maximization based ITG (inversion transduction grammars) alignment. Our new approach strongly biases early-stage SMT learning towards semantically valid alignments. Unlike previous attempts that have proposed using semantic frame based evaluation metrics as the objective function for late-stage tuning of less than a dozen loglinear mixture weights, our approach instead applies the semantic metric at one of the earliest stages of SMT training, where it may impact millions of model parameters. The choice of XMEANT is motivated by empirical studies that have shown ITG constraints to cover almost all crosslingual semantic frame alternations, which resemble the crosslingual semantic frame matching measured by XMEANT. Our experiments purposely restrict training data to small amounts to show the technique‚Äôs utility in the absence of a huge corpus, to study the effects of semantic generalizations while avoiding overreliance on memorization. Results show that directly driving ITG training with the crosslingual semantic frame based objective function not only helps to further sharpen the ITG constraints, but still avoids excising relevant portions of the search space, and leads to better performance than either conventional ITG or GIZA++ based approaches.  
Reasoning over several premises is not a common feature of RTE systems as it usually requires deep semantic analysis. On the other hand, FraCaS is a collection of entailment problems consisting of multiple premises and covering semantically challenging phenomena. We employ the tableau theorem prover for natural language to solve the FraCaS problems in a natural way. The expressiveness of a type theory, the transparency of natural logic and the schematic nature of tableau inference rules make it easy to model challenging semantic phenomena. The efÔ¨Åciency of theorem proving also becomes challenging when reasoning over several premises. After adapting to the dataset, the prover demonstrates state-of-the-art competence over certain sections of FraCaS. 
In a complex sentence comprised of one or more subclauses, the overt or hidden attitudes between the various entities depend on the factuality projection of the verbs, their polar effects, and the modality and afÔ¨Årmative status (negated or not) of the clauses. If factuality is given, some referents might even be considered to beneÔ¨Åt or to suffer from the (effects of the) described situation, independently of their relations to the other referents. An interesting question is, how the reader evaluates all this from his/her perspective. We introduce an approach based on Description Logics that integrates these various perspectives into a joint model. 
Recent models in distributional semantics consider derivational patterns (e.g., use ‚Üí use + f ul ) as the result of a compositional process, where base term and afÔ¨Åx are combined. We exploit such models for German particle verbs (PVs), and focus on the task of learning a mapping function between base verbs and particle verbs. Our models apply particle-verb motivated training-space restrictions relying on nearest neighbors, as well as recent advances from zeroshot-learning. The models improve the mapping between base terms and derived terms for a new PV derivation dataset, and also across existing derivation datasets for German and English. 
Distributional semantic models can predict many linguistic phenomena, including word similarity, lexical ambiguity, and semantic priming, or even to pass TOEFL synonymy and analogy tests (Landauer and Dumais, 1997; GrifÔ¨Åths et al., 2007; Turney and Pantel, 2010). But what does it take to create a competitive distributional model? Levy et al. (2015) argue that the key to success lies in hyperparameter tuning rather than in the model‚Äôs architecture. More hyperparameters trivially lead to potential performance gains, but what do they actually do to improve the models? Are individual hyperparameters‚Äô contributions independent of each other? Or are only speciÔ¨Åc parameter combinations beneÔ¨Åcial? To answer these questions, we perform a quantitative and qualitative evaluation of major hyperparameters as identiÔ¨Åed in previous research. 
In this paper, we aim to close the gap from extensive, human-built semantic resources and corpus-driven unsupervised models. The particular resource explored here is VerbNet, whose organizing principle is that semantics and syntax are linked. To capture patterns of usage that can augment knowledge resources like VerbNet, we expand a Dirichlet process mixture model to predict a VerbNet class for each sense of each verb, allowing us to incorporate annotated VerbNet data to guide the clustering process. The resulting clusters align more closely to hand-curated syntactic/semantic groupings than any previous models, and can be adapted to new domains since they require only corpus counts. 
Recognizing lexical inferences between pairs of terms is a common task in NLP applications, which should typically be performed within a given context. Such context-sensitive inferences have to consider both term meaning in context as well as the Ô¨Åne-grained relation holding between the terms. Hence, to develop suitable lexical inference methods, we need datasets that are annotated with Ô¨Åne-grained semantic relations in-context. Since existing datasets either provide outof-context annotations or refer to coarsegrained relations, we propose a methodology for adding context-sensitive annotations. We demonstrate our methodology by applying it to phrase pairs from PPDB 2.0, creating a novel dataset of Ô¨Ånegrained lexical inferences in-context and showing its utility in developing contextsensitive methods. 
The interpretation of adjective-noun pairs plays a crucial role in tasks such as recognizing textual entailment. Formal semantics often places adjectives into a taxonomy which should dictate adjectives‚Äô entailment behavior when placed in adjective-noun compounds. However, we show experimentally that the behavior of subsective adjectives (e.g. red) versus non-subsective adjectives (e.g. fake) is not as cut and dry as often assumed. For example, inferences are not always symmetric: while ID is generally considered to be mutually exclusive with fake ID, fake ID is considered to entail ID. We discuss the implications of these Ô¨Åndings for automated natural language understanding. 
We investigate style accommodation in online discussions, in particular its interplay with content agreement and disagreement. Using a new model for measuring style accommodation, we Ô¨Ånd that speakers coordinate on style more noticeably if they disagree than if they agree, especially if they want to establish rapport and possibly persuade their interlocutors. 
Segmenting text into semantically coherent fragments improves readability of text and facilitates tasks like text summarization and passage retrieval. In this paper, we present a novel unsupervised algorithm for linear text segmentation (TS) that exploits word embeddings and a measure of semantic relatedness of short texts to construct a semantic relatedness graph of the document. Semantically coherent segments are then derived from maximal cliques of the relatedness graph. The algorithm performs competitively on a standard synthetic dataset and outperforms the best-performing method on a real-world (i.e., non-artiÔ¨Åcial) dataset of political manifestos. 
We describe the implementation of a Word Sense Disambiguation (WSD) tool in a Dutch Text-to-Pictograph translation system, which converts textual messages into sequences of pictographic images. The system is used in an online platform for Augmentative and Alternative Communication (AAC). In the original translation process, the appropriate sense of a word was not disambiguated before converting it into a pictograph. This often resulted in incorrect translations. The implementation of a WSD tool provides a better semantic understanding of the input messages.  tomatically augments written text with Beta2 or Sclera3 pictographs and is primarily conceived to improve the comprehension of textual content. The Pictograph-to-Text translation system (Sevens et al., 2015b) allows the user to insert a series of Beta or Sclera pictographs, automatically translating this image sequence into natural language text where possible. This facilitates the construction of textual content. The Text-to-Pictograph translation process did not yet perform Word Sense Disambiguation (WSD) to select the appropriate sense of a word before converting it into a pictograph. Instead, the most frequent sense of the word was chosen. This sometimes resulted in incorrect pictograph translations (see Figure 1).  
In this paper, we propose methods to take into account the disagreement between crowd annotators as well as their skills for weighting instances in learning algorithms. The latter can thus better deal with noise in the annotation and produce higher accuracy. We created two passage reranking datasets: one with crowdsource platform, and the second with an expert who completely revised the crowd annotation. Our experiments show that our weighting approach reduces noise improving passage reranking up to 1.47% and 1.85% on MRR and P@1, respectively. 
Learning embeddings of words and knowledge base elements is a promising approach for open domain question answering. Based on the remark that relations and entities are distinct object types lying in the same embedding space, we analyze the beneÔ¨Åt of adding a regularizer favoring the embeddings of entities to be orthogonal to those of relations. The main motivation comes from the observation that modifying the embeddings using prior knowledge often helps performance. The experiments show that incorporating the regularizer yields better results on a challenging question answering benchmark. 
In this paper, we explore the role of constituent properties in English and German noun-noun compounds (corpus frequencies of the compounds and their constituents; productivity and ambiguity of the constituents; and semantic relations between the constituents), when predicting the degrees of compositionality of the compounds within a vector space model. The results demonstrate that the empirical and semantic properties of the compounds and the head nouns play a signiÔ¨Åcant role. 
The linguistic experiences of a person are an important part of their individuality. In this paper, we show that people can be modelled as vectors in a semantic space, using their personal interaction with speciÔ¨Åc language data. We also demonstrate that these vectors can be taken as representative of ‚Äòthe kind of person‚Äô they are. We build over 4000 speaker-dependent subcorpora using logs of Wikipedia edits, which are then used to build distributional vectors that represent individual speakers. We show that such ‚Äòperson vectors‚Äô are informative to others, and they inÔ¨Çuence basic patterns of communication like the choice of one‚Äôs interlocutor in conversation. Tested on an informationseeking scenario, where natural language questions must be answered by addressing the most relevant individuals in a community, our system outperforms a standard information retrieval algorithm by a considerable margin. 
We introduce positive-only projection (PoP), a new algorithm for constructing semantic spaces and word embeddings. The PoP method employs random projections. Hence, it is highly scalable and computationally efÔ¨Åcient. In contrast to previous methods that use random projection matrices R with the expected value of 0 (i.e., E(R) = 0), the proposed method uses R with E(R) > 0. We use Kendall‚Äôs œÑb correlation to compute vector similarities in the resulting non-Gaussian spaces. Most importantly, since E(R) > 0, weighting methods such as positive pointwise mutual information (PPMI) can be applied to PoP-constructed spaces after their construction for efÔ¨Åciently transferring PoP embeddings onto spaces that are discriminative for semantic similarity assessments. Our PoP-constructed models, combined with PPMI, achieve an average score of 0.75 in the MEN relatedness test, which is comparable to results obtained by state-of-the-art algorithms. 
Users combine attributes and types to describe and classify entities into categories. These categories are fundamental for organising knowledge in a decentralised way acting as tags and predicates. When searching for entities, categories frequently describes the search query. Considering that users do not know in which terms the categories are expressed, they might query the same concept by a paraphrase. While some categories are composed of simple expressions (e.g. Presidents of Ireland), others have more complex compositional patterns (e.g. French Senators Of The Second Empire). This work proposes a hybrid semantic model based on syntactic analysis, distributional semantics and named entity recognition to recognise paraphrases of entity categories. Our results show that the proposed model outperformed the comparative baseline, in terms of recall and mean reciprocal rank, thus being suitable for addressing the vocabulary gap between user queries and entity categories. 
Givenness (Schwarzschild, 1999) is one of the central notions in the formal pragmatic literature discussing the organization of discourse. In this paper, we explore where distributional semantics can help address the gap between the linguistic insights into the formal pragmatic notion of Givenness and its implementation in computational linguistics. As experimental testbed, we focus on short answer assessment, in which the goal is to assess whether a student response correctly answers the provided reading comprehension question or not. Current approaches only implement a very basic, surface-based perspective on Givenness: A word of the answer that appears as such in the question counts as GIVEN. We show that an approach approximating Givenness using distributional semantics to check whether a word in a sentence is similar enough to a word in the context to count as GIVEN is more successful quantitatively and supports interesting qualitative insights into the data and the limitations of a basic distributional semantic approach identifying Givenness at the lexical level. 
A difÔ¨Åcult task when generating text from knowledge bases (KB) consists in Ô¨Ånding appropriate lexicalisations for KB symbols. We present an approach for lexicalising knowledge base relations and apply it to DBPedia data. Our model learns lowdimensional embeddings of words and RDF resources and uses these representations to score RDF properties against candidate lexicalisations. Training our model using (i) pairs of RDF triples and automatically generated verbalisations of these triples and (ii) pairs of paraphrases extracted from various resources, yields competitive results on DBPedia data. 
In our paper we present our rule-based system for semantic processing. In particular we show examples and solutions that may be challenge our approach. We then discuss problems and shortcomings of Task 2 ‚Äì iSTS. We comment on the existence of a tension between the inherent need to on the one side, to make the task as much as possible ‚Äúsemantically feasible‚Äù. Whereas the detailed presentation and some notes in the guidelines refer to inferential processes, paraphrases and the use of commonsense knowledge of the world for the interpretation to work. We then present results and some conclusions. 
Morpho-syntactic lexicons provide information about the morphological and syntactic roles of words in a language. Such lexicons are not available for all languages and even when available, their coverage can be limited. We present a graph-based semi-supervised learning method that uses the morphological, syntactic and semantic relations between words to automatically construct wide coverage lexicons from small seed sets. Our method is language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing.
Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: reverse dictionaries that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences.
Understanding cross-cultural differences has important implications for world affairs and many aspects of the life of society. Yet, the majority of text-mining methods to date focus on the analysis of monolingual texts. In contrast, we present a statistical model that simultaneously learns a set of common topics from multilingual, non-parallel data and automatically discovers the differences in perspectives on these topics across linguistic communities. We perform a behavioural evaluation of a subset of the differences identified by our model in English and Spanish to investigate their psychological validity.
This paper presents an empirical study of linguistic formality. We perform an analysis of humans{'} perceptions of formality in four different genres. These findings are used to develop a statistical model for predicting formality, which is evaluated under different feature settings and genres. We apply our model to an investigation of formality in online discussion forums, and present findings consistent with theories of formality and linguistic coordination.
Algorithmic decipherment is a prime example of a truly unsupervised problem. The first step in the decipherment process is the identification of the encrypted language. We propose three methods for determining the source language of a document enciphered with a monoalphabetic substitution cipher. The best method achieves 97{\%} accuracy on 380 languages. We then present an approach to decoding anagrammed substitution ciphers, in which the letters within words have been arbitrarily transposed. It obtains the average decryption word accuracy of 93{\%} on a set of 50 ciphertexts in 5 languages. Finally, we report the results on the Voynich manuscript, an unsolved fifteenth century cipher, which suggest Hebrew as the language of the document.
The Tier-based Strictly 2-Local (TSL2) languages are a class of formal languages which have been shown to model long-distance phonotactic generalizations in natural language (Heinz et al., 2011). This paper introduces the Tier-based Strictly 2-Local Inference Algorithm (2TSLIA), the first nonenumerative learner for the TSL2 languages. We prove the 2TSLIA is guaranteed to converge in polynomial time on a data sample whose size is bounded by a constant.
Existing work on domain adaptation for statistical machine translation has consistently assumed access to a small sample from the test distribution (target domain) at training time. In practice, however, the target domain may not be known at training time or it may change to match user needs. In such situations, it is natural to push the system to make safer choices, giving higher preference to domain-invariant translations, which work well across domains, over risky domain-specific alternatives. We encode this intuition by (1) inducing latent subdomains from the training data only; (2) introducing features which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance.
Answer sentence ranking and answer extraction are two key challenges in question answering that have traditionally been treated in isolation, i.e., as independent tasks. In this article, we (1) explain how both tasks are related at their core by a common quantity, and (2) propose a simple and intuitive joint probabilistic model that addresses both via joint computation but task-specific application of that quantity. In our experiments with two TREC datasets, our joint model substantially outperforms state-of-the-art systems in both tasks.
The strongly typed syntax of grammar formalisms such as CCG, TAG, LFG and HPSG offers a synchronous framework for deriving syntactic structures and semantic logical forms. In contrast{---}partly due to the lack of a strong type system{---}dependency structures are easy to annotate and have become a widely used form of syntactic analysis for many languages. However, the lack of a type system makes a formal mechanism for deriving logical forms from dependency structures challenging. We address this by introducing a robust system based on the lambda calculus for deriving neo-Davidsonian logical forms from dependency trees. These logical forms are then used for semantic parsing of natural language to Freebase. Experiments on the Free917 and Web-Questions datasets show that our representation is superior to the original dependency trees and that it outperforms a CCG-based representation on this task. Compared to prior work, we obtain the strongest result to date on Free917 and competitive results on WebQuestions.
We consider the problem of disambiguating concept mentions appearing in documents and grounding them in multiple knowledge bases, where each knowledge base addresses some aspects of the domain. This problem poses a few additional challenges beyond those addressed in the popular Wikification problem. Key among them is that most knowledge bases do not contain the rich textual and structural information Wikipedia does; consequently, the main supervision signal used to train Wikification rankers does not exist anymore. In this work we develop an algorithmic approach that, by carefully examining the relations between various related knowledge bases, generates an indirect supervision signal it uses to train a ranking model that accurately chooses knowledge base entries for a given mention; moreover, it also induces prior knowledge that can be used to support a global coherent mapping of all the concepts in a given document to the knowledge bases. Using the biomedical domain as our application, we show that our indirectly supervised ranking model outperforms other unsupervised baselines and that the quality of this indirect supervision scheme is very close to a supervised model. We also show that considering multiple knowledge bases together has an advantage over grounding concepts to each knowledge base individually.
We introduce a new approach to training a semantic parser that uses textual entailment judgements as supervision. These judgements are based on high-level inferences about whether the meaning of one sentence follows from another. When applied to an existing semantic parsing task, they prove to be a useful tool for revealing semantic distinctions and background knowledge not captured in the target representations. This information is used to improve the quality of the semantic representations being learned and to acquire generic knowledge for reasoning. Experiments are done on the benchmark Sportscaster corpus (Chen and Mooney, 2008), and a novel RTE-inspired inference dataset is introduced. On this new dataset our method strongly outperforms several strong baselines. Separately, we obtain state-of-the-art results on the original Sportscaster semantic parsing task.
The field of grammatical error correction (GEC) has grown substantially in recent years, with research directed at both evaluation metrics and improved system performance against those metrics. One unvisited assumption, however, is the reliance of GEC evaluation on error-coded corpora, which contain specific labeled corrections. We examine current practices and show that GEC{'}s reliance on such corpora unnaturally constrains annotation and automatic evaluation, resulting in (a) sentences that do not sound acceptable to native speakers and (b) system rankings that do not correlate with human judgments. In light of this, we propose an alternate approach that jettisons costly error coding in favor of unannotated, whole-sentence rewrites. We compare the performance of existing metrics over different gold-standard annotations, and show that automatic evaluation with our new annotation scheme has very strong correlation with expert rankings (œÅ = 0.82). As a result, we advocate for a fundamental and necessary shift in the goal of GEC, from correcting small, labeled error types, to producing text that has native fluency.
Transition-based approaches based on local classification are attractive for dependency parsing due to their simplicity and speed, despite producing results slightly below the state-of-the-art. In this paper, we propose a new approach for approximate structured inference for transition-based parsing that produces scores suitable for global scoring using local models. This is accomplished with the introduction of error states in local training, which add information about incorrect derivation paths typically left out completely in locally-trained models. Using neural networks for our local classifiers, our approach achieves 93.61{\%} accuracy for transition-based dependency parsing in English.
We present a new approach for generating role-labeled training data using Linked Lexical Resources, i.e., integrated lexical resources that combine several resources (e.g., Word-Net, FrameNet, Wiktionary) by linking them on the sense or on the role level. Unlike resource-based supervision in relation extraction, we focus on complex linguistic annotations, more specifically FrameNet senses and roles. The automatically labeled training data (www.ukp.tu-darmstadt.de/knowledge-based-srl/) are evaluated on four corpora from different domains for the tasks of word sense disambiguation and semantic role classification. Results show that classifiers trained on our generated data equal those resulting from a standard supervised setting.
Methods for Named Entity Recognition and Disambiguation (NERD) perform NER and NED in two separate stages. Therefore, NED may be penalized with respect to precision by NER false positives, and suffers in recall from NER false negatives. Conversely, NED does not fully exploit information computed by NER such as types of mentions. This paper presents J-NERD, a new approach to perform NER and NED jointly, by means of a probabilistic graphical model that captures mention spans, mention types, and the mapping of mentions to entities in a knowledge base. We present experiments with different kinds of texts from the CoNLL{'}03, ACE{'}05, and ClueWeb{'}09-FACC1 corpora. J-NERD consistently outperforms state-of-the-art competitors in end-to-end NERD precision, recall, and F1.
We present a method for unsupervised open-domain relation discovery. In contrast to previous (mostly generative and agglomerative clustering) approaches, our model relies on rich contextual features and makes minimal independence assumptions. The model is composed of two parts: a feature-rich relation extractor, which predicts a semantic relation between two entities, and a factorization model, which reconstructs arguments (i.e., the entities) relying on the predicted relation. The two components are estimated jointly so as to minimize errors in recovering arguments. We study factorization models inspired by previous work in relation factorization and selectional preference modeling. Our models substantially outperform the generative and agglomerative-clustering counterparts and achieve state-of-the-art performance.
We tackle unsupervised part-of-speech (POS) tagging by learning hidden Markov models (HMMs) that are particularly well-suited for the problem. These HMMs, which we call anchor HMMs, assume that each tag is associated with at least one word that can have no other tag, which is a relatively benign condition for POS tagging (e.g., {``}the{''} is a word that appears only under the determiner tag). We exploit this assumption and extend the non-negative matrix factorization framework of Arora et al. (2013) to design a consistent estimator for anchor HMMs. In experiments, our algorithm is competitive with strong baselines such as the clustering method of Brown et al. (1992) and the log-linear model of Berg-Kirkpatrick et al. (2010). Furthermore, it produces an interpretable model in which hidden states are automatically lexicalized by words.
How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence{'}s representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks. We release code at: https://github.com/yinwenpeng/Answer{\_}Selection.
Continuous word representations have been remarkably useful across NLP tasks but remain poorly understood. We ground word embeddings in semantic spaces studied in the cognitive-psychometric literature, taking these spaces as the primary objects to recover. To this end, we relate log co-occurrences of words in large corpora to semantic similarity assessments and show that co-occurrences are indeed consistent with an Euclidean semantic space hypothesis. Framing word embedding as metric recovery of a semantic space unifies existing word embedding algorithms, ties them to manifold learning, and demonstrates that existing algorithms are consistent metric recovery methods given co-occurrence counts from random walks. Furthermore, we propose a simple, principled, direct metric recovery algorithm that performs on par with the state-of-the-art word embedding and manifold learning methods. Finally, we complement recent focus on analogies by constructing two new inductive reasoning datasets{---}series completion and classification{---}and demonstrate that word embeddings can be used to solve them as well.
Rule-based stemmers such as the Porter stemmer are frequently used to preprocess English corpora for topic modeling. In this work, we train and evaluate topic models on a variety of corpora using several different stemming algorithms. We examine several different quantitative measures of the resulting models, including likelihood, coherence, model stability, and entropy. Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability.
We propose a novel approach to cross-lingual part-of-speech tagging and dependency parsing for truly low-resource languages. Our annotation projection-based approach yields tagging and parsing models for over 100 languages. All that is needed are freely available parallel texts, and taggers and parsers for resource-rich languages. The empirical evaluation across 30 test languages shows that our method consistently provides top-level accuracies, close to established upper bounds, and outperforms several competitive baselines.
We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.
We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models. The addition of skip-gram features yields a model that is in the same league as the state-of-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark. On the Gigaword corpus further improvements are observed using features that cross sentence boundaries. The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data.
The growing work in multi-lingual parsing faces the challenge of fair comparative evaluation and performance analysis across languages and their treebanks. The difficulty lies in teasing apart the properties of treebanks, such as their size or average sentence length, from those of the annotation scheme, and from the linguistic properties of languages. We propose a method to evaluate the effects of word order of a language on dependency parsing performance, while controlling for confounding treebank properties. The method uses artificially-generated treebanks that are minimal permutations of actual treebanks with respect to two word order properties: word order variation and dependency lengths. Based on these artificial data on twelve languages, we show that longer dependencies and higher word order variability degrade parsing performance. Our method also extends to minimal pairs of individual sentences, leading to a finer-grained understanding of parsing errors.
Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.
Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT{'}14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT{'}14 English-to-German task.
Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.
Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.
Canonical correlation analysis (CCA) is a method for reducing the dimension of data represented using two views. It has been previously used to derive word embeddings, where one view indicates a word, and the other view indicates its context. We describe a way to incorporate prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets.
We train one multilingual model for dependency parsing and use it to parse sentences in several languages. The parsing model uses (i) multilingual word clusters and embeddings; (ii) token-level language information; and (iii) language-specific features (fine-grained POS tags). This input representation enables the parser not only to parse effectively in multiple languages, but also to generalize across languages based on linguistic universals and typological similarities, making it more effective to learn from limited annotations. Our parser{'}s performance compares favorably to strong baselines in a range of data scenarios, including when the target language has a large treebank, a small treebank, or no treebank for training.
We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving very strong accuracies for English and Chinese, without relying on external word embeddings. The parser{'}s implementation is available for download at the first author{'}s webpage.
Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.
Efficient methods for storing and querying are critical for scaling high-order m-gram language models to large corpora. We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500{\mbox{$\times$}}, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying).
We release Galactic Dependencies 1.0{---}a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a {``}nearby{''} source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.
We propose two models for verbalizing numbers, a key component in speech recognition and synthesis systems. The first model uses an end-to-end recurrent neural network. The second model, drawing inspiration from the linguistics literature, uses finite-state transducers constructed with a minimal amount of training data. While both models achieve near-perfect performance, the latter model can be trained using several orders of magnitude less data than the former, making it particularly useful for low-resource languages.
The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture{'}s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1{\%} errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.
Automatic satire detection is a subtle text classification task, for machines and at times, even for humans. In this paper we argue that satire detection should be approached using common-sense inferences, rather than traditional text classification methods. We present a highly structured latent variable model capturing the required inferences. The model abstracts over the specific entities appearing in the articles, grouping them into generalized categories, thus allowing the model to adapt to previously unseen situations.
Taxonomies play an important role in many applications by organizing domain knowledge into a hierarchy of {`}is-a{'} relations between terms. Previous work on automatic construction of taxonomies from text documents either ignored temporal information or used fixed time periods to discretize the time series of documents. In this paper, we propose a time-aware method to automatically construct and effectively maintain a taxonomy from a given series of documents preclustered for a domain of interest. The method extracts temporal information from the documents and uses a timestamp contribution function to score the temporal relevance of the evidence from source texts when identifying the taxonomic relations for constructing the taxonomy. Experimental results show that our proposed method outperforms the state-of-the-art methods by increasing F-measure up to 7{\%}{--}20{\%}. Furthermore, the proposed method can incrementally update the taxonomy by adding fresh relations from new data and removing outdated relations using an information decay function. It thus avoids rebuilding the whole taxonomy from scratch for every update and keeps the taxonomy effectively up-to-date in order to track the latest information trends in the rapidly evolving domain.
Natural Language Processing has broadened in scope to tackle more and more challenging language understanding and reasoning tasks. The core NLP tasks remain predominantly unimodal, focusing on linguistic input, despite the fact that we, humans, acquire and use language while communicating in perceptually rich environments. Moving towards human-level AI will require the integration and modeling of multiple modalities beyond language. With this tutorial, our aim is to introduce researchers to the areas of NLP that have dealt with multimodal signals. The key advantage of using multimodal signals in NLP tasks is the complementarity of the data in different modalities. For example, we are less likely to nd descriptions of yellow bananas or wooden chairs in text corpora, but these visual attributes can be readily extracted directly from images. Multimodal signals, such as visual, auditory or olfactory data, have proven useful for models of word similarity and relatedness, automatic image and video description, and even predicting the associated smells of words. Finally, multimodality offers a practical opportunity to study and apply multitask learning, a general machine learning paradigm that improves generalization performance of a task by using training signals of other related tasks.All material associated to the tutorial will be available at http://multimodalnlp.github.io/
Argumentation and debating represent primary intellectual activities of the human mind. People in all societies argue and debate, not only to convince others of their own opinions but also in order to explore the differences between multiple perspectives and conceptualizations, and to learn from this exploration. The process of reaching a resolution on controversial topics typically does not follow a simple sequence of purely logical steps. Rather it involves a wide variety of complex and interwoven actions. Presumably, pros and cons are identified, considered, and weighed, via cognitive processes that often involve persuasion and emotions, which are inherently harder to formalize from a computational perspective.This wide range of conceptual capabilities and activities, have only in part been studied in fields like CL and NLP, and typically within relatively small sub-communities that overlap the ACL audience. The new field of Computational Argumentation has very recently seen significant expansion within the CL and NLP community as new techniques and datasets start to become available, allowing for the first time investigation of the computational aspects of human argumentation in a holistic manner.The main goal of this tutorial would be to introduce this rapidly evolving field to the CL community. Specifically, we will aim to review recent advances in the field and to outline the challenging research questions - that are most relevant to the ACL audience - that naturally arise when trying to model human argumentation.We will further emphasize the practical value of this line of study, by considering real-world CL and NLP applications that are expected to emerge from this research, and to impact various industries, including legal, finance, healthcare, media, and education, to name just a few examples.The first part of the tutorial will provide introduction to the basics of argumentation and rhetoric. Next, we will cover fundamental analysis tasks in Computational Argumentation, including argumentation mining, revealing argument relations, assessing arguments quality, stance classification, polarity analysis, and more. After the coffee break, we will first review existing resources and recently introduced benchmark data. In the following part we will cover basic synthesis tasks in Computational Argumentation, including the relation to NLG and dialogue systems, and the evolving area of Debate Technologies, defined as technologies developed directly to enhance, support, and engage with human debating. Finally, we will present relevant demos, review potential applications, and discuss the future of this emerging field.
Moving beyond post-editing machine translation, a number of recent research efforts have advanced computer aided translation methods that allow for more interactivity, richer information such as confidence scores, and the completed feedback loop of instant adaptation of machine translation models to user translations.This tutorial will explain the main techniques for several aspects of computer aided translation: confidence measures;interactive machine translation (interactive translation prediction);bilingual concordancers;translation option display;paraphrasing (alternative translation suggestions);visualization of word alignment;online adaptation;automatic reviewing;integration of translation memory;eye tracking, logging, and cognitive user models;For each of these, the state of the art and open challenges are presented. The tutorial will also look under the hood of the open source CASMACAT toolkit that is based on MATECAT, and available as a ``Home Edition'' to be installed on a desktop machine. The target audience of this tutorials are researchers interested in computer aided machine translation and practitioners who want to use or deploy advanced CAT technology.
Representing the semantics of linguistic items in a machine {\-}interpretable form has been a major goal of Natural Language Processing since its earliest days. Among the range of different linguistic items, words have attracted the most research attention. However, word representations have an important limitation: they conflate different meanings of a word into a single vector. Representations of word senses have the potential to overcome this inherent limitation. Indeed, the representation of individual word senses and concepts has recently gained in popularity with several experimental results showing that a considerable performance improvement can be achieved across different NLP applications upon moving from word level to the deeper sense and concept levels. Another interesting point regarding the representation of concepts and word senses is that these models can be seamlessly applied to other linguistic items, such as words, phrases, sentences, etc.This tutorial will first provide a brief overview of the recent literature concerning word representation (both count based and neural network based). It will then describe the advantages of moving from the word level to the deeper level of word senses and concepts, providing an extensive review of state {\-}of {\-}the {\-}art systems. Approaches covered will not only include those which draw upon knowledge resources such as WordNet, Wikipedia, BabelNet or FreeBase as reference, but also the so {\-}called multi {\-}prototype approaches which learn sense distinctions by using different clustering techniques. Our tutorial will discuss the advantages and potential limitations of all approaches, showing their most successful applications to date. We will conclude by presenting current open problems and lines of future work.
Neural Machine Translation (NMT) is a simple new architecture for getting machines to learn to translate. Despite being relatively new (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), NMT has already shown promising results, achieving state-of-the-art performances for various language pairs (Luong et al, 2015a; Jean et al, 2015; Luong et al, 2015b; Sennrich et al., 2016; Luong and Manning, 2016). While many of these NMT papers were presented to the ACL community, research and practice of NMT are only at their beginning stage. This tutorial would be a great opportunity for the whole community of machine translation and natural language processing to learn more about a very promising new approach to MT. This tutorial has four parts.In the first part, we start with an overview of MT approaches, including: (a) traditional methods that have been dominant over the past twenty years and (b) recent hybrid models with the use of neural network components. From these, we motivate why an end-to-end approach like neural machine translation is needed. The second part introduces a basic instance of NMT. We start out with a discussion of recurrent neural networks, including the back-propagation-through-time algorithm and stochastic gradient descent optimizers, as these are the foundation on which NMT builds. We then describe in detail the basic sequence-to-sequence architecture of NMT (Cho et al., 2014; Sutskever et al., 2014), the maximum likelihood training approach, and a simple beam-search decoder to produce translations.The third part of our tutorial describes techniques to build state-of-the-art NMT. We start with approaches to extend the vocabulary coverage of NMT (Luong et al., 2015a; Jean et al., 2015; Chitnis and DeNero, 2015). We then introduce the idea of jointly learning both translations and alignments through an attention mechanism (Bahdanau et al., 2015); other variants of attention (Luong et al., 2015b; Tu et al., 2016) are discussed too. We describe a recent trend in NMT, that is to translate at the sub-word level (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2016), so that language variations can be effectively handled. We then give tips on training and testing NMT systems such as batching and ensembling. In the final part of the tutorial, we briefly describe promising approaches, such as (a) how to combine multiple tasks to help translation (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Zoph and Knight, 2016) and (b) how to utilize monolingual corpora (Sennrich et al., 2016). Lastly, we conclude with challenges remained to be solved for future NMT.PS: we would also like to acknowledge the very first paper by Forcada and {\~N}eco (1997) on sequence-to-sequence models for translation!
The development of game theory in the early 1940's by John von Neumann was a reaction against the then dominant view that problems in economic theory can be formulated using standard methods from optimization theory. Indeed, most real-world economic problems involve conflicting interactions among decision-making agents that cannot be adequately captured by a single (global) objective function. The main idea behind game theory is to shift the emphasis from optimality criteria to equilibrium conditions. Game theory provides a framework to model complex scenarios, with applications in economics and social science but also in different fields of information technology. With the recent development of algorithmic game theory, it has been used to solve problems in computer vision, pattern recognition, machine learning and natural language processing.Game-theoretic frameworks have been used in different ways to study language origin and evolution. Furthermore, the so-called game metaphor has been used by philosophers and linguists to explain how language evolved and how it works. Ludwig Wittgenstein, for example, famously introduced the concept of a language game to explain the conventional nature of language, and put forward the idea of the spontaneous formation of a common language that gradually emerges from the interactions among the speakers within a population.This concept opens the way to the interpretation of language as a complex adaptive system composed of linguistic units and their interactions, which gives rise to the emergence of structural properties. It is the core part of many computational models of language that are based on classical game theory and evolutionary game theory. With the former it is possible to model how speakers form a signaling system in which the ambiguity of the symbols is minimized; with the latter it is possible to model how speakers coordinate their linguistic choices according to the satisfaction that they have about the outcome of a communication act, converging to a common language. In the same vein, many other attempts have been proposed to explain how other characteristics of language follow similar dynamics.Game theory, and in particular evolutionary game theory, thanks to their ability to model interactive situations and to integrate information from multiple sources, have also been used to solve specific problems in natural language processing and information retrieval, such as language generation, word sense disambiguation and document and text clustering.The goal of this tutorial is to offer an introduction to the basic concepts of game theory and to show its main applications in the study of language, from different perspectives. We shall assume no pre-existing knowledge of game theory by the audience, thereby making the tutorial self-contained and understandable by a non-expert.
Billions of short texts are produced every day, in the form of search queries, ad keywords, tags, tweets, messenger conversations, social network posts, etc. Unlike documents, short texts have some unique characteristics which make them difficult to handle. First, short texts, especially search queries, do not always observe the syntax of a written language. This means traditional NLP techniques, such as syntactic parsing, do not always apply to short texts. Second, short texts contain limited context. The majority of search queries contain less than 5 words, and tweets can have no more than 140 characters. Because of the above reasons, short texts give rise to a significant amount of ambiguity, which makes them extremely difficult to handle. On the other hand, many applications, including search engines, ads, automatic question answering, online advertising, recommendation systems, etc., rely on short text understanding. In all these applications, the necessary first step is to transform an input text into a machine-interpretable representation, namely to ``understand'' the short text. A growing number of approaches leverage external knowledge to address the issue of inadequate contextual information that accompanies the short texts. These approaches can be classified into two categories: Explicit Representation Model (ERM) and Implicit Representation Model (IRM). In this tutorial, we will present a comprehensive overview of short text understanding based on explicit semantics (knowledge graph representation, acquisition, and reasoning) and implicit semantics (embedding and deep learning). Specifically, we will go over various techniques in knowledge acquisition, representation, and inferencing has been proposed for text understanding, and we will describe massive structured and semi-structured data that have been made available in the recent decade that directly or indirectly encode human knowledge, turning the knowledge representation problems into a computational grand challenge with feasible solutions insight.
The ubiquity of metaphor in language (Lakoff and Johnson 1980) has served as impetus for cognitive linguistic approaches to the study of language, mind, and the study of mind (e.g. Thibodeau {\&} Boroditsky 2011). While native speakers use metaphor naturally and easily, the treatment and interpretation of metaphor in computational systems remains challenging because such systems have not succeeded in developing ways to recognize the semantic elements that define metaphor. This tutorial demonstrates MetaNet's frame-based semantic analyses, and their informing of MetaNet's automatic metaphor identification system. Participants will gain a complete understanding of the theoretical basis and the practical workings of MetaNet, and acquire relevant information about the Frame Semantics basis of that knowledge base and the way that FrameNet handles the widespread phenomenon of metaphor in language. The tutorial is geared to researchers and practitioners of language technology, not necessarily experts in metaphor analysis or knowledgeable about either FrameNet or MetaNet, but who are interested in natural language processing tasks that involve automatic metaphor processing, or could benefit from exposure to tools and resources that support frame-based deep semantic, analyses of language, including metaphor as a widespread phenomenon in human language.
 Semantic role labeling (SRL) identiÔ¨Åes the predicate-argument structure in text with semantic labels. It plays a key role in understanding natural language. In this paper, we present POLYGLOT, a multilingual semantic role labeling system capable of semantically parsing sentences in 9 different languages from 4 different language groups. The core of POLYGLOT are SRL models for individual languages trained with automatically generated Proposition Banks (Akbik et al., 2015). The key feature of the system is that it treats the semantic labels of the English Proposition Bank as ‚Äúuniversal semantic labels‚Äù: Given a sentence in any of the supported languages, POLYGLOT applies the corresponding SRL and predicts English PropBank frame and role annotation. The results are then visualized to facilitate the understanding of multilingual SRL with this uniÔ¨Åed semantic representation. 
The reading material used in a language learning classroom should ideally be rich in terms of the grammatical constructions and vocabulary to be taught and in line with the learner‚Äôs interests. We developed an online Information Retrieval system that helps teachers search for texts appropriate in form, content, and reading level. It identiÔ¨Åes the 87 grammatical constructions spelled out in the ofÔ¨Åcial English language curriculum of schools in Baden-Wu¬®rttemberg, Germany. The tool incorporates a classical efÔ¨Åcient algorithm for reranking the results by assigning weights to selected constructions and prioritizing the documents containing them. Supplemented by an interactive visualization module, it allows for a multifaceted presentation and analysis of the retrieved documents. 
We introduce, TermSuite, a JAVA and UIMA-based toolkit to build terminologies from corpora. TermSuite follows the classic two steps of terminology extraction tools, the identiÔ¨Åcation of term candidates and their ranking, but implements new features. It is multilingually designed, scalable, and handles term variants. We focus on the main components: UIMA Tokens Regex for deÔ¨Åning term and variant patterns over word annotations, and the grouping component for clustering terms and variants that works both at morphological and syntactic levels. 
Despite the abundance of biomedical literature and health discussions in online communities, it is often tedious to retrieve informative contents for health-centric information needs. Users can query scholarly work in PubMed by keywords and MeSH terms, and resort to Google for everything else. This demo paper presents the DeepLife system, to overcome the limitations of existing search engines for life science and health topics. DeepLife integrates large knowledge bases and harnesses entity linking methods, to support search and exploration of scientiÔ¨Åc literature, newspaper feeds, and social media, in terms of keywords and phrases, biomedical entities, and taxonomic categories. It also provides functionality for entityaware text analytics over health-centric contents. 
Publicly available knowledge repositories, such as Wikipedia and Freebase, beneÔ¨Åt signiÔ¨Åcantly from volunteers, whose contributions ensure that the knowledge keeps expanding and is kept up-to-date and accurate. User interactions are often limited to hypertext, tabular, or graph visualization interfaces. For spatio-temporal information, however, other interaction paradigms may be better-suited. We present an integrated system that combines crowdsourcing, automatic or semi-automatic knowledge harvesting from text, and visual analytics. It enables users to analyze large quantities of structured data and unstructured textual data from a spatio-temporal perspective and gain deep insights that are not easily observed in individual facts. 
The current release of the ODIN (Online Database of Interlinear Text) database contains over 150,000 linguistic examples, from nearly 1,500 languages, extracted from PDFs found on the web, representing a signiÔ¨Åcant source of data for language research, particularly for low-resource languages. Errors introduced during PDF-totext conversion or poorly formatted examples can make the task of automatically analyzing the data more difÔ¨Åcult, so we aim to clean and normalize the examples in order to maximize accuracy during analysis. In this paper we describe a system that allows users to automatically and manually correct errors in the source data in order to get the best possible analysis of the data. We also describe a RESTful service for managing collections of linguistic examples on the web. All software is distributed under an open-source license. 
Monitoring mobility- and industryrelevant events is important in areas such as personal travel planning and supply chain management, but extracting events pertaining to speciÔ¨Åc companies, transit routes and locations from heterogeneous, high-volume text streams remains a signiÔ¨Åcant challenge. We present Spree, a scalable system for real-time, automatic event extraction from social media, news and domain-speciÔ¨Åc RSS feeds. Our system is tailored to a range of mobilityand industry-related events, and processes German texts within a distributed linguistic analysis pipeline implemented in Apache Flink. The pipeline detects and disambiguates highly ambiguous domain-relevant entities, such as street names, and extracts various events with their geo-locations. Event streams are visualized on a dynamic, interactive map for monitoring and analysis. 
We present TranscRater, an open-source tool for automatic speech recognition (ASR) quality estimation (QE). The tool allows users to perform ASR evaluation bypassing the need of reference transcripts and conÔ¨Ådence information, which is common to current assessment protocols. TranscRater includes: i) methods to extract a variety of quality indicators from (signal, transcription) pairs and ii) machine learning algorithms which make possible to build ASR QE models exploiting the extracted features. ConÔ¨Årming the positive results of previous evaluations, new experiments with TranscRater indicate its effectiveness both in WER prediction and transcription ranking tasks. 
We present TMop, the Ô¨Årst open-source tool for automatic Translation Memory (TM) cleaning. The tool implements a fully unsupervised approach to the task, which allows spotting unreliable translation units (sentence pairs in different languages, which are supposed to be translations of each other) without requiring labeled training data. TMop includes a highly conÔ¨Ågurable and extensible set of Ô¨Ålters capturing different aspects of translation quality. It has been evaluated on a test set composed of 1,000 translation units (TUs) randomly extracted from the English-Italian version of MyMemory, a large-scale public TM. Results indicate its effectiveness in automatic removing ‚Äúbad‚Äù TUs, with comparable performance to a state-of-the-art supervised method (76.3 vs. 77.7 balanced accuracy). 
Research at the intersection of language and other modalities, most notably vision, is becoming increasingly important in natural language processing. We introduce a toolkit that can be used to obtain feature representations for visual and auditory information. MMFEAT is an easy-to-use Python toolkit, which has been developed with the purpose of making non-linguistic modalities more accessible to natural language processing researchers. 
FREEBASE contains entities and relation information but is highly incomplete. Relevant information is ubiquitous in web text, but extraction deems challenging. We present JEDI, an automated system to jointly extract typed named entities and FREEBASE relations using dependency pattern from text. An innovative method for constraint solving on entity types of multiple relations is used to disambiguate pattern. The high precision in the evaluation supports our claim that we can detect entities and relations together, alleviating the need to train a custom classiÔ¨Åer for an entity type1. 
We present a new release of OpenDial, an open-source toolkit for building and evaluating spoken dialogue systems. The toolkit relies on an information-state architecture where the dialogue state is represented as a Bayesian network and acts as a shared memory for all system modules. The domain models are speciÔ¨Åed via probabilistic rules encoded in XML. OpenDial has been deployed in several application domains such as human‚Äìrobot interaction, intelligent tutoring systems and multi-modal in-car driver assistants. 
The MUSEEC (MUltilingual SEntence Extraction and Compression) summarization tool implements several extractive summarization techniques ‚Äì at the level of complete and compressed sentences ‚Äì that can be applied, with some minor adaptations, to documents in multiple languages. The current version of MUSEEC provides the following summarization methods: (1) MUSE ‚Äì a supervised summarizer, based on a genetic algorithm (GA), that ranks document sentences and extracts top‚Äìranking sentences into a summary, (2) POLY ‚Äì an unsupervised summarizer, based on linear programming (LP), that selects the best extract of document sentences, and (3) WECOM ‚Äì an unsupervised extension of POLY that compiles a document summary from compressed sentences. In this paper, we provide an overview of MUSEEC methods and its architecture in general. 
Current education standards in the U.S. require school students to read and understand complex texts from different subject areas (e.g., social studies). However, such texts usually contain Ô¨Ågurative language, complex phrases and sentences, as well as unfamiliar discourse relations. This may present an obstacle to students whose native language is not English ‚Äî a growing sub-population in the US. 1 One way to help such students is to create classroom activities centered around linguistic elements found in subject area texts (DelliCarpini, 2008). We present a web-based tool that uses NLP algorithms to automatically generate customizable linguistic activities that are grounded in language learning research. 
We demonstrate a simple and easy-to-use system to produce logical semantic representations of sentences. Our software operates by composing semantic formulas bottom-up given a CCG parse tree. It uses Ô¨Çexible semantic templates to specify semantic patterns. Templates for English and Japanese accompany our software, and they are easy to understand, use and extend to cover other linguistic phenomena or languages. We also provide scripts to use our semantic representations in a textual entailment task, and a visualization tool to display semantically augmented CCG trees in HTML. 
META is developed to unite machine learning, information retrieval, and natural language processing in one easy-to-use toolkit. Its focus on indexing allows it to perform well on large datasets, supporting online classiÔ¨Åcation and other out-of-core algorithms. META‚Äôs liberal open source license encourages contributions, and its extensive online documentation, forum, and tutorials make this process straightforward. We run experiments and show META‚Äôs performance is competitive with or better than existing software. 
In this paper, we present MDSWriter, a novel open-source annotation tool for creating multi-document summarization corpora. A major innovation of our tool is that we divide the complex summarization task into multiple steps which enables us to efÔ¨Åciently guide the annotators, to store all their intermediate results, and to record user‚Äìsystem interaction data. This allows for evaluating the individual components of a complex summarization system and learning from the human writing process. MDSWriter is highly Ô¨Çexible and can be adapted to various other tasks. 
We present Jigg, a Scala (or JVMbased) NLP annotation pipeline framework, which is easy to use and is extensible. Jigg supports a very simple interface similar to Stanford CoreNLP, the most successful NLP pipeline toolkit, but has more Ô¨Çexibility to adapt to new types of annotation. On this framework, system developers can easily integrate their downstream system into a NLP pipeline from a raw text by just preparing a wrapper of it. 
In our media-driven world the perception of companies and institutions in the media is of major importance. The creation of press reviews analyzing the media response to company-related events is a complex and time-consuming task. In this demo we present a system that combines advanced text mining and machine learning approaches in an extensible press review system. The system collects documents from heterogeneous sources and enriches the documents applying different mining, Ô¨Åltering, classiÔ¨Åcation, and aggregation algorithms. We present a system tailored to the needs of the press department of a major German University. We explain how the different components have been trained and evaluated. The system enables us demonstrating the live analyzes of news and social media streams as well as the strengths of advanced text mining algorithms for creating a comprehensive media analysis. 
We present a computer-assisted language learning (CALL) system that generates Ô¨Åll-in-the-blank items for preposition usage. The system takes a set of carrier sentences as input, chooses a preposition in each sentence as the key, and then automatically generates distractors. It personalizes item selection for the user in two ways. First, it logs items to which the user previously gave incorrect answers, and offers similar items in a future session as review. Second, it progresses from easier to harder sentences, to minimize any hindrance on preposition learning that might be posed by difÔ¨Åcult vocabulary. 
This paper presents a conversational, multimedia, virtual science tutor for elementary school students. It is built using state of the art speech recognition and spoken language understanding technology. This virtual science tutor is unique in that it elicits self-explanations from students for various science phenomena by engaging them in spoken dialogs and guided by illustrations, animations and interactive simulations. There is a lot of evidence that self-explanation works well as a tutorial paradigm, Summative evaluations indicate that students are highly engaged in the tutoring sessions, and achieve learning outcomes equivalent to expert human tutors. Tutorials are developed through a process of recording and annotating data from sessions with students, and then updating tutor models. It enthusiastically supported by students and teachers. Teachers report that it is feasible to integrate into their curriculum. 
We present pigeo, a Python geolocation prediction tool that predicts a location for a given text input or Twitter user. We discuss the design, implementation and application of pigeo, and empirically evaluate it. pigeo is able to geolocate informal text and is a very useful tool for users who require a free and easy-to-use, yet accurate geolocation service based on pre-trained models. Additionally, users can train their own models easily using pigeo‚Äôs API. 
We present a prototype of a novel technology for second language instruction. Our learn-by-reading approach lets a human learner acquire new words and constructions by encountering them in context. To facilitate reading comprehension, our technology presents mixed native language (L1) and second language (L2) sentences to a learner and allows them to interact with the sentences to make the sentences easier (more L1-like) or harder (more L2-like) to read. Eventually, our system should continuously track a learner‚Äôs knowledge and learning style by modeling their interactions, including performance on a pop quiz feature. This will allow our system to generate personalized mixed-language texts for learners. 
In this paper, we present Roleo, a web tool for visualizing the vector spaces generated by the evaluation of distributional memory (DM) models over thematic Ô¨Åt judgements. A thematic Ô¨Åt judgement is a rating of the selectional preference of a verb for an argument that Ô¨Ålls a given thematic role. The DM approach to thematic Ô¨Åt judgements involves the construction of a sub-space in which a prototypical role-Ô¨Åller can be built for comparison to the noun being judged. We describe a publicly-accessible web tool that allows for querying and exploring these spaces as well as a technique for visualizing thematic Ô¨Åt sub-spaces efÔ¨Åciently for web use. 
We introduce MediaGist, an online system for crosslingual analysis of aggregated news and commentaries based on summarization and sentiment analysis technologies. It is designed to assist journalists to detect and explore news topics, which are controversially reported or discussed in different countries. News articles from current week are clustered separately in currently 5 languages and the clusters are then linked across languages. Sentiment analysis provides a basis to compute controversy scores and summaries help to explore the differences. Recognized entities play an important role in most of the system‚Äôs modules and provide another way to explore the data. We demonstrate the capabilities of MediaGist by listing highlights from the last week and present a rough evaluation of the system. 
We introduce GoWvis1, an interactive web application that represents any piece of text inputted by the user as a graph-ofwords and leverages graph degeneracy and community detection to generate an extractive summary (keyphrases and sentences) of the inputted text in an unsupervised fashion. The entire analysis can be fully customized via the tuning of many text preprocessing, graph building, and graph mining parameters. Our system is thus well suited to educational purposes, exploration and early research experiments. The new summarization strategy we propose also shows promise. 
We present a robust and efÔ¨Åcient parallelizable multilingual UIMA-based platform for automatically annotating textual inputs with different layers of linguistic description, ranging from surface level phenomena all the way down to deep discourse-level information. In particular, given an input text, the pipeline extracts: sentences and tokens; entity mentions; syntactic information; opinionated expressions; relations between entity mentions; co-reference chains and wikiÔ¨Åed entities. The system is available in two versions: a standalone distribution enables design and optimization of userspeciÔ¨Åc sub-modules, whereas a server-client distribution allows for straightforward highperformance NLP processing, reducing the engineering cost for higher-level tasks. 
We present new/s/leak, a novel tool developed for and with the help of journalists, which enables the automatic analysis and discovery of newsworthy stories from large textual datasets. We rely on different NLP preprocessing steps such named entity tagging, extraction of time expressions, entity networks, relations and metadata. The system features an intuitive web-based user interface based on network visualization combined with data exploring methods and various search and faceting mechanisms. We report the current state of the software and exemplify it with the WikiLeaks PlusD (Cablegate) data. 
We propose a new dataset for evaluating a Japanese lexical simpliÔ¨Åcation method. Previous datasets have several deÔ¨Åciencies. All of them substitute only a single target word, and some of them extract sentences only from newswire corpus. In addition, most of these datasets do not allow ties and integrate simpliÔ¨Åcation ranking from all the annotators without considering the quality. In contrast, our dataset has the following advantages: (1) it is the Ô¨Årst controlled and balanced dataset for Japanese lexical simpliÔ¨Åcation with high correlation with human judgment and (2) the consistency of the simpliÔ¨Åcation ranking is improved by allowing candidates to have ties and by considering the reliability of annotators. 
A hierarchical word alignment model that searches for k-best partial alignments on target constituent 1-best parse trees has been shown to outperform previous models. However, relying solely on 1-best parses trees might hinder the search for good alignments because 1-best trees are not necessarily the best for word alignment tasks in practice. This paper introduces a dependency forest based word alignment model, which utilizes target dependency forests in an attempt to minimize the impact on limitations attributable to 1-best parse trees. We present how k-best alignments are constructed over target-side dependency forests. Alignment experiments on the Japanese-English language pair show a relative error reduction of 4% of the alignment score compared to a model with 1-best parse trees. 
Adverse drug events (ADEs) are medical complications co-occurring with a period of drug usage. IdentiÔ¨Åcation of ADEs is a primary way of evaluating available quality of care. As more social media users begin discussing their drug experiences online, public data becomes available for researchers to expand existing electronic ADE reporting systems, though non-standard language inhibits ease of analysis. In this study, portions of a new corpus of approximately 160,000 tweets were used to create a lexicon-driven ADE detection system using semi-supervised, pattern-based bootstrapping. This method was able to identify misspellings, slang terms, and other non-standard language features of social media data to drive a competitive ADE detection system. 
Quantitative analysis of human brain activity based on language representations, such as the semantic categories of words, have been actively studied in the Ô¨Åeld of brain and neuroscience. Our study aims to generate natural language descriptions for human brain activation phenomena caused by visual stimulus by employing deep learning methods, which have gained interest as an effective approach to automatically describe natural language expressions for various type of multi-modal information, such as images. We employed an image-captioning system based on a deep learning framework as the basis for our method by learning the relationship between the brain activity data and the features of an intermediate expression of the deep neural network owing to lack of training brain data. We conducted three experiments and were able to generate natural language sentences which enabled us to quantitatively interpret brain activity. 
Works on Twitter community detection have yielded new ways to extract valuable insights from social media. Through this technique, Twitter users can be grouped into different types of communities such as those who have the same interests, those who interact a lot, or those who have similar sentiments about certain topics. Computationally, information is represented as a graph, and community detection is the problem of partitioning the graph such that each community is more densely connected to each other than to the rest of the network. It has been shown that incorporating sentiment analysis can improve community detection when looking for sentiment-based communities. However, such works only perform sentiment analysis in isolation without considering the tweet‚Äôs various contextual information. Examples of these contextual information are social network structure, and conversational, author, and topic contexts. Disregarding these information poses a problem because at times, context is needed to clearly infer the sentiment of a tweet. Thus, this research aims to improve detection of sentiment-based communities on Twitter by performing contextual sentiment analysis. 
This paper evaluates the challenges involved in shallow parsing of Dravidian languages which are highly agglutinative and morphologically rich. Text processing tasks in these languages are not trivial because multiple words concatenate to form a single string with morpho-phonemic changes at the point of concatenation. This phenomenon known as Sandhi, in turn complicates the individual word identiÔ¨Åcation. Shallow parsing is the task of identiÔ¨Åcation of correlated group of words given a raw sentence. The current work is an attempt to study the effect of Sandhi in building shallow parsers for Dravidian languages by evaluating its effect on Malayalam, one of the main languages from Dravidian family. We provide an in-depth analysis of effect of Sandhi in developing a robust shallow parser pipeline with experimental results emphasizing on how sensitive the individual components of shallow parser are, towards the accuracy of a sandhi splitter. Our work can serve as a guiding light for building robust text processing systems in Dravidian languages. 
Online newspaper articles can accumulate comments at volumes that prevent close reading. Summarisation of the comments allows interaction at a higher level and can lead to an understanding of the overall discussion. Comment summarisation requires topic clustering, comment ranking and extraction. Clustering must be robust as the subsequent extraction relies on a good set of clusters. Comment data, as with many social media datasets, contains very short documents and the number of words in the documents is a limiting factors on the performance of LDA clustering. We evaluate whether we can combine comments to form larger documents to improve the quality of clusters. We Ô¨Ånd that combining comments with comments that reply to them produce the highest quality clusters. 
 In this work we explore some challenges related to analysing one form of the Arabic language called Arabizi. Arabizi, a portmanteau of Araby-Englizi, meaning Arabic-English, is a digital trend in texting Non-Standard Arabic using Latin script. Arabizi users express their natural dialectal Arabic in text without following a uniÔ¨Åed orthography. We address the challenge of identifying Arabizi from multi-lingual data in Twitter, a preliminary step for analysing sentiment from Arabizi data. We annotated a corpus of Twitter data streamed across two Arab countries, extracted linguistic features and trained a classiÔ¨Åer achieving an average Arabizi identiÔ¨Åcation accuracy of 94.5%. We also present the percentage of Arabizi usage on Twitter across both countries providing important insights for researchers in NLP and sociolinguistics.  
Previous optimisations of parameters affecting the word-context association measure used in distributional vector space models have focused either on highdimensional vectors with hundreds of thousands of dimensions, or dense vectors with dimensionality of a few hundreds; but dimensionality of a few thousands is often applied in compositional tasks as it is still computationally feasible and does not require the dimensionality reduction step. We present a systematic study of the interaction of the parameters of the association measure and vector dimensionality, and derive parameter selection heuristics that achieve performance across word similarity and relevance datasets competitive with the results previously reported in the literature achieved by highly dimensional or dense models. 
We present a new, sizeable dataset of noun‚Äì noun compounds with their syntactic analysis (bracketing) and semantic relations. Derived from several established linguistic resources, such as the Penn Treebank, our dataset enables experimenting with new approaches towards a holistic analysis of noun‚Äìnoun compounds, such as jointlearning of noun‚Äìnoun compounds bracketing and interpretation, as well as integrating compound analysis with other tasks such as syntactic parsing. 
The idea behind this proposal is to investigate the possibility of utilizing NLP tools, statistical topic modeling techniques and freely available online resources to propose a system able to provide dialogue contribution suggestions which are relevant to the context, yet out of the main activity of the dialogue (i.e. off-activity talk). The aim is to evaluate the effects of a tool that automatically suggests offactivity talks in form of some sentences relevant to the dialogue context. The evaluation is to be done over two test-sets of open domain and closed-domain in a conversational quiz-like setting. The outcome of this work will be a satisfactory point of entry to investigate the hypothesis that adding automatically generated offactivity talks feature to a conversational agent can lead to building up engagement of the dialogue partner(s). 
We propose a method for improving the dependency parsing of complex sentences. This method assumes segmentation of input sentences into clauses and does not require to re-train a parser of one‚Äôs choice. We represent a sentence clause structure using clause charts that provide a layer of embedding for each clause in the sentence. Then we formulate a parsing strategy as a two-stage process where (i) coordinated and subordinated clauses of the sentence are parsed separately with respect to the sentence clause chart and (ii) their dependency trees become subtrees of the Ô¨Ånal tree of the sentence. The object language is Czech and the parser used is a maximum spanning tree parser trained on the Prague Dependency Treebank. We have achieved an average 0.97% improvement in the unlabeled attachment score. Although the method has been designed for the dependency parsing of Czech, it is useful for other parsing techniques and languages. 
The computing cost of many NLP tasks increases faster than linearly with the length of the representation of a sentence. For parsing the representation is tokens, while for operations on syntax and semantics it will be more complex. In this paper we propose a new task of sentence chunking: splitting sentence representations into coherent substructures. Its aim is to make further processing of long sentences more tractable. We investigate this idea experimentally using the Dependency Minimal Recursion Semantics (DMRS) representation. 
The availability of large documentsummary corpora have opened up new possibilities for using statistical text generation techniques for abstractive summarization. Progress in Extractive text summarization has become stagnant for a while now and in this work we compare the two possible alternates to it. We present an argument in favor of abstractive summarization compared to an ensemble of extractive techniques. Further we explore the possibility of using statistical machine translation as a generative text summarization technique and present possible research questions in this direction. We also report our initial Ô¨Åndings and future direction of research. 
Sarcasm can radically alter or invert a phrase‚Äôs meaning. Sarcasm detection can therefore help improve natural language processing (NLP) tasks. The majority of prior research has modeled sarcasm detection as classiÔ¨Åcation, with two important limitations: 1. Balanced datasets, when sarcasm is actually rather rare. 2. Using Twitter users‚Äô self-declarations in the form of hashtags to label data, when sarcasm can take many forms. To address these issues, we create an unbalanced corpus of manually annotated Twitter conversations. We compare human and machine ability to recognize sarcasm on this data under varying amounts of context. Our results indicate that both class imbalance and labelling method affect performance, and should both be considered when designing automatic sarcasm detection systems. We conclude that for progress to be made in real-world sarcasm detection, we will require a new class labelling scheme that is able to access the ‚Äòcommon ground‚Äô held between conversational parties. 
This paper proposes a new unsupervised technique for clustering a collection of documents written by distinct individuals into authorial components. We highlight the importance of utilizing syntactic structure to cluster documents by author, and demonstrate experimental results that show the method we outline performs on par with state-of-the-art techniques. Additionally, we argue that this feature set outperforms previous methods in cases where authors consciously emulate each other‚Äôs style or are otherwise rhetorically similar. 
In addition to the positive and negative sentiments expressed by speakers, opinions on the web also convey suggestions. Such text comprise of advice, recommendations and tips on a variety of points of interest. We propose that suggestions can be extracted from the available opinionated text and put to several use cases. The problem has been identiÔ¨Åed only recently as a viable task, and there is a lot of scope for research in the direction of problem deÔ¨Ånition, datasets, and methods. From an abstract view, standard algorithms for tasks like sentence classiÔ¨Åcation and keyphrase extraction appear to be usable for suggestion mining. However, initial experiments reveal that there is a need for new methods, or variations in the existing ones for addressing the problem speciÔ¨Åc challenges. We present a research proposal which divides the problem into three main research questions; we walk through them, presenting our analysis, results, and future directions. 
In this paper, we propose a cross-lingual convolutional neural network (CNN) model that is based on word and phrase embeddings learned from unlabeled data in two languages and dependency grammar. Compared to traditional machine translation (MT) based methods for cross lingual sentence modeling, our model is much simpler and does not need parallel corpora or language specific features. We only use a bilingual dictionary and dependency parser. This makes our model particularly appealing for resource poor languages. We evaluate our model using English and Chinese data on several sentence classification tasks. We show that our model achieves a comparable and even better performance than the traditional MT-based method. 
This paper introduces a new corpus, QA-It, for the classiÔ¨Åcation of non-referential it. Our dataset is unique in a sense that it is annotated on question answer pairs collected from multiple genres, useful for developing advanced QA systems. Our annotation scheme makes clear distinctions between 4 types of it, providing guidelines for many erroneous cases. Several statistical models are built for the classiÔ¨Åcation of it, showing encouraging results. To the best of our knowledge, this is the Ô¨Årst time that such a corpus is created for question answering. 
In this research, we build a WikiÔ¨Åcation corpus for advancing Japanese Entity Linking. This corpus consists of 340 Japanese newspaper articles with 25,675 entity mentions. All entity mentions are labeled by a Ô¨Åne-grained semantic classes (200 classes), and 19,121 mentions were successfully linked to Japanese Wikipedia articles. Even with the Ô¨Åne-grained semantic classes, we found it hard to deÔ¨Åne the target of entity linking annotations and to utilize the Ô¨Åne-grained semantic classes to improve the accuracy of entity linking. 
Text categorization has become a key research Ô¨Åeld in the NLP community. However, most works in this area are focused on Western languages ignoring other Semitic languages like Arabic. These languages are of immense political and social importance necessitating robust categorization techniques. In this paper, we present a novel three-stage technique to efÔ¨Åciently classify Arabic documents into different categories based on the words they contain. We leverage the signiÔ¨Åcance of root-words in Arabic and incorporate a combination of Markov clustering and Deep Belief Networks to classify Arabic words into separate groups (clusters). Our approach is tested on two public datasets giving a F-Measure of 91.02%. 
The topological Ô¨Åeld model is commonly used to describe the regularities in German word order. In this work, we show that topological Ô¨Åelds can be predicted reliably using sequence labeling and that the predicted Ô¨Åeld labels can inform a transitionbased dependency parser. 
The enormous scale of unlabeled text available today necessitates scalable schemes for representation learning in natural language processing. For instance, in this paper we are interested in classifying the intent of a user query. While our labeled data is quite limited, we have access to virtually an unlimited amount of unlabeled queries, which could be used to induce useful representations: for instance by principal component analysis (PCA). However, it is prohibitive to even store the data in memory due to its sheer size, let alone apply conventional batch algorithms. In this work, we apply the recently proposed matrix sketching algorithm to entirely obviate the problem with scalability (Liberty, 2013). This algorithm approximates the data within a speciÔ¨Åed memory bound while preserving the covariance structure necessary for PCA. Using matrix sketching, we signiÔ¨Åcantly improve the user intent classiÔ¨Åcation accuracy by leveraging large amounts of unlabeled queries. 
Low-dimensional vector representations are widely used as stand-ins for the text of words, sentences, and entire documents. These embeddings are used to identify similar words or make predictions about documents. In this work, we consider embeddings for social media users and demonstrate that these can be used to identify users who behave similarly or to predict attributes of users. In order to capture information from all aspects of a user‚Äôs online life, we take a multiview approach, applying a weighted variant of Generalized Canonical Correlation Analysis (GCCA) to a collection of over 100,000 Twitter users. We demonstrate the utility of these multiview embeddings on three downstream tasks: user engagement, friend selection, and demographic attribute prediction. 
This paper deals with a double-implicit problem in opinion mining and sentiment analysis. We aim at identifying aspects and polarities of opinionated statements not consisting of opinion words and aspect terms. As a case study, opinion words and aspect terms are first extracted from Chinese hotel reviews, and then grouped into positive (negative) clusters and aspect term clusters. We observe that an implicit opinion and its neighbor explicit opinion tend to have the same aspect and polarity. Under the observation, we construct an implicit opinions corpus annotated with aspect class labels and polarity automatically. Aspect and polarity classifiers trained by using this corpus is used to recognize aspect and polarity of implicit opinions. 
Finding domain invariant features is critical for successful domain adaptation and transfer learning. However, in the case of unsupervised adaptation, there is a significant risk of overÔ¨Åtting on source training data. Recently, a regularization for domain adaptation was proposed for deep models by (Ganin and Lempitsky, 2015). We build on their work by suggesting a more appropriate regularization for denoising autoencoders. Our model remains unsupervised and can be computed in a closed form. On standard text classiÔ¨Åcation adaptation tasks, our approach yields the state of the art results, with an important reduction of the learning cost. 
Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition, and potentially omitting important global context. To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identiÔ¨Åes important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese. 
We present a novel technique for training translation models for statistical machine translation by aligning source sentences to their oracle-BLEU translations. In contrast to previous approaches which are constrained to phrase training, our method also allows the re-estimation of reordering models along with the translation model. Experiments show an improvement of up to 0.8 BLEU for our approach over a competitive Arabic-English baseline trained directly on the word-aligned bitext using heuristic extraction. As an additional beneÔ¨Åt, the phrase table size is reduced dramatically to only 3% of the original size. 
We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach. We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to ngram-based scores while providing more relevant outputs. 
We consider two graph models of semantic change. The Ô¨Årst is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods. In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the word‚Äôs meaning across two time points. We apply our two models to corpora across three different languages. We Ô¨Ånd that semantic change is linear in two senses. Firstly, today‚Äôs embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both Ô¨Åndings as new laws/hypotheses of semantic change. 
We describe a model which jointly performs word segmentation and induces vowel categories from formant values. Vowel induction performance improves slightly over a baseline model which does not segment; segmentation performance decreases slightly from a baseline using entirely symbolic input. Our high joint performance in this idealized setting implies that problems in unsupervised speech recognition reÔ¨Çect the phonetic variability of real speech sounds in context. 
Event detection remains a challenge due to the difÔ¨Åculty at encoding the word semantics in various contexts. Previous approaches heavily depend on languagespeciÔ¨Åc knowledge and pre-existing natural language processing (NLP) tools. However, compared to English, not all languages have such resources and tools available. A more promising approach is to automatically learn effective features from data, without relying on languagespeciÔ¨Åc resources. In this paper, we develop a hybrid neural network to capture both sequence and chunk information from speciÔ¨Åc contexts, and use them to train an event detector for multiple languages without any manually encoded features. Experiments show that our approach can achieve robust, efÔ¨Åcient and accurate results for multiple languages (English, Chinese and Spanish). 
Syntactic parsers perform poorly in prediction of Argument-Cluster Coordination (ACC). We change the PTB representation of ACC to be more suitable for learning by a statistical PCFG parser, affecting 125 trees in the training set. Training on the modiÔ¨Åed trees yields a slight improvement in EVALB scores on sections 22 and 23. The main evaluation is on a corpus of 4th grade science exams, in which ACC structures are prevalent. On this corpus, we obtain an impressive √ó2.7 improvement in recovering ACC structures compared to a parser trained on the original PTB trees. 
In the translation industry, human translations are assessed by comparison with the source texts. In the Machine Translation (MT) research community, however, it is a common practice to perform quality assessment using a reference translation instead of the source text. In this paper we show that this practice has a serious issue ‚Äì annotators are strongly biased by the reference translation provided, and this can have a negative impact on the assessment of MT quality. 
This paper presents a cross-lingual projection technique for training class-based language models. We borrow from previous success in projecting POS tags and NER mentions to that of a trained classbased language model. We use a CRF to train a model to predict when a sequence of words is a member of a given class and use this to label our language model training data. We show that we can successfully project the contextual cues for these classes across pairs of languages and retain a high quality class model in languages with no supervised class data. We present empirical results that show the quality of the projected models as well as their effect on the down-stream speech recognition objective. We are able to achieve over 70% of the WER reduction when using the projected class models as compared to models trained on human annotations. 
Retrieving semantic similar short texts is a crucial issue to many applications, e.g., web search, ads matching, questionanswer system, and so forth. Most of the traditional methods concentrate on how to improve the precision of the similarity measurement, while current real applications need to efÔ¨Åciently explore the top similar short texts semantically related to the query one. We address the efÔ¨Åciency issue in this paper by investigating the similarity strategies and incorporating them into the FAST framework (efÔ¨Åcient FrAmework for semantic similar Short Texts retrieval). We conduct comprehensive performance evaluation on real-life data which shows that our proposed method outperforms the state-ofthe-art techniques. 
This paper presents a spinal parsing algorithm that can jointly detect empty elements. This method achieves state-of-theart performance on English and Japanese empty element recovery problems. 
We investigate the effectiveness of semantic generalizations/classiÔ¨Åcations for capturing the regularities of the behavior of verbs in terms of their metaphoricity. Starting from orthographic word unigrams, we experiment with various ways of deÔ¨Åning semantic classes for verbs (grammatical, resource-based, distributional) and measure the effectiveness of these classes for classifying all verbs in a running text as metaphor or non metaphor. 
Over the past decade, e-Commerce has rapidly grown enabling customers to purchase products with the click of a button. But to be able to do so, one has to understand the semantics of a user query and identify that in digital lifestyle tv, digital lifestyle is a brand and tv is a product. In this paper, we develop a series of structured prediction algorithms for semantic tagging of shopping queries with the product, brand, model and product family types. We model wide variety of features and show an alternative way to capture knowledge base information using embeddings. We conduct an extensive study over 37, 000 manually annotated queries and report performance of 90.92 F1 independent of the query length. 
Recent work in learning vector-space embeddings for multi-relational data has focused on combining relational information derived from knowledge bases with distributional information derived from large text corpora. We propose a simple approach that leverages the descriptions of entities or phrases available in lexical resources, in conjunction with distributional semantics, in order to derive a better initialization for training relational models. Applying this initialization to the TransE model results in signiÔ¨Åcant new stateof-the-art performances on the WordNet dataset, decreasing the mean rank from the previous best of 212 to 51. It also results in faster convergence of the entity representations. We Ô¨Ånd that there is a tradeoff between improving the mean rank and the hits@10 with this approach. This illustrates that much remains to be understood regarding performance improvements in relational models. 
In unsupervised semantic role labeling, identifying the role of an argument is usually informed by its dependency relation with the predicate. In this work, we propose a neural model to learn argument embeddings from the context by explicitly incorporating dependency relations as multiplicative factors, which bias argument embeddings according to their dependency roles. Our model outperforms existing state-of-the-art embeddings in unsupervised semantic role induction on the CoNLL 2008 dataset and the SimLex999 word similarity task. Qualitative results demonstrate our model can effectively bias argument embeddings based on their dependency role. 
In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentencelevel or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a wordto-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-toFrench task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015). 
In this paper, we propose the TBCNNpair model to recognize entailment and contradiction between two sentences. In our model, a tree-based convolutional neural network (TBCNN) captures sentencelevel semantics; then heuristic matching layers like concatenation, element-wise product/difference combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin. 
In this paper we improve over the hierarchical Pitman-Yor processes language model in a cross-domain setting by adding skipgrams as features. We Ô¨Ånd that adding skipgram features reduces the perplexity. This reduction is substantial when models are trained on a generic corpus and tested on domain-speciÔ¨Åc corpora. We also Ô¨Ånd that within-domain testing and crossdomain testing require different backoff strategies. We observe a 30-40% reduction in perplexity in a cross-domain language modelling task, and up to 6% reduction in a within-domain experiment, for both English and Flemish-Dutch. 
We release the Simple Paraphrase Database, a subset of of the Paraphrase Database (PPDB) adapted for the task of text simpliÔ¨Åcation. We train a supervised model to associate simpliÔ¨Åcation scores with each phrase pair, producing rankings competitive with state-of-theart lexical simpliÔ¨Åcation models. Our new simpliÔ¨Åcation database contains 4.5 million paraphrase rules, making it the largest available resource for lexical simpliÔ¨Åcation. 
Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily identiÔ¨Åed in text, word segmentation is a key Ô¨Årst step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word segmentation systems use neural models to learn representations for predicting word boundaries. We show that these same representations, jointly trained with an NER system, yield signiÔ¨Åcant improvements in NER for Chinese social media. In our experiments, jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results. 
We introduce a new multilingual resource containing judgments about nominal compound compositionality in English, French and Portuguese. It covers 3 √ó 180 noun-noun and adjective-noun compounds for which we provide numerical compositionality scores for the head word, for the modiÔ¨Åer and for the compound as a whole, along with possible paraphrases. This resource was constructed by native speakers via crowdsourcing. It can serve as basis for evaluating tasks such as lexical substitution and compositionality prediction. 
We present an open web platform for developing, compiling, and running rulebased speech to sign language translation applications. Speech recognition is performed using the Nuance Recognizer 10.2 toolkit, and signed output, including both manual and non-manual components, is rendered using the JASigning avatar system. The platform is designed to make the component technologies readily accessible to sign language experts who are not necessarily computer scientists. Translation grammars are written in a version of Synchronous Context-Free Grammar adapted to the peculiarities of sign language. All processing is carried out on a remote server, with content uploaded and accessed through a web interface. Initial experiences show that simple translation grammars can be implemented on a time-scale of a few hours to a few days and produce signed output readily comprehensible to Deaf informants. Overall, the platform drastically lowers the barrier to entry for researchers interested in building applications that generate high-quality signed language. 
In word alignment certain source words are only needed for Ô¨Çuency reasons and do not have a translation on the target side. Most word alignment models assume a target NULL word from which they generate these untranslatable source words. Hypothesising a target NULL word is not without problems, however. For example, because this NULL word has a position, it interferes with the distribution over alignment jumps. We present a word alignment model that accounts for untranslatable source words by generating them from preceding source words. It thereby removes the need for a target NULL word and only models alignments between word pairs that are actually observed in the data. Translation experiments on English paired with Czech, German, French and Japanese show that the model outperforms its traditional IBM counterparts in terms of BLEU score. 
This work explores the use of unsupervised morph segmentation along with statistical language models for the task of vocabulary expansion. Unsupervised vocabulary expansion has large potential for improving vocabulary coverage and performance in different natural language processing tasks, especially in lessresourced settings on morphologically rich languages. We propose a combination of unsupervised morph segmentation and statistical language models and evaluate on languages from the Babel corpus. The method is shown to perform well for all the evaluated languages when compared to the previous work on the task. 
Recent work has revealed the potential of using visual representations for bilingual lexicon learning (BLL). Such image-based BLL methods, however, still fall short of linguistic approaches. In this paper, we propose a simple yet effective multimodal approach that learns bilingual semantic representations that fuse linguistic and visual input. These new bilingual multi-modal embeddings display signiÔ¨Åcant performance gains in the BLL task for three language pairs on two benchmarking test sets, outperforming linguistic-only BLL models using three different types of state-of-the-art bilingual word embeddings, as well as visual-only BLL models. 
In this paper we study how to identify persuasive posts in the online forum discussions, using data from Change My View sub-Reddit. Our analysis conÔ¨Årms that the users‚Äô voting score for a comment is highly correlated with its metadata information such as published time and author reputation. In this work, we propose and evaluate other features to rank comments for their persuasive scores, including textual information in the comments and social interaction related features. Our experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 
We demonstrate the value of collecting semantic parse labels for knowledge base question answering. In particular, (1) unlike previous studies on small-scale datasets, we show that learning from labeled semantic parses signiÔ¨Åcantly improves overall performance, resulting in absolute 5 point gain compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering. 
As a Ô¨Årst step towards agents learning to communicate about their visual environment, we propose a system that, given visual representations of a referent (CAT) and a context (SOFA), identiÔ¨Åes their discriminative attributes, i.e., properties that distinguish them (has_tail). Moreover, although supervision is only provided in terms of discriminativeness of attributes for pairs, the model learns to assign plausible attributes to speciÔ¨Åc objects (SOFA-has_cushion). Finally, we present a preliminary experiment conÔ¨Årming the referential success of the predicted discriminative attributes. 
Dimensional sentiment analysis aims to recognize continuous numerical values in multiple dimensions such as the valencearousal (VA) space. Compared to the categorical approach that focuses on sentiment classification such as binary classification (i.e., positive and negative), the dimensional approach can provide more fine-grained sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts: regional CNN and LSTM to predict the VA ratings of texts. Unlike a conventional CNN which considers a whole text as input, the proposed regional CNN uses an individual sentence as a region, dividing an input text into several regions such that the useful affective information in each region can be extracted and weighted according to their contribution to the VA prediction. Such regional information is sequentially integrated across regions using LSTM for VA prediction. By combining the regional CNN and LSTM, both local (regional) information within sentences and long-distance dependency across sentences can be considered in the prediction process. Experimental results show that the proposed method outperforms lexicon-based, regression-based, and NN-based methods proposed in previous studies. 
In all previous work on deep multi-task learning we are aware of, all task supervisions are on the same (outermost) layer. We present a multi-task learning architecture with deep bi-directional RNNs, where different tasks supervision can happen at different layers. We present experiments in syntactic chunking and CCG supertagging, coupled with the additional task of POS-tagging. We show that it is consistently better to have POS supervision at the innermost rather than the outermost layer. We argue that this is because ‚Äúlowlevel‚Äù tasks are better kept at the lower layers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks. Finally, we also show how this architecture can be used for domain adaptation. 
In this paper, we propose a method for referring to the real world to improve named entity recognition (NER) specialized for a domain. Our method adds a stacked autoencoder to a text-based deep neural network for NER. We Ô¨Årst train the stacked auto-encoder only from the real world information, then the entire deep neural network from sentences annotated with NEs and accompanied by real world information. In our experiments, we took Japanese chess as the example. The dataset consists of pairs of a game state and commentary sentences about it annotated with gamespeciÔ¨Åc NE tags. We conducted NER experiments and showed that referring to the real world improves the NER accuracy. 
Finding quality descriptions on the web, such as those found in Wikipedia articles, of newer companies can be difÔ¨Åcult: search engines show many pages with varying relevance, while multi-document summarization algorithms Ô¨Ånd it difÔ¨Åcult to distinguish between core facts and other information such as news stories. In this paper, we propose an entity-focused, hybrid generation approach to automatically produce descriptions of previously unseen companies, and show that it outperforms a strong summarization baseline. 
We present a new annotation method for collecting data on relation inference in context. We convert the inference task to one of simple factoid question answering, allowing us to easily scale up to 16,000 high-quality examples. Our method corrects a major bias in previous evaluations, making our dataset much more realistic. 
This paper addresses an automatic classiÔ¨Åcation of preposition types in German, comparing hard and soft clustering approaches and various window- and syntax-based co-occurrence features. We show that (i) the semantically most salient preposition features (i.e., subcategorised nouns) are the most successful, and that (ii) soft clustering approaches are required for the task but reveal quite different attitudes towards predicting ambiguity. 
Decision-making is often dependent on uncertain data, e.g. data associated with conÔ¨Ådence scores or probabilities. We present a comparison of different information presentations for uncertain data and, for the Ô¨Årst time, measure their effects on human decision-making. We show that the use of Natural Language Generation (NLG) improves decision-making under uncertainty, compared to state-of-theart graphical-based representation methods. In a task-based study with 442 adults, we found that presentations using NLG lead to 24% better decision-making on average than the graphical presentations, and to 44% better decision-making when NLG is combined with graphics. We also show that women achieve signiÔ¨Åcantly better results when presented with NLG output (an 87% increase on average compared to graphical presentations). 
Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet2vec, which Ô¨Ånds vectorspace representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing signiÔ¨Åcantly better when the input contains many outof-vocabulary words or unusual character sequences. Our tweet2vec encoder is publicly available1. 
Constrained translation has improved statistical machine translation (SMT) by combining it with translation memory (TM) at sentence-level. In this paper, we propose using a constrained word lattice, which encodes input phrases and TM constraints together, to combine SMT and TM at phrase-level. Experiments on English‚Äì Chinese and English‚ÄìFrench show that our approach is signiÔ¨Åcantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination. 
We present a neural network based automatic post-editing (APE) system to improve raw machine translation (MT) output. Our neural model of APE (NNAPE) is based on a bidirectional recurrent neural network (RNN) model and consists of an encoder that encodes an MT output into a Ô¨Åxed-length vector from which a decoder provides a post-edited (PE) translation. APE translations produced by NNAPE show statistically signiÔ¨Åcant improvements of 3.96, 2.68 and 1.35 BLEU points absolute over the original MT, phrase-based APE and hierarchical APE outputs, respectively. Furthermore, human evaluation shows that the NNAPE generated PE translations are much better than the original MT output. 
We address the problem of automatically cleaning a large-scale Translation Memory (TM) in a fully unsupervised fashion, i.e. without human-labelled data. We approach the task by: i) designing a set of features that capture the similarity between two text segments in different languages, ii) use them to induce reliable training labels for a subset of the translation units (TUs) contained in the TM, and iii) use the automatically labelled data to train an ensemble of binary classiÔ¨Åers. We apply our method to clean a test set composed of 1,000 TUs randomly extracted from the English-Italian version of MyMemory, the world‚Äôs largest public TM. Our results show competitive performance not only against a strong baseline that exploits machine translation, but also against a state-of-the-art method that relies on human-labelled data. 
Recently, neural network models have achieved consistent improvements in statistical machine translation. However, most networks only use one-hot encoded input vectors of words as their input. In this work, we investigated the exponentially decaying bag-of-words input features for feed-forward neural network translation models and proposed to train the decay rates along with other weight parameters. This novel bag-of-words model improved our phrase-based state-of-the-art system, which already includes a neural network translation model, by up to 0.5% BLEU and 0.6% TER on three different translation tasks and even achieved a similar performance to the bidirectional LSTM translation model.  context length on source and target sides. Using the Bag-of-Words (BoW) model as additional input of a neural network based language model, (Mikolov et al., 2015) have achieved very similar perplexities on automatic speech recognition tasks in comparison to the long short-term memory (LSTM) neural network, whose structure is much more complex. This suggests that the bagof-words model can effectively store the longer term contextual information, which could show improvements in statistical machine translation as well. Since the bag-of-words representation can cover as many contextual words without further modifying the network structure, the problem of limited context window size of feed-forward neural networks is reduced. Instead of predeÔ¨Åning Ô¨Åxed decay rates for the exponentially decaying bag-of-words models, we propose to learn the decay rates from the training data like other weight parameters in the neural network model.  
 We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full ngram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modiÔ¨Åed NMT beam-search decoder we Ô¨Ånd gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.  
We automatically predict properties of wines on the basis of smell and Ô¨Çavor descriptions from experts‚Äô wine reviews. We show wine experts are capable of describing their smell and Ô¨Çavor experiences in wine reviews in a sufÔ¨Åciently consistent manner, such that we can use their descriptions to predict properties of a wine based solely on language. The experimental results show promising F-scores when using lexical and semantic information to predict the color, grape variety, country of origin, and price of a wine. This demonstrates, contrary to popular opinion, that wine experts‚Äô reviews really are informative. 
Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors‚Äô age and income. We conÔ¨Årm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media. 
Optimism is linked to various personality factors as well as both psychological and physical health, but how does it relate to the way a person tweets? We analyze the online activity of a set of Twitter users in order to determine how well machine learning algorithms can detect a person‚Äôs outlook on life by reading their tweets. A sample of tweets from each user is manually annotated in order to establish ground truth labels, and classiÔ¨Åers are trained to distinguish between optimistic and pessimistic users. Our results suggest that the words in people‚Äôs tweets provide ample evidence to identify them as optimists, pessimists, or somewhere in between. Additionally, several applications of these trained models are explored. 
Access to data is critical to any machine learning component aimed at training an accurate predictive model. In reality, data is often a subject of technical and legal constraints. Data may contain sensitive topics and data owners are often reluctant to share them. Instead of access to data, they make available decision making procedures to enable predictions on new data. Under the black box classiÔ¨Åer constraint, we build an effective domain adaptation technique which adapts classiÔ¨Åer predictions in a transductive setting. We run experiments on text categorization datasets and show that signiÔ¨Åcant gains can be achieved, especially in the unsupervised case where no labels are available in the target domain. 
Microblogging sites have emerged as major platforms for bloggers to create and consume posts as well as to follow other bloggers and get informed of their updates. Due to the large number of users, and the huge amount of posts they create, it becomes extremely difÔ¨Åcult to identify relevant and interesting blog posts. In this paper, we propose a novel convex collective matrix completion (CCMC) method that effectively utilizes user-item matrix and incorporates additional user activity and topic-based signals to recommend relevant content. The key advantage of CCMC over existing methods is that it can obtain a globally optimal solution and can easily scale to large-scale matrices using Hazan‚Äôs algorithm. To the best of our knowledge, this is the Ô¨Årst work which applies and studies CCMC as a recommendation method in social media. We conduct a large scale study and show significant improvement over existing state-ofthe-art approaches. 
We present a new, structured approach to text simpliÔ¨Åcation using conditional random Ô¨Åelds over top-down traversals of dependency graphs that jointly predicts possible compressions and paraphrases. Our model reaches readability scores comparable to word-based compression approaches across a range of metrics and human judgements while maintaining more of the important information. 
We present a named entity recognition (NER) system for tagging Ô¨Åction: LitNER. Relative to more traditional approaches, LitNER has two important properties: (1) it makes no use of handtagged data or gazetteers, instead it bootstraps a model from term clusters; and (2) it leverages multiple instances of the same name in a text. Our experiments show it to substantially outperform off-the-shelf supervised NER systems. 
Online reviews are a growing market, but it is struggling with fake reviews. They undermine both the value of reviews to the user, and their trust in the review sites. However, fake positive reviews can boost a business, and so a small industry producing fake reviews has developed. The two sides are facing an arms race that involves more and more natural language processing (NLP). So far, NLP has been used mostly for detection, and works well on human-generated reviews. But what happens if NLP techniques are used to generate fake reviews as well? We investigate the question in an adversarial setup, by assessing the detectability of different fake-review generation strategies. We use generative models to produce reviews based on meta-information, and evaluate their effectiveness against deceptiondetection models and human judges. We Ô¨Ånd that meta-information helps detection, but that NLP-generated reviews conditioned on such information are also much harder to detect than conventional ones. 
Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and afÔ¨Åxaware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. 
Bilingual models that capture the semantics of sentences are typically only evaluated on cross-lingual transfer tasks such as cross-lingual document categorization or machine translation. In this work, we evaluate the quality of the monolingual representations learned with a variant of the bilingual compositional model of Hermann and Blunsom (2014), when viewing translations in a second language as a semantic annotation as the original language text. We show that compositional objectives based on phrase translation pairs outperform compositional objectives based on bilingual sentences and on monolingual paraphrases. 
Traditional event detection methods heavily rely on manually engineered rich features. Recent deep learning approaches alleviate this problem by automatic feature engineering. But such efforts, like tradition methods, have so far only focused on single-token event mentions, whereas in practice events can also be a phrase. We instead use forward-backward recurrent neural networks (FBRNNs) to detect events that can be either words or phrases. To the best our knowledge, this is one of the Ô¨Årst efforts to handle multi-word events and also the Ô¨Årst attempt to use RNNs for event detection. Experimental results demonstrate that FBRNN is competitive with the state-of-the-art methods on the ACE 2005 and the Rich ERE 2015 event detection tasks. 
We describe the Iraq Body Count Corpus (IBC-C) dataset, the Ô¨Årst substantial armed conÔ¨Çict-related dataset which can be used for conÔ¨Çict analysis. IBCC provides a ground-truth dataset for conÔ¨Çict speciÔ¨Åc named entity recognition, slot Ô¨Ålling, and event de-duplication. IBCC is constructed using data collected by the Iraq Body Count project which has been recording incidents from the ongoing war in Iraq since 2003. We describe the dataset‚Äôs creation, how it can be used for the above three tasks and provide initial baseline results for the Ô¨Årst task (named entity recognition) using Hidden Markov Models, Conditional Random Fields, and Recursive Neural Networks. 
Uncovering thematic structures of SNS and blog posts is a crucial yet challenging task, because of the severe data sparsity induced by the short length of texts and diverse use of vocabulary. This hinders effective topic inference of traditional LDA because it infers topics based on document-level co-occurrence of words. To robustly infer topics in such contexts, we propose a latent concept topic model (LCTM). Unlike LDA, LCTM reveals topics via co-occurrence of latent concepts, which we introduce as latent variables to capture conceptual similarity of words. More speciÔ¨Åcally, LCTM models each topic as a distribution over the latent concepts, where each latent concept is a localized Gaussian distribution over the word embedding space. Since the number of unique concepts in a corpus is often much smaller than the number of unique words, LCTM is less susceptible to the data sparsity. Experiments on the 20Newsgroups show the effectiveness of LCTM in dealing with short texts as well as the capability of the model in handling held-out documents with a high degree of OOV words. 
This paper studies the effect of limited precision data representation and computation on word embeddings. We present a systematic evaluation of word embeddings with limited memory and discuss methods that directly train the limited precision representation with limited memory. Our results show that it is possible to use and train an 8-bit Ô¨Åxed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks. 
ClassiÔ¨Åcation of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classiÔ¨Åcation on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content. 
There are different deÔ¨Ånitions of what a troll is. Certainly, a troll can be somebody who teases people to make them angry, or somebody who offends people, or somebody who wants to dominate any single discussion, or somebody who tries to manipulate people‚Äôs opinion (sometimes for money), etc. The last deÔ¨Ånition is the one that dominates the public discourse in Bulgaria and Eastern Europe, and this is our focus in this paper. In our work, we examine two types of opinion manipulation trolls: paid trolls that have been revealed from leaked ‚Äúreputation management contracts‚Äù and ‚Äúmentioned trolls‚Äù that have been called such by several different people. We show that these deÔ¨Ånitions are sensible: we build two classiÔ¨Åers that can distinguish a post by such a paid troll from one by a non-troll with 81-82% accuracy; the same classiÔ¨Åer achieves 81-82% accuracy on so called mentioned troll vs. non-troll posts. 
Phrase table pruning is the act of removing phrase pairs from a phrase table to make it smaller, ideally removing the least useful phrases Ô¨Årst. We propose a phrase table pruning method that formulates the task as a submodular function maximization problem, and solves it by using a greedy heuristic algorithm. The proposed method can scale with input size and long phrases, and experiments show that it achieves higher BLEU scores than state-of-the-art pruning methods. 
Bidirectional long short-term memory (biLSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel biLSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that biLSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed. 
In this paper, we propose LexVec, a new method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent cooccurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks. 
We introduce an approach to train lexicalized parsers using bilingual corpora obtained by merging harmonized treebanks of different languages, producing parsers that can analyze sentences in either of the learned languages, or even sentences that mix both. We test the approach on the Universal Dependency Treebanks, training with MaltParser and MaltOptimizer. The results show that these bilingual parsers are more than competitive, as most combinations not only preserve accuracy, but some even achieve signiÔ¨Åcant improvements over the corresponding monolingual parsers. Preliminary experiments also show the approach to be promising on texts with code-switching and when more languages are added. 
Modern coreference resolution systems require linguistic and general knowledge typically sourced from costly, manually curated resources. Despite their intuitive appeal, results have been mixed. In this work, we instead implement Ô¨Åne-grained surface-level features motivated by cognitive theory. Our novel Ô¨Åne-grained feature specialisation approach signiÔ¨Åcantly improves the performance of a strong baseline, achieving state-of-the-art results of 65.29 and 61.13% on CoNLL-2012 using gold and automatic preprocessing, with system extracted mentions. 
This paper presents a novel approach to automated sentence completion based on pointwise mutual information (PMI). Feature sets are created by fusing the various types of input provided to other classes of language models, ultimately allowing multiple sources of both local and distant information to be considered. Furthermore, it is shown that additional precision gains may be achieved by incorporating feature sets of higher-order n-grams. Experimental results demonstrate that the PMI model outperforms all prior models and establishes a new state-of-the-art result on the Microsoft Research Sentence Completion Challenge. 
Using corpus data of spoken dialogue, we examine the convergence of syntactic complexity levels between interlocutors in natural conversations, as it occurs within spans of topic episodes. The Ô¨Åndings of general convergence in the Switchboard and BNC corpora are compatible with an information-theoretic model of dialogue and with Interactive Alignment Theory. 
Nowadays, many scholarly messages are posted on Chinese microblogs and more and more researchers tend to find scholarly information on microblogs. In order to exploit microblogging to benefit scientific research, we propose a scholarly microblog recommendation system in this study. It automatically collects and mines scholarly information from Chinese microblogs, and makes personalized recommendations to researchers. We propose two different neural network models which learn the vector representations for both users and microblog texts. Then the recommendation is accomplished based on the similarity between a user‚Äôs vector and a microblog text‚Äôs vector. We also build a dataset for this task. The two embedding models are evaluated on the dataset and show good results compared to several baselines. 
We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors signiÔ¨Åcantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66‚Äì0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex999, and on distinguishing antonyms from synonyms. 
We explore the applicability of machine translation evaluation (MTE) methods to a very different problem: answer ranking in community Question Answering. In particular, we adopt a pairwise neural network (NN) architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings, and which efÔ¨Åciently models complex non-linear interactions. The evaluation results show state-of-the-art performance, with sizeable contribution from both the MTE features and from the pairwise NN architecture. 
We provide a solution for elementary science tests using instructional materials. We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a uniÔ¨Åed max-margin framework that learns to Ô¨Ånd these hidden structures (given a corpus of questionanswer pairs and instructional materials), and uses what it learns to answer novel elementary science questions. Our evaluation shows that our framework outperforms several strong baselines. 
Prominent semantic annotations take an inclusive approach to argument span annotation, marking arguments as full constituency subtrees. Some works, however, showed that identifying a reduced argument span can be beneÔ¨Åcial for various semantic tasks. While certain practical methods do extract reduced argument spans, such as in Open-IE , these solutions are often ad-hoc and system-dependent, with no commonly accepted standards. In this paper we propose a generic argument reduction criterion, along with an annotation procedure, and show that it can be consistently and intuitively annotated using the recent QA-SRL paradigm. 
We present improvements to our incremental proposition-based summariser, which is inspired by Kintsch and van Dijk‚Äôs (1978) text comprehension model. Argument overlap is a central concept in this summariser. Our new model replaces the old overlap method based on distributional similarity with one based on lexical chains. We evaluate on a new corpus of 124 summaries of educational texts, and show that our new system outperforms the old method and several stateof-the-art non-proposition-based summarisers. The experiment also veriÔ¨Åes that the incremental nature of memory cycles is beneÔ¨Åcial in itself, by comparing it to a non-incremental algorithm using the same underlying information. 
Machine comprehension tests the system‚Äôs ability to understand a piece of text through a reading comprehension task. For this task, we propose an approach using the Abstract Meaning Representation (AMR) formalism. We construct meaning representation graphs for the given text and for each question-answer pair by merging the AMRs of comprising sentences using cross-sentential phenomena such as coreference and rhetorical structures. Then, we reduce machine comprehension to a graph containment problem. We posit that there is a latent mapping of the question-answer meaning representation graph onto the text meaning representation graph that explains the answer. We present a uniÔ¨Åed max-margin framework that learns to Ô¨Ånd this mapping (given a corpus of texts and question-answer pairs), and uses what it learns to answer questions on novel texts. We show that this approach leads to state of the art results on the task. 
Cross-lingual word embeddings are used for cross-lingual information retrieval or domain adaptations. In this paper, we extend Eigenwords, spectral monolingual word embeddings based on canonical correlation analysis (CCA), to crosslingual settings with sentence-alignment. For incorporating cross-lingual information, CCA is replaced with its generalization based on the spectral graph embeddings. The proposed method, which we refer to as Cross-Lingual Eigenwords (CL-Eigenwords), is fast and scalable for computing distributed representations of words via eigenvalue decomposition. Numerical experiments of English-Spanish word translation tasks show that CLEigenwords is competitive with stateof-the-art cross-lingual word embedding methods. 
L2 learners often produce ‚Äúungrammatical‚Äù word combinations such as, e.g., *give a suggestion or *make a walk. This is because of the ‚Äúcollocationality‚Äù of one of their items (the base) that limits the acceptance of collocates to express a speciÔ¨Åc meaning (‚Äòperform‚Äô above). We propose an algorithm that delivers, for a given base and the intended meaning of a collocate, the actual collocate lexeme(s) (make / take above). The algorithm exploits the linear mapping between bases and collocates from examples and generates a collocation transformation matrix which is then applied to novel unseen cases. The evaluation shows a promising line of research in collocation discovery. 
Incorporating lexical knowledge from semantic resources (e.g., WordNet ) has been shown to improve the quality of distributed word representations. This knowledge often comes in the form of relational triplets (x, r, y) where words x and y are connected by a relation type r. Existing methods either ignore the relation types, essentially treating the word pairs as generic related words, or employ rather restrictive assumptions to model the relational knowledge. We propose a novel approach to model relational knowledge based on low-rank subspace regularization, and conduct experiments on standard tasks to evaluate its effectiveness. 
We decompose a standard embedding space into interpretable orthogonal subspaces and a ‚Äúremainder‚Äù subspace. We consider four interpretable subspaces in this paper: polarity, concreteness, frequency and part-of-speech (POS) subspaces. We introduce a new calculus for subspaces that supports operations like ‚Äú‚àí1 √ó hate = love‚Äù and ‚Äúgive me a neutral word for greasy‚Äù (i.e., oleaginous). This calculus extends analogy computations like ‚Äúking‚àíman+woman = queen‚Äù. For the tasks of Antonym ClassiÔ¨Åcation and POS Tagging our method outperforms the state of the art. We create test sets for Morphological Analogies and for the new task of Polarity Spectrum Creation. 
Recent comparative studies have demonstrated the usefulness of dependencybased contexts (DEPS) for learning distributed word representations for similarity tasks. In English, DEPS tend to perform better than the more common, less informed bag-of-words contexts (BOW). In this paper, we present the Ô¨Årst crosslinguistic comparison of different context types for three different languages. DEPS are extracted from ‚Äúuniversal parses‚Äù without any language-speciÔ¨Åc optimization. Our results suggest that the universal DEPS (UDEPS) are useful for detecting functional similarity (e.g., verb similarity, solving syntactic analogies) among languages, but their advantage over BOW is not as prominent as previously reported on English. We also show that simple ‚Äúpost-parsing‚Äù Ô¨Åltering of useful UDEPS contexts leads to consistent improvements across languages. 
Computational Argumentation has two main goals - the detection and analysis of arguments on the one hand, and the synthesis of arguments on the other. Much attention has been given to the former, but considerably less to the latter. A key component in synthesizing arguments is the synthesis of claims. One way to do so is by employing argumentation mining to detect claims within an appropriate corpus. In general, this appears to be a hard problem. Thus, it is interesting to explore if - for the sake of synthesis - there may be other ways to generate claims. Here we explore such a method: we extract the predicate of simple, manuallydetected, claims, and attempt to generate novel claims from them. Surprisingly, this simple method yields fairly good results. 
We propose a framework to model human comprehension of discourse connectives. Following the Bayesian pragmatic paradigm, we advocate that discourse connectives are interpreted based on a simulation of the production process by the speaker, who, in turn, considers the ease of interpretation for the listener when choosing connectives. Evaluation against the sense annotation of the Penn Discourse Treebank conÔ¨Årms the superiority of the model over literal comprehension. A further experiment demonstrates that the proposed model also improves automatic discourse parsing. 
Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efÔ¨Åcient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while Ô¨Çexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efÔ¨Åcient inference.1 
Big data presents new challenges for understanding large text corpora. Topic modeling algorithms help understand the underlying patterns, or ‚Äútopics‚Äù, in data. Researchersauthor often read these topics in order to gain an understanding of the underlying corpus. It is important to evaluate the interpretability of these automatically generated topics. Methods have previously been designed to use crowdsourcing platforms to measure interpretability. In this paper, we demonstrate the necessity of a key concept, coherence, when assessing the topics and propose an effective method for its measurement. We show that the proposed measure of coherence captures a different aspect of the topics than existing measures. We further study the automation of these topic measures for scalability and reproducibility, showing that these measures can be automated. 
Scoring the quality of persuasive essays is an important goal of discourse analysis, addressed most recently with highlevel persuasion-related features such as thesis clarity, or opinions and their targets. We investigate whether argumentation features derived from a coarse-grained argumentative structure of essays can help predict essays scores. We introduce a set of argumentation features related to argument components (e.g., the number of claims and premises), argument relations (e.g., the number of supported claims) and typology of argumentative structure (chains, trees). We show that these features are good predictors of human scores for TOEFL essays, both when the coarsegrained argumentative structure is manually annotated and automatically predicted. 
Morphological reinÔ¨Çection is the task of generating a target form given a source form, a source tag and a target tag. We propose a new way of modeling this task with neural encoder-decoder models. Our approach reduces the amount of required training data for this architecture and achieves state-of-the-art results, making encoder-decoder models applicable to morphological reinÔ¨Çection even for lowresource languages. We further present a new automatic correction method for the outputs based on edit trees. 
Most previous work on annotation projection has been limited to a subset of IndoEuropean languages, using only a single source language, and projecting annotation for one task at a time. In contrast, we present an Integer Linear Programming (ILP) algorithm that simultaneously projects annotation for multiple tasks from multiple source languages, relying on parallel corpora available for hundreds of languages. When training POS taggers and dependency parsers on jointly projected POS tags and syntactic dependencies using our algorithm, we obtain better performance than a standard approach on 20/23 languages using one parallel corpus; and 18/27 languages using another. 
Recently, many neural network models have been applied to Chinese word segmentation. However, such models focus more on collecting local information while long distance dependencies are not well learned. To integrate local features with long distance dependencies, we propose a dependency-based gated recursive neural network. Local features are Ô¨Årst collected by bi-directional long short term memory network, then combined and reÔ¨Åned to long distance dependencies via gated recursive neural network. Experimental results show that our model is a competitive model for Chinese word segmentation. 
Morphologically rich languages (MRL) are languages in which much of the structural information is contained at the wordlevel, leading to high level word-form variation. Historically, syntactic parsing has been mainly tackled using generative models. These models assume input features to be conditionally independent, making difÔ¨Åcult to incorporate arbitrary features. In this paper, we investigate the greedy discriminative parser described in (Legrand and Collobert, 2015), which relies on word embeddings, in the context of MRL. We propose to learn morphological embeddings and propagate morphological information through the tree using a recursive composition procedure. Experiments show that such embeddings can dramatically improve the average performance on different languages. Moreover, it yields state-of-the art performance for a majority of languages. 
For many of the world‚Äôs languages, there are no or very few linguistically annotated resources. On the other hand, raw text, and often also dictionaries, can be harvested from the web for many of these languages, and part-of-speech taggers can be trained with these resources. At the same time, previous research shows that eye-tracking data, which can be obtained without explicit annotation, contains clues to partof-speech information. In this work, we bring these two ideas together and show that given raw text, a dictionary, and eyetracking data obtained from naive participants reading text, we can train a weakly supervised PoS tagger using a secondorder HMM with maximum entropy emissions. The best model use type-level aggregates of eye-tracking data and signiÔ¨Åcantly outperforms a baseline that does not have access to eye-tracking data. 
The aim of this paper is to investigate suitable evaluation strategies for the task of word-level quality estimation of machine translation. We suggest various metrics to replace F1-score for the ‚ÄúBAD‚Äù class, which is currently used as main metric. We compare the metrics‚Äô performance on real system outputs and synthetically generated datasets and suggest a reliable alternative to the F1-BAD score ‚Äî the multiplication of F1-scores for different classes. Other metrics have lower discriminative power and are biased by unfair labellings. 
Medical sciences have long since established an ethics code for experiments, to minimize the risk of harm to subjects. Natural language processing (NLP) used to involve mostly anonymous corpora, with the goal of enriching linguistic analysis, and was therefore unlikely to raise ethical concerns. As NLP becomes increasingly wide-spread and uses more data from social media, however, the situation has changed: the outcome of NLP experiments and applications can now have a direct effect on individual users‚Äô lives. Until now, the discourse on this topic in the Ô¨Åeld has not followed the technological development, while public discourse was often focused on exaggerated dangers. This position paper tries to take back the initiative and start a discussion. We identify a number of social implications of NLP and discuss their ethical signiÔ¨Åcance, as well as ways to address them. 
Increasing amounts of digital data in historical linguistics necessitate the development of automatic methods for the detection of cognate words across languages. Recently developed methods work well on language families with moderate time depths, but they are not capable of identifying cognate morphemes in words which are only partially related. Partial cognacy, however, is a frequently recurring phenomenon, especially in language families with productive derivational morphology. This paper presents a pilot approach for partial cognate detection in which networks are used to represent similarities between word parts and cognate morphemes are identified with help of state-of-theart algorithms for network partitioning. The approach is tested on a newly created benchmark dataset with data from three sub-branches of Sino-Tibetan and yields very promising results, outperforming all algorithms which are not sensible to partial cognacy. 
Semantic parsers map natural language statements into meaning representations, and must abstract over syntactic phenomena, resolve anaphora, and identify word senses to eliminate ambiguous interpretations. Abstract meaning representation (AMR) is a recent example of one such semantic formalism which, similar to a dependency parse, utilizes a graph to represent relationships between concepts (Banarescu et al., 2013). As with dependency parsing, transition-based approaches are a common approach to this problem. However, when trained in the traditional manner these systems are susceptible to the accumulation of errors when they Ô¨Ånd undesirable states during greedy decoding. Imitation learning algorithms have been shown to help these systems recover from such errors. To effectively use these methods for AMR parsing we Ô¨Ånd it highly beneÔ¨Åcial to introduce two novel extensions: noise reduction and targeted exploration. The former mitigates the noise in the feature representation, a result of the complexity of the task. The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space. We achieve state-ofthe art results, and improve upon standard transition-based parsing by 4.7 F1 points. 
Modeling crisp logical regularities is crucial in semantic parsing, making it difÔ¨Åcult for neural models with no task-speciÔ¨Åc prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a highprecision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision. 
A core problem in learning semantic parsers from denotations is picking out consistent logical forms‚Äîthose that yield the correct denotation‚Äîfrom a combinatorially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more expressive class of logical forms, and show how to use dynamic programming to efÔ¨Åciently represent the complete set of consistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance. To address this, we generate Ô¨Åctitious worlds and use crowdsourced denotations on these worlds to Ô¨Ålter out spurious logical forms. On the WIKITABLEQUESTIONS dataset, we increase the coverage of answerable questions from 53.5% to 76%, and the additional crowdsourced supervision lets us rule out 92.1% of spurious logical forms. 
Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domainor representation-speciÔ¨Åc. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations. 
Slot Ô¨Ålling aims to extract the values (slot Ô¨Ållers) of speciÔ¨Åc attributes (slots types) for a given entity (query) from a largescale corpus. Slot Ô¨Ålling remains very challenging over the past seven years. We propose a simple yet effective unsupervised approach to extract slot Ô¨Ållers based on the following two observations: (1) a trigger is usually a salient node relative to the query and Ô¨Åller nodes in the dependency graph of a context sentence; (2) a relation is likely to exist if the query and candidate Ô¨Åller nodes are strongly connected by a relation-speciÔ¨Åc trigger. Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and AfÔ¨Ånity Propagation for a given (query, Ô¨Åller) pair and then label the slot type based on the identiÔ¨Åed triggers. Our approach achieves 11.6%-25% higher F-score over state-ofthe-art English slot Ô¨Ålling methods. Our experiments also demonstrate that as long as a few trigger seeds, name tagging and dependency parsing capabilities exist, this approach can be quickly adapted to any language and new slot types. Our promising results on Chinese slot Ô¨Ålling can serve as a new benchmark. 
We apply phrase-based and neural models to a core task in interactive machine translation: suggesting how to complete a partial translation. For the phrase-based system, we demonstrate improvements in suggestion quality using novel objective functions, learning techniques, and inference algorithms tailored to this task. Our contributions include new tunable metrics, an improved beam search strategy, an n-best extraction method that increases suggestion diversity, and a tuning procedure for a hierarchical joint model of alignment and translation. The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5% to 41.2% in a large-scale English-German experiment. Our recurrent neural translation system increases accuracy yet further to 53.0%, but inference is two orders of magnitude slower. Manual error analysis shows the strengths and weaknesses of both approaches. 
Attention mechanism has enhanced stateof-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach signiÔ¨Åcantly improves both translation quality and alignment quality over standard attention-based NMT.1 
Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Targetside monolingual data plays an important role in boosting Ô¨Çuency for phrasebased statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic backtranslation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English‚ÜîGerman (+2.8‚Äì3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish‚ÜíEnglish (+2.1‚Äì3.4 BLEU), obtaining new state-of-the-art results. We also show that Ô¨Åne-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English‚ÜíGerman. 
One major drawback of phrase-based translation is that it segments an input sentence into continuous phrases. To support linguistically informed source discontinuity, in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experiments on Chinese‚ÄìEnglish and German‚Äì English tasks show that our system is signiÔ¨Åcantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmentation, our system obtains further improvement, especially on German‚ÄìEnglish. 
As a new generation of cognitive robots start to enter our lives, it is important to enable robots to follow human commands and to learn new actions from human language instructions. To address this issue, this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of Ô¨Çuents and automatically acquires these hypothesis spaces by interacting with humans. The learned hypothesis spaces can be used to automatically plan for lower-level primitive actions towards physical world interaction. Our empirical results have shown that the representation of a hypothesis space of Ô¨Çuents, combined with the learned hypothesis selection algorithm, outperforms a previous baseline. In addition, our approach applies incremental learning, which can contribute to life-long learning from humans in the future. 
We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order. 
The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including traditional count-based and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each timestep, the decision of which softmax layer to use is adaptively made by an MLP which is conditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed model, we observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset. 
In this paper, we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states. This generalization provides a uniÔ¨Åed framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 
We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words: a syntactic dependency tree and a forest of lexical units including multiword expressions (MWEs). This combined representation allows us to capture both the syntactic and semantic structure of MWEs, which in turn enables deeper downstream semantic analysis, especially for semicompositional MWEs. The proposed system extends the arc-standard transition system for dependency parsing with transitions for building complex lexical units. Experiments on two different data sets show that the approach signiÔ¨Åcantly improves MWE identiÔ¨Åcation accuracy (and sometimes syntactic accuracy) compared to existing joint approaches. 
Dynamic oracle training has shown substantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing. The present article introduces a dynamic oracle for transition-based constituent parsing. Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features, trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers. 
Metaphorical expressions are pervasive in natural language and pose a substantial challenge for computational semantics. The inherent compositionality of metaphor makes it an important test case for compositional distributional semantic models (CDSMs). This paper is the Ô¨Årst to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework. We propose a method to learn metaphors as linear transformations in a vector space and Ô¨Ånd that, across a variety of semantic domains, explicitly modeling metaphor improves the resulting semantic representations. We then use these representations in a metaphor identiÔ¨Åcation task, achieving a high performance of 0.82 in terms of F-score. 
Idiom token classiÔ¨Åcation is the task of deciding for a set of potentially idiomatic phrases whether each occurrence of a phrase is a literal or idiomatic usage of the phrase. In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classiÔ¨Åcation. We show that classiÔ¨Åers using these representations have competitive performance compared with the state of the art in idiom token classiÔ¨Åcation. Importantly, however, our models use only the sentence containing the target phrase as input and are thus less dependent on a potentially inaccurate or incomplete model of discourse context. We further demonstrate the feasibility of using these representations to train a competitive general idiom token classiÔ¨Åer. 
We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the objective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks. 
Metaphor is a common linguistic tool in communication, making its detection in discourse a crucial task for natural language understanding. One popular approach to this challenge is to capture semantic incohesion between a metaphor and the dominant topic of the surrounding text. While these methods are effective, they tend to overclassify target words as metaphorical when they deviate in meaning from its context. We present a new approach that (1) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and (2) captures the motivation of speakers to express emotions and abstract concepts metaphorically. Experiments on an online breast cancer discussion forum dataset demonstrate a signiÔ¨Åcant improvement in metaphor detection over the state-of-theart. These experimental results also reveal a tendency toward metaphor usage in personal topics and certain emotional contexts. 
Neural networks are among the state-ofthe-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.1 
We introduce a new methodology for intrinsic evaluation of word representations. SpeciÔ¨Åcally, we identify four fundamental criteria based on the characteristics of natural language that pose difÔ¨Åculties to NLP systems; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria. Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points. We show the limits of these point-based intrinsic evaluations. We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models. 
 A shared bilingual word embedding space (SBWES) is an indispensable resource in a variety of cross-language NLP and IR tasks. A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces, where the mapping critically relies on a seed word lexicon used in the learning process. In this work, we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions (i.e., lexicon source, lexicon size, translation method, translation pair reliability). On the basis of our analysis, we propose a simple but effective hybrid bilingual word embedding (BWE) model. This model (HYBWE) learns the mapping between two monolingual embedding spaces using only highly reliable symmetric translation pairs from a seed document-level embedding space. We perform bilingual lexicon learning (BLL) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models, all of which use more expensive bilingual signals. Effectively, we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal (document alignments) along with monolingual data. 
We propose a brand new ‚ÄúLiberal‚Äù Event Extraction paradigm to extract events and discover event schemas from any input corpus simultaneously. We incorporate symbolic (e.g., Abstract Meaning Representation) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema. Experiments on general and speciÔ¨Åc domains demonstrate that this framework can construct high-quality schemas with many event and argument role types, covering a high proportion of event types and argument roles in manually deÔ¨Åned schemas. We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predeÔ¨Åned event types. The extraction quality of new event types is also promising. 
Event extraction from texts aims to detect structured information such as what has happened, to whom, where and when. Event extraction and visualization are typically considered as two different tasks. In this paper, we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks beneÔ¨Åt from each other. We model each event as a joint distribution over named entities, a date, a location and event-related keywords. Moreover, both tweets and event instances are associated with coordinates in the visualization space. The manifold assumption that the intrinsic geometry of tweets is a low-rank, non-linear manifold within the high-dimensional space is incorporated into the learning framework using a regularization. Experimental results show that the proposed approach can effectively deal with both event extraction and visualization and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization. 
There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, Ô¨Ånding the latter to be roughly comparable to the former in terms of predicting missing events in documents. 
Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the speciÔ¨Åc mentions of predicates and entities. For each model, we investigate four implementations: a ‚Äústandard‚Äù N-gram language model and three discriminatively trained ‚Äúneural‚Äù language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically ‚Äì we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing. 
Domain adaptation is an important research topic in sentiment analysis area. Existing domain adaptation methods usually transfer sentiment knowledge from only one source domain to target domain. In this paper, we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains. We Ô¨Årst extract both global and domain-speciÔ¨Åc sentiment knowledge from the data of multiple source domains using multi-task learning. Then we transfer them to target domain with the help of words‚Äô sentiment polarity relations extracted from the unlabeled target domain data. The similarities between target domain and different source domains are also incorporated into the adaptation process. Experimental results on benchmark dataset show the effectiveness of our approach in improving cross-domain sentiment classiÔ¨Åcation performance. 
Through a particular choice of a predicate (e.g., ‚Äúx violated y‚Äù), a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y: (1) writer‚Äôs perspective: projecting x as an ‚Äúantagonist‚Äù and y as a ‚Äúvictim‚Äù, (2) entities‚Äô perspective: y probably dislikes x, (3) effect: something bad happened to y, (4) value: y is something valuable, and (5) mental state: y is distressed by the event. We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations. First, we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments. We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations. Empirical results conÔ¨Årm that connotation frames can be induced from various data sources that reÔ¨Çect how language is used in context. We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media. 
Sentiment classiÔ¨Åcation aims to automatically predict sentiment polarity (e.g., positive or negative) of user generated sentiment data (e.g., reviews, blogs). Due to the mismatch among different domains, a sentiment classiÔ¨Åer trained in one domain may not work well when directly applied to other domains. Thus, domain adaptation for sentiment classiÔ¨Åcation algorithms are highly desirable to reduce the domain discrepancy and manual labeling costs. To address the above challenge, we propose a novel domain adaptation method, called Bi-Transferring Deep Neural Networks (BTDNNs). The proposed BTDNNs attempts to transfer the source domain examples to the target domain, and also transfer the target domain examples to the source domain. The linear transformation of BTDNNs ensures the feasibility of transferring between domains, and the distribution consistency between the transferred domain and the desirable domain is constrained with a linear data reconstruction manner. As a result, the transferred source domain is supervised and follows similar distribution as the target domain. Therefore, any supervised method can be used on the transferred source domain to train a classiÔ¨Åer for sentiment classiÔ¨Åcation in a target domain. We conduct experiments on a benchmark composed of reviews of 4 types of Amazon products. Experimental results show that our proposed approach signiÔ¨Åcantly outperforms the several baseline methods, and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation.  
We present a new approach for documentlevel sentiment inference, where the goal is to predict directed opinions (who feels positively or negatively towards whom) for all entities mentioned in a text. To encourage more complete and consistent predictions, we introduce an ILP that jointly models (1) sentence- and discourse-level sentiment cues, (2) factual evidence about entity factions, and (3) global constraints based on social science theories such as homophily, social balance, and reciprocity. Together, these cues allow for rich inference across groups of entities, including for example that CEOs and the companies they lead are likely to have similar sentiment towards others. We evaluate performance on new, densely labeled data that provides supervision for all pairs, complementing previous work that only labeled pairs mentioned in the same sentence. Experiments demonstrate that the global model outperforms sentence-level baselines, by providing more coherent predictions across sets of related entities. 
Different from traditional active learning based on sentence-wise full annotation (FA), this paper proposes active learning with dependency-wise partial annotation (PA) as a Ô¨Åner-grained unit for dependency parsing. At each iteration, we select a few most uncertain words from an unlabeled data pool, manually annotate their syntactic heads, and add the partial trees into labeled data for parser retraining. Compared with sentence-wise FA, dependency-wise PA gives us more Ô¨Çexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence. Our work makes the following contributions. First, we are the Ô¨Årst to apply a probabilistic model to active learning for dependency parsing, which can 1) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics, and 2) directly learn parameters from PA based on a forest-based training objective. Second, we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English. Finally, we conduct human annotation experiments to compare FA and PA on real annotation time and quality. 
We present a novel dependency parsing method which enforces two structural properties on dependency trees: bounded block degree and well-nestedness. These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency parsing to context-sensitive grammatical formalisms. We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search. Experimentally, we see that these methods are efÔ¨Åcient and competitive compared to a baseline unconstrained parser, while enforcing structural properties in all cases. 
Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for ad hoc information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, underperform corpus and query speciÔ¨Åc embeddings for retrieval tasks. These results suggest that other tasks beneÔ¨Åting from global embeddings may also beneÔ¨Åt from local embeddings. 
Community Question Answering (cQA) services like Yahoo! Answers1, Baidu Zhidao2, Quora3, StackOverÔ¨Çow4 etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions. The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives. The main challenge in this task is the ‚Äúlexicosyntactic‚Äù gap between the current and the previous questions. In this paper, we propose a novel approach called ‚ÄúSiamese Convolutional Neural Network for cQA (SCQA)‚Äù to Ô¨Ånd the semantic similarity between the current and the archived questions. SCQA consist of twin convolutional neural networks with shared parameters and a contrastive loss function joining them. SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA forum archives. The model projects semantically similar question pairs nearer to each other and dissimilar question pairs farther away from each other in the semantic space. Experiments on large scale reallife ‚ÄúYahoo! Answers‚Äù dataset reveals that SCQA outperforms current state-of-theart approaches based on translation models, topic models and deep neural network 1https://answers.yahoo.com/ 2http://zhidao.baidu.com/ 3http://www.quora.com/ 4http://stackoverflow.com/  based models which use non-shared parameters. 
In this work, we focus on the problem of news citation recommendation. The task aims to recommend news citations for both authors and readers to create and search news references. Due to the sparsity issue of news citations and the engineering difÔ¨Åculty in obtaining information on authors, we focus on content similarity-based methods instead of collaborative Ô¨Åltering-based approaches. In this paper, we explore word embedding (i.e., implicit semantics) and grounded entities (i.e., explicit semantics) to address the variety and ambiguity issues of language. We formulate the problem as a reranking task and integrate different similarity measures under the learning to rank framework. We evaluate our approach on a real-world dataset. The experimental results show the efÔ¨Åcacy of our method. 
Grapheme-to-phoneme (g2p) models are rarely available in low-resource languages, as the creation of training and evaluation data is expensive and time-consuming. We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages. We then develop phoneme and language distance metrics based on phonological and linguistic knowledge; applying those, we adapt g2p models for highresource languages to create models for related low-resource languages. We provide results for models for 229 adapted languages. 
Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within Ô¨Åxed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long shortterm memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches, our models achieve competitive or better performances with previous stateof-the-art methods. 
Character-based and word-based methods are two main types of statistical models for Chinese word segmentation, the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model, with the advantages that word-level features can be easily utilized. Neural models have been exploited for character-based Chinese word segmentation, giving high accuracies by making use of external character embeddings, yet requiring less feature engineering. In this paper, we study a neural model for word-based Chinese word segmentation, by replacing the manuallydesigned discrete features with neural features in a word-based segmentation framework. Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature, and a further combination of discrete and neural features gives top accuracies. 
Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging MCTest benchmark. Partly because of its limited size, prior work on MCTest has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for MCTest, outperforming previous feature-engineered approaches slightly and previous neural approaches by a signiÔ¨Åcant margin (over 15 percentage points). 
Broad domain question answering is often difÔ¨Åcult in the absence of structured knowledge bases, and can beneÔ¨Åt from shallow lexical methods (broad coverage) and logical reasoning (high precision). We propose an approach for incorporating both of these signals in a uniÔ¨Åed framework based on natural logic. We extend the breadth of inferences afforded by natural logic to include relational entailment (e.g., buy ‚Üí own) and meronymy (e.g., a person born in a city is born the city‚Äôs country). Furthermore, we train an evaluation function ‚Äì akin to gameplaying ‚Äì to evaluate the expected truth of candidate premises on the Ô¨Çy. We evaluate our approach on answering multiple choice science questions, achieving strong results on the dataset. 
Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems. Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning. Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm Ô¨Årst and harder samples can be introduced successively. We introduce a number of heuristics that improve upon selfpaced learning. Then, we argue that incorporating easy, yet, a diverse set of samples can further improve learning. We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them. 
Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers. In this work, we propose a series of deep learning models to address passage answer selection. To match passage answers to questions accommodating their complex semantic relations, unlike most previous work that utilizes a single deep learning structure, we develop hybrid models that process the text using both convolutional and recurrent neural networks, combining the merits on extracting linguistic information from both structures. Additionally, we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question, which is imperative for better modeling long answer sequences. The results on two public benchmark datasets, InsuranceQA and TREC-QA, show that our proposed models outperform a variety of strong baselines. 
Question answering requires access to a knowledge base to check facts and reason about information. Knowledge in the form of natural language text is easy to acquire, but difÔ¨Åcult for automated reasoning. Highly-structured knowledge bases can facilitate reasoning, but are difÔ¨Åcult to acquire. In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this tradeoff. We Ô¨Årst use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations, easily and efÔ¨Åciently via crowd-sourcing. We then use this annotated data to train a semistructured feature-driven model for question answering that uses tables as a knowledge base. In benchmark evaluations, we signiÔ¨Åcantly outperform both a strong unstructured retrieval baseline and a highlystructured Markov Logic Network model. 
Traditional approaches to extractive summarization rely heavily on humanengineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs1. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation. 
Most sentence embedding models typically represent each sentence only using word surface, which makes these models indiscriminative for ubiquitous homonymy and polysemy. In order to enhance representation capability of sentence, we employ conceptualization model to assign associated concepts for each sentence in the text corpus, and then learn conceptual sentence embedding (CSE). Hence, this semantic representation is more expressive than some widely-used text representation models such as latent topic model, especially for short-text. Moreover, we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efÔ¨Åcient prediction. In the experiments, we evaluate the CSE models on two tasks, text classiÔ¨Åcation and information retrieval. The experimental results show that the proposed models outperform typical sentence embed-ding models. 
Most current chatbot engines are designed to reply to user utterances based on existing utterance-response (or Q-R)1 pairs. In this paper, we present DocChat, a novel information retrieval approach for chatbot engines that can leverage unstructured documents, instead of Q-R pairs, to respond to utterances. A learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly. We evaluate our proposed approach in both English and Chinese: (i) For English, we evaluate DocChat on WikiQA and QASent, two answer sentence selection tasks, and compare it with state-of-the-art methods. Reasonable improvements and good adaptability are observed. (ii) For Chinese, we compare DocChat with XiaoIce2, a famous chitchat engine in China, and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses. 
In conversation, speakers tend to ‚Äúaccommodate‚Äù or ‚Äúalign‚Äù to their partners, changing the style and substance of their communications to be more similar to their partners‚Äô utterances. We focus here on ‚Äúlinguistic alignment,‚Äù changes in word choice based on others‚Äô choices. Although linguistic alignment is observed across many different contexts and its degree correlates with important social factors such as power and likability, its sources are still uncertain. We build on a recent probabilistic model of alignment, using it to separate out alignment attributable to words versus word categories. We model alignment in two contexts: telephone conversations and microblog replies. Our results show evidence of alignment, but it is primarily lexical rather than categorical. Furthermore, we Ô¨Ånd that discourse acts modulate alignment substantially. This evidence supports the view that alignment is shaped by strategic communicative processes related to the ongoing discourse. 
The applicability of entropy rate constancy to dialogue is examined on two spoken dialogue corpora. The principle is found to hold; however, new entropy change patterns within the topic episodes of dialogue are described, which are different from written text. Speaker‚Äôs dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy, respectively, which results in local convergence between these speakers in each topic episode. This implies that the sentence entropy in dialogue is conditioned on different contexts determined by the speaker‚Äôs roles. Explanations from the perspectives of grounding theory and interactive alignment are discussed, resulting in a novel, uniÔ¨Åed informationtheoretic approach of dialogue. 
To establish sophisticated dialogue systems, text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues. Little attention has been given to this topic in text planning in contrast to dialogues that are fully aligned with anticipated user interests. When considering dialogues with congruent and incongruent interlocutor interests, dialogue partners are facing the constant challenge of Ô¨Ånding a balance between cooperation and competition. We introduce the concept of fairness that operationalize an equal and adequate, i.e. equitable satisfaction of all interlocutors‚Äô interests. Focusing on Question-Answering (QA) settings, we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests. Due to the fact that fairness is subjective per se, we present promising results from an empirical study (N=107) in which human subjects interacted with a QA system implementing the proposed approach. 
Modeling interactions between two sentences is crucial for a number of natural language processing tasks including Answer Selection, Dialogue Act Analysis, etc. While deep learning methods like Recurrent Neural Network or Convolutional Neural Network have been proved to be powerful for sentence modeling, prior studies paid less attention on interactions between sentences. In this work, we propose a Sentence Interaction Network (SIN) for modeling the complex interactions between two sentences. By introducing ‚Äúinteraction states‚Äù for word and phrase pairs, SIN is powerful and Ô¨Çexible in capturing sentence interactions for different tasks. We obtain signiÔ¨Åcant improvements on Answer Selection and Dialogue Act Analysis without any feature engineering. 
In this study, we introduce a nondeterministic method for referring expression generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are evaluated using the VaREG corpus. Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model. 
How much is 131 million US dollars? To help readers put such numbers in context, we propose a new task of automatically generating short descriptions known as perspectives, e.g. ‚Äú$131 million is about the cost to employ everyone in Texas over a lunch period‚Äù. First, we collect a dataset of numeric mentions in news articles, where each mention is labeled with a set of rated perspectives. We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity, numeric proximity and semantic compatibility. In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neural network. Our system obtains a 15.2% F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation. 
Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale questionanswer corpora available. In this paper we present the 30M Factoid QuestionAnswer Corpus, an enormous questionanswer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the questiongeneration model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear to be comparable in quality to real human-generated questions. * First authors. ‚ó¶ Email: {iulian.vlad.serban,caglar.gulcehre, sungjin.ahn,sarath.chandar.anbil.parthipan, aaron.courville,yoshua.bengio}@umontreal.ca Email: alberto.garcia-duran@utc.fr ‚Ä† CIFAR Senior Fellow  
Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured speciÔ¨Åcation. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks. 
Research on generating referring expressions has so far mostly focussed on ‚Äúoneshot reference‚Äù, where the aim is to generate a single, discriminating expression. In interactive settings, however, it is not uncommon for reference to be established in ‚Äúinstallments‚Äù, where referring information is offered piecewise until success has been conÔ¨Årmed. We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories. We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict semantically appropriate expressions. In a human evaluation, we observe that users are sensitive to inadequate object names - which unfortunately are not unlikely to be generated from low-level visual input. We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words. We enhance a word-based REG with contextaware, referential installments and Ô¨Ånd that they substantially improve the referential success of the system. 
Entity resolution is the task of linking each mention of an entity in text to the corresponding record in a knowledge base (KB). Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities that are related in the KB. We explore attentionlike mechanisms for coherence, where the evidence for each candidate is based on a small set of strong relations, rather than relations to all other entities in the document. The rationale is that documentwide support may simply not exist for non-salient entities, or entities not densely connected in the KB. Our proposed system outperforms state-of-the-art systems on the CoNLL 2003, TAC KBP 2010, 2011 and 2012 tasks. 
Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identiÔ¨Åcation effect in the B3, CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this Ô¨Çaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the ofÔ¨Åcial CoNLL scorer. 
Properties of corpora, such as the diversity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short texts. We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algorithms. We show that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset, but do not help with less creative texts. Yet the choice of similarity metric interacts with the choice of clustering method. We Ô¨Ånd that graphbased clustering methods perform well on tightly clustered data but poorly on loosely clustered data. Semantic similarity metrics generate loosely clustered output even when applied to a tightly clustered dataset. Thus, the best performing clustering systems could not use semantic metrics. 
Word embedding maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is inÔ¨Çuenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classiÔ¨Åcation tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document. 
News reader comments found in many on-line news websites are typically massive in amount. We investigate the task of Cultural-common Topic Detection (CTD), which is aimed at discovering common discussion topics from news reader comments written in different languages. We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages. We also develop a partially collapsed Gibbs sampler which effectively incorporates the term translation relationship into the detection of cultural-common topics for model parameter learning. Experimental results show improvements over the state-of-the-art model. 
Document collections often have links between documents‚Äîcitations, hyperlinks, or revisions‚Äîand which links are added is often based on topical similarity. To model these intuitions, we introduce a new topic model for documents situated within a network structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features. Experiments on a scientiÔ¨Åc paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions. 
Sentiment Analysis (SA) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web. SA of English has been thoroughly researched; however research on SA of Arabic has just flourished. Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics. In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter. We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method. The evaluation is performed on internal and external datasets. The performance of these automatically generated lexicons was very promising, albeit the simple method used for classification. The best F-score obtained was 89.58% on the internal dataset and 63.1-64.7% on the external datasets. 
This paper proposes an unsupervised approach for segmenting a multiauthor document into authorial components. The key novelty is that we utilize the sequential patterns hidden among document elements when determining their authorships. For this purpose, we adopt Hidden Markov Model (HMM) and construct a sequential probabilistic model to capture the dependencies of sequential sentences and their authorships. An unsupervised learning method is developed to initialize the HMM parameters. Experimental results on benchmark datasets have demonstrated the signiÔ¨Åcant beneÔ¨Åt of our idea and our approach has outperformed the state-of-the-arts on all tests. As an example of its applications, the proposed approach is applied for attributing authorship of a document and has also shown promising results. 
Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which speciÔ¨Åc words contribute to the text‚Äôs score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative. 
Digital personal assistants are becoming both more common and more useful. The major NLP challenge for personal assistants is machine understanding: translating natural language user commands into an executable representation. This paper focuses on understanding rules written as If-Then statements, though the techniques should be portable to other semantic parsing tasks. We view understanding as structure prediction and show improved models using both conventional techniques and neural network models. We also discuss various ways to improve generalization and reduce overÔ¨Åtting: synthetic training data from paraphrase, grammar combinations, feature selection and ensembles of multiple systems. An ensemble of these techniques achieves a new state of the art result with 8% accuracy improvement. 
We introduce the Treebank of Learner English (TLE), the Ô¨Årst publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First CertiÔ¨Åcate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language1. 
Neuro-imaging studies on reading different parts of speech (PoS) report somewhat mixed results, yet some of them indicate different activations with different PoS. This paper addresses the difÔ¨Åculty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution. We show that once we solve this problem, fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4%. 
A major challenge of semantic parsing is the vocabulary mismatch problem between natural language and target ontology. In this paper, we propose a sentence rewriting based semantic parsing method, which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form. SpeciÔ¨Åcally, we propose two sentence-rewriting methods for two common types of mismatch: a dictionary-based method for 1N mismatch and a template-based method for N-1 mismatch. We evaluate our sentence rewriting based semantic parser on the benchmark semantic parsing dataset ‚Äì WEBQUESTIONS. Experimental results show that our system outperforms the base system with a 3.4% gain in F1, and generates logical forms more accurately and parses sentences more robustly. 
While unsupervised anaphoric zero pronoun (AZP) resolvers have recently been shown to rival their supervised counterparts in performance, it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features. To address these weaknesses, we propose a supervised approach to AZP resolution based on deep neural networks, taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings. Our approach achieves stateof-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus. 
Supervised machine learning models for automated essay scoring (AES) usually require substantial task-speciÔ¨Åc training data in order to make accurate predictions for a particular writing task. This limitation hinders their utility, and consequently their deployment in real-world settings. In this paper, we overcome this shortcoming using a constrained multi-task pairwisepreference learning approach that enables the data from multiple tasks to be combined effectively. Furthermore, contrary to some recent research, we show that high performance AES systems can be built with little or no task-speciÔ¨Åc training data. We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-speciÔ¨Åc training data and in scenarios where the number of tasks increases. 
How can we enable computers to automatically answer questions like ‚ÄúWho created the character Harry Potter‚Äù? Carefully built knowledge bases provide rich sources of facts. However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question. In particular, we focus on the most common questions ‚Äî ones that can be answered with a single fact in the knowledge base. We propose CFO, a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases. Our approach Ô¨Årst zooms in a question to Ô¨Ånd more probable candidate subject mentions, and infers the Ô¨Ånal answers with a uniÔ¨Åed conditional probabilistic framework. Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7% on a dataset of 108k questions ‚Äì the largest public one to date. It outperforms the current state of the art by an absolute margin of 11.8%. 
We revisit Levin‚Äôs theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classiÔ¨Åcation of more than 600 German verbs taking clausal and non-Ô¨Ånite arguments. Grasping the meaning components of Levin-classes is known to be hard. We address this challenge by setting up a multi-perspective semantic characterization of the inferred classes. To this end, we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons ‚Äì the German wordnet GermaNet, VerbNet and FrameNet ‚Äì and perform a detailed analysis and evaluation of the resulting German‚ÄìEnglish classiÔ¨Åcation (available at www.ukp.tu-darmstadt. de/modality-verbclasses/). 
 Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequenceto-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT‚Äô15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system. 
Coordination is an important and common syntactic construction which is not handled well by state of the art parsers. Coordinations in the Penn Treebank are missing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsistencies. In this work, we initiated manual annotation process for solving these issues. We identify the different elements in a coordination phrase and label each element with its function. We add phrase boundaries when these are missing, unify inconsistencies, and Ô¨Åx errors. The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations. We make the coordination annotation publicly available, in hope that they will facilitate further research into coordination disambiguation. 1 
User traits disclosed through written text, such as age and gender, can be used to personalize applications such as recommender systems or conversational agents. However, human perception of these traits is not perfectly aligned with reality. In this paper, we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets. We systematically analyze the quality and possible biases of these predictions. We identify the textual cues which lead to miss-assessments of traits or make annotators more or less conÔ¨Ådent in their choice. Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception. 
Motivated by the Ô¨Åndings in social science that people‚Äôs opinions are diverse and variable while together they are shaped by evolving social norms, we perform personalized sentiment classiÔ¨Åcation via shared model adaptation over time. In our proposed solution, a global sentiment model is constantly updated to capture the homogeneity in which users express opinions, while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals. Global model sharing alleviates data sparsity issue, and individualized model adaptation enables efÔ¨Åcient online model learning. Extensive experimentations are performed on two large review collections from Amazon and Yelp, and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classiÔ¨Åcation solutions. 
Our goal is to generate reading lists for students that help them optimally learn technical material. Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query. This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered. Here we formulate an information-theoretic view of concept dependency and present methods to construct a ‚Äúconcept graph‚Äù automatically from a text corpus. We perform the Ô¨Årst human evaluation of concept dependency edges (to be published as open data), and the results verify the feasibility of automatic approaches for inferring concepts and their dependency relations. This result can support search capabilities that may be tuned to help users learn a subject rather than retrieve documents based on a single query. 
We prove that log-linearly interpolated backoff language models can be efÔ¨Åciently and exactly collapsed into a single normalized backoff model, contradicting Hsu (2007). While prior work reported that log-linear interpolation yields lower perplexity than linear interpolation, normalizing at query time was impractical. We normalize the model ofÔ¨Çine in advance, which is efÔ¨Åcient due to a recurrence relationship between the normalizing factors. To tune interpolation weights, we apply Newton‚Äôs method to this convex problem and show that the derivatives can be computed efÔ¨Åciently in a batch process. These Ô¨Åndings are combined in new open-source interpolation tool, which is distributed with KenLM. With 21 out-of-domain corpora, log-linear interpolation yields 72.58 perplexity on TED talks, compared to 75.91 for linear interpolation. 
Recently a few systems for automatically solving math word problems have reported promising results. However, the datasets used for evaluation have limitations in both scale and diversity. In this paper, we build a large-scale dataset which is more than 9 times the size of previous ones, and contains many more problem types. Problems in the dataset are semiautomatically obtained from community question-answering (CQA) web pages. A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users, which signiÔ¨Åcantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 
Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture semantic information from massive amounts of textual content. As a result, many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models. In this work, we study how word embeddings can be used in Word Sense Disambiguation, one of the oldest tasks in Natural Language Processing and ArtiÔ¨Åcial Intelligence. We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture, and perform a deep analysis of how different parameters affect performance. We show how a WSD system that makes use of word embeddings alone, if designed properly, can provide signiÔ¨Åcant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features. 
Several large cloze-style context-questionanswer datasets have been introduced recently: the CNN and Daily Mail news data and the Children‚Äôs Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for questionanswering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets. 
We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations that connect them ‚Äî the Ô¨Årst such attempt using a deep learning approach. Perhaps surprisingly, we Ô¨Ånd that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach (Yang and Cardie, 2013) to opinion entities extraction, performing below even the standalone sequencetagging CRF. Incorporating sentence-level and a novel relation-level optimization, however, allows the LSTM to identify opinion relations and to perform within 1‚Äì 3% of the state-of-the-art joint model for opinion entities and the IS-FROM relation; and to perform as well as the state-of-theart for the IS-ABOUT relation ‚Äî all without access to opinion lexicons, parsers and other preprocessing components required for the feature-rich CRF+ILP approach. 
This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identiÔ¨Åcation into a transition-based system. We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies. An experimental result demonstrates that our parser achieves a good balance between constituent parsing and nonlocal dependency identiÔ¨Åcation. 
Can we train a system that, on any new input, either says ‚Äúdon‚Äôt know‚Äù or makes a prediction that is guaranteed to be correct? We answer the question in the afÔ¨Årmative provided our model family is wellspeciÔ¨Åed. SpeciÔ¨Åcally, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efÔ¨Åcient method that reasons over the inÔ¨Ånite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset. 
Dialogue topic tracking is a sequential labelling problem of recognizing the topic state at each time step in given dialogue sequences. This paper presents various artiÔ¨Åcial neural network models for dialogue topic tracking, including convolutional neural networks to account for semantics at each individual utterance, and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history. The experimental results demonstrate that our proposed models can signiÔ¨Åcantly improve the tracking performances in human-human conversations. 
Lexico-semantic knowledge of our native language provides an initial foundation for second language learning. In this paper, we investigate whether and to what extent the lexico-semantic models of the native language (L1) are transferred to the second language (L2). SpeciÔ¨Åcally, we focus on the problem of lexical choice and investigate it in the context of three typologically diverse languages: Russian, Spanish and English. We show that a statistical semantic model learned from L1 data improves automatic error detection in L2 for the speakers of the respective L1. Finally, we investigate whether the semantic model learned from a particular L1 is portable to other, typologically related languages. 
Fill-in-the-blank items are commonly featured in computer-assisted language learning (CALL) systems. An item displays a sentence with a blank, and often proposes a number of choices for Ô¨Ålling it. These choices should include one correct answer and several plausible distractors. We describe a system that, given an English corpus, automatically generates distractors to produce items for preposition usage. We report a comprehensive evaluation on this system, involving both experts and learners. First, we analyze the difÔ¨Åculty levels of machine-generated carrier sentences and distractors, comparing several methods that exploit learner error and learner revision patterns. We show that the quality of machine-generated items approaches that of human-crafted ones. Further, we investigate the extent to which mismatched L1 between the user and the learner corpora affects the quality of distractors. Finally, we measure the system‚Äôs impact on the user‚Äôs language proÔ¨Åciency in both the short and the long term. 
We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speakeraddressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges. 
Deep Random Walk (DeepWalk) can learn a latent space representation for describing the topological structure of a network. However, for relational network classiÔ¨Åcation, DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task. In this paper, we present Discriminative Deep Random Walk (DDRW), a novel method for relational network classiÔ¨Åcation. By solving a joint optimization problem, DDRW can learn the latent space representations that well capture the topological structure and meanwhile are discriminative for the network classiÔ¨Åcation task. Our experimental results on several real social networks demonstrate that DDRW significantly outperforms DeepWalk on multilabel network classiÔ¨Åcation tasks, while retaining the topological structure in the latent space. DDRW is stable and consistently outperforms the baseline methods by various percentages of labeled data. DDRW is also an online method that is scalable and can be naturally parallelized. 
Automatically recognising medical concepts mentioned in social media messages (e.g. tweets) enables several applications for enhancing health quality of people in a community, e.g. real-time monitoring of infectious diseases in population. However, the discrepancy between the type of language used in social media and medical ontologies poses a major challenge. Existing studies deal with this challenge by employing techniques, such as lexical term matching and statistical machine translation. In this work, we handle the medical concept normalisation at the semantic level. We investigate the use of neural networks to learn the transition between layman‚Äôs language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology. We evaluate our approaches using three different datasets, where social media texts are extracted from Twitter messages and blog posts. Our experimental results show that our proposed approaches signiÔ¨Åcantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%. 
We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora. The basic idea is to encourage two asymmetric latent-variable translation models (i.e., source-to-target and target-to-source) to agree on identifying latent phrase and word alignments. The agreement is deÔ¨Åned at both word and phrase levels. We develop a Viterbi EM algorithm for jointly training the two unidirectional models efÔ¨Åciently. Experiments on the ChineseEnglish dataset show that agreementbased learning signiÔ¨Åcantly improves both alignment and translation performance. 
Recently, there is rising interest in modelling the interactions of text pair with deep neural networks. In this paper, we propose a model of deep fusion LSTMs (DF-LSTMs) to model the strong interaction of text pair in a recursive matching way. SpeciÔ¨Åcally, DF-LSTMs consist of two interdependent LSTMs, each of which models a sequence under the inÔ¨Çuence of another. We also use external memory to increase the capacity of LSTMs, thereby possibly capturing more complicated matching patterns. Experiments on two very large datasets demonstrate the efÔ¨Åcacy of our proposed architecture. Furthermore, we present an elaborate qualitative analysis of our models, giving an intuitive understanding how our model worked. 
We construct a humans-in-the-loop supervised learning framework that integrates crowdsourcing feedback and local knowledge to detect job-related tweets from individual and business accounts. Using data-driven ethnography, we examine discourse about work by fusing languagebased analysis with temporal, geospational, and labor statistics information. 
Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel wordcharacter solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT‚Äô15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1‚àí11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inÔ¨Çected language with a very complex vocabulary, but also build correct representations for English source words. 
State-of-the-art sequence labeling systems traditionally require large amounts of taskspeciÔ¨Åc knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that beneÔ¨Åts from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks ‚Äî Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets ‚Äî 97.55% accuracy for POS tagging and 91.21% F1 for NER. 
Automatic spoken language assessment systems are becoming increasingly important to meet the demand for English second language learning. This is a challenging task due to the high error rates of, even state-of-the-art, non-native speech recognition. Consequently current systems primarily assess Ô¨Çuency and pronunciation. However, content assessment is essential for full automation. As a Ô¨Årst stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech. Standard approaches to off-topic response detection assess similarity between the response and question based on bag-of-words representations. An alternative framework based on Recurrent Neural Network Language Models (RNNLM) is proposed in this paper. The RNNLM is adapted to the topic of each test question. It learns to associate example responses to questions with points in a topic space constructed using these example responses. ClassiÔ¨Åcation is done by ranking the topic-conditional posterior probabilities of a response. The RNNLMs associate a broad range of responses with each topic, incorporate sequence information and scale better with additional training data, unlike standard methods. On experiments conducted on data from the Business Language Testing Service (BULATS) this approach outperforms standard approaches. 
Most machine translation systems construct translations from a closed vocabulary of target word forms, posing problems for translating into languages that have productive compounding processes. We present a simple and effective approach that deals with this problem in two phases. First, we build a classiÔ¨Åer that identiÔ¨Åes spans of the input text that can be translated into a single compound word in the target language. Then, for each identiÔ¨Åed span, we generate a pool of possible compounds which are added to the translation model as ‚Äúsynthetic‚Äù phrase translations. Experiments reveal that (i) we can effectively predict what spans can be compounded; (ii) our compound generation model produces good compounds; and (iii) modest improvements are possible in end-to-end English‚ÄìGerman and English‚ÄìFinnish translation tasks. We additionally introduce KomposEval, a new multi-reference dataset of English phrases and their translations into German compounds. 
In this paper, we propose a novel mechanism for enriching the feature vector, for the task of sarcasm detection, with cognitive features extracted from eye-movement patterns of human readers. Sarcasm detection has been a challenging research problem, and its importance for NLP applications such as review summarization, dialog systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the behaviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by this observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data. We perform statistical classiÔ¨Åcation using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of Fscore), over the performance of the best reported system. 
We present a new proof that O2 is a multiple context-free language. It contrasts with a recent proof by Salvati (2015) in its avoidance of concepts that seem speciÔ¨Åc to two-dimensional geometry, such as the complex exponential function. Our simple proof creates realistic prospects of widening the results to higher dimensions. This Ô¨Ånding is of central importance to the relation between extreme free word order and classes of grammars used to describe the syntax of natural language. 
Context is crucial for identifying argumentative relations in text, but many argument mining methods make little use of contextual features. This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences. Experiments on student essays demonstrate that the proposed features improve predictive performance in two argumentative relation classiÔ¨Åcation tasks. 
A key goal in natural language generation (NLG) is to enable fast generation even with large vocabularies, grammars and worlds. In this work, we build upon a recently proposed NLG system, Sentence Tree Realization with UCT (STRUCT). We describe four enhancements to this system: (i) pruning the grammar based on the world and the communicative goal, (ii) intelligently caching and pruning the combinatorial space of semantic bindings, (iii) reusing the lookahead search tree at different search depths, and (iv) learning and using a search control heuristic. We evaluate the resulting system on three datasets of increasing size and complexity, the largest of which has a vocabulary of about 10K words, a grammar of about 32K lexicalized trees and a world with about 11K entities and 23K relations between them. Our results show that the system has a median generation time of 8.5s and Ô¨Ånds the best sentence on average within 25s. These results are based on a sequential, interpreted implementation and are signiÔ¨Åcantly better than the state of the art for planningbased NLG systems. 
Effective text classiÔ¨Åcation requires experts to annotate data with labels; these training data are time-consuming and expensive to obtain. If you know what labels you want, active learning can reduce the number of labeled documents needed. However, establishing the label set remains difÔ¨Åcult. Annotators often lack the global knowledge needed to induce a label set. We introduce ALTO: Active Learning with Topic Overviews, an interactive system to help humans annotate documents: topic models provide a global overview of what labels to create and active learning directs them to the right documents to label. Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions, topic models (even by themselves) lead to better label sets, and ALTO‚Äôs combination is best overall. 
Computationally modeling the evolution of science by tracking how scientiÔ¨Åc topics rise and fall over time has important implications for research funding and public policy. However, little is known about the mechanisms underlying topic growth and decline. We investigate the role of rhetorical framing: whether the rhetorical role or function that authors ascribe to topics (as methods, as goals, as results, etc.) relates to the historical trajectory of the topics. We train topic models and a rhetorical function classiÔ¨Åer to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010. We Ô¨Ånd that a topic‚Äôs rhetorical function is highly predictive of its eventual growth or decline. For example, topics that are rhetorically described as results tend to be in decline, while topics that function as methods tend to be in early phases of growth. 
This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method. 
Intelligent assistants on mobile devices, such as Siri, have recently gained considerable attention as novel applications of dialogue technologies. A tremendous amount of real users of intelligent assistants provide us with an opportunity to explore a novel task of predicting whether users will continually use their intelligent assistants in the future. We developed prediction models of prospective user engagement by using large-scale user logs obtained from a commercial intelligent assistant. Experiments demonstrated that our models can predict prospective user engagement reasonably well, and outperforms a strong baseline that makes prediction based past utterance frequency. 
A common use of language is to refer to visually present objects. Modelling it in computers requires modelling the link between language and perception. The ‚Äúwords as classiÔ¨Åers‚Äù model of grounded semantics views words as classiÔ¨Åers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words. It was recently shown to perform well in a game-playing scenario with a small number of object types. We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available. Using a pre-trained convolutional neural network to extract image region features, and augmenting these with positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, Ô¨Ånd bounding box of its referent), while, as we argue, being conceptually simpler and more Ô¨Çexible. 
Event extraction is a particularly challenging information extraction task, which intends to identify and classify event triggers and arguments from raw text. In recent works, when determining event types (trigger classiÔ¨Åcation), most of the works are either pattern-only or feature-only. However, although patterns cannot cover all representations of an event, it is still a very important feature. In addition, when identifying and classifying arguments, previous works consider each candidate argument separately while ignoring the relationship between arguments. This paper proposes a Regularization-Based Pattern Balancing Method (RBPB). Inspired by the progress in representation learning, we use trigger embedding, sentence-level embedding and pattern features together as our features for trigger classiÔ¨Åcation so that the effect of patterns and other useful features can be balanced. In addition, RBPB uses a regularization method to take advantage of the relationship between arguments. Experiments show that we achieve results better than current state-of-art equivalents. 
This paper presents a novel model for Japanese predicate argument structure (PAS) analysis based on a neural network framework. Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language, such as case disappearance and argument omission. To unravel this problem, we learn selectional preferences from a large raw corpus, and incorporate them into a SOTA PAS analysis model, which considers the consistency of all PASs in a given sentence. We demonstrate that the proposed PAS analysis model signiÔ¨Åcantly outperforms the base SOTA system. 
We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we Ô¨Årst create (for experimental purposes) an entailment dataset for the clinical domain, and a highly competitive supervised entailment system, ENT, that is effective (out of the box) on two domains. We then explore self-training and active learning strategies to address the lack of labeled data. With self-training, we successfully exploit unlabeled data to improve over ENT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning experiments demonstrate that we can match (and even beat) ENT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain. 
The distinction between restrictive and non-restrictive modiÔ¨Åcation in noun phrases is a well studied subject in linguistics. Automatically identifying non-restrictive modiÔ¨Åers can provide NLP applications with shorter, more salient arguments, which were found beneÔ¨Åcial by several recent works. While previous work showed that restrictiveness can be annotated with high agreement, no large scale corpus was created, hindering the development of suitable classiÔ¨Åcation algorithms. In this work we devise a novel crowdsourcing annotation methodology, and an accompanying large scale corpus. Then, we present a robust automated system which identiÔ¨Åes non-restrictive modiÔ¨Åers, notably improving over prior methods. 
This study proposes the bilingual segmented topic model (BiSTM), which hierarchically models documents by treating each document as a set of segments, e.g., sections. While previous bilingual topic models, such as bilingual latent Dirichlet allocation (BiLDA) (Mimno et al., 2009; Ni et al., 2009), consider only cross-lingual alignments between entire documents, the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments. This study also presents a method for simultaneously inferring latent topics and segmentation boundaries, incorporating unsupervised topic segmentation (Du et al., 2013) into BiSTM. Experimental results show that the proposed model signiÔ¨Åcantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction (up to +0.083 extraction accuracy). 
This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classiÔ¨Åcation; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art. 
Attention based recurrent neural networks have shown advantages in representing natural language sentences (Hermann et al., 2015; Rockta¬®schel et al., 2015; Tan et al., 2015). Based on recurrent neural networks (RNN), external attention information was added to hidden representations to get an attentive sentence representation. Despite the improvement over nonattentive models, the attention mechanism under RNN is not well studied. In this work, we analyze the deÔ¨Åciency of traditional attention based RNN models quantitatively and qualitatively. Then we present three new RNN models that add attention information before RNN hidden representation, which shows advantage in representing sentence and achieves new stateof-art results in answer selection task. 
Relation classiÔ¨Åcation is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text. We propose a novel convolutional neural network architecture for this task, relying on two levels of attention in order to better discern patterns in heterogeneous contexts. This architecture enables endto-end learning from task-speciÔ¨Åc labeled data, forgoing the need for external knowledge such as explicit dependency structures. Experiments show that our model outperforms previous state-of-the-art methods, including those relying on much richer forms of prior knowledge. 
Knowledge bases (KBs) are often greatly incomplete, necessitating a demand for KB completion. The path ranking algorithm (PRA) is one of the most promising approaches to this task. Previous work on PRA usually follows a single-task learning paradigm, building a prediction model for each relation independently with its own training data. It ignores meaningful associations among certain relations, and might not get enough training data for less frequent relations. This paper proposes a novel multi-task learning framework for PRA, referred to as coupled PRA (CPRA). It Ô¨Årst devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other, and then employs a multi-task learning strategy to effectively couple the prediction of such relations. As such, CPRA takes into account relation association and enables implicit data sharing among them. We empirically evaluate CPRA on benchmark data created from Freebase. Experimental results show that CPRA can effectively identify coherent clusters in which relations are highly correlated. By further coupling such relations, CPRA signiÔ¨Åcantly outperforms PRA, in terms of both predictive accuracy and model interpretability. 
In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long shortterm memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on four corpora (IMDB, BBC, Penn TreeBank, and Fil9), we demonstrate that the proposed model improves perplexity signiÔ¨Åcantly. In the experiments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained larger-context language model, we discover that content words, including nouns, adjectives and verbs, beneÔ¨Åt most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily. 
Website privacy policies are often ignored by Internet users, because these documents tend to be long and difÔ¨Åcult to understand. However, the signiÔ¨Åcance of privacy policies greatly exceeds the attention paid to them: these documents are binding legal agreements between website operators and their users, and their opaqueness is a challenge not only to Internet users but also to policy regulators. One proposed alternative to the status quo is to automate or semi-automate the extraction of salient details from privacy policy text, using a combination of crowdsourcing, natural language processing, and machine learning. However, there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies. To remedy this problem, we introduce a corpus of 115 privacy policies (267K words) with manual annotations for 23K Ô¨Åne-grained data practices. We describe the process of using skilled annotators and a purpose-built annotation tool to produce the data. We provide Ô¨Åndings based on a census of the annotations and show results toward automating the annotation procedure. Finally, we describe challenges and opportunities for the research community to use this corpus to advance research in both privacy and language technologies. 
We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query. Building on recent work by (Wang et al., 2015), the interpretable logical forms, which are structured objects obeying certain constraints, are enumerated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor. Our experiments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incorporating the grammatical constraints. 
Word embeddings ‚Äì distributed representations of words ‚Äì in deep learning are beneÔ¨Åcial for many tasks in NLP. However, different embedding sets vary greatly in quality and characteristics of the captured information. Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings. Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings compared to individual embedding sets. One advantage of metaembeddings is the increased vocabulary coverage. We release our metaembeddings publicly at http:// cistern.cis.lmu.de/meta-emb. 
In this paper, we investigate the possibility to automatically generate sports news from live text commentary scripts. As a preliminary study, we treat this task as a special kind of document summarization based on sentence extraction. We formulate the task in a supervised learning to rank framework, utilizing both traditional sentence features for generic document summarization and novelly designed task-speciÔ¨Åc features. To tackle the problem of local redundancy, we also propose a probabilistic sentence selection algorithm. Experiments on our collected data from football live commentary scripts and corresponding sports news demonstrate the feasibility of this task. Evaluation results show that our methods are indeed appropriate for this task, outperforming several baseline methods in different aspects. 
One of the major challenges for statistical machine translation (SMT) is to choose the appropriate translation rules based on the sentence context. This paper proposes a continuous space rule selection (CSRS) model for syntax-based SMT to perform this context-dependent rule selection. In contrast to existing maximum entropy based rule selection (MERS) models, which use discrete representations of words as features, the CSRS model is learned by a feed-forward neural network and uses real-valued vector representations of words, allowing for better generalization. In addition, we propose a method to train the rule selection models only on minimal rules, which are more frequent and have richer training data compared to non-minimal rules. We tested our model on different translation tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 
This paper presents neural probabilistic parsing models which explore up to thirdorder graph-based parsing with maximum likelihood training criteria. Two neural network extensions are exploited for performance improvement. Firstly, a convolutional layer that absorbs the inÔ¨Çuences of all words in a sentence is used so that sentence-level information can be effectively captured. Secondly, a linear layer is added to integrate different order neural models and trained with perceptron method. The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies. 
 We propose a novel reranking method to extend a deterministic neural dependency parser. Different to conventional k-best reranking, the proposed model integrates search and learning by utilizing a dynamic action revising process, using the reranking model to guide modiÔ¨Åcation for the base outputs and to rerank the candidates. The dynamic reranking model achieves an absolute 1.78% accuracy improvement over the deterministic baseline parser on PTB, which is the highest improvement by neural rerankers in the literature. 
Cross-lingual sentiment classiÔ¨Åcation aims to adapt the sentiment resource in a resource-rich language to a resource-poor language. In this study, we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages. Different from previous research which only gets bilingual word embedding, our Bilingual Document Representation Learning model BiDRL directly learns document representations. Both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space. The experiments are based on the multilingual multi-domain Amazon review dataset. We use English as the source language and use Japanese, German and French as the target languages. The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages. 
Most of the sequence tagging tasks in natural language processing require to recognize segments with certain syntactic role or semantic meaning in a sentence. They are usually tackled with Conditional Random Fields (CRFs), which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information. Semi-Markov Conditional Random Fields (Semi-CRFs) model segments directly but extracting segment-level features for Semi-CRFs is still a very challenging problem. This paper presents Gated Recursive Semi-CRFs (grSemi-CRFs), which model segments directly and automatically learn segmentlevel features through a gated recursive convolutional neural network. Our experiments on text chunking and named entity recognition (NER) demonstrate that grSemi-CRFs generally outperform other neural models. 
The automatic detection of causal relationships in text is important for natural language understanding. This task has proven to be difÔ¨Åcult, however, due to the need for world knowledge and inference. We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality. Unlike the explicit markers, a closed class, these markers vary signiÔ¨Åcantly in their linguistic forms. We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases, creating a training set via distant supervision. We also train a causal classiÔ¨Åer using features from the open class markers and semantic features providing contextual information. The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text. 
Modeling relation paths has offered signiÔ¨Åcant gains in embedding models for knowledge base (KB) completion. However, enumerating paths between two entities is very expensive, and existing approaches typically resort to approximation with a sampled subset. This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it. In this paper, we propose the Ô¨Årst exact dynamic programming algorithm which enables efÔ¨Åcient incorporation of all relation paths of bounded length, while modeling both relation types and intermediate nodes in the compositional path representations. We conduct a theoretical analysis of the efÔ¨Åciency gain from the approach. Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion. 
We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion (KBC). Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a Ô¨Åxed set. However, the tuples in ConceptNet (Speer and Havasi, 2012) deÔ¨Åne relations between an unbounded set of phrases. We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones. We Ô¨Ånd strong performance from a bilinear model using a simple additive architecture to model phrases. We manually evaluate our trained model‚Äôs ability to assign quality scores to novel tuples, Ô¨Ånding that it can propose tuples at the same quality level as mediumconÔ¨Ådence tuples from ConceptNet. 
We consider the task of learning a contextdependent mapping from utterances to denotations. With only denotations at training time, we must search over a combinatorially large space of logical forms, which is even larger with context-dependent utterances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we Ô¨Ånd that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new contextdependent semantic parsing datasets, and develop a new left-to-right parser. 
Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suÔ¨Äer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they usually operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shiftreduce parser. Our model supports batched computation for a speedup of up to 25√ó over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evaluate it on the Stanford NLI entailment task and show that it signiÔ¨Åcantly outperforms other sentence-encoding models. 
Recently, many NLP tasks have beneÔ¨Åted from distributed word representation. However, it remains unknown whether embedding models are really immune to the typological diversity of languages, despite the language-independent architecture. Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space. Experiment results reveal the language universal and speciÔ¨Åc properties encoded in various word representation. Additionally, strong evidence supports the utility of word form, especially for inÔ¨Çectional languages. 
Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity‚Äîthe rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation‚Äîindependent of frequency, words that are more polysemous have higher rates of semantic change. 
This paper complements semantic role representations with spatial knowledge beyond indicating plain locations. Namely, we extract where entities are (and are not) located, and for how long (seconds, hours, days, etc.). Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts. Experimental results show that the task can be automated. 
There has recently been a lot of interest in unsupervised methods for learning sense distributions, particularly in applications where sense distinctions are needed. This paper analyses a state-of-the-art method for sense distribution learning, and optimises it for application to the entire vocabulary of a given language. The optimised method is then used to produce LEXSEMTM: a sense frequency and semantic dataset of unprecedented size, spanning approximately 88% of polysemous, English simplex lemmas, which is released as a public resource to the community. Finally, the quality of this data is investigated, and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET Ô¨Årst sense for lemmas missing from SEMCOR, and at least on par with SEMCOR-based distributions otherwise. 
We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exempliÔ¨Åes a wide range of linguistic phenomena, and that none of several state-ofthe-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text. 
We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classiÔ¨Åcation and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classiÔ¨Åcation, information extraction, and question answering. We Ô¨Ånd that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%. 
We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods, parsing results signiÔ¨Åcantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-Ô¨Åne expectation-maximization techniques for these languages. 
Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call ‚Äústack-propagation‚Äù. We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model. 
We examine communications in a social network to study user emotional contrast ‚Äì the propensity of users to express different emotions than those expressed by their neighbors. Our analysis is based on a large Twitter dataset, consisting of the tweets of 123,513 users from the USA and Canada. Focusing on Ekman‚Äôs basic emotions, we analyze differences between the emotional tone expressed by these users and their neighbors of different types, and correlate these differences with perceived user demographics. We demonstrate that many perceived demographic traits correlate with the emotional contrast between users and their neighbors. Unlike other approaches on inferring user attributes that rely solely on user communications, we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors. 
State legislatures often rely on existing text when drafting new bills. Resource and expertise constraints, which often drive this copying behavior, can be taken advantage of by lobbyists and special interest groups. These groups provide model bills, which encode policy agendas, with the intent that the models become actual law. Unfortunately, model legislation is often opaque to the public‚Äìboth in source and content. In this paper we present LOBBYBACK, a system that reverse engineers model legislation from observed text. LOBBYBACK identiÔ¨Åes clusters of bills which have text reuse and generates ‚Äúprototypes‚Äù that represent a canonical version of the text shared between the documents. We demonstrate that LOBBYBACK accurately reconstructs model legislation and apply it to a dataset of over 550k bills. 
We propose a new task in the Ô¨Åeld of computational argumentation in which we investigate qualitative properties of Web arguments, namely their convincingness. We cast the problem as relation classiÔ¨Åcation, where a pair of arguments having the same stance to the same prompt is judged. We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation ‚ÄúA is more convincing than B‚Äù exhibits properties of total ordering; these Ô¨Åndings are used as global constraints for cleaning the crowdsourced data. We propose two tasks: (1) predicting which argument from an argument pair is more convincing and (2) ranking all arguments to the topic based on their convincingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman‚Äôs correlation in a cross-topic evaluation. We release the newly created corpus UKPConvArg1 and the experimental software under open licenses. 
An extensive literature in computational social science examines how features of messages, advertisements, and other corpora affect individuals‚Äô decisions, but these analyses must specify the relevant features of the text before the experiment. Automated text analysis methods are able to discover features of text, but these methods cannot be used to obtain the estimates of causal effects‚Äîthe quantity of interest for applied researchers. We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments. We prove the conditions to identify the treatment effects of texts and introduce the supervised Indian Buffet process to discover those treatments. Our method enables us to discover treatments in a training set using a collection of texts and individuals‚Äô responses to those texts, and then estimate the effects of these interventions in a test set of new texts and survey respondents. We apply the model to an experiment about candidate biographies, recovering intuitive features of voters‚Äô decisions and revealing a penalty for lawyers and a bonus for military service. 
Structured prediction from bandit feedback describes a learning scenario where instead of having access to a gold standard structure, a learner only receives partial feedback in form of the loss value of a predicted structure. We present new learning objectives and algorithms for this interactive scenario, focusing on convergence speed and ease of elicitability of feedback. We present supervised-to-bandit simulation experiments for several NLP tasks (machine translation, sequence labeling, text classiÔ¨Åcation), showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence. 
This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Qlearning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text. 
We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure. COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efÔ¨Åcacy of COPYNET. For example, COPYNET can outperform regular RNN-based model with remarkable margins on text summarization tasks. 
 Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being dependent on annotated training data for training new models for every new domain. However, several applications encounter scenarios where models need to transfer/adapt across domains when the label sets vary both in terms of count of labels as well as their connotations. This paper presents Ô¨Årst-of-its-kind transfer learning algorithm for cross-domain classiÔ¨Åcation with multiple source domains and disparate label sets. It starts with identifying transferable knowledge from across multiple domains that can be useful for learning the target domain task. This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task. Experimental results validate the efÔ¨Åcacy of the proposed algorithm against strong baselines on a real world social media and the 20 Newsgroups datasets. 
Languages with rich inÔ¨Çectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For instance, each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are unlikely to observe all inÔ¨Çections of a given lemma. This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information. We solve this problem by exploiting existing morphological resources that can enumerate a word‚Äôs component morphemes. We present a latentvariable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create embeddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy, and word similarity.  
Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typologically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 
Recent work has shown that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the Ô¨Ånding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations, and (2) supervised learning to classify vector differences into relation types. We Ô¨Ånd that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items. 
We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves signiÔ¨Åcant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially beneÔ¨Åt more NLP tasks. 
The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder‚Äì decoder with a subword-level encoder and a character-level decoder on four language pairs‚ÄìEn-Cs, En-De, En-Ru and En-Fi‚Äì using the parallel corpora from WMT‚Äô15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru. 
Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efÔ¨Åciently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses. 
Neural machine translation (NMT) models typically operate with a Ô¨Åxed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English‚ÜíGerman and English‚ÜíRussian by up to 1.1 and 1.3 BLEU, respectively. 
Word pairs, which are one of the most easily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations held between text segments. However, because of the data sparsity problem, the performance achieved by using word pair features is limited. In this paper, in order to overcome the data sparsity problem, we propose the use of word embeddings to replace the original words. Moreover, we adopt a gated relevance network to capture the semantic interaction between word pairs, and then aggregate those semantic interactions using a pooling layer to select the most informative interactions. Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations. 
Quotation detection is the task of locating spans of quoted speech in text. The state of the art treats this problem as a sequence labeling task and employs linear-chain conditional random Ô¨Åelds. We question the efÔ¨Åcacy of this choice: The Markov assumption in the model prohibits it from making joint decisions about the begin, end, and internal context of a quotation. We perform an extensive analysis with two new model architectures. We Ô¨Ånd that (a), simple boundary classiÔ¨Åcation combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model signiÔ¨Åcantly outperforms all others, by relaxing the Markov assumption. 
This paper addresses the problem of speech act recognition in written asynchronous conversations (e.g., fora, emails). We propose a class of conditional structured models deÔ¨Åned over arbitrary graph structures to capture the conversational dependencies between sentences. Our models use sentence representations encoded by a long short term memory (LSTM) recurrent neural model. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTMs provide better task-speciÔ¨Åc representations, and (ii) the global joint model improves over local models. 
This paper describes the Ô¨Årst robust approach to automatically labeling clauses with their situation entity type (Smith, 2003), capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure. Previous work on this task used a small data set from a limited domain, and relied mainly on words as features, an approach which is impractical in larger settings. We provide a new corpus of texts from 13 genres (40,000 clauses) annotated with situation entity types. We show that our sequence labeling approach using distributional information in the form of Brown clusters, as well as syntactic-semantic features targeted to the task, is robust across genres, reaching accuracies of up to 76%. 
Activities and events in our lives are structural, be it a vacation, a camping trip, or a wedding. While individual details vary, there are characteristic patterns that are speciÔ¨Åc to each of these scenarios. For example, a wedding typically consists of a sequence of events such as walking down the aisle, exchanging vows, and dancing. In this paper, we present a data-driven approach to learning event knowledge from a large collection of photo albums. We formulate the task as constrained optimization to induce the prototypical temporal structure of an event, integrating both visual and textual cues. Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content. 
Automatically generating a natural language description of an image is a fundamental problem in artiÔ¨Åcial intelligence. This task involves both computer vision and natural language processing and is called ‚Äúimage caption generation.‚Äù Research on image caption generation has typically focused on taking in an image and generating a caption in English as existing image caption corpora are mostly in English. The lack of corpora in languages other than English is an issue, especially for morphologically rich languages such as Japanese. There is thus a need for corpora sufÔ¨Åciently large for image captioning in other languages. We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese. As the Japanese portion of the corpus is small, our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion. Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus, indicating that image understanding using a resource-rich language beneÔ¨Åts a resource-poor language. 
We study the problem of automatically building hypernym taxonomies from tex-  Seafish Shark  (a) Input  (b) Output Seafish  tual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representa-  Ray  ‚Äúseafish, such as sharks and rays‚Ä¶‚Äù  Shark wordvec closeness Ray visual similarity  ‚Äúshark and ray are a group of seafish‚Ä¶‚Äù  ‚Äúeither ray or shark lives in ‚Ä¶‚Äù  Figure 1: An overview of our system. (a) Input: a  collection of label items, represented by text and  images; (b) Output: we build a taxonomy from  scratch by extracting features based on distributed  representations of text and images.  tions of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.  NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complemen-  
Linguistics studies have shown that action verbs often denote some Change of State (CoS) as the result of an action. However, the causality of action verbs and its potential connection with the physical world has not been systematically explored. To address this limitation, this paper presents a study on physical causality of action verbs and their implied changes in the physical world. We Ô¨Årst conducted a crowdsourcing experiment and identiÔ¨Åed eighteen categories of physical causality for action verbs. For a subset of these categories, we then deÔ¨Åned a set of detectors that detect the corresponding change from visual perception of the physical environment. We further incorporated physical causality modeling and state detection in grounded language understanding. Our empirical studies have demonstrated the effectiveness of causality modeling in grounding language to perception. 
This paper presents a problem-reduction approach to extractive multi-document summarization: we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning. For the summarization, we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized. To this end, we derive an approximation of the ROUGE-N score of a set of sentences, and deÔ¨Åne a principled discrete optimization problem for sentence selection. Mathematical and empirical evidence suggests that the sentence selection step is solved almost exactly, thus reducing the problem to the sentence scoring task. We perform a detailed experimental evaluation on two DUC datasets to demonstrate the validity of our approach. 
There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner English. To address this problem, in this paper, we Ô¨Årst propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it. Second, we show their usefulness, reporting on (a) inter-annotator agreement rate, (b) characteristic CFG rules in the corpora, and (c) parsing performance on them. In addition, we explore methods to improve phrase structure parsing for learner English (achieving an F -measure of 0.878). Finally, we release the full annotation guidelines, the annotated data, and the improved parser model for learner English to the public. 
We present half-life regression (HLR), a novel model for spaced repetition practice with applications to second language acquisition. HLR combines psycholinguistic theory with modern machine learning techniques, indirectly estimating the ‚Äúhalflife‚Äù of a word or concept in a student‚Äôs long-term memory. We use data from Duolingo ‚Äî a popular online language learning application ‚Äî to Ô¨Åt HLR models, reducing error by 45%+ compared to several baselines at predicting student recall rates. HLR model weights also shed light on which linguistic concepts are systematically challenging for second language learners. Finally, HLR was able to improve Duolingo daily student engagement by 12% in an operational user study. 
Foreign language learners can acquire new vocabulary by using cognate and context clues when reading. To measure such incidental comprehension, we devise an experimental framework that involves reading mixed-language ‚Äúmacaronic‚Äù sentences. Using data collected via Amazon Mechanical Turk, we train a graphical model to simulate a human subject‚Äôs comprehension of foreign words, based on cognate clues (edit distance to an English word), context clues (pointwise mutual information), and prior exposure. Our model does a reasonable job at predicting which words a user will be able to understand, which should facilitate the automatic construction of comprehensible text for personalized foreign language education. 
We present a computational analysis of three language varieties: native, advanced non-native, and translation. Our goal is to investigate the similarities and differences between non-native language productions and translations, contrasting both with native language. Using a collection of computational methods we establish three main results: (1) the three types of texts are easily distinguishable; (2) nonnative language and translations are closer to each other than each of them is to native language; and (3) some of these characteristics depend on the source or native language, while others do not, reÔ¨Çecting, perhaps, uniÔ¨Åed principles that similarly affect translations and non-native language. 
We present a pairwise context-sensitive Autoencoder for computing text pair similarity. Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs. Our model outperforms the state-of-the-art models in two semantic retrieval tasks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 
Online news editors ask themselves the same question many times: what is missing in this news article to go online? This is not an easy question to be answered by computational linguistic methods. In this work, we address this important question and characterise the constituents of news article editorial quality. More speciÔ¨Åcally, we identify 14 aspects related to the content of news articles. Through a correlation analysis, we quantify their independence and relation to assessing an article‚Äôs editorial quality. We also demonstrate that the identiÔ¨Åed aspects, when combined together, can be used effectively in quality control methods for online news. 
 Named Entity Disambiguation (NED) algorithms disambiguate mentions of named entities with respect to a knowledge-base, but sometimes the context might be poor or misleading. In this paper we introduce the acquisition of two kinds of background information to alleviate that problem: entity similarity and selectional preferences for syntactic positions. We show, using a generative Na¬®ive Bayes model for NED, that the additional sources of context are complementary, and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets, yielding the third best and the best results, respectively. We provide examples and analysis which show the value of the acquired background information.  
Finding paraphrases in text is an important task with implications for generation, summarization and question answering, among other applications. Of particular interest to those applications is the speciÔ¨Åc formulation of the task where the paraphrases are templated, which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities. Previous work has focused on mining paraphrases from parallel and comparable corpora, or mining very short sub-sentence synonyms and paraphrases. In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates, utilizing a rich type system for the slots, from a plain text corpus. 
We cast sentence boundary detection and syntactic parsing as a joint problem, so an entire text document forms a training instance for transition-based dependency parsing. When trained with an early update or max-violation strategy for inexact search, we observe that only a tiny part of these very long training instances is ever exploited. We demonstrate this effect by extending the ArcStandard transition system with swap for the joint prediction task. When we use an alternative update strategy, our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation. A comparison between a standard pipeline and our joint model furthermore empirically shows the usefulness of syntactic information on the task of sentence boundary detection. 
 Precise evaluation metrics are important  for assessing progress in high-level lan-  guage generation tasks such as machine  translation or image captioning. Histor-  ically, these metrics have been evaluated using correlation with human judgment. However, human-derived scores are often  Figure 1: A few select entries from the SICK dataset. All of these entries follow the same ‚ÄúNegated Subject‚Äù transformation between sentence 1 and sentence 2, yet humans anno-  alarmingly inconsistent and are also limited tated them with an inconsistently wide range of scores (from  in their ability to identify precise areas of weakness. In this paper, we perform a case study for metric evaluation by measuring  
For many applications, the query speed of N -gram language models is a computational bottleneck. Although massively parallel hardware like GPUs offer a potential solution to this bottleneck, exploiting this hardware requires a careful rethinking of basic algorithms and data structures. We present the Ô¨Årst language model designed for such hardware, using B-trees to maximize data parallelism and minimize memory footprint and latency. Compared with a single-threaded instance of KenLM (HeaÔ¨Åeld, 2011), a highly optimized CPUbased language model, our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task. When we saturate both devices, the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures. Our implementation is freely available at https://github.com/XapaJIaMnu/gLM 
Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across languages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model using Wsabie, a discriminative embeddingbased model with rank-based learning. In our evaluation on 11 languages, on average this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experiments show that the method performs best when projecting between related language pairs. Despite the inherently lossy projection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average. 
While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semisupervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves signiÔ¨Åcant improvements over state-of-the-art SMT and NMT systems. 
Training neural network language models over large vocabularies is computationally costly compared to count-based models such as Kneser-Ney. We present a systematic comparison of neural strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We extend self normalization to be a proper estimator of likelihood and introduce an efÔ¨Åcient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney. 
Distributional semantic models (DSMs) are often evaluated on artiÔ¨Åcial similarity datasets containing single words or fully compositional phrases. We present a large-scale multilingual evaluation of DSMs for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French. We build a total of 816 DSMs and perform 2,856 evaluations using word2vec, GloVe, and PPMI-based models. In addition to the DSMs, we compare the impact of different parameters, such as level of corpus preprocessing, context window size and number of dimensions. The results obtained have a high correlation with human judgments, being comparable to or outperforming the state of the art for some datasets (Spearman‚Äôs œÅ=.82 for the Reddy dataset). 
We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun‚Äôs antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our Ô¨Ånal system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 
We describe and evaluate a simple method to extract parallel sentences from comparable corpora. The approach, termed STACC, is based on expanded lexical sets and the Jaccard similarity coefÔ¨Åcient. We evaluate our system against state-of-theart methods on a large range of datasets in different domains, for ten language pairs, showing that it either matches or outperforms current methods across the board and gives signiÔ¨Åcantly better results on the noisiest datasets. STACC is a portable method, requiring no particular adaptation for new domains or language pairs, thus enabling the efÔ¨Åcient mining of parallel sentences in comparable corpora. 
We propose a joint formulation for learning task-speciÔ¨Åc cross-lingual word embeddings, along with classiÔ¨Åers for that task. Unlike prior work, which Ô¨Årst learns the embeddings from parallel data and then plugs them in a supervised learning problem, our approach is oneshot: a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-speciÔ¨Åc loss. We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension, a limitation which does not exist for other co-regularizers (such as the 1distance). Despite its simplicity, our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German, with training times below 1 minute. On the TED Corpus, we obtain the highest reported scores on 10 out of 11 languages. 
Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation. To date, no effort has been put into integrating the supersenses into distributional word representations. We present a novel joint embedding model of words and supersenses, providing insights into the relationship between words and supersenses in the same vector space. Using these embeddings in a deep neural network model, we demonstrate that the supersense enrichment leads to a signiÔ¨Åcant improvement in a range of downstream classiÔ¨Åcation tasks. 
Parsing for a wide variety of grammar formalisms can be performed by intersecting Ô¨Ånite tree automata. However, naive implementations of parsing by intersection are very inefÔ¨Åcient. We present techniques that speed up tree-automata-based parsing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 
Distributional semantics creates vectorspace representations that capture many forms of semantic similarity, but their relation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Using a mean-Ô¨Åeld approximation, we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known (versus unknown). We use this framework to reinterpret an existing distributionalsemantic model (Word2Vec) as approximating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previous results. 
We propose a new unsupervised learning model, hidden softmax sequence model (HSSM), based on Boltzmann machine for dialogue structure analysis. The model employs three types of units in the hidden layer to discovery dialogue latent structures: softmax units which represent latent states of utterances; binary units which represent latent topics speciÔ¨Åed by dialogues; and a binary unit that represents the global general topic shared across the whole dialogue corpus. In addition, the model contains extra connections between adjacent hidden softmax units to formulate the dependency between latent states. Two different kinds of real world dialogue corpora, Twitter-Post and AirTicketBooking, are utilized for extensive comparing experiments, and the results illustrate that the proposed model outperforms sate-ofthe-art popular approaches. 
High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the Ô¨Årst completely datadriven approach for generating high level summaries of source code. Our model, CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverÔ¨Çow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the Ô¨Årst end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large margin. 
To create accessible content for deaf users, we investigate automatically synthesizing animations of American Sign Language (ASL), including grammatically important facial expressions and head movements. Based on recordings of humans performing various types of syntactic face and head movements (which include idiosyncratic variation), we evaluate the efÔ¨Åcacy of Continuous ProÔ¨Åle Models (CPMs) at identifying an essential ‚Äúlatent trace‚Äù of the performance, for use in producing ASL animations. A metric-based evaluation and a study with deaf users indicated that this approach was more effective than a prior method for producing animations. 
There are numerous studies suggesting that published news stories have an important effect on the direction of the stock market, its volatility, the volume of trades, and the value of individual stocks mentioned in the news. There is even some published research suggesting that automated sentiment analysis of news documents, quarterly reports, blogs and/or twitter data can be productively used as part of a trading strategy. This paper presents just such a family of trading strategies, and then uses this application to re-examine some of the tacit assumptions behind how sentiment analyzers are generally evaluated, in spite of the contexts of their application. This discrepancy comes at a cost. 
The run time complexity of state-of-theart inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the resulting parse trees. Solving the inference problem in run time complexity determined solely by the number of edges (m) is hence of obvious importance. We propose such an inference algorithm for Ô¨Årst-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the Ô¨Årst-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n2) directed MST inference. 
Conventional topic models are ineffective for topic extraction from microblog messages since the lack of structure and context among the posts renders poor message-level word co-occurrence patterns. In this work, we organize microblog posts as conversation trees based on reposting and replying relations, which enrich context information to alleviate data sparseness. Our model generates words according to topic dependencies derived from the conversation structures. In speciÔ¨Åc, we differentiate messages as leader messages, which initiate key aspects of previously focused topics or shift the focus to different topics, and follower messages that do not introduce any new information but simply echo topics from the messages that they repost or reply. Our model captures the different extents that leader and follower messages may contain the key topical words, thus further enhances the quality of the induced topics. The results of thorough experiments demonstrate the effectiveness of our proposed model. 
 Distant supervised relation extraction has been widely used to Ô¨Ånd novel relational facts from text. However, distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the performance of relation extraction. To alleviate this issue, we propose a sentence-level attention-based model for relation extraction. In this model, we employ convolutional neural networks to embed the semantics of sentences. Afterwards, we build sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances. Experimental results on real-world datasets show that, our model can make full use of all informative sentences and effectively reduce the inÔ¨Çuence of wrong labelled instances. Our model achieves signiÔ¨Åcant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/NRE.  
Frames deÔ¨Åned in FrameNet (FN) share highly similar structures with events in ACE event extraction program. An event in ACE is composed of an event trigger and a set of arguments. Analogously, a frame in FN is composed of a lexical unit and a set of frame elements, which play similar roles as triggers and arguments of ACE events respectively. Besides having similar structures, many frames in FN actually express certain types of events. The above observations motivate us to explore whether there exists a good mapping from frames to event-types and if it is possible to improve event detection by using FN. In this paper, we propose a global inference approach to detect events in FN. Further, based on the detected results, we analyze possible mappings from frames to event-types. Finally, we improve the performance of event detection and achieve a new state-of-the-art result by using the events automatically detected from FN. 
Solving simple arithmetic word problems is one of the challenges in Natural Language Understanding. This paper presents a novel method to learn to use formulas to solve simple arithmetic word problems. Our system, analyzes each of the sentences to identify the variables and their attributes; and automatically maps this information into a higher level representation. It then uses that representation to recognize the presence of a formula along with its associated variables. An equation is then generated from the formal description of the formula. In the training phase, it learns to score the <formula, variables> pair from the systematically generated higher level representation. It is able to solve 86.07% of the problems in a corpus of standard primary school test questions and beats the state-of-the-art by a margin of 8.07%. 
In this paper we explore the correlation between the sound of words and their meaning, by testing if the polarity (‚Äògood guy‚Äô or ‚Äòbad guy‚Äô) of a character‚Äôs role in a work of Ô¨Åction can be predicted by the name of the character in the absence of any other context. Our approach is based on phonological and other features proposed in prior theoretical studies of Ô¨Åctional names. These features are used to construct a predictive model over a manually annotated corpus of characters from motion pictures. By experimenting with different mixtures of features, we identify phonological features as being the most discriminative by comparison to social and other types of features, and we delve into a discussion of speciÔ¨Åc phonological and phonotactic indicators of a character‚Äôs role‚Äôs polarity. 
We examine adjective-noun (AN) composition in the task of recognizing textual entailment (RTE). We analyze behavior of ANs in large corpora and show that, despite conventional wisdom, adjectives do not always restrict the denotation of the nouns they modify. We use natural logic to characterize the variety of entailment relations that can result from AN composition. Predicting these relations depends on context and on commonsense knowledge, making AN composition especially challenging for current RTE systems. We demonstrate the inability of current stateof-the-art systems to handle AN composition in a simpliÔ¨Åed RTE task which involves the insertion of only a single word. 
Essay stance classiÔ¨Åcation, the task of determining how much an essay‚Äôs author agrees with a given proposition, is an important yet under-investigated subtask in understanding an argumentative essay‚Äôs overall content. We introduce a new corpus of argumentative student essays annotated with stance information and propose a computational model for automatically predicting essay stance. In an evaluation on 826 essays, our approach significantly outperforms four baselines, one of which relies on features previously developed speciÔ¨Åcally for stance classiÔ¨Åcation in student essays, yielding relative error reductions of at least 11.3% and 5.3%, in micro and macro F-score, respectively. 
Word segmentation is a fundamental task for Chinese language processing. However, with the successive improvements, the standard metric is becoming hard to distinguish state-of-the-art word segmentation systems. In this paper, we propose a new psychometric-inspired evaluation metric for Chinese word segmentation, which addresses to balance the very skewed word distribution at different levels of difficulty1. The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 
Today‚Äôs extraction of temporal information for events heavily depends on annotated temporal links. These so called TLINKs capture the relation between pairs of event mentions and time expressions. One problem is that the number of possible TLINKs grows quadratic with the number of event mentions, therefore most annotation studies concentrate on links for mentions in the same or in adjacent sentences. However, as our annotation study shows, this restriction results for 58% of the event mentions in a less precise information when the event took place. This paper proposes a new annotation scheme to anchor events in time. Not only is the annotation effort much lower as it scales linear with the number of events, it also gives a more precise anchoring when the events have happened as the complete document can be taken into account. Using this scheme, we annotated a subset of the TimeBank Corpus and compare our results to other annotation schemes. Additionally, we present some baseline experiments to automatically anchor events in time. Our annotation scheme, the automated system and the annotated corpus are publicly available.1 
We focus on two leading state-of-the-art approaches to grammatical error correction ‚Äì machine learning classiÔ¨Åcation and machine translation. Based on the comparative study of the two learning frameworks and through error analysis of the output of the state-of-the-art systems, we identify key strengths and weaknesses of each of these approaches and demonstrate their complementarity. In particular, the machine translation method learns from parallel data without requiring further linguistic input and is better at correcting complex mistakes. The classiÔ¨Åcation approach possesses other desirable characteristics, such as the ability to easily generalize beyond what was seen in training, the ability to train without human-annotated data, and the Ô¨Çexibility to adjust knowledge sources for individual error types. Based on this analysis, we develop an algorithmic approach that combines the strengths of both methods. We present several systems based on resources used in previous work with a relative improvement of over 20% (and 7.4 F score points) over the previous state-of-the-art. 
Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classiÔ¨Åcation into four pre-deÔ¨Åned categories. We also utilize convolution neural network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classiÔ¨Åcation task of disease names. 
We present the Ô¨Årst domain adaptation model for authorship attribution to leverage unlabeled data. The model includes extensions to structural correspondence learning needed to make it appropriate for the task. For example, we propose a median-based classiÔ¨Åcation instead of the standard binary classiÔ¨Åcation used in previous work. Our results show that punctuation-based character n-grams form excellent pivot features. We also show how singular value decomposition plays a critical role in achieving domain adaptation, and that replacing (instead of concatenating) non-pivot features with correspondence features yields better performance. 
The canonical word order of Japanese double object constructions has attracted considerable attention among linguists and has been a topic of many studies. However, most of these studies require either manual analyses or measurements of human characteristics such as brain activities or reading times for each example. Thus, while these analyses are reliable for the examples they focus on, they cannot be generalized to other examples. On the other hand, the trend of actual usage can be collected automatically from a large corpus. Thus, in this paper, we assume that there is a relationship between the canonical word order and the proportion of each word order in a large corpus and present a corpusbased analysis of canonical word order of Japanese double object constructions. 
In this paper, with the help of knowledge base, we build and formulate a semantic space to connect the source and target languages, and apply it to the sequence-to-sequence framework to propose a Knowledge-Based Semantic Embedding (KBSE) method. In our KBSE method, the source sentence is Ô¨Årstly mapped into a knowledge based semantic space, and the target sentence is generated using a recurrent neural network with the internal meaning preserved. Experiments are conducted on two translation tasks, the electric business data and movie data, and the results show that our proposed method can achieve outstanding performance, compared with both the traditional SMT methods and the existing encoder-decoder models. 
Entity linking (EL) is the task of disambiguating mentions in text by associating them with entries in a predeÔ¨Åned database of mentions (persons, organizations, etc). Most previous EL research has focused mainly on one language, English, with less attention being paid to other languages, such as Spanish or Chinese. In this paper, we introduce LIEL, a Language Independent Entity Linking system, which provides an EL framework which, once trained on one language, works remarkably well on a number of different languages without change. LIEL makes a joint global prediction over the entire document, employing a discriminative reranking framework with many domain and language-independent feature functions. Experiments on numerous benchmark datasets, show that the proposed system, once trained on one language, English, outperforms several state-of-the-art systems in English (by 4 points) and the trained model also works very well on Spanish (14 points better than a competitor system), demonstrating the viability of the approach. 
We discuss an approximate similarity search for word embeddings, which is an operation to approximately Ô¨Ånd embeddings close to a given vector. We compared several metric-based search algorithms with hash-, tree-, and graphbased indexing from different aspects. Our experimental results showed that a graph-based indexing exhibits robust performance and additionally provided useful information, e.g., vector normalization achieves an efÔ¨Åcient search with cosine similarity. 
Learning distributed representations for relation instances is a central technique in downstream NLP applications. In order to address semantic modeling of relational patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset (Zeichner et al., 2012). In addition, we conduct a comparative study of different encoders including additive composition, RNN, LSTM, and GRU for composing distributed representations of relational patterns. We also present Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in the relation classiÔ¨Åcation task. 
Anaphor resolution is an important task in NLP with many applications. Despite much research effort, it remains an open problem. The difÔ¨Åculty of the problem varies substantially across different sub-problems. One sub-problem, in particular, has been largely untouched by prior work despite occurring frequently throughout corpora: the anaphor that has multiple antecedents, which here we call multi-antecedent anaphors or manaphors. Current coreference resolvers restrict anaphors to at most a single antecedent. As we show in this paper, relaxing this constraint poses serious problems in coreference chain-building, where each chain is intended to refer to a single entity. This work provides a formalization of the new task with preliminary insights into multi-antecedent noun-phrase anaphors, and offers a method for resolving such cases that outperforms a number of baseline methods by a signiÔ¨Åcant margin. Our system uses local agglomerative clustering on candidate antecedents and an existing coreference system to score clusters to determine which cluster of mentions is antecedent for a given anaphor. When we augment an existing coreference system with our proposed method, we observe a substantial increase in performance (0.6 absolute CoNLL F1) on an annotated corpus. 
Labeling topics learned by topic models is a challenging problem. Previous studies have used words, phrases and images to label topics. In this paper, we propose to use text summaries for topic labeling. Several sentences are extracted from the most related documents to form the summary for each topic. In order to obtain summaries with both high relevance, coverage and discrimination for all the topics, we propose an algorithm based on submodular optimization. Both automatic and manual analysis have been conducted on two real document collections, and we find 1) the summaries extracted by our proposed algorithm are superior over the summaries extracted by existing popular summarization methods; 2) the use of summaries as labels has obvious advantages over the use of words and phrases. 
In this paper, we propose a neural network model for graph-based dependency parsing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual information instead of using high-order factorization, and enable our model to use much fewer features than previous work. In addition, we propose an effective way to learn sentence segment embedding on sentence-level based on an extra forward LSTM network. Although our model uses only Ô¨Årst-order factorization, experiments on English Peen Treebank and Chinese Penn Treebank show that our model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models. 
Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artiÔ¨Åcial intelligence. This paper proposes a novel generative model (TransG) to address the issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples. The new model can discover latent semantics for a relation and leverage a mixture of relationspeciÔ¨Åc component vectors to embed a fact triple. To the best of our knowledge, this is the Ô¨Årst generative model for knowledge graph embedding, and at the Ô¨Årst time, the issue of multiple relation semantics is formally discussed. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines. 
Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We Ô¨Årst present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F1 of 53.3%, a substantial improvement over the state-of-the-art. 
Semantic deÔ¨Åcit is a symptom of language impairment in Alzheimer‚Äôs disease (AD). We present a generalizable method for automatic generation of information content units (ICUs) for a picture used in a standard clinical task, achieving high recall, 96.8%, of human-supplied ICUs. We use the automatically generated topic model to extract semantic features, and train a random forest classiÔ¨Åer to achieve an F-score of 0.74 in binary classiÔ¨Åcation of controls versus people with AD using a set of only 12 features. This is comparable to results (0.72 F-score) with a set of 85 manual features. Adding semantic information to a set of standard lexicosyntactic and acoustic features improves F-score to 0.80. While control and dementia subjects discuss the same topics in the same contexts, controls are more informative per second of speech. 
Part of the unique cultural heritage of China is the Chinese couplet. Given a sentence (namely an antecedent clause), people reply with another sentence (namely a subsequent clause) equal in length. Moreover, a special phenomenon is that corresponding characters from the same position in the two clauses match each other by following certain constraints on semantic and/or syntactic relatedness. Automatic couplet generation by computer is viewed as a difÔ¨Åcult problem and has not been fully explored. In this paper, we formulate the task as a natural language generation problem using neural network structures. Given the issued antecedent clause, the system generates the subsequent clause via sequential language modeling. To satisfy special characteristics of couplets, we incorporate the attention mechanism and polishing schema into the encoding-decoding process. The couplet is generated incrementally and iteratively. A comprehensive evaluation, using perplexity and BLEU measurements as well as human judgments, has demonstrated the effectiveness of our proposed approach. 
Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1 
We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein‚Äôs language games: a human wishes to accomplish some task (e.g., achieving a certain conÔ¨Åguration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer‚Äôs capabilities. We created a game called SHRDLURN in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans‚Äô strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing that modeling pragmatics on a semantic parsing model accelerates learning for more strategic players. 
Arbitrariness of the sign‚Äîthe notion that the forms of words are unrelated to their meanings‚Äîis an underlying assumption of many linguistic theories. Two lines of research have recently challenged this assumption, but they produce differing characterizations of non-arbitrariness in language. Behavioral and corpus studies have conÔ¨Årmed the validity of localized form-meaning patterns manifested in limited subsets of the lexicon. Meanwhile, global (lexicon-wide) statistical analyses instead Ô¨Ånd diffuse form-meaning systematicity across the lexicon as a whole. We bridge the gap with an approach that can detect both local and global formmeaning systematicity in language. In the kernel regression formulation we introduce, form-meaning relationships can be used to predict words‚Äô distributional semantic vectors from their forms. Furthermore, we introduce a novel metric learning algorithm that can learn weighted edit distances that minimize kernel regression error. Our results suggest that the English lexicon exhibits far more global form-meaning systematicity than previously discovered, and that much of this systematicity is focused in localized formmeaning patterns. 
Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods, which received less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, that achieves results comparable to distributional methods. We then extend the approach to integrate both pathbased and distributional signals, signiÔ¨Åcantly improving upon the state-of-the-art on this task. 
We present an approach to improve statistical machine translation of image descriptions by multimodal pivots deÔ¨Åned in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolingually captioned images, and on state-ofthe-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines. 
Combining deep neural networks with structured logic rules is desirable to harness Ô¨Çexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative Ô¨Årst-order logic rules. SpeciÔ¨Åcally, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems. 
Linguistic drift is a process that produces slow irreversible changes in the grammar and function of a language‚Äôs constructions. Importantly, changes in a part of a language can have trickle down effects, triggering changes elsewhere in that language. Although such causally triggered chains of changes have long been hypothesized by historical linguists, no explicit demonstration of the actual causality has been provided. In this study, we use cooccurrence statistics and machine learning to demonstrate that the functions of morphological cases experience a slow, irreversible drift along history, even in a language as conservative as is Icelandic. Crucially, we then move on to demonstrate ‚Äìusing the notion of Granger-causality‚Äì that there are explicit causal connections between the changes in the functions of the different cases, which are consistent with documented processes in the history of Icelandic. Our technique provides a means for the quantitative reconstruction of connected networks of subtle linguistic changes. 
 The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user‚Äôs intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to signiÔ¨Åcantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning. 
We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-ofspeech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-speciÔ¨Åc transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models. 
Rescoring approaches for parsing aims to re-rank and change the order of parse trees produced by a general parser for a given sentence. The re-ranking performance depends on whether or not the rescoring function is able to precisely estimate the quality of parse trees by using more complex features from the whole parse tree. However it is a challenge to design an appropriate rescoring function since complex features usually face the severe problem of data sparseness. And it is also difficult to obtain sufficient information requisite in re-estimatation of tree structures because existing annotated Treebanks are generally small-sized. To address the issue, in this paper, we utilize a large amount of auto-parsed trees to learn the syntactic and sememtic information. And we propose a simple but effective score function in order to integrate the scores provided by the baseline parser and dependency association scores based on dependency-based word embeddings, learned from auto-parsed trees. The dependency association scores can relieve the problem of data sparseness, since they can be still calculated by word embeddings even without occurrence of a dependency word pair in a corpus. Moreover, semantic role labels are also considered to distinct semantic relation of word pairs. Experimental results show that our proposed model improves the base Chinese parser significantly. Keywords: Word Embedding, Parsing, Word Dependency, Rescoring. 1. Introduction How to solve structural ambiguity is an important task in building a high-performance statistical parser, particularly for Chinese. Since Chinese is an analytic language, words play different grammatical functions without inflections. A great deal of ambiguous structures will be produced by a parser if no structure evaluation is applied. Therefore, the major task of a  ÔÄ™ Institute of Information science, Academia Sinica, Taipei, Taiwan E-mail: {morris, ma}@iis.sinica.edu.tw  20  Yu-Ming Hsieh and Wei-Yun Ma  parser is to determine the most plausible parse tree from these ambiguity structures. Re-ranking approaches are widely used in parsing natural language sentences for further advancing the performance of statistical parsers (Shen, Sarkar & Toshi, 2003; Hsieh, Yang & Chen, 2007; Johnson & Ural, 2010; Le, Zuidema & Scha, 2013; Zhu, Qiu, Chen & Huang, 2015). It is an intuitive and efficient strategy to determine the most plausible parse tree from a set of candidate parse trees of a sentence through a rescoring approach. Treebanks are a widely used resource in parsing task, as it provides useful statistical distributions regarding grammar rules, words, part-of-speeches (PoS), and word-to-word association1. However it is difficult to obtain sufficient information requisite in re-estimation of tree structures from existing annotated Treebanks since sizes of treebanks are generally small and insufficient, resulting in a common problem of data sparseness, especially for more complex features in a re-scoring scenario, such as word-to-word dependency associations. So learning information and knowledge from analyzing large-scaled unlabeled data is a compulsory strategy, which is proved useful in the previous works (Wu, 2003; Chen, 2008; Yu et al., 2008). In this paper, we utilize a large amount of auto-parsed trees to learn the syntactic and semantic information and present a simple but effective score function in order to integrate the scores provided by the base parser and word-to-word dependency association scores. The dependency association scores are based on dependency-based word embeddings, learned from a large amount of auto-parsed tress. The score function can relieve the problem of data sparseness, since the dependency association scores can still be calculated by word embeddings even without the occurrence of a dependency word pair in a corpus. In addition, Kim, Song, Park & Lee (2015) proves that the dependency labels (i.e., semantic role labels) in re-ranking parsed tree are important information. As a result, semantic role labels are also considered to distinct semantic relation of word pairs. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks (Turian, Ratinov & Bengio, 2010; Socher et al., 2013; Bansal, Gimpel & Livescu, 2014). The word2vec package (Mikolov, Chen, Corrado & Dean, 2013) is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec package learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding (Bansal et al., 2014; Levy & Goldberg, 
The performance of automatic speech recognition (ASR) often degrades dramatically in noisy environments. In this paper, we present a novel use of dictionary learning approach to normalizing the magnitude modulation spectra of speech features so as to retain more noise-resistant and important acoustic characteristics. To this end, we employ the K-SVD method to create sparse representations for a common set of basis vectors that span the intrinsic temporal structure inherent in the modulation spectra of clean training speech features. In addition, taking into account the non-negativity property of amplitude modulation spectrum, we utilize the nonnegative K-SVD method, paired with the nonnegative sparse coding method, to capture more noise-robust features. All experiments were conducted on the Aurora-2 corpus and task. The empirical evidence shows that our methods can offer substantial improvements over the baseline NMF method. Finally, we also integrate the proposed variants of the K-SVD method with other well-known robustness methods like Advanced Front-End (AFE), Cepstral Mean and Variance Normalization (CMVN) and Histogram Equalization (HEQ) to further confirm their utility. Keywords: Robustness, Automatic Speech Recognition, Modulation Spectrum, Sparse Coding, Dictionary Learning. 1. Á∑íË´ñ Ë™ûÈü≥ÊòØ‰∫∫È°ûÊúÄÂ∏∏‰ΩøÁî®ÁöÑ‰∏ÄÁ®ÆË®äÊÅØË°®ÈÅîÊñπÂºè„ÄÇÂú®Êó•Â∏∏ÁîüÊ¥ª‰∏≠ÔºåË™ûÈü≥ÂæÄÂæÄÂ∏∂ÊúâÂ§ßÈáè‰∏îÈáçË¶ÅÁöÑ Ë®äÊÅØÔºåÂõ†Ê≠§ÊàëÂÄëÂ∞çË™ûÈü≥Ë®äËôüÈÄ≤Ë°åËôïÁêÜ„ÄÅÂàÜÊûêÔºåÊØ´ÁÑ°ÁñëÊÖÆÁöÑÈùûÂ∏∏ÂÖ∑ÊúâÁôºÂ±ïÊÄßÔºåËÄåË™ûÈü≥Ëæ®Ë≠ò ËóâÁî±Â∞áË™ûÈü≥Ë®äËôüËΩâÊèõÊàêÊñáÂ≠óÁÇ∫ÁõÆÊ®ôÔºåÁÑ°Ë´ñÊòØÂú®Ë™ûÊÑèÊÉÖÊÑüÂàÜÊûê„ÄÅË™ûË®ÄËºîÂä©Â≠∏Áøí„ÄÅÊô∫ÊÖßÊ©üÂô® Ë™ûÈü≥Ë≠òÂà•‰∏äÊúâËëóÁõ∏Áï∂Âª£Ê≥õÁöÑÊáâÁî®„ÄÇ ÁÑ∂ËÄåÂ§ßÂ§öÊï∏ÁöÑËá™ÂãïË™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±ÔºåÂú®‰∏çË¢´Âπ≤ÊìæÁöÑÊÉÖÊ≥Å‰∏ãÔºåÁöÜËÉΩÁç≤ÂæóËâØÂ•ΩÁöÑË™ûÈü≥Ëæ®Ë≠ò ÊïàÊûúÔºå‰ΩÜÊòØÂú®ÁèæÂØ¶Áí∞Â¢É‰∏≠ÔºåËá™ÂãïË™ûÈü≥Ëæ®Ë≠òÂçªÂæÄÂæÄÂõ†ÁÇ∫Ê∏¨Ë©¶Áí∞Â¢ÉËàáË®ìÁ∑¥Áí∞Â¢É‰∏çÂåπÈÖç (Mismatch) (Tabrikian, Fostck & Messer, 1999)Ôºå‰ΩøÂæóÊ≠§Á≥ªÁµ±‰πãÊïàËÉΩË°∞ÈÄÄ‰πãÁèæË±°„ÄÇ‰∏äËø∞ÊâÄÈÄ† ÊàêÁí∞Â¢É‰∏çÂåπÈÖçÂïèÈ°åÁöÑÁ®ÆÁ®ÆÂõ†Á¥†ÂåÖÂê´‰∫ÜÔºöË™ûËÄÖËÖîË™øËÆäÁï∞„ÄÅÂä†ÊàêÊÄßËÉåÊôØÈõúË®ä„ÄÅÊë∫Á©çÊÄßÈÄöÈÅìÈõú Ë®ä Âèä ÂÖ∂ ‰ªñ Ë™û ËÄÖ Áôº Èü≥ ÁöÑ Âπ≤ Êìæ Á≠â „ÄÇ ÊâÄ Ë¨Ç ÁöÑ Ë™û Èü≥ Ëæ® Ë≠ò ‰πã Âº∑ ÂÅ• ÊÄß ÊäÄ Ë°ì (Li, Deng, Gong & Haeb-Umbach, 2014)ÔºåÂç≥ÊòØËá¥ÂäõÊñºÈôç‰Ωé‰∏äËø∞Âõ†Á¥†ÊâÄÂ∏∂‰æÜ‰πãÂΩ±ÈüøÔºåÈÄ≤ËÄå‰ΩøË™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±Âú® ‰∏çÂåπÈÖçÂïèÈ°åÂ≠òÂú®ÁöÑÁí∞Â¢É‰∏ãÔºå‰ªçËÉΩ‰øùÊúâ‰∏ÄÂÆöÁöÑËæ®Ë≠òËÉΩÂäõ„ÄÇ ËøëÂπ¥‰æÜÔºåÂ≠óÂÖ∏Â≠∏Áøí(Dictionary Learning)ÊñπÊ≥ïË¢´Âª£Ê≥õÂú∞ÊáâÁî®Âú®ÂúñÂÉè(Lu, Shi & Jia, 2013)„ÄÅ Ë™ûÈü≥ËôïÁêÜ‰πãÈ†òÂüü(Gemmeke, Viratnen & Hurmalainen, 2011) (He, Sun & Han, 2015)ÔºåÂÖ∂Ê†∏ÂøÉ Ê¶ÇÂøµÊòØÂà©Áî®Â≠óÂÖ∏‰æÜÁ∑öÊÄßÂú∞Ë°®Á§∫Ë®äËôü‰∏¶Áç≤ÂæóÂÖ∂Á®ÄÁñèË°®Á§∫(Sparse Representation)ÔºåÂ≠óÂÖ∏Â≠∏Áøí ÊòØÂæûÂ≠óÂÖ∏(Dictionary)‰∏≠ÈÅ∏ÂèñÂ∞ëÈáèÁöÑÂéüÂ≠ê(Atoms)‰æÜË°®Á§∫Ë®äËôüÔºåÂÖ∂‰∏≠ÊØè‰∏ÄÂÄãÂéüÂ≠êÈÉΩÂèØ‰ª•Áï∂‰Ωú  ‰ΩøÁî®Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊñºÂº∑ÂÅ•ÊÄßË™ûÈü≥Ëæ®Ë≠ò  37  ÊòØ‰∏ÄÂÄãÂü∫Á§éË®äËôüÁöÑË°®ÈÅîÔºåËÄåÊâÄÊúâÂéüÂ≠êÁµÑÊàêÁöÑÈõÜÂêàÁ®±ÁÇ∫Â≠óÂÖ∏„ÄÇÂú®Â≠óÂÖ∏Â≠∏ÁøíÊñπÊ≥ï‰∏≠ÔºåÊàëÂÄëÂèØ ‰ª•Áõ¥Êé•ÊåëÈÅ∏Á∂ìÈÅéËôïÁêÜÈÅéÂæåÁöÑË®äËôüÊàêÁÇ∫ÁØÑÊú¨Â≠óÂÖ∏(Gemmeke et al., 2011)ÔºåÊàñËÄÖÊòØÁî±ÂÖ∂‰ªñËá™Âãï Â≠∏ÁøíÂ≠óÂÖ∏ÁöÑÊñπÊ≥ï‰æÜÊ±ÇÂæóÂ≠óÂÖ∏Ôºå‰∏ÄËà¨Â∏∏Ë¶ãÁöÑÂ≠óÂÖ∏Â≠∏ÁøíÊñπÊ≥ïÊúâÊúÄÂÑ™ÊñπÂêëÊ≥ï(Method of Optimal Directions) (Engan, Aase & Husoy, 1999)„ÄÅK-Â•áÁï∞ÂÄºÂàÜËß£Ê≥ï(K-SVD) (Aharon, Elad & Bruckstein, 2006)„ÄÅÈö®Ê©üÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï(Stochastic Gradient Descent) (Bottou, 1998)ÂèäÁ∑ö‰∏äÂ≠óÂÖ∏ Â≠∏ÁøíÊ≥ï(Online Dictionary Learning) (Mairal, Bach, Ponce & Sapiro, 2010)„ÄÇÂè¶Â§ñÂú®Â≠óÂÖ∏Â≠∏Áøí Ê≥ï‰∏≠ÔºåÂÖ∂ÂéüÂ≠êÁõ∏Â∞çÊáâÁöÑÊ¨äÈáç‰πüÈ†àË¶Å‰∏Ä‰ΩµÊõ¥Êñ∞ÔºåÈóúÊñºÊ¨äÈáçÁöÑÊõ¥Êñ∞ÊñπÂºèÂèØ‰ª•Áî±ÁØÑÊï∏ÁöÑÈôêÂà∂ÂÅö ÂçÄÂàÜÔºå0-ÂºèÁØÑÊï∏(0-Norm)Â∏∏Ë¶ãÁöÑÊñπÊ≥ïÁÇ∫ÂåπÈÖçËøΩËπ§ÊºîÁÆóÊ≥ï(Matching Pursuit, MP) (Mallat & Zhang, 1993)„ÄÅÊ≠£‰∫§ÂåπÈÖçËøΩËπ§ÊºîÁÆóÊ≥ï(Orthogonal Matching Pursuit, OMP) (Pati, Rezaiifar & Krishnaprased, 1993)Ôºå‰∏äËø∞ÂÖ©Á®ÆÊñπÂºèÈÉΩÊòØÈÄèÈÅéË®àÁÆóÊÆòÂ∑ÆËàáÂéüÂ≠êÁöÑÈóúËÅØÁ®ãÂ∫¶‰æÜÊ±ÇÂèñÊ¨äÈáç„ÄÇ 1-ÂºèÁØÑÊï∏(1-Norm)Â∏∏Ë¶ãÁöÑÊñπÊ≥ïÁÇ∫Âü∫Á§éËøΩÊ±ÇÊ≥ï(Basis Pursuit, BP) (Chen, Donoho & Saunders, 2001)‰ª•ÂèäÊúÄÂ∞èÁµïÂ∞çÂ£ìÁ∏ÆÈÅ∏ÊìáÊ≥ï(LASSO) (Tibshirani, 1996)ÔºåÊ≠§ÂÖ©Á®ÆÊñπÊ≥ïÂ∞áÁõÆÊ®ôÂáΩÊï∏Ë¶ñÁÇ∫ ÊúÄ‰Ω≥Áï´ÂúñÂáΩÊï∏Ôºå‰∏¶ÈÄèÈÅéËø≠‰ª£Êõ¥Êñ∞Ê±ÇÂæóÂÖ∂Ëß£„ÄÇ Êú¨Ë´ñÊñáÊó®Âú®Êé¢Á©∂‰ΩøÁî®Â≠óÂÖ∏Â≠∏ÁøíÊ≥ï‰ª•Âèä‰∏Ä‰∫õÊîπÈÄ≤ÊñπÊ≥ï‰æÜÂàÜËß£Ë™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜÔºå‰ª•Áç≤ ÂæóËºÉÂÖ∑Âº∑ÂÅ•ÊÄßÁöÑË™ûÈü≥ÁâπÂæµ„ÄÇÂ≠óÂÖ∏ÈáçÂª∫ÊòØÂ≠óÂÖ∏Â≠∏ÁøíÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂïèÈ°åÔºåÂÖ∂ÁõÆÊ®ôÂú®ÊñºÂ¶Ç‰ΩïÂæûÂéü ÂßãË®äËôü‰∏≠Â≠∏ÁøíÂá∫ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑÂéüÂ≠ê‰æÜÁµÑÊàêÂ≠óÂÖ∏Ôºå‰∏îÂú®Â≠óÂÖ∏Â≠∏ÁøíÊñπÊ≥ïË£°ÈÄöÂ∏∏Ë¢´Èö®Ëëó‰ΩøÁî®Á®Ä ÁñèÁ∑®Á¢º‰æÜÊ±ÇÂèñÂéüÂ≠êÁöÑÊ¨äÈáçÔºåËÄåÁ®ÄÁñèÁ∑®Á¢ºÁöÑÁõÆÁöÑÂú®ÊñºÂ∞áË®äËôüË°®Á§∫ÁÇ∫ÂêÑÂÄãÂéüÂ≠êÁöÑÁ®ÄÁñèÁ∑öÊÄßÁµÑ ÂêàÔºåÊúüÊúõËÉΩÂ§†Ê±ÇÂèñÂÖ∑Ë™øËÆäÈ†ªË≠úÂ±ÄÈÉ®ÊÄßÁöÑÈáçË¶ÅË≥áË®ä„ÄÇÂú®Êú¨Ë´ñÊñá‰∏≠ÔºåÊàëÂÄëÂàÜÂà•‰ΩøÁî®‰∫Ü K-SVD ÊºîÁÆóÊ≥ïÊê≠ÈÖçÂåπÈÖçËøΩËπ§ÊºîÁÆóÊ≥ï‰ª•ÂèäÊ≠£‰∫§ÂåπÈÖçËøΩËπ§ÊºîÁÆóÊ≥ïÔºå‰æÜÂæóÂà∞‰πæÊ∑®ÁöÑË™ûÈü≥Ë™øËÆäÈ†ªË≠úÂº∑Â∫¶ ÊàêÂàÜ„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÂõ†Ë™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜÁöÜÁÇ∫Ê≠£ÂÄºÔºåÊâÄ‰ª•ÊàëÂÄëÊèêÂá∫‰ΩøÁî®ÈùûË≤† K-SVD Êê≠ÈÖçÈùû Ë§áÊï∏Á®ÄÁñèÁ∑®Á¢º(Hoyer, 2004)‰æÜËß£Ê±∫ÈÄôÂÄãË≠∞È°åÔºå‰ª•Â¢ûÈÄ≤Ëá™ÂãïË™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±Âú®ÊäóÂô™‰∏äÁöÑÊïàËÉΩ„ÄÇ Ê≠§Â§ñÔºåÊàëÂÄëÂòóË©¶Â∞áÂ≠óÂÖ∏Â≠∏ÁøíÊñπÊ≥ïËàá‰∏Ä‰∫õÁ∂ìÂÖ∏ÁöÑÁâπÂæµÂº∑ÂÅ•ÊÄßÊäÄË°ìÁµêÂêàÔºåÂ¶ÇÔºöÈÄ≤ÈöéÂâçÁ´ØÊ®ôÊ∫ñ Ê≥ï(Advanced Front-End, AFE)„ÄÅËÆäÁï∞Êï∏Ê≠£Ë¶èÂåñÊ≥ï(Cepstral Mean and Variance Normalization, CMVN)„ÄÅÁµ±Ë®àÂúñÁ≠âÂåñÊ≥ï(Histogram Equalization, HEQ)Ôºå‰ª•È©óË≠âÈÄô‰∫õÊîπÈÄ≤ÊñπÊ≥ï‰πãÂØ¶Áî®ÊÄß„ÄÇ 2. Áõ∏ÈóúÊñáÁçª Âú®Ë™ûÈü≥Ëæ®Ë≠ò‰∏≠ÔºåÂº∑ÂÅ•ÊÄßË™ûÈü≥ÁâπÂæµÊäÄË°ì‰∏ªË¶ÅÊúâÂÖ©ÂÄãÊñπÊ≥ïÔºåÁ¨¨‰∏ÄÊòØ‰ª•Ê®°ÂûãÁÇ∫Âü∫Á§éÁöÑÂº∑ÂÅ•ÊÄßÊäÄ Ë°ì (Model-based Technique) Ôºå Á¨¨ ‰∫å ÊòØ ‰ª• Ë™û Èü≥ Áâπ Âæµ ÁÇ∫ Âü∫ Á§é ÁöÑ Âº∑ ÂÅ• ÊÄß ÊäÄ Ë°ì (Feature-based Technique)ÔºåÂàÜÂà•‰ªãÁ¥πÂ¶Ç‰∏ãÔºö Á¨¨‰∏ÄÔºå‰ª•Ê®°ÂûãÁÇ∫Âü∫Á§éÁöÑÂº∑ÂÅ•ÊÄßÊäÄË°ìÊòØ‰ΩøÁî®Â∞ëÈáèÁöÑÊ∏¨Ë©¶Áí∞Â¢É‰πãË™øÈÅ©Ë™ûÊñô‰æÜÂ∞çËÅ≤Â≠∏Ê®°Âûã ÈÄ≤Ë°åË™øÊï¥Ôºå‰ΩøËÅ≤Â≠∏Ê®°ÂûãÂèØ‰ª•ÂéªËøë‰ººÊñºËº∏ÂÖ•ÈõúË®äË™ûÈü≥ÁöÑÊ©üÁéáÂàÜÂ∏ÉÂèÉÊï∏ÔºåÈÅîÂà∞Èôç‰ΩéÁí∞Â¢É‰∏çÂåπ ÈÖçÁöÑÊÉÖÂΩ¢„ÄÇÂ∏∏Ë¶ãÁöÑÊäÄË°ìÊúâÊúÄÂ§ßÁõ∏‰ººÂ∫¶Á∑öÊÄßÂõûÊ≠∏Ê≥ï(Maximum Likelihood Linear Regression, MLLR) ( Leggetter & Woodland, 1995)„ÄÅÊúÄÂ§ß‰∫ãÂæåÊ©üÁéáÊ≥ïÂâá(Maximum a Posteriori, MAP) (Gauvain & Lee, 1994)„ÄÅÂπ≥Ë°åÊ®°ÂûãÁµêÂêàÊ≥ï(Parallel Model Combination, PMC) (Gales & Young, 1996)„ÄÅÂêëÈáèÊ≥∞ÂãíÁ¥öÊï∏(Vector Taylor Series, VTS) (Kim, Un & Kim, 1998)„ÄÅÈÅ∫Â§±Áâπ ÂæµÁêÜË´ñ(Missing Feature Theory, MFT) (Van Segbroeck & Van Hamme, 2011)„ÄÇ Á¨¨‰∫åÔºå‰ª•Ë™ûÈü≥ÁâπÂæµÁÇ∫Âü∫Á§éÁöÑÂº∑ÂÅ•ÊÄßÊäÄË°ìÊòØÂú®‰∏çÊõ¥ÊîπËÅ≤Â≠∏Ê®°ÂûãÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂà©Áî®‰πæÊ∑®ÁöÑ  38  È°èÂøÖÊàê Á≠â  Ë™ûÈü≥ÁâπÂæµÂéªË®ìÁ∑¥ÔºåÊúüÊúõÂ∞áÂ∏∂ÊúâÈõúË®äÁöÑË™ûÈü≥ÁâπÂæµÈÇÑÂéüÊàê‰πæÊ∑®ÁöÑË™ûÈü≥ÁâπÂæµÔºåÊú¨Êñá‰ΩøÁî®Ë™ûÈü≥ÂèÉ Êï∏Ê≠£Ë¶èÂåñÊ≥ï(Feature Normalization)ÔºåÊ≠§ÊñπÊ≥ïÁõÆÁöÑÂú®Ê≠£Ë¶èÂåñË™ûÈü≥ÁâπÂæµÊú¨Ë∫´ÁöÑÁâπÂæµÂÄºÂèäÁµ±Ë®à ÂàÜÂ∏ÉÔºåÂÜçÂà©Áî®Ê∏¨Ë©¶Ë™ûÈü≥ÁâπÂæµÁöÑÁâπÂæµÂÄº‰æÜÊ∂àÈô§ÈõúË®äÂπ≤ÊìæÊâÄÂ∏∂‰æÜÁöÑÂΩ±ÈüøÔºåÊ≠§ÊñπÊ≥ïÂ∏∏Ë¶ãÁöÑÊäÄË°ì ÊúâÂÄíÈ†ªË≠úÂπ≥ÂùáÂÄºÊ∏õÂéªÊ≥ï(Cepstral Mean Subtraction, CMS) (Viikki & Laurila, 1998)„ÄÅËÆäÁï∞Êï∏ Ê≠£Ë¶èÂåñÊ≥ï(Cepstral Mean and Variance Normalization, CMVN) (Viikki, Bye & Laurila, 1998)„ÄÅ Áµ±Ë®àÂúñÁ≠âÂåñÊ≥ï(Histogram Equalization, HEQ) (de la Torre et al., 2005)„ÄÅÂÄíÈ†ªË≠úÂπ≥ÂùáÂÄºËàáËÆä Áï∞Êï∏Ê≠£Ë¶èÂåñÁµêÂêàËá™ÂõûÊ≠∏ÂãïÊÖãÂπ≥ÂùáÊøæÊ≥¢Ê≥ï(Cepstral Mean and Variance Normalization plus Auto-regressive-moving Average Filtering, MVA) (Chen & Bilmes, 2007)„ÄÇ Ë™ûÈü≥ÂèÉÊï∏Ê≠£Ë¶èÂåñÊ≥ïÁöÑÁâπËâ≤ÊòØÊúâÊïà‰∏îÂÆπÊòìÂØ¶ÁèæÊñºÂ§ßÈÉ®ÂàÜÁöÑËá™ÂãïË™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±Ôºå‰ΩÜÊòØÂè™ ÈÅ©Áî®Êñº‰ΩúÁî®ÊñºÂô™Èü≥Á∑©ÊÖ¢ËÆäÂåñÁöÑÊÉÖÂΩ¢Ôºå‰∏¶‰∏îÂÖ∂ÊîπÈÄ≤ÊïàÊûúÂæÄÂæÄÊòØÊúâÈôêÁöÑ„ÄÇËÄåË™ûÈü≥Ê®°ÂûãË™øÈÅ©Ê≥ïÔºå Âú®Ëæ®Ë≠ò‰∏äÂèØ‰ª•ÊúâËºÉÂ•ΩÁöÑÊïàÊûúÔºåÁî±ÊñºÊ≠§ÊñπÊ≥ïÊúÉÊ†πÊìöÂô™Èü≥Áí∞Â¢É‰æÜÊõ¥Êñ∞ËÅ≤Â≠∏Ê®°ÂûãÔºåÊâÄ‰ª•Ê≠§ÊñπÊ≥ï Âú®ÂØ¶‰Ωú‰∏äÈùûÂ∏∏ÁöÑËÄóÊôÇ„ÄÇ Ê≠§Â§ñÔºåËàáÊú¨Ë´ñÊñáÊúÄÁõ∏ÈóúÁöÑÁ†îÁ©∂ÊòØË™ûÈü≥ÁâπÂæµÁöÑË™øËÆäÈ†ªË≠úÂº∑ÂÅ•ÊÄßÊäÄË°ì(ÂºµÂ∫≠Ë±™Ôºå2015)Ôºå ÂÖ∂Ë´ñÊñáÂØ¶ÂÅö‰∫ÜÂ§öÁ®ÆÈùûË≤†Áü©Èô£ÂàÜËß£Ê≥ïËôïÁêÜÊñºË™øËÆäÈ†ªË≠úÁöÑÂº∑ÂÅ•ÊÄßÊäÄË°ì„ÄÇ‰∏¶Âú®Á®ÄÁñèÈùûË≤†Áü©Èô£ÂàÜ Ëß£Ê≥ï(Sparse Nonnegative Matrix Factorization, SNMF) (Hoyer, 2004)ÈáùÂ∞çÂü∫Â∫ïÁü©Èô£ÂÅöÁ®ÄÁñè ÂåñÁöÑÈôêÂà∂ÔºåÂÖ∂ÁµêÊûúÈ°ØÁ§∫Â∞çÂü∫Â∫ïÁü©Èô£Á®ÄÁñèÂåñÂ∞çË™ûÈü≥Âº∑ÂÅ•ÊÄßÁöÑËæ®Ë≠òÁµêÊûúÊòØÊúâÁõ∏Áï∂ÁöÑÂä©Áõä„ÄÇÂõ† Ê≠§ÊàëÂÄëÂà©Áî®Á®ÄÁñèÁ∑®Á¢ºÁöÑÊñπÊ≥ïÔºå‰∏¶‰∏îÂª∂‰º∏Âà∞Â≠óÂÖ∏Â≠∏ÁøíÊ≥ï„ÄÇÂà©Áî®Â∏∂ÊúâÁ®ÄÁñèÊÄßÁöÑÊñπÊ≥ïÔºåÊõ¥Á≤æÁ¢∫ Âú∞Ë°®Á§∫Ë™ûÈü≥Ë®äËôü„ÄÇÊàëÂÄëÂ∏åÊúõÂú®Ë®ìÁ∑¥ÈöéÊÆµÊôÇÔºåËÆìÂ≠óÂÖ∏Â≠∏Âæó‰∏çÂèóÂô™Èü≥Âπ≤ÊìæÁöÑ‰πæÊ∑®Ë™ûÈü≥ÁâπÂæµÔºå ‰∏¶Âú®Ê∏¨Ë©¶ÊôÇËÆìÂ∏∂ÊúâÂô™Èü≥ÁöÑË™ûÊñôÈÄèÈÅé‰πæÊ∑®Ë™ûÈü≥ÁâπÂæµÂ≠óÂÖ∏Âú®‰∏çË∂ÖÈÅéË™§Â∑Æ‰ª•ÂèäÁØÑÊï∏ÁöÑÈôêÂà∂‰∏ãÊ±Ç ÂèñÂÖ∂Â∞çÊáâÁöÑÁ®ÄÁñèÊ¨äÈáçÔºå‰ª•Ê≠§ÈÇÑÂéüÂô™Èü≥Ë™ûÊñôÔºåÊúüÊúõËÉΩÈôç‰ΩéÁí∞Â¢É‰∏çÂåπÈÖçÊÄß‰∏¶Áç≤ÂæóËºÉÂÑ™ËâØÁöÑËæ® Ë≠òÊïàÊûú„ÄÇ 3. Ë™øËÆäÈ†ªË≠úÊ≠£Ë¶èÂåñÊ≥ï  3.1 Ë™øËÆäÈ†ªË≠ú‰πãÁ∞°‰ªã Â∞çÊñº‰∏ÄË™ûÈü≥ÁâπÂæµÊôÇÈñìÂ∫èÂàó  ËÄåË®ÄÔºåÂÖ∂Ë™øËÆäÈ†ªË≠úÂÆöÁæ©Â¶Ç‰∏ãÔºö  ‚àë  Ôºå0  (1)  ÂÖ∂‰∏≠Ôºån Ëàá k ‰æùÂ∫èÁÇ∫Èü≥Ê°ÜÁ¥¢ÂºïËàáË™øËÆäÈ†ªÁéáÁ¥¢ÂºïÔºåDFT ÁÇ∫Èõ¢Êï£ÂÇÖÁ´ãËëâËΩâÊèõ(Discrete Fourier Transform, DFT)Ôºå ‰ª£Ë°®Êüê‰∏ÄÁ∂≠Â∫¶Ë™ûÈü≥ÁâπÂæµÊôÇÈñìÂ∫èÂàóÔºå ‰ª£Ë°®Ë™ûÈü≥ÁâπÂæµÊôÇÈñìÂ∫èÂàó ÁöÑË™øËÆäÈ†ªË≠ú„ÄÇÂºè(1)ÂèØÁúãÂá∫Ë™øËÆäÈ†ªË≠úÂèØ‰ª•Âª£Ê≥õÁöÑÂàÜÊûêË™ûÂè•‰∏≠Ë™ûÈü≥ÁâπÂæµÈö®ÊôÇÈñìËÆäÂåñÁöÑË≥áË®ä„ÄÇ ËÄå È†ªË≠úÂ∫èÂàóÂèØË¶ñÁÇ∫‰∏ÄÁ®ÆÂ∞çÊñºÂéüÂßãË™ûÈü≥Ë®äËôü‰ΩúÈôç‰ΩéÂèñÊ®£(Down-Sampled)ÂæåÁöÑË™øËÆäË®äËôü (Áî±Ë®äËôüÂèñÊ®£ÁéáËΩâËá≥Èü≥Ê°ÜÂèñÊ®£Áéá)ÔºåÊ≠§Â∫èÂàóÂç≥ÁÇ∫ÊâÄÂ±¨Ë™ûÈü≥ÁâπÂæµÊôÇÈñìÂ∫èÂàó‰πãË™øËÆäÈ†ªË≠ú (Modulation Spectrum)„ÄÇÁî±Âºè(1)ÂèØÁü•ÔºåË™øËÆäÈ†ªË≠ú ‰πãÊúÄÈ´òÈ†ªÁéáËàáÁâπÂæµÂ∫èÂàó ‰πãÂèñÊ®£È†ª Áéá(Èü≥Ê°ÜÂèñÊ®£Áéá)Áõ∏Èóú„ÄÇ‰æãÂ¶ÇÔºåÂú®‰∏ÄËà¨Ë®≠ÂÆö‰∏ãÔºåÈü≥Ê°ÜÂèñÊ®£ÁéáÁÇ∫ 100 HzÔºåÂâáÊúÄÈ´òË™øËÆäÈ†ªÁéáÁÇ∫ 50 Hz„ÄÇ  ‰ΩøÁî®Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊñºÂº∑ÂÅ•ÊÄßË™ûÈü≥Ëæ®Ë≠ò  39  ÈÅéÂéªÂ∑≤Êúâ‰∏çÂ∞ëÂ≠∏ËÄÖÁ†îÁ©∂Ë™ûÈü≥ÁâπÂæµ‰πãË™øËÆäÈ†ªË≠úÁöÑÁâπÊÄßÔºåÁôºÁèæ‰∫ÜË™øËÆäÈ†ªË≠ú‰∏≠ÁöÑ‰ΩéÈ†ªÊàêÂàÜ ÊòØÊØîÈ´òÈ†ªÊàêÂàÜÈÇÑË¶ÅÈáçË¶ÅÁöÑÁâπÊÄß(Chen & Bilmes, 2007)„ÄÇËÄåË™øËÆäÈ†ªË≠ú‰πã‰ΩéÈ†ªÊàêÂàÜ(Á¥Ñ 1Hz Ëá≥ 16Hz)Â∞çÊñºË™ûÈü≥Ëæ®Ë≠òÁ≤æÁ¢∫Â∫¶‰πüÊúâÂØÜÂàáÁöÑÈóú‰øÇÔºåÊΩõËóè‰∫ÜÊúÄÈáçË¶ÅÁöÑË™ûÊÑèË≥áË®ä„ÄÇÂÖ∂‰∏≠ÔºåÊúÄÈáçË¶ÅÁöÑ ÊòØ‰ΩçÊñº 4 HzÔºåÊúâÂ≠∏ËÄÖÊåáÂá∫Ôºå4 Hz ÊòØ‰∫∫ËÄ≥ËÅΩË¶∫ÊúÄÁÇ∫ÊïèÊÑü‰πãË™øËÆäÈ†ªÁéá(Gales & Young, 1996)Ôºõ ‰πüÊúâÂ≠∏ËÄÖË™çÁÇ∫Ôºå4 Hz ÁÇ∫‰∫∫È°ûÂ§ßËÖ¶ÁöÆÂ±§ÊÑüÁü•‰πãÈáçË¶ÅË™øËÆäÈ†ªÁéá(Kim et al., 1998)„ÄÇÁï∂Ë™ûÈü≥Ë®ä ËôüÂèóÂà∞ÈõúË®äÂΩ±ÈüøÊôÇÔºåÂÖ∂Ë™ûÈü≥ÁâπÂæµÊôÇÈñìÂ∫èÂàóÊúÉÂèóÂà∞ÂΩ±ÈüøËÄåÂ§±ÁúüÔºåÂèäÂÖ∂Ë™øËÆäÈ†ªË≠ú‰πüÊúÉË∑üËëóÂèó Âà∞ÁâΩÈÄ£„ÄÇÂæàÂ§öÂ≠∏ËÄÖÊèêÂá∫‰ΩúÁî®Âú®Ë™øËÆäÈ†ªË≠úÁöÑÊ≠£Ë¶èÂåñÊ≥ïÔºå‰ª•ÊîπÂñÑË™øËÆäÈ†ªË≠úÂèóÂà∞ÈõúË®äÂπ≤ÊìæÁöÑÂΩ± Èüø„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂèØÂ∞áË®±Â§öÁôºÂ±ïÂú®Ë™ûÈü≥ÁâπÂæµÊôÇÈñìÂ∫èÂàóÁöÑÊ≠£Ë¶èÂåñÊ≥ïÊáâÁî®Âú®Ë™øËÆäÈ†ªË≠ú‰ΩøÂÖ∂Ê≠£Ë¶è ÂåñÔºåËÄåÊ≠£Ë¶èÂåñÁöÑÂ∞çË±°ÊòØÂ∞çÂÖ∂Ë™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ| |‰æÜÈÄ≤Ë°åËôïÁêÜÔºå‰∏¶‰øùÊåÅÂÖ∂Áõ∏‰ΩçËßí‰∏çËÆä ‚à† ÁöÑÈÉ®ÂàÜ„ÄÇÊé•ËëóËôïÁêÜÊõ¥Êñ∞ÂæåÁöÑÂº∑Â∫¶ÊàêÂàÜÊúÉËàáÂéüÂßãÁõ∏‰ΩçÊàêÂàÜÁµêÂêàÔºåÂÜçÁ∂ìÁî±ÂèçÂÇÖ Á´ãËëâËΩâÊèõ(Inverse Discrete Fourier Transform, IDFT)‰æÜÊ±ÇÂæóÊñ∞ÁöÑË™ûÈü≥ÁâπÂæµÊôÇÈñìÂ∫èÂàó„ÄÇËã•Ë™ø ËÆäÈ†ªË≠úÁöÑÂº∑Â∫¶ËÉΩÂ§†Ë¢´ÊúâÊïàÁöÑÊ≠£Ë¶èÂåñÔºå‰æøËÉΩÂ§†ÊúâÊïàËß£Ê±∫ÈõúË®äÁî¢ÁîüÁöÑÁí∞Â¢É‰∏çÂåπÈÖç‰πãÂïèÈ°åÔºå‰Ωø Ëá™ÂãïË™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±‰ΩøÁî®Êñ∞ÁöÑË™ûÈü≥ÁâπÂæµÊôÇËÉΩÂ§†Áç≤ÂæóËºÉ‰Ω≥ÁöÑËæ®Ë≠òÁéá„ÄÇ‰ª•‰∏ãÂ∞áÊúÉÁ∞°ÂñÆÂõûÈ°ß‰∏Ä‰∫õ Â∏∏Ë¶ãÁöÑË™øËÆäÈ†ªË≠úÊ≠£Ë¶èÂåñÊ≥ï„ÄÇ  3.2 Ë™øËÆäÈ†ªË≠úÂπ≥ÂùáÊ≠£Ë¶èÂåñÊ≥ï(Spectral Mean Normalization, SMN)  ÂÅáË®≠Áï∂ÂêÑÁ®ÆÈü≥Á¥†Âú®ÁêÜÊÉ≥Áí∞Â¢É‰∏≠Âç†ÁöÑÊØî‰æãÊé•Ëøë‰∏ÄËá¥ÊôÇÔºåÊØè‰∏ÄÁ∂≠Â∫¶ÁâπÂæµÁöÑË™øËÆäÈ†ªË≠ú‰πãÂπ≥ÂùáÂÄº  ÊáâË©≤ÁÇ∫‰∏ÄÂÄãÂÆöÂÄº(Huang, Tu & Hung, 2009)Ôºö  ||  (2)  Âú®Âºè(2)‰∏≠Ôºå| |ÁÇ∫ÂéüÂßãÁöÑË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜÔºå ÁÇ∫ÂñÆ‰∏ÄË™ûÂè•ÁöÑË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ‰πãÂπ≥  ÂùáÂÄºÔºå ÁÇ∫ÊâÄÊúâË®ìÁ∑¥Ë™ûÂè•ÁöÑË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ‰πãÂπ≥ÂùáÂÄºÔºåËÄå  ‰æøÊòØÊõ¥Êñ∞ÈÅéÂæåÁöÑË™øËÆäÈ†ª  Ë≠úÂº∑Â∫¶ÊàêÂàÜ„ÄÇ  3.3 Ë™ø ËÆä È†ª Ë≠ú Âπ≥ Âùá Ëàá ËÆä Áï∞ Êï∏ Ê≠£ Ë¶è Âåñ Ê≥ï (Spectral Mean and Variance Normalization, SMVN) Èô§‰∫ÜË¶ÅÊ≠£Ë¶èË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ‰πãÂπ≥ÂùáÂÄºÔºå‰πüË¶ÅÊ≠£Ë¶èÂÖ∂ËÆäÁï∞Êï∏(ÂºµÂ∫≠Ë±™Ôºå2015)„ÄÇÂÅáË®≠ÁâπÂæµ ÂêëÈáèÂèÉÊï∏‰πãÂπ≥ÂùáÂÄºÂú®ÁêÜÊÉ≥Áí∞Â¢É‰∏≠ÊØî‰æãÊé•Ëøë‰∏ÄËá¥ÊôÇÔºåÂπ≥ÂùáÂÄºÊáâÁÇ∫Èõ∂Ôºå‰∏îÁâπÂæµÂêëÈáèÂèÉÊï∏‰πãÂàÜ Â∏ÉÂèØ‰ª•Âà©Áî®ËÆäÁï∞Êï∏‰æÜÈÄ≤Ë°åÊ™¢Ê∏¨Ôºö  ||  (3)  Âú®Âºè(3)‰∏≠Ôºå Ëàá ÁÇ∫ÂñÆ‰∏ÄË™ûÂè•ÁöÑË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ‰πãÂπ≥ÂùáÂÄºËàáËÆäÁï∞Êï∏Ôºõ Ëàá ÁÇ∫ÊâÄÊúâË®ì Á∑¥Ë™ûÂè•ÁöÑË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ‰πãÂπ≥ÂùáÂÄºËàáËÆäÁï∞Êï∏Ôºå ‰æøÊòØÊõ¥Êñ∞ÈÅéÂæåÁöÑË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ„ÄÇ  3.4 Ë™øËÆäÈ†ªË≠úÁµ±Ë®àÂúñÁ≠âÂåñÊ≥ï(Spectral Histogram Equalization, SHE) Âà©Áî®ÈùûÁ∑öÊÄßÁöÑËΩâÊèõ(Nonlinear Transform)Ôºå‰∏çÂè™Â∞áË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ‰πãÂπ≥ÂùáÂÄºËàáËÆäÁï∞Êï∏‰Ωú Ê≠£Ë¶èÂåñÔºå‰πü‰ΩøË®ìÁ∑¥Ë™ûÂè•ËàáÊ∏¨Ë©¶Ë™ûÂè•ÁöÑË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜË∂®ÊñºÊìÅÊúâÂêå‰∏ÄÂÄãÊ©üÁéáÂàÜÂ∏ÉÂáΩÊï∏Ôºå  40  È°èÂøÖÊàê Á≠â  Ê≠£Ë¶èÂåñÂÖ®ÈÉ®ÈöéÂ±§ÁöÑÂãïÂ∑Æ(Viikki & Laurila, 1998)Ôºö  ||  (4)  Âú® Âºè (4) ‰∏≠ Ôºå ‚Äß ÁÇ∫ ÂñÆ ‰∏Ä Ë™û Âè• ÁöÑ Ë™ø ËÆä È†ª Ë≠ú Âº∑ Â∫¶ ÁöÑ Ê©ü Áéá ÂàÜ Â∏É (Probability Distribution Function, PDF)Ôºå ÂâáÊòØÂà©Áî®ÊâÄÊúâË®ìÁ∑¥Ë™ûÂè•‰πãË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊâÄÊ±ÇÁöÑÂèÉËÄÉÊ©üÁéáÂàÜÂ∏ÉÔºå ‰æøÊòØÊõ¥Êñ∞ÈÅéÂæåÁöÑË™øËÆäÈ†ªË≠úÂº∑Â∫¶ÊàêÂàÜ„ÄÇ  3.5 ÂàÜÈ†ªÊÆµË™øËÆäÈ†ªË≠úÁµ±Ë®àÊ≠£Ë¶èÂåñÊ≥ï Ê≠§ÊñπÊ≥ïÁöÑÊ¶ÇÂøµÊòØÊÉ≥Ë¶ÅÊîπÈÄ≤Ë™øËÆäÈ†ªË≠úÁµ±Ë®àÊ≠£Ë¶èÂåñÊ≥ïÔºõË™øËÆäÈ†ªË≠úÁµ±Ë®àÊ≠£Ë¶èÂåñÊ≥ïÊòØÂ∞áÂÖ®ÈÉ®Ë™øËÆä È†ªÂ∏∂ÁöÑÈ†ªË≠úÂº∑Â∫¶ÂÄºË¶ñÁÇ∫ÊòØÂêå‰∏ÄÈö®Ê©üËÆäÊï∏(Random Variable)ÁöÑÊ®£Êú¨(Samples)Ôºå‰∏îÂ∞á‰πã‰∏Ä‰Ωµ ÈÄ≤Ë°åÊ≠£Ë¶èÂåñÁöÑÂãï‰Ωú„ÄÇ‰ΩÜÊòØÂâçÈù¢ÊèêÂà∞Âú®Ë™ûÈü≥Ëæ®Ë≠ò‰∏≠Ôºå‰∏çÂêåË™øËÆäÈ†ªÁéáÁöÑÊàêÂàÜÊúâ‰∏çÂêåÁöÑÈáçË¶ÅÊÄßÔºå ‰ΩéÈ†ªÊàêÂàÜÊòØÊØîÈ´òÈ†ªÊàêÂàÜÈÇÑË¶ÅÁõ∏Â∞çÈáçË¶ÅÁöÑÔºåÂõ†ÁÇ∫Ë™ûË®ÄÁöÑÈáçË¶ÅË≥áË®äËºÉÈõÜ‰∏≠Êñº‰ΩéÈ†ªÊàêÂàÜ„ÄÇÂõ†Ê≠§ ÊúâÂ≠∏ËÄÖÊèêÂá∫Â∞áË™øËÆäÈ†ªÂ∏∂ÂàÜÊàêË®±Â§öÂ≠êÈ†ªÊÆµÔºåÂÜçÂàÜÂà•Â∞çÊØè‰∏ÄÂÄãÂ≠êÈ†ªÊÆµÁöÑÈ†ªË≠úÂº∑Â∫¶‰Ωú‰∏äËø∞ÊâÄÊèê ÁöÑË™øËÆäÈ†ªË≠úÊ≠£Ë¶èÂåñÁöÑÊñπÊ≥ïÔºåËÄå‰∏çÊòØÂñÆÁ¥îÁõ¥Êé•Â∞çÊï¥ÂÄãÂÖ®ÈÉ®Ë™øËÆäÈ†ªÂ∏∂ÂÅöËôïÁêÜ(Viikki et al., 1998)„ÄÇÂõ†ÁÇ∫Ë¶ÅÂº∑Ë™ø‰ΩéË™øËÆäÈ†ªÁéáÁöÑÈáçË¶ÅÊÄßÔºåÊâÄ‰ª•Âú®‰ΩéÈ†ªÈÉ®ÂàÜÁöÑÂ≠êÈ†ªÊÆµÊìÅÊúâËºÉÁ¥∞ÁöÑÈ†ªÂØ¨ÔºåÂ≠ê È†ªÊÆµÁöÑÊï∏Èáè‰πüÊØîËºÉÂ§öÔºåËÄåÈ´òË™øËÆäÈ†ªÁéá‰æøÊåÅÊúâÁõ∏ÂèçÁöÑÁâπÊÄß„ÄÇÁî±ÊñºÊéåÊè°‰Ωè‰ΩéÈ†ªÊàêÂàÜÁöÑË≥áË®äÔºå Ê†πÊìöÂ≠∏ËÄÖÁöÑÂØ¶È©óÊï∏ÊìöÔºåÈ°ØÁ§∫Âá∫Â∞áË™øËÆäÈ†ªÁéáÂàÜÈ†ªÊÆµ‰∏îÊ≠£Ë¶èÂåñÁöÑÂÅöÊ≥ïÔºåËÉΩÊØîÂÖ®È†ªÂ∏∂Ê≠£Ë¶èÂåñÁöÑ ÊñπÂºèÁç≤ÂæóËºÉÂ•ΩÁöÑÊïàËÉΩ„ÄÇ 4. ‰ΩøÁî®Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊñºË™øËÆäÈ†ªË≠úÂàÜËß£  4.1 Â≠óÂÖ∏Â≠∏ÁøíÊ≥ï‰ªãÁ¥π Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊòØ‰∏ÄÁ®ÆÂà©Áî®Â≠óÂÖ∏‰æÜË°®Á§∫Ë≥áÊñôÁöÑÊñπÊ≥ï„ÄÇÂÖ∂Á≤æÈ´ìÂú®ÊñºÈÄèÈÅéÂ≠∏ÁøíËÄå‰æÜÁöÑÂ≠óÂÖ∏ (Dictionary)ÔºåÈÖçÂêàÁ®ÄÁñèÁ∑®Á¢º(Sparse Coding)ÊåëÈÅ∏Âá∫Â≠óÂÖ∏‰∏≠ÈáçË¶ÅÁöÑÂéüÂ≠ê(Atoms)Ôºõ‰∏¶‰∏î‰ΩøÂæó ‰∏Ä‰∫õÂ§öÈ§òÁöÑË≥áË®ä(Redundancy)Á®ÄÁñèÔºåÊúÄÂæå‰ª•Á∑öÊÄßÁµÑÂêàÁöÑÊñπÂºèËøë‰ººÈÇÑÂéüÂá∫ÂéüÂßãË®äËôü(Tosic & Frossard, 2011)„ÄÇÁõ∏Â∞çÊñºÂÖ∂‰ªñÈôçÁ∂≠ÊñπÊ≥ïÁöÑÊäÄË°ìÂ¶ÇÔºö‰∏ªÊàêÂàÜÂàÜÊûêÊ≥ï(Principal Component Analysis )„ÄÅÁ∑öÊÄßÈëëÂà•ÂàÜÊûê(Linear Discriminant Analysis)ÔºåÂ≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊ≤íÊúâÊãòÊ≥•ÊñºÊ∏õÂ∞ëË≥á ÊñôÁ∂≠Â∫¶ÁöÑÁâπÊÄßÔºõÂèç‰πãÔºåÂ≠óÂÖ∏Â≠∏ÁøíÊ≥ïËøΩÊ±ÇÁöÑÊòØÂ¶Ç‰ΩïÁøíÂæóË≥áÊñô‰∏≠ÈáçË¶ÅÁöÑÁâπÂæµÔºå‰ª•ÂèäÊΩõÂú®ÁöÑË≥á ÊñôÊÑèÁæ©„ÄÇÊâÄ‰ª•Â≠óÂÖ∏ÈÄöÂ∏∏ÈÉΩÊòØÈÄèÈÅéÈÅ†Â§ßÊñºËº∏ÂÖ•Ë≥áÊñôÁöÑÁ∂≠Â∫¶ÔºåÈâÖÁ¥∞Èù°ÈÅ∫Ë°®Á§∫Ë≥áÊñô„ÄÇÂ∞±Ë®àÁÆóË§á ÈõúÂ∫¶ËÄåË®ÄÔºåÁõ∏ÊØî‰πã‰∏ãËºÉËä±ÊôÇÈñì‰∏î‰∏¶ÈùûÊØèÁ®ÆË≥áÊñôÈÉΩÈÅ©ÂêàÂçáÁ∂≠ÁöÑÂ≠óÂÖ∏Â≠∏ÁøíÊ≥ïÔºåÊàëÂÄë‰πüÂèØ‰ª•‰æù ÁÖßË≥áÊñôÁöÑÂ±¨ÊÄßÂ≠óÂÖ∏Ë®≠Ë®àÊàêÈôçÁ∂≠ÁöÑÂ≠óÂÖ∏„ÄÇ‰∏ÄËà¨ËÄåË®ÄÔºåÂ≠óÂÖ∏Â≠∏ÁøíÊ≥ïÂèØ‰ª•‰æùÁÖßË≥áÊñôÈÅ∏ÂèñÁöÑÊ¶ÇÂøµ ÂàÜÊàê‰∏âÁ®ÆÂ∏∏Ë¶ãÁöÑÊñπÊ≥ïÔºåÂàÜÂà•ÊòØÔºöÊ©üÁéáÂ≠∏ÁøíÊ≥ï(Tosic & Frossard, 2011) (Wipf & Rao, 2004) (Probabilistic Learning Methods) „ÄÅ Âêë Èáè Á∑® Á¢º Ëàá ÂàÜ Áæ§ Ê≥ï (Gersho & Gray, 1991) (Vector Quantization or Clustering Methods)„ÄÅÁâπÊÆäÁµêÊßãÊ≥ï(Tosic & Frossard, 2011) (Particular Construction Methods)„ÄÇÈóúÊñºÊ©üÁéáÂ≠óÂÖ∏Â≠∏ÁøíÊ≥ï(Wipf & Rao, 2004)ÔºåÊé°Áî®Á®ÄÁñèË≤ùÊ∞èÂ≠∏ÁøíÊ≥ï (Sparse Bayesian Learning)ÂéªËß£Ê±∫Â≠óÂÖ∏ÁöÑÁõÆÊ®ôÂáΩÊï∏‰ª•ÂèäÂ≠óÂÖ∏Êõ¥Êñ∞ÂïèÈ°å„ÄÇËÄåÁâπÊÆäÁµêÊßãÂ≠óÂÖ∏Â≠∏ ÁøíÊ≥ï(Yaghoobi, Daudet & Davies, 2009)Ôºå‰ªñÂÄëÊé¢Ë®éËëóÈáùÂ∞ç‰∏çÂêåËº∏ÂÖ•‰ø°ËôüÊôÇÔºåÂèØ‰ª•Âú®ÁõÆÊ®ô ÂáΩÊï∏Âä†ÂÖ•‰∏çÂêåÁöÑÂèÉÊï∏Áü©Èô£ÂáΩÊï∏(Parametric Function)‰ΩøÂæóÂ≠óÂÖ∏ÊúÄ‰Ω≥Âåñ„ÄÇËÄåÂêëÈáèÁ∑®Á¢ºËàáÂàÜÁæ§  ‰ΩøÁî®Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊñºÂº∑ÂÅ•ÊÄßË™ûÈü≥Ëæ®Ë≠ò  41  Ê≥ïÈô§‰∫ÜËëóÂêçÁöÑ K-SVD ‰ª•Â§ñ(Aharon et al., 2006)ÔºåËøëÂπ¥‰æÜÊúâ‰∫õÁ†îÁ©∂Â≠∏ËÄÖÂü∫Êñº K-SVD Ë≥áÊñôÂàÜ Â∏ÉÊòØÈ´òÊñØÂàÜÂ∏ÉÁöÑÂÅáË®≠ËÄåÊèêÂá∫ÊãâÂºèÂàÜÂ∏ÉÁöÑË™™Ê≥ïÔºåÊîπÂñÑ K-SVD ÈáçÂª∫È†Ö(Reconstruction Term)2ÂºèÁØÑÊï∏ÁöÑÈôêÂà∂ÁÇ∫ 1-ÂºèÁØÑÊï∏ÔºåÂÖ∂ÊèêÂá∫ÁöÑÊñπÊ≥ïÁÇ∫ l1-K-SVD(Mukherjee, Basu & Seelamantula, 2016)ÔºåÂÖ∂ÊñπÊ≥ïËÉΩÊúâÊïàÁöÑÊîπÂñÑ 2-ÂºèÁØÑÊï∏ÈÅéÊñºÂπ≥Êªë(Over-smoothing)ÁöÑÂïèÈ°åÔºåÂÖ∂ÊñπÊ≥ïÈô§‰∫ÜÊèê Âçá‰∫ÜËæ®Ë≠òÁéáÔºå‰ª•ÂèäÂä†Âø´‰∫ÜÊî∂ÊñÇÈÄüÂ∫¶Ôºå‰∏¶ÊãâËøë‰∫ÜÂéüÂßãÂ≠óÂÖ∏ËàáÁøíÂæóÂ≠óÂÖ∏ÁöÑÂ∞§ÊãâË∑ùÈõ¢(Euclidean Distance)„ÄÇ  4.2 K-SVDÂ≠óÂÖ∏Â≠∏ÁøíÊ≥ï  K-SVD Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊòØÁî± Aharon Á≠âÊèêÂá∫(Aharon et al., 2006)ÔºåÂÖ∂Ê†πÊú¨ÊÉ≥Ê≥ïÊ∫êËá™ÊñºÂêëÈáèÁ∑®Á¢º ÂïèÈ°å(Vector Quantization)‰ª•Âèä K-means ÊºîÁÆóÊ≥ï(Gersho & Gray, 1991)ÔºåÂ±¨ÊñºÂª£Áæ©ÁöÑ K-means (Generalized K-means)ÊºîÁÆóÊ≥ï„ÄÇAharon Á≠âË™çÁÇ∫Á∑®Á¢ºÂïèÈ°åÂèØ‰ª•ÈáùÂ∞çÂ≠óÂÖ∏ÂéüÂ≠êÂä†ÂÖ• 0 ÂºèÁØÑÊï∏(0-Norm)ÁöÑÈôêÂà∂‰ΩøÂæóÂ≠óÂÖ∏Âú®Ëøë‰ººÈÇÑÂéüËº∏ÂÖ•Ë≥áÊñôÊôÇÔºåËÉΩÂà™Ê∏õÊéâ‰∏çÂøÖË¶ÅÁöÑË≥áË®äÔºå‰∏¶Èáç Êñ∞Ë°®Á§∫ÁÇ∫Âºè(5)Ôºö  ,‚Äñ  ‚Äñ  ‚àÄÔºå  (5)  ÂÖ∂‰∏≠ Y ÁÇ∫ÂéüÂßãË®äËôü„ÄÅD ÁÇ∫Ê¨≤Â≠∏ÁøíÁöÑÂ≠óÂÖ∏„ÄÅX ÁÇ∫Â≠óÂÖ∏Áõ∏Â∞çÊáâÁöÑÊ¨äÈáçÁü©Èô£„ÄÅ ÊòØ‰∏ÄÂÄãÈùûÈõ∂ÁöÑ ÂØ¶Êï∏„ÄÇËø≠‰ª£Êõ¥Êñ∞ÁöÑÈóú‰øÇÂºè(5)‰πüÂèØ‰ª•Á≠âÂÉπË°®Á§∫ÁÇ∫Âºè(6)Ôºö  , ‚àë‚Äñ ‚Äñ  ‚Äñ  ‚Äñ  (6)  ÂÖ∂‰∏≠ ÁÇ∫‰∏ÄÂÄãÁµ¶ÂÆöÁöÑÈåØË™§ÂÆπÂøçÂÄº„ÄÇ  Âú® K-SVD Â≠óÂÖ∏Â≠∏ÁøíÊ≥ï‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÈ°û‰ºº K-means Ëø≠‰ª£Êõ¥Êñ∞ÁöÑÊñπÂºèÊ±ÇËß£ÁÆóÂºè(6)„ÄÇÂú® Á®ÄÁñèÁ∑®Á¢ºÈöéÊÆµ‰∏≠ÔºåÊàëÂÄëÂèØ‰ª•‰ΩøÁî®‰ªª‰ΩïÂåπÈÖçËøΩËπ§ÁöÑÊñπÊ≥ïÊâæÂá∫Ê¨äÈáçÁü©Èô£ XÔºå‰∏¶‰∏îÈÄèÈÅé 0 ÂºèÁØÑ Êï∏ÁöÑÈôêÂà∂ÔºåËÆìÊ¨äÈáçÁü©Èô£‰∏≠ÁöÑÂêëÈáè ÂÖßÁöÑÂÖÉÁ¥†(Element)ÂÄãÊï∏‰∏çÊúÉË∂ÖÈÅé ÔºåËóâÊ≠§‰ΩøÂæóÁü©Èô£ X Á®ÄÁñè„ÄÇÊé•ËëóÂú®Â≠óÂÖ∏Êõ¥Êñ∞ÈöéÊÆµÔºåÊàëÂÄëÊòØ‰ª•Â≠óÂÖ∏ÁöÑË°å(Column)ÂêëÈáè ÁÇ∫ÂñÆ‰ΩçÔºå‰∫¶Ë®ò‰ΩúÂéüÂ≠ê(Atom) ÈÄêÊ≠•Ëø≠‰ª£Êõ¥Êñ∞„ÄÇÈÄèÈÅéÊÆòÂ∑ÆÁü©Èô£(Residual Matrix) ÔºåÂ¶ÇÂºè(7)ÔºåÂÖ∂‰∏≠ ÁÇ∫Ê¨äÈáçÁü©Èô£(Weight Matrix)X ÁöÑÁ¨¨ k Âàó(Row)ÂêëÈáè„ÄÅ ÁÇ∫Â≠óÂÖ∏ÁöÑÁ¨¨ k Ë°å„ÄÇ ‰ª£Ë°®ËëóÊï¥ÂÄãÈáçÂª∫‰ø°Ëôü DX Â∞ë‰∫ÜÁ¨¨ k ÁµÑÂéüÂ≠ê(Atom)‰πãÊ¨äÈáçÂêëÈáèÂæåËàáËº∏ÂÖ•‰ø°Ëôü(Input Signal)Y ÁöÑÂ∑Æ„ÄÇ  (7)  Êé•ËëóÊàëÂÄëÂÆöÁæ©‰∫ÜœâÔºåœâÊòØÁÇ∫‰∫ÜÂèñÂæó ‰∏≠ÊâÄÂ∞çÊáâÁöÑÊ¨äÈáçÁü©Èô£‰∏çÁÇ∫ 0 ÁöÑÂàóÂêëÈáèËÄåÂ≠òÂú®ÁöÑ  Êï∏Â≠óÈõÜÂêàÔºåÂÖ∂ÂÆöÁæ©ÂºèÂ¶Ç‰∏ãÂºè(8)„ÄÇÂÖ∂‰∏≠ Ë°®Á§∫Ê¨äÈáçÁü©Èô£‰∏≠Á¨¨ k ÂàóÁöÑÂêëÈáè(Row vector)ÔºõËÄå ‰ª£  Ë°®Ê¨äÈáçÁü©Èô£‰∏≠ÁöÑÁ¨¨ i Ë°åÁöÑÂêëÈáè(Column vector)„ÄÇ  œâ1  ,  0 |1  ,  0  (8)  Áï∂ Âä†ÂÖ•œâÈõÜÂêàÂæåÔºåÂæóÂà∞ÁöÑÈùûÈõ∂ÁöÑÂÖÉÁ¥†Áü©Èô£ÔºåÊàëÂÄëÈáçÊñ∞Ë®òÂÅö Á®±ÁÇ∫ÈôêÂà∂ÊÆòÂ∑ÆÁü©Èô£ (Restricted Residual Matrix)„ÄÇ‰æÜÂ∞ç ‰ΩøÁî®Â•áÁï∞ÂÄºÁü©Èô£ÂàÜËß£Ê≥ïÂèñÂæó ‰∏≠Á¨¨‰∏ÄÁ≠ÜÈáçË¶ÅÁöÑË≥áË®äÔºå ÂàÜËß£ÂæåÊâÄÂæóÂà∞ÁöÑÂ∑¶Â•áÁï∞ÂÄºÂêëÈáèÁü©Èô£ÊàëÂÄëË®òÁÇ∫ UÔºõÂ•áÁï∞ÂÄºÁü©Èô£ÊàëÂÄëË®ò‰Ωú‚àÜÔºõÂè≥Â•áÁï∞ÂÄºÂêëÈáèÁü© Èô£ÂØ´Êàê „ÄÇÈÇ£È∫ºÊñ∞ÁöÑÂéüÂ≠ê(Atom) Á≠âÊñºÁ¨¨‰∏ÄÁ≠ÜÂ∑¶Â•áÁï∞ÂÄºÂêëÈáèÁöÑÂÄº ÔºõËÄåÂ∞çÊáâÁöÑÊ¨äÈáçÂêëÈáè (Weight Vector) ÂèØ‰ª•ÈÄèÈÅéÁ¨¨‰∏ÄÂÄãÂ•áÁï∞ÂÄºËàáÁ¨¨‰∏ÄÁ≠ÜÂè≥Â•áÁï∞ÂÄºÂêëÈáèÁöÑ‰πòÁ©çÔºå‚àÜ , ‚àô ‰æÜÊõ¥  42  È°èÂøÖÊàê Á≠â  ÊºîÁÆóÊ≥ï 1„ÄÅK-SVD Â≠óÂÖ∏Â≠∏ÁøíÊ≥ï ÂàùÂßãÈöéÊÆµ: D ‚àà R ÔºåË®≠ J 1„ÄÇ  1„ÄÅÁ®ÄÁñèÁ∑®Á¢ºÈöéÊÆµ(Sparse Coding Stage):Êú¨Ë´ñÊñá‰ΩøÁî® MP„ÄÅOMP ÁöÑÊºîÁÆóÊ≥ïÊ±ÇÂæóx Ôºö  for i 1,2, ‚Ä¶ ‚Ä¶ , N  ‚Äñ  ‚Äñ  ‚Äñ‚Äñ  2„ÄÅÂ≠óÂÖ∏Êõ¥Êñ∞ÈöéÊÆµ(Dictionary Update Stage):  ÔÅ¨ ÂÆöÁæ©:Â≠óÂÖ∏ D ÁöÑË°åÂêëÈáè: „ÄÅÊ¨äÈáçÁü©Èô£ÁöÑÂàóÂêëÈáè:x „ÄÅËÄåÊúâË¢´ÂéüÂ≠ê‰ΩøÁî®Âà∞ÁöÑÂ∞ç  ÊáâÊ¨äÈáçË°åÂêëÈáèË®òÈåÑÊàêÈõÜÂêà  |1  ,x  0„ÄÇ  ÔÅ¨ Ë®àÁÆó  E  Ôºå  ÔÅ¨ ÊÆòÂ∑ÆÁü©Èô£ ÈÄèÈÅé ÂèñÂæó ‰∏≠‰∏çÁÇ∫Èõ∂ÁöÑË°åÂêëÈáèÔºåÂæóÂà∞Êñ∞ÁµÑÊàêÁü©Èô£  Âú®Âà©Áî® SVD ÂàÜËß£Ê≥ïE U‚àÜV „ÄÇ  Êõ¥Êñ∞:  Êõ¥Êñ∞:  ‚àÜ 1,1 ‚àô  J J1  ÂèçË¶Ü 1„ÄÅ2 Áõ¥Âà∞Êî∂ÊñÇ  Êñ∞ÔºåÊõ¥Êñ∞Âºè(9)Â¶Ç‰∏ãÔºö  update:  ‚àÜ, ‚àô  (9)  ÂÆåÊï¥ÁöÑ K-SVD ‰ΩúÊ≥ïÂ¶ÇÊºîÁÆóÊ≥ï 1 ÊâÄÁ§∫„ÄÇ  4.3 ÈùûË≤†K-SVDÂ≠óÂÖ∏Â≠∏ÁøíÊ≥ï ÈùûË≤† K-SVD Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊòØÂíå K-SVD Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÂêåÊôÇË™ïÁîüÁöÑÔºåÊ∫êËá™ÊñºÂêå‰∏ÄÁµÑÁ†îÁ©∂ÂúòÈöä„ÄÇ ÁÇ∫‰∫Ü‰Ωø K-SVD Êõ¥ËÉΩÂ§†ÂÆåÊï¥ÁöÑÊèèËø∞Ëº∏ÂÖ•Ë≥áÊñôÁÇ∫Ê≠£Êï∏ÊôÇÁöÑÊÉÖÊ≥ÅÔºõÈÄ≤ËÄåÂú® K-SVD Ê±ÇËß£Ê¨äÈáç‰ª• ÂèäÊõ¥Êñ∞Â≠óÂÖ∏ÊôÇÂä†ÂÖ•ÈùûË≤†Êï∏ÁöÑÈôêÂà∂ÔºåËÄåÊñ∞ÁîüÊàêÁöÑÊºîÁÆóÊ≥ïÁ®±ÁÇ∫ NN-K-SVD„ÄÇNN-K-SVD ÁÇ∫‰∫Ü ‰ΩøËº∏Âá∫ÁöÑÂ≠óÂÖ∏(Dictionary)‰ª•ÂèäÊ¨äÈáçÁü©Èô£(Weight)ÁöÜÁÇ∫Ê≠£Êï∏ÔºåÊâÄ‰ª•Âú®Â≠∏ÁøíÁöÑÁ®ÄÁñèÁ∑®Á¢ºÈöéÊÆµ (Sparse Coding Stage)ÊôÇÔºåÊ±ÇËß£Ê¨äÈáçÁöÑÊñπÊ≥ïÂà©Áî®ÈùûË≤†Êï∏Á®ÄÁñèÁ∑®Á¢ºÊ≥ï(Non-negative Sparse Coding) (Hoyer, 2004)„ÄÇ  ‰ΩøÁî®Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊñºÂº∑ÂÅ•ÊÄßË™ûÈü≥Ëæ®Ë≠ò  43  ÊºîÁÆóÊ≥ï 2„ÄÅÈùûË≤† K-SVD Â≠óÂÖ∏Êõ¥Êñ∞Ë¶èÂâá ÂàùÂßãÈöéÊÆµ:  ‰ª§  0  Ôºåx i  0  Ôºå  ÂÖ∂‰∏≠ „ÄÅv ÊòØ ÈáçË§á J Ê¨°:  Êé•ÈÅéÂ•áÁï∞ÂÄºÂàÜËß£ÂæåÔºåÁ¨¨‰∏ÄÁ≠ÜÂ•áÁï∞ÂÄºÂ∑¶ÂêëÈáèËàáÂè≥Â•áÁï∞ÂÄºÂêëÈáè„ÄÇ  (1) ‰ª§:d  Ôºå Áï∂d i  0  (2) ‰ª§:x  Ôºå Áï∂x i  0  Âú®ÈùûË≤† K-SVD ÊºîÁÆóÊ≥ï‰∏≠ÔºåÂ≠∏ÁøíÁöÑÊñπÊ≥ïËàá K-SVD Â¶ÇÂá∫‰∏ÄËΩç„ÄÇ‰∏çÈÅéÁÇ∫‰∫ÜÊªøË∂≥ÈùûË≤†Êï∏ÁöÑ  ÈôêÂà∂ÔºåÊàëÂÄëÂ∞çÁõÆÊ®ôÂáΩÊï∏Âºè(5)‰∫Ü‰∏Ä‰∫õË™øÊï¥„ÄÇÊàëÂÄëÂú®ÊúÄÂ∞èÂπ≥ÊñπÊ≥ïË£°Âä†ÂÖ•ÈùûË≤†Êï∏ÁöÑÈôêÂà∂„ÄÇ‰πüÂ∞±  ÊòØË™™Â∞çÊñºÂ≠óÂÖ∏ÔºåÊàëÂÄëÂè™‰øùÁïôÂâç L Â§ßÁöÑÂéüÂ≠ê(Atom)‰æÜÂèÉËàáÂ≠óÂÖ∏Â≠∏ÁøíÔºåÂ¶ÇÂºèÂ≠ê(10)„ÄÇ  ‚Äñ  ‚Äñ s. t. x 0  (10)  Âú®Á®ÄÁñèÈöéÊÆµ(Sparse Coding Stage)ÔºåÊàëÂÄë‰ΩøÁî®‰∏äËø∞ÊèêÂèäÁöÑÈùûË≤†Á®ÄÁñèÁ∑®Á¢ºÊ≥ï(NNSC)‰∏≠Ê±Ç ÂèñÁü©Èô£ S ÁöÑÊñπÂºèÔºåÊ±ÇËß£Ê¨äÈáç x„ÄÇËÄåÂú®Â≠óÂÖ∏Êõ¥Êñ∞ÈöéÊÆµ(Dictionary update stage)ÔºåÁÇ∫‰∫Ü‰ΩøÂæóÂéü Â≠ê (Atom)Êõ¥Êñ∞Âæå‰πüÁÇ∫Ê≠£Êï∏ÔºåÂâáÂä†ÂÖ•ÂéüÂ≠ê‰πãÂÄºÁÇ∫Ê≠£ÁöÑÈôêÂà∂ÔºåÂ¶ÇÂºèÂ≠ê(11)ÊâÄÁ§∫Ôºö  ,  .. ,  0  (11)  ËÄåÊõ¥Êñ∞ÊÆòÂ∑ÆÁü©Èô£ Âà©Áî® SVD ÂàÜËß£ÊôÇÂèØËÉΩÁî¢ÁîüË≤†Êï∏ÔºåÊàëÂÄëÊé°Áî®‰∏äÂúñÁöÑÊºîÁÆóÊ≥ïÔºåÂ¶ÇÊûú Á¨¨‰∏ÄÊ¨° SVD ÂàÜËß£ÂæåÁöÑÂ∑¶Âè≥Â•áÁï∞ÂÄºÂêëÈáèÁöÜË≤†Êï∏ÔºõÂâáÂêå‰πò(-1)„ÄÇÊé•‰∏ã‰æÜÂ•áÁï∞ÂÄºÂêëÈáèÂÖßÁöÑÂÖÉÁ¥† (Element)Â∞èÊñºÈõ∂ÂâáË®≠ÁÇ∫Èõ∂ÔºåÂÖ∂‰ªñÂ§ßÊñºÈõ∂ÁöÑÊï∏Â≠óÂâá‰øùÁïô‰∏çËÆä„ÄÇÈñãÂßãÈáçË§á J Ê¨°‰ΩøÂæó Ë∂ä‰æÜË∂ä Êé•Ëøëd „ÄÇÂÆåÊï¥ÈùûË≤† K-SVD Êõ¥Êñ∞Ë¶èÂâáÂ¶ÇÊºîÁÆóÊ≥ï 2 ÊâÄÁ§∫„ÄÇ  4.4 Ê¨äÈáçÁöÑÊõ¥Êñ∞ÊñπÊ≥ï  4.4.1 ÂåπÈÖçËøΩËπ§  ÂåπÈÖçËøΩËπ§(Matching Pursuit, MP)ÔºåÊòØÂ≠óÂÖ∏Â≠∏ÁøíÊ≥ïÁ¨¨‰∏ÄÈöéÊÆµÁ®ÄÁñèÁ∑®Á¢º(Sparse Coding )‰∏≠ÔºåÊ±Ç ÂæóÊ¨äÈáçÁü©Èô£(Weight) X ÁöÑÂ∏∏Ë¶ãËß£Ê≥ï„ÄÇÊºîÁÆóÊ≥ïÊ¶ÇÂøµÁÇ∫Ë≤™Â©™Ê≥ïÂâá„ÄÇÈ¶ñÂÖàÂ∞áËº∏ÂÖ•‰ø°Ëôü(Input signal) x ‰ª§ÊàêÂÜóÈ§òÈ†ÖÈáè(Residual) r„ÄÇÂÜçÂà©Áî®ÊäïÂΩ±ÈáèÁöÑÊñπÂºèË°°ÈáèÂá∫ËàáÂÜóÈ§òÂêëÈáè r Áõ∏ÈóúÁ®ãÂ∫¶ÊúÄÂ§ßÁöÑÂéü Â≠ê(Atom) ÂæåÔºåÂâáÂÜóÈ§òÂêëÈáè r ËàáÊúÄÁõ∏ÈóúÁöÑÂéüÂ≠ê ÁöÑÂÖßÁ©çÂÄºÂºè(12)Âç≥ÁÇ∫Â∞çÊáâÊ¨äÈáç Œ± Âú®Á¨¨ i  Á∂≠Â∫¶ÁöÑÂàÜÊï∏„ÄÇ  ‚àô  (12)  44  È°èÂøÖÊàê Á≠â  ÊºîÁÆóÊ≥ï 3: ÂåπÈÖçËøΩËπ§ÊºîÁÆóÊ≥ï(MP)  1. Ëº∏ÂÖ•Ë≥áÊñô:Ëº∏ÂÖ•‰ø°Ëôü x„ÄÅÂ≠óÂÖ∏ D„ÄÇ  2. Ëº∏Âá∫Ë≥áÊñô:Ê¨äÈáçÂêëÈáè Œ±„ÄÇ  3. ÁõÆÊ®ôÂáΩÊï∏:  min ‚àà ‚Äñ 4. ÂàùÂßãÊ≠•È©ü:  ‚Äñ  . ‚Äñ‚Äñ  Œ± ‚Üê 0„ÄÅr ÂÜóÈ§òÂêëÈáè ‚Üê x  5. while ‚Äñ ‚Äñ  do  ÔÅ¨ ÊâæÂ∞ãËàáÂÜóÈ§òÂêëÈáè(Residual Vector)ÈóúËÅØÁ®ãÂ∫¶ÊúÄÂ§ßÁöÑÂéüÂ≠ê„ÄÇ  ƒ±ÃÇ ‚Üê  ÃÇ ,‚Ä¶‚Ä¶, | ‚àô |  ÔÅ¨ Êõ¥Êñ∞ÂÜóÈ§òÂêëÈáè‰ª•ÂèäÂ∞çÊáâÁöÑÊ¨äÈáç(Weight)  6. end while  Œ± ÃÇ ‚ÜêŒ± ÃÇ  ÃÇ‚àô  r‚Üêr  ÃÇ‚àô  ÃÇ  ‰πãÂæåÂÜçÂ∞áËº∏ÂÖ•‰ø°Ëôü r Ê∏õÂéª‰ø°Ëôü r Âú®ÊúÄÁõ∏ÈóúÁöÑÂéüÂ≠ê ‰∏äÁöÑÊäïÂΩ±ÈáèÂàÜÈáèÂç≥ÁÇ∫Âºè(13)ÔºåÊâÄÂæó Âà∞ÁöÑÂÜóÈ§òÂêëÈáè r ÂâáÁÇ∫‰∏ã‰∏ÄÊ¨°Ëø≠‰ª£ÁöÑËº∏ÂÖ•‰ø°Ëôü„ÄÇ  ‚àô  (13)  ÂÆåÊï¥ÂåπÈÖçËøΩËπ§ÁöÑÁÆóÊ≥ïÂ¶ÇÊºîÁÆóÊ≥ï 3 Â¶ÇÁ§∫„ÄÇ  4.4.2 Ê≠£‰∫§ÂåπÈÖçËøΩËπ§  Ê≠£‰∫§ÂåπÈÖçËøΩËπ§Ê≥ï(Orthogonal Matching Pursuit, OMP)ÊòØÂåπÈÖçËøΩËπ§Ê≥ïÁöÑÊîπËâØÊñπÊ≥ï„ÄÇÊ≠£‰∫§ÂåπÈÖç ËøΩËπ§Ê≥ï(OMP)Âú®Ë°°ÈáèÂÜóÈ§òÂêëÈáèËàáÂéüÂ≠êÁöÑÈóúËÅØÁ®ãÂ∫¶ÊôÇËÄÉÊÖÆÁöÑÊòØÊ≠£‰∫§ÊäïÂΩ±ÈáèÔºåËÄå‰∏çÊòØÊäïÂΩ±Èáè„ÄÇ ËÄåË®àÁÆóÊ≠£‰∫§ÊäïÂΩ±ÈáèÁöÑÊñπÊ≥ïÁÇ∫ Gram-Schmidt Ê≠£‰∫§ÂåñÊ≥ïÔºåÂÖ∂‰∏≠Êõ¥Êñ∞ÂÜóÈ§òÂêëÈáè œÑ ÂèäÊ¨äÈáç Œ± Â¶Ç Âºè(14)Âèä(15)ÊâÄÁ§∫Ôºö  œÑ‚Üê I  x  (14)  ‚Üê  x  (15)  4.4.3 ÈùûË≤†Êï∏Á®ÄÁñèÁ∑®Á¢ºÊ≥ï ÈùûË≤†Êï∏Á®ÄÁñèÁ∑®Á¢ºÊ≥ï(Non-Negative Sparse Coding, NNSC)ÁöÑÊõ¥Êñ∞ÊºîÁÆóÊ≥ïÊòØÂèÉÁÖßÈùûË≤†Áü©Èô£ÂàÜ Ëß£ÁöÑ‰πòÊ≥ïÂºèÊõ¥Êñ∞Âºè(Multiplicative Update Rules) (ÂºµÂ∫≠Ë±™Ôºå2015)ÔºåÂÖ∂ÁâπÊÄßÁÇ∫ËóâËëóËº∏ÂÖ•Áü©Èô£ ÁöÜÁÇ∫Ê≠£Êï∏ÔºåÁÑ∂ËÄåÈÄèÈÅé‰πòÊ≥ïÊõ¥Êñ∞ÁöÑÈóú‰øÇ‰ΩøÂæóÊõ¥Êñ∞ÂæåÁöÑÁü©Èô£‰πüÁÇ∫Ê≠£Êï∏„ÄÇÈùûË≤†Êï∏Á®ÄÁñèÁ∑®Á¢ºÊòØÁÇ∫ ‰∫ÜÊ±ÇËß£Âºè(10)ËÄåÁî¢ÁîüÁöÑÊñπÊ≥ï„ÄÇ  ‰ΩøÁî®Â≠óÂÖ∏Â≠∏ÁøíÊ≥ïÊñºÂº∑ÂÅ•ÊÄßË™ûÈü≥Ëæ®Ë≠ò  45  ÊºîÁÆóÊ≥ï 4: ÈùûË≤†Êï∏Á®ÄÁñèÁ∑®Á¢º(NNSC)  ÁõÆÊ®ôÂáΩÊï∏:  ,  
Mispronunciation detection and diagnosis are part and parcel of a computer assisted pronunciation training (CAPT) system, collectively facilitating second-language (L2) learners to pinpoint erroneous pronunciations in a given utterance so as to improve their spoken proficiency. This thesis presents a continuation of such a general line of research and the major contributions are three-fold. First, we compared the performance of different pronunciation features in mispronunciation detection. Second, we propose an effective training approach that estimates the deep neural network based acoustic models involved in the mispronunciation detection process by optimizing an objective directly linked to the ultimate evaluation metric. Third, we can linearly combine two F1-score when we consider F1-score as final objective function. It can effectively deal with the label imbalance problem. A series of experiments on a Mandarin mispronunciation detection task seem to show the performance merits of the proposed methods. Keywords: Computer Assisted Pronunciation Training, Mispronunciation Detection, Automatic Speech Recognition, Discrimetive Training, Deep Neural Networks. 1. Á∑íË´ñ ÂÖ®ÁêÉÂåñÊôÇ‰ª£‰æÜËá®ÔºåÁÇ∫ÊèêÂçáÂÄã‰∫∫ÁöÑÁ´∂Áà≠ÂäõÔºåÂ§ñË™ûËÉΩÂäõÂ∑≤ÂàóÁÇ∫Âü∫Êú¨ÁöÑÊäÄËÉΩ‰πã‰∏Ä„ÄÇÂõ†Ê≠§ÈõªËÖ¶Ëºî Âä©Ë™ûË®ÄÂ≠∏Áøí(Computer Assisted Language Learning, CALL)Âú®Áèæ‰ªäÂ∑≤ÊòØÈùûÂ∏∏ÂÖ∑ÊúâÊΩõÂäõÁöÑÁ†î Á©∂ÔºõÂÖ∂ÁõÆÁöÑÊòØÈÄèÈÅéÈõªËÖ¶Ëá™ÂãïÂà§Êñ∑Â§ñË™ûÂ≠∏ÁøíËÄÖÁöÑÂ≠∏ÁøíÁãÄÊ≥Å‰∏¶Áµ¶‰∫àÊúâÂπ´Âä©ÁöÑÂõûÈ•ã„ÄÇËøëÂπ¥‰æÜÔºå Áî±Êñº‰∏≠ÂúãÂ∏ÇÂ†¥ÁöÑÂø´ÈÄüÁôºÂ±ïÔºåÂÖ®ÁêÉËèØË™ûÂ≠∏ÁøíÁÜ±ÊΩÆÂ∏≠Êç≤ËÄå‰æÜÔºåÂ≠∏ÁøíËèØË™ûÁöÑ‰∫∫Êï∏È†ê‰º∞Â∑≤Á∂ìË∂ÖÈÅé ‰∏ÄÂÑÑ„ÄÇÂú®Ë®±Â§öÈùûËèØË™ûË™ûÁ≥ªÁöÑ‰∫ûÊ¥≤„ÄÅÊ≠êÊ¥≤‰ª•ÂèäÁæéÊ¥≤ÂúãÂÆ∂ÔºåËèØË™ûÂ∑≤Á∂ìÈÄêÊº∏ÊàêÁÇ∫‰∏ÄÁ®ÆÂøÖÈ†àÂ≠∏Áøí ÁöÑË™ûË®Ä(Hu, Qian, Soong & Wang, 2014)„ÄÇË™ûË®ÄÂ≠∏ÁøíÂèØÂàÜÁÇ∫ËÅΩ(Listening)„ÄÅË™™(Speaking)„ÄÅËÆÄ (Reading)ÂíåÂØ´(Writing)Á≠âÂõõÈ°ûÂ≠∏ÁøíÈù¢ÂêëÔºåÂÖ∂‰∏≠Âè£Ë™™ËàáÊõ∏ÂØ´Ê∏¨È©óÁöÑË©ïÈáèÂæÄÂæÄÈúÄË¶ÅÂ∞àÊ•≠ÁöÑË™û Ë®ÄÊïôÂ∏´‰æÜË©ïÊñ∑Ôºå‰ΩÜË™ûË®ÄÊïôÂ∏´ÁöÑÂüπÈ§äÂ∞öÁÑ°Ê≥ïÊªøË∂≥ÈÅç‰ΩàÂÖ®ÁêÉÁöÑÈúÄÊ±Ç„ÄÇÊú¨ÁØáË´ñÊñáÂ∞áÂ∞àÊ≥®ÊñºÈõªËÖ¶ ËºîÂä©ÁôºÈü≥Ë®ìÁ∑¥(Computer Assisted Pronunciation Training, CAPT)Ôºå‰πüÂ∞±ÊòØ„ÄåË™™„ÄçÁöÑÊäÄË°ìÈÄ≤ Ë°åË®éË´ñ„ÄÇ ÈõªËÖ¶ËºîÂä©ÁôºÈü≥Ë®ìÁ∑¥ÊúÄ‰∏ªË¶ÅÁõÆÁöÑÂ∞±ÊòØË¶ÅËÆìÁ¨¨‰∫åÂ§ñË™û(Second-Language, L2)Â≠∏ÁøíËÄÖÊúâÊõ¥ Â§öÁöÑÊ©üÊúÉÁ∑¥ÁøíÁôºÈü≥ÔºõÈÅéÂéªÁ¨¨‰∫åÂ§ñË™ûÂ≠∏ÁøíËÄÖË¶ÅÈÄ≤Ë°åÁôºÈü≥Á∑¥ÁøíÈÉΩÈúÄË¶ÅÈÖçÂêàË™ûË®ÄÊïôÂ∏´ÁöÑÊéàË™≤ÊôÇ ÈñìÔºåËã•Â∞áÈõªËÖ¶ËºîÂä©ÁôºÈü≥Ë®ìÁ∑¥ÊôÆÂèäÂà∞ÁèæÊúâÁöÑÊô∫ÊÖßÂûãË°åÂãïË£ùÁΩÆÔºåÂ∞áÊúÉÊúâÊõ¥Â§öÁöÑÁ¨¨‰∫åÂ§ñË™ûÂ≠∏Áøí ËÄÖÂõ†Ê≠§ÂèóÊÉ†„ÄÇÈõªËÖ¶ËºîÂä©ÁôºÈü≥Ë®ìÁ∑¥‰∏≠ÁöÑÈ¶ñË¶Å‰ªªÂãôÁÇ∫Ëá™ÂãïÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ÔºõÊ™¢Ê∏¨ÈÅéÁ®ãÊòØË´ãÂ≠∏Áøí ËÄÖ ËÆÄ Ë™¶Âè£ Ë™™Êïô Êùê ÔºåÈáù Â∞çÂ≠∏ Áøí ËÄÖÂøµ Ë™¶ÁöÑ ÈåÑ Èü≥Ôºå Ê®ôË®ò Â≠∏ ÁøíËÄÖ ÁöÑÁôº Èü≥ ÊòØÊ≠£ Á¢∫Áôº Èü≥ (Correct Pronunciation)ÊàñÈåØË™§ÁôºÈü≥(Mispronunciation)ÔºåÊ®ôË®òÁöÑÁõÆÊ®ôÂèØ‰ª•ÊòØÈü≥Á¥†(Phone)Â±§Ê¨°(Witt & Young, 2000)„ÄÅÈü≥ÁØÄ(Syllable)Â±§Ê¨°(Zhang, Huang, Soong, Chu & Wang, 2008)ÊàñË©û(Word) Â±§Ê¨°(Chen & Jang, 2015)„ÄÇÁï∂Á≥ªÁµ±ÊåáÂá∫Â≠∏ÁøíËÄÖÁöÑÈåØË™§ÁôºÈü≥ÊôÇÔºåÂ∞áÂèØ‰ª•ÈáùÂ∞çË©≤ÈåØË™§ÁôºÈü≥ÈÄ≤Ë°å  Ë©ï‰º∞Â∞∫Â∫¶Áõ∏ÈóúÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÊñºËèØË™ûÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨‰πãÁ†îÁ©∂  57  ÂÅèË™§ÂõûÈ•ãÔºåË©≤ÈöéÊÆµË¢´Á®±ÁÇ∫ÈåØË™§ÁôºÈü≥Ë®∫Êñ∑(Harrison, Lau, Meng & Wang, 2008; Harrison, Lo, Qian & Meng, 2009; Lo, Zhang & Meng, 2010; Wang & Lee, 2012; Wang & Lee, 2015)„ÄÇÈåØ Ë™§ÁôºÈü≥Ê™¢Ê∏¨ÁÇ∫ÈõªËÖ¶ËºîÂä©ÁôºÈü≥Ë®ìÁ∑¥‰∏≠ÁöÑÁ¨¨‰∏ÄÊ≠•ÔºåÁï∂ÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ÂèØ‰ª•Á≤æÊ∫ñÁöÑÈ†êÊ∏¨Â≠∏ÁøíËÄÖÁöÑ ÁôºÈü≥ÁãÄÊ≥ÅÊôÇÔºåÊâçËÉΩÊúâÊïàÁöÑÈÄ≤Ë°åÈåØË™§ÁôºÈü≥Ë®∫Êñ∑„ÄÇÊú¨Á†îÁ©∂‰∏ªÊó®Âú®Êé¢Ë®éÂ¶Ç‰ΩïÊèêÂçáÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ ‰πãÊïàËÉΩÔºüÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ÂèØË¶ñÁÇ∫‰∫åÈ°ûÂàÜÈ°ûÂïèÈ°åÔºåÂ∞çÊñºÁôºÈü≥Ê™¢Ê∏¨ÁöÑÁµêÊûúÂú®Ë™ûË®ÄÂ∞àÂÆ∂ËàáÁ≥ªÁµ±ÁöÑ Ê±∫Á≠ñ‰πãÈñìÂèØ‰ª•Áî¢ÁîüÂõõÁ®ÆÁµêÂ±ÄÔºöËã•Â≠∏ÁøíËÄÖÁöÑÁôºÈü≥Ê≠£Á¢∫ÔºåÁ≥ªÁµ±ÂçªÂà§Êñ∑ÁÇ∫ÁôºÈü≥ÈåØË™§Á®±ÁÇ∫ÊòØÈåØË™§ ÁöÑÊãíÁµï(False Rejections, FR)ÔºõËÄåÂ≠∏ÁøíËÄÖÁôºÈü≥ÈåØË™§ÔºåÁ≥ªÁµ±Ë™çÂÆöÁÇ∫ÁôºÈü≥Ê≠£Á¢∫ÂâáÁ®±ÁÇ∫ÈåØË™§ÁöÑÊé• Âèó(False Acceptances, FA)ÔºõÂ≠∏ÁøíËÄÖÁôºÈü≥Ê≠£Á¢∫ÔºåÁ≥ªÁµ±Âà§Êñ∑ÁÇ∫ÁôºÈü≥Ê≠£Á¢∫Á®±ÁÇ∫Ê≠£Á¢∫ÁöÑÊé•Âèó(True Acceptances, TA)ÔºõÂ≠∏ÁøíËÄÖÁôºÈü≥ÈåØË™§ÔºåÁ≥ªÁµ±Âà§ÂÆöÁÇ∫ÁôºÈü≥ÈåØË™§Á®±ÁÇ∫Ê≠£Á¢∫ÁöÑÊãíÁµï(True Rejections, TR) „ÄÇ ‰∏ä Ëø∞ ÁöÑ Âõõ Á®Æ Êåá Ê®ô ÂèØ ‰ª• Ë®à ÁÆó Âá∫ ÂÖ∂ ÂÆÉ Ë©ï ‰º∞ ÁöÑ Ê®ô Ê∫ñ Ôºå ‰æã Â¶Ç Âè¨ Âõû Áéá (Recall) Ëàá Á≤æ Ê∫ñ Â∫¶ (Precision)ÔºåÊúâË®±Â§öÁôºÈü≥Ê™¢Ê∏¨ÁöÑÁ†îÁ©∂ÁöÜ‰ª•Ë©≤Ë©ï‰º∞ÊñπÂºè‰ΩúÁÇ∫Ë©ïÈáèÁ≥ªÁµ±ÂÑ™Âä£ÁöÑÊ∫ñÂâá(Hu et al., 2015; Huang, Xu, Wang & Silamu, 2015)„ÄÇÊàëÂÄëÂèØÊõ¥ÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®Âè¨ÂõûÁéáËàáÁ≤æÊ∫ñÂ∫¶ÁöÑË™øÂíåÂπ≥ ÂùáÔºçF Â∫¶Èáè(F -Score)ÂÅöÁÇ∫Ê∫ñÂâáÔºåF Â∫¶ÈáèÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ(Natural Language Processing, NLP)ËàáË≥áË®äÊ™¢Á¥¢(Information Retrieval, IR)Á≠âÁ†îÁ©∂‰∏≠Âª£ÁÇ∫‰ΩøÁî®ÔºåÁîöËá≥ÊúâË®±Â§ö‰ªªÂãôÁõ¥Êé•Â∞áË©≤ ÊåáÊ®ô‰ΩúÁÇ∫Ê®°ÂûãË®ìÁ∑¥ÁöÑÁõÆÊ®ô(Fujino, Isozaki & Suzuki, 2008; Dembczynski, Waegeman, Cheng & H√ºllerme√≠er, 2011; Ye, Chai, Lee & Chieu, 2012)„ÄÇÂú®ÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨‰ªªÂãô‰∏≠‰πüÊúâÈ°û‰ººÊÉ≥Ê≥ï ÁöÑ Á†î Á©∂ Â∑≤ Ë¢´ Êé¢ Ë®é (Huang et al., 2015; Qian, Soong & Meng, 2010; Huang, Wang & Abudureyimu, 2012)„ÄÇ ËøëÂπ¥‰æÜÔºåÂú®Ë™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±‰∏≠ÁöÑËÅ≤Â≠∏Ê®°ÂûãÂ∑≤Áî±Ê∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø(Deep Neural Network, DNN)Âèñ‰ª£ÂÇ≥Áµ±ÁöÑÈ´òÊñØÊ∑∑ÂêàÊ®°Âûã(Gaussian Mixture Model, GMM)Ôºå‰∏¶Âú®Ë™ûÈü≥Ëæ®Ë≠ò‰ªªÂãô‰∏äÂèñ ÂæóÂ∑®Â§ßÁöÑÈÄ≤Ê≠•(Hinton et al., 2012)„ÄÇÂú®ÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ÁöÑÁõ∏ÈóúÁ†îÁ©∂‰∏≠‰πüÂõ†ÁÇ∫Ê∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø ËÅ≤Â≠∏Ê®°ÂûãÁöÑ‰ΩøÁî®ËÄåÂú®ÊïàËÉΩ‰∏äÊúâÈ°ØËëóÁöÑÊèêÂçá(Hu et al., 2015; Qian, Meng & Soong, 2012; Hu et al., 2014)„ÄÇÂü∫Êñº‰∏äËø∞Á†îÁ©∂ÁöÑÂïüÁôºÔºåÊàëÂÄëÂª∂Á∫åÈÅéÂéªÂ≠∏ËÄÖ‰ª•ÊúÄÂ§ßÂåñÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨‰ªªÂãôÁöÑÊïà ËÉΩ(Huang et al., 2015; Hsu, Yang, Hung & Chen, 2016)ÁÇ∫ÁõÆÊ®ôÂáΩÊï∏Â∞çÊ®°ÂûãÈÄ≤Ë°åË™øÊï¥ÁöÑÊÉ≥Ê≥ïÔºå ‰∏¶ÂØ¶‰ΩúÊñºÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØËÅ≤Â≠∏Ê®°ÂûãÁöÑÊû∂Êßã‰∏äÊé¢Ë®éÂ∞çÊñºÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨‰ªªÂãôÁöÑÂΩ±Èüø„ÄÇ Êú¨ÁØáË´ñÊñáÂú®Á¨¨‰∫åÁØÄÂ∞á‰ªãÁ¥πÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨Áõ∏ÈóúÁ†îÁ©∂ÁöÑÁôºÂ±ïËøëÊ≥ÅÔºõÁ¨¨‰∏âÁØÄÁ∞°ÂñÆÁöÑÂõûÈ°ßÈåØ Ë™§ÁôºÈü≥Ê™¢Ê∏¨‰ªªÂãô‰∏≠ËºÉÂ∏∏Ë¢´‰ΩøÁî®ÁöÑÊñπÊ≥ïÔºõÁ¨¨ÂõõÁØÄÂâáÊòØË®éË´ñÂü∫ÊñºÁ¨¨‰∏âÁØÄÁöÑÁôºÈü≥Ê™¢Ê∏¨ÊñπÊ≥ïÂ¶Ç‰Ωï ÂØ¶ÁèæÊúÄÂ§ßÂåñÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨F Â∫¶Èáè‰πãË®ìÁ∑¥ÔºõÁ¨¨‰∫îÁØÄÂâáÊòØÂæûÂØ¶È©ó‰∏≠Êé¢Ë®éÊúÄÂ§ßÂåñÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ F Â∫¶Èáè‰πãË®ìÁ∑¥Â∞çÊñºÁôºÈü≥Ê™¢Ê∏¨‰ªªÂãôÁöÑÂΩ±ÈüøÔºõÊúÄÂæåÔºåÂú®Á¨¨ÂÖ≠ÁØÄÔºåÊàëÂÄëÊèêÂá∫ÁµêË´ñËàá‰∏Ä‰∫õÊú™‰æÜÂèØ ËÉΩÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇ 2. ÊñáÁçªÊé¢Ë®é ÈåØ Ë™§ Áôº Èü≥ Ê™¢ Ê∏¨ Â§ß Ëá¥ ÂèØ ÂàÜ ÁÇ∫ Âü∫ Êñº ÈñÄ Ê™ª ÂÄº (Thresholding-Based) Ëàá Âü∫ Êñº ÂàÜ È°û Âô® (Classification-Based)Á≠âÂÖ©Á®ÆÂÅöÊ≥ï„ÄÇÂÖ©ËÄÖÂ∑ÆÂà•Âú®ÊñºÊòØÂê¶‰ΩøÁî®ÊòéÁ¢∫ÁöÑÈñÄÊ™ªÂÄº‰æÜÂà§Êñ∑ÁôºÈü≥ÁÇ∫Ê≠£ Á¢∫ÊàñÈåØË™§ÔºõÂü∫ÊñºÂàÜÈ°ûÂô®ÂâáÊòØÊï¥ÂêàÂ§öÁ®ÆÁâπÂæµ‰∏¶Ë®ìÁ∑¥‰∫åÂÖÉÂàÜÈ°ûÂô®‰æÜÊ±∫ÂÆöÁôºÈü≥ÊòØÂê¶ÂêàÊ†º„ÄÇÂü∫Êñº ÈñÄ Ê™ª ÂÄº Á≠â Êñπ Ê≥ï Êó© Êúü Áî± (Hsu et al., 2016) Êèê Âá∫ ‰∏â Á®Æ Áôº Èü≥ Ê™¢ Ê∏¨ Áâπ Âæµ Ôºö Â∞ç Êï∏ Áõ∏ ‰ºº Â∫¶ ÂÄº (Log-Likelihood) „ÄÅ Â∞ç Êï∏ ‰∫ã Âæå Ê©ü Áéá (Log Posterior Probability) „ÄÅ ÊÆµ ËêΩ ÂçÄ Èñì Èï∑ Â∫¶ (Segment  58  Ë®±ÊõúÈ∫í Á≠â  Duration)Â∞çÊñºÁôºÈü≥Ê™¢Ê∏¨ÊïàÊûúÁöÑÂΩ±Èüø„ÄÇÂ≠∏ËÄÖ Kim Âú®ÂØ¶È©ó‰∏≠ÊåáÂá∫Â∞çÊï∏‰∫ãÂæåÊ©üÁéáÁÇ∫Ë°®ÁèæËºÉÂ•ΩÁöÑ ÁôºÈü≥Ê™¢Ê∏¨ÂàÜÊï∏ (Kim, Franco & Neumeyer, 1997)„ÄÇ‰πãÂæåÂâáÊúâÂ≠∏ËÄÖÁ∞°Âåñ‰∫ãÂæåÊ©üÁéáÁöÑË®àÁÆóÊñπÂºè ‰∏¶Â∞áÂÖ∂Á®±‰Ωú GOP (Goodness of Pronunciation) (Witt & Young, 2000)Ôºå‰πãÂæå‰πüÊúâÁ†îÁ©∂ÈáùÂ∞ç GOP Á≠âÊñπÊ≥ïÈÄ≤Ë°åÊîπËâØ(Zhang et al., 2008)„ÄÇÂõ†ÁÇ∫Âü∫ÊñºÈñÄÊ™ªÂÄº‰πãÊñπÊ≥ïÂ±ïÁèæ‰∫ÜÁ∞°ÊΩîÊúâÊïàÁöÑÂÑ™ÈªûÔºå ÊâÄ‰ª•Â≠∏ËÄÖÂÄëÊèêÂá∫‰ª•ÊúÄÂ§ßÂåñÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨‰πãF Â∫¶Èáè‰ΩúÁÇ∫ÁõÆÊ®ôÂ∞çËÅ≤Â≠∏Ê®°ÂûãÈÄ≤Ë°åÈëëÂà•ÂºèË®ìÁ∑¥ (Huang et al., 2012)„ÄÇ ËÄåÂü∫ÊñºÂàÜÈ°ûÂô®ÁöÑÁôºÈü≥Ê™¢Ê∏¨ÊñπÊ≥ïÔºåËºÉÊó©ÊòØÁî±(Wei, Hu, Hu & Wang, 2009)ÊâÄÊèêÂá∫ÁöÑÔºåÈô∏ Á∫å‰πüÊúâË®±Â§ö‰∏çÂêåÁöÑÁôºÈü≥ÁâπÂæµ(Lee & Glass, 2012; Laborde et al., 2016)ÊàñÊòØ‰∏çÂêåÂàÜÈ°ûÊ®°Âûã (Hu et al., 2015)„ÄÇ‰∫ãÂØ¶‰∏äÔºåÂú®ÈåØË™§ÁôºÈü≥Ë®∫Êñ∑‰ªªÂãô‰∏≠ÔºåÊó©Â∑≤ÈñãÂßãÊï¥ÂêàÂ§öÁ®ÆÁâπÂæµÔºå‰æãÂ¶ÇÂºïÂÖ• ÈüªÂæãÁâπÂæµ(Strik, Truong, De Wet & Cucchiarini, 2007)„ÄÇ‰ΩÜÈÄôÈ°ûÁöÑÂÅöÊ≥ïÈÉΩÂè™ÊúâÂú®ÁâπÂÆöÁöÑÁôºÈü≥ ÊâçËÉΩ‰ΩøÁî®(‰æãÂ¶ÇËç∑Ëò≠Ë™ûÁöÑ/x/Êàñ/k/)Ôºå‰∏îÈüªÂæãÁâπÂæµÂÆπÊòìÂõ†ÁÇ∫‰∏çÂêåË™ûËÄÖËÄåÁî¢ÁîüÁÑ°Ê≥ïÈ†êÊúüÁöÑËÆäÂåñ„ÄÇ ÁÑ∂ËÄåÔºå‰πüÊúâÂ≠∏ËÄÖÂü∫ÊñºË™ûÈü≥Ëæ®Ë≠òÊ®°ÁµÑ‰æÜÈÄ≤Ë°åÁôºÈü≥Ë®∫Êñ∑(Hu et al., 2015)Ôºå‰ΩÜÂæûÂØ¶È©óÊï∏ÊìöÁúã‰æÜ Ë∑ùÈõ¢ÁêÜÊÉ≥ÁöÑÊ∫ñÁ¢∫ÁéáÈÇÑÊúâ‰∏ÄÊÆµÂ∑ÆË∑ù„ÄÇÊúâ‰∫õÂ≠∏ËÄÖË™çÁÇ∫ÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ËàáË®∫Êñ∑ÊáâË©≤Ë¶ÅË¶ñÁÇ∫Ë™ûÈü≥Ëæ® Ë≠òÁöÑ‰ªªÂãô(Harrison et al., 2008; Harrison et al., 2009; Qian et al., 2012)ÔºåÂ∞áË®ìÁ∑¥Ë≥áÊñôÁöÑÈåØË™§ ÂûãÊÖã(Error Pattern)ÈÉΩË®òÈåÑÂú®Ê®°Âûã‰∏≠ÔºõÂÄòËã•Ê∏¨Ë©¶Ë≥áÊñôÂá∫ÁèæË®ìÁ∑¥ÊôÇÂæûÊú™ÁöÑÈåØË™§ÂûãÊÖãÔºåËæ®Ë≠òÁµê ÊûúÂ∞áÊúÉÁÑ°Ê≥ïÈ†êÊúüÔºå‰∏îË©≤ÊÉÖÊ≥ÅÊúÉÂõ†ÁÇ∫Â§ñË™ûÂ≠∏ÁøíËÄÖÁöÑÊØçË™û‰∏çÂêå‰ΩøÂæóÊõ¥ÂÆπÊòìÁôºÁîü„ÄÇ 3. ÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨  L2ÂæÖÊ∏¨Ë™ûÂè• ÊèêÁ§∫ÊñáÊú¨ (Ê®ôÊ∫ñÈü≥Á¥†Â∫èÂàó)  Ë™ûÈü≥ÁâπÂæµ Êì∑Âèñ Âº∑Âà∂Â∞ç‰Ωç  Ë®ìÁ∑¥ÈöéÊÆµ L1Ë®ìÁ∑¥Ë™ûÂè• (ÁöÜÁÇ∫Ê≠£Á¢∫ÁôºÈü≥)  Ë™ûÈü≥ÁâπÂæµ Êì∑Âèñ  L1Ë™ûÂè•ÁöÑÂ∞çÊáâÊñáÊú¨  ËÅ≤Â≠∏Ê®°Âûã Ë®ìÁ∑¥  Èü≥Á¥†Â±§Ê¨° ÁôºÈü≥ÁâπÂæµ Êì∑Âèñ  ÁôºÈü≥ÁÇ∫ Ê≠£Á¢∫/ÈåØË™§ Ê±∫Á≠ñÂáΩÊï∏ Ê∏¨Ë©¶ÈöéÊÆµ  ÂàùÂßãËÅ≤Â≠∏Ê®°Âûã (GMM-HMMÊàñ DNN-HMM)  (ÊúÄÂ§ßÂåñË™ûÈü≥Ëæ®Ë≠òÁµêÊûúË®ìÁ∑¥Ôºå‰æãÂ¶ÇÔºö ÊúÄÂ§ßÂåñÁõ∏‰ººÂ∫¶ÂÄºÊàñÊúÄÂ∞èÂåñ‰∫§ÂèâÁÜµ)  Âúñ 1. Âü∫Á§éÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ÊµÅÁ®ãÂúñ [Figure 1. The flowchart of the mispronunciation detection process.]  Ë©ï‰º∞Â∞∫Â∫¶Áõ∏ÈóúÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÊñºËèØË™ûÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨‰πãÁ†îÁ©∂  59  ÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ÁöÑÂü∫Êú¨ÊµÅÁ®ãÂ¶ÇÂúñ 1 ÊâÄÁ§∫„ÄÇÊàëÂÄëÈ¶ñÂÖà‰ΩøÁî®ÊØçË™ûËÄÖÁöÑË™ûÊñôÂ∫´Ë®ìÁ∑¥Ë™ûÈü≥Ëæ®Ë≠òÊâÄÈúÄ ÁöÑËÅ≤Â≠∏Ê®°ÂûãÔºåÂú®Â∞áÂ§ñË™ûÂ≠∏ÁøíËÄÖÁöÑÁôºÈü≥Ë™ûÂè•ËàáÊ≠£Á¢∫ÁöÑÊñáÊú¨ÂÅöÂº∑Âà∂Â∞ç‰ΩçÔºõÊé•ËëóÂ∞áËÅ≤Â≠∏Ê®°ÂûãÁÆó Âá∫ÁöÑ‰∫ãÂæåÊ©üÁéá‰ΩúÁÇ∫ÁôºÈü≥Ê™¢Ê∏¨ÁâπÂæµÈÄ≤Ë°åÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨„ÄÇÈåØË™§ÁôºÈü≥Ê™¢Ê∏¨ÁöÑÊó©ÊúüÁ†îÁ©∂‰∏≠ÔºåÊúâÂ≠∏ ËÄÖÂª∂Á∫å(Kim et al., 1997)ÁöÑËßÄÂØü‰∏¶Â∞á‰∫ãÂæåÊ©üÁéáÊîπËâØ‰∏¶Á®±‰Ωú GOP (Witt & Young, 2000)Ôºå‰πü ÊòØÊúÄÂ∏∏Ë¢´‰ΩøÁî®ÁöÑÁôºÈü≥Ê™¢Ê∏¨ÊñπÊ≥ï„ÄÇGOP ÁöÑË®àÁÆóÊñπÂºèÂ¶Ç‰∏ãÔºö  GOP , ‚â° log ,  ,| ,  (1)  , log ‚àë ‚àà  ,| ,  , ,|  ,  (2)  log ,  ,| ,  ‚àà,  ,|  (3)  ÂÖ∂‰∏≠ GOP ÊòØÈü≥Á¥†ÊÆµËêΩ , Â∞çÊáâÁõÆÊ®ôÈü≥Á¥† , ÁöÑ‰∫ãÂæåÊ©üÁéáÔºåÂÖ∂‰∏≠ u Ëàá n Ë°®Á§∫Á¨¨ u ÂÄãË™ûÂè•ÁöÑ Á¨¨ n ÂÄãÈü≥Á¥†ÔºåÊ†πÊìöË≤ùÊ∞èÂÆöÁêÜÂ∞áÂºè(1)ËΩâÊèõÊàêÂºè(2)Ôºõ , ÊòØË©≤ÊÆµËêΩÂ∞çÊáâÁöÑÈü≥Á¥†ÈõÜÂêàÔºåÂèØ‰ª•ÊòØ ÂÖ®ÈÉ®Èü≥Á¥†ÊàñÈÉ®ÂàÜËºÉÊ∑∑Ê∑ÜÁöÑÈü≥Á¥†Ôºå , ÂâáÊòØÈü≥Á¥†ÊÆµËêΩÁöÑÁ∂ìÊ≠∑ÊôÇÈñì(Duration)„ÄÇÊàëÂÄëÂÅáË®≠ÊØèÂÄã Èü≥Á¥†ÁöÑ‰∫ãÂâçÊ©üÁéáÁõ∏ÂêåÔºå‰∏îÂè™‰ΩøÁî®ÊúÄÂ§ßÁõ∏‰ººÂ∫¶ÂÄºÁöÑÈü≥Á¥†ÔºåÂç≥ÊúÄÊ∑∑Ê∑ÜÈü≥Á¥†ÂÅöÁÇ∫ÂàÜÊØçÈ†ÖÔºåÂ¶ÇÂºè (3)„ÄÇÂÖ∂‰∏≠ , | , ÊòØÂ∑≤Áü•Èü≥Á¥† , Ë¶ÅÂèñÂæóÈü≥Á¥†ÊÆµËêΩ , ÁöÑÁõ∏‰ººÂ∫¶ÂÄºÔºåË®àÁÆó , | , ÂèØ ‰ª•ÈÄèÈÅéÂ∑≤Áü•ÁöÑÊñáÊú¨ÂÖßÂÆπÂ∞çË™ûÂè•ÈÄ≤Ë°åÂº∑Âà∂Â∞ç‰ΩçÂèñÂæóÂ∞çÊáâÈü≥Á¥† , ÁöÑÁãÄÊÖãÂ∫èÂàó ‚àó , , ‚Ä¶ , ÔºåÂêåÊôÇ‰πüÂèØ‰ª•ÂæóÂà∞Èü≥Á¥†ÊÆµËêΩÂçÄÈñìÂ∞çÊáâÁöÑËµ∑ÂßãÊôÇÈñì ËàáÁµêÊùüÊôÇÈñì „ÄÇÂºè(3) ÊâÄË®àÁÆóÁöÑ GOP ÂàÜÊï∏‰ΩúÁÇ∫Ê±∫Á≠ñÁôºÈü≥ÈåØË™§ËàáÂê¶ÁöÑË©ï‰º∞‰æùÊìöÔºå‰∏¶Á∂ìÈÅéÂºè(3)Ê±∫ÂÆöÁôºÈü≥Á®ãÂ∫¶ÁöÑÂàÜ Êï∏„ÄÇÊàëÂÄëÂÆöÁæ©ÂáΩÊï∏ D(¬∑)Ë°®Á§∫ÁôºÈü≥ÁöÑÊ±∫Á≠ñÂáΩÊï∏Ôºö  D,  ‚àô  ,  (4)  ËÄå D(¬∑)Êé•Ëøë 1 Ë°®Á§∫ÁôºÈü≥ÂèØËÉΩÈåØË™§ÔºåÊé•Ëøë 0 ÂâáË°®Á§∫ÁôºÈü≥Ê≠£Á¢∫Ôºå Ë°®Á§∫Ê±∫Á≠ñÁî®ÁöÑÈñÄÊ™ªÂÄºÔºåËÄåÂèÉ Êï∏ Áî®‰æÜÂ∞á GOP ÂàÜÊï∏ÊîæÂ§ßÊàñÁ∏ÆÂ∞è„ÄÇ‰∏äËø∞ÂÖ©ÂÄãÂèÉÊï∏ÁöÜÂèØ‰ª•Ë®≠Ë®àÁÇ∫Èü≥Á¥†Áõ∏‰æùÔºåËã•ÁÇ∫Èü≥Á¥†Áõ∏‰æù ÂâáÁî® Ëàá Ë°®Á§∫„ÄÇÊé•ËëóÊàëÂÄëÂà©Áî®ÊåáÁ§∫ÂáΩÊï∏Âà§ÂÆöÁôºÈü≥ÊòØÂê¶ÈåØË™§Ôºö  D,  
This paper sets out to explore the use of multi-task learning (MTL) techniques for more accurate estimation of the parameters involved in neural network based acoustic models, so as to improve the accuracy of meeting speech recognition. Our main contributions are two-fold. First, we conduct an empirical study to leverage various auxiliary tasks to enhance the performance of multi-task learning on meeting speech recognition. Furthermore, we also study the synergy effect of combing multi-task learning with disparate acoustic models, such as deep neural network (DNN) and convolutional neural network (CNN) based acoustic models, with the expectation to increase the generalization ability of acoustic modeling. Second, since the way to modulate the contribution (weights) of different auxiliary tasks during acoustic model training is far from optimal and actually a matter of heuristic judgment, we thus propose a simple model adaptation method to alleviate such a problem. A series of experiments have been carried out on the Mandarin meeting recording (MMRC) corpora, which seem to reveal the effectiveness of our proposed methods in relation to several existing baselines. Keywords: Multi-Task Learning, Deep Learning, Neural Network, Meeting Speech Recognition. 1. Á∑íË´ñ Âè£Ë™ûÂ∞çË©±ÊòØ‰∫∫Ëàá‰∫∫‰πãÈñìÊúÄËá™ÁÑ∂ÁöÑÊ∫ùÈÄöÊñπÂºèÔºåÂèØ‰ª•È†êÊúüÂÆÉ‰πüÊòØ‰∫∫ÂÄëËàá‰∫∫Â∑•Êô∫ÊÖßÂä©ÁêÜÁ≠âÊ©üÂô® ÈñìÊúÄÈáçË¶ÅÁöÑ‰∫íÂãïÊñπÂºè„ÄÇËøëÂÖ≠ÂçÅÂπ¥‰æÜÔºåËá™ÂãïË™ûÈü≥Ëæ®Ë≠òÁöÑÁ†îÁ©∂Ê¥ªÂãïÂçÅÂàÜÊ¥ªË∫çÔºå‰∏¶‰∏îÂ∑≤ÂèñÂæó‰∫Ü Â∑®Â§ßÁöÑÊàêÂäü„ÄÇÂú®Á†îÁ©∂ÂàùÊúüÔºåË™ûÈü≥Ëæ®Ë≠òÂô®Âè™ËÉΩÂú®ÂÆâÈùúÁöÑÁí∞Â¢É‰∏≠Ë≠òÂà•‰∏ÄÂÄãÂñÆÁç®ÁöÑË©ûÂΩô„ÄÇ1980 Âπ¥‰ª£Ôºå‰ª•È´òÊñØÊ∑∑ÂêàÊ®°Âûã-Èö±ËóèÂºèÈ¶¨ÂèØÂ§´Ê®°Âûã(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM) ÂÅö ÁÇ∫ ËÅ≤ Â≠∏ Ê®° Âûã ‰Ωø Âæó Ë™û Èü≥ Ëæ® Ë≠ò Êúâ ËÉΩ Âäõ ÈÄ≤ Ë°å Â§ß Ë©û ÂΩô Èáè ÈÄ£ Á∫å Ë™û Èü≥ Ë≠ò Âà• „ÄÇ Áî± Êñº GMM-HMM ÁöÑÊû∂ÊßãÊòìÊñºË®ìÁ∑¥Ê®°ÂûãÂíåÈÄ≤Ë°åËÅ≤Â≠∏Ëß£Á¢ºÔºåÂõ†Ê≠§Ëøë‰∫åÂçÅÂπ¥‰æÜ GMM-HMM ÊòØËá™Âãï Ë™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±ÁöÑ‰∏ªÊµÅËÅ≤Â≠∏Ê®°ÂûãÔºåËÅ≤Â≠∏Ê®°ÂûãÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ª•Êõ¥Â•ΩÁöÑÊ®°ÂûãÁµêÊßãËàáË®ìÁ∑¥Êºî ÁÆóÊ≥ïÊîπËâØ GMM-HMM„ÄÇÈ°ØËëóÁöÑÊàêÊûúÂåÖÂê´ÁãÄÊÖãËÅØÁπ´(State Tying) (Young & Woodland, 1993)„ÄÅ Èëë Âà• Âºè Ë®ì Á∑¥ (Discriminative Training) (Povey, 2004)Ëàá ÊúÄ Â§ß Áõ∏ ‰ºº Â∫¶ Á∑ö ÊÄß ËΩâ Êèõ (Maximum Likelihood Linear Transformation, MLLT) (Gales, 1998)„ÄÇÂú® GMM-HMM Ê®°Âûã‰∏ªÂ∞éË™ûÈü≥Áïå ÁöÑÊôÇÊúüÂÖßÔºåÁ†îÁ©∂Â≠∏ËÄÖÂÄë‰πüÊé¢Á¥¢‰∫ÜË®±Â§ö‰∏çÂêåÁöÑËÅ≤Â≠∏Ê®°ÂûãÊñπÊ≥ïÔºåÁÑ∂ËÄåÂçªÊ≤íÊúâ‰∏ÄÁ®ÆÊñπÊ≥ïÂèØ‰ª•ÂÉè GMM-HMM ÊªøË∂≥Âª∫ÁΩÆÊàêÊú¨ÂíåËæ®Ë≠òÊïàËÉΩÁöÑÂπ≥Ë°°„ÄÇÈÅéÂéªÁöÑ‰∫îÂπ¥ÂÖßÊàëÂÄëÁúãË¶ã‰∫ÜÊ∑±Â±§Â≠∏ÁøíÊû∂ÊßãÂíå  ËûçÂêàÂ§ö‰ªªÂãôÂ≠∏ÁøíÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØËÅ≤Â≠∏Ê®°ÂûãË®ìÁ∑¥ÊñºÊúÉË≠∞Ë™ûÈü≥Ëæ®Ë≠ò‰πãÁ†îÁ©∂  87  ÊäÄË°ìÂú®ÈõªËÖ¶Ë¶ñË¶∫„ÄÅË™ûË®ÄÂèäË™ûË®ÄÂ≠∏ÁøíÈ†òÂüüÁöÑÂ∑®Â§ßÊàêÂäü„ÄÇÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØËàáÂÖ∂ËÆäÈ´îÊúÄÁµÇÂèñ‰ª£ ‰∫Ü GMMÔºåÊ∑∑ÂêàÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø-Èö±ËóèÂºèÈ¶¨ÂèØÂ§´Ê®°Âûã(Hybrid Deep Neural Networks-Hidden Markov Model, DNN-HMM)Â∑≤ÊàêÁÇ∫Â§ßÂ§öÊï∏Ëá™ÂãïË™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±ÁöÑËÅ≤Â≠∏Ê®°Âûã„ÄÇDNN ÁöÑÁ´ÑËµ∑ÂèØ Ê≠∏ÂäüÊñº‰ª•‰∏ãÂÖ≠Á®ÆÂõ†Á¥†Ôºö1)Ê∑±Â±§Â≠∏ÁøíÊû∂ÊßãÂèäÊºîÁÆóÊ≥ïÔºõ2)ÈÄöÁî®Ë®àÁÆóÂúñÂΩ¢ËôïÁêÜÂô®(General Purpose Graphical Processing Units, GPGPU)ÁöÑÁôºÂ±ïÔºõ3)Êï∏ÂçÉÂ∞èÊôÇÁöÑÂ∑≤ËΩâÂØ´Ë™ûÈü≥Ë®ìÁ∑¥Ë≥áÊñôÂèäÊõ¥Â§öÁöÑ Êú™Ê®ôË®òË≥áÊñôÔºõ4)Ë°åÂãïÂºèÁ∂≤ÈöõÁ∂≤Ë∑ØÂíåÈõ≤Á´ØË®àÁÆóÔºõ5)ÂæûÁîüÊ¥ªÂà∞Â∑•‰ΩúÁí∞Â¢ÉÈÉΩÂª£Ê≥õÂú∞Âá∫ÁèæË™ûÈü≥Ëæ® Ë≠òÊäÄË°ìÈúÄÊ±Ç„ÄÇ  Âúñ 1. Ë™ûÈü≥Ëæ®Ë≠òÊµÅÁ®ãËàáÊú¨Ë´ñÊñáÂòóË©¶ÊîπËâØ‰πãËôï(ËôõÁ∑öÊ®ôË®òÈÉ®ÂàÜ) [Figure 1. Illustration of the Enhanced Speech Recognition System] ÈõñÁÑ∂Ëá™ÂãïË™ûÈü≥Ëæ®Ë≠òÊäÄË°ìÂ∑≤Á∂ìÊòØ‰∏ÄÈ†ÖÊàêÁÜüÁöÑÊäÄË°ìÔºå‰ΩÜÊòØÂú®ÂØ¶ÈöõÊáâÁî®‰∏ä‰ªçÊúâË®±Â§öÂïèÈ°åÈúÄ Ë¶ÅË¢´Ëß£Ê±∫„ÄÇ‰æãÂ¶Ç‰ΩøÁî®Êô∫ÊÖßÂûãÊâãÊ©üÈåÑÈü≥ÊôÇÂæÄÂæÄÈõ¢ÊâãÊ©üÈ∫•ÂÖãÈ¢®ËºÉÈÅ†ÔºåÈåÑÈü≥ÂìÅË≥™ÂÆπÊòìÂèóÁí∞Â¢ÉÂΩ± Èüø„ÄÇÊ≠§Â§ñÔºåÁèæ‰ªäË™ûÈü≥Ëæ®Ë≠òÈ†òÂüü‰πüÈù¢Ëá®ËëóÊµ∑ÈáèË©ûÂΩô„ÄÅËá™Áî±‰∏çÂèóÈôêÁöÑ‰ªªÂãô„ÄÅÂêµÈõúÁöÑÈÅ†Ë∑ùÈõ¢Ë™û Èü≥„ÄÅËá™ÁôºÊÄßÁöÑÂè£Ë™ûÂèäË™ûË®ÄÊ∑∑ÈõúÊÉÖÊôØÁöÑÊåëÊà∞(Yu & Deng, 2014)„ÄÇËÄåÊúÉË≠∞Ë™ûÈü≥Ëæ®Ë≠ò1Ê≠£Ê∂µËìã‰∫Ü ‰∏äËø∞Â§ßÈÉ®ÂàÜÁöÑÂõ∞Â¢ÉËàáÊåëÊà∞ÔºåÊòØ‰∏ÄÂÄãÁõ∏Áï∂Âõ∞Èõ£ÁöÑË™ûÈü≥Ëæ®Ë≠ò‰ªªÂãô„ÄÇÊú¨Ë´ñÊñáÂ∞áÂïèÈ°åË¶ñÁÇ∫Ë®ìÁ∑¥Ëàá Ê∏¨Ë©¶Áí∞Â¢É‰∏çÂåπÈÖçÔºõÈô§‰∫ÜË™ûÈü≥ÂíåÊñáÂ≠óÁöÑ‰ΩøÁî®‰∏çÂêåÔºå‰πüÂåÖÂê´‰∫ÜÂ§öË™ûË®ÄÁöÑÊ∑∑Áî®„ÄÇÁ∞°ËÄåË®Ä‰πãÔºå‰∏ä Ëø∞ÂïèÈ°åÊòØÂú®ËÄÉÈ©óË™ûÈü≥Ëæ®Ë≠ò‰πã‰∏ÄËà¨ÂåñËÉΩÂäõ„ÄÇÁÇ∫ÂÖãÊúçÊ≠§ÂïèÈ°åÔºåÊàëÂÄëÈ¶ñÂÖàÊØîËºÉÂêÑÁ®ÆËºîÂä©‰ªªÂãô‰æÜ Âä†Âº∑Â§ö‰ªªÂãôÂ≠∏ÁøíÂú®ÊúÉË≠∞Ë™ûÈü≥Ëæ®Ë≠òÁöÑË°®Áèæ„ÄÇÂÖ∂Ê¨°ÔºåËóâÁî±Â§öÊ®£Áï∞Ë≥™Ê®°ÂûãÁöÑÊï¥Âêà‰πüÊòØÂ∏∏Ë¶ãÁöÑÊñπ Ê≥ïÔºåÊú¨Ë´ñÊñáÈÄèÈÅéÂØ¶È©ó‰æÜÈ©óË≠âÁï∞Ë≥™Ê®°ÂûãÁµêÂêàÁöÑÊïàÊûú„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÈáçÊñ∞Ë™øÈÅ©Ê≥ï‰ª•Ê∏õËºï ÂÇ≥Áµ±Ë™øÊï¥ËºîÂä©‰ªªÂãôÊ¨äÈáçÊñπÊ≥ïÁöÑÁº∫Èô∑„ÄÇÊâÄÊúâÁµêÊûúÊòØÈ©óË≠âÊñº‰∏ÄÂú®Âè∞ÁÅ£ÊâÄÊî∂ÈåÑÁöÑÊúÉË≠∞Ë™ûÊñôÂ∫´Ôºå ÂÖßÂÆπÂ§öÁÇ∫‰∏≠ÊñáÂíå‰∏≠Ëã±ÊñáÊ∑∑Áî®Ë™ûÂè•„ÄÇÂúñ 1 ÊâÄÁ§∫ÁÇ∫Ë™ûÈü≥Ëæ®Ë≠òÊµÅÁ®ãËàáÊú¨Ë´ñÊñáÂòóË©¶ÊîπÈÄ≤ÁöÑÈÉ®ÂàÜ(‰ª• 1ÊúÉË≠∞Ë™ûÈü≥ÁöÑÂÖßÂÆπÂ¶ÇË°® 1 ÊâÄÁ§∫„ÄÇ‰∏çÈõ£ÁôºÁèæË™ûÊñô‰∏≠Èô§‰∫Ü‰∏≠Ëã±Êñá‰∫§‰∫í‰ΩøÁî®Â§ñÔºå‰πüÊúâËá™ÁÑ∂Â∞çË©±ÊâÄÂåÖÂê´ÁöÑË™û Âä©Ë©û„ÄÅÂè£È†≠Á¶™„ÄÅÂè£ÂêÉËàáË¥ÖË©û„ÄÇÂ∞àÊúâÂêçË©ûËàáÊôÇÈ´¶Êñ∞Ë©û‰πüÂèØËÉΩÂá∫ÁèæÂú®Ë™ûÊñô‰∏≠„ÄÇ  88  Ê•äÊòéÁø∞ Á≠â  ËôõÁ∑öÊ®ôË®ò)„ÄÇ Ë°® 1. ÊúÉË≠∞Ë™ûÈü≥ÁØÑ‰æã [Table 1. Meeting transcription examples extracted from MMRC]  Ë™ûÂè•Á∑®Ëôü  Ë™ûÂè•ÂÖßÂÆπ  U0006-002932 ÊïôÊùêÁöÑ scenario ÂèØËÉΩ target audience Â¶ÇÊûúÊòØÈÄôÁæ§‰∫∫ÂèØËÉΩÊòØÊáâË©≤ ÊÄéÈ∫ºÊÄéÈ∫ºËÆì‰ªñ aggregate Âá∫‚ºÄÂ•óÁêÜË´ñËÆì‰ªñÂÜçÁôºÊèÆÊõ¥Â§ß  U0000-001623 ÂëÉËÄÉÊ≥ïÂæãÁöÑÊ≥ïÂ≠∏Èô¢ÁöÑÁ¢©Â£´Âêº ÁÑ∂ÂæåÂëÉÂ∞±ÈÜ´Â≠∏Èô¢ÁöÑÁ¢©Â£´ÈÇÑÊúâÈÇ£ÂÄã U0000-001624 ÈÇ£ÂÄãÁâôÁßëÁöÑÁ¢©Â£´ÂêºÈÇ£ÂÄãÈÇ£ÂÄãË£úÁøíÁè≠Ë£úÂæóÂæàÂÖáÂïäÈÇ£ÁîüÊÑèÈÉΩÂæàÂ•Ω  U0002-000118 ÂëÉ cover Âà∞ÁÑ∂Âæå‰πüË¨õÂà∞ÈÄôÂÄã ÊàëÂÄë‰∏äÊ¨°Ë´áÁöÑÈÄôÂÄã megatrend Âêº  U0002-000122 Â∞±ÊòØ knowledge formulation ÂëÉ diversification ÈÇ£ÂÄã is great  Êú¨Ë´ñÊñáÂæåÁ∫åÁ´†ÁØÄÂÆâÊéíÂ¶Ç‰∏ãÔºöÁ¨¨‰∫åÂ∞èÁØÄÂ∞áÁ∞°‰ªãÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÁõ∏ÈóúÊñáÁçªÊé¢Ë®éÔºåÁ¨¨‰∏âÂ∞èÁØÄ ‰ªãÁ¥πÂ§ö‰ªªÂãôÂ≠∏ÁøíÁöÑÁôºÂ±ïÊºîÈÄ≤‰ª•ÂèäÊàëÂÄëÊÉ≥Ë¶ÅÊé¢Ë®éÁöÑËºîÂä©‰ªªÂãôÔºåÁ¨¨ÂõõÂ∞èÁØÄ‰ªãÁ¥πÊàëÂÄëÊèêÂá∫ÁöÑÈáç Êñ∞Ë™øÊï¥Ê≥ïÔºåÁ¨¨‰∫îÂ∞èÁØÄÂâáËß£ÊûêÂü∫Á§éÂØ¶È©óÂèäÂ§ö‰ªªÂãôÂ≠∏ÁøíÁöÑÂØ¶È©óÁµêÊûúÔºåÊúÄÂæåÂú®Á¨¨ÂÖ≠Â∞èÁØÄÈÄ≤Ë°åÁµê Ë´ñËàáÊé¢Ë®éÊú™‰æÜÂèØËÉΩÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇ 2. È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰πãÁõ∏ÈóúÊñáÁçªÊé¢Ë®é Âú®Ê©üÂô®Â≠∏ÁøíÁöÑÈ†òÂüü‰∏≠ÔºåÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑËµ∑Ê∫êÂèØ‰ª•ËøΩÊ∫ØÂà∞ 1943 Âπ¥ÁöÑÊï∏Â≠∏ÂÆ∂ McCullochÔºå‰ªñË®≠ Ë®à‰∫Ü‰∏ÄÂ•óÊï∏Â≠∏ÊñπÊ≥ïÊ®°Êì¨Á•ûÁ∂ìÂÖÉÈÅã‰ΩúÁöÑÊ®°ÂºèÔºåÊòØÈñãÂïü‰∫ÜÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁ†îÁ©∂Â§ßÈñÄÁöÑÂÖàÈ©Ö„ÄÇÊé•Ëëó Âú® 1957 Âπ¥ÔºåRosenblatt ÊòØÁ¨¨‰∏ÄÂÄãÂ∞áÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ¶ÇÂøµ‰ªòË´∏ÂØ¶Ë°åÁöÑÂ≠∏ËÄÖÔºåÊèêÂá∫‰∫ÜÊÑüÁü•Âô® (Perceptron)Ê®°Âûã„ÄÇ1975 Âπ¥ÔºåWerbos ÊèêÂá∫ÂÄíÂÇ≥Â∞éÊºîÁÆóÊ≥ï(Backpropagation Algorithm) (Werbos, 1974)ÊîπÂñÑÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèÉÊï∏Êõ¥Êñ∞ÁöÑÊñπÂºè„ÄÇÁµÇÊñºÂú® 1988 Âπ¥ÔºåRumelhart Á≠â‰∫∫ÁôºÊòé‰∫ÜÂ§öÂ±§ÊÑüÁü• Âô®(Multilayer Perceptron, MLP) (Rumelhart, Hinton & Ronald, 1988)ÔºåÂõ†ÁÇ∫Â§öÂ±§ÊÑüÁü•Âô®ÈÅ©Áî® ÊñºÊõ¥Â§öÂÖÉÁöÑÂïèÈ°åÔºå‰ΩøÂæóÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÁ†îÁ©∂ÁÜ±ÊΩÆÂÜçÂ∫¶ÁÜ±Áµ°Ëµ∑‰æÜ„ÄÇ Âú®Ë™ûÈü≥Ëæ®Ë≠òÈ†òÂüü‰∏≠ÔºåÂæû 1992 Âπ¥Ëµ∑ÔºåÂ∞±Èô∏Á∫åÊúâË®±Â§öÂ∞áÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØËàáÈö±ËóèÂºèÈ¶¨ÂèØÂ§´Ê®° ÂûãÁµêÂêà(Hidden Markov Model, HMM)ÁöÑÁ†îÁ©∂„ÄÇ‰æãÂ¶ÇÂú® 1998 Âπ¥ÔºåCook Á≠â‰∫∫(Cook et al., 1999) ‰ΩøÁî®Âª£Êí≠Êñ∞ËÅûÁöÑË™ûÊñôÔºåË®ìÁ∑¥Â§öÂÄãÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑Ø(Recurrent Neural Network, RNN)Ëàá MLP ÁöÑËÅ≤Â≠∏Ê®°ÂûãÔºå‰∏¶ÈÄèÈÅé ROVER (Recognition Output Voting Error Reduction, ROVER) (Fiscus, 1997)ÁöÑÊñπÊ≥ïÁµ±Êï¥ÈÄô‰∫õÊ®°ÂûãÁöÑËæ®Ë≠òÁµêÊûú„ÄÇ2000 Âπ¥ÊôÇÔºåÂ≠∏ËÄÖÂÄëÊåáÂá∫È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰πüÊòØÂêàÈÅ©ÁöÑ ÁâπÂæµÊì∑ÂèñÂ∑•ÂÖ∑Ôºå‰æãÂ¶Ç Bottleneck ÁâπÂæµÊàñ Tandem ÁâπÂæµ(Hermansky, Ellis & Sharma, 2000)„ÄÇ Êó©ÊúüÁöÑÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁ†îÁ©∂ÂèóÈôêÊñºÁ°¨È´îË®àÁÆóË≥áÊ∫êÁöÑ‰∏çË∂≥Ôºå‰∏î‰∏çÊòìÈÄ≤Ë°åÂπ≥Ë°åÂåñËôïÁêÜÔºå‰ΩøÂæó Áõ∏ÈóúÁ†îÁ©∂Ê≤íÊúâÈ°ØËëóÂú∞Á™ÅÁ†¥„ÄÇÁõ¥Âà∞ 2006 Âπ¥ÈñãÂßãÔºåÂ≠∏ËÄÖÂÄëÂú®Ë®ìÁ∑¥ÊºîÁÆóÊ≥ïËàáÊû∂Êßã‰∏äÊèêÂá∫‰∫Ü‰∏ÄÁ≥ª ÂàóÊîπÈÄ≤(Hinton, Osindero & Teh, 2006; Poultney, Chopra & Cun, 2006, Bengio, Lamblin, Popovici & Larochelle, 2007)ÔºåËÄåÂæåÂπæÂπ¥ÁöÑ GPGPU ÈÅãÁÆóË®≠ÂÇôÁôºÂ±ïËøÖÈÄüÔºå‰ΩøÂæóÊ∑±Â±§È°ûÁ•ûÁ∂ì  ËûçÂêàÂ§ö‰ªªÂãôÂ≠∏ÁøíÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØËÅ≤Â≠∏Ê®°ÂûãË®ìÁ∑¥ÊñºÊúÉË≠∞Ë™ûÈü≥Ëæ®Ë≠ò‰πãÁ†îÁ©∂  89  Á∂≤Ë∑ØÊ®°ÂûãË®àÁÆóÊàêÊú¨ÂïèÈ°åÂ§ßÂπÖÈôç‰ΩéÔºå‰πüËÆìÂ≠∏ËÄÖÂÄëÈ°òÊÑèÊäïÂÖ•Ê≠§Á†îÁ©∂„ÄÇÁèæ‰ªä‰∏ªÊµÅÁöÑËÅ≤Â≠∏Ê®°ÂûãË®≠ Ë®àÂç≥ÊòØÂà©Áî®Ê∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèñ‰ª£È´òÊñØÊ∑∑ÂêàÊ®°Âûã(Hinton et al., 2012)„ÄÇÈô§‰∫ÜÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø ‰πãÂ§ñÔºåÂ≠∏ËÄÖÂÄë‰πüÂòóË©¶ÂºïÂÖ•È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØËÆäÈ´îÔºå‰æãÂ¶Ç CNN (Abdel-Hamid et al., 2014)Ëàá RNN (Graves, Mohamed & Hinton, 2013)„ÄÇÈÄô‰∫õÊñ∞Á©éÁöÑÊ∑±Â±§Ê®°ÂûãÂú®Ë™ûÈü≥Ëæ®Ë≠òÈ†òÂüü‰πüÊúâÈ°ØËëóÁöÑÊàê Âäü(Sercu, Puhrsch, Kingsbury & LeCun, 2016)„ÄÇ 3. Â§ö‰ªªÂãôÂ≠∏ÁøíÊé¢Ë®é Â§ö‰ªªÂãôÂ≠∏Áøí(Caruana, 1997)ÊàñËÄÖÂ≠∏ÊúÉÂ≠∏Áøí(Learning To Learn) (Thrun & Pratt, 1988)ÊòØ‰∏ÄÁ®Æ Ê©üÂô®Â≠∏ÁøíÁöÑÊäÄË°ìÔºåÂÖ∂ÁõÆÁöÑÊòØÂ∏åÊúõËóâÁî±ÂÖ±ÂêåÂ≠∏ÁøíÊï∏ÂÄãÁõ∏ÈóúÁöÑËºîÂä©‰ªªÂãôÔºå‰ª•ÊèêÂçá‰∏ª‰ªªÂãôÁöÑ‰∏Ä Ëà¨ÂåñËÉΩÂäõ„ÄÇÂ§ö‰ªªÂãôÂ≠∏ÁøíÂ§ßÁ¥ÑÂú®‰∫åÂçÅÂπ¥ÂâçÈñãÂßãÊàêÁÇ∫‰∏ÄÈ†ÖÁÜ±ÈñÄÁöÑÁ†îÁ©∂ÔºåÊúâË®±Â§öË´ñÊñá‰ª•ÁêÜË´ñÁöÑ ËßíÂ∫¶ÂàÜÊûêÂ§ö‰ªªÂãôÂ≠∏ÁøíÁöÑË°åÁÇ∫Ëàá‰∏ÄËà¨ÂåñËÉΩÂäõÁïåÈôê(Generalization Bound)Ôºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÊúâ ÈóúÂ§ö‰ªªÂãôÂ≠∏ÁøíÁöÑÁµ±Ë®àÁêÜË´ñÔºå‰πüÈÄ≤‰∏ÄÊ≠•ÂæóÁü•ÔºåÈÄèÈÅéÁõ∏ÈóúËºîÂä©‰ªªÂãôÊâÄÁî¢ÁîüÁöÑÂèÉÊï∏ÂÅáË®≠Á©∫Èñì (Parameter Hypothesis Space)‰ΩúÁÇ∫Âü∫Á§éÔºåËÉΩÂ§†Êèê‰æõÊõ¥Â•ΩÁöÑÂàùÂßãÂèÉÊï∏ÂÅáË®≠Á©∫ÈñìÁµ¶ÂÖ∂ÂÆÉÊñ∞ÁöÑËºî Âä©‰ªªÂãô„ÄÇËøëÂπ¥‰æÜÔºåÂ§ö‰ªªÂãôÂ≠∏ÁøíÁöÑÁ†îÁ©∂ÈñãÂßãÊé¢Á¥¢Ëá™ÂãïÂú∞Â≠∏Áøí‰ªªÂãô‰πãÈñìÁöÑÈóú‰øÇÔºåZhang Á≠â‰∫∫ Â∞áÂ≠∏Áøí‰ªªÂãô‰πãÈñìÁöÑÈóú‰øÇË¶ñÁÇ∫Ê±ÇËß£Âá∏ÂáΩÊï∏(Convex Function)ÁöÑÈÅéÁ®ã(Zhang & Yeung, 2014)Ôºõ ÂÅáË®≠Êï∏ÂÄãÁ∑öÊÄßÂõûÊ≠∏‰ªªÂãôÁöÑÂèÉÊï∏ÂÖ∑ÊúâÁõ∏ÂêåÁöÑÁü©Èô£Â∏∏ÊÖãÂàÜ‰Ωà‰∫ãÂâçÊ©üÁéá(Matrix-Variate Normal Distribution Prior)Ôºå‰∏¶Áî±ÂÖ±ËÆäÁï∞Êï∏Áü©Èô£ÂÆöÁæ©‰ªªÂãôËàá‰ªªÂãô‰πãÈñìÁöÑÈóú‰øÇÔºåÊ®°ÂûãË®ìÁ∑¥ÊôÇËÉΩÈñìÊé•Â≠∏ Áøí Â¶Ç ‰Ωï Êõø Ê≠£ ‰æã ‰ªª Âãô Èóú ‰øÇ (Positive Task Correlation) Ëàá Ë≤† ‰æã ‰ªª Âãô Èóú ‰øÇ (Negative Task Correlation) ÁöÑÁõ∏ÈóúÊÄßÂª∫Á´ãÊ®°Âûã„ÄÇÂæå‰æÜ Zhang Á≠â‰∫∫Êõ¥ËûçÂÖ•‰∫ÜÂ§ö‰ªªÂãôÁâπÂæµÈÅ∏Âèñ(Multi-Task Feature Selection) ËàáÁõ∏ÈóúÊÄßÂ≠∏Áøí(Relationship Learning)Â∞çÈ´òÁ∂≠Â∫¶ÁöÑËº∏ÂÖ•Ë≥áÊñôÈÄ≤Ë°åËôïÁêÜ„ÄÇÊìö Â≠∏ËÄÖÁöÑÁ†îÁ©∂Ë≠âÊòéÔºåÂÅáË®≠Â§öÂÄã‰ªªÂãô‰πãÈñìÂΩºÊ≠§Áõ∏ÈóúÔºåÈÄöÈÅé‰∏ÄËµ∑Â≠∏ÁøíÁöÑÊñπÂºè‰æÜÂÖ±‰∫´ÂÖßÂú®ÁöÑË°®Á§∫ Ë≥áË®äÔºåÂ∞±ËÉΩÂ§†ÈÅîÂà∞Áü•Ë≠òËΩâÁßªÁöÑÊïàÊûú„ÄÇÂÖ∂ÂØ¶È©óÁµêÊûú‰πüË≠âÂØ¶Ê≠§ÊñπÊ≥ïÂú®Ê®°ÂûãÈÅáÂà∞Ê≤íÁúãÈÅéÁöÑË≥áÊñô (Unseen Data)ÊôÇ‰πüËÉΩÊúâ‰∏çÈåØÁöÑÊàêÊïà„ÄÇ 3.1 Ë™ûÈü≥Ëæ®Ë≠ò‰∏≠ÁöÑÂ§ö‰ªªÂãôÂ≠∏Áøí Â§ö‰ªªÂãôÂ≠∏ÁøíÁµêÂêàÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊû∂Êßã(Multi-Task Deep Neural Network, MTL-DNN)Â¶Ç Âúñ 2 ÊâÄÁ§∫„ÄÇË™ûÈü≥Ëæ®Ë≠òÈ†òÂüü‰∏≠‰πüÊúâË®±Â§öÁ†îÁ©∂ÂÖàÈÄ≤ÂòóË©¶ËûçÂêàË™ûÈü≥È†òÂüüÂèäÂ§ö‰ªªÂãôÂ≠∏ÁøíÊäÄË°ì„ÄÇ‰æã Â¶Ç Parveen Á≠â‰∫∫(Parveen & Green, 2003)Êé¢Ë®é‰∫Ü 11 Á®Æ‰∏çÂêåÁöÑÂàÜÈ°û‰ªªÂãôËàáË™ûÈü≥Â¢ûÂº∑‰ªªÂãôÁöÑÂΩ± ÈüøÔºå‰æãÂ¶ÇË™ûËÄÖÁöÑÊÄßÂà•ÊàñÊÉÖÁ∑íÁ≠â„ÄÇÂØ¶È©óÁµêÊûúÁôºÁèæÂú®ÂàÜÈ°û‰ªªÂãô‰∏≠ÔºåÂ§ö‰ªªÂãôË®ìÁ∑¥ÂÑ™ÊñºÂñÆ‰ªªÂãôË®ì Á∑¥ „ÄÇ Chen Á≠â ‰∫∫ (Chen & Mak, 2015) Ââá ÊòØ ÈÄè ÈÅé Â§ö ‰ªª Âãô Â≠∏ Áøí ÁöÑ Áâπ ÊÄß Ôºå ‰Ωø Âæó Ë≥á Ê∫ê Ë±ê ÂØå (Resource-Rich)ÁöÑË™ûË®ÄËÉΩÂ§†Âú®Ê®°ÂûãË®ìÁ∑¥ÁöÑÈÅéÁ®ã‰∏≠ÔºåËºîÂä©Ë≥áÊ∫êË≤ß‰πè(Resource-Poor)Ë™ûË®ÄÔºåÊèê ÂçáÂÆÉÁöÑËæ®Ë≠òÊïàÊûú„ÄÇSeltzer Á≠â‰∫∫ÁöÑÁ†îÁ©∂(Seltzer & Droppo, 2013)ÂâáÊòØÊé¢Ë®é‰∫Ü‰ª•ÁõÆÂâçÈü≥Ê°ÜÁöÑÈü≥ Á¥†Ê®ôË®ò„ÄÅÈÑ∞ËøëÈü≥Á¥†ÁãÄÊÖãÊ®ôË®ò(State Contexts)ÂèäÈÑ∞ËøëÈü≥Á¥†ÁöÑÈü≥Á¥†Ê®ôË®ò (Phone Contexts) ÂÅö ÁÇ∫ËºîÂä©‰ªªÂãôË®ìÁ∑¥ËÅ≤Â≠∏Ê®°ÂûãÁöÑÊïàÊûúÔºåÂÖ∂ÊñáÁçªÊåáÂá∫Âú®Ëã±Ë™ûÈü≥Á¥†Ë™ûÊñô(TIMIT) (Garofolo, Lamel, Fisher, Fiscus & Pallett, 1993)ÁöÑË™ûÈü≥Ëæ®Ë≠ò‰ªªÂãô‰∏≠ÊúâÈ°ØËëóÂú∞ÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåSeltzer Á≠â‰∫∫ÁöÑÁ†îÁ©∂ Âè™ÂÅúÁïôÂú®ÂñÆÈÄ£Èü≥Á¥†(Monophone)ÔºåÊ≤íÊúâ‰ΩøÁî®Âà∞‰∏âÈÄ£Èü≥Á¥†(Triphone)ÁöÑË≥áË®äÔºå‰πüÊ≤íÊúâÊé¢Ë®éËã• ‰ΩøÁî®‰∏âÈÄ£Èü≥Á¥†ÁãÄÊÖãÊ®ôË®òÂèñ‰ª£ÂñÆÈÄ£Èü≥Á¥†ÁãÄÊÖãÊ®ôË®òÂÅöÁÇ∫ËºîÂä©‰ªªÂãôÁöÑÊàêÊïàÔºåÈÄôÊ≠£ÊòØÊú¨Ë´ñÊñáÊÉ≥Êé¢  90  Ê•äÊòéÁø∞ Á≠â  Á©∂ÁöÑÂÖ∂‰∏≠‰∏ÄÈ†ÖÂïèÈ°å„ÄÇÂè¶Â§ñÔºåÂ≠∏ËÄÖÂÄëÂòóË©¶Â∞áÂ§öÁ®ÆË™ûË®ÄÁöÑË™ûÊñôÊ∑∑ÂêàÂú®‰∏ÄËµ∑ÔºåÂÖ±ÂêåË®ìÁ∑¥‰∏ÄÂÄãË∑® Ë™ûË®ÄÁöÑËÅ≤Â≠∏Ê®°Âûã(Ghoshal,Swietojanski & Renals, 2013; Huang, Li, Yu, Deng & Gong, 2013)Ôºå Ë≠âÂØ¶ÈÄôÁ®ÆÂÅöÊ≥ïÁ¢∫ÂØ¶ËÉΩÂ§†ÊèêÂçáÊ∫ñÁ¢∫Áéá„ÄÇÂ§öË™ûË®ÄÁöÑË≥áÊñô‰∏ªË¶Å‰ΩøÁî®ÊñºË®ìÁ∑¥ÈöéÊÆµÔºåÊâÄÊúâË™ûË®ÄÁöÑË®ì Á∑¥Ë≥áÊñôÁöÜÊúÉË™øÊï¥Â∫ïÂ±§ÂÖ±‰∫´ÁöÑÈö±ËóèÂ±§„ÄÇÊØèÁ®ÆË™ûË®ÄÊúâÂêÑËá™Â∞çÊáâÁöÑËº∏Âá∫Â±§ÔºåÂêÑÂÄãËº∏ÂÖ•Ë™ûË®ÄÁöÑËÅ≤ Â≠∏ÁâπÂæµÈô§‰∫ÜË™øÊï¥Â∫ïÂ±§ÁöÑÈö±ËóèÂ±§Â§ñÔºå‰πüÊúÉÊõ¥Êñ∞ÂÖ∂ÊâÄÂ∞çÊáâË™ûË®ÄÁöÑËº∏Âá∫Â±§ÔºåÂÖ∂ÂÆÉË™ûË®ÄÁöÑËº∏Âá∫Â±§ Â∞á‰∏çÊúÉË¢´ Êõ¥Êñ∞„ÄÇÂ¶ÇÊ≠§‰∏Ä‰æÜÔºåÈö±ËóèÂ±§Â∞±ÂèØË¢´Ë¶ñÁÇ∫‰∏ÄÂ±§‰∏ÄÂ±§ÁöÑÁâπÂæµËêÉÂèñÂô®„ÄÇÁ∏ΩËÄåË®Ä‰πãÔºåÂ§ö‰ªª ÂãôÂ≠∏ÁøíÂÖÅË®±Â≠∏ÁøíÂ§öÂÄã‰ªªÂãôÊôÇÔºå‰ª•Âª∫Ë®≠ÊÄß(Constructive)ÊàñÁ†¥Â£ûÊÄß(Destructive)ÈåØË™§Ë®äËôüÊ¢ØÂ∫¶ Êõ¥Êñ∞Èö±ËóèÂ±§„ÄÇÂú®Â§ö‰ªªÂãôÂ≠∏ÁøíÁöÑÊ°ÜÊû∂‰∏ãÔºåÊ®°ÂûãÂ∞áÊúÉÂêåÊôÇÂ≠∏ÁøíÔºö(1)‰∏ª‰ªªÂãôÔºõ(2)‰∏ÄÂÄãÊàñÊï∏ÂÄãÁõ∏ ÈóúÁöÑËºîÂä©‰ªªÂãôÔºõ(3)‰ªªÂãôÈñìÂÖ±‰∫´ÁöÑÈö±ËóèÂ±§„ÄÇ  Âúñ 2. Â§ö‰ªªÂãôÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁ§∫ÊÑèÂúñ [Figure 2. Illustration of the MTL-DNN system] 3.2 ËºîÂä©‰ªªÂãôÊé¢Ë®é ÊúâÂ≠∏ËÄÖÁöÑÁ†îÁ©∂ÊåáÂá∫ÔºåÂ§ö‰ªªÂãôÂ≠∏Áøí‰∏¶‰∏ç‰øùË≠âÊïàËÉΩÊúÉÊèêÂçáÔºåË®ìÁ∑¥ÁöÑÊºîÁÆóÊ≥ïÂèä‰ªªÂãôÊòØÂê¶Áõ∏ÈóúÂêå Ê®£‰πüÊòØÈáçË¶ÅÁöÑÈóúÈçµ(Caruana, 1997)„ÄÇÊúâÈëíÊñºÊ≠§ÔºåÊú¨Ë´ñÊñáÂæûÂÖ©Â§ßÈ°ûÁ†îÁ©∂Èù¢ÂêëÔºåÁØ©ÈÅ∏Âá∫ 10 Á®Æ ËºîÂä©‰ªªÂãôÈÄ≤Ë°åÊé¢Ë®éÔºåÂ¶ÇÂúñ 3 ÊâÄÁ§∫„ÄÇÂÖ∂‰∏≠‰∏ÄÂÄãÈù¢ÂêëÊòØË™ûË®ÄËàáÈü≥ÈüªÂ≠∏Ë≥áË®äÔºåÊ≠§È°ûÂûãÁöÑË≥áË®ä‰∏ª Ë¶ÅÂàÜÁÇ∫ 3 È°ûÔºöÈü≥Ê°ÜÂ∞çÊáâÁãÄÊÖãÊ®ôË®ò„ÄÅÈü≥Ê°ÜÂ∞çÊáâÈü≥Á¥†Ê®ôË®òËàáÂ§öË™ûË®ÄÂèäË∑®Ë™ûË®ÄË≥áË®ä„ÄÇÂè¶‰∏ÄÂÄãÈù¢ ÂêëÂâáÊòØËá™ÂãïË™ûÈü≥Ëæ®Ë≠òÂõûÈ•ãÔºåÊàëÂÄëÊé°Áî®ÁöÑÊòØÊ®°ÂûãÂ£ìÁ∏ÆÊäÄË°ì(Model Compression) (Bucilu«é, Caruana & Niculescu-Mizil, 2006; Hinton, Vinyals & Dean, 2015)ÔºåÂæûÂ∑≤Ë®ìÁ∑¥ÂÆåÊàêÁöÑÂº∑ÂÅ•Ê®° Âûã‰∏≠ÔºåÂ∞áÁü•Ë≠òÂú®Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠ËΩâÁßªÂà∞ÂæÖË®ìÁ∑¥Ê®°Âûã„ÄÇÊé•‰∏ã‰æÜÂ∞áË©≥Á¥∞‰ªãÁ¥πÊàëÂÄë‰ΩøÁî®ÁöÑËºîÂä©‰ªªÂãôÔºö  ËûçÂêàÂ§ö‰ªªÂãôÂ≠∏ÁøíÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØËÅ≤Â≠∏Ê®°ÂûãË®ìÁ∑¥ÊñºÊúÉË≠∞Ë™ûÈü≥Ëæ®Ë≠ò‰πãÁ†îÁ©∂  91  Âúñ 3. Êú¨Ë´ñÊñá‰ΩøÁî®ÁöÑËºîÂä©‰ªªÂãô‰∏ÄË¶Ω [Figure 3. Auxiliry tasks used in this paper]  3.2.1 Èü≥Ê°ÜÂ∞çÊáâÁãÄÊÖãÊ®ôË®òÔºö  Èü≥Ê°ÜÂ∞çÊáâÁãÄÊÖãÊ®ôË®òÊòØ‰ª•È†êÊ∏¨ÁõÆÂâçÈü≥Ê°ÜÁöÑÂâç‰∏ÄÂÄãÊàñÂæå‰∏ÄÂÄãÈü≥Ê°ÜÁöÑ HMM ÁãÄÊÖãÊ®ôË®òÂÅöÁÇ∫ËºîÂä© ‰ªªÂãô„ÄÇÁî±Êñº‰ª•ÂæÄÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØËÅ≤Â≠∏Ê®°ÂûãÁöÑË®ìÁ∑¥ÊñπÂºèÔºåÈÄöÂ∏∏ÊòØ‰ª•È†êÊ∏¨ÁõÆÂâçÈü≥Ê°ÜÁöÑ HMM ÁãÄÊÖã  Ê®ôË®òÁÇ∫ÁõÆÊ®ôÔºåÈÄô‰ΩøÂæó‰∏ª‰ªªÂãôÁöÑË®ìÁ∑¥ÁõÆÊ®ô‰∏¶Ê≤íÊúâÈÑ∞ËøëÈü≥Ê°ÜÁöÑÁãÄÊÖãË≥áË®ä„ÄÇÈÄôÈ°ûËºîÂä©‰ªªÂãôÂâáÊòØ ÊúüÊúõËÉΩÂ§†Êèê‰æõÊ®°ÂûãË®ìÁ∑¥ÁöÑÈÅéÁ®ã‰∏≠ËÉΩÂ§†Âä†ÂÖ•ÈÄô‰∫õÈ°çÂ§ñË≥áË®ä„ÄÇ‰ª•È†êÊ∏¨ÈÑ∞ËøëÈü≥Ê°Ü t+1 ÁöÑÁãÄÊÖãÊ®ô  Ë®òÁÇ∫‰æãÔºåÂÅáË®≠ÁõÆÂâçÈü≥Ê°ÜË°®Á§∫ÁÇ∫ Ôºå‰∏ã‰∏ÄÂÄãÊôÇÈñìÈªûÁöÑÁãÄÊÖãÊ®ôË®òË°®Á§∫ÁÇ∫ ÔºåÂâáÂè≥ÈÇäÈü≥Ê°ÜÁãÄ  ÊÖãÊ®ôË®ò‰πãÁõÆÊ®ôÂáΩÊï∏Ë°®Á§∫ÁÇ∫Ôºö  ‚àë ln  |  (1)  3.2.2 Èü≥Ê°ÜÂ∞çÊáâÈü≥Á¥†Ê®ôË®òÔºö  ÈÄôÈ°ûËºîÂä©‰ªªÂãôÂàùÂßãÁöÑË®≠Ë®àÁêÜÂøµË∑ü Triphone Ê®°ÂûãÈ°û‰ººÔºå‰∏çÂêåÁöÑÂú∞ÊñπÂú®Êñº‰ª•ÂæÄÁî¢Áîü Triphone Ê®°ÂûãÊâÄ‰ΩøÁî®ÁöÑ Decision Tree State Tying ÊäÄË°ì(Young & Woodland, 1993)ÊòØÈùúÊÖãÁöÑ‰∫åÂÖÉË°® Á§∫Ôºå‰∏îËàá DNN ÁöÑÈö±ËóèÂ±§ÁÑ°Èóú„ÄÇËÄåËûçÂÖ•Â§ö‰ªªÂãôÂ≠∏ÁøíËÉΩÂ§†Âú®Ë®ìÁ∑¥ÁöÑÈÅéÁ®ã‰∏≠ÔºåÊèê‰æõÈÑ∞ËøëÈü≥Á¥†  ÁöÑË≥áË®äÔºåÈÅîÂà∞ÂãïÊÖãÊõ¥Êñ∞ÔºåËá™ÂãïÂΩ±ÈüøÈö±ËóèÂ±§ÁöÑÊïàÊûú„ÄÇÊàëÂÄëÈô§‰∫ÜÈÅµÂæ™ÂéüÊúâÁöÑË®≠Ë®àÁêÜÂøµÂ§ñÔºå‰πü ÈÄ≤‰∏ÄÊ≠•Âª∂‰º∏Âá∫‰∏çÂêåÁöÑËºîÂä©‰ªªÂãô„ÄÇ‰ª•È†êÊ∏¨ÁõÆÂâçÈü≥Ê°ÜÊâÄÂ±¨Èü≥Á¥†ÁÇ∫‰æãÔºåÂÅáË®≠ÊôÇÈñì t ÁöÑÈü≥Ê°ÜÊâÄÂ∞ç  ÊáâÁöÑÈü≥Á¥†Ê®ôË®òË°®Á§∫ÁÇ∫ ÔºåÈü≥Ê°ÜÂ∞çÊáâË™ûÈü≥ÁâπÂæµÂêëÈáèË°®Á§∫ÁÇ∫ ÔºåÂâáÊ≠§ËºîÂä©‰ªªÂãôÁöÑÁõÆÊ®ôÂáΩÊï∏ÂèØ  ‰ª•Ë°®Á§∫ÁÇ∫Ôºö  ‚àë ln  |  (2)  92  Ê•äÊòéÁø∞ Á≠â  
Previous studies on emotion classification mainly focus on the emotional state of the writer. By contrast, our research emphasizes emotion detection from the readers' perspective. The classification of documents into reader-emotion categories can be applied in several ways, and one of the applications is to retain only the documents that trigger desired emotions to enable users to retrieve documents that contain relevant contents and at the same time instill proper emotions. However, current information retrieval (IR) systems lack the ability to discern emotions within texts, and the detection of reader‚Äôs emotion has yet to achieve a comparable performance. Moreover, previous machine learning-based approaches generally use statistical models that are not in a human-readable form. Thereby, it is difficult to pinpoint the reason for recognition failures and understand the types of emotions that the articles inspired on their readers. In this paper, we propose a flexible emotion template-based approach (TBA) for reader-emotion detection that simulates such process in a human perceptive manner. TBA is a highly automated process that incorporates various knowledge sources to learn an emotion template from raw text that characterize an emotion and are comprehensible for humans. Generated templates are adopted to predict reader‚Äôs emotion through an alignment-based matching algorithm that allows an emotion template to be partially matched through a statistical scoring scheme. Experimental results demonstrate that our approach can effectively detect reader‚Äôs emotions by exploiting the syntactic structures and semantic associations in the context, while  ÔÄ™ Institute of Information Science, Academia Sinica, Taipei, Taiwan E-mail: {changyc, johannchu, hsu}@iis.sinica.edu.tw ÔÄ´ Department of Information Management, National Taiwan University, Taipei, Taiwan E-mail: patonchen@ntu.edu.tw  30  Yung-Chun Chang et al.  outperforming currently well-known statistical text classification methods and the stat-of-the-art reader-emotion detection method. Keywords: Reader-Emotion Detection, Emotion Template, Template-based Approach, Text Classification, Sentiment Analysis. 1. Introduction With the rapid growth of the Internet, the web has become a powerful medium for disseminating information. People can easily share information of daily experiences and their opinion anytime and anywhere on the social media, such as blogs, Twitter and Facebook. Therefore, sentiment analysis studies have gained increasing interest in recent years with academia and business corporations attempting to analyse and predict public trends by mining opinions that are subjective statements and reflect people‚Äôs sentiments or perceptions about topics (Pang et al., 2002). Moreover, human feelings can be quickly collected through emotion detection (Quan & Ren, 2009; Das & Bandyopadhyay, 2009). While previous researches on emotions mainly focused on detecting the emotions that the authors of the documents were expressing, it is worthy of noting that the reader emotions, in some aspects, differ from that of the authors and may be even more complex (Lin et al., 2008; Tang & Chen, 2012). Regarding a news article for instance, while a journalist objectively reports up-going oil price and does not express his or her emotion in the text, a reader could yield angry or negative emotions. On the other hand, an infamous politician‚Äôs blog entry describing his miserable day may not cause the opposing readers to feel the same way. While the author of an article may directly express his/her emotions through sentiment words within the text, a reader‚Äôs emotion possesses a more complex nature, as even general words can evoke different types of reader‚Äôs emotions depending on the reader‚Äôs personal experience and knowledge (Lin et al., 2007). Instead of detecting writer‚Äôs emotion, which has been already investigated extensively in previous studies (Zhang & Liu, 2010; Mukherjee & Liu, 2010; Si et al., 2013), this paper aims to uncover the emotions of readers triggered by the document. Such research holds great potential for novel applications. For instance, an enterprise that possesses the business intelligence that is capable of identifying the emotional effect that a document inflicts on its readers can provide services to retain only the documents that evokes the desired emotions, enabling users to retrieve documents with relevant contents and meanwhile being instilled the proper emotions. As a result, users benefit by obtaining opportunities and advantages in the competitive market through a more efficient and quick manner. However, current information retrieval systems lack the ability to discern emotion within texts, and reader-emotion detection has yet to achieve comparable performance (Lin et al., 2007). Machine learning-based approaches are widely used for sentiment analysis and emotion detection related researches. These approaches can usually generate accurate classifiers that assign a category label for  Linguistic Template Extraction for Recognizing Reader-Emotion  31  each document with much lower labour cost. Nevertheless, the statistical models used by these classifiers are generally not in a human-readable form. Thus, it is difficult to pinpoint the reason for recognition failures and understand what exact emotion of the reader is triggered. In light of this rationale, we proposed a flexible template-based approach (TBA) for reader-emotion detection that simulates such process in human perception. TBA is a highly automated process that integrates various types of knowledge to generate discriminative linguistic patterns for document representation. These patterns can be acknowledged as the essential knowledge for humans to understand different kinds of emotions. Furthermore, TBA recognizes reader‚Äôs emotions of documents using an alignment-based algorithm that allows an emotion template to be partially matched through a statistical scoring scheme. Our experiments demonstrate that TBA can achieve a higher performance than other well-known text categorization methods and the state-of-the-art reader-emotion detection method. The remainder of this paper is organized as follows. The next section contains a review of related works on reader-emotion detection approaches. We introduce the proposed emotion template-based approach for reader-emotion detection in Section 3, and its evaluation is described in Section 4. Finally, we provide some concluding remarks and potential future avenues of research in Section 5. 2. Related Work Textual articles are one of the most common ways for persons to convey their feelings. Identifying essential factors that affect emotion transition is important for human language understanding. With the rapid growth of computer-mediated communication applications, such as social websites and micro-blogs, the research on emotion classification has been attracting more and more attention recently from enterprises toward business intelligence (Chen et al., 2010; Purver & Battersby, 2012). In general, a single text may possess two types of emotions: writer-emotion and reader- emotion. The research of writer-emotion investigates the emotion expressed by the writer when writing the text. Pang et al. (2002) designed an algorithm to classify movie reviews into positive and negative emotions. Mishne (2005), and Yang and Chen (2006) used emoticons as tags to train SVM (Cortes & Vapnik, 1995) classifiers at the document or sentence level, respectively. In their studies, emoticons were taken as mood or emotion tags, and textual keywords were considered as features. Wu et al. (2006) proposed a sentence level emotion recognition method using dialogs as their corpus, in which ‚ÄúHappy‚Äù, ‚ÄúUnhappy‚Äù, or ‚ÄúNeutral‚Äù was assigned to each sentence as its emotion category. Yang et al. (2006) adopted Thayer‚Äôs model (1989) to classify music emotions. Each music segment can be classified into four classes of moods. As for sentiment analysis research, Read (2005) used emoticons in newsgroup articles to extract relevant instances for training polarity classifiers.  32  Yung-Chun Chang et al.  Nevertheless, the research of reader-emotion concerns the emotions expressed by a reader after reading the text. As the writer and readers may view the same text from different perspectives, they do not always share the same emotion. Since the recent increase in the popularity of Internet, certain news websites, such as Yahoo! Kimo News, incorporate the Web 2.0 technologies that allow readers to express their emotions toward news articles. Classifying emotions from the readers‚Äô point of view is a challenging task, and research on this topic is relatively sparse as compared to those considering the writers‚Äô perspective. While writer-emotion classification has been extensively studied, there are only a few studies on reader-emotion classification. Lin et al. (2007) first described the task of reader-emotion classification on news articles and classified Yahoo! News articles into 8 emotion classes (e.g. happy, angry, or depressing) from the readers‚Äô perspectives. They combined bigrams, words, metadata and word emotion categories to train a classifier for determining the reader-emotions toward news. Yang et al. (2009) automatically annotated reader-emotions on a writer-emotion corpus with a reader-emotion classifier, and studied the interactions between writers and readers with the writer-reader-emotion corpus. Our approach differs from existing reader-emotion detection approaches in a number of aspects. First, we proposed an emotion template-based approach that mimics the perceptual behaviour of humans in understanding. Second, the generated emotion templates are human readable, and can be represented as the domain knowledge required for detecting reader-emotion. Therefore, it is helpful in elucidating how articles trigger certain types of emotions in their readers in a more comprehensive manner. Finally, in addition to syntactic features, TBA further considers the surrounding context and semantic associations to efficiently recognize reader-emotions. 3. Learning Reader-emotion Template from Raw Text In this paper, we present a template-based approach (TBA) for detecting the reader-emotion of documents. We model reader-emotion detection as a classification problem, and define the reader-emotion detection task as the following. Let W= {w1, w2, ‚Ä¶,wk} be a set of words, D= {d1, d2, ‚Ä¶, dm} be a set of documents, and E= {e1, e2, ‚Ä¶ , en} be a set of reader-emotions. Each document d is a set of words such that d‚äÜW. The goal of this task is to decide the most appropriate reader-emotion ei for a document dj, although one or more emotions can be associated with a document. Our proposed method is different in that we take advantage of multiple knowledge sources, and implement an algorithm to automatically generate templates that represent discriminative patterns in documents. Our system, using the proposed method, mainly consists of three components: Crucial Element Labelling (CEL), Emotion Template Generation (ETG), and Emotion Template Matching (ETM), as shown in Figure 1. The CEL first uses prior knowledge to mark the semantic classes of words in the corpus. Then the ETG  Linguistic Template Extraction for Recognizing Reader-Emotion  33  collects frequently co-occurring elements, and generates templates for each emotion. These templates are stored in the emotion-dependent knowledge base to provide domain-specific knowledge for our emotion detection. During the detection process, an article is first labelled by the CEL as well. Subsequently, the ETM applies an alignment-based algorithm that utilizes our knowledge base to calculate the similarity between each emotion and the article to determine the main emotion of this article. Details of these components will be disclosed in the following sections.  Figure 1. Architecture of our system. 3.1 Crucial Element Labelling (CEL) TBA attempts to simulate the human perception of an emotion through the recognition of crucial elements. In this work, we capture crucial elements within documents by adopting a three-layer labelling approach that utilizes various knowledge sources, such as lexical dictionaries and Wikipedia, to induce template elements. First of all, since keywords within a reader-emotion are often considered as important information, we used the log likelihood ratio (LLR) (Manning & Sch√ºtze, 1999), an effective feature selection method, to learn a set of reader-emotion specific keywords. Given a training dataset, LLR employs Equation (1) to calculate the likelihood of the assumption that the occurrence of a word w in reader-emotion E is not random. In (1), ei denotes the specific reader‚Äôs emotion in the training dataset; N(ei) and N(¬¨ei) are the numbers of on-emotion and off-emotion documents, respectively. N(w^ei),  34  Yung-Chun Chang et al.  denoted as k, is the number of on-emotion documents containing w; the number of off-emotion  documents containing w, N(w^¬¨ei), is denoted as l. Altogether, the formula expresses the ratio of two likelihood functions. To simplify the formula, we also define m as the number of  on-emotion documents with no word w (that is, m = N(ei) - k), and n as that of off-emotion documents (n = N(¬¨ei) ‚Äì l). The probabilities p(w), p(w|ei), and p(w|^ei) are estimated using maximum likelihood estimation. A word with a large LLR value is closely associated with the  reader-emotion. We rank the words in the training dataset based on their LLR values and  select the top 200 to compile an emotion keyword list.  LLR( w, ei  )  ÔÄΩ  2  log  ÔÉ© ÔÉ™ ÔÉ™ÔÉ´  p(w  |  ei  )k  (1 ÔÄ≠  p(w | ei p(w)k  ))m p(w | ÔÉòei )l (1ÔÄ≠ ÔÄ´l (1ÔÄ≠ p(w))mÔÄ´n  p(w  |  ÔÉòei  ))n  ÔÉπ ÔÉ∫ ÔÉ∫ÔÉª  (1)  Figure 2. Architecture of named entity ontology. Next, we aim to recognize named entities (NEs) from text to facilitate document comprehension and improve the performance of identifying topics (Bashaddadh & Mohd, 2011). Our labelling algorithm uses a string matching scheme to single out the keywords (if they exist in the sequences); therefore, segmentation is not required in the preprocess. However, exact-matching NEs may overlook many template elements since it omits semantic  Linguistic Template Extraction for Recognizing Reader-Emotion  35  context. To remedy this problem, this paper adopts a novel structure to construct the NE ontology (NEO) for labelling crucial elements based on the levels of organization mentioned in (Lee et al. 2005) and (Wang et al. 2010). Figure 2 depicts the architecture of the NE ontology, which includes an emotion layer, a semantic layer, and an instance layer. There are eight types of emotions in the emotion layer, namely ‚ÄúAngry‚Äù, ‚ÄúWorried‚Äù, ‚ÄúBoring‚Äù, ‚ÄúHappy‚Äù, ‚ÄúOdd‚Äù, ‚ÄúDepressing‚Äù, ‚ÄúInformative‚Äù and ‚ÄúWarm‚Äù. Moreover, each semantic class in the semantic layer denotes a general semantic meaning of named entities that can be aggregated from many emotions, including "ÊîøÊ≤ª‰∫∫Áâ© (Politician)", "ÁñæÁóÖ (Disease)" and others. The instance layer represents 6323 named entities extracted from documents across eight emotions by the Stanford NER. In order to minimize the labour cost of instance generalization, we utilize Wikipedia to semi-automatically label NEs with their semantic classes as a way of generalization. Only NE labels for persons, places and organizations are taken into consideration, and Wikipedia‚Äôs category tags are used to label NEs recognized by the Stanford NER1. We select the category tag to which the most topic paths are associated as the main semantic label for NEs in documents. A topic path is the classification hierarchy of a certain category; it can be considered as the traversal from general categories to more specific ones. A category name with more associated topic paths is considered to be more suitable to represent a NE for its appropriate scope of semantic coverage. For example, a query ‚ÄúÊ≠êÂ∑¥È¶¨ (Obama)‚Äù to the Wikipedia would return a page titled ‚ÄúÂ∑¥ÊãâÂÖã‚Ä¢Ê≠êÂ∑¥È¶¨ (Barack Obama)‚Äù. Within this page, there are a number of category tags such as ‚ÄúÊ∞ë‰∏ªÈª® (Democratic Party)‚Äù and ‚ÄúÁæéÂúãÁ∏Ω Áµ± (Presidents of the United States)‚Äù. These two category tags contain three and seven topic paths, respectively. Suppose ‚ÄúÁæéÂúãÁ∏ΩÁµ±(Presidents of the United States)‚Äù is the one with more topic paths, our system will label ‚ÄúÂ∑¥ÊãâÂÖã‚Ä¢Ê≠êÂ∑¥È¶¨ (Barack Obama)‚Äù with the tag ‚Äú[ÁæéÂúãÁ∏Ω Áµ± (Presidents of the United States)]‚Äù. Domain experts further annotated each named entity by their corresponding semantic classes for the purpose of generalization if the NE term not included in Wikipedia to its category tags. Each instance in the instance layer can connect to multiple semantic classes according to the generalized relations. For example, named entity ‚ÄúÂñ¨‰∏π (Jordan)‚Äù can be generalized to ‚ÄúÂúãÂÆ∂ (country)‚Äù and ‚Äú‰∫∫Âêç (people)‚Äù. In this manner, we can transform plain NEs to a more general class and increase the coverage of each label. Finally, to incorporate even richer semantic context into our semantic template, we use the Extended HowNet (Chen et al., 2005), which is an extension of the HowNet (Dong et al. 2010). Extended HowNet contains a structured representation of knowledge and semantics. It connects approximately 90000 words in the CKIP Chinese Lexical Knowledge Base and HowNet, and includes additional highly frequent words that are specific in Traditional Chinese. It also contains a different formulation of each word to better fit its semantic 
Personalized language models are useful in many applications, such as personalized search and personalized recommendation. Nevertheless, it is challenging to build a personalized language model for cold start users, in which the size of the training corpus of those users is too small to create a reasonably accurate and representative model. We introduce a generalized framework to enrich the personalized language models for cold start users. The cold start problem is solved with content written by friends on social network services. Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph. The factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights. The intrinsic and extrinsic experiments show significant improvement on cold start users. Keywords: Language Model, Factor Graph, Social Network Analysis, Smoothing, Cold-Start Problem. 1. Introduction Personalized language models on social network services are useful in many aspects (Xue et al., 2009; Wen et al., 2012; Clements, 2007). For instance, if the authorship of a document is in doubt, a language model may be used as a generative model to identify it. In this sense, a language model serves as a proxy of one‚Äôs writing style. Furthermore, personalized language models can improve the quality of information retrieval and content-based recommendation 
Conventional search engines usually consider a search query corresponding only to a simple task. Nevertheless, due to the explosive growth of web usage in recent years, more and more queries are driven by complex tasks. A complex task may consist of multiple sub-tasks. To accomplish a complex task, users may need to obtain information of various task-related entities corresponding to the sub-tasks. Users usually have to issue a series of queries for each entity during searching a complex search task. For example, the complex task ‚Äútravel to Beijing‚Äù may involve several task-related entities, such as ‚Äúhotel room,‚Äù ‚Äúflight tickets,‚Äù and ‚Äúmaps‚Äù. Understanding complex tasks with task-related entities can allow a search engine to suggest integrated search results for each sub-task simultaneously. To understand and improve user behavior when searching a complex task, we propose an entity-driven complex task model (ECTM) based on exploiting microblogs and query logs. Experimental results show that our ECTM is effective in identifying the comprehensive task-related entities for a complex task and generates good quality complex task names based on the identified task-related entities. Keywords: Complex Search Task, Task Name Identification, Task-related Entity. 1. Introduction Conventional search engines usually consider single queries corresponding only to a simple search need. In reality, however, more and more queries are driven by complex search tasks (Guo & Agichtein, 2010; Jones & Klinkner, 2008). Generally, a real-life complex search task usually has more than one sub-task to be accomplished. Therefore, users usually cannot accomplish a complex search task by submitting only a single query. Some researchers have worked to try to identify sub-tasks in order to help users deal with complex search tasks (Tan  ÔÄ™ Departmenr of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan E-mail: playif@gmail.com; whlu@mail.ncku.edu.tw  70  Ting-Xuan Wang and Wen-Hsiang Lu  et al., 2006; MacKay et al., 2008; Ji et al., 2011; Kotov et al., 2011; Yamamoto et al., 2012). Nevertheless, only identifying sub-tasks in a complex search task is not sufficient to help users who want to search the complex task name directly e.g., ‚ÄúÂåó‰∫¨ÊóÖÈÅä (travel to Beijing)‚Äù. Understanding complex task names for complex search tasks can help search engines deal with complex-task-based queries. A complex search task can be represented by a task event and a task topic. For example, as shown in Figure 1, a complex task ‚Äútravel to Beijing,‚Äù which is composed of a task topic ‚ÄúBeijing‚Äù and a task event ‚Äútravel,‚Äù has at least three sub-tasks, including ‚Äúbook flight ticket,‚Äù ‚Äúreserve hotel room,‚Äù and ‚Äúsurvey maps‚Äù. Users need to issue at least three queries for each sub-task including ‚ÄúBeijing flight ticket,‚Äù ‚ÄúBeijing hotel,‚Äù and ‚ÄúBeijing map‚Äù. The queries targeting a sub-task usually focus on a task-related entity, such as ‚Äúflight ticket,‚Äù ‚Äúhotel room,‚Äù and ‚Äúmaps‚Äù. Therefore, understanding task-related entities is very important for a complex task and can help search engines provide integrated search results containing a variety of information of distinct task-related entities.  Figure 1. The structure of a complex task with task-related entities and search queries. When users search for a complex task, we have found the users often have a task event that triggers the users to perform exploratory or comparative search behaviors, such as ‚Äúprepare something,‚Äù ‚Äúbuy something,‚Äù or ‚Äútravel somewhere‚Äù. Furthermore, the search behaviors are usually around a certain task topic that is the subject of interest in the complex task. Users may describe the task event and task topic of their complex task with various task-related entities in microblogs, e.g., Twitter or Weibo1. Microblogs are a miniature version 
This study proposes a sung lyrics verification system for detecting if the lyrics sung by a performer are incorrect and further pointing out the potential mistake that the performer made. In essence, sung lyrics verification is similar to the problem of speech utterance verification in the speech recognition research community, and therefore the techniques in the letter can be applied to the former. However, our preliminary experiment found that a speech utterance verification system cannot handle singing data well, mainly because of the significant differences between singing and speech. To tackle this problem, we develop two strategies, respectively, from a signal processing perspective and from a model processing perspective. In the signal processing, recognizing that the vowels are often lengthened during singing, we propose vowel shrinking and vowel decimation to adjust the length of a vowel in singing to a normal length in speaking. In the model processing, we include a duration model concept in the acoustic modeling to reduce the differences between singing and speech. Our experiments show that the proposed methods can improve the performance of the sung lyrics verification to 72% and 90% accuracy using vowel shrinking, vowel decimation, and duration model approach, respectively, compared to 63% accuracy obtained with the baseline speech utterance verification system. ÈóúÈçµË©ûÔºöÂî±Ë©ûÁ¢∫Ë™çÔºåË™ûÂè•Á¢∫Ë™çÔºåÊØçÈü≥Â£ìÁ∏ÆÔºåÊØçÈü≥Ë£ÅÂâ™ Keywords: Singing Evaluation, Sung Lyrics Verification, Vowel Shrinking, Vowel Decimation, Duration Model ‰∏Ä„ÄÅÁ∑íË´ñ Âî±Ê≠åÊòØ‰∫∫È°ûÁöÑÂ§©Ë≥¶Ôºå‰ΩÜË¶ÅÂî±ÂæóÂ•ΩËÅΩÊàñÊúâÊäÄÂ∑ßÂâáÈúÄË¶ÅÂ∞ãÊ±ÇÁÆ°ÈÅì‰æÜÁ≤æÈÄ≤„ÄÇÈÄöÂ∏∏ÔºåÊàëÂÄëËóâÁî±Âà• ‰∫∫Âè£‰∏≠ÂæóÁü•Ëá™Â∑±Âî±Ê≠åÊòØÂê¶Â•ΩËÅΩÔºåÁîöËá≥ÊòØËÅòË´ãÊ≠åÂî±ËÄÅÂ∏´ÈÄ≤Ë°åÊåáÂ∞é„ÄÇÁÑ∂ËÄåÔºåÁ∂ìÁî±Ê≠åÂî±ËÄÅÂ∏´Êåá Â∞éÈõñËÉΩÂ§†ËÆìÂ≠∏ÁøíËÄÖ‰∫ÜËß£Ëá™Ë∫´Ê≠åÂî±ÊäÄÂ∑ß‰∏äÁöÑÁº∫Èªû‰∏¶Âä†‰ª•ÊîπÈÄ≤Ôºå‰ΩÜ‰∏¶ÈùûÊâÄÊúâ‰∫∫ÈÉΩÊúâËÉΩÂäõËÅòË´ã Â∞àÊ•≠‰∫∫Â£´‰æÜÊåáÂ∞é„ÄÇÂõ†Ê≠§ÔºåËã•Êúâ‰∏ÄÂ•óÁ≥ªÁµ±ËÉΩÂ§†Âú®‰ªª‰ΩïÊôÇÈñìÊàñÊòØ‰ªª‰ΩïÂú∞ÈªûÊèê‰æõÂ¶ÇÂ∞àÊ•≠‰∫∫Â£´Ëà¨ ÁöÑÊåáÂ∞éÔºåÊåáÂá∫‰ΩøÁî®ËÄÖÂú®Âî±Ê≠åÊôÇÊâÄÁäØÁöÑÈåØË™§ÔºåËÆìÂÖ∂ÊèêÂçáÊ≠åÂî±ÂØ¶ÂäõÔºåÂ∞áÊúÉÊòØ‰∏ÄÂ§ßÂä©Áõä„ÄÇ 36  Á∂úËßÄÁõÆÂâçÂ∏ÇÈù¢‰∏äÁöÑÂç°Êãâ OK ‰º¥Âî±Á≥ªÁµ±‰∏≠ÔºåÂÖ∑ÊúâËá™ÂãïÊ≠åÂî±Ë©ïÂàÜÂäüËÉΩÁöÑ‰∏çÂú®Â∞ëÊï∏Ôºå‰ΩÜ Â§ßÂ§öÊï∏‰ªç‰ª•Â®õÊ®ÇÊïàÊûúÁÇ∫‰∏ªÔºå‰∏¶Ê≤íÊúâÂØ¶ÈöõË©ïÂàÜÊàñÊåáÂ∞éÊïàÊûú„ÄÇÂú®Â≠∏Ë°ìÁ†îÁ©∂‰∏≠ÔºåÊúÄÂÆåÊï¥ÁöÑÂç°Êãâ OK Ê≠åÂî±Ë©ïÂàÜÁ≥ªÁµ±[1]Êé°Áî®„ÄåÈü≥È´ò„Äç„ÄÅ„ÄåÂãïÊÖãÈü≥Èáè„ÄçËàá„ÄåÂíåË´ßÂ∫¶„Äç‰∏âÈ†Ö‰æùÊìöÈÄ≤Ë°åË©ïÂàÜÔºå‰ΩÜÂçª ÂøΩÁï•‰∫Ü„ÄåÊ≠åË©û„ÄçÈÄôÈ†Ö‰æùÊìö„ÄÇÁï∂ÊºîÂî±ËÄÖÊ≤íÊúâÂî±Âú®Ê≠åÊõ≤ÁöÑÁØÄÂ•è‰∏äÔºåÊàñÊòØÂî±Êàê‰∏çÂêåÁöÑÂ≠óË©ûÔºå‰æø ÊúÉÁî¢ÁîüÂî±ÈåØÊ≠åË©ûÁöÑÊÉÖÂΩ¢„ÄÇ‰∏¶‰∏îÔºåÂú®ÁúüÂØ¶Ê≠åÂî±ÊØîË≥Ω‰∏≠ÔºåÊ≠åË©ûÂú®Ë©ïÂØ©Ë©ïÂàÜÊôÇ‰πü‰Ωî‰∫ÜÁõ∏Áï∂ÁöÑÊØî Èáç„ÄÇÂõ†Ê≠§Ôºå„ÄåÊ≠åË©û„ÄçÊòØÂÖ∂‰∏≠‰∏ÄÈ†Ö‰∏çÂèØÂøΩÁï•ÁöÑË©ïÂàÜ‰æùÊìö„ÄÇÊúâÈëëÊñºÁõÆÂâçÂ∞öÊú™Êúâ‰∫∫ÈáùÂ∞ç„ÄåÂî±Ë©û Á¢∫Ë™ç„ÄçÈÄ≤Ë°åÊé¢Ë®éÔºåÊú¨Á†îÁ©∂ÂòóË©¶Ë©ï‰º∞Ëá™ÂãïÂî±Ë©ûÁ¢∫Ë™çÁöÑÂèØË°åÊÄß„ÄÇ ‰∫å„ÄÅÊáâÁî®Ë™ûÂè•Á¢∫Ë™çÁ≥ªÁµ±ÊñºÂî±Ë©ûÁ¢∫Ë™ç ‰∏ÄÈñãÂßãÔºåÊú¨Á†îÁ©∂Âª∫Á´ã‰∫Ü‰∏ÄÂÄã‰ª•Èö±ËóèÂºèÈ¶¨ÂèØÂ§´Ê®°Âûã(Hidden Markov Model)ÁÇ∫Âü∫Á§éÁöÑ‰∏≠ÊñáË™û Âè•Á¢∫Ë™çÁ≥ªÁµ±[2]ÔºåË©ï‰º∞ÂÖ∂Áî®Êñº‰∏≠ÊñáÂî±Ë©ûÁ¢∫Ë™çÂïèÈ°åÁöÑÂèØËÉΩÊÄßËàáÊïàËÉΩ„ÄÇÊàëÂÄëÈÄèÈÅé Hidden Markov Model Toolkit (HTK) [3]‰æÜÂØ¶ÁèæË™ûÂè•Á¢∫Ë™çÁ≥ªÁµ±ÔºåÂÖ∂‰∏≠ËÅ≤Â≠∏Ê®°ÂûãÊòØ‰ª•Ê¨°Èü≥ÁØÄ (Sub-syllable)ÁÇ∫ÂñÆ‰ΩçÔºåÂÖ±‰ΩøÁî®‰∏ÄÁôæ‰∫îÂçÅ‰∏ÄÂÄãËÅ≤Â≠∏Ê®°Âûã(Âê´ÈùúÈü≥)ÔºåÊØè‰∏ÄÂÄãÊ®°ÂûãÁöÜÁÇ∫Ê∑∑ÂêàÈ´ò ÊñØÊ©üÁéáÂØÜÂ∫¶‰πãÈÄ£Á∫åÂûãÈö±ËóèÂºèÈ¶¨ÂèØÂ§´Ê®°Âûã„ÄÇËÄåÁî®‰ª•Ë®ìÁ∑¥Áî¢ÁîüË©≤Ê®°ÂûãÁöÑË™ûÈü≥Ë≥áÊñôÊòØ TCC-300 [4]„ÄÇ ËÄÉÊÖÆ‰∏≠ÊñáÂü∫Êú¨Èü≥ÁØÄÁ¥ÑÊúâ 411 ÂÄãÔºåÊàëÂÄëÂà©Áî®Ê¨°Èü≥ÁØÄÊ®°ÂûãÊãºÂá∫Ê≠§ 411 ÂÄãÈü≥ÁØÄ„ÄÇËàâ‰æã‰æÜË™™Ôºå Âúñ‰∏ÄÁÇ∫‰∏≠ÊñáÈü≥„ÄåÂ•Ω„ÄçÁöÑËÅ≤Â≠∏Ê®°ÂûãÂúñÔºåÂÆÉÂåÖÂê´Â≠êÈü≥Ê®°Âûã„Äåh_a„ÄçËàáÊØçÈü≥Ê®°Âûã„Äåau„ÄçÔºåÂÖ∂‰∏≠„Äåh_a„Äç Ê®° Âûã ‰Ωø Áî® ‰∫Ü ÂÖ© ÂÄã ÁãÄ ÊÖã Ôºå„Äå au „Äç Ê®° Âûã Ââá ‰Ωø Áî® ‰∫Ü ‰∏â ÂÄã ÁãÄ ÊÖã ‰æÜ Êèè Ëø∞ Ôºå ËÄå ÔÅª ÔÅΩ a11, a12 , a22 , a23 , a33 , a34 , a44 , a45 , a55 , a56 ÁÇ∫ÁãÄÊÖãËΩâÁßªÊ©üÁéá„ÄÇ  a11  a22  a33  a44  a55  S1  a12  S2 a23  S1  a34  S2  a45  S3  a 56  h_a  au  Âúñ‰∏Ä„ÄÅ‰∏≠ÊñáÈü≥„ÄåÂ•Ω„Äç‰πãËÅ≤Â≠∏Ê®°ÂûãÂúñ  37  Â¶ÇÂúñ‰∫åÊâÄÁ§∫ÔºåÁµ¶ÂÆö‰∏ÄÊÆµÊ≠åË©ûÂæåÔºåÊàëÂÄë‰æùÂÖ∂Ê≠åË©ûÁöÑÊ¨°Èü≥ÁØÄÁôºÈü≥‰∏≤Êé•Âá∫Ê®°ÂûãÔÅå„ÄÇÂâáÁï∂‰∏Ä  ÊÆµÊ≠åÂî±ËÅ≤Èü≥ÂèóÊ∏¨ÊôÇÔºåÁ≥ªÁµ±Â∞áÂÖ∂ÊôÇÂüüË®äËôüËΩâÊàêÁâπÂæµÂèÉÊï∏ OÔºå‰∏¶Âà©Áî®Á∂≠ÁâπÊØîÊºîÁÆóÊ≥ï(Viterbi  Algorithm)Ë®àÁÆóÁâπÂæµÂèÉÊï∏ O Áõ∏Â∞çÊñºÊ®°ÂûãÔÅåÁöÑÂ∞çÊï∏‰ººÁÑ∂Áéá ln Pr(O|ÔÅå)„ÄÇÁêÜË´ñ‰∏äÔºå‰ººÁÑ∂ÁéáË∂äÂ§ßÔºå  ‰ª£Ë°®Ë©≤Ê≠åËÅ≤ÊâÄÂî±ÁöÑÊ≠åË©ûË∂äÊ≠£Á¢∫Ôºõ‰ººÁÑ∂ÁéáË∂äÂ∞èÔºå‰ª£Ë°®Ë©≤Ê≠åËÅ≤ÊâÄÂî±ÁöÑÊ≠åË©ûË∂ä‰∏çÊ≠£Á¢∫„ÄÇ‰ΩÜÁÇ∫‰∫Ü  ÈáèÂåñÊ≠£Á¢∫ÊÄßÊàêÁÇ∫ÂèØÂà§Êñ∑ÁöÑÊï∏ÂÄºÔºåÊàëÂÄëÈúÄË¶ÅÊúâ‰∏ÄÂÄãÂü∫Ê∫ñ‰ººÁÑ∂Áéá‰æÜÂÅöÊØîËºÉÔºå‰∫¶Âç≥ÈÄ≤Ë°å‰ººÁÑ∂Áéá  ÁöÑÊ≠£Ë¶èÂåñ„ÄÇÊú¨Ë´ñÊñáÊé°Áî®È°û‰ººÊñáÁçª[5]ÊâÄË®éË´ñÁöÑÊñπÊ≥ïÔºåÈÄèÈÅéË™ûÈü≥Ëæ®Ë™çÊ≥ïÂà§Êñ∑ÂèóÊ∏¨Ê≠åËÅ≤ O ÊúÄ  ÂèØËÉΩÊòØÂî±ÁîöÈ∫ºÔºå‰æãÂ¶ÇÔÅå*ÁÇ∫Á∂≠ÁâπÊØîÊºîÁÆóÊ≥ïÊâÄÊ±ÇÂá∫‰πãÊúÄ‰Ω≥Ë∑ØÂæëÊâÄÂ∞çÊáâÁöÑÊ®°Âûã‰∏≤ÔºåÂâáÁ≥ªÁµ±Ê†πÊìö  ÊñπÁ®ãÂºè(1)ÊâÄÂæó‰πãÂàÜÊï∏Âà§Êñ∑ÂèóÊ∏¨Ê≠åËÅ≤ O ÊòØÂê¶Âî±ÈåØ  Ê≠£Á¢∫  ÂàÜÊï∏ ÔÄΩ ln Pr(O | ÔÅå) ÔÄ≠ ln Pr(O | ÔÅåÔÄ™)  ÔÄæ ÔÇ£  ÔÅ§  (1)  ‰∏çÊ≠£Á¢∫  ÂÖ∂‰∏≠ Œ¥ ÁÇ∫ÂèØË™ø‰πãËá®ÁïåÂÄº(Threshold)„ÄÇ  151ÂÄã‰∏≠ÊñáÊ¨° Èü≥ÁØÄÊ®°ÂûãÁµÑ  ÂæÖÊ∏¨ Ê≠åËÅ≤  ÁâπÂæµÊì∑Âèñ ÁâπÂæµÂèÉÊï∏  Ê®°Âûã‰ººÁÑ∂ ÁéáË®àÁÆó  ‰ººÁÑ∂Áéá Ê±∫Á≠ñ  ÊúâÂî±ÈåØ Êàñ ÁÑ°Âî±ÈåØ  Áµ¶ÂÆöÊ≠åË©û  Âúñ‰∫å„ÄÅ‰ΩøÁî®Ë™ûÂè•Á¢∫Ë™çÁ≥ªÁµ±ÈÄ≤Ë°åÂî±Ë©ûÁ¢∫Ë™ç  ‰∏â„ÄÅÈáùÂ∞çÊ≠åÂî±Ë®äËôüÁâπÊÄß‰æÜÊîπÂñÑË™ûÂè•Á¢∫Ë™çÁ≥ªÁµ± Áî±ÊñºÊ≠åÂî±ËÅ≤Èü≥Ë®äËôüÂèØ‰ª•Ë¶ñÁÇ∫Ë™ûÈü≥Ë®äËôüÁöÑ‰º∏Á∏Æ„ÄÅËÆäÂΩ¢ÈÅéÂæåÁâàÊú¨ÔºåÊàëÂÄëÁôºÁèæÂà©Áî®‰∏äËø∞Ë™ûÂè•Á¢∫ Ë™çÊñπÂºèÈÄ≤Ë°åÂî±Ë©ûÁ¢∫Ë™çÁµêÊûú‰∏¶‰∏çÂ¶ÇÁêÜÊÉ≥„ÄÇÁÇ∫Ê≠§ÔºåÊú¨Á†îÁ©∂ÂæûËÅ≤Èü≥Ë®äËôüËôïÁêÜÈÄ≤Ë°åÊîπÂñÑÂòóË©¶„ÄÇ‰∏ª Ë¶ÅÊÉ≥Ê≥ïÊòØËÄÉÊÖÆÊ≠åÂî±ÊôÇÂ∏∏Âõ†ÊØçÈü≥Ë¢´ÊãâÈï∑Ëã•Âπ≤ÂÄçËÄåÈÄ†ÊàêËàáË™™Ë©±ÊôÇÁöÑË®äËôüÁõ∏Â∑ÆÁîöÂ§öÔºåÊàëÂÄëÂõ†Ê≠§ Ë©¶ÂúñÊâæÂá∫Ê≠åÂî±‰∏≠ÁöÑÊØçÈü≥‰ΩçÁΩÆÔºå‰∏¶Â∞çÂÖ∂Èï∑Â∫¶Â£ìÁ∏ÆÊàñË£ÅÂâ™Ôºå‰ΩøÂÖ∂Êé•ËøëË™ûÈü≥Ë®äËôüÔºåËÆìË™ûÂè•Á¢∫Ë™ç ÊñπÊ≥ïËºÉËÉΩÊ≠£Â∏∏ÈÅã‰Ωú„ÄÇ ‰∏≠ÊñáÁÇ∫‰∏ÄÂ≠ó‰∏ÄÈü≥ÁØÄÁµêÊßãÔºåÊØè‰∏ÄÂÄãÈü≥ÁØÄÁöÜÁî±Â≠êÈü≥(ÂèØËÉΩ‰∏çÂåÖÂê´)„ÄÅÊØçÈü≥ËàáËÅ≤Ë™øÊâÄÁµÑÊàê„ÄÇ  38  ËÄÉÊÖÆ‰∏ÄÈ¶ñÊ≠åÊõ≤Â§ßËá¥ÂåÖÂê´Ê≠åË©ûÂíåÊóãÂæãÂÖ©ÈÉ®ÂàÜÔºåÁï∂‰æùÁÖßÊ≠åË©ûÂÖßÂÆπÈÄ≤Ë°åÊúóËÆÄÊâÄÁî¢ÁîüÁöÑËÅ≤Èü≥Ë®äËôü ÁÇ∫Ë™ûÈü≥Ë®äËôüÔºõËÄåËã•Âú®ÂêåÊ®£Ê≠åË©ûÂÖßÂÆπÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂä†ÂÖ•ÊóãÂæãÈÄ≤Ë°åÊ≠åÂî±ÔºåÊâÄÁî¢ÁîüÁöÑËÅ≤Èü≥Ë®äËôüÂç≥ ÁÇ∫Ê≠åÂî±ËÅ≤Èü≥Ë®äËôü„ÄÇËã•ËàáË™ûÈü≥Ë®äËôüÁõ∏ÊØîÔºåÊ≠åÂî±ËÅ≤Èü≥Ë®äËôüÂú®ÂêåÊ®£Ê≠åË©ûÂÖßÂÆπ‰∏äÁöÑÈï∑Â∫¶ÈÄöÂ∏∏ËºÉ Èï∑Ôºå‰∏ÄËà¨ÊòØÈÖçÂêàÊ≠åÊõ≤ÊóãÂæãÂ∞áÊ≠åË©ûÈÉ®ÂàÜÊãâÈï∑ÔºåËÄåÊãâÈï∑ÁöÑËÅ≤Èü≥ÈÉ®ÂàÜÂ§öÁÇ∫ÊØçÈü≥ÈÉ®ÂàÜ„ÄÇÂõ†Ê≠§È¶ñÂÖàÔºå ÊàëÂÄëÈúÄË¶ÅÊâæÂà∞ÊØçÈü≥ÁöÑÊâÄÂú®‰ΩçÁΩÆ„ÄÇÂúñ‰∏âÁÇ∫‰∏Ä‰∏≠ÊñáÂ≠ó‰πãÂ≠êÈü≥(Consonant)ËàáÊØçÈü≥(Vowel)ÁöÑ‰Ωç ÁΩÆÂúñ„ÄÇÂæûËÅ≤Èü≥Ë®äËôüÊ≥¢ÂΩ¢Âúñ‰∏äËßÄÂØüÔºåËÉΩÁôºÁèæÊØçÈü≥ÈÉ®ÂàÜÂÖ∑ÊúâÈÄ±ÊúüÊÄßÔºõÂèç‰πãÔºåÂ≠êÈü≥ÈÉ®ÂàÜÂâáÂ§ßÂ§ö ÁÑ°ÈÄ±ÊúüÊÄß„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂ∞ãÊâæ‰∏ÄÊÆµÊ≠åÂî±ËÅ≤Èü≥Ë®äËôüÊàñË™ûÈü≥Ë®äËôüÂÖ∑ÊúâÈÄ±ÊúüÊÄßÁöÑ‰ΩçÁΩÆÂç≥Áõ∏Áï∂ÊñºÁ≠â ÊñºÊâæÂà∞ÂÖ∂ÊØçÈü≥ÊâÄÂú®‰ΩçÁΩÆ„ÄÇ ÈÄ±ÊúüÁöÑÂÄíÊï∏ÁÇ∫È†ªÁéáÔºå‰∏ÄÊÆµËÅ≤Èü≥Ë®äËôü‰πãÈ†ªÁéáÁöÑÈ´ò‰ΩéÂ∞çÊáâÂà∞ÊôÇÂüü‰∏äÁöÑÈü≥È´ò(Pitch)„ÄÇÂõ† Ê≠§ÔºåÊàëÂÄëË®àÁÆó‰∏ÄÊÆµËÅ≤Èü≥Ë®äËôüÁöÑÈü≥È´òÂÄº‰∏¶Ë®≠ÂÆö‰∏ÄËá®ÁïåÂÄºÔºåÁï∂È´òÊñºÊ≠§Ëá®ÁïåÂÄºÂç≥Âà§ÂÆöÁÇ∫ÊØçÈü≥Ôºå ‰æøÂèØÈÅîÂà∞ÊØçÈü≥ÂÅµÊ∏¨ÁöÑÁõÆÊ®ô„ÄÇÁÇ∫Ê≠§ÔºåÊú¨Á†îÁ©∂Âà©Áî® YAAPT (Yet Another Algorithm for Pitch Tracking) [6]ÊñπÊ≥ïÈÄ≤Ë°åÈü≥È´òÁöÑËøΩËπ§Ôºå‰ª•‰æøÈÅîÂà∞ÊØçÈü≥ÁöÑÂÅµÊ∏¨„ÄÇÂúñÂõõÁÇ∫‰∏ÄÊÆµËÅ≤Èü≥Ë®äËôü‰πãÈü≥È´ò ËøΩËπ§Á§∫ÊÑèÂúñ„ÄÇ Âúñ‰∏â„ÄÅ‰∏≠ÊñáÂ≠ó„ÄåÊôÇ„Äç‰πãÂ≠êÈü≥ËàáÊØçÈü≥‰ΩçÁΩÆÂúñ 39  ÂúñÂõõ„ÄÅÊºîÂî±Ê≠åË©û„ÄåÊúâÊôÇÂÄôÔºåÊúâÊôÇÂÄô„Äç‰πãÈü≥È´òËøΩËπ§Á§∫ÊÑèÂúñ Ôºà‰∏ÄÔºâ„ÄÅÊØçÈü≥Â£ìÁ∏Æ ÊâæÂà∞ËÅ≤Èü≥Ë®äËôüÁöÑÊØçÈü≥‰ΩçÁΩÆÂæåÔºåÊàëÂÄëÂ∞áÂÖ∂Â£ìÁ∏ÆÔºå‰ΩøÂÖ∂Èï∑Â∫¶ËÉΩÂ§†Êé•Ëøë‰∏ÄËà¨ÁöÑË™ûÈü≥Èï∑Â∫¶„ÄÇÊú¨Á†î Á©∂Âà©Áî® Phase Vocoder [7][8]ÊñπÊ≥ïÔºåÈáùÂ∞çË∂ÖÈÅé‰∏ÄÂÆöÈï∑Â∫¶ÁöÑÊØçÈü≥ÈÉ®ÂàÜÈÄ≤Ë°åÂ£ìÁ∏Æ„ÄÇÂúñ‰∫îÁÇ∫ÊØçÈü≥ Â£ìÁ∏ÆÊµÅÁ®ãÂúñÔºåÊ≠åÂî±ËÅ≤Èü≥Ë®äËôüÁ∂ìÁî±ÊØçÈü≥ÂÅµÊ∏¨ÂæåÔºåÂæûËÅ≤Èü≥Ë®äËôüÁöÑËµ∑Âßã‰ΩçÁΩÆ‰æùÂ∫èË®àÁÆóÊØçÈü≥Èü≥Ê°Ü (Frame)ÁöÑÊï∏Èáè„ÄÇÁï∂ÊØçÈü≥Èü≥Ê°ÜË∂ÖÈÅéÊüêÊï∏ÈáèÊôÇ(Êú¨Á†îÁ©∂Ë®≠ÂÆöÁÇ∫ 10)ÔºåÁ≥ªÁµ±‰æøËóâÁî± Phase Vocoder Â∞áÊ≠§ÊÆµÊØçÈü≥ÈÉ®ÂàÜÈÄ≤Ë°åÂ£ìÁ∏ÆÔºåÊúÄÂæåÂæóÂà∞Â£ìÁ∏ÆÂæåÁöÑËÅ≤Èü≥Ë®äËôü„ÄÇÂúñÂÖ≠(a)ÁÇ∫‰∏ÄÊÆµÊ≠åË©ûÁöÑË™ûÈü≥Ë®ä ËôüÂúñÔºåËÄåÂúñÂÖ≠(b)ËàáÂúñÂÖ≠(c)ÁÇ∫ÂêåÊ®£‰∏ÄÊÆµÊ≠åË©û‰πãÊ≠åÂî±ËÅ≤Èü≥Â£ìÁ∏ÆÂâçÂæåÁöÑË®äËôüÂúñ„ÄÇÊàëÂÄëÂèØ‰ª•Áúã Âà∞Â£ìÁ∏ÆÂæåÁöÑÊ≠åËÅ≤Ë®äËôüÈï∑Â∫¶Êé•ËøëË™™Ë©±ÁöÑËÅ≤Èü≥Ë®äËôü„ÄÇ  ÂæÖÊ∏¨ Ê≠åËÅ≤  ÊØçÈü≥ÂÅµÊ∏¨  ÊØçÈü≥Èï∑Â∫¶ÊòØÂê¶ÈÅéÈï∑?  ÊòØ Phase Vocoder Â£ìÁ∏ÆÈï∑Â∫¶  Á∏ÆÊ∏õÈï∑Â∫¶Âæå ÁöÑÊ≠åËÅ≤  Âúñ‰∫î„ÄÅÂ£ìÁ∏ÆÊ≠åËÅ≤‰∏≠ÁöÑÊØçÈü≥  40  Ôºà‰∫åÔºâ„ÄÅÊØçÈü≥Ë£ÅÂâ™ Áî±ÊñºÊØçÈü≥ÊòØÈÄ±ÊúüÊÄßË®äËôüÔºåÂà™Èô§ÂÖ∂‰∏≠ÈÉ®ÂàÜÁöÑÈáçË§áÁâáÊÆµÂæå‰∏¶‰∏çÂΩ±ÈüøÂÖ∂ÊØçÈü≥ÁöÑÁâπÊÄßÔºåÂõ†Ê≠§ÊàëÂÄë ÂòóË©¶ÊØçÈü≥Ë£ÅÂâ™ÔºåÂ∞áÈÅéÈï∑ÁöÑÊ≠åÂî±ÊØçÈü≥Áõ¥Êé•ÂàáÁü≠Ôºå‰ΩøÂÖ∂ËºÉÂÉèË™ûÈü≥Ë®äËôüÁöÑÈï∑Â∫¶„ÄÇË£ÅÂâ™ÊñπÊ≥ïÂêåÊ®£ ÊòØÂÖàÂÅµÊ∏¨Ê≠åËÅ≤‰∏≠ÁöÑÊØçÈü≥‰ΩçÁΩÆÔºåÁÑ∂ÂæåÈáùÂ∞çÈÅéÈï∑ÁöÑÊØçÈü≥Áõ¥Êé•Ââ™ÂéªÂÖ∂ÂæåÂçäÈÉ®ÂàÜ‰∏ÄÂÆöÊØî‰æãÁöÑÈï∑ Â∫¶„ÄÇÂúñÂÖ≠(d)ÁÇ∫‰∏äËø∞ÂúñÂÖ≠(b)‰πãÊ≠åÂî±ËÅ≤Èü≥Á∂ìÁî±ÊØçÈü≥Ë£ÅÂâ™ÂæåÁöÑËÅ≤Èü≥Ë®äËôü„ÄÇ ÂúñÂÖ≠(a)„ÄÅÊ≠£Â∏∏Ë™ûÈÄüÂî∏Ê≠åË©û„ÄåÁ≠âÂà∞È¢®ÊôØÈÉΩÁúãÈÄè„Äç‰πãË™ûÈü≥Ë®äËôü ÂúñÂÖ≠(b)„ÄÅÊºîÂî±Ê≠åË©û„ÄåÁ≠âÂà∞È¢®ÊôØÈÉΩÁúãÈÄè„Äç‰πãËÅ≤Èü≥Ë®äËôü ÂúñÂÖ≠(c)„ÄÅÂ∞á(b)‰πãÊ≠åËÅ≤Á∂ìÁî±ÊØçÈü≥Â£ìÁ∏ÆÂæåÁöÑËÅ≤Èü≥Ë®äËôü ÂúñÂÖ≠(d)„ÄÅÂ∞á(b)‰πãÊ≠åËÅ≤Á∂ìÁî±ÊØçÈü≥Ë£ÅÂâ™ÂæåÁöÑËÅ≤Èü≥Ë®äËôü 41  Âõõ„ÄÅÂØ¶È©ó Ôºà‰∏ÄÔºâ„ÄÅË≥áÊñôÂ∫´ Âõ†ÁÇ∫‰∏¶Ê≤íÊúâÂÖàÂâçÁ†îÁ©∂Êé¢Ë®éÂî±Ë©ûÁ¢∫Ë™çÂïèÈ°åÔºåÊàëÂÄëÂõ†Ê≠§Ëá™Ë°åÈåÑË£ΩÊ≠åÂî±ËÅ≤Èü≥Ë≥áÊñôÂ∫´ÈÄ≤Ë°åÂØ¶È©ó„ÄÇ Êú¨Á†îÁ©∂ÈÇÄË´ã‰∫Ü‰∫î‰ΩçÂ•≥Ê≠åËÄÖËàá‰∏Ä‰ΩçÁî∑Ê≠åËÄÖÔºåÊØè‰ΩçÊ≠åËÄÖÁöÜÂú®ÂêåÊ®£‰∏ÄÂÆâÈùúÁöÑÊàøÈñìÂÖßÊ∏ÖÂî±ÂçÅ‰∫îÈ¶ñ ‰∏≠ÊñáÊµÅË°åÊ≠åÊõ≤ÔºåÂåÖÂê´Á¥ÑÂêÑÂçäÁöÑÂø´Ê≠åËàáÊÖ¢Ê≠å„ÄÇÁÑ∂ÂæåÊ®°Êì¨Âú®Âç°Êãâ OK Ê≠åÂî±Áí∞Â¢É‰∏ãÂèØËÉΩÁôºÁîüÁöÑ ÂõõÁ®ÆÂî±ÈåØË©ûÊÉÖÊ≥ÅÔºåÂàÜÂà•Ë´ãÊ≠åËÄÖÈåÑË£ΩÁõ∏ÂêåÂî±ÈåØË©ûÁöÑÊ≠åËÅ≤ÔºåÂ¶ÇË°®‰∏ÄÊâÄÁ§∫ÔºåÂõ†Ê≠§ÊØè‰ΩçÊ≠åËÄÖÂÖ±ÈåÑ Ë£Ω‰∏ÉÂçÅ‰∫îÂÄãÊ≠åÂî±Èü≥Ê™î„ÄÇ  Ë°®‰∏Ä„ÄÅÊ≠åÂî±ÊÉÖÊ≥ÅÂàóË°®  ÊÉÖÊ≥ÅÁ∑®Ëôü  ÊºîÂî±ÊñπÂºè  
From the Internet slang ‚ÄúÂ®µ‰õá·ºà‚ØôÂª†·∏Æ(You‚Äôll lose if being serious)‚Äù to a wide variety of collocations with the word ‚ÄúÂ®µ‰õá(being serious)‚Äù, the Internet users in Taiwan show a preference to the word ‚ÄúÂ®µ‰õá.‚Äù While the dictionary definitions of this word have included ‚Äúbeing practical‚Äù, ‚Äúbeing serious‚Äù, and ‚Äúnot being careless,‚Äù this study aims to bridge the gap between dictionary and authentic language use, and see how this word is differently used during online text discussion. Written Mandarin corpora Sinica Corpus and COPENS are selected as the analysis data for comparison between formal written texts and online discussion texts in terms of the negative tone in a sentence, collocations, and connotations. The results show that Taiwanese Internet users have divided opinions about whether to restrict the word to a positive connotation, or impose a negative connotation on this word. However, although the slang is less seen on the Internet, the word ‚ÄúÂ®µ‰õá‚Äù is still frequently used. This study concludes that people try to convey the message that they can take whatever they think important serious regardless of the conventions, and still hold onto a serious attitude. ÊñÑÊåùÂ®ÜÁÉâÂ®µ‰õáÁÇª‰∂Ç·∂≤„îØ‚´øÂ¶∂Â©æÁÇªCOPENSÁÇª‚èé‚≠ÇÂ©Ü„ØãÁÇª„èïÊÉµÂ®ÜÁÇª‚Ñè„µù Keywords: Â®µ‰õá, online text discussion, COPENS, negative tone, collocation, connotation 1. Introduction During daily conversations or online text discussion, people usually produce sentences starting with ‚Äú„ÜπÂ•¢‚ºø(I think)‚Äù to express their thoughts, yet people become increasingly accustomed to the insertion of ‚ÄúÂ®µ‰õá(serious, seriously, seriousness, or to be serious)‚Äù in a sentence such as ‚Äú„ÜπÂ®µ‰õáÂ•¢‚ºø(I seriously think, or seriously speaking)‚Äù to emphasize their attitude. This phenomenon of insertion does not occur alone in the abovementioned example sentence, but is widely seen during informal discussion. While people can propose their ideas in a serious manner, they show the opposite attitude with responses like the sentences ‚Äú‚∏°‚óÉÊÅãÊπ§Â®µ‰õá(Why are you so serious)‚Äù and ‚ÄúÂ®µ‰õá‚ØôÂª†·∏Æ (You will lose if being serious).‚Äù Therefore, this research aims to study the contradiction in the insertion phenomenon from the perspective of Corpus Linguistics, for corpora are capable of 53  presenting the authentic language used in real situations. 2. Literature Review 2.1 The Origin of the Internet Slang ‚ÄúÂ®µ‰õá‚ØôÂª†·∏Æ‚Äù The Internet slang ‚ÄúÂ®µ‰õá·ºà‚ØôÂª†·∏Æ(You‚Äôll lose if being serious)‚Äù is originated from a Hong Kong web novel of the same name by Su, Lin(„úøÂñØ) [1]. One of its main characters repeats this phrase when giving advice on a love tangle in the novel, and continues to explain that whoever puts true heart into a relationship is very likely to lose someday(ÂÆ©Â≠å‰õá„àΩ‚Ñç·∏Æ ‰õá‚Ω´‚ê¥‰õá„Å≠ÁÇª‚ØôÊô¶‚Éµ‚õê„ù∏·∂®‚£ë·ªÇ‚éÄ„Ü∏‰áô„Å≠‰ò¨‚£ôÂ≥ç‰æ≠) [2]. Soon afterwards, this phrase became a trend among Internet users from Hong Kong to Taiwan. Although this phrase is coined with a fixed string of words, the word ‚ÄúÂ®µ‰õá‚Äù has been extracted from this phrase, and used frequently by Internet users. 2.2 Dictionary Definitions Since the word ‚ÄúÂ®µ‰õá‚Äù is particularly popularized by Internet users, this study looks into the dictionary definitions of this word, and selects the dictionaries complied by Ministry of Education, Taiwan(R.O.C.) (·∑ïÂéó„Æπ‚ö≥„îÅÂÅöÊÇê) [3], Yuan-Liou Publisher (ÊÄà„≥©‚Ü¢‰á∞‰£¶) [4], Far East Book Co., Ltd. (ÊÄà„úô‚öæ„ö†ÂÅâ·∫•„ö±Êó∏‚Ñî‚é†) [5], National Central Library (‚ö≥‚≠û‚öæ„ö† Ê£ê) [6], and Chinese Wordnet of Academia Sinica (·∑ï‚£ñ‰ûº‰®ûÊòä·∑ï„îØÂ®Ü‚ºÅ‰µöÂ∂ó, shortened as CWN) [7] [8]. The dictionary complied by CWN is called Skyfire Dictionary (‚£ë‰Äì‚´ø‚Ñ†), which provides Mandarin-Mandarin word definitions with English equivalents to each sub-definition. The Mandarin definitions are sorted from Kang-Xi Dictionary (‚πü‰ÖÅ‚´ø‚Ñ†) and Shuo-Wen Dictionary (Â©í„îØÂ¶ã‚´ø) [9]. As for the English equivalents, the complier uses WordNet [9], a corpus established by Princeton University that ‚Äúresembles a thesaurus‚Äù, according to its website, for it ‚Äúgroups words together based on meaning‚Äù [10]. Although Skyfire Dictionary might be of less authority than the other dictionaries, it offers Mandarin-Mandarin definitions with English equivilants at the same time, which allows extra insights into why Taiwanese Internet users start blending the multiple meanings of the word ‚ÄúÂ®µ‰õá.‚Äù The following table 54  lists some of the dictionary definitions of the word ‚ÄúÂ®µ‰õá.‚Äù  Table 1. Dictionary Definitions of ‚ÄúÂ®µ‰õá‚Äù  Dictionaries (Source language) „îÅÂÅöÊÇêÊÖµ‰∂ê‚ö≥Â©Ü ÂΩï‚Ñ†·æñÂ¶™„õî (Mandarin) ÊÄà„≥©„≥£‰ìê·∑ï„îØ ‚£èÂΩï‚Ñ†(Mandarin) ÊÄà„úôÂäô„ªä„ªäÂäôÊöÅ‚éπ ÂΩï‚Ñ†(English) Âèî·π¢‰é¶·∫ã„ªäÂäôÂΩï‚Ñ† (English) ‚£ë‰Äì‚´ø‚Ñ† (Mandarin with English equivelants)  Definitions or Translations in Mandarin a. ‚ÜØ‚ÆéÂ±àÂ±î‰æ¥·∂µÊ•îÂó∂Êôê·ΩßÀ§ b. ‰îû‰õáÀ§ ·∏≥Â∂∑‚Æé·∂µÊôê·Ωß ‚ôúÂÄ≠‰ò¨ ‚ôúÂÄ≠‰ò¨; „ö±„Ç∑Â¨Ä‰ò¨; ‚øç‚ÜØ‚õò a. (conscientious) ‚ºä‚≠°„É≥‚πé ‚Æ∞‚Ω´‰æ¥‚âí‚âÉ‰ò¨À§ b. (conscientious) ‚ºä‚≠°„É≥‚πé ‚ôúÂÄ≠‰ò¨À§ c. (serious) ‚ºä‚≠°„ö±„ñ∂‰°¢À£ ‚û≠‚≠Ç‰ò¨·∑£Â•®„Ç∑Â¨Ä‰ò¨À§ d. (serious) ‚ºä‚≠°‰¥ªÂø∂‚Æë„É∂ ‰æ´„Ññ‰ò¨ÁÇª‚∫Ω·π≥Â©íÂ®ô‰æ≠‰ò¨ ‰ö≥„±ΩÀ§‚∑†„ö±À¨‚Ä¶Â©í·ºÆÀ≠À£ À¨‚Ä¶Â©íÂ¥ü·ºÆÀ≠À§ e. (serious) „à≤·πæ·π¢‰ò¨Â®ô‰îû ·ºÑ·∏≥‚Æé‰æ¥‚ôúÂÄ≠‚Æµ‚º≠À§  Definitions or Translations in English a. Being practical and responsible; not being slipshod or careless. b. To believe something is true. To do things in a practical way; not being careless. To be serious.  Serious; conscientious; in  earnest  a. To describe a concentrative  and diligent attitude.  b. To describe a serious  attitude.  c. To describe having a clear  and determined subjective  consciousness.  d. To describe having  considered something in a  careful way and introduce  the interlocutor‚Äôs opinion.  Common  collocations  include: ‚Ä¶ speaking.  e. To believe what someone  says is true and take it  seriously.  From the above table, at least two of the dictionaries mention ‚Äú ‚ÜØ ‚Æé / Â∂∑ ‚Æé (being practical)‚Äù, ‚Äú·∂µÊôê·Ωß(not being careless)‚Äù, ‚Äú‰îû‰õá/‰îû·ºÑ·∏≥‚Æé(to believe something is true)‚Äù, and ‚Äúbeing serious(‚ôúÂÄ≠)‚Äù as the sub-definitions of the word ‚ÄúÂ®µ‰õá.‚Äù What‚Äôs more, the Skyfire Dictionary offers more details in the word‚Äôs definition, especially ‚Äú‚ôúÂÄ≠(being serious)‚Äù in b. and e. On top of that, the Skyfire Dictionary and Âèî·π¢‰é¶·∫ã„ªäÂäôÂΩï‚Ñ† show how the different  55  sub-definitions of the word ‚ÄúÂ®µ‰õá‚Äù can be distinguished by the different English words, namely ‚Äúconscientious‚Äù and ‚Äúserious.‚Äù Yet, the definition of the word ‚ÄúÂ®µ‰õá‚Äù is more confusing in Mandarin because one single word of ‚ÄúÂ®µ‰õá‚Äù can suffice to mean both being ‚Äúserious‚Äù and ‚Äúconscientious.‚Äù 3. Research Questions and Hypotheses 3.1 Research Questions a) Usage: How does the usage of ‚ÄúÂ®µ‰õá‚Äù differ between formal written texts and online discussion texts? How frequent is a negative tone chosen by the speakers? b) Context: In formal written texts, what kinds of issues do people take a serious attitude toward? During online discussion, about what topics do people express their seriousness? c) When a speaker says ‚ÄúÂ§©Â®µ‰õá(to be serious)‚Äù or ‚Äú·∂µÂ§©Â®µ‰õá(not to be serious)‚Äù during online discussion, what is his or her purpose and who is the addressee? 3.2 Hypotheses 3.2.1 Usage In formal written texts, ‚ÄúÂ®µ‰õá‚Äù appears in sentences with an affirmative tone more often than those with a negative tone, while affirmative and negative tones are equally used during online discussion. 3.2.2 Context In formal written texts, the subject matters center around academic learning(‚¨†„§ï), professional career(ÂÄü„µó), and political issues(„ìß„±£Â¨òÊü¥), which are deemed important by the public. During online discussion, however, ‚ÄúÂ®µ‰õá‚Äù encompasses a wider range of topics‚Äî people can take a serious attitude toward something less commonly thought to be important by the public, yet the speakers regard the topics as important. 3.2.3 Purpose In formal situations, writers often judge themselves or others about whether they are serious enough and demand a serious attitude. In contrast, speakers during online discussion often emphasize they are serious or not instead of pointing out others‚Äô un-serious attitude, or 56  even ask others not to be so serious. During online discussion, the use of ‚ÄúÂ®µ‰õá‚Äù allows speakers to set or change the atmosphere of discussion; yet, it is less frequently needed in formal situations. 4. Methods 4.1 Corpora Since corpora are designed for storing authentic language production in a variety of settings, linguists often use them for language analysis of one corpus or comparison between two corpora or more. For this study, Academia Sinica Balanced Corpus of Modern Chinese(·∑ï ‰ûºÊòä„ªäÂ©Ü‚∏õÂ†âÂ©Ü„ïÅ‚πì, shortened as Sinica Corpus) [11] is chosen for language analysis of the word ‚ÄúÂ®µ‰õá‚Äù in formal written texts, while Corpora Open and Search(COPENS) [12] is selected for online text discussion data. Currently, Sinica Corpus contains 17,554,089 character tokens collected from more formal language materials between 1981 and 2007 [11]. As for COPENS, it is a new corpus established by Graduate Institute of Linguistics, National Taiwan University, Taiwan, and gathers materials of language used on several online discussion platforms including microblog Plurk(‚óø„¥í) and Taiwanese bulletin board system PTT(„à°Â∑äÂ∑ä) [12]. The total amount of language data has exceeded 731 million character tokens as of now. Therefore, both Sinica Corpus and COPENS are able to provide large amount of updated language materials that suit the language settings for this study. In addition to the language settings of the two corpora, both of them also sort the language materials under subcategories to yield more accurate results. Under the subcategories of COPENS, online sources, namely Plurk(‚óø„¥í) and PTT(„à°Â∑äÂ∑ä), are marked. However, all of the data in Sinica Corpus is included for language analysis in order to collect as much data as possible as the comparison group of COPENS. In both Sinica Corpus and COPENS, the word ‚ÄúÂ®µ‰õá‚Äù is searched as the keyword with parts of speech tags provided in parenthesis after each word by the compliers. The results show that the two corpora tag ‚ÄúÂ®µ‰õá‚Äù only as a (VH), which means intransitive stative verb [13]. In other words, no data of ‚ÄúÂ®µ‰õá‚Äù in these two corpora is tagged as an adjective, a noun, or other parts of speech, so the controversy of whether Chinese lacks adjectives is not a concern of this 57  study. Thus, this study leaves out the investigation of ‚ÄúÂ®µ‰õá‚Äù as an adjective and other parts of speech. 4.2 Random Selection of Concordance Lines in COPENS The searches return 575 and 57,044 results in Sinica Corpus and COPENS respectively. Among the 57,044 results in COPENS, 57,036 are concordance lines from PTT, while the other 8 concordance lines are originally posted on Plurk. Due to the large amount of data in COPENS, a selection of the data is needed for data analysis. Following and modifying Hunston‚Äôs method [14], this study selects 200 concordance lines from the 57,044 concordance lines in COPENS at the interval of 10 concordance lines, and the content of the selected data is given in Appendix 1 with the number of each concordance line provided. Unlike Huston‚Äôs method in which 30 after 30 random lines are chosen for analysis, this study expands the amount of data to 200 lines, and closely analyzes the selected data. The one-time selection of larger amount of language data allows for calculating and demonstrating the proportions of the data based on usage, context, and purpose in the discussion section. However, some of the data that represents rare yet salient linguistic characteristics of the word ‚ÄúÂ®µ‰õá‚Äù is also included in the section of discussion to present the full picture of the change in meaning of the word ‚ÄúÂ®µ‰õá.‚Äù 5. Discussion 5.1 Usage 5.1.1 The Frequency of the Word ‚ÄúÂ®µ‰õá‚Äù in Sinica Corpus and COPENS The table below calculates and compares how frequently the word ‚ÄúÂ®µ‰õá‚Äù is used in written texts and online discussion texts.  Table 2. The Frequency of the Word ‚ÄúÂ®µ‰õá‚Äù in the Sinica Corpus and COPENS  Numbers of concordance lines with the word ‚ÄúÂ®µ‰õá‚Äù Character tokens of the corpus  Sinica Corpus 575 17,554,089  COPENS  57,044  Roughly 731 millions  58  Calculation  575√ó2 √ó 17,554,089 1,000,0001  57,044 √ó 2 731,000,000 √ó 1,000,000 ~ 57,044√ó2 √ó 1,000,0001 731,999,999  Frequency of the word ‚ÄúÂ®µ‰õá‚Äù  65.5118018  156.0711354~155.8579237  (times/per million character tokens)  Date of data retrieval  2016/06/16  2016/06/16  
This paper tries to demonstrate our exploratory efforts in tackling with the ‚Äúhigh accuracy-low quantity‚Äù problem of human word sense annotation task in Chinese, and ultimately reach the goal of automatic word sense annotation. Our proposed annotation architecture consists of explicit and implicit aspects of of crowdsourcing approach. Explicit method focuses on the general issues of crowdsourcing and made adjustments on current MTurk framework. Implicit method concentrates on the idea of Game with a Purpose (GWAP) design, which originates from a well-known video game Super Mario. Keywords: WSD, Crowdsourcing, GWAP, Machine learning, Chinese Wordnet 1. Introduction Sense-aware system has become central to many NLP and related intelligent systems. The core technique involved is the Word Sense Disambiguation (WSD) which can determine the proper sense of each word in varied contexts. Current WSD models rely largely on gold standard data from manual annotation that has been suffering from the problems of high accuracy, low quantity and low efficiency. This paper aims to sketch a preliminary blueprint of (word) sense annotation service by resorting to crowdsourcing (CS) approaches tailored for the Chinese WSD task. Over the past years, crowdsourcing is an emerging collaborative way for collecting annotated corpus data and other types of language resources, with the advantages of being able to greatly increase the quantity and reduce time-cost by distribute the work to the public. Current implementations of crowdsourcing platforms include MTurks (e.g., Amazon Mechanical Turk; CrowdFlower), Game with a Purpose (GWAP) and Altruistic (or volunteer-based) crowdsourcing (e.g., Crowdcrafting). Although the explicit crowdsourcing method such as MTurks has been applied for years on several renowned platforms such as Yahoo!Answer, Quora, and so forth, several problems remain unsolved; for example, the recruitment of annotators, the annotator quality, and the design of the 83  platforms for the recruitment. Inspired by the CrowdTruth project1, we propose an internal-external adjusted framework to increase the ground-truth quality in the context of semantic annotation task. The explicit crowdsourcing has tackled with the main problems discovered in manual annotation; however, issue such expanses and interestedoriented bias still remain unsolved. Thus led to our second design, the implicit crowdsourcing-game. GWAP design for annotations is not as common as the explicit approach since it is difficult to make an annotation game ‚Äúinteresting‚Äù and collect the required data in limited time. However, we assume that the implicit approach will become the trend by collecting data from players with greater diversities, better reflect the language user distinct, and more importantly, with low cost. The design contributed by this paper shall be viewed as a pilot design and hope to attract relevant experts for further development. Following the introduction, Section 2 begins with a source review on English SENSEVAL, and Chinese Wordnet that we relied on, followed by a sense labelled annotation for test data and for our analysis of annotation problems in Section 3. We propose a crowdsourcing-based experiment design in Section 4, and a GWAP design in Section 5. And Section 6 concludes the paper. 2. Related Resources SENSEVAL [1] is the international organization devoted to the sense data distribution and evaluation of Word Sense Disambiguation Systems. We use (SENSEVAL-1) sample words as our pre-selected sample. Verbs that meet the following criteria were translated into Chinese as our examples: (1) There is no homonymy, (2), the number of polysemy is between 5 and 10, and (3) the major syntactic role of the word is verb. Another resource used in this work is the Chinese Wordnet (CWN) [2], which has been developed mainly based on the English WordNet framework: synonymous lemmata are clustered as synsets, which are interconnected with various lexical semantic relations, such as antonymy, paranymy, hypernymy-hyponymy, meronymy-holonymy, etc. CWN is used as the sense 
2. Linguistic Characteristics of Vietnamese Vietnamese is an isolating language with lexical tones and monosyllabic word structure. These characteristics are evident in all aspects: phonetic, vocabulary, and grammar. For vocabularies, Le [7] and Nguyen [11] proposed three common standards used to classify them: 1) essential meaning of the word type, 2) the function of the word in the sentence, and 3) the ability to combine with other words. Both Vietnamese and English words can be divided into content words and function words. Content words carry lexical meaning; while, function words relate lexical words to each other. For both languages, content words may be further divided into nouns, adjectives, and verbs. Nouns are words that represent entities; adjectives represent qualities or characteristics; and verbs represent actions or states. In English, most adverbs are content words, but Vietnamese adverbs are function words. Generally, these words modify any part of speech other than a noun. Adverbs can modify verbs, adjectives, clauses, sentences, and other adverbs. In this paper, we only focus on verbs and adverbs. 2.1 Vietnamese Verbs Verbs denote action, state, or occurrence, and form the main part of the predicate of a sentence. In Vietnamese, there are some types of verbs [1,10] as follows: - Intransitive verb (denotes Vin): Intransitive verbs are not used with an object; they relate only to the subject. For example: ng·ªß sleep, ng·ªìi sit, kh√≥c cry, c∆∞·ªùi smile etc. 205  - Transitive verb (denotes Vex1): Transitive verbs are action verb that have an object to receive that action. For example: l√†m do, tr·ªìng plant, x√¢y build, ph√°t tri·ªÉn develop, ƒë√†n √°p suppress, mua b√°n purchase etc. - Verb of giving and receiving (denotes Vex2): For example: cho give, g·ª≠i send, t·∫∑ng offer, bi·∫øu donate etc.; nh·∫≠n get, vay lend etc. - Verb of command (denotes Vex3): this type of verb presents activities that promote or prevent one from doing something else. For example: khuy√™n advice, b·∫Øt bu·ªôc obligatory, ƒë·ªÅ ngh·ªã suggest, ƒë√¨nh ch·ªâ suspend etc. - Verb of moving, direction (denotes Vdr). For example: v√†o in, ra out, l√™n up, xu·ªëng down, ƒë·∫øn come, l·∫°i back etc. - Modal verb (denotes Vt): is a type of verb that is used to indicate modality, that is: likelihood, ability, permission, and obligation. For example: c·∫ßn need, mu·ªën want, ∆∞·ªõc wish etc. There are some kind of modal verbs: o A need (denotes Vt1): n√™n should, ph·∫£i have to‚Ä¶ o An ability (denotes Vt2): c√≥ l·∫Ω may, c√≥ th·ªÉ can, kh√¥ng th·ªÉ cannot... o A volition (denotes Vt3): d·ª± ƒë·ªãnh intend, d√°m dare... o A wishing (denotes Vt4): hy v·ªçng hope, ∆∞·ªõc wish, m∆° dream... o A recipient, stand (denotes Vt5): ƒë·∫°t obtain, nh·∫≠n get... o A judge (denotes Vt6): cho, th·∫•y... - Verb of mentality, awareness (denotes Vin1): h·ªëi ti·∫øc regret etc. - Verb of emotion (denotes Vin2): h·∫°nh ph√∫c happy, bu·ªìn sad, gi·∫≠n angry etc. - Verb of physiology (denotes Vin3): mong want etc. - Verb of nature, morality, personality (denotes Vin4): nh·ªãn condescend, tha th·ª© forgive etc. 2.2 Vietnamese Adverbs Adverbs are words that modify or describe verbs, adjectives, clauses, sentences, and other adverbs. Generally, these words modify any part of speech other than a noun. The following observations relate to Vietnamese adverbs when comparing them with English adverbs. Morphology. English adverbs are content words but Vietnamese adverbs are function words. To the best of our knowledge, there are approximately 600 Vietnamese adverbs while English has more than 6,000 adverbs. Syntactic. In English, the adverb is the head of the phrase, can appear alone, or can be modified by other words. An adverb phrase is a subordinate clause in a sentence. In Vietnamese, adverbs do not have primary grammatical functions in a clause (subject, predicate). Function. English adverbs modify a verb, adjective, or another adverb. The adverb typically expresses the manner, time, place, cause, or circumstance in which something has happened. In 206  Vietnamese, adverbs do not have real meaning for describing the name, action, status, nature, and quantity of things. Adverbs only contain grammatical meaning based on the part of speech they modify. Position. There are three normal positions for adverbs in an English sentence: before the subject, between the subject and the verb, and at the end of the clause. Vietnamese adverbs can precede or follow the words they modify. Classification. English adverbs have the following types: time adverbs, degree adverbs, manner adverbs, frequency adverbs, and place adverbs. For Vietnamese, a selection of the types of adverbs and their ability to combine with verbs are presented in Table 1.  Table 1. Vietnamese adverbs and their ability to combine with verbs.  Types PV1 the same, similar PV2 continuation PV31 time relation (present+ future) PV32 time relation (pass) PV41 frequency (increase) PV42 frequency (decrease) PV5 degree PV6 confirmation PV7 command PH negation  Adverbs ƒë·ªÅu both c≈©ng too c√πng jointly v·∫´n still s·∫Ω will ƒëang - ing  Kinds of verbs PV1 + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) PV2 + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) PV3 + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3)  Verb phrases c√πng chu·∫©n b·ªã (prepared jointly) v·∫´n c∆∞·ªùi (still smile) Anh s·∫Ω thi r·ªõt. (He will fail the exam.)  v·ª´a just ƒë√£ -ed t·ª´ng th∆∞·ªùng usually hay often nƒÉng always √≠t rarely hi·∫øm rarely r·∫•t very, h∆°i a bit qu√° too, l·∫Øm much c·ª±c extremely c√≥ to ƒë·ª´ng don‚Äôt ch·ªõ shouldn‚Äôt kh√¥ng don‚Äôt ch∆∞a yet  PV3 + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) PV4 + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) PV4 + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) PV5 + (Vin1, Vin2, Vin3) PV6 + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) PV7 + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) PH + (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3)  Anh ·∫•y t·ª´ng thi r·ªõt. (He has failed the exam.) hay ƒÉn tr·ªÖ (often eat lately) √≠t ƒëi tr·ªÖ (rarely go late) r·∫•t y√™u (very love) c√≥ t·ªìn t·∫°i (to exist) ch·ªõ hi·ªÉu l·∫ßm (shouldn‚Äôt misconceive) kh√¥ng ƒëi (don‚Äôt go)  207  PV9 immediateness PV10 mediateness PV11 direction PV12 direction PV13 activity direction PV14 quickview PV15 ascription PV16 joint PV17 do for himself or he does it with himself PV18 describe a positive PV19 describe a negative  ngay right li·ªÅn right t·ª©c kh·∫Øc right t·ª©c th√¨ right d·∫ßn d·∫ßn gradually d·∫ßn gradually t·ª´ t·ª´ slowly v√†o into ra out l√πi back ƒëi away, v·ªÅ back t·ªõi to, qua over l·∫°i back qua through cho for v·ªõi along l·∫•y out ƒë∆∞·ª£c obtain ph·∫£i ought  (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) + PV9 (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) + PV10 Vdr + PV11 Vdr + PV12 Vdr + PV13 (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) + PV14 (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) + PV15 (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) + PV16 (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) + PV17 (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) + PV18 (Vdr, Vin1, Vin2, Vin3, Vin4, Vex1, Vex2, Vex3) + PV19  quay ngay (spin right) d·∫ßn d·∫ßn c·∫£i thi·ªán (gradually improved) c·ª© n√≥i v√†o (talk into) l·∫°i b√†n ra (talk out) mang l·∫°i (bring back) ƒê·ªçc qua (read through) Ng∆∞·ªùi ta c∆∞·ªùi cho. (people can laugh) ƒêi v·ªõi (go along) C·∫ßm l·∫•y. (Take it out) T√¥i mua ƒë∆∞·ª£c c√°i √°o ƒë·∫πp. (I acquire a nice shirt.) C√¥ ·∫•y tin ph·∫£i ng∆∞·ªùi x·∫•u. (She has trusted a bad guy.)  3. Proposed Model In this model, we try to compute the sentiment scores for word phrases that include verbs and adverbs based on Vietnamese linguistic characteristics. By combining with some adverbs, the verb phrases will have a smoother sentiment scaling. 3.1 System architecture Our system architect is presented in Figure 1. We used the English sentiment dictionary, SentiWordNet, and the translate tools Vdict* and Google Translate** to build the core verb lexicons with sentiment scores for Vietnamese. The fuzzy rules then computed the sentiment scores for the whole phrase, which included the verbs and associated adverbs.  * http://vdict.com ** https://translate.google.com/  208  SentiWordNet Core Verb Lexicons  Vdict / Google Translate Fuzzy Rules  Adverbs  Verb Phrases with polarity  Fig.1. System architecture.  Building core verb lexicons. We constructed a handcrafted opinion dictionary containing approximately 1,000 verbs. The number of words was high enough to cater to the problem we sought to solve. These words: - appeared in the review corpus obtained from [16,17]. - are matched with corresponding English words in SentiWordNet; we used Vdict and Google Translate to check this. To meet the scope of this project, we assigned opinion word scores that were the same as the scores of words in SentiWordNet. In Table 2, we describe some of the opinion words that appear in this core dictionary.  Table 2. Fragment of Core Opinion Dictionary.  Term y√™u love gh√©t hate tin t∆∞·ªüng trust k√≠nh n·ªÉ respect  Positive Score 0.375 0 0.625 0.5  Negative Score 0 0.75 0 0  POS Verb Verb Verb Verb  Tag Vin2 Vin2 Vin2 Vin2  3.2 Fuzzy Rules Overall sentiment scores for the verb phrases were calculated thanks to fuzzy rules that were associated with the combination between the verb (denotes x) and the adverb (denotes y). We used fuzzy functions to incorporate the effect of the adverbs in the verb phrases. We considered the sentiment score of a verb to be its initial fuzzy score ùúá(x). Based on Vietnamese linguistic characteristics, we realized five sentiment shifting scalings for adverbs that go along with verbs;  209  these were intensifier, booster, diminisher, minimizer, and modifier. General principles for classifying adverbs are as follows: 1. Adverbs of degree: There are five levels: intensifier, booster, diminisher, minimizer, and modifier. Some Vietnamese adverbs of degree are presented by Table 3. 2. Other adverbs: There are three levels that are booster, diminisher, and modifier: ÔÇ∑ Booster: PV1, PV2, PV31, PV41, PV6, PV9, PV10, PV13, PV16, PV17, PV18. ÔÇ∑ Diminisher: PV32, PV42, PV10, PV12, PV14, PV15. ÔÇ∑ Modifier: PH, PV19. Some of these adverbs are presented by Table 4.  Table 3. Some Vietnamese adverbs of degree with their scalings.  intensifier c·ª±c k·ª≥ extremely c·ª±c strongly si√™u super  booster r·∫•t very qu√° too l·∫Øm much  diminisher kh√° rather t∆∞∆°ng ƒë·ªëi relatively t·∫°m rather  minimizer c≈©ng seemingly h∆°i a bit r·ªìi already  modifier kh√¥ng no ch·∫≥ng no ch·∫£ no  Table 4. Some other adverbs with their scalings.  booster  diminisher  modifier  ƒë·ªÅu both v·∫´n still hay often  ph·∫£i to hi·∫øm rarely t·ª´ng already  ch·∫£ no kh√¥ng not ch∆∞a yet  In our system, Vietnamese adverbs are organized in a database. In Table 5, we describe some of the adverbs that appear in our adverb database. In the table, ‚ÄúTag‚Äù is the scaling category to which an adverb can belong.  Table 5. Some Vietnamese adverbs with their tags.  Adverbs c·ª±c k·ª≥ extremely kh√¥ng no ph·∫£i to hay often hi·∫øm rarely  Types PV5 PH PV19 PV41 PV42  Tag intensifier modifier modifier booster diminisher  Similar to Zadeh‚Äôs proposition [6,8,19], if the verb phrase had an adverb, its modified fuzzy score was computed by (1):  210  We chose ùõø = 4, 2, 1/2, or 1/4 if the adverb was a(n) intensifier, booster, diminisher, or minimizer. which gives us a modified fuzzy score, as indicated in (2).  with - ∆í(Œº(x),y) is the sentiment score of a verb phrase, in which x: verb, y: adverb. - Œº(x) is the sentiment score of a verb. Table 6 presents an example of verb phrases and their sentiment scores.  Table 6. Sentiment score of verb phrases.  f(ùõç(x),y)  ùõç(x)  intensifier  booster  diminisher minimizer  modifier  verb  c·ª±c k·ª≥ (y√™u) extremely (love) 0.85  r·∫•t (y√™u) very love 0.61  kh√° (y√™u) rather love 0.21  c≈©ng (y√™u) seemingly love 0.11  kh√¥ng (y√™u) doesn‚Äôt love 0  y√™u love 0.375  According to the formula (2), if the adverb was a modifier (y.tag = modifier), we had two cases. For example: ∆í(tin ph·∫£i trust (a bad guy)) = - ∆í(tin trust) = - 0.625, but ∆í(ƒë·ª´ng hi·ªÉu l·∫ßm shouldn‚Äôt misconceive) = 0  4. Experiments  Cohen's kappa coefficient. Two judges participated in categorizing the adverbs as intensifier,  booster, diminisher, minimizer, or modifier. To compute the ‚Äúbetween judges‚Äô agreement,‚Äù we  used the Cohen‚Äôs kappa coefficient [5], as follows:  where  ùëò = Pr(a)‚àíPr(e) 
We introduce possibilities of automatic evaluation of surface text coherence (cohesion) in texts written by learners of Czech during certified exams for non-native speakers. On the basis of a corpus analysis, we focus on finding and describing relevant distinctive features for automatic detection of A1‚ÄìC1 levels (established by CEFR ‚Äì the Common European Framework of Reference for Languages) in terms of surface text coherence. The CEFR levels are evaluated by human assessors and we try to reach this assessment automatically by using several discourse features like frequency and diversity of discourse connectives, density of discourse relations etc. We present experiments with various features using two machine learning algorithms. Our results of automatic evaluation of CEFR coherence/cohesion marks (compared to human assessment) achieved 73.2% success rate for the detection of A1‚ÄìC1 levels and 74.9% for the detection of A2‚ÄìB2 levels. 
The popularity of social networks has made them a perfect medium for activity or advertising campaign promotion. Therefore, many people use Facebook pages to announce their advertising campaign. The purpose of this study is to extract activity events by constructing two named entity recognition models, namely activity name and location, via a Web NER model generation tool [1]. We enhance the tool by improving the tokenizer and alignment technique. In addition, we also use a large database of FB checkin places for location name recognition improvement. For entity relation extraction, we apply sequential pattern mining to 229  find rules for start date, end date, and location coupling. We use 1,300 posts from Facebook to test the activity event extraction performance. The experimental results show 0.727, 0.694 F1score for activity name and location recognition; and 0.865, 0.72 F1-score for start and end date extraction. Overall, the extraction performance for activity event extraction is 0.708. ÈóúÈçµË©ûÔºöÊ¥ªÂãï‰∫ã‰ª∂Êì∑ÂèñÔºåÂëΩÂêçÂØ¶È´îËæ®Ë≠òÔºåÁ§æÁæ§Â™íÈ´î‰∫ã‰ª∂ Keywords: Activity Event Extraction, Named Entity Recognition, Social Media Event ‰∏Ä„ÄÅ Á∑íË´ñ Á∂≤ÈöõÁ∂≤Ë∑ØÁôºÂ±ïÊîπËÆä‰∫Ü‰∫∫ÂÄëÁç≤ÂæóÊ¥ªÂãïË®äÊÅØÁöÑÁøíÊÖ£ÔºåÂú®ÈÅéÂéªÊòØËóâÁî±ÈõªË¶ñ„ÄÅÂπ≥Èù¢Â™íÈ´î„ÄÅÂª£Êí≠Â™í È´îÁöÑÂÆ£ÂÇ≥‰æÜÂæóÁü•Ê¥ªÂãïË≥áË®äÔºå‰ΩÜÈö®ËëóÁ∂≤Ë∑ØÁöÑÈÄ≤Ê≠•ÂíåÁ§æÁæ§Âπ≥Âè∞ÁöÑËì¨ÂãÉÁôºÂ±ïÔºåÂïÜÂÆ∂Êúâ‰∫ÜÊñ∞ÁöÑÂÇ≥ Êí≠ÈÄîÂæëÔºå‰∏çÂÉÖÂ¢ûÂä†‰∫ÜËàáÂÆ¢Êà∂ÈñìÁöÑ‰∫íÂãïÊÄßÔºå‰πüÂèØ‰ª•Êõ¥Âø´ÂõûÊáâÂÆ¢Êà∂ÁöÑÂïèÈ°å„ÄÇ ÂæóÁü•Ê¥ªÂãïÊàñÂÑ™ÊÉ†ÁöÑË®äÊÅØÊòØÊó•Â∏∏ÁîüÊ¥ªÂèäÊóÖÈÅä‰ºëÈñíË¶èÂäÉÈáçË¶ÅÁöÑ‰∏ÄÁí∞ÔºåËã•ËÉΩÂèñÂæóËºÉÂ§öÁöÑÊ¥ªÂãïË≥á Ë®äÔºåÂ∞±ËÉΩÊñπ‰æø‰∫∫ÂÄëÊõ¥ÊúâÊïàÁöÑË¶èÂäÉ„ÄÇFacebook ÊòØÁõÆÂâçÂè∞ÁÅ£ÊúÄÊµÅË°åÁöÑÁ§æÁæ§Âπ≥Âè∞ÔºåÊØèÂ§©Êúâ‰∏Ä Âçä‰ª•‰∏äÁöÑÂè∞ÁÅ£‰∫∫Âú®‰ΩøÁî®ÊâãÊ©üÔºåÈÄôÊ®£ÁöÑÁèæË±°‰ΩøÂæó‰∏çÂ∞ëÂïÜÂÆ∂„ÄÅÊîøÂ∫ú„ÄÅÁµÑÁπîÊúÉËá™Â∑±Á∂ìÁáüÁöÑÁ≤âÁµ≤ È†ÅÔºå‰∏¶Âú®Á≤âÁµ≤È†Å‰∏äÁôº‰ΩàÊ¥ªÂãïË®äÊÅØÊàñÂïÜÂÆ∂ÂÑ™ÊÉ†ÔºåÊàëÂÄëÁôºÁèæÈÄô‰∫õË∑üÊ¥ªÂãïÊúâÈóúÁöÑË®äÊÅØÊï∏ÈáèÊúÉÊØî ‰∏Ä‰∫õÁèæÊúâÁöÑÊ¥ªÂãïÂÖ¨ÂëäÁ∂≤Á´ôÔºàÂ¶Ç CityTalk ÊàñÊ¥ªÂãïÈÄöÔºâ‰æÜÁöÑÊõ¥Â§öÂíåÂç≥ÊôÇÔºåÂõ†Ê≠§Êú¨ÊñáÁöÑÁõÆÁöÑÂç≥ ÁÇ∫Âæû Facebook Êì∑ÂèñÊ¥ªÂãïË®äÊÅØ„ÄÇ Âúñ‰∏Ä„ÄÅÊ¥ªÂãïË®äÊÅØÊì∑ÂèñÂèäÊ¥ªÂãïÂú∞ÂúñÊúçÂãôÁöÑÂëàÁèæ Êó©ÊúüÁ§æÁæ§Âπ≥Âè∞Â∞öÊú™Ëì¨ÂãÉÁôºÂ±ïÊôÇÔºåÊÉ≥Ë¶ÅÁç≤ÂèñÊ¥ªÂãïË®äÊÅØÂè™ËÉΩ‰ª∞Ë≥¥ CityTalk„ÄÅÊ¥ªÂãïÈÄöÈÄôÈ°ûÁ∂≤Á´ô Êü•Ë©¢Ê¥ªÂãï‰∫ã‰ª∂Ë≥áË®äÔºåÂê¶ÂâáÂ∞±Âè™ËÉΩËæõËã¶Âú∞ÁÄèË¶ΩÊîøÂ∫ú„ÄÅÂ≠∏Ê†°Á∂≤Á´ôÂÖ¨Âëä‰æÜÂæóÁü•‰∫ã‰ª∂„ÄÇËÄåÈÄôÊ®£ÁöÑ 230  ÁèæË±°ÂæÄÂæÄÊúÉ‰ΩøÂæóË≥áË®äÁº∫Â∞ëÊï¥ÂêàËàáÂíå‰ΩøÁî®ËÄÖÁöÑ‰∫íÂãï„ÄÇÂ¶ÇÊûúËÉΩÂ∞á‰∏çÂêåÁÆ°ÈÅìÁöÑË≥áË®äÂ¶Ç CityTalk„ÄÅ Ê¥ªÂãïÈÄö„ÄÅÊîøÂ∫ú„ÄÅÂ≠∏Ê†°Á∂≤Á´ôÂÖ¨ÂëäÂíåÁ§æÁæ§Â™íÈ´îÂÅöÁµêÂêàÔºå‰æøÂèØ‰ª•‰æùÊ¥ªÂãïÁöÑÂèóÊ≠°ËøéÁ®ãÂ∫¶ÂíåË®éË´ñÁ®ã Â∫¶ÔºåÊèê‰æõ‰∏ÄÂÄãÊ¥ªÂãïÂú∞ÂúñÊúçÂãôÔºàÂ¶ÇÂúñ‰∏ÄÊâÄÁ§∫ÔºâÔºåÂÉèÊòØÊ¥ªÂãïÁöÑË©ïË´ñ„ÄÅÊ¥ªÂãïÂâ™ÂΩ±ÂíåÊ¥ªÂãïÁöÑÂúñÁâá/ ÂΩ±ÁâáÔºåÈÄô‰∫õÂ∞çÊñº‰∫ÜËß£Ê¥ªÂãïÈÄ≤Ë°åÂíåÂèÉËàáÊúâÂæàÂ§ßÁöÑÂπ´Âä©„ÄÇÊâÄ‰ª•Êï¥ÂêàÁèæÊúâÁöÑÊ¥ªÂãïÂÖ¨ÂëäÁ∂≤Á´ôÔºå‰∏¶ ÂíåÁ§æÁæ§Â™íÈ´îÂÅöÁµêÂêàÊòØÊú¨Á†îÁ©∂ÁöÑÁõÆÊ®ô„ÄÇ Âú®Êú¨Á†îÁ©∂Áï∂‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®Êñº Facebook Ê¥ªÂãï‰∫ã‰ª∂ÁöÑÊì∑ÂèñÔºå‰∏¶ÊèêÂá∫ÊñπÊ≥ïÂæûÁ≤âÁµ≤È†ÅÁôºÊñá‰∏≠Êì∑ ÂèñÊ¥ªÂãïÂèäÂÖ∂ÈáçË¶ÅË≥áË®äÔºåÁ≥ªÁµ±Â∞áÊì∑ÂèñÂá∫‰æÜÁöÑÊ¥ªÂãï‰∫ã‰ª∂ÁµêÂêàÈõªÂ≠êÂú∞ÂúñËàáÊôÇÈñìËª∏ÔºåÂπ´Âä©‰ΩøÁî®ËÄÖ ‰∫ÜËß£Á≥ªÁµ±Êì∑ÂèñÂá∫‰æÜÁöÑÊ¥ªÂãï‰∫ã‰ª∂„ÄÇ ‰∫å„ÄÅ Áõ∏ÈóúÁ†îÁ©∂ Ê†πÊìö Sarawagi ÁöÑ Survey Â∏∏Ë¶ãÁöÑË≥áË®äÊì∑ÂèñÔºàInformation ExtractionÔºâ[2]‰ªªÂãôÂåÖÊã¨ÂØ¶È´î(Entity)„ÄÅ Èóú‰øÇ(Relation)„ÄÅÂ±¨ÊÄß(Descriptor)„ÄÅÁµêÊßã(Table, List, Ontology, etc.)Á≠âÂõõÈ°û„ÄÇËÄåË≥áË®äÊì∑ÂèñÁöÑ ÊñπÊ≥ïÊúâ Hand-coded Âèä Learning-Based ÂÖ©Á®ÆÊñπÂºèÔºåÂèØÁî¢ÁîüË¶èÂâáÂºè(Rule-Based)ÊàñÊòØÁµ±Ë®à Âºè(Statistical Model)„ÄÇ (‰∏Ä) Á§æÁæ§Â™íÈ´î‰∫ã‰ª∂Êì∑ÂèñÁ≥ªÁµ± Â∏∏Ë¶ãÁöÑ‰∫ã‰ª∂Êì∑ÂèñÂÅöÊ≥ïÔºå‰∏ªË¶ÅÊòØÂà©Áî®ÂëΩÂêçÂØ¶È´îËæ®Ë≠ò(NER)ÁöÑÊäÄË°ìÂéªË≠òÂà•ÊñáÁ´†‰∏≠Âíå‰∫ã‰ª∂Áõ∏Èóú ÁöÑÂØ¶È´îÔºå‰∏¶ÈÄ≤Ë°å association task Ë≠òÂà•Âá∫ÂØ¶È´îÈñìÈóú‰øÇÔºåÊé•ËëóÈÄèÈÅé‰∫∫Â∑•Âª∫Á´ãÁöÑË¶èÂâáÂéªÊì∑ÂèñÂÆö Áæ©ÁöÑ‰∫ã‰ª∂„ÄÇËàâ‰æãËÄåË®ÄÔºåTweets calendar Á≥ªÁµ±[3]ÁöÑÊì∑ÂèñÁöÑÁõÆÊ®ôÊòØ Twitter ‰∏äÈñãÊîæÈ†òÂüüÁöÑ‰∫ã ‰ª∂ÔºåÂÆöÁæ©‰∫ã‰ª∂ÁÇ∫(Entity, Event Phrase, Date, Type)ÔºåÂÖ∂ÁõÆÊ®ôÊòØÊì∑Âèñ‰∫∫Áâ©„ÄÅ‰∫ã‰ª∂ÂèäÊó•ÊúüÁ≠â‰∏â Á®ÆË≥áË®äÔºåÂÜçÂ∞á‰∫ã‰ª∂ÂàÜÈ°ûÔºåÁµÑÂêàÊàê‰∫ã‰ª∂ÁöÑ 4-tuple Â±¨ÊÄßÂæóÂà∞Â¶Ç:(Steve Jobs, died, 10/6/11, DEATH)ÁöÑ‰∫ã‰ª∂Ë≥áË®ä„ÄÇÂÖ∂‰ΩúÊ≥ïÊòØÊ®ôË®òÂÆåÂëΩÂêçÂØ¶È´î‰πãÂæåÔºåÂà©Áî®Âç°ÊñπÊ∏¨ÂÆö(Chi square)‰æÜÂÅöÈóú ‰øÇÁöÑÈ©óË≠âÔºå‰ª•Âº∑ÂåñÂØ¶È´îÂíåÊôÇÈñìÈóú‰øÇ‰∏¶ÂæóÂá∫Ââç 100„ÄÅ500„ÄÅ1,000 tupleÔºåÁµÑÂêàÊàêÂÆåÊï¥ÁöÑ‰∫ã‰ª∂ Èóú‰øÇ„ÄÇÊèõË®Ä‰πãÔºå‰∫ã‰ª∂ÂøÖÈ†àË¢´Áúæ‰∫∫Â§öÊ¨°ÊèêÂèäÔºåÊâçÊúâË∂≥Â§†Ë≥áË®äË≠âÊòé‰∫ã‰ª∂ÁÇ∫Áúü„ÄÇ (‰∫å) Êñ∞ËÅûÁöÑ‰∫ã‰ª∂Êì∑ÂèñÁ≥ªÁµ± Âè¶Â§ñÔºåWang[4]ÂâáÊèêÂá∫‰∏ÄÂÄãÊáâÁî®ÊñºÊñ∞ËÅûÂ™íÈ´î‰∏äÁöÑ‰∫ã‰ª∂Êì∑ÂèñÁ≥ªÁµ±ÔºåÂÖ∂ÁõÆÁöÑÊòØ 5W1H ÁöÑË™ûÊÑè Â±§Á¥öÁöÑÂÖÉÁ¥†Êì∑ÂèñÁ≥ªÁµ±ÔºåÊñ∞ËÅû‰∫ã‰ª∂ÁöÑ 5W1H ÂÖÉÁ¥†‰∫ã‰ª∂Â±¨ÊÄßÂÆöÁæ©Â¶ÇË°®‰∏Ä„ÄÇÊñπÊ≥ïÊòØË®≠Ë®à‰∏Ä‰∫õ ÁâπÂæµ(feature)ÂæûÊñ∞ËÅûÊ®ôÈ°å‰∏≠ÊâæÂá∫Êñ∞ËÅû‰∏≠ÁöÑ‰∏ªÈ°åÂè•ÔºåÊé•ËëóÈÄèÈÅéË™ûÁæ©ËßíËâ≤Ê®ôË®ªÔºåÊúÄÂæåÂ∞áÊì∑Âèñ 231  Âá∫‰æÜÁöÑÂÖÉÁ¥†Â°´ÂÖ•Âà∞ News Ontology Event model ‰æõÂæåÁ∫åÁöÑÂà©Áî®„ÄÇ N. Kanhabua[5]ÊèêÂá∫ÁöÑÁ†îÁ©∂ÊòØË∑üÂÖ¨ÂÖ±Ë°õÁîü‰∫ã‰ª∂Áõ∏ÈóúÁöÑ‰∫ã‰ª∂Êì∑ÂèñÔºå‰∏ªÈ°åÊòØÁñæÁóÖÁàÜÁôºÁöÑ‰∫ã‰ª∂ Êì∑ÂèñÁ≥ªÁµ±ÔºåÁñæÁóÖÁàÜÁôºÁöÑ‰∫ã‰ª∂ÂÆöÁæ©Â¶ÇË°®‰∏ÄÔºåÂåÖÊã¨ÂèóÂÆ≥‰∫∫„ÄÅÁñæÁóÖ„ÄÅÊôÇÈñìÂèäÂú∞ÈªûÔºå‰∏ªË¶ÅË¶ÅËß£Ê±∫  ÁöÑÂïèÈ°åÊòØÊâæÂà∞ÁñæÁóÖÂíåÁñæÁóÖÈáçË¶ÅÁöÑÊôÇÁ®ãË°®ÈÅîÂºèÔºåÂÆöÁæ©ÈÄôÊ®£ÁöÑÂïèÈ°åÁÇ∫ÂàÜÈ°ûÂïèÈ°åÔºå‰∏¶ÊèêÂá∫‰∫ÜÁõ∏  ÈóúÊéíÂ∫è(relevance ranking) ÊñπÊ≥ïÂéªÂà§Âà•ÈáçË¶ÅÁöÑÊôÇÁ®ãË°®ÈÅîÂºè„ÄÇÂú®‰∏çÂêåÂàÜÈ°ûÊñπÊ≥ï‰∏≠ÊúÄÂ•ΩÁöÑÊïà ÊûúÊòØÊé°Áî® J48ÔºåÊ∫ñÁ¢∫Áéá(accuracy)ËÉΩÈÅîÂà∞ 0.65„ÄÇ  Ë°®‰∏Ä„ÄÅÊñ∞ËÅû‰∫ã‰ª∂ÁöÑ 5W1H ÂÖÉÁ¥†‰∫ã‰ª∂Â±¨ÊÄßË™™ÊòéÂíåÁñæÁóÖÁàÜÁôºÁöÑ‰∫ã‰ª∂ÂÆöÁæ©  5W1H What When Where Who Whom How  News 5W1H Event  Disease Event e: (v, m, l, t)  ÊäµÈÅî  disease m  8Êó•  time t  Ê∏•Âè∞ËèØ Âä†ÊãøÂ§ß  location l  ‰∏≠ÂúãÂúãÂÆ∂‰∏ªÂ∏≠ËÉ°Èå¶Êø§  victim v  Âä†ÊãøÂ§ßÈ¶ñÈÉΩÊ∏•Âè∞ËèØ  ‰∏≠ÂúãÂúãÂÆ∂‰∏ªÂ∏≠ËÉ°Èå¶Êø§ÊäµÈÅîÂä†ÊãøÂ§ßÈ¶ñÈÉΩÊ∏•Â§™ËèØÈÄ≤Ë°åËªç‰∫ãË®™Âïè  ‰∏â„ÄÅ Á≥ªÁµ±Êû∂Êßã  Êú¨Á†îÁ©∂ÁöÑÁ≥ªÁµ±Êû∂ÊßãÂ¶ÇÂúñ‰∫åÔºåÁ≥ªÁµ±È¶ñÂÖàÈÄ≤Ë°åË≥áÊñôÁöÑËíêÈõÜÔºåÂåÖÊã¨ CityTalk Âíå FB ÁöÑÁôºÊñáÔºåÊé• ËëóÈÄèÈÅéÊ¥ªÂãïÁõ∏ÈóúÁöÑÈóúÈçµÂ≠óÂèñÂæóË∑üÊ¥ªÂãïËºÉÁÇ∫Áõ∏ÈóúÁöÑÁôºÊñáÔºå‰∏¶Âà©Áî®Ê¥ªÂãïÂêçÁ®±Ëæ®Ë≠òÁöÑÊ®°ÂûãÈÄ≤Ë°å Ê®ôË®òÔºåÂè™ÊúâÂåÖÂê´Ê¥ªÂãïÂêçÁ®±ÁöÑË≤ºÊñáÊâçÊúÉÂà©Áî®ÊôÇÁ®ãË°®ÈÅîÂºè„ÄÅÂú∞Èªû„ÄÅÂú∞ÂùÄËæ®Ë≠òÁöÑÊ®°ÂûãÈÄ≤Ë°åÊ®ôË®ò„ÄÇ ÊúÄÂæåÂà©Áî®‰∫ã‰ª∂Èóú‰øÇËÄ¶ÂêàÁöÑÊ®°ÁµÑÂ∞áÁôºÊñáÁöÑÊ¥ªÂãï‰∫ã‰ª∂ÁöÑÈóú‰øÇÊâæÂá∫‰æÜÔºå‰∏¶ÊîæÂà∞‰∫ã‰ª∂ÁöÑË≥áÊñôÂ∫´Ôºå ‰∏¶Êèê‰æõ‰ªãÈù¢Áµ¶‰ΩøÁî®ËÄÖÊü•ÁúãÊì∑ÂèñÂá∫‰æÜÁöÑÊ¥ªÂãïË≥áË®ä„ÄÇ  (‰∏Ä) Ê¥ªÂãï‰∫ã‰ª∂ÂÆöÁæ©  Âúñ‰∫å„ÄÅÁ≥ªÁµ±Êû∂ÊßãÂúñ  Ê¥ªÂãï‰∫ã‰ª∂ÁöÑÂÆöÁæ©ÂèØÁî±Ê¥ªÂãïÂêçÁ®±„ÄÅÈñãÂßã„ÄÅÁµêÊùüÊôÇÈñì„ÄÅÂú∞ÈªûÔºàÊàñÂú∞ÂùÄÔºâÂõõÂÄãÂü∫Êú¨ÂÖÉÁ¥†ÁµÑÊàêÔºå Áî±Êñº FB Â§ßÈÉ®ÂàÜÊèêÂèä‰∫ã‰ª∂ÁöÑÁôºÊñáÈÉΩÂè™ÊèêÂà∞ÂñÆ‰∏Ä‰∫ã‰ª∂ÔºåÂõ†Ê≠§Êú¨Á†îÁ©∂Ê¥ªÂãï‰∫ã‰ª∂Êì∑Âèñ‰ªªÂãôÂç≥Âæû ÊØèÁØáË≤ºÊñá‰∏≠ÂÖàË°åËæ®Ë≠òÊ¥ªÂãïÂêçÁ®±ÔºåÂÜçÊì∑ÂèñÊ¥ªÂãïÁöÑÊó•ÊúüÂèäÂú∞Èªû„ÄÇ‰ª•Âúñ‰∏âË≤ºÊñáÁÇ∫‰æãÔºåË°®‰∫åÂç≥ÁÇ∫  232  Ê¥ªÂãï‰∫ã‰ª∂Êì∑ÂèñÁöÑËº∏Âá∫„ÄÇ  Ë°®‰∫å„ÄÅÊ¥ªÂãï‰∫ã‰ª∂ÁöÑÈóú‰øÇ  Activity Name  Start Date End Date  Location/Address  „Äå‰∏ñÁïåÊñáÂåñÈÅ∫Áî¢ÈáçÊÖ∂Â§ßË∂≥Áü≥ ÊòéÊôö  3 Êúà 13 Êó• ÂÜ¨Â±±Ê≤≥Ë¶™Ê∞¥ÂÖ¨Âúí  ÂàªÂΩ©ÁáàÊö®ÁâΩÊâãÂòâÂπ¥ËèØ„Äç (2016-02-06) (2016-03-13)  (‰∫å) Ë≥áÊñôÊî∂ÈõÜ  Âúñ‰∏â„ÄÅ‰∫ã‰ª∂Êì∑ÂèñÁØÑ‰æãË™™Êòé  Âú®Ë≥áÊñôÊî∂ÈõÜÊñπÈù¢ÔºåÁ≥ªÁµ±‰∏ªË¶ÅÊúÉÊî∂ÈõÜ FB„ÄÅCityTalk ÂèäÊêúÂ∞ãÂºïÊìéÁöÑÊ¥ªÂãïÈóúÈçµÂ≠óÊü•Ë©¢ÁµêÊûú„ÄÇ FB Á∂≤Á´ô‰∏äÁöÑË≥áË®äÔºåÂåÖÊã¨ÊâìÂç°Âú∞Èªû„ÄÅÂè∞ÁÅ£ÂÖ¨ÈñãÁ≤âÁµ≤Â∞àÈ†Å„ÄÅÂíå FB ‰∫ã‰ª∂ÁöÑÁôºÊñáÔºåÂÖ∂‰∏≠ÊâìÂç°Âú∞ ÈªûÂåÖÊã¨ 245 Ëê¨ Places„ÄÇÁî±Êñº FB Ê≤íÊúâÂÉè Twitter Êèê‰æõÂ∞çÊï¥ÂÄã FB Á∂≤Á´ôÁôºÊñáÁöÑÊêúÂ∞ã APIÔºåÂõ† Ê≠§ÊàëÂÄëÂè™ËÉΩÂà©Áî® FB Graph API Â∞ç 22 Ëê¨ÂÄãÂè∞ÁÅ£ÂÖ¨ÈñãÁ≤âÁµ≤Â∞àÈ†Å(ÈÄèÈÅéËß£Êûê 1,400 Ëê¨Á≠Ü FB Object id ÊâÄÂæó)ÔºåÂàÜÂà•Áõ£ËÅΩÈóúÊ≥®ÁöÑÁ≤âÁµ≤Á∂≤È†ÅÂèñÂæóÁôºÊñáË≥áÊñô„ÄÇÂú® 2015/9~2016/8 ÊúàÈñìÁôºÊñáËíê ÈõÜÊ®°ÁµÑÂÖ±Êî∂ÈõÜÁ≤âÁµ≤È†ÅÁöÑÁôºÊñá 2,947 Ëê¨ÁØá‰ª•ÂèäÂíåË©≤ÁôºÊñáÁöÑÂâç 100 ÁØáÂõûÊáâ„ÄÇÂè¶Â§ñÁ≥ªÁµ±‰πüÊî∂ÈõÜ FB ‰∫ã‰ª∂ÁöÑÁôºÊñáÔºå‰∏¶Âà©Áî®Áà¨Ëü≤Á®ãÂºèÊäìÂèñ CityTalk Á∂≤Á´ô‰∏äÁöÑÊ¥ªÂãï‰∫ã‰ª∂„ÄÇ  (‰∏â) Ê¥ªÂãïÂêçÁ®±ÂØ¶È´îÁöÑË≠òÂà•  Ê¥ªÂãïÂêçÁ®±ÁöÑÊ®°ÂûãÊòØÊ†πÊìö CityTalk Á∂≤Á´ôËíêÈõÜÂõû‰æÜÁöÑÊ¥ªÂãïÂêçÁ®±ÂÅöÊü•Ë©¢Ë©ûÔºåÂ∞çÊêúÂ∞ãÂºïÊìéË©¢ÂïèÁµê ÊûúÔºå‰∏¶Á∂ìÁî±Ëá™ÂãïÊ®ôË®òÂæóÂà∞Ë®ìÁ∑¥ÊñáÊú¨ÔºåÊ¥ªÂãïÂêçÁ®±Â±¨ÊñºÈï∑ÂëΩÂêçÂØ¶È´îÔºåÊâÄ‰ª•Ê≠ßÁæ©ÊÄßÁ≠âÂïèÈ°åÁôºÁîü Ê©üÊúÉÊØîËºÉÂ∞ëÔºåÂõ†Ê≠§ÊàëÂÄëÂèØ‰ª•ÈÄ≤Ë°åËá™ÂãïÊ®ôË®òÁç≤ÂæóÂ§ßÈáèÊ®ôË®òÁöÑË®ìÁ∑¥ÊñáÊú¨„ÄÇÂÆåÊï¥Ë®ìÁ∑¥ÈÅéÁ®ãÂíå‰Ωø Áî® Huang[1]ÁöÑÊ®°ÂûãÂíåÊîπÈÄ≤ÈÉ®ÂàÜÂèØ‰ª•ÂèÉËÄÉ‰∏ãÂúñÂõõÔºåË®ìÁ∑¥Ê®°ÂûãÁöÑÈÉ®ÂàÜÈ¶ñÂÖàÂ∞áÊàëÂÄëÁà¨Ëü≤Á®ãÂºè ÊíàÂõû‰æÜÁöÑÊñáÊú¨ÈÄèÈÅé tokenization Ê®°ÁµÑÂ∞á token ÂÅöËºÉÂ•ΩÁöÑÂàáÂâ≤ÔºåÊé•ËëóÈÄèÈÅéËá™ÂãïÊ®ôË®òÊ®°ÁµÑËá™ ÂãïÊ®ôË®òÂØ¶È´îÔºåÊ®ôË®òÂá∫ÂØ¶È´îÁöÑÊñáÊú¨ÊàëÂÄë‰∏çÊúÉÊï¥ÊÆµ‰ΩøÁî®ËÄåÊòØÈÄèÈÅé String split Ê®°ÁµÑÂè™Áïô entity ÂâçÂæåÂõ∫ÂÆöÈï∑Â∫¶ÁöÑÁØÑÂúçÔºåÊé•ËëóÈÄèÈÅé Feature Mining Ê®°ÁµÑÂæóÂà∞Â≠óÂÖ∏Ê™îÔºåÈÄèÈÅéÈÄô‰∫õÂ≠óÂÖ∏Ê™îÁ∂ìÁî± Generate Feature Matrix Ê®°ÁµÑÁî¢Áîü CRF ++Ë®ìÁ∑¥Ê†ºÂºè‰∏¶Ë®ìÁ∑¥Áî¢Áîü CRF Model„ÄÇÂú®‰ªªÂãô‰∏≠Êîπ ÂñÑ Huang[1]ÁöÑÊéíÊØîÔºàAlignmentÔºâÊñπÊ≥ïÔºåÂä†ÂÖ•ÂèØËá™Ë®ÇÁæ©ÁöÑ tokenizer Ê®°ÁµÑÔºåÂÖÅË®±Â∞ç token ÂÅö  233  ÂéªË©ûÂππË®≠ÂÆö„ÄÇÊéíÊØîÊñπÂºèÊòØÂèÉËÄÉ T.-S. Chen[6]ÊèêÂá∫ÁöÑ Global alignmentÔºåÈáùÂ∞çÈï∑Â∫¶Â§ßÊñº k ÁöÑ Ê¥ªÂãïÂêçÁ®±ÊéíÊØîÊ®ôË®òÊ¢ù‰ª∂ÂåÖÊã¨(1)‰∏çÂÖÅË®±ÂÖ©ÊéíÊØîÂ∫èÂàó‰∏≠ÁöÑÂ≠óÂÖÉ mismatch ÁöÑÂ∞çÊáâÔºå(2)ÂÖ©Áõ∏ÈÑ∞ matched token ‰πãÈñìÂá∫Áèæ Gap Êï∏Ëá≥Â§öÁÇ∫ MaxGÔºå‰∏î(3)ÈáçË¶ÜÁöÑ token ÊØî‰æãÂøÖÈ†àÂ§ßÊñºÈñÄÊ™ªÂÄº rÔºåÊªøË∂≥‰ª•‰∏ä‰∏âËÄÖÊ¢ù‰ª∂ÁöÑÊØîÂ∞çÁ≥ªÁµ±Âç≥Â∞áÂÖ∂Ê®ôË®òÁÇ∫Âá∫ÁèæÁØÑ‰æã„ÄÇ ÂúñÂõõ„ÄÅÊ¥ªÂãïÂêçÁ®±Ëæ®Ë≠òÊ®°ÂûãÂª∫Á´ãÂíåÊõ¥Êñ∞ Huang Â∑•ÂÖ∑ÁöÑË™™Êòé (Âõõ) Â∑•ÂÖ∑ÁöÑÊì¥ÂÖÖ Â∑•ÂÖ∑ÁöÑÊì¥ÂÖÖÊòØÁÇ∫‰∫ÜÊîπÈÄ≤ÂéüÂÖà Huang[1]Â∑•ÂÖ∑Âú® FB ÊñáÊú¨ÊïàËÉΩ‰∏çÂ•ΩÁöÑÂïèÈ°åÔºåÈô§‰∫Ü‰∏äËø∞ÊîπÂñÑ Èï∑ÂØ¶È´î Uni-Labeling Ê®°ÁµÑÊ®ôË®òÁöÑÊ∫ñÁ¢∫ÊÄß‰πãÂ§ñÔºå‰∏¶Êñ∞Â¢ûÈï∑ÂØ¶È´îÊéíÊØî Full-Labeling Ê®°ÁµÑ„ÄÇËºÉ Â§ß ÁöÑ Êîπ ËÆä ÊòØ Êì¥ ÂÖÖ Ê°Ü Êû∂ ÁöÑ ÂèØ Êì¥ Â±ï ÊÄß (scalability) Ôºå Êñ∞ Â¢û (1) ‰∏ç Âêå Ë≥á Êñô ‰æÜ Ê∫ê ÁöÑ Â§ö Á∑ö Á®ã (multithreading)Ê®ôË®òÂíåÊ®ôË®òÁµêÊûúÁöÑÂÄâÂÑ≤„ÄÅ(2)ÊîØÊè¥ word-based ÊñπÊ≥ïÁöÑÊ®ôË®òÔºàÂ¶ÇË°®‰∏âÔºâ„ÄÅ(3) Êñ∑Âè•Ê®°ÁµÑÔºåÂíå(4)Êì∑ÂèñÂá∫ÁöÑÂØ¶È´îÁ≤æÊ∫ñÂ∫¶„ÄÇ ÁÇ∫‰∫ÜÊï¥‰Ωµ word-based Âíå character-based ÊñπÊ≥ï‰∏¶Êèê‰æõÊõ¥Ë±êÂØåÂâçËôïÁêÜÂäüËÉΩÊñ∞Â¢û Tokenizer Ê®°ÁµÑÔºåÂØ¶‰ΩúÈÄèÈÅéÁßªÊ§ç‰∫Ü Lucene ÁöÑ AnalyzerÔºåÂèØËá™Ë°åÊõøÊèõÊ†πÊìö‰ªªÂãôÊâÄÈúÄ‰∏¶Ëá™Ë°åË®≠ÂÆöË©ûÂππ Ë®≠ÂÆö„ÄÇÁï∂Êé°Áî® character based ÊôÇÔºåÊàëÂÄë‰ΩøÁî® Jflex ÈÄôÂ•óÂ∑•ÂÖ∑ÂÆöÁæ©Â∏∏Ë¶ãÁöÑ token ÂûãÊÖã(ÂåÖÊã¨ money„ÄÅalphanum„ÄÅChinese or Japan„ÄÅURL„ÄÅE-mail Á≠âÂÖ± 20 ÂÄã)ÔºåËÄåÁï∂Êé°Áî® word based ÊôÇ ÂâáÈ†êË®≠‰ΩøÁî® IK AnalyzerÔºå‰∏çÈÅéÁî±Êñº IK Analyzer ÊúÉÊøæÈô§Êú™ÂÆöÁæ©ÁöÑ tokenÔºåÈÄ†Êàê token ÈÅ∫ Â§±ÔºåÂõ†Ê≠§ÊàëÂÄëÊîπÂØ´ÈÉ®ÂàÜÁ®ãÂºèÁßªÈô§ÈÄôÊ®£ÁöÑË®≠ÂÆöÔºå‰∏¶Ê∑ªÂä†ÊáâÁî®Â∫ñ‰∏ÅÊñ∑Ë©ûÂíå MMSeg Ëá™ÂÆöÁæ©ÁöÑ Ë©ûÂ∫´ÔºåÂ¶ÇÊûúË¶Å‰ΩøÁî®ÂÖ∂ÂÆÉ‰∏≠ÊñáÊñ∑Ë©ûÔºåÂèØËá™Ë°åÂ∞ÅË£ùÊñ∑Ë©û‰æÜÂèñ‰ª£È†êË®≠„ÄÇÊ®ôË®òÁ≤æÊ∫ñÂ∫¶ÊòØÊåáË≠òÂà•Âá∫ 234  ÁöÑÂØ¶È´îÂíåÂéüÊñáÊòØ‰∏ÄËá¥ÁöÑ„ÄÇÊàëÂÄëË¶Å‰øùË≠âÊâÄÊúâÂ∞çÊñáÊú¨ÂÅöÁöÑÂâçËôïÁêÜÈÉΩ‰∏çÊúÉÂΩ±ÈüøË≠òÂà•Âá∫ÁöÑÂØ¶È´î„ÄÇ  Ë°®‰∏â„ÄÅ‰∏çÂêåÊñπÊ≥ï tokenizer Ëá™ÂãïÁî¢ÁîüÁöÑÊ¥ªÂãïÂêçÁ®±Ëæ®Ë≠òÁâπÂæµÂÄºÁØÑ‰æã  ID  Ë™™Êòé  Èï∑ character based  word based  ‚Ä¶  ‚Ä¶  ‚Ä¶  ‚Ä¶  ‚Ä¶  10 Â∏∏Ë¶ãÊñºÊ¥ªÂãïÂêçÁ®±ÂâçÊñπÁöÑ token 1  Âà∞„ÄÅÂú®  ËàâËæ¶„ÄÅÂèÉÂä†  11 Â∏∏Ë¶ãÊñºÊ¥ªÂãïÂêçÁ®±ÂâçÊñπÁöÑ token 2  ËàâËæ¶„ÄÅÊé®Âá∫  Êé®Âá∫/„Äå„ÄÅ‰∏ÄÂπ¥‰∏ÄÂ∫¶/ÁöÑ  12 Â∏∏Ë¶ãÊñºÊ¥ªÂãïÂêçÁ®±ÂâçÊñπÁöÑ token 3  Ê¥ªÂãïÔºö„ÄÅÂèÉÂä†„Äå  Ê¥ªÂãï/ÂêçÁ®±/:„ÄÅÁØÄÁõÆ/ÂêçÁ®±/:  13 Â∏∏Ë¶ãÊñºÊ¥ªÂãïÂêçÁ®±ÂæåÊñπÁöÑ token 1  „Äë„ÄÅ„Äç  Âç≥Êó•Ëµ∑„ÄÅÈñãÂπïÂºè  14 Â∏∏Ë¶ãÊñºÊ¥ªÂãïÂêçÁ®±ÂæåÊñπÁöÑ token 2  Ëµ∑Ë∑ë„ÄÅÊú¨Ê¨°  Ë°®Êºî/Ê¥ªÂãï„ÄÅÊòéÊó•/ÁôªÂ†¥  15 Â∏∏Ë¶ãÊñºÊ¥ªÂãïÂêçÁ®±ÂæåÊñπÁöÑ token 3  ‰æÜÂõâ~  ÁÜ±ÁÉà/ÈñãË∑ë/Âõâ„ÄÅ /ÈñãÂπï/ÁõõÊ≥Å  ‚Ä¶  ‚Ä¶  ‚Ä¶  ‚Ä¶  ‚Ä¶  (‰∫î) Âú∞ÈªûÂØ¶È´îÁöÑË≠òÂà•  ÈåØË™§Ê®ôË®òÁöÑÂú∞ÈªûÊúÉÂΩ±ÈüøÂà§ÂÆöÊ¥ªÂãïÂú∞ÈªûÔºåÂ∞çÊñºÂú∞ÈªûËæ®Ë≠òÊ®°ÂûãÊàëÂÄëÊõ¥ÁúãÈáçÁ≤æÁ¢∫Áéá(precision) ÂíåÂØ¶È´îÈÇäÁïåÔºåÁ≤æÁ¢∫ÁéáÊòØ‰∏çË¶ÅÈåØË™§Âú∞Ë≠òÂà•‰∏Ä‰∫õÂú∞ÈªûÔºåÊé° CRF ÊñπÊ≥ïÂè¨ÂõûÁéáÂæàÈ´ò‰ΩÜË≠òÂà•ÈåØË™§ ‰æãÂ≠ê‰πüÂæàÂ§ö‰æãÂ¶Ç:‚ÄúÂÖçË≤ª‚ÄùË≠òÂà•ÊàêÂú∞ÈªûÔºåÂè¶Â§ñË≠òÂà•Âá∫‰æÜÁöÑÂú∞ÈªûÊúâÊôÇÂÄôÈÇäÁïå‰∏çÊòØÈÇ£È∫ºÂÆåÊï¥ Â¶Ç: ‚Äú‰∏âÁÅ£ÈÑâ‰∫îÁ©ÄÂªüÂâçÂª£Â†¥ÔºÇË¢´Ë≠òÂà•ÁÇ∫‚Äú‰∏âÁÅ£ÔºÇÂíå‚ÄúÂªüÂâçÔºÇÔºåÈÄô‰∫õÂéüÂõ†ÈÉΩÂèØËÉΩÈÄ†ÊàêÂú∞Èªû ËΩâ GPS ÈåØË™§„ÄÇÂè¶Â§ñÈÇÑÊúâÂéüÂõ†ÊòØÂú∞Èªû NER Êé° CRF ÊñπÊ≥ïËá™ÂãïÊ®ôË®òË≤†ÊìîÂ§™Â§ß„ÄÇÁÇ∫Ê≠§Âè¶Â§ñÂØ¶ ÂÅö‰∫ÜÈÖçÂêà FBPlaceDB Ê®ôË®òÊñπÊ≥ïÁöÑÂú∞Èªû NER Ê®°ÁµÑËß£Ê±∫‰∏äËø∞ÊèêÂà∞ÁöÑÂïèÈ°å„ÄÇÊñπÊ≥ïÂèÉËÄÉ Facebook Deduplicating a places [7]ÊÉ≥Ê≥ïÂéªÂØ¶ÁèæÂú∞Èªû NER Ê®°ÁµÑ„ÄÇÁî±Êñº FB Place Ë≥áÊñôÂ∫´Êî∂ ÈåÑÁöÑÂêçÁ®±Êúâ 245 Ëê¨ÔºåÂõ†Ê≠§Âà©Áî® Apache Solr Â∞áÊâìÂç°Âú∞ÈªûÂª∫ÊàêÂÄâÂÑ≤ÔºåÂêåÊôÇÁÇ∫Âä†Âø´Â∞çÂè•Â≠êÁöÑ Ê®ôË®òÊàëÂÄëÂ∞áÂè•Â≠êÂàáÂâ≤Êàê n-gram (n=4~10)ÔºåÂàÜÂà•Êü•Ë©¢ÊúÄÁõ∏ÈóúÁöÑ k ÂÄãÂú∞ÈªûÂêçÁ®± place„ÄÇÂè¶Â§ñ Âª∫Á´ãÂÖ©ÂÄãË∑üÂú∞ÈªûÂèäÂïÜÂÆ∂ÂêçÁ®±ÊúâÈóúÁöÑÂ≠óÂÖ∏Ê™î CoreDic Âèä LocBgDicÔºöÊ†∏ÂøÉÂ≠óÂÖ∏Ê™î CoreDic ÊòØÂà©Áî® MSRA Ë®ìÁ∑¥Âá∫ÁöÑÂØ¶È´î(ÂåÖÂê´‰∫∫Âêç„ÄÅÂú∞Âêç„ÄÅÁµÑÁπîÂêç)Ëæ®Ë≠òÊ®°ÂûãÔºåÊ®ôË®ò 92 Ëê¨ÂÄãÈªÉÈ†ÅÂïÜ ÂÆ∂ÂêçÁ®±‰∏≠Âá∫ÁèæÁöÑÂØ¶È´îÔºå‰∏¶Âà™ÊéâÂá∫ÁèæÈ†ªÁéáÂ∞ëÊñº 6 Ê¨°ÁöÑÂØ¶È´îÂêçÁ®±ÔºåÂÅöÁÇ∫Ê†∏ÂøÉÂ≠óÂÖ∏Ê™î(7,993 Ë©û)Ôºõ ËÄåÂú∞ÈªûÂïÜÂÆ∂ËÉåÊôØÂ≠óÂÖ∏Ê™î LocBgDic Êî∂ÈõÜÊñπÂºèÂâáÊòØÂ∞áÂïÜÂÆ∂ÂêçÁ®±Á∂ìÈÅé‰∏≠ÊñáÊñ∑Ë©ûÂæåÔºåÂèñË©ûÈ†ªÂ§ß Êñº 500 ‰∏îÂ≠òÂú®Â∫ñ‰∏ÅÊñ∑Ë©ûÂíå MMSeg È†êË®≠Ë©ûÂ∫´‰∏≠‰ΩÜ‰∏çÂ±¨Êñº CoreDic ÁöÑÂ≠óË©û(1,361 Ë©û)„ÄÇ Âà©Áî®ÈÄôÂÖ©ÂÄãÂ≠óÂÖ∏Ê™îÔºåÊàëÂÄëÂèØ‰ª•Â∞çÊØè‰∏ÄÂÄãÂè•Â≠ê‰∏≠ÁöÑ n-gram ÂèäÂÖ∂Êü•Ë©¢Âà∞ÁöÑ place Ë©ïÂàÜ„ÄÇÁµ¶ÂàÜ ÂéüÂâáÁÇ∫(1)ÈÅøÂÖçË∑ü Location ÊàñÂïÜÂÆ∂ÁÑ°ÈóúÁöÑÂ≠óÊúÉÂæóÂà∞ÂàÜÊï∏„ÄÇ(2)Áï∂Ë¶ÅÊ®ôË®òÁöÑÂ∞çË±°ÂíåË≥áÊñôÂ∫´ÁöÑ Âú∞ÈªûÁõ∏ÈóúÂ∫¶Âà§ÂÆöÂàÜÊï∏Ë∂ÖÈÅéÈñÄÊ™ªÂÄºÔºåÊàëÂÄëË™çÁÇ∫ÂÖ∂ÂØ¶ÂèØË¶ñÁÇ∫Áõ∏ÈóúÔºåÂç≥ÂèØ‰ª•Âú∞ÈªûÊ®ôË®òÂè•Â≠ê„ÄÇ(3) ÊØîËºÉÂ•ΩÁöÑÂØ¶È´îÈÇäÁïåÊáâÂèñÂæóËºÉÈ´òÁöÑÂàÜÊï∏ÔºåÂ¶Ç n-gram1=‚ÄúÊ∏ÖËèØ/Â§ßÂ≠∏/Êó∫ÂÆè/È§®ÔºÇËàá n-gram2= ‚ÄúÊ∏ÖËèØ/Â§ßÂ≠∏/Êó∫ÔºÇÂàÜÂà•Âíå place=‚ÄúÊ∏ÖËèØ/Â§ßÂ≠∏/Êó∫ÂÆè/È§®ÔºÇÂåπÈÖçÊâÄÂæóÁöÑÂàÜÊï∏ÔºåÂâçËÄÖËºÉÂæåËÄÖÁÇ∫  235  È´ò„ÄÇÂÆåÊï¥ÁöÑÁµ¶ÂàÜÊñπÂºèÂÆöÁæ©Â¶Ç‰∏ãÂúñ‰∫î„ÄÇ ÂÖ∂‰∏≠ CoreFind ÁõÆÁöÑÁÇ∫ÊâæÂá∫Ëº∏ÂÖ•Â≠ó‰∏≤‰∏≠ÂèØËÉΩÂá∫ÁèæÁöÑÊ†∏ÂøÉÂ≠óË©ûÈõÜÂêà(Âà©Áî® MSRA NER Ê®°Âûã Ê®ôË®òÁöÑÂØ¶È´îÂä†‰∏ä CoreDic Ê®ôË®òÁöÑÂ≠óË©û)ÔºåBgFind ÈÄèÈÅé‰∏≠ÊñáÊñ∑Ë©û‰∏¶ÊéíÈô§Ê†∏ÂøÉÂ≠óÔºåËàá LocBgDic ‰∫§ÈõÜÂæóÂà∞ÁöÑËÉåÊôØÂ≠óË©ûÈõÜÂêàÔºåÂâ©È§ò‰∏çÂ±¨ÊñºÊ†∏ÂøÉÂ≠óÂíåËÉåÊôØÂ≠óÁöÑÈõÜÂêàÊàëÂÄëÂ∞áÂÖ∂ÂÆöÁæ©ÁÇ∫ÊèèËø∞Â≠ó„ÄÇ CoreFind, BgFind Âèä Descriptor ‰∏âÂÄãÊ®°ÁµÑËÅØÈõÜÊâÄÂæóÁöÑÂ≠óË©ûÊï∏Âç≥ÊòØ Count ÂáΩÊï∏ÂõûÂÇ≥ÂÄº„ÄÇÊºî ÁÆóÊ≥ï‰∏ªË¶ÅÊòØË®àÁÆóÂá∫ n-gram Âèä placei ÂÖ±ÂêåÁöÑÂØ¶È´îÊ†∏ÂøÉË©û CoreSet(ÂéªÊéâÊ≤íÊúâÂá∫ÁèæÂú® placei ‰∏≠ Ê†∏ÂøÉË©û)Ôºå‰ª•ÂèäÂÖ±ÂêåËÉåÊôØË©û BgSetÔºå‰∏¶‰æù Eq.(1)Ë®àÁÆó n-gram Ëàá placei ÁöÑÁõ∏‰ººÂ∫¶ÔºåËã•Áõ∏‰ººÂ∫¶ Â§ßÊñºÈñÄÊ™ªÂÄºÔºåÂâáÁî®Ë©≤Âú∞Èªû placei ÈÄ≤Ë°å Partial Alignment Ê®ôË®òÈÄôÂÄãÂè•Â≠êÔºåÈÄ≤Ë°åÊúÄÂÆåÊï¥Ê®ôË®ò„ÄÇ  For each n-gram (n=4 to 10) in a sentence s  ‚Äì Query FBPlaceDB to obtain top k place names  ‚Äì For each placei from top k place names 1. CoreSet = CoreFind (placei) ‚à© CoreFind(n-gram)  2. Find the Segment Core Entity based on common core, core length and frequency in  CoreDic  3. BgSet = BgFind (n-gram) ‚à© BgFind(placei)  4. Compute Descriptor(n-gram) and Descriptor(placei), respectively  ùõΩ √ó |ùê∂ùëúùëüùëíùëÜùëíùë°| + (1 ‚àí ùõΩ) √ó |ùêµùëîùëÜùëíùë°| 5. ùëÅùê∂ùêµùëÜùëñùëö(n-gram, placei) = ùõΩ √ó ùëöùëñùëõ(|ùê∂ùëúùë¢ùëõùë°(n-gram)|, |ùê∂ùëúùë¢ùëõùë°(placei)|)  Eq. (1)  6. If ( NCBSim(n-gram, placei) > threshold ) then Label sentence s with placei  Âúñ‰∫î„ÄÅFBPlaceDB Âú∞ÈªûËæ®Ë≠òÊ®ôË®òÊºîÁÆóÊ≥ï Ëàâ‰æãËÄåË®ÄÔºåÂè•Â≠ê s1=„Äå‰∏ªËæ¶ÂñÆ‰ΩçÔºöÈ´òÈõÑÂ∏ÇÊîøÂ∫úÂãûÂ∑•Â±ÄË®ìÁ∑¥Â∞±Ê•≠‰∏≠ÂøÉ„ÄçÔºåÂà©Áî®‰∏çÂêåÁöÑ n-gram  ÂèØÊü•Ë©¢Âà∞‚ÄúÈ´òÈõÑÂ∏ÇÊîøÂ∫ú‚Äù„ÄÅ‚ÄúÊñ∞ÂåóÂ∏ÇÊîøÂ∫úÂãûÂ∑•Â±Ä‚Äù„ÄÅ‚ÄúÂãûÂ∑•Â±ÄË®ìÁ∑¥Â∞±Ê•≠‰∏≠ÂøÉ‚Äù„ÄÅ‚ÄúÊñóÂÖ≠Â∞±  Ê•≠‰∏≠ÂøÉ‚Äù„ÄÅ‚ÄúÈ´òÈõÑÂ∏ÇÊîøÂ∫úÂãûÂ∑•Â±ÄÂãûÂ∑•ÊïôËÇ≤ÁîüÊ¥ª‰∏≠ÂøÉ‚Äù„ÄÅ‚ÄúÈ´òÈõÑÂ∏ÇÊîøÂ∫úÂãûÂ∑•Â±ÄË®ìÁ∑¥Â∞±Ê•≠‰∏≠ÂøÉ  Â§ßÂØÆËÅ∑Ë®ìÂ†¥Âüü‚ÄùÁ≠âÁõ∏ÈóúÁöÑ FB ÊâìÂç°ÈªûÔºåÈÄèÈÅéÊºîÁÆóÊ≥ïÊúÄÂæåÂè•Â≠ê‰∏≠ÊúÉÊ®ôË®òÂà∞ÁöÑÂú∞ÈªûÁÇ∫„ÄåÈ´òÈõÑ  Â∏ÇÊîøÂ∫úÂãûÂ∑•Â±ÄË®ìÁ∑¥Â∞±Ê•≠‰∏≠ÂøÉ„ÄçÔºåÈõñÁÑ∂ÈÄôÁ≠ÜÂú∞ÈªûÂêçÁ®±‰∏çÂú® FBPlaceDB ‰∏≠Ôºå‰ΩÜÊòØÂõ†ÁÇ∫Ëàá‚ÄúÈ´ò  ÈõÑÂ∏ÇÊîøÂ∫úÂãûÂ∑•Â±ÄË®ìÁ∑¥Â∞±Ê•≠‰∏≠ÂøÉÂ§ßÂØÆËÅ∑Ë®ìÂ†¥Âüü‚ÄùÁ∂ìÈÅéÊéíÊØîÔºåÂçªËÉΩÂÆåÊï¥ÁöÑÈÄ≤Ë°åÊ®ôË®ò„ÄÇ  (ÂÖ≠) Âú∞ÂùÄÂØ¶È´îÁöÑË≠òÂà•ÂíåÊôÇÁ®ãË°®ÈÅîÂºèÊ®ôË®òÊ®°ÁµÑ  Âú∞ÂùÄÂØ¶È´îÁöÑË≠òÂà•Ê®°ÁµÑ‰∏ªË¶ÅÊáâÁî®ÊòØÂèÉËÄÉ 2012 Âπ¥ Chang Á≠â‰∫∫[8]ÊèêÂá∫ÁöÑÂè∞ÁÅ£Âú∞ÂçÄÂú∞ÂùÄÊì∑ÂèñÔºå ‰ΩøÁî® CRF ÂíåÈÖçÂêàË°®Âõõ 17 Á®ÆÂú∞ÂùÄÁâπÂæµÂÅöË®ìÁ∑¥Ë©≤Ê®°ÂûãÔºå‰∏¶ÈÖçÂêàÊ•µÂ§ßÂàÜÂ≠êÂ∫èÂàóÊºîÁÆóÊ≥ï  236  ÔºàMaximal Scoring SubsequencesÔºâÂÖ∂F1-score Á¥ÑÂú® 0.94 Ëá≥ 0.99 ÂçÄÈñì„ÄÇÂè¶Â§ñÊú¨ÊñáÁ†îÁ©∂‰∏ª Ë¶ÅÊñáÊú¨Êì∑ÂèñÂ∞çË±°ÁÇ∫ FB PostÔºå‰ΩÜÊú™‰æÜ‰∫ã‰ª∂Êì∑ÂèñÁöÑ‰ªªÂãôÊúÉÊì¥ÂÖÖÊàê Web Data ExtractionÔºåÁÇ∫‰∫Ü  Áõ∏ÂÆπ HTML Á∂≤È†Å(semi-structured)Âíå FB Post (free text)Êé°Áî® Su [9]Á≥ªÁµ±Ê®°ÁµÑÊì∑ÂèñÂè∞ÁÅ£Âú∞ÂùÄ„ÄÇ  Ë°®Âõõ„ÄÅÂú∞ÂùÄÊì∑ÂèñÊ®°ÂûãËæ®Ë≠òÁâπÂæµÂÄº  ID Feature  ICCS  ID Feature  ICCS  
The development of the internet has facilitated the flow of information. However, this explosive growth of information has led to fundamental importance being overlooked: Reading material can be understood. Research on readability formulas aims to predict, to a reasonable extent, the degree to which a text can be understood. It does so mainly by analyzing and translating the information within a text into readability features, which are used to train a readability model, in order to automatically predict the readability of a given text. In recent years, the development of deep neural networks, applied to speech recognition, image processing and natural language processing has improved significantly on the performance. Therefore, this paper proposes a readability model built with deep neural network and word vector representation, and which is capable of analyzing cross-domain texts, in accordance with the diverse topics of text contents. The authors aim to make the readability model capable of analyzing text readability with more accurate, as well as possess domain generalization capacity. ÈóúÈçµË©ûÔºöÂèØËÆÄÊÄßÔºåË©ûÂêëÈáèÔºåÂàÜÈ°ûÔºåÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÊîØÂêëÈáèÊ©ü Keywords: Readability, Word2vec, Classification, Deep Neural Network, Support Vector Machine. ‰∏Ä„ÄÅÁ∑íË´ñ ÂèØËÆÄÊÄß(Readability)ÊòØÊåáÈñ±ËÆÄÊùêÊñôËÉΩÂ§†Ë¢´ËÆÄËÄÖÁêÜËß£ÁöÑÁ®ãÂ∫¶[1],[2],[3],[4]ÔºåÁï∂ËÆÄËÄÖÈñ±ËÆÄÈ´òÂèØ ËÆÄÊÄßÁöÑÊñá‰ª∂ÊôÇÔºåÊúÉÁî¢ÁîüËºÉÂ•ΩÁöÑÁêÜËß£ÂèäÂ≠∏Âæå‰øùÁïôÊïàÊûú[2],[3]„ÄÇÁî±ÊñºÊñá‰ª∂ÁöÑÂèØËÆÄÊÄßÊòØÂ¶ÇÊ≠§Èáç Ë¶ÅÔºåÂõ†Ê≠§Êó©Âú® 1923 Âπ¥ Lively Âíå Pressey Â∞±ÊèêÂá∫ÊñπÊ≥ï‰æÜÊé¢Ë®éÊïôÁßëÊõ∏‰∏≠Â≠óÂΩôÈõ£Â∫¶ÁöÑÂïèÈ°å[5]„ÄÇ Âú® 1928 Âπ¥ Vogel Âíå Washburne ÂâáÊòØÊèêÂá∫‰∏ÄÂÄã Winnetka Formula ‰æÜË©ïÈáèÂ∞èÂ≠©ËÆÄÁâ©ÁöÑÂèØËÆÄ ÊÄß[6]„ÄÇÂèØËÆÄÊÄßÁ†îÁ©∂‰∏ÄÁõ¥ÊåÅÁ∫å‰∏çÊñ∑ÁöÑÁôºÂ±ïÔºåÊìö Chall Ëàá Dale Âú® 1995 Âπ¥ÁöÑÁµ±Ë®àÔºåÂà∞ 1980 Âπ¥ÁÇ∫Ê≠¢Áõ∏ÈóúÁöÑÂèØËÆÄÊÄßÂÖ¨ÂºèÂ∑≤Á∂ìË∂ÖÈÅé 200 Â§öÂâáÊñá‰ª∂ÂèØËÆÄÊÄßÂÖ¨Âºè[7]„ÄÇÈÄô‰∫õÂÇ≥Áµ±ÁöÑÂèØËÆÄÊÄßÂÖ¨ ÂºèÂ§ßÂ§öÈÉΩÊòØ‰ΩøÁî®Ë™ûË®ÄÁâπÂæµ‰æÜË©ïÈáèÊñá‰ª∂ÁöÑÂèØËÆÄÊÄßÔºå‰æãÂ¶ÇÔºöËëóÂêçÁöÑ Flesch Reading Ease ÂÖ¨Âºè ‰ª•Ë©ûÂΩôÈü≥ÁØÄÊï∏ÂÅöÁÇ∫Ë™ûÊÑèÁöÑÊåáÊ®ôÔºå‰ª•Âè•Â≠êÁöÑÈï∑Â∫¶‰ΩúÁÇ∫Ë™ûÊ≥ïÁöÑÊåáÊ®ôÔºåË®àÁÆóË©ûÂΩôÁöÑÂπ≥ÂùáÈü≥ÁØÄÊï∏ ËàáÊñá‰ª∂ÁöÑÂπ≥ÂùáÂè•Â≠êÈï∑Â∫¶‰æÜË©ï‰º∞Êñá‰ª∂Èõ£Â∫¶ÔºåÁï∂Êñá‰ª∂ÁöÑË©ûÂΩôÈü≥ÁØÄÊï∏ÊÑàÂ§ö„ÄÅÂè•Â≠êÊÑàÈï∑ÔºåÂâáË©≤Êñá 256  ‰ª∂ÊÑàÂõ∞Èõ£[8]„ÄÇChall Âíå Dale(1995)Âä†ÂÖ•‰∫Ü„ÄåÈõ£Ë©ûÊØîÁéá„ÄçÂÅöÁÇ∫Ë©ï‰º∞Êñá‰ª∂Èõ£Â∫¶ÁöÑÊñπÂºèÔºåÈõ£Ë©û Âá∫ÁèæÊÑàÂ§öÔºåË°®Á§∫Êñá‰ª∂ÊÑàÂõ∞Èõ£[7]„ÄÇËá≥‰ªäÔºåÂèØËÆÄÊÄßÊ®°ÂûãÁöÑÁôºÂ±ï‰æùËàäËì¨ÂãÉÁôºÂ±ïÔºå‰∏¶Èö®ËëóÊ©üÂô® Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÂ¥õËµ∑ÔºåÁ†îÁ©∂‰∫∫Âì°Âæó‰ª•Áî®Êõ¥Á¥∞Á∑ªÁöÑÊºîÁÆóÊ≥ïËÆìÂèØËÆÄÊÄßÊ®°ÂûãÂèØ‰ª•Á¥çÂÖ•Êõ¥Â§öÂÖÉÁöÑÂèØ ËÆÄÊÄßÊåáÊ®ôÔºå‰ª•ÊèêÂçáÊ®°ÂûãÊ∫ñÁ¢∫Áéá [9],[10],[11]„ÄÇ ÈõñÁÑ∂Ê©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÊèêÂçá‰∫ÜÂèØËÆÄÊÄßÊ®°ÂûãÁöÑÊ∫ñÁ¢∫ÁéáÔºåÁÑ∂ËÄåÊ®°ÂûãÊâÄÊé°Áî®ÁöÑÁâπÂæµ‰ªçËàäÁÇ∫ÈÅé ÂéªÁöÑ‰∏ÄËà¨Ë™ûË®ÄÁâπÂæµÔºåËÄå‰∏ÄËà¨Ë™ûË®ÄÁâπÂæµÂè™ÂñÆÁ¥îËÄÉÈáèË™ûÊÑè„ÄÅË™ûÊ≥ïÂíåÈõ£Ë©ûÊØîÁéáÁ≠âËÆäÈ†Ö‰∏¶‰∏çË∂≥‰ª• ÂèçÊò†Êñá‰ª∂Èõ£Â∫¶„ÄÇGraesser„ÄÅSinger Âíå Trabasso(1994)ÊåáÂá∫ÔºåÂÇ≥Áµ±Ë™ûË®ÄÁâπÂæµÂÖ¨ÂºèÁÑ°Ê≥ïÂèçÊò†Èñ± ËÆÄÁöÑÁúüÂØ¶Ê≠∑Á®ãÔºåÊñá‰ª∂ÁöÑË™ûÊÑèË™ûÊ≥ïÂè™ÊòØÊñá‰ª∂ÁöÑÊ∑∫Â±§Ë™ûË®ÄÁâπÂæµÔºåÊ≤íÊúâËÄÉÈáèÊñá‰ª∂ÁöÑÂáùËÅöÁâπÊÄß [12]„ÄÇCollins-Thompson(2014)‰∫¶ÊåáÂá∫ÂÇ≥Áµ±ÂèØËÆÄÊÄßÂÖ¨ÂºèÂÉÖËëóÈáçÂú®Êñá‰ª∂ÁöÑË°®Ê∑∫Ë≥áË®äÔºåËÄåÂøΩÁï• Êñá‰ª∂ÈáçË¶ÅÁöÑÊ∑±Â±§ÁâπÂæµ„ÄÇÈÄô‰πüËÆìÂÇ≥Áµ±ÂèØËÆÄÊÄßÂÖ¨ÂºèÂú®È†êÊ∏¨ÊñáÊú¨ÂèØËÆÄÊÄßÁöÑÁµêÊûúÂ∏∏ÈÅ≠ÂèóÂà∞Ë≥™Áñë [13]„ÄÇÊ≠§Â§ñÔºå‰∏ÄËà¨Ë™ûË®ÄÁâπÂæµ‰∫¶ÁÑ°Ê≥ïÂà§Êñ∑Ë©ûÂΩôÂú®‰∏çÂêåÈ†òÂüüÊôÇÔºåÂÖ∂Ë©ûÂΩôËÉåÂæåÊâÄ‰ª£Ë°®ÁöÑÊÑèÁæ©„ÄÇ Âõ†ÁÇ∫ÁâπÂÆöÈ†òÂüüÊñáÊú¨ÁöÑÂÖßÂÆπËëóÈáçÂú®Èó°Ëø∞È†òÂüüÁöÑ„ÄåÁü•Ë≠òÊ¶ÇÂøµ„ÄçÔºåËÄåÈÄôÊ®£Â≠êÁöÑÊèèËø∞ÊñπÂºèÊúâÂà•Êñº ‰∏ÄËà¨Ë™ûÊñáÁöÑÊïòËø∞ÊñáÊàñÊïÖ‰∫ãÈ´îÁöÑÁµêÊßã„ÄÇYan Á≠â‰∫∫(2006)Â∞±ÊòéÁ¢∫ÊåáÂá∫Âú®Ë®àÁÆóÁæéÂúãÂ§ßÂûãÈÜ´Â≠∏Ë≥á ÊñôÂ∫´(Medical Subject Headings, MeSH)‰∏≠ÁöÑÂ∞àÊ•≠Ë°ìË™ûÂéªÊé¢Ë®éÔºåÁôºÁèæË™ûË®ÄÁâπÂæµÂÖ¨ÂºèÁöÑÈü≥ÁØÄ Êï∏„ÄÅÂ≠óÈï∑ËàáÈÜ´Â≠∏È°ûÂ∞àÊ•≠Ë©ûÂΩôÁöÑÂõ∞Èõ£Â∫¶ÁÑ°Áõ∏Èóú„ÄÇÊèõÂè•Ë©±Ë™™ÔºåÊé°Áî®‰∏ÄËà¨Ë™ûË®ÄÁâπÂæµÁöÑÂèØËÆÄÊÄßÂÖ¨ ÂºèÁÑ°Ê≥ïÂèçÊò†ÁâπÂÆöÈ†òÂüüÊñá‰ª∂‰∏≠Â∞àÊ•≠Ë°ìË™ûÁöÑÈõ£Â∫¶[14]„ÄÇ ÈáùÂ∞ç‰∏ÄËà¨Ë™ûË®ÄÁâπÂæµÁÑ°Ê≥ïË°®ÂæµÁâπÂÆöÈ†òÂüüÁü•Ë≠òÁµêÊßãÁöÑÂïèÈ°åÔºåÈñãÂßãÊúâÂ≠∏ËÄÖÈáùÂ∞çÈÄôÂÄãË≠∞È°åÈÄ≤ Ë°åÁ†îÁ©∂„ÄÇ‰æãÂ¶ÇÔºåYan Á≠â‰∫∫(2006)Âà©Áî®Êú¨È´îË´ñÁöÑÊäÄË°ìÂ∞áÁæéÂúãÂúãÂÆ∂ÈÜ´Â≠∏Ë≥áÊñôÂ∫´(Medical Subject Headings, MeSH)ÁöÑÈÜ´Â≠∏Á¨¶ËôüÈöéÂ±§Ë≥áÊñôÂ∫´‰ΩúÁÇ∫Ê¶ÇÂøµË≥áÊñôÂ∫´ÔºåÂæû‰∏≠ÊâæÂá∫ÊØè‰∏ÄÂÄãÈÜ´Â≠∏È°ûÊñá‰ª∂ ‰∏≠ÁöÑÊ¶ÇÂøµÔºå‰∏¶Ë®àÁÆóÊ¶ÇÂøµÂà∞Ê≠§Ê®πÁãÄÁµêÊßãÊúÄÂ∫ïÈÉ®ÁöÑË∑ùÈõ¢ÔºåÂæóÂá∫ÊØèÁØáÊñá‰ª∂Ê¶ÇÂøµÊ∑±Â∫¶ÊåáÊ®ô (Document Scop)[14]„ÄÇBorst Á≠â‰∫∫(2008)ÂâáÊòØÂà©Áî®Ë©ûË°®ÁöÑÊñπÂºèÂ∞áÊØèÂÄãË©ûÂΩôÁöÑ„ÄåÈ°ûÂà•Ë§áÈõúÂ∫¶„Äç Ëàá„ÄåË©ûÈ†ª„ÄçÂÖ©ÂÄãÂàÜÊï∏Âä†Á∏Ω‰æÜË®àÁÆóË©ûÂΩôË§áÈõúÂ∫¶Ôºå‰ΩúÁÇ∫Ë©ï‰º∞ÈÜ´Â≠∏È°ûÁ∑ö‰∏äÊñá‰ª∂Ë©ûÂΩô„ÄÅÂè•Â≠ê„ÄÅÂèä Êñá‰ª∂Èõ£Â∫¶ÁöÑ‰æùÊìö[15]„ÄÇChang Á≠â‰∫∫(2013)ÂâáÊòØÂ∞áÊΩõËóèË™ûÊÑèÂàÜÊûê(Latent Semantic Analysis, LSA)ÊáâÁî®Âú®ÁâπÂÆöÈ†òÂüüÁöÑÊïôÁßëÊõ∏‰∏äÔºåÈÄèÈÅéÂ•áÁï∞ÂÄºÂàÜËß£(Singular Value Decomposition, SVD) Â∞áÁ∂≠Â∫¶Á∏ÆÊ∏õ‰ª•Êì∑ÂèñÂá∫Ë™ûÊñôÂ∫´ÁöÑË™ûÊÑèÁ©∫Èñì‰æÜË°®ÈÅîÊñá‰ª∂ÊΩõËóèË™ûÊÑèÂ±¨ÊÄßÔºåÊé•ËëóÂÜç‰ª•È§òÂº¶Ê∏¨Èáè (cosine measure)ÁöÑÊñπÂºè‰æÜÁç≤ÂæóÈ†òÂüüÊñá‰ª∂‰∏≠‰∏çÂêåÈõ£Â∫¶ÁöÑÊ¶ÇÂøµË©ûÂΩô[16]„ÄÇ ÈÄèÈÅéË°®ÂæµÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÁµêÊßã‰æÜÂÅöÁÇ∫Êñá‰ª∂Ë°®Á§∫(Document Representation)ÔºåÈõñÁÑ∂ÂèØ‰ª• Â¢ûÈÄ≤ÂèØËÆÄÊÄßÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇ‰ΩÜÂçª‰πüÈôêÂà∂ÂèØËÆÄÊÄßÊ®°ÂûãÁöÑ‰∏ÄËà¨Âåñ(Generalization)ËÉΩÂäõÔºöÂøÖÈ†àÂÖà 257  Á¢∫Ë™çÊñá‰ª∂ÊâÄÂ±¨ÁöÑÈ†òÂüüÂæåÔºåÊâçËÉΩ‰ΩøÁî®Áõ∏Â∞çÊáâÁöÑÂèØËÆÄÊÄßÊ®°Âûã‰æÜË©ï‰º∞Êñá‰ª∂ÂèØËÆÄÊÄß„ÄÇËÄåÊ≠§ÈôêÂà∂Èô§ ‰∫ÜÂ∞áÈÄ†ÊàêÂèØËÆÄÊÄßÊ®°ÂûãÂú®ÂØ¶Èöõ‰∏äÊáâÁî®ÁöÑÂïèÈ°åÂ§ñÔºåÂ¶Ç‰Ωï‰∫ãÂÖàÂ∞çÊñºÊñá‰ª∂ÊâÄÂ±¨È†òÂüüÁöÑÁïåÂÆöÊõ¥ÊòØ‰∏Ä Â§ßËÄÉÈ©ó„ÄÇ‰æãÂ¶ÇÁ∂≤Ë∑ØÊñáÊú¨ÊàñËÄÖË™≤Â§ñËÆÄÁâ©ÁöÑÂÖßÂÆπÈùûÂ∏∏Â§öÂÖÉÔºåÂõ†Ê≠§Èõ£‰ª•ÂçÄÂà•Êñá‰ª∂ÊâÄÂ±¨ÁöÑÈ†òÂüü„ÄÇ ÊâÄ‰ª•ÁôºÂ±ï‰∏ÄÂÄãË∑®È†òÂüüÊñá‰ª∂ÁöÑÂèØËÆÄÊÄßÊ®°ÂûãÊòØÂøÖÈ†àÁöÑ„ÄÇ Êú¨Ë´ñÊñáÁöÑÂÖßÂÆπÂÆâÊéíÂ¶Ç‰∏ãÔºöÁ¨¨‰∫åÁØÄÂ∞áÊèèËø∞ÁõÆÂâçÂèØËÆÄÊÄßÊ®°ÂûãÊáâÁî®ÊñºË©ïÈáèÁ∂≤Ë∑ØÊñáÊú¨ÁöÑÁõ∏Èóú Á†îÁ©∂Ôºå‰∏¶Ë™™ÊòéÂÖ∂Ê®°ÂûãÂèØÊîπÈÄ≤ÁöÑÁ©∫Èñì„ÄÇÁ¨¨‰∏âÁØÄÂ∞áÊèêÂá∫Âü∫ÊñºÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèäË©ûÂêëÈáèÊäÄË°ì‰æÜ Âª∫ÊßãÂá∫‰∏ÄÂÄãËÉΩÂ§†ÂêåÊôÇÂàÜÊûê‰∏çÂêåÈ†òÂüüÊñá‰ª∂ÂèØËÆÄÊÄßÁöÑÊ®°Âûã„ÄÇÁ¨¨ÂõõÁØÄÂ∞áÂëàÁèæÊú¨Ë´ñÊñáÊâÄÊèêÂá∫Ê®°Âûã ÁöÑÊïàËÉΩ„ÄÇÊúÄÂæåÁ¨¨‰∫îÁØÄÊòØÁ∏ΩÁµêÂèäÊú™‰æÜÁ†îÁ©∂ÁöÑÊñπÂêë„ÄÇ ‰∫å„ÄÅÁõ∏ÈóúÁ†îÁ©∂ Â¶ÇÂêå Collins-Thompson Âú® 2014 ÊâÄËø∞ÔºöËÆÄËÄÖÊòØÂê¶ËÉΩÁêÜËß£Êñá‰ª∂Â∏∏Ë¢´Ë¶ñÁÇ∫Êñá‰ª∂ÁöÑÈáçË¶ÅÂÉπÂÄº‰πã ‰∏ÄÔºåÁÑ∂ËÄåÂú®Ë®≠Ë®àÁ∂≤È†ÅÊñáÊú¨ÊôÇÂçªÂ∏∏ÂøΩÁï•ÈÄôÂÄãÂü∫Êú¨ÁöÑÂÉπÂÄºËÄåÂ∞éËá¥ËÆÄËÄÖÂú®Èñ±ËÆÄ‰∏äÁöÑÂõ∞Èõ£[13]Ôºå ËÄåÈÄô‰πüÊ≠£Â•ΩÂá∏È°Ø‰∫ÜÊñá‰ª∂ÂèØËÆÄÊÄßÁöÑÈáçË¶ÅÊÄßÂíåÂïèÈ°åÊâÄÂú®„ÄÇÂú®ÈÄôË≥áË®äÁàÜÁÇ∏ÁöÑÁèæ‰ª£ÔºåÁ∂≤Ë∑ØÊñáÊú¨Êõ¥ ÊòØËÆÄËÄÖÁç≤ÂèñË≥áË®äÁöÑ‰∏ªË¶Å‰æÜÊ∫ê„ÄÇÁÑ∂ËÄåÁõÆÂâçÂ∏∏ÁúãÂà∞Ë©ïÈáèÁ∂≤È†ÅÂèØËÆÄÊÄßÁöÑÊúçÂãôÂ¶Ç Readability Test [17]„ÄÅThe Readability Test Tool[18]‰æùËàäÂà©Áî®ÂÇ≥Áµ±ÁöÑÂèØËÆÄÊÄßÂÖ¨ÂºèÂ¶Ç Flesch Reading Ease[8]„ÄÅ Flesch Kincaid Grade Level[19]Âèä Gunning Fog Score [20]‰æÜË©ïÈáè‰∏çÂêåÈ†òÂüüÁöÑÁ∂≤Ë∑ØÊñáÊú¨ÂèØ ËÆÄÊÄß„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÂèØËÆÄÊÄßÂÖ¨ÂºèÂÉÖ‰ª•Ë©ûÂΩôÔºàÂ¶ÇÈü≥ÁØÄÊï∏ËàáÈõ£Ë©ûÊØîÁéáÔºâÊàñÂè•Â≠êÔºàÂ¶ÇÂè•Èï∑Ôºâ‰æÜÁï∂ ‰ΩúÂèØËÆÄÊÄßÊåáÊ®ôÔºå‰ΩÜÈÄôÈ°ûË°®Ê∑∫ÁöÑË™ûË®ÄÊåáÊ®ôÂçªÁ∂ìÂ∏∏Ë¢´Ë≥™ÁñëË©ûÂΩôË∂äÈï∑ÁöÑË©û‰∏ç‰∏ÄÂÆöÂ∞±Ë∂äÂõ∞Èõ£Ôºå‰∏¶ ‰∏î‰ª•Âè•Â≠êÈï∑Â∫¶Áï∂ÊàêÂè•Ê≥ïË§áÈõúÁöÑÊåáÊ®ôÂâáÈÅéÊñºÁõ¥Ë¶∫‰∏çÂ§†Á≤æÁ∑ª[21]„ÄÇKidwell„ÄÅ Lebanon Âíå Collins-Thompson (2011)ÊåáÂá∫Áï∂ÊñáÊú¨Â≠óÊï∏Â∞ëÊñº 300 Â≠óÊôÇÔºåÂÖ∂ÂÇ≥Áµ±ÂèØËÆÄÊÄßÂÖ¨ÂºèÊòØ‰∏çÈÅ©Áî®ÁöÑ„ÄÇ ËÄåÈÄôÂÄãÂïèÈ°åÂ∞çÊñºÂ≠óÊï∏Èõ£‰ª•ÊéßÂà∂ÁöÑÁ∂≤Ë∑ØÊñáÊú¨ËÄåË®ÄÈ°ØÁÑ∂ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÈôêÂà∂[22]„ÄÇ ÁõÆÂâçÂ∑≤ÊúâË®±Â§öÁ†îÁ©∂ÊåáÂá∫ÂÇ≥Áµ±ÂèØËÆÄÊÄßÂÖ¨Âºè‰∏¶‰∏çÈÅ©ÂêàË©ïÈáèÁ∂≤È†ÅÊñáÊú¨ÁöÑÂèØËÆÄÊÄß [23],[24],[25]ÔºåÂõ†Ê≠§Â¶Ç‰ΩïÊâçÊîπÂñÑÂèØËÆÄÊÄßÊ®°Âûã‰æÜÈÅ©Áî®ÊñºÂú®Á∂≤È†ÅÊñáÊú¨‰æøÊòØÂÄºÂæóÁ†îÁ©∂ÁöÑË≠∞È°å„ÄÇ Miltsakaki Âú® 2007 Âπ¥ÊèêÂá∫ Read-X Á≥ªÁµ±‰æÜÈáùÂ∞ç‰∏çÂêåÈ°ûÂà•Á∂≤È†ÅÊñáÊú¨ÈÄ≤Ë°åÂèØËÆÄÊÄßÁöÑË©ï‰º∞ÔºåÂÆÉ ÊòØÂà©Áî®‰∏âÁ®ÆÂèØËÆÄÊÄßÂÖ¨ÂºèÔºöLix readability formula„ÄÅ Rix readability formula Âíå Coleman-Liau redability formula Á≠â‰æÜË©ïÈáèÊñá‰ª∂ÁöÑÂèØËÆÄÊÄßÈõ£Â∫¶Ôºå‰ΩÜ Read-X Á≥ªÁµ±Êú™Â∞á‰∏âÁ®ÆÂèØËÆÄÊÄßÁöÑÈõ£Â∫¶ÈÄ≤ Ë°åÊï¥ÂêàÔºåÈÄôÂ∞áÈÄ†Êàê‰ΩøÁî®ËÄÖÂ∞çÊñºÊñá‰ª∂ÂèØËÆÄÊÄßÈõ£Â∫¶ÈÄ†ÊàêÁñëÊÖÆ[26],[27]„ÄÇÁõ¥Âà∞ 2009 Âπ¥ Miltsakaki Âà©Áî®Âπ≥ÂùáÂ∞á Lix Readability Formula„ÄÅRix Readability Formula Âíå Coleman-Liau Readability 258  Formula ÁöÑÈõ£Â∫¶ÈÄ≤Ë°åÊï¥ÂêàÔºåÁÑ∂ËÄåÂçª‰πüÁôºÁèæÈÄôÊ®£Â≠êÁöÑÊï¥ÂêàÊñπÂºèÁÑ°Ê≥ïÊúâÊïàÂçÄÂàÜÂá∫ 9-10 Âπ¥Á¥öÂèä 11-13 Âπ¥Á¥öÁöÑÈõ£Â∫¶[28]„ÄÇEickhoff Á≠â‰∫∫ÂâáÊòØËóâÁî±ÂÅµÊ∏¨‰∏ªÈ°åÁöÑÊñπÂºè‰æÜÂçÄÂàÜÂì™Á®ÆÁ∂≤È†ÅÈÅ©ÂêàÂ∞èÂ≠© Â≠êÈñ±ËÆÄ[29]„ÄÇÁÑ∂ËÄåÊ≠§Á®ÆÂà§ÂÆöÊñá‰ª∂ÂèØËÆÄÊÄßÁöÑÊñπÂºèÊòØÂê¶ÈÅ©Áî®ÊñºËû∫ÊóãÂºèÊïôÂ≠∏(Spiral Curriculum) ÁöÑÊñáÊú¨ÂÄºÂæóÂïÜÊ¶∑„ÄÇKanungo Âíå Orr(2009)ÂâáÊòØÈáùÂ∞çÊêúÂ∞ãÂºïÊìéÊâÄÊêúÂ∞ãÂá∫‰æÜÁöÑÁ∂≤È†ÅÊëòË¶Å‰æÜË©ï ÈáèÂèØËÆÄÊÄßÔºåÂèØÊÉúÊâÄÊé°Áî®ÁöÑÂèØËÆÄÊÄßÁâπÂæµ‰ªçËàäÁÇ∫‰∏ÄËà¨ÁöÑË™ûË®ÄÁâπÂæµÔºå‰∏¶ÁÑ°Ê≥ïË°®ÂæµÁâπÂÆöÈ†òÂüüÊñá‰ª∂ ÁöÑÁü•Ë≠òÁµêÊßãÔºåÂõ†Ê≠§ÂÖ∂Êñá‰ª∂ÂèØËÆÄÊÄßÁµêÊûúÊòØÂê¶ËÉΩÈÄ≤‰∏ÄÊ≠•Â∞çÊáâÁ∂≤È†ÅÈÅ©ËÆÄÁöÑÂπ¥ÈΩ°ÂâáÈúÄÈÄ≤‰∏ÄÊ≠•ÁöÑÈ©ó Ë≠â[30]„ÄÇ Èô§‰∫ÜÂèØËÆÄÊÄßÊåáÊ®ôÁöÑÂïèÈ°å‰πãÂ§ñÔºåÂèóÁõäÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊäÄË°ìËàáÊ©üÊ¢∞Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÂ¥õËµ∑Ôºå Á†îÁ©∂‰∫∫Âì°Âæó‰ª•Áî®Êõ¥Á≤æÁ∑ªÁöÑÊ®°ÂûãÊºîÁÆóÊ≥ï‰æÜÊ∏¨ÈáèÊñáÊú¨ÁöÑÂèØËÆÄÊÄßÔºå‰ΩøÂèØËÆÄÊÄßÊ®°Âûã‰∏çÂÉÖÂèØ‰ª•Á¥çÂÖ• Êõ¥Â§öÂÖÉÁöÑÂèØËÆÄÊÄßÊåáÊ®ôÔºå‰∏¶‰∏îÂ∞çÊñºÊ®°ÂûãÁöÑÊïàËÉΩ‰∫¶ÊúâÊòéÈ°ØÁöÑÊèêÂçá[31],[32],[34] ÔºåËÄåÂÖ∂‰∏≠ÊâÄÊé° Áî®ÂàÜÈ°ûÁöÑÂ∑•ÂÖ∑Âèà‰ª•ÊîØÂêëÈáèÊ©ü(Support Vector Machine, SVM)ÊúÄÁÇ∫Â∏∏Ë¶ã„ÄÇÁÑ∂ËÄåÊîØÂêëÈáèÊ©üÁõ∏Â∞ç ÊñºÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø(Deep Neural Network, DNN)ËÄåË®ÄÔºåÊòØÂ±¨Êñº‰∏ÄÁ®ÆÊ∑∫Â±§ÁöÑÁµêÊßãÔºåÁõÆÂâçÂ∑≤ ÊúâÁ†îÁ©∂Ë≠âÊòéÈÄôÁ®ÆÊ∑∫Â±§ÁµêÊßãÁöÑÊ©üÊ¢∞Â≠∏ÁøíÊºîÁÆóÊ≥ïÂú®Ëß£Ê±∫Á∞°ÂñÆÊàñÈôêÂà∂ËºÉÂ§öÁöÑÂàÜÈ°ûÂïèÈ°å‰∏äÔºåÊòØÂèØ ‰ª•ÂæóÂà∞‰∏çÈåØÁöÑÊïàÊûú„ÄÇ‰ΩÜÊòØÂèóÈôêÊñºÊ®°ÂûãÂª∫Ê®°ÂíåË°®Á§∫ÁöÑËÉΩÂäõÔºåÁï∂ËôïÁêÜÁöÑÂïèÈ°åÊòØÊõ¥ÁÇ∫Ë§áÈõúÁöÑÊÉÖ ÂΩ¢‰∏ãÔºåÂ∞±ÊúÉÈù¢Ëá®ÂêÑÁ®ÆÁöÑÂõ∞Èõ£[35]„ÄÇÂõ†Ê≠§ÔºåÂæû‰∏äËø∞ÁöÑÁ†îÁ©∂ÂèØ‰ª•ÁôºÁèæÔºåÂ∞áÂèØËÆÄÊÄßÊ®°ÂûãÈÅãÁî®Ëá≥ Á∂≤Ë∑ØÊñá‰ª∂ÊòØ‰∏ÄÂÄãÂøÖÁÑ∂ÁöÑË∂®Âã¢Ôºå‰ΩÜ‰πüÂõ†ÁÇ∫Á∂≤Ë∑ØÊñá‰ª∂ÊúâËëóË®±Â§öË§áÈõú‰∏îÁÑ°Ê≥ïÊéåÊéßÁöÑÂõ†Á¥†ÔºåËá¥‰Ωø ÂèØËÆÄÊÄßÊ®°ÂûãÁöÑÁôºÂ±ïÈúÄË¶ÅËÄÉÊÖÆÁöÑÊõ¥Âä†Âë®Âª∂„ÄÇÂõ†Ê≠§ÔºåÊú¨Ë´ñÊñáÂ∞áÂà©Áî®Ê∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèäË©ûÂêëÈáè ÊäÄË°ì‰æÜÂª∫ÊßãË∑®È†òÂüüÊñá‰ª∂ÁöÑÂèØËÆÄÊÄßÊ®°Âûã„ÄÇ ‰∏â„ÄÅÂü∫ÊñºÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèäË©ûÂêëÈáèË°®Á§∫Ê≥ï‰πãÂèØËÆÄÊÄßÊ®°ÂûãÂª∫Á´ã (‰∏Ä)„ÄÅË©ûÂêëÈáèË°®Á§∫ Ë©ûÂêëÈáèË°®Á§∫ÁöÑËßÄÂøµÊúÄÊó©Áî± Hinton Âú® 1986 Âπ¥ÊâÄÊèêÂá∫ÔºåÂèàË¢´Á®±ÁÇ∫Ë©ûË°®Á§∫(Word Representation or Word Embedding)[36] „ÄÇ Bengio Âú® 2003 Âπ¥ Êèê Âá∫ Âõû È•ã Âºè È°û Á•û Á∂ì Á∂≤ Ë∑Ø Ë™û Ë®Ä Ê®° Âûã (Feed-forward Neural Network Language Model(FFNNLM)ÁöÑË®ìÁ∑¥Êû∂ÊßãÔºåÂæûÊñá‰ª∂‰∏≠Ë©ûÂΩôÂâç ÂæåÁõ∏ÈÑ∞ÁöÑÈóú‰øÇ‰æÜÊ±ÇÂèñË©ûÂêëÈáèË°®Á§∫[37]„ÄÇËÄåËøëÊúü Google ÊâÄÁôºË°®ÁöÑ Word2Vec ÂâáÂèØË¶ñÁÇ∫ FFNNLM ÁöÑÂæåÁπºÊñπÊ≥ï[38]„ÄÇÁÑ∂ËÄåË∑ü FFNNLM Êû∂Êßã‰∏ç‰∏ÄÊ®£ÁöÑÊòØÔºåWord2vec ÂéªÈô§‰∫Ü FFNNLM Âú®Ë®ìÁ∑¥ÊôÇÊúÄËÄóÊôÇÁöÑÈùûÁ∑öÊÄßÈö±ËóèÂ±§ÔºåÂÉÖ‰øùÁïôËº∏ÂÖ•Â±§„ÄÅÊäïÂΩ±Â±§ÂíåËº∏Âá∫Â±§Ôºå‰ΩøÂÖ∂Êû∂ÊßãÊõ¥Âä†Á∞°ÂñÆ„ÄÇ Word2vec Êèê‰æõ‰∫Ü‰∫åÁ®ÆË®ìÁ∑¥ÊñπÂºèÔºåÂàÜÂà•ÊòØÈÄ£Á∫åË©ûË¢ãÊ®°Âûã(Continuous Bag-of-words Model, 259  CBOW)ÂèäÁï•Ë©ûÊ®°Âûã(Skip-gram Model, Skip-gram)„ÄÇÈÄ£Á∫åË©ûË¢ãÊ®°Âûã‰∏ªË¶ÅÁöÑÁ≤æÁ•ûÊòØÁî±ÁõÆÊ®ôË©û  ‰πãÂ§ñÁöÑÂâçÂæåÊñá‰æÜÈ†êÊ∏¨ÁõÆÊ®ôË©ûÁöÑÊ©üÁéáÔºõËÄåÁï•Ë©ûÊ®°ÂûãÁöÑË®ìÁ∑¥ÊñπÂºèÊ≠£Â•ΩÁõ∏ÂèçÔºåÂÆÉÊòØÁî±ÁõÆÊ®ôË©ûÊú¨  Ë∫´‰æÜÂéªÈ†êÊ∏¨ÂâçÂæåÊñáÁöÑÊ©üÁéáÔºå‰∫åÁ®ÆË®ìÁ∑¥Ê®°ÂûãÁ§∫ÊÑèÂúñÂ¶ÇÂúñ 1 ÂíåÂúñ 2 ÊâÄÁ§∫„ÄÇÂú® Word2Vec ‰∏≠‰∏ç  Ë´ñÊòØÈÄ£Á∫åË©ûË¢ãÊ®°ÂûãÈÇÑÊòØÁï•Ë©ûÊ®°ÂûãÔºåÂú®Ëº∏Âá∫Â±§ÈÉΩÂèØ‰ª•Êé°Áî® Hierarchical Softmax ÊàñÊòØ  Negative Sampling ÂÖ©Á®ÆÊ®°Âºè‰æÜÂ¢ûÈÄ≤Ë®ìÁ∑¥ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºå‰∏çË´ñÊòØÈÄ£Á∫åË©ûË¢ãÊ®°ÂûãÈÇÑÊòØÁï•Ë©ûÊ®°  ÂûãÈÉΩÊòØÂü∫Êñº‰∏ÄÂÄãÈï∑Â∫¶‰æÜÁúãË©ûÂΩô‰πãÈñìÁöÑÈóú‰øÇÔºåÂç≥ÊâÄË¨ÇÁöÑ Shallow Window-Based ÁöÑÊñπÊ≥ï„ÄÇ  Jeffrey Pennington ÂâáÊòØÂú® 2014 Âπ¥ÊèêÂá∫‰∏ÄÂÄã GloVe ÁöÑÊºîÁÆóÊ≥ï‰æÜÂêåÊôÇËÄÉÊÖÆÂÖ®ÂüüÂèäÂçÄÂüüË©ûÂΩô  ‰πãÈñìÁöÑÈóú‰øÇÔºå‰ª•ÊèêÂçá Word Embedding ÁöÑÊïàÊûú[39]„ÄÇËÄåÊ†πÊìöÈÅéÂéªÁöÑÂØ¶È©óÔºåÈÄ£Á∫åË©ûË¢ãÊ®°Âûã„ÄÅ  Áï•Ë©ûÊ®°ÂûãÂèä GloVe Âú®ÂèØËÆÄÊÄßÁ†îÁ©∂ÁöÑÊïàËÉΩÂ∑ÆÁï∞‰∏çÂ§ßÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊú¨Ë´ñÊñáÂ∞áÂü∫ÊñºÈÄ£Á∫åË©ûË¢ãÊ®°Âûã  Ë©ûÂêëÈáèË°®Á§∫ÊñπÂºè‰æÜÊê≠ÈÖçÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂª∫ÊßãÂá∫‰∏ÄÂÄãË∑®È†òÂüüÊñá‰ª∂ÁöÑÂèØËÆÄÊÄßÊ®°Âûã[40]„ÄÇ  INPUT  PROJECTION  OUTPUT  W(t-2)  W(t-1) W(t+1)  SUM W(t)  W(t+2) Âúñ‰∏Ä„ÄÅÈÄ£Á∫åË©ûË¢ãÊ®°ÂûãË®ìÁ∑¥ÊºîÁÆóÊ≥ï  260  INPUT  PROJECTION  OUTPUT W(t-2)  W(t-1) SUM W(t) W(t+1)  Âúñ‰∫å„ÄÅÁï•Ë©ûÊ®°ÂûãË®ìÁ∑¥ÊºîÁÆóÊ≥ï  W(t+2)  (‰∫å)„ÄÅÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø  ËøëÂπ¥‰æÜÊ∑±Â±§Â≠∏ÁøíÁöÑÁõ∏ÈóúÁ†îÁ©∂Â∞áÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊàêÂäüÊáâÁî®Âú®ÂêÑÈ†òÂüüÔºå‰∏ªË¶ÅÂéüÂõ†ÁÇ∫‰∏ÄÁ≥ªÂàóÁöÑË®ìÁ∑¥  ÊºîÁÆóÊ≥ïÂèäÊ®°ÂûãÊû∂ÊßãË¢´ÊèêÂá∫ÔºåÂÖãÊúçÂÇ≥Áµ±ÁöÑÂ§öÂ±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØË®ìÁ∑¥Âõ∞Èõ£ÂïèÈ°å„ÄÇ‰æãÂ¶ÇÂà©Áî® Deep  Belief Network(DBN)‰æÜÂàùÂßãÂåñÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèÉÊï∏ÔºåÂ∏∏Â∏∏ÊúÉÂæóÂà∞ÊØîÈö®Ê©üÂàùÂßãÂåñÁöÑÊñπÊ≥ï‰æÜ  ÂæóÂà∞Êõ¥Â•ΩÁöÑÁµêÊûú[35],[41]„ÄÇÂèàÂ¶Ç Hinton Âà©Áî® rectified linear units (ReLU) ‰ΩúÁÇ∫Èö±ËóèÂ±§ÁöÑÊøÄ  ÁôºÂáΩÊï∏Ôºå‰ª•ÂÖãÊúçÁï∂Á∂≤Ë∑ØÂú®ÂÖ©ÂÄãÊñπÂêëÈÉΩÊé•ËøëÈ£ΩÂíåÊôÇÔºåÈÄ†ÊàêÊ¢ØÂ∫¶ËÆäÂåñÂæàÂ∞èÔºåÊï¥ÂÄãÁ∂≤Ë∑ØÁöÑÂ≠∏Áøí  ËÆäÁöÑÂæàÊÖ¢[35],[42]„ÄÇÊú¨Ë´ñÊñáÁöÑÊ®°ÂûãÂà©Áî® ReLU ‰ΩúÁÇ∫Èö±ËóèÂ±§ÁöÑÊøÄÁôºÂáΩÊï∏„ÄÇÊ≠§ÂáΩÊï∏ÂèØ‰ª•Ë°®Á§∫  ÁÇ∫max‚Å°(0, ùë•)ÔºåÂç≥‰øùÁïôÊ≠£Êï∏ÈÉ®ÂàÜ„ÄÇÁî±ÊñºÂÖ∂Â∞éÂáΩÊï∏ÁÇ∫Â∏∏Êï∏ÔºåÂèØ‰ª•ÈÅøÂÖçÂÖ∏ÂûãÁöÑÊ¢ØÂ∫¶Ê∂àÂ§±(gradient  vanish)ÂïèÈ°å„ÄÇÊú¨Ë´ñÊñáÊé°Áî®Á∂≤Ë∑ØÊ®°ÂûãÊû∂ÊßãÁÇ∫ 1024 Á∂≠ ReLU Èö±ËóèÂ±§Âíå 12 Á∂≠ softmax ‰ΩúÁÇ∫Ëº∏  Âá∫Â±§ÔºåÈ†êÊ∏¨ÊñáÊú¨Â±¨ÊñºÂêÑÂπ¥Á¥öÊïôÊùêÊ©üÁéá„ÄÇÊàëÂÄëÂ∑≤Áü•Ë™≤Á®ãÈõ£ÊòìÂ∫¶ÊòØÊº∏ÈÄ≤Èóú‰øÇÔºåÈö®ËëóÂπ¥Á¥öÊèêÂçá  ËÄåÂÖßÂÆπÊõ¥Ë§áÈõúÔºå‰ΩÜÂÖ∏ÂûãÁöÑÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ≤íÊúâËÄÉÈáèÊ≠§Âõ†Á¥†„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ÁõÆÊ®ôÂáΩÊï∏ÊáâÂä†ÂÖ•  ‰∏ÄÊ≠£ÂâáÈ†ÖÔºåÊªøË∂≥ÂâçËø∞ÁöÑËßÄÂØü„ÄÇ‰ª§Ëº∏Âá∫Â±§Á∂≤Ë∑ØÂèÉÊï∏ÁÇ∫Áü©Èô£ùëäùêøÔºåÂèØ‰ª•Ë°®ÈÅîÁÇ∫ 12 ÂÄãË°åÈ†ÖÈáè  (column vectors)ÔºåÂàÜÂà•Â∞çÊáâÂêÑÂπ¥Á¥öÔºåÂ¶Ç‰∏ãÂºè(1)Ôºö  ùëäùêø = |ùê∞1ùêø‚Å°ùê∞2ùêø ‚Ä¶‚Å°ùê∞1ùêø2|  (1)  Êú¨Ë´ñÊñáÊèêÂá∫ÁöÑÊñπÊ≥ïÁÇ∫ÈôêÂà∂ÈÑ∞ËøëÂπ¥Á¥öÁöÑË°åÈ†ÖÈáèË∑ùÈõ¢ÔºåÁõ∏ÈÑ∞ÁöÑÂπ¥Á¥öÊáâË©≤ÊúâÁõ∏ËøëÁöÑÂêëÈáèÔºå  ÂèØ‰ª•Ëá™ÁÑ∂Âú∞Ë°®ÈÅîÂπ¥Á¥öÁöÑÈÄ£Á∫åÊÄß„ÄÇÊ≠§Ê≠£ÂâáÈ†ÖÂèØ‰ª•Ë°®ÈÅîÁÇ∫‰∏ãÂºè(2)Ôºö  ùëÖ(ùëäùêø) = ‚àë1ùëñ=11‚Äñùê∞ùëñùêø ‚àí ùê∞ùëñùêø+1‚Äñ  (2)  ÊàëÂÄëÂ∏åÊúõÊ≠§Ê≠£ÂâáÈ†ÖÂèØ‰ª•ÈôêÂà∂ÈÑ∞ËøëÂπ¥Á¥öÂêëÈáèÁöÑË∑ùÈõ¢ÔºåËÄåÂÆåÊï¥ÁöÑÊ∏õÊêçÂáΩÊï∏ÁÇ∫‰∫§ÂèâÁÜµ(cross  entropy)ÂíåÊ≠£ÂâáÈ†ÖÁöÑÁµêÂêàÔºåÂ¶Ç‰∏ãÂºè(3)Ôºö  ùêø(ùúÉ) = ‚àí ‚àë1ùëñ=21(ùë¶ùëñ ‚àí log ùë£ùëñùêø) ‚àí ‚àë1ùëó=11‚Äñùê∞ùëó ‚àí ùê∞ùëó+1‚Äñ  (3)  ÂÖ∂‰∏≠ùë¶ùëñÁÇ∫Âπ¥Á¥öÊ®ôË®òÔºåùë£ùëñùêøÁÇ∫Ê®°ÂûãËº∏Âá∫Âπ¥Á¥ö i ÁöÑÊ©üÁéá„ÄÇÊàëÂÄëËóâÊ≠§ÈºìÂãµËº∏Âá∫Â±§ÂèÉÊï∏ÂëàÁèæ 12  Âπ¥Á¥öÁöÑÊµÅÂΩ¢(manifold)ÔºåÂç≥Áõ∏ÈÑ∞ÂÖ©ÂÄãÂπ¥Á¥öÂΩºÊ≠§Âú®Ëº∏Âá∫Â±§Á©∫Èñì‰∏≠Áõ∏Ëøë„ÄÇ  261  (‰∏â)„ÄÅË®ìÁ∑¥ÂèØËÆÄÊÄßÊ®°Âûã Âª∫ÁΩÆË∑®È†òÂüüÊñá‰ª∂ÁöÑÂèØËÆÄÊÄßÊ®°ÂûãÊµÅÁ®ãÂ¶ÇÂúñ 3 ÊâÄÁ§∫ÔºåÊú¨Á†îÁ©∂ÊùêÊñôÈÅ∏Ëá™ 98 Âπ¥Â∫¶Ëá∫ÁÅ£ H„ÄÅK„ÄÅN ‰∏âÂ§ßÂá∫ÁâàÁ§æÊâÄÂá∫ÁâàÁöÑ 1-12 Âπ¥Á¥öÂØ©ÂÆöÁâàÁöÑÂúãË™ûÁßë„ÄÅÁ§æÊúÉÁßë„ÄÅËá™ÁÑ∂ÁßëÂèäÈ´îËÇ≤ÂíåÂÅ•Â∫∑ÊïôËÇ≤Á≠â ÂõõÂÄãÈ†òÂüüÁöÑÊïôÁßëÊõ∏ÂÖ®ÈÉ®ÂÖ±Ë®à 6,230 ÁØáÔºåÂêÑÁâàÊú¨ÊïôÁßëÊõ∏ÂùáÁ∂ìÁî±Â∞àÂÆ∂Ê†πÊìöË™≤Á®ãÁ∂±Ë¶ÅÁ∑®Âà∂ËÄåÊàê„ÄÇ Êú¨Á†îÁ©∂ÂØ¶È©óÊãÜÊàêÂÖ©ÂÄãË≥áÊñôÈõÜÂàÜÂà•ÁÇ∫Ôºö‰∏Ä„ÄÅË≥áÊñôÈõÜ AÔºöÁî±ÂúãË™ûÁßë„ÄÅÁ§æÊúÉÁßëÂèäËá™ÁÑ∂ÁßëÁ≠â‰∏âÂÄã È†òÂüüÁöÑÊïôÁßëÊõ∏ÂÖ±Ë®à 4,648 ÁØá„ÄÇ‰∫å„ÄÅË≥áÊñôÈõÜ BÔºöÁî±ÂúãË™ûÁßë„ÄÅÁ§æÊúÉÁßë„ÄÅËá™ÁÑ∂ÁßëÂèäÈ´îËÇ≤ÂíåÂÅ•Â∫∑ ÊïôËÇ≤Á≠âÂõõÂÄãÈ†òÂüüÁöÑÊïôÁßëÊõ∏ÂÖ±Ë®à 6,230 ÁØá„ÄÇËóâÊ≠§ËßÄÂØüÂú®Ë≥áÊñôÈõÜÊÑàË§áÈõúÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞çÊñºÊîØÂêë ÈáèÊ©üÂèäÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊâÄÈÄ†ÊàêÁöÑÂΩ±ÈüøÁÇ∫‰Ωï„ÄÇËÄåÊï¥ÂØ¶È©óÁöÑÊµÅÁ®ãÁöÜÊé°Áî® 5-fold ‰∫§‰∫íÈ©óË≠âÁöÑÊñπ ÂºèÈÄ≤Ë°åÔºåÈ¶ñÂÖàÂ∞áÊñá‰ª∂Âà©Áî® WECAn[43]‰æÜÈÄ≤Ë°å‰∏≠ÊñáÊñá‰ª∂ÁöÑÊñ∑Ë©ûÔºåÂÜçÂ∞áË®ìÁ∑¥Ë≥áÊñôÂà©Áî® Word2Vec[38]‰æÜÂàÜÂà•ÂæóÂà∞ÈÄ£Á∫åË©ûË¢ãÊ®°ÂûãË©ûÂêëÈáèÂ∞çÁÖßË°®„ÄÇÊé•ËëóÂ∞áË®ìÁ∑¥Ë≥áÊñôÁöÑÊØè‰∏ÄÁØáË™≤Êñá‰æù Êìö‰ΩøÁî®Âà∞ÁöÑË©ûÂΩôÂæûË©ûÂêëÈáèÂ∞çÁÖßË°®‰∏≠ÂèñÂá∫ÂêëÈáèÔºå‰∏¶Â∞áÈÄô‰∫õÂêëÈáèÂÖ®ÈÉ®Áõ∏Âä†ÔºåÊúÄÂæåÊâÄÂæóÂà∞ÁöÑÂêë Èáè‰æøÊòØÈÄô‰∏ÄÁØáË™≤ÊñáÁöÑÂèØËÆÄÊÄßÁâπÂæµÔºåËÄåÂÆÉÁöÑÈ°ûÂà•Â∞±ÊòØË™≤ÊñáÊâÄÂ±¨ÁöÑÂπ¥Á¥ö„ÄÇÊú¨Á†îÁ©∂ÂàÜÂà•Âà©Áî® Keras [44] Âíå LIBSVM[45]‰æÜË®ìÁ∑¥Âá∫Ê∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂèäÊîØÂêëÈáèÊ©üÂèØËÆÄÊÄßÊ®°Âûã„ÄÇÂú®È©óË≠âÂèØ ËÆÄÊÄßÊ®°ÂûãÁöÑÈöéÊÆµÔºåÊú¨Á†îÁ©∂Â∞áÊ∏¨Ë©¶Ë≥áÊñô‰ΩøÁî®Âà∞ÁöÑË©ûÂΩô‰∏ÄÊ®£ÂæûË©ûÂêëÈáèÂ∞çÁÖßË°®ÂèñÂá∫ÂêëÈáèÔºå‰∏¶Â∞á ÈÄô‰∫õÂêëÈáèÂÖ®ÈÉ®Áõ∏Âä†ÔºåÂ¶ÇÈÅáÂà∞Ë©ûÂêëÈáèÂ∞çÁÖßË°®Ê≤íÊúâÁöÑË©ûÂΩôÊôÇÔºåÂâá‰∏çËôïÁêÜË©≤Ë©ûÂΩô„ÄÇÂú®ÂèñÂæóÊ∏¨Ë©¶ Ë≥áÊñôÁöÑÂèØËÆÄÊÄßÁâπÂæµÂæåÔºå‰æøÂèØËº∏ÂÖ•Ëá≥Â∑≤Ë®ìÁ∑¥Â•ΩÁöÑÂèØËÆÄÊÄßÊ®°Âûã‰æÜÈ†êÊ∏¨Êñá‰ª∂ÁöÑÂπ¥Á¥öÂÄº„ÄÇ Ë≥áÊñôÈõÜA(4648ÁØá) Ë≥áÊñôÈõÜB(6230ÁØá) Êñ∑Ë©û  Ë®ìÁ∑¥Ë≥áÊñô  Ê∏¨Ë©¶Ë≥áÊñô  Word Embedding  Ë©ûÂêëÈáèÂ∞çÁÖßË°®  Ê†πÊìöË≥áÊñôÂä†Á∏Ω Ë©ûÂêëÈáè SVM / DNN SVM Ê®°Âûã / DNN Ê®°Âûã ÊñáÊú¨ÂèØËÆÄÊÄß È†êÊ∏¨ÁµêÊûú  5-fold cross validation  Âúñ‰∏â„ÄÅÂèØËÆÄÊÄßÊ®°ÂûãË®ìÁ∑¥ÂèäÊ∏¨Ë©¶ÊµÅÁ®ãÂúñ  262  Âõõ„ÄÅÂØ¶È©óÁµêÊûú Êú¨Á†îÁ©∂ÁöÑÂØ¶È©óÁµêÊûúÂ¶ÇË°®‰∏ÄÂíåË°®‰∫åÊâÄÁ§∫ÔºåËÄåÂõõÁ®ÆÂèØËÆÄÊÄßÊ®°ÂûãÁöÑÈåØË™§Áü©Èô£ÂàÜÂà•Â¶ÇË°®‰∏â„ÄÅË°®Âõõ„ÄÅ Ë°®‰∫îÂèäË°®ÂÖ≠„ÄÇÂæûÁµêÊûúÂèØ‰ª•ÁôºÁèæ‰∏çË´ñÊòØÂú®‰∏âÁ®ÆÈ†òÂüüÊñáÊú¨ÈÇÑÊòØÂõõÁ®ÆÈ†òÂüüÊñáÊú¨ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊ∑±Â±§ È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊ∫ñÁ¢∫ÁéáÈÉΩÂÑ™ÊñºÊîØÂêëÈáèÊ©ü„ÄÇÁÑ∂ËÄåÊàëÂÄë‰πü‰∏çÈõ£ÁôºÁèæÂú®Âä†ÂÖ• 1,582 ÁØáÁöÑÈ´îËÇ≤ÂíåÂÅ• Â∫∑ÊïôËÇ≤ÂæåÔºåÊ®°ÂûãÂàÜÈ°ûÁöÑÈõ£Â∫¶Â§ßÂπÖÂ∫¶ÁöÑ‰∏äÂçá„ÄÇÂ∞çÊîØÂêëÈáèÊ©üÊ®°ÂûãËÄåË®ÄÔºåÊ∫ñÁ¢∫ÁéáÊ∏õÂ∞ë‰∫Ü 9.5% ÁöÑÊ∫ñÁ¢∫ÁéáÔºåËÄåÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÂè™ÊúâÊ∏õÂ∞ë‰∫Ü 7.32%ÁöÑÊ∫ñÁ¢∫Áéá„ÄÇÈÄôÈ°ØÁ§∫Âú®ÊñáÊú¨ÂèØËÆÄÊÄßÂàÜÈ°ûÁöÑ ÈÄôÂÄãÁ†îÁ©∂È†òÂüü‰∏≠ÔºåÊ∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊØîÊîØÂêëÈáèÊ©üÊõ¥ËÉΩÂ§†ËôïÁêÜÊõ¥ÁÇ∫Ë§áÈõúÁöÑË≥áÊñô„ÄÇ  Ë°®‰∏Ä„ÄÅÂØ¶È©ó‰∏ÄÔºöÈÄ£Á∫åË©ûË¢ãÊ®°Âûã 100 Á∂≠Â∫¶‰πã‰∏âÁ®ÆÈ†òÂüüÊñáÊú¨ÊïàËÉΩÊØîËºÉ  ÈÅ©Áî®Âπ¥Á¥ö 1-12 Âπ¥Á¥ö  ÈÅ©Áî®È†òÂüü  ÂàÜÈ°ûÊºîÁÆóÊ≥ï  ÂúãË™û„ÄÅÁ§æÊúÉ„ÄÅËá™ÁÑ∂ÂÖ±Ë®à ÊîØÂêëÈáèÊ©ü  4,648 ÁØá  Ê∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø  Ê∫ñÁ¢∫Áéá(%) 70.83 74.27  Ë°®‰∫å„ÄÅÂØ¶È©ó‰∫åÔºöÈÄ£Á∫åË©ûË¢ãÊ®°Âûã 100 Á∂≠Â∫¶‰πãÂõõÁ®ÆÈ†òÂüüÊñáÊú¨ÊïàËÉΩÊØîËºÉ  ÈÅ©Áî®Âπ¥Á¥ö  ÈÅ©Áî®È†òÂüü  ÂàÜÈ°ûÊºîÁÆóÊ≥ï  Ê∫ñÁ¢∫Áéá(%)  1-12 Âπ¥Á¥ö  ÂúãË™û„ÄÅÁ§æÊúÉ„ÄÅËá™ÁÑ∂„ÄÅ ÊîØÂêëÈáèÊ©ü  È´îËÇ≤ÂíåÂÅ•Â∫∑ÊïôËÇ≤ÂÖ±Ë®à  6,230 ÁØá  Ê∑±Â±§È°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø  61.33 66.95  263  Ë°®‰∏â„ÄÅÂØ¶È©ó‰∏ÄÊîØÂêëÈáèÊ©üÊ®°Âûã‰πãÈåØË™§Áü©Èô£  Ê®°ÂûãÈ†ê‰º∞Âπ¥Á¥ö  Ê∫ñÁ¢∫Áéá  
This paper presents the challenges and the methodology followed in the design of a new Input Method (IME) for the Taiwanese (Hokkien) language. We first describe the context, the motivations and some of the main issues related to the input of text in Taiwanese on modern computer systems and mobile devices. Then we present the available resources which our system is based on. We will describe the whole architecture of our system. But since the cornerstone of modern IME is the Language Model (LM), the main Natural Language Processing issue on which we will focus in this paper is the estimation of a LM in the case of this under-resourced language. The solution we propose to rely on unsupervised word segmentation which preserves some degree of ambiguity. Keywords: Unsupervized Word Segmentation, Language Modeling, Input Method, Taiwanese 1. Introduction Taiwanese Hokkien (or simply ‚ÄúTaiwanese,‚Äù T√¢i-g√≠ Âè∞Ë™û, throughout the rest of the paper) is a language spoken by a vast majority of the Taiwanese people. It is a Sinitic language of the Minnan (b√¢n-l√¢m-g√≠, Èñ©ÂçóË™û) group. Since our work is based on readily available resources which describe the variety in use in Taiwan, it is better fitted for Taiwanese, but it may be useful to more than 60M people in and outside Taiwan who speak closely related variants. Although this language is still widely spoken in Taiwan, Taiwanese has never been the official language, the efforts in standardization and institutionalization only started in the last decades. 284  Even without state-run institutionalization, written Taiwanese has been in use in printed and handwritten documents for centuries. Depending on the situation, different scripts have been used, including H√†n characters (H√†n-jƒ´), Latin alphabet, adapted versions of Japanese kana and Zhuyin fuhao (Ê≥®Èü≥Á¨¶Ëôü). Nowadays, H√†n-jƒ´ and Latin are the two scripts which cover the vast majority of produced texts. Zhuyin is mostly used for annotation of rare H√†n characters or in teaching materials, and for code-mixing in spontaneous writing. Texts written using the Latin script can be divided between different Romanization types, the two more important which are encountered in our resources are PehÃç -≈çe-jƒ´ (POJ) and T√¢i-l√¥ (‚ÄúTaiwan Romanization System‚Äù, hereafter TRS). The first one is also called ‚ÄúChurch Romanization‚Äù due to its origin in missionary works and the latter is recommended by Taiwan‚Äôs Ministry of Education since 2006. As a result, the actual situation of written Taiwanese is an interesting case of polyorthography where one scripter can choose between H√†n-jƒ´ and Romanization or (more frequently) mix the two scripts. This requires some specific features from an IME. In the past decades, the status of Taiwanese at school has changed from being forbidden to being taught in classes of ‚ÄúMother Languages‚Äù in primary schools. However the curriculum is still very limited and even if a large majority of Taiwanese people can speak the language, only a very small proportion is actually literate in Taiwanese. However, almost all Taiwanese are familiar with H√†n-jƒ´ and Zhuyin phonetic transcription (taught to be used for writing Mandarin down). This recent history also leads to a very limited place for Taiwanese in the computing world and this language is usually neglected by computer software designers (even its ISO code ‚Äònan‚Äô is very rarely recognized as an option). In addition to the various political and sociolinguistics factors which may lead some to consider Taiwanese Hokkien as an endangered language, we want to stress the impact of the ease to use a language on modern devices. The possibility and the convenience to input texts seem to us to be of first importance to ensure language preservation. This is especially the case for Taiwanese as modern technologies are omnipresent in Taiwan and an important part of language use among 285  Taiwanese people is made online. For more details about the history of written Taiwanese, interested readers may refer to [1]. For an overview of the current state of Taiwanese text processing, one can refer to [2]. Multiple Input Methods (IME) have already been developed for desktop computers by different organizations over the years, the most noticeable being probably the FHL‚Äôs Taigi IME1 to type in POJ and Âê≥ ÂÆà Á¶Æ Ëá∫ Ë™û Ê≥® Èü≥ Ëº∏ ÂÖ• Ê≥ï 2 to type in Zhuyin. The Ministry of Education also provides an IME for desktop computers3. More details about available IMEs can be found in [2] (p. 144). As mobile devices progressively took the largest share of online communications, IMEs for Taiwanese did not follow and no IME was available on mobiles until very recently (2014 for our own first try on Android4 and 2016 for iOS5). We believe that not only such softwares are crucially needed, but they also have to catch up with state-of-the-art Mandarin IME. For now, they are still behind in terms of functionalities, performance and convenience to be adopted by a large number of users (who are typically bilingual with Mandarin). There is still a long way to go. Our objective is thus to design an IME for Taiwanese on mobile devices which would benefit from modern NLP techniques. To do so, we need efficient Language Models (LM) to provide smarter candidate ranking and prediction. LMs are the cornerstone of modern IMEs for such features. However, unlike Mandarin, Taiwanese lacks of linguistically annotated resources such as segmented corpora to train word-level models. This pushed us to look for unsupervised solutions to be able to benefit from (raw) language corpora without the need for costly and time consuming manual annotation. In this paper, we will present the core architecture of our IME, with a special focus on how we address word segmentation to train the LM needed for input prediction. 
Based on the assumption that comment with positive sentimental polarity to a negative issue has high probability to be a sarcasm, we propose a simple yet efficient method to collect sarcastic textual data by crowdsourcing with social media and merging game with a purpose approach. Taking advantage of Facebook's reaction button, posts triggering strong negative emotion are collected. Next, by using PTT's search engine, we successfully connect PTT's comments to the collected posts in Facebook and build the sarcasm corpus. Based on the corpus data, the performance comparison of sarcasm detection between SVM with na√Øve features and Convolutional Neural Network models is conducted. An impressive accuracy rate and great potentials of the corpus are demonstrated. Keywords: sarcasm, PTT, convolutional neural network, support vector machine, crowdsourcing. 1. Introduction Sentiment analysis is important in automatic interpreting large number of feedbacks from the internet society. However, the usage of sarcasm which typically conveys a negative opinion using positive words could flip the polarity of a message thus interfere the accuracy of the sentiment analysis (Maynard et al. 2014). Therefore, to improve the performance of sentiment analysis model, detection of sarcasm is definitely necessary (Bo et al. 2008). 299  Linguistically, sarcasm has been regarded as a complicated speech act which utters the opposite of what it literally means, and it distinguishes itself with irony in its intention of making the target the butt of derisive contempt (Ling et al. 2016). Sarcasm can be grammaticalized and lexicalized in various patterns, and often requires context-dependent readings with human involvement. Therefore, the construction of sarcasm corpus providing wider windows as well as training data for predictive model has long been considered as an uneasy task.  However, with the rapid growth of social media platform like Twitter and Facebook, a new  solution is provided via crowdsourcing. For instance, a popular method in previous studies,  some groups  -  
With high parsing accuracy with rich semantic representations, English Resource Semantics is a valuable source of information for many semantically¬≠sensitive NLP tasks. ERS¬≠based systems have achieved state¬≠of¬≠the¬≠art results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE taskÕæ Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transfer¬≠based MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 
The first two categories regard answering factoid questions, where the main difference of the problem settings is the information source used for extracting answers. QA with knowledge base aims to answer natural language questions using real-world facts stored in an existing, large-scale database. The representative approach for this task is to develop a semantic parser (of questions), which will be the main focus. Other approaches like text matching in the embedding space and those driven by information extraction will also be discussed. The other category, QA with the Web, targets answering questions using mainly from the facts extracted from general text corpora derived from the Web. In addition to the common components and techniques used in this setting, including passage retrieval, entity recognition and question analysis, we will also introduce latest work on how to leverage and incorporate additional structured and semi-structured data to improve the performance. The third category of the QA problems that we will highlight is the non-factoid questions. Due to its broad coverage, we will briefly cover three exemplary topics: story comprehension, reasoning questions and paragraph QA. The tutorial will conclude by summarizing a whole area of exciting and dynamic research that is worthy of more detailed investigation for many years to come. 
‚óè With end¬≠to¬≠end models taking on more complex tasks, the design of architecture and mechanisms often needs more domain knowledge from linguists and other domain experts. 
Related languages share a lot of linguistic features and the divergences among them are at a lower level of the NLP pipeline. The objective of the tutorial is to discuss how the relatedness among languages can be leveraged to bridge this language divergence thereby achieving some/all of these goals: (i) improving translation quality, (ii) achieving better generalization, (iii) sharing linguistic resources, and (iv) reducing resource requirements. 
This paper presents rstWeb, a new browserbased interface for Rhetorical Structure Theory and other discourse relation annotations. Expanding on previous tools for RST, rstWeb allows annotators to work online using only a browser. Project administrators can easily collect multiple annotations of the same documents on a central server, keep track of annotation processes and assign tasks and annotation schemes to users. A local version using an embedded web framework is also available, running offline on a desktop browser under the localhost. 
We present the design and evaluation of a web-based peer review system that uses natural language processing to automatically evaluate and provide instant feedback regarding the presence of solutions in peer reviews. Student reviewers can then choose to either revise their reviews to address the system‚Äôs feedback, or ignore the feedback and submit their original reviews. A system deployment in multiple high school classrooms shows that our solution prediction model triggers instant feedback with high precision, and that the feedback is successful in increasing the number of peer reviews with solutions. 
 In this paper, we present Farasa, a fast and accurate Arabic segmenter. Our approach is based on SVM-rank using linear kernels. We measure the performance of the segmenter in terms of accuracy and efÔ¨Åciency, in two NLP tasks, namely Machine Translation (MT) and Information Retrieval (IR). Farasa outperforms or is at par with the stateof-the-art Arabic segmenters (Stanford and MADAMIRA), while being more than one order of magnitude faster.  
We present iAppraise: an open-source framework that enables the use of eye-tracking for MT evaluation. It connects Appraise, an opensource toolkit for MT evaluation, to a low-cost eye-tracking device, to make its usage accessible to a broader audience. It also provides a set of tools for extracting and exploiting gaze data, which facilitate eye-tracking analysis. In this paper, we describe different modules of the framework, and explain how the tool can be used in a MT evaluation scenario. During the demonstration, the users will be able to perform an evaluation task, observe their own reading behavior during a replay of the session, and export and extract features from the data. 
This paper introduces Linguistica 5, a software for unsupervised learning of linguistic structure. It is a descendant of Goldsmith‚Äôs (2001, 2006) Linguistica. Open-source and written in Python, the new Linguistica 5 is both a graphical user interface software and a Python library. While Linguistica 5 inherits its predecessors‚Äô strength in unsupervised learning of natural language morphology, it incorporates signiÔ¨Åcant improvements in multiple ways. Notable new features include tools for data visualization as well as straightforward extensions for both its components and embedding in other programs. 
In this paper, we use multilingual Natural Language Processing (NLP) tools to improve the reading experience of parallel texts on mobile devices. Such enterprise poses multiple challenging issues both from the NLP and from the Human Computer Interaction (HCI) perspectives. We discuss these problems, and report on our own solutions, now implemented in a full-Ô¨Çedged bilingual reading device. 
New Dimensions in Testimony is a prototype dialogue system that allows users to conduct a conversation with a real person who is not available for conversation in real time. Users talk to a persistent representation of Holocaust survivor Pinchas Gutter on a screen, while a dialogue agent selects appropriate responses to user utterances from a set of pre-recorded video statements, simulating a live conversation. The technology is similar to existing conversational agents, but to our knowledge this is the Ô¨Årst system to portray a real person. The demonstration will show the system on a range of screens (from mobile phones to large TVs), and allow users to have individual conversations with Mr. Gutter. 
While intelligent writing assistants have become more common, they typically have little support for revision behavior. We present ArgRewrite, a novel web-based revision assistant that focus on rewriting analysis. The system supports two major functionalities: 1) to assist students as they revise, the system automatically extracts and analyzes revisions; 2) to assist teachers, the system provides an overview of students‚Äô revisions and allows teachers to correct the automatically analyzed results, ensuring that students get the correct feedback. 
Word clusters improve performance in many NLP tasks including training neural network language models, but current increases in datasets are outpacing the ability of word clusterers to handle them. In this paper we present a novel bidirectional, interpolated, reÔ¨Åning, and alternating (BIRA) predictive exchange algorithm and introduce ClusterCat, a clusterer based on this algorithm. We show that ClusterCat is 3‚Äì85 times faster than four other well-known clusterers, while also improving upon the predictive exchange algorithm‚Äôs perplexity by up to 18% . Notably, ClusterCat clusters a 2.5 billion token English News Crawl corpus in 3 hours. We also evaluate in a machine translation setting, resulting in shorter training times achieving the same translation quality measured in BLEU scores. ClusterCat is portable and freely available. 
We demonstrate the Task Completion Platform (TCP); a multi-domain dialogue platform that can host and execute large numbers of goal-orientated dialogue tasks. The platform features a task conÔ¨Åguration language, TaskForm, that allows the deÔ¨Ånition of each individual task to be decoupled from the overarching dialogue policy used by the platform to complete those tasks. This separation allows for simple and rapid authoring of new tasks, while dialogue policy and platform functionality evolve independent of the tasks. The current platform includes machine learnt models that provide contextual slot carry-over, Ô¨Çexible item selection, and task selection/switching. Any new task immediately gains the beneÔ¨Åt of these pieces of built-in platform functionality. The platform is used to power many of the multi-turn dialogues supported by the Cortana personal assistant. 
There has been a recent interest in understanding text to perform mathematical reasoning. In particular, most of these efforts have focussed on automatically solving school level math word problems. In order to make advancements in this area accessible to people, as well as to facilitate this line of research, we release the ILLINOIS MATH SOLVER, a web based tool that supports performing mathematical reasoning. ILLINOIS MATH SOLVER can answer a wide range of mathematics questions, ranging from compositional operation questions like ‚ÄúWhat is the result when 6 is divided by the sum of 7 and 5 ?‚Äù to elementary school level math word problems, like ‚ÄúI bought 6 apples. I ate 3 of them. How many do I have left ?‚Äù. The web based demo can be used as a tutoring tool for elementary school students, since it not only outputs the Ô¨Ånal result, but also the mathematical expression to compute it. The tool will allow researchers to understand the capabilities and limitations of a state of the art arithmetic problem solver, and also enable crowd based data acquisition for mathematical reasoning. The system is currently online at https://cogcomp.cs.illinois. edu/page/demo_view/Math. 
LingoTurk is an open-source, freely available crowdsourcing client/server system aimed primarily at psycholinguistic experimentation where custom and specialized user interfaces are required but not supported by popular crowdsourcing task management platforms. LingoTurk enables user-friendly local hosting of experiments as well as condition management and participant exclusion. It is compatible with Amazon Mechanical Turk and ProliÔ¨Åc Academic. New experiments can easily be set up via the Play Framework and the LingoTurk API, while multiple experiments can be managed from a single system. 
We present a simple, prepackaged solution to generating paraphrases of English sentences. We use the Paraphrase Database (PPDB) for monolingual sentence rewriting and provide machine translation language packs: prepackaged, tuned models that can be downloaded and used to generate paraphrases on a standard Unix environment. The language packs can be treated as a black box or customized to speciÔ¨Åc tasks. In this demonstration, we will explain how to use the included interactive webbased tool to generate sentential paraphrases. 
This paper presents a tag-based statistical math word problem solver with understanding, reasoning, and explanation. It analyzes the text and transforms both body and question parts into their tag-based logic forms, and then performs inference on them. The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information, which can be used to identify the desired operand and filter out irrelevant quantities. The proposed approach is thus less sensitive to the irrelevant information and could provide the answer more precisely. Also, it can handle much more problem types other than addition and subtraction. 
The sheer volume of unstructured multimedia data (e.g., texts, images, videos) posted on the Web during events of general interest is overwhelming and difÔ¨Åcult to distill if seeking information relevant to a particular concern. We have developed a comprehensive system that searches, identiÔ¨Åes, organizes and summarizes complex events from multiple data modalities. It also recommends events related to the user‚Äôs ongoing search based on previously selected attribute values and dimensions of events being viewed. In this paper we brieÔ¨Çy present the algorithms of each component and demonstrate the system‚Äôs capabilities 1. 
We demonstrate SODA (Service Oriented Domain Adaptation) for efÔ¨Åcient and scalable cross-domain microblog categorization which works on the principle of transfer learning. It is developed on a novel similarity-based iterative domain adaptation algorithm while extended with features such as active learning and interactive GUI to be used by business professionals. SODA demonstrates efÔ¨Åcient classiÔ¨Åcation accuracy on new collections while minimizing and sometimes eliminating the need for expensive data labeling efforts. SODA also implements an active learning (AL) technique to select informative instances from the new collection to seek annotations, if a small amount of labeled data is required by the adaptation algorithm. 
Foreign students at German universities often have difÔ¨Åculties following lectures as they are often held in German. Since human interpreters are too expensive for universities we are addressing this problem via speech translation technology deployed in KIT‚Äôs lecture halls. Our simultaneous lecture translation system automatically translates lectures from German to English in real-time. Other supported language directions are English to Spanish, English to French, English to German and German to French. Automatic simultaneous translation is more than just the concatenation of automatic speech recognition and machine translation technology, as the input is an unsegmented, practically inÔ¨Ånite stream of spontaneous speech. The lack of segmentation and the spontaneous nature of the speech makes it especially difÔ¨Åcult to recognize and translate it with sufÔ¨Åcient quality. In addition to quality, speed and latency are of the utmost importance in order for the system to enable students to follow lectures. In this paper we present our system that performs the task of simultaneous speech translation of university lectures by performing speech translation on a stream of audio in real-time and with low latency. The system features several techniques beyond the basic speech translation task, that make it Ô¨Åt for real-world use. Examples of these features are a continuous stream speech recognition without any prior segmentation of the input audio, punctuation prediction, run-on decoding and run-on translation with continuously updating displays in order to keep the latency as low as possible.  
 Zara the Supergirl is an interactive system that, while having a conversation with a user, uses its built in sentiment analysis, emotion recognition, facial and speech recognition modules, to exhibit the human-like response of sharing emotions. In addition, at the end of a 5-10 minute conversation with the user, it can give a comprehensive personality analysis based on the user‚Äôs interaction with Zara. This is a Ô¨Årst prototype that has incorporated a full empathy module, the recognition and response of human emotions, into a spoken language interactive system that enhances human-robot understanding. Zara was shown at the World Economic Forum in Dalian in September 2015. 
In this paper, we present Kathaa1, an open source web based Visual Programming Framework for NLP applications. It supports design, execution and analysis of complex NLP systems by choosing and visually connecting NLP modules from an already available and easily extensible Module library. It models NLP systems as a Directed Acyclic Graph of optionally parallalized information Ô¨Çow, and lets the user choose and use available modules in their NLP applications irrespective of their technical proÔ¨Åciency. Kathaa exposes a precise Module deÔ¨Ånition API to allow easy integration of external NLP components (along with their associated services as docker containers), it allows everyone to publish their services in a standardized format for everyone else to use it out of the box. 
Despite widespread adoption in NLP, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. In this work, we describe LIME, a novel explanation technique that explains the predictions of any classiÔ¨Åer in an interpretable and faithful manner. We further present a method to explain models by presenting representative individual predictions and their explanations in a non-redundant manner. We propose a demonstration of these ideas on different NLP tasks such as document classiÔ¨Åcation, politeness detection, and sentiment analysis, with classiÔ¨Åers like neural networks and SVMs. The user interactions include explanations of free-form text, challenging users to identify the better classiÔ¨Åer from a pair, and perform basic feature engineering to improve the classiÔ¨Åers. 
We present an end-to-end method for learning verb-speciÔ¨Åc semantic frames with feedforward neural network (FNN). Previous works in this area mainly adopt a multi-step procedure including part-of-speech tagging, dependency parsing and so on. On the contrary, our method uses a FNN model that maps verbspeciÔ¨Åc sentences directly to semantic frames. The simple model gets good results on annotated data and has a good generalization ability. Finally we get 0.82 F-score on 63 verbs and 0.73 F-score on 407 verbs. 
Following up on numerous reports of analogybased identiÔ¨Åcation of ‚Äúlinguistic regularities‚Äù in word embeddings, this study applies the widely used vector offset method to 4 types of linguistic relations: inÔ¨Çectional and derivational morphology, and lexicographic and encyclopedic semantics. We present a balanced test set with 99,200 questions in 40 categories, and we systematically examine how accuracy for different categories is affected by window size and dimensionality of the SVD-based word embeddings. We also show that GloVe and SVD yield similar patterns of results for different categories, offering further evidence for conceptual similarity between count-based and neural-net based models. 
In this paper, we develop and evaluate several techniques for identifying argumentative paragraphs in Chinese editorials. We Ô¨Årst use three methods of evaluation to score a paragraph‚Äôs argumentative nature: a relative word frequency approach; a method which targets known argumentative words in our corpus; and a combined approach which uses elements from the previous two. Then, we determine the best score thresholds for separating argumentative and non-argumentative paragraphs. The results of our experimentation show that our relative word frequency approach provides a reliable way to identify argumentative paragraphs with a F1 score of 0.91, though challenges in accurate scoring invite improvement through context-aware means. 
This paper proposes an automatic tag assignment approach to various e-commerce products where tag allotment is done solely based on the visual features in the image. It then builds a tag based product retrieval system upon these allotted tags. The explosive growth of e-commerce products being sold online has made manual annotation infeasible. Without such tags it‚Äôs impossible for customers to be able to Ô¨Ånd these products. Hence a scalable approach catering to such large number of product images and allocating meaningful tags is essential and could be used to make an efÔ¨Åcient tag based product retrieval system. In this paper we propose one such approach based on feature extraction using Deep Convolutional Neural Networks to learn descriptive semantic features from product images. Then we use inverse distance weighted K-nearest neighbours classiÔ¨Åers along with several other multi-label classiÔ¨Åcation approaches to assign appropriate tags to our images. We demonstrate the functioning of our algorithm for the Amazon product dataset for various categories of products like clothing and apparel, electronics, sports equipment etc. Keywords: Content based image retrieval, Multi-Modal data embeddings and search, Automatic Image Annotation, E-commerce product categorization 
This thesis proposal outlines the use of unsupervised data-driven methods for paraphrasing tasks. We motivate the development of knowledge-free methods at the guiding use case of multi-document summarization, which requires a domain-adaptable system for both the detection and generation of sentential paraphrases. First, we deÔ¨Åne a number of guiding research questions that will be addressed in the scope of this thesis. We continue to present ongoing work in unsupervised lexical substitution. An existing supervised approach is Ô¨Årst adapted to a new language and dataset. We observe that supervised lexical substitution relies heavily on lexical semantic resources, and present an approach to overcome this dependency. We describe a method for unsupervised relation extraction, which we aim to leverage in lexical substitution as a replacement for knowledge-based resources. 
Spammer detection on social network is a challenging problem. The rigid anti-spam rules have resulted in emergence of "smart" spammers. They resemble legitimate users who are difÔ¨Åcult to identify. In this paper, we present a novel spammer classiÔ¨Åcation approach based on Latent Dirichlet Allocation (LDA), a topic model. Our approach extracts both the local and the global information of topic distribution patterns, which capture the essence of spamming. Tested on one benchmark dataset and one self-collected dataset, our proposed method outperforms other stateof-the-art methods in terms of averaged F1score. 
Sindhi, an Indo-Aryan language with more than 75 million native speakers1 is a resourcepoor language in terms of the availability of language technology tools and resources. In this thesis, we discuss the approaches taken to develop resources and tools for a resourcepoor language with special focus on Sindhi. The major contributions of this work include raw and annotated datasets, a POS Tagger, a Morphological Analyser, a Transliteration and a Machine Translation System. 
We conducted an ArtiÔ¨Åcial Language Learning experiment to examine the production behavior of language learners in a dynamic communicative setting. Participants were exposed to a miniature language with two optional formal devices and were then asked to use the acquired language to transfer information in a cooperative game. The results showed that language learners optimize their use of the optional formal devices to transfer information efÔ¨Åciently and that they avoid the production of ambiguous information. These results could be used within the context of a language model such that the model can more accurately reÔ¨Çect the production behavior of human language learners. 
Shallow discourse parsing enables us to study discourse as a coherent piece of information rather than a sequence of clauses, sentences and paragraphs. In this paper, we identify arguments of explicit discourse relations in Hindi. This is the Ô¨Årst such work carried out for Hindi. Building upon previous work carried out on discourse connective identiÔ¨Åcation in Hindi, we propose a hybrid pipeline which makes use of both sub-tree extraction and linear tagging approaches. We report state-ofthe-art performance for this task. 
We examine if common machine learning techniques known to perform well in coarsegrained emotion and sentiment classification can also be applied successfully on a set of fine-grained emotion categories. We first describe the grounded theory approach used to develop a corpus of 5,553 tweets manually annotated with 28 emotion categories. From our preliminary experiments, we have identified two machine learning algorithms that perform well in this emotion classification task and demonstrated that it is feasible to train classifiers to detect 28 emotion categories without a huge drop in performance compared to coarser-grained classification schemes. 
The translation of patents or scientiÔ¨Åc papers is a key issue that should be helped by the use of statistical machine translation (SMT). In this paper, we propose a method to improve Chinese‚ÄìJapanese patent SMT by premarking the training corpus with aligned bilingual multi-word terms. We automatically extract multi-word terms from monolingual corpora by combining statistical and linguistic Ô¨Åltering methods. We use the sampling-based alignment method to identify aligned terms and set some threshold on translation probabilities to select the most promising bilingual multi-word terms. We pre-mark a Chinese‚Äì Japanese training corpus with such selected aligned bilingual multi-word terms. We obtain the performance of over 70% precision in bilingual term extraction and a signiÔ¨Åcant improvement of BLEU scores in our experiments on a Chinese‚ÄìJapanese patent parallel corpus. 
Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many social media services address the problem of identifying hate speech, but the deÔ¨Ånition of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lomas, 2015). We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hatespeech detection. We also present a dictionary based the most indicative words in our data. 
Extractive summarization techniques typically aim to maximize the information coverage of the summary with respect to the original corpus and report accuracies in ROUGE scores. Automated text summarization techniques should consider the dimensions of comprehensibility, coherence and readability. In the current work, we identify the discourse structure which provides the context for the creation of a sentence. We leverage the information from the structure to frame a monotone (non-decreasing) sub-modular scoring function for generating comprehensible summaries. Our approach improves the overall quality of comprehensibility of the summary in terms of human evaluation and gives sufÔ¨Åcient content coverage with comparable ROUGE score. We also formulate a metric to measure summary comprehensibility in terms of Contextual Independence of a sentence. The metric is shown to be representative of human judgement of text comprehensibility. 
Computational phylogenetics has been shown to be effective over grammatical characteristics. Recent work suggests that constraintbased formalisms are compatible with such an approach (Eden, 2013). In this paper, we report on simulations to determine how useful constraint-based formalisms are in phylogenetic research and under what conditions. 
In the task of question answering, Memory Networks have recently shown to be quite effective towards complex reasoning as well as scalability, in spite of limited range of topics covered in training data. In this paper, we introduce Factual Memory Network, which learns to answer questions by extracting and reasoning over relevant facts from a Knowledge Base. Our system generate distributed representation of questions and KB in same word vector space, extract a subset of initial candidate facts, then try to Ô¨Ånd a path to answer entity using multi-hop reasoning and reÔ¨Ånement. Additionally, we also improve the run-time efÔ¨Åciency of our model using various computational heuristics.  pairs as training set, instead of strong supervision in the form of (question, associated facts in KB) pairs. The major contributions of this paper are twofold: Ô¨Årst, we introduce factual memory networks, which are used to answer questions in natural language (e.g ‚ÄúWhere was Bill Gates born?‚Äù) using facts stored in the form of (subject, predicate, object) triplets in a KB (e.g (‚ÄôBill Gates‚Äô, ‚Äôplace of birth‚Äô, ‚ÄôSeattle‚Äô)). We evaluate our system against current baselines on various benchmark datasets. Since KBs can be extremely large, making it computationally inefÔ¨Åcient to search over all entities and paths, our second goal of this paper is to increase the efÔ¨Åciency of our model in terms of various performance measures and provide better coverage of relevant facts, by intelligently selecting which nodes to expand.  
The success of many language modeling methods and applications relies heavily on the amount of data available. This problem is further exacerbated in statistical machine translation, where parallel data in the source and target languages is required. However, large amounts of data are only available for a small number of languages; as a result, many language modeling techniques are inadequate for the vast majority of languages. In this paper, we attempt to lessen the problem of a lack of training data for low-resource languages by adding data from related high-resource languages in three experiments. First, we interpolate language models trained on the target language and on the related language. In our second experiment, we select the sentences most similar to the target language and add them to our training corpus. Finally, we integrate data from the related language into a translation model for a statistical machine translation application. Although we do not see many signiÔ¨Åcant improvements over baselines trained on a small amount of data in the target language, we discuss some further experiments that could be attempted in order to augment language models and translation models with data from related languages. 
Automatic Machine Translation metrics, such as BLEU, are widely used in empirical evaluation as a substitute for human assessment. Subsequently, the performance of a given metric is measured by its strength of correlation with human judgment. When a newly proposed metric achieves a stronger correlation over that of a baseline, it is important to take into account the uncertainty inherent in correlation point estimates prior to concluding improvements in metric performance. ConÔ¨Ådence intervals for correlations with human judgment are rarely reported in metric evaluations, however, and when they have been reported, the most suitable methods have unfortunately not been applied. For example, incorrect assumptions about correlation sampling distributions made in past evaluations risk over-estimation of signiÔ¨Åcant differences in metric performance. In this paper, we provide analysis of each of the issues that may lead to inaccuracies before providing detail of a method that overcomes previous challenges. Additionally, we propose a new method of translation sampling that in contrast achieves genuine high conclusivity in evaluation of the relative performance of metrics. 
A major beneÔ¨Åt of tree-to-tree over treeto-string translation is that we can use target-side syntax to improve reordering. While this is relatively simple for binarized constituency parses, the reordering problem is considerably harder for dependency parses, in which words can have arbitrarily many children. Previous approaches have tackled this problem by restricting grammar rules, reducing the expressive power of the translation model. In this paper we propose a general model for dependency tree-to-tree reordering based on Ô¨Çexible non-terminals that can compactly encode multiple insertion positions. We explore how insertion positions can be selected even in cases where rules do not entirely cover the children of input sentence words. The proposed method greatly improves the Ô¨Çexibility of translation rules at the cost of only a 30% increase in decoding time, and we demonstrate a 1.2‚Äì1.9 BLEU improvement over a strong tree-to-tree baseline. 
 any one of the preceding claims  Active learning is a framework that makes it possible to efÔ¨Åciently train statistical models by selecting informative examples from a pool of unlabeled data. Previous work has found this framework effective for machine translation (MT), making it possible to train better translation models with less effort, particularly when annotators translate short phrases instead of full sentences. However, previous methods for phrase-based active learning in MT fail to consider whether the selected units are coherent and easy for human translators to translate, and also have problems with selecting redundant phrases with similar content. In this paper, we tackle these problems by proposing two new methods for selecting more syntactically coherent and less redundant segments in active learning for MT. Experiments using both simulation and extensive manual translation by professional translators Ô¨Ånd the proposed method effective, achieving both greater gain of BLEU score for the same number of translated words, and allowing translators to be more conÔ¨Ådent in their translations1. 
We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoderdecoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attentionbased neural translation model. 
Many languages use honoriÔ¨Åcs to express politeness, social distance, or the relative social status between the speaker and their addressee(s). In machine translation from a language without honoriÔ¨Åcs such as English, it is difÔ¨Åcult to predict the appropriate honoriÔ¨Åc, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honoriÔ¨Åcs in neural machine translation (NMT) via side constraints, focusing on English‚ÜíGerman. We show that by marking up the (English) source side of the training data with a feature that encodes the use of honoriÔ¨Åcs on the (German) target side, we can control the honoriÔ¨Åcs produced at test time. Experiments show that the choice of honoriÔ¨Åcs has a big impact on translation quality as measured by BLEU, and oracle experiments show that substantial improvements are possible by constraining the translation to the desired level of politeness. 
The neural network joint model of translation or NNJM (Devlin et al., 2014) combines source and target context to produce a powerful translation feature. However, its softmax layer necessitates a sum over the entire output vocabulary, which results in very slow maximum likelihood (MLE) training. This has led some groups to train using Noise Contrastive Estimation (NCE), which side-steps this sum. We carry out the Ô¨Årst direct comparison of MLE and NCE training objectives for the NNJM, showing that NCE is signiÔ¨Åcantly outperformed by MLE on large-scale ArabicEnglish and Chinese-English translation tasks. We also show that this drop can be avoided by using a recently proposed translation noise distribution. 
We study the problem of generating abstractive summaries for opinionated text. We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and Ô¨Çuent summaries. An importance-based sampling method is designed to allow the encoder to integrate information from an important subset of input. Automatic evaluation indicates that our system outperforms state-ofthe-art abstractive and extractive summarization systems on two newly collected datasets of movie reviews and arguments. Our system summaries are also rated as more informative and grammatical in human evaluation. 
A key challenge for timeline summarization is to generate a concise, yet complete storyline from large collections of news stories. Previous studies in extractive timeline generation are limited in two ways: Ô¨Årst, most prior work focuses on fully-observable ranking models or clustering models with hand-designed features that may not generalize well. Second, most summarization corpora are text-only, which means that text is the sole source of information considered in timeline summarization, and thus, the rich visual content from news images is ignored. To solve these issues, we leverage the success of matrix factorization techniques from recommender systems, and cast the problem as a sentence recommendation task, using a representation learning approach. To augment text-only corpora, for each candidate sentence in a news article, we take advantage of top-ranked relevant images from the Web and model the image using a convolutional neural network architecture. Finally, we propose a scalable low-rank approximation approach for learning joint embeddings of news stories and images. In experiments, we compare our model to various competitive baselines, and demonstrate the stateof-the-art performance of the proposed textbased and multimodal approaches. 
Community created content (e.g., product descriptions, reviews) typically discusses one entity at a time and it can be hard as well as time consuming for a user to compare two or more entities. In response, we deÔ¨Åne a novel task of automatically generating entity comparisons from text. Our output is a table that semantically clusters descriptive phrases about entities. Our clustering algorithm is a Gaussian extension of probabilistic latent semantic analysis (pLSA), in which each phrase is represented in word vector embedding space. In addition, our algorithm attempts to balance information about entities in each cluster to generate meaningful comparison tables, where possible. We test our system‚Äôs effectiveness on two domains, travel articles and movie reviews, and Ô¨Ånd that entitybalanced clusters are strongly preferred by users. 
Student course feedback is generated daily in both classrooms and online course discussion forums. Traditionally, instructors manually analyze these responses in a costly manner. In this work, we propose a new approach to summarizing student course feedback based on the integer linear programming (ILP) framework. Our approach allows different student responses to share co-occurrence statistics and alleviates sparsity issues. Experimental results on a student feedback corpus show that our approach outperforms a range of baselines in terms of both ROUGE scores and human evaluation. 
A corpus of inference rules between a pair of relation phrases is typically generated using the statistical overlap of argument-pairs associated with the relations (e.g., PATTY, CLEAN). We investigate knowledge-guided linguistic rewrites as a secondary source of evidence and Ô¨Ånd that they can vastly improve the quality of inference rule corpora, obtaining 27 to 33 point precision improvement while retaining substantial recall. The facts inferred using cleaned inference rules are 29-32 points more accurate. 
In this paper we present the Ô¨Årst, to the best of our knowledge, discourse parser that is able to predict non-tree DAG structures. We use Integer Linear Programming (ILP) to encode both the objective function and the constraints as global decoding over local scores. Our underlying data come from multi-party chat dialogues, which require the prediction of DAGs. We use the dependency parsing paradigm, as has been done in the past (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015), but we use the underlying formal framework of SDRT and exploit SDRT‚Äôs notions of left and right distributive relations. We achieve an Fmeasure of 0.531 for fully labeled structures which beats the previous state of the art. 
Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don‚Äôt know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations. 
Moving from limited-domain natural language generation (NLG) to open domain is difÔ¨Åcult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is Ô¨Årst trained on counterfeited data synthesised from an out-of-domain dataset, and then Ô¨Åne tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while signiÔ¨Åcantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges conÔ¨Årm that the procedure greatly improves generator performance when only a small amount of data is available in the domain. 
We propose a Ô¨Årst-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the audience would react. We model the setuppunchline relation of conversational humor with a Long Short-Term Memory, with utterance encodings obtained from a Convolutional Neural Network. Out neural network framework is able to improve the F-score of 8% over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes. 
Public debates are a common platform for presenting and juxtaposing diverging views on important issues. In this work we propose a methodology for tracking how ideas Ô¨Çow between participants throughout a debate. We use this approach in a case study of Oxfordstyle debates‚Äîa competitive format where the winner is determined by audience votes‚Äîand show how the outcome of a debate depends on aspects of conversational Ô¨Çow. In particular, we Ô¨Ånd that winners tend to make better use of a debate‚Äôs interactive component than losers, by actively pursuing their opponents‚Äô points rather than promoting their own ideas over the course of the conversation. 
Semantic Role Labeling (SRL) captures semantic roles (or participants) such as agent, patient, and theme associated with verbs from the text. While it provides important intermediate semantic representations for many traditional NLP tasks (such as information extraction and question answering), it does not capture grounded semantics so that an artiÔ¨Åcial agent can reason, learn, and perform the actions with respect to the physical environment. To address this problem, this paper extends traditional SRL to grounded SRL where arguments of verbs are grounded to participants of actions in the physical world. By integrating language and vision processing through joint inference, our approach not only grounds explicit roles, but also grounds implicit roles that are not explicitly mentioned in language descriptions. This paper describes our empirical results and discusses challenges and future directions. 
Metaphor is pervasive in our communication, which makes it an important problem for natural language processing (NLP). Numerous approaches to metaphor processing have thus been proposed, all of which relied on linguistic features and textual data to construct their models. Human metaphor comprehension is, however, known to rely on both our linguistic and perceptual experience, and vision can play a particularly important role when metaphorically projecting imagery across domains. In this paper, we present the Ô¨Årst metaphor identiÔ¨Åcation method that simultaneously draws knowledge from linguistic and visual data. Our results demonstrate that it outperforms linguistic and visual models in isolation, as well as being competitive with the best-performing metaphor identiÔ¨Åcation methods, that rely on hand-crafted knowledge about domains and perception. 
Recently there has been a lot of interest in learning common representations for multiple views of data. Typically, such common representations are learned using a parallel corpus between the two views (say, 1M images and their English captions). In this work, we address a real-world scenario where no direct parallel data is available between two views of interest (say, V1 and V2) but parallel data is available between each of these views and a pivot view (V3). We propose a model for learning a common representation for V1, V2 and V3 using only the parallel data available between V1V3 and V2V3. The proposed model is generic and even works when there are n views of interest and only one pivot view which acts as a bridge between them. There are two speciÔ¨Åc downstream applications that we focus on (i) transfer learning between languages L1,L2,...,Ln using a pivot language L and (ii) cross modal access between images and a language L1 using a pivot language L2. Our model achieves state-of-the-art performance in multilingual document classiÔ¨Åcation on the publicly available multilingual TED corpus and promising results in multilingual multimodal retrieval on a new dataset created and released as a part of this work. 
 We introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, or multimodal embeddings. We Ô¨Ånd that textual embeddings perform well when goldstandard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our Ô¨Åndings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambiguation task. VerSe is made publicly available and can be downloaded at: https://github. com/spandanagella/verse. 
Obtaining common sense knowledge using current information extraction techniques is extremely challenging. In this work, we instead propose to derive simple common sense statements from fully annotated object detection corpora such as the Microsoft Common Objects in Context dataset. We show that many thousands of common sense facts can be extracted from such corpora at high quality. Furthermore, using WordNet and a novel submodular k-coverage formulation, we are able to generalize our initial set of common sense assertions to unseen objects and uncover over 400k potentially useful facts. 
We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efÔ¨Åcient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese. 
We present expected F-measure training for shift-reduce parsing with RNNs, which enables the learning of a global parsing model optimized for sentence-level F1. We apply the model to CCG parsing, where it improves over a strong greedy RNN baseline, by 1.47% F1, yielding state-of-the-art results for shiftreduce CCG parsing. 
We demonstrate that a state-of-the-art parser can be built using only a lexical tagging model and a deterministic grammar, with no explicit model of bi-lexical dependencies. Instead, all dependencies are implicitly encoded in an LSTM supertagger that assigns CCG lexical categories. The parser signiÔ¨Åcantly outperforms all previously published CCG results, supports efÔ¨Åcient and optimal A‚àó decoding, and beneÔ¨Åts substantially from semisupervised tri-training. We give a detailed analysis, demonstrating that the parser can recover long-range dependencies with high accuracy and that the semi-supervised learning enables signiÔ¨Åcant accuracy gains. By running the LSTM on a GPU, we are able to parse over 2600 sentences per second while improving state-of-the-art accuracy by 1.1 F1 in domain and up to 4.5 F1 out of domain. 
In this paper we present new state-of-the-art performance on CCG supertagging and parsing. Our model outperforms existing approaches by an absolute gain of 1.5%. We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags. 
Word segmentation is usually recognized as the Ô¨Årst step for many Chinese natural language processing tasks, yet its impact on these subsequent tasks is relatively under-studied. For example, how to solve the mismatch problem when applying an existing word segmenter to new data? Does a better word segmenter yield a better subsequent NLP task performance? In this work, we conduct an initial attempt to answer these questions on two related subsequent tasks: semantic slot Ô¨Ålling in spoken language understanding and named entity recognition. We propose three techniques to solve the mismatch problem: using word segmentation outputs as additional features, adaptation with partial-learning and taking advantage of n-best word segmentation list. Experimental results demonstrate the effectiveness of these techniques for both tasks and we achieve an error reduction of about 11% for spoken language understanding and 24% for named entity recognition over the baseline systems. 
State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-speciÔ¨Åc knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures‚Äîone based on bidirectional LSTMs and conditional random Ô¨Åelds, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-speciÔ¨Åc knowledge or resources such as gazetteers. 1 
We introduce a novel technique called dynamic feature induction that keeps inducing high dimensional features automatically until the feature space becomes ‚Äòmore‚Äô linearly separable. Dynamic feature induction searches for the feature combinations that give strong clues for distinguishing certain label pairs, and generates joint features from these combinations. These induced features are trained along with the primitive low dimensional features. Our approach was evaluated on two core NLP tasks, part-of-speech tagging and named entity recognition, and showed the state-of-the-art results for both tasks, achieving the accuracy of 97.64 and the F1-score of 91.00 respectively, with about a 25% increase in the feature space. 
In named entity recognition task especially for massive data like Twitter, having a large amount of high quality gazetteers can alleviate the problem of training data scarcity. One could collect large gazetteers from knowledge graph and phrase embeddings to obtain high coverage of gazetteers. However, large gazetteers cause a side-effect called ‚Äúfeature under-training‚Äù, where the gazetteer features overwhelm the context features. To resolve this problem, we propose the dropout conditional random Ô¨Åelds, which decrease the inÔ¨Çuence of gazetteer features with a high weight. Our experiments on named entity recognition with Twitter data lead to higher F1 score of 69.38%, about 4% better than the strong baseline presented in Smith and Osborne (2006). 
Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate contextaware predictions. We demonstrate that our approach substantially outperforms the stateof-the-art methods for event extraction as well as a strong baseline for entity extraction. 
Event extraction is a particularly challenging problem in information extraction. The stateof-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneÔ¨Åting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset. 
Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TREELSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TREELSTM deÔ¨Ånes the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated subtree. We further enhance the modeling power of TREELSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance. 
Recurrent Neural Networks (RNNs) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only ampliÔ¨Åes the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform indepth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state of the art by a large margin.1 
This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the beneÔ¨Åts of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classiÔ¨Åcation, but also word prediction. As a result, it outperforms state-ofthe-art alternatives for two tasks: implicit discourse relation classiÔ¨Åcation in the Penn Discourse Treebank, and dialog act classiÔ¨Åcation in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline. 
This paper presents a data-driven investigation of phonesthemes, phonetic units said to carry meaning associations, thus challenging the traditionally assumed arbitrariness of language. Phonesthemes have received a substantial amount of attention within the cognitive science literature on sound iconicity, but nevertheless remain a controversial and understudied phenomenon. Here we employ NLP techniques to address two main questions: How can the existence of phonesthemes be tested at a large scale with quantitative methods? And how can the meaning arguably carried by a phonestheme be induced automatically from word embeddings? We develop novel methods to make progress on these fronts and compare our results to previous work, obtaining substantial improvements. 
This paper provides a binary, token-based classiÔ¨Åcation of German particle verbs (PVs) into literal vs. non-literal usage. A random forest improving standard features (e.g., bagof-words; affective ratings) with PV-speciÔ¨Åc information and abstraction over common nouns signiÔ¨Åcantly outperforms the majority baseline. In addition, PV-speciÔ¨Åc classiÔ¨Åcation experiments demonstrate the role of shared particle semantics and semantically related base verbs in PV meaning shifts. 
Idioms pose a great challenge to natural language understanding. A system that can automatically paraphrase idioms in context has applications in many NLP tasks. This paper proposes a phrasal substitution method to replace idioms with their Ô¨Ågurative meanings in literal English. Our approach identiÔ¨Åes relevant replacement phrases from an idiom‚Äôs dictionary deÔ¨Ånition and performs appropriate grammatical and referential transformations to ensure that the idiom substitution Ô¨Åts seamlessly into the original context. The proposed method has been evaluated both by automatic metrics and human judgments. Results suggest that high quality paraphrases of idiomatic expressions can be achieved. 
Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage Ô¨Ånancial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can signiÔ¨Åcantly improve the stock prediction accuracy on a standard Ô¨Ånancial database over the baseline system using only the historical price information. 
Children learn the meaning of words by being exposed to perceptually rich situations (linguistic discourse, visual scenes, etc). Current computational learning models typically simulate these rich situations through impoverished symbolic approximations. In this work, we present a distributed word learning model that operates on child-directed speech paired with realistic visual scenes. The model integrates linguistic and extra-linguistic information (visual and social cues), handles referential uncertainty, and correctly learns to associate words with objects, even in cases of limited linguistic exposure. 
We propose recurrent support vector machine (RSVM) for slot tagging. This model is a combination of the recurrent neural network (RNN) and the structured support vector machine. RNN extracts features from the input sequence. The structured support vector machine uses a sequence-level discriminative objective function. The proposed model therefore combines the sequence representation capability of an RNN with the sequence-level discriminative objective. We have observed new state-ofthe-art results on two benchmark datasets and one private dataset. RSVM obtained statistical signiÔ¨Åcant 4% and 2% relative average F1 score improvement on ATIS dataset and Chunking dataset, respectively. Out of eight domains in Cortana live log dataset, RSVM achieved F1 score improvement on seven domains. Experiments also show that RSVM significantly speeds up the model training by skipping the weight updating for non-support vector training samples, compared against training using RNN with CRF or minimum cross-entropy objectives. 
We tackle the task of extracting tweets that mention a speciÔ¨Åc event from all tweets that contain relevant keywords, for which the main challenges include unbalanced positive and negative cases, and the unavailability of manually labeled training data. Existing methods leverage a few manually given seed events and large unlabeled tweets to train a classiÔ¨Åer, by using expectation regularization training with discrete ngram features. We propose a LSTM-based neural model that learns tweet-level features automatically. Compared with discrete ngram features, the neural model can potentially capture non-local dependencies and deep semantic information, which are more effective for disambiguating subtle semantic differences between true event mentions and false cases that use similar wording patterns. Results on both tweets and forum posts show that our neural model is more effective compared with a state-of-the-art discrete baseline. 
Neural machine translation (NMT) with recurrent neural networks, has proven to be an effective technique for end-to-end machine translation. However, in spite of its promising advances over traditional translation methods, it typically suffers from an issue of unbalanced outputs, that arise from both the nature of recurrent neural networks themselves, and the challenges inherent in machine translation. To overcome this issue, we propose an agreement model for neural machine translation and show its effectiveness on large-scale Japaneseto-English and Chinese-to-English translation tasks. Our results show the model can achieve improvements of up to 1.4 BLEU over the strongest baseline NMT system. With the help of an ensemble technique, this new end-to-end NMT approach Ô¨Ånally outperformed phrasebased and hierarchical phrase-based Moses baselines by up to 5.6 BLEU points. 
We tackle the problem of identifying deceptive agents in highly-motivated high-conÔ¨Çict dialogues. We consider the case where we only have textual information. We show the usefulness of psycho-linguistic deception and persuasion features on a small dataset for the game of Werewolf. We analyse the role of syntax and we identify some characteristics of players in deceptive roles. 
This study aims to measure the variation between writers in their choices of referential form by collecting and analysing a new and publicly available corpus of referring expressions. The corpus is composed of referring expressions produced by different participants in identical situations. Results, measured in terms of normalized entropy, reveal substantial individual variation. We discuss the problems and prospects of this Ô¨Ånding for automatic text generation applications. 
Automatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to Ô¨Ånd out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities‚Äô semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work. 
We introduce a bootstrapping algorithm for regression that exploits word embedding models. We use it to infer four psycholinguistic properties of words: Familiarity, Age of Acquisition, Concreteness and Imagery and further populate the MRC Psycholinguistic Database with these properties. The approach achieves 0.88 correlation with humanproduced values and the inferred psycholinguistic features lead to state-of-the-art results when used in a Lexical SimpliÔ¨Åcation task. 
This paper proposes a variability normalization algorithm to reduce the variability between intra-topic documents for topic classiÔ¨Åcation. Firstly, an optimization problem is constructed based on linear variability removable assumption. Secondly, a new feature space for document representation is found by solving the optimization problem with kernel principle component analysis (KPCA). Finally, effective feature transformation is taken through linear projection. As for experiments, state-of-the-art SVM and KNN algorithm are adopted for topic classiÔ¨Åcation respectively. Experimental results on a free-style conversational corpus show that the proposed variability normalization algorithm for topic classiÔ¨Åcation achieves 3.8% absolute improvement for micro-F1 measure. 
We present a neural network based shiftreduce CCG parser, the Ô¨Årst neural-network based parser for CCG. We also study the impact of neural network based tagging models, and greedy versus beam-search parsing, by using a structured neural network model. Our greedy parser obtains a labeled F-score of 83.27%, the best reported result for greedy CCG parsing in the literature (an improvement of 2.5% over a perceptron based greedy parser) and is more than three times faster. With a beam, our structured neural network model gives a labeled F-score of 85.57% which is 0.6% better than the perceptron based counterpart. 
For topic models, such as LDA, that use a bag-of-words assumption, it becomes especially important to break the corpus into appropriately-sized ‚Äúdocuments‚Äù. Since the models are estimated solely from the term cooccurrences, extensive documents such as books or long journal articles lead to diffuse statistics, and short documents such as forum posts or product reviews can lead to sparsity. This paper describes practical inference procedures for hierarchical models that smooth topic estimates for smaller sections with hyperpriors over larger documents. Importantly for large collections, these online variational Bayes inference methods perform a single pass over a corpus and achieve better perplexity than ‚ÄúÔ¨Çat‚Äù topic models on monolingual and multilingual data. Furthermore, on the task of detecting document translation pairs in large multilingual collections, polylingual topic models (PLTM) with multi-level hyperpriors (mlhPLTM) achieve signiÔ¨Åcantly better performance than existing online PLTM models while retaining computational efÔ¨Åciency. 
Historical documents frequently exhibit extensive orthographic variation, including archaic spellings and obsolete shorthand. OCR tools typically seek to produce so-called diplomatic transcriptions that preserve these variants, but many end tasks require transcriptions with normalized orthography. In this paper, we present a novel joint transcription model that learns, unsupervised, a probabilistic mapping between modern orthography and that used in the document. Our system thus produces dual diplomatic and normalized transcriptions simultaneously, and achieves a 35% relative error reduction over a state-of-the-art OCR model on diplomatic transcription, and a 46% reduction on normalized transcription. 
Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from Ô¨Åxed context windows. In this application, we explored recurrent neural network frameworks and show that they signiÔ¨Åcantly outperformed the CRF models. 
When evaluating the quality of topics generated by a topic model, the convention is to score topic coherence ‚Äî either manually or automatically ‚Äî using the top-N topic words. This hyper-parameter N , or the cardinality of the topic, is often overlooked and selected arbitrarily. In this paper, we investigate the impact of this cardinality hyper-parameter on topic coherence evaluation. For two automatic topic coherence methodologies, we observe that the correlation with human ratings decreases systematically as the cardinality increases. More interestingly, we Ô¨Ånd that performance can be improved if the system scores and human ratings are aggregated over several topic cardinalities before computing the correlation. In contrast to the standard practice of using a Ô¨Åxed value of N (e.g. N = 5 or N = 10), our results suggest that calculating topic coherence over several different cardinalities and averaging results in a substantially more stable and robust evaluation. We release the code and the datasets used in this research, for reproducibility.1 
It has been shown that transition-based methods can be used for syntactic word ordering and tree linearization, achieving signiÔ¨Åcantly faster speed compared with traditional best-Ô¨Årst methods. State-of-the-art transitionbased models give competitive results on abstract word ordering and unlabeled tree linearization, but signiÔ¨Åcantly worse results on labeled tree linearization. We demonstrate that the main cause for the performance bottleneck is the sparsity of SHIFT transition actions rather than heavy pruning. To address this issue, we propose a modiÔ¨Åcation to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efÔ¨Åciency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-Ô¨Årst-search. 
This paper presents a novel approach using recurrent neural networks for estimating the quality of machine translation output. A sequence of vectors made by the prediction method is used as the input of the Ô¨Ånal recurrent neural network. The prediction method uses bi-directional recurrent neural network architecture both on source and target sentence to fully utilize the bi-directional quality information from source and target sentence. Our experiments show that the proposed recurrent neural networks approach achieves a performance comparable to the existing stateof-the-art models for estimating the sentencelevel quality of English-to-Spanish translation. 
State-of-the-art word embeddings, which are often trained on bag-of-words (BOW) contexts, provide a high quality representation of aspects of the semantics of nouns. However, their quality decreases substantially for the task of verb similarity prediction. In this paper we show that using symmetric pattern contexts (SPs, e.g., ‚ÄúX and Y‚Äù) improves word2vec verb similarity performance by up to 15% and is also instrumental in adjective similarity prediction. The unsupervised SP contexts are even superior to a variety of dependency contexts extracted using a supervised dependency parser. Moreover, we observe that SPs and dependency coordination contexts (Coor) capture a similar type of information, and demonstrate that Coor contexts are superior to other dependency contexts including the set of all dependency contexts, although they are still inferior to SPs. Finally, there are substantially fewer SP contexts compared to alternative representations, leading to a massive reduction in training time. On an 8G words corpus and a 32 core machine, the SP model trains in 11 minutes, compared to 5 and 11 hours with BOW and all dependency contexts, respectively. 
Existing research on multiclass text classification mostly makes the closed world assumption, which focuses on designing accurate classifiers under the assumption that all test classes are known at training time. A more realistic scenario is to expect unseen classes during testing (open world). In this case, the goal is to design a learning system that classifies documents of the known classes into their respective classes and also to reject documents from unknown classes. This problem is called open (world) classification. This paper approaches the problem by reducing the open space risk while balancing the empirical risk. It proposes to use a new learning strategy, called center-based similarity (CBS) space learning (or CBS learning), to provide a novel solution to the problem. Extensive experiments across two datasets show that CBS learning gives promising results on multiclass open text classification compared to state-ofthe-art baselines. 
Recent approaches based on artiÔ¨Åcial neural networks (ANNs) have shown promising results for short-text classiÔ¨Åcation. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction. 
In a multi-label text classiÔ¨Åcation task, in which multiple labels can be assigned to one text, label co-occurrence itself is informative. We propose a novel neural network initialization method to treat some of the neurons in the Ô¨Ånal hidden layer as dedicated neurons for each pattern of label co-occurrence. These dedicated neurons are initialized to connect to the corresponding co-occurring labels with stronger weights than to others. In experiments with a natural language query classiÔ¨Åcation task, which requires multi-label classiÔ¨Åcation, our initialization method improved classiÔ¨Åcation accuracy without any computational overhead in training and evaluation. 
Bidirectional long short-term memory (BLSTM) recurrent neural network (RNN) has been successfully applied in many tagging tasks. BLSTM-RNN relies on the distributed representation of words, which implies that the former can be futhermore improved through learning the latter better. In this work, we propose a novel approach to learn distributed word representations by training BLSTM-RNN on a specially designed task which only relies on unlabeled data. Our experimental results show that the proposed approach learns useful distributed word representations, as the trained representations signiÔ¨Åcantly elevate the performance of BLSTM-RNN on three tagging tasks: part-ofspeech tagging, chunking and named entity recognition, surpassing word representations trained by other published methods. 
An increasing amount of research has recently focused on representing affective states as continuous numerical values on multiple dimensions, such as the valence-arousal (VA) space. Compared to the categorical approach that represents affective states as several classes (e.g., positive and negative), the dimensional approach can provide more finegrained sentiment analysis. However, affective resources with valence-arousal ratings are still very rare, especially for the Chinese language. Therefore, this study builds 1) an affective lexicon called Chinese valence-arousal words (CVAW) containing 1,653 words, and 2) an affective corpus called Chinese valencearousal text (CVAT) containing 2,009 sentences extracted from web texts. To improve the annotation quality, a corpus cleanup procedure is used to remove outlier ratings and improper texts. Experiments using CVAW words to predict the VA ratings of the CVAT corpus show results comparable to those obtained using English affective resources. 
Automatically learning script knowledge has proved difÔ¨Åcult, with previous work not or just barely beating a most-frequent baseline. Script knowledge is a type of world knowledge which can however be useful for various task in NLP and psycholinguistic modelling. We here propose a model that includes participant information (i.e., knowledge about which participants are relevant for a script) and show, on the Dinners from Hell corpus as well as the InScript corpus, that this knowledge helps us to signiÔ¨Åcantly improve prediction performance on the narrative cloze task. 
We address the task of annotating images with semantic tuples. Solving this problem requires an algorithm able to deal with hundreds of classes for each argument of the tuple. In such contexts, data sparsity becomes a key challenge. We propose handling this sparsity by incorporating feature representations of both the inputs (images) and outputs (argument classes) into a factorized log-linear model. 
Multitask learning has been proven a useful technique in a number of Natural Language Processing applications where data is scarce and naturally diverse. Examples include learning from data of different domains and learning from labels provided by multiple annotators. Tasks in these scenarios would be the domains or the annotators. When faced with limited data for each task, a framework for the learning of tasks in parallel while using a shared representation is clearly helpful: what is learned for a given task can be transferred to other tasks while the peculiarities of each task are still modelled. Focusing on machine translation quality estimation as application, in this paper we show that multitask learning is also useful in cases where data is abundant. Based on two large-scale datasets, we explore models with multiple annotators and multiple languages and show that state-of-the-art multitask learning algorithms lead to improved results in all settings. 
Group discussions are essential for organizing every aspect of modern life, from faculty meetings to senate debates, from grant review panels to papal conclaves. While costly in terms of time and organization effort, group discussions are commonly seen as a way of reaching better decisions compared to solutions that do not require coordination between the individuals (e.g. voting)‚Äîthrough discussion, the sum becomes greater than the parts. However, this assumption is not irrefutable: anecdotal evidence of wasteful discussions abounds, and in our own experiments we Ô¨Ånd that over 30% of discussions are unproductive. We propose a framework for analyzing conversational dynamics in order to determine whether a given task-oriented discussion is worth having or not. We exploit conversational patterns reÔ¨Çecting the Ô¨Çow of ideas and the balance between the participants, as well as their linguistic choices. We apply this framework to conversations naturally occurring in an online collaborative world exploration game developed and deployed to support this research. Using this setting, we show that linguistic cues and conversational patterns extracted from the Ô¨Årst 20 seconds of a team discussion are predictive of whether it will be a wasteful or a productive one. 
Property norms have the potential to aid a wide range of semantic tasks, provided that they can be obtained for large numbers of concepts. Recent work has focused on text as the main source of information for automatic property extraction. In this paper we examine property norm prediction from visual, rather than textual, data, using cross-modal maps learnt between property norm and visual spaces. We also investigate the importance of having a complete feature norm dataset, for both training and testing. Finally, we evaluate how these datasets and cross-modal maps can be used in an image retrieval task. 
Cross-lingual WikiÔ¨Åcation is the task of grounding mentions written in non-English documents to entries in the English Wikipedia. This task involves the problem of comparing textual clues across languages, which requires developing a notion of similarity between text snippets across languages. In this paper, we address this problem by jointly training multilingual embeddings for words and Wikipedia titles. The proposed method can be applied to all languages represented in Wikipedia, including those for which no machine translation technology is available. We create a challenging dataset in 12 languages and show that our proposed approach outperforms various baselines. Moreover, our model compares favorably with the best systems on the TAC KBP2015 Entity Linking task including those that relied on the availability of translation from the target language to English. 
Search tasks, comprising a series of search queries serving a common informational need, have steadily emerged as accurate units for developing the next generation of task-aware web search systems. Most prior research in this area has focused on segmenting chronologically ordered search queries into higher level tasks. A more naturalistic viewpoint would involve treating query logs as convoluted structures of tasks-subtasks, with complex search tasks being decomposed into more focused sub-tasks. In this work, we focus on extracting sub-tasks from a given collection of on-task search queries. We jointly leverage insights from Bayesian nonparametrics and word embeddings to identify and extract sub-tasks from a given collection of ontask queries. Our proposed model can inform the design of the next generation of task-based search systems that leverage user‚Äôs task behavior for better support and personalization. 
We introduce several probabilistic models for learning the lexicon of a semantic parser. Lexicon learning is the Ô¨Årst step of training a semantic parser for a new application domain and the quality of the learned lexicon signiÔ¨Åcantly affects both the accuracy and efÔ¨Åciency of the Ô¨Ånal semantic parser. Existing work on lexicon learning has focused on heuristic methods that lack convergence guarantees and require signiÔ¨Åcant human input in the form of lexicon templates or annotated logical forms. In contrast, our probabilistic models are trained directly from question/answer pairs using EM and our simplest model has a concave objective that guarantees convergence to a global optimum. An experimental evaluation on a set of 4th grade science questions demonstrates that our models improve semantic parser accuracy (35-70% error reduction) and efÔ¨Åciency (4-25x more sentences per second) relative to prior work despite using less human input. Our models also obtain competitive results on GEO880 without any datasetspeciÔ¨Åc engineering. 
In this paper we present a word decompounding method that is based on distributional semantics. Our method does not require any linguistic knowledge and is initialized using a large monolingual corpus. The core idea of our approach is that parts of compounds (like ‚Äúcandle‚Äù and ‚Äústick‚Äù) are semantically similar to the entire compound, which helps to exclude spurious splits (like ‚Äúcandles‚Äù and ‚Äútick‚Äù). We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and signiÔ¨Åcantly outperforms two unsupervised baselines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter.  on dictionaries or are trained in a supervised fashion. Both approaches require substantial manual work and do not adapt to vocabulary change. In this paper we introduce an unsupervised method for decompounding that relies on distributional semantics. For the computation of the semantic model we solely rely on a tokenized monolingual corpus and do not require any further linguistic processing. Most previous research on compound splitting concentrates on the detection of lemmas that form the compound. Whereas this is important for several tasks, in this work we focus on the splitting of a compound into its word units without any base form reduction, arguing that lemmatization is either part of the task pipeline anyways (e.g. IR) or not required (e.g. for ASR). 2 Related Work  
How should one apply deep learning to tasks such as morphological reinÔ¨Çection, which stochastically edit one string to get another? A recent approach to such sequence-to-sequence tasks is to compress the input string into a vector that is then used to generate the output string, using recurrent neural networks. In contrast, we propose to keep the traditional architecture, which uses a Ô¨Ånite-state transducer to score all possible output strings, but to augment the scoring function with the help of recurrent networks. A stack of bidirectional LSTMs reads the input string from leftto-right and right-to-left, in order to summarize the input context in which a transducer arc is applied. We combine these learned features with the transducer to deÔ¨Åne a probability distribution over aligned output strings, in the form of a weighted Ô¨Ånite-state automaton. This reduces hand-engineering of features, allows learned features to examine unbounded context in the input string, and still permits exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinÔ¨Çection and lemmatization. 
Morphological inÔ¨Çection generation is the task of generating the inÔ¨Çected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inÔ¨Çection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inÔ¨Çection generation. 
In this paper, we address the task of languageindependent, knowledge-lean and unsupervised compound splitting, which is an essential component for many natural language processing tasks such as machine translation. Previous methods on statistical compound splitting either include language-speciÔ¨Åc knowledge (e.g., linking elements) or rely on parallel data, which results in limited applicability. We aim to overcome these limitations by learning compounding morphology from inÔ¨Çectional information derived from lemmatized monolingual corpora. In experiments for Germanic languages, we show that our approach signiÔ¨Åcantly outperforms language-dependent stateof-the-art methods in Ô¨Ånding the correct split point and that word inÔ¨Çection is a good approximation for compounding morphology. 
Many puns create humor through the relationship between a pun and its phonologically similar target. For example, in ‚ÄúDon‚Äôt take geologists for granite‚Äù the word ‚Äúgranite‚Äù is a pun with the target ‚Äúgranted‚Äù. The recovery of the target in the mind of the listener is essential to the success of the pun. This work introduces a new model for automatic target recovery and provides the Ô¨Årst empirical test for this task. The model draws upon techniques for automatic speech recognition using weighted Ô¨Ånite-state transducers, and leverages automatically learned phone edit probabilities that give insight into how people perceive sounds and into what makes a good pun. The model is evaluated on a small corpus where it is able to automatically recover a large fraction of the pun targets. 
We present a model of morphological segmentation that jointly learns to segment and restore orthographic changes, e.g., funniest ‚Üí fun-y-est. We term this form of analysis canonical segmentation and contrast it with the traditional surface segmentation, which segments a surface form into a sequence of substrings, e.g., funniest ‚Üí funn-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian.  funniest  funyest  fun y est  memikulmu menpikulmu men pikul mu  Zulassung zulassenung zu lassen ung Orthography Underlying Form Segmentation Figure 1: Examples of canonical segmentation for English (top), Indonesian (middle) and German (bottom).  
Accurate automatic processing of Web queries is important for high-quality information retrieval from the Web. While the syntactic structure of a large portion of these queries is trivial, the structure of queries with question intent is much richer. In this paper we therefore address the task of statistical syntactic parsing of such queries. We Ô¨Årst show that the standard dependency grammar does not account for the full range of syntactic structures manifested by queries with question intent. To alleviate this issue we extend the dependency grammar to account for segments ‚Äì independent syntactic units within a potentially larger syntactic structure. We then propose two distant supervision approaches for the task. Both algorithms do not require manually parsed queries for training. Instead, they are trained on millions of (query, page title) pairs from the Community Question Answering (CQA) domain, where the CQA page was clicked by the user who initiated the query in a search engine. Experiments on a new treebank1 consisting of 5,000 Web queries from the CQA domain, manually parsed using the proposed grammar, show that our algorithms outperform alternative approaches trained on various sources: tens of thousands of manually parsed OntoNotes sentences, millions of unlabeled CQA queries and thousands of manually segmented CQA queries. 
While neural networks have been successfully applied to many NLP tasks the resulting vectorbased models are very difÔ¨Åcult to interpret. For example it‚Äôs not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We Ô¨Årst plot unit values to visualize compositionality of negation, intensiÔ¨Åcation, and concessive clauses, allowing us to see wellknown markedness asymmetries in negation. We then introduce methods for visualizing a unit‚Äôs salience, the amount that it contributes to the Ô¨Ånal composed meaning from Ô¨Årst-order derivatives. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks. 
In many languages, sparse availability of resources causes numerous challenges for textual analysis tasks. Text classiÔ¨Åcation is one of such standard tasks that is hindered due to limited availability of label information in lowresource languages. Transferring knowledge (i.e. label information) from high-resource to low-resource languages might improve text classiÔ¨Åcation as compared to the other approaches like machine translation. We introduce BRAVE (Bilingual paRAgraph VEctors), a model to learn bilingual distributed representations (i.e. embeddings) of words without word alignments either from sentencealigned parallel or label-aligned non-parallel document corpora to support cross-language text classiÔ¨Åcation. Empirical analysis shows that classiÔ¨Åcation models trained with our bilingual embeddings outperforms other stateof-the-art systems on three different crosslanguage text classiÔ¨Åcation tasks. 
This paper addresses the problem of comment classiÔ¨Åcation in community Question Answering. Following the state of the art, we approach the task with a global inference process to exploit the information of all comments in the answer-thread in the form of a fully connected graph. Our contribution comprises two novel joint learning models that are on-line and integrate inference within learning. The Ô¨Årst one jointly learns two node- and edge-level MaxEnt classiÔ¨Åers with stochastic gradient descent and integrates the inference step with loopy belief propagation. The second model is an instance of fully connected pairwise CRFs (FCCRF). The FCCRF model signiÔ¨Åcantly outperforms all other approaches and yields the best results on the task to date. Crucial elements for its success are the global normalization and an Ising-like edge potential. 
This paper introduces a new annotated corpus based on an existing informal text corpus: the NUS SMS Corpus (Chen and Kan, 2013). The new corpus includes 76,490 noun phrases from 26,500 SMS messages, annotated by university students. We then explored several graphical models, including a novel variant of the semi-Markov conditional random Ô¨Åelds (semi-CRF) for the task of noun phrase chunking. We demonstrated through empirical evaluations on the new dataset that the new variant yielded similar accuracy but ran in significantly lower running time compared to the conventional semi-CRF. 
We propose an end-to-end, domainindependent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model Ô¨Årst encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-Ô¨Åne aligner to identify the small subset of salient records to talk about, and Ô¨Ånally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WEATHERGOV dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam Ô¨Ålter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the ROBOCUP dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved. 
Language generation from purely semantic representations is a challenging task. This paper addresses generating English from the Abstract Meaning Representation (AMR), consisting of re-entrant graphs whose nodes are concepts and edges are relations. The new method is trained statistically from AMRannotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-tostring transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at: http://github.com/jflanigan/jamr 
We present a corpus of 2,380 natural language queries paired with machine readable formulae that can be executed against world wide geographic data of the OpenStreetMap (OSM) database. We use the corpus to learn an accurate semantic parser that builds the basis of a natural language interface to OSM. Furthermore, we use response-based learning on parser feedback to adapt a statistical machine translation system for multilingual database access to OSM. Our framework allows to map fuzzy natural language expressions such as ‚Äúnearby‚Äù, ‚Äúnorth of‚Äù, or ‚Äúin walking distance‚Äù to spatial polygons on an interactive map. Furthermore, it combines syntactic complexity and compositionality with a reasonable lexical variability of queries, making it an interesting new publicly available dataset for research on semantic parsing. 
We propose a framework for devising empirically testable algorithms for bridging the communication gap between humans and robots. We instantiate our framework in the context of a problem setting in which humans give instructions to robots using unrestricted natural language commands, with instruction sequences being subservient to building complex goal conÔ¨Ågurations in a blocks world. We show how one can collect meaningful training data and we propose three neural architectures for interpreting contextually grounded natural language commands. The proposed architectures allow us to correctly understand/ground the blocks that the robot should move when instructed by a human who uses unrestricted language. The architectures have more difÔ¨Åculty in correctly understanding/grounding the spatial relations required to place blocks correctly, especially when the blocks are not easily identiÔ¨Åable.  As robots become increasingly ubiquituous, we need to learn to interact with them intelligently, in the same manner we interact with members of our own species. To make rapid progress in this area, we propose to use an intellectual framework that has the same ingredients that have transformed our Ô¨Åeld: appealing science problem deÔ¨Ånitions; publicly available datasets; and easily computable, objective evaluation metrics. In this paper, we study the problem of HumanRobot Natural Language Communication in a setting inspired by a traditional AI problem ‚Äì blocks world (Winograd, 1972). After reviewing previous work (Section 2), we propose a novel Human-Robot Communication Problem that is testable empirically (Section 3.1) and we describe the publicly available datasets (Section 3.2) and evaluation metric that we devised to support our research (Section 6). We then introduce a set of algorithms for solving our problem and we evaluate their performance both objectively and subjectively (Sections 4‚Äì8).  
In this paper, we examine the impact of employing contextual, structural information from a tree-structured document set to derive a language model. Our results show that this information signiÔ¨Åcantly improves the accuracy of the resultant model. 
We examine different features and classiÔ¨Åers for the categorization of opinion words into actor and speaker view. To our knowledge, this is the Ô¨Årst comprehensive work to address sentiment views on the word level taking into consideration opinion verbs, nouns and adjectives. We consider many high-level features requiring only few labeled training data. A detailed feature analysis produces linguistic insights into the nature of sentiment views. We also examine how far global constraints between different opinion words help to increase classiÔ¨Åcation performance. Finally, we show that our (prior) word-level annotation correlates with contextual sentiment views.  (2) All representatives praisedactor the Ô¨Ånal agreement. (3) Sarah excelledspeaker in virtually every subject. (4) The government wastedspeaker a lot of money. The distinction between those categories is relevant for related tasks in sentiment analysis, most importantly, opinion holder and target extraction. This has already been demonstrated for verbs (Wiegand and Ruppenhofer, 2015). For example, even though the noun Peter has the same grammatical relation to the opinion verb in (5) & (6), in the former sentence it is a holder but in the latter it is a target. Similar cases can be observed for opinion nouns (7) & (8) and opinion adjectives (9) & (10). Only the knowledge of sentiment views helps us to assign opinion roles correctly.  
This paper presents a clustering approach that simultaneously identiÔ¨Åes product features and groups them into aspect categories from online reviews. Unlike prior approaches that Ô¨Årst extract features and then group them into categories, the proposed approach combines feature and aspect discovery instead of chaining them. In addition, prior work on feature extraction tends to require seed terms and focus on identifying explicit features, while the proposed approach extracts both explicit and implicit features, and does not require seed terms. We evaluate this approach on reviews from three domains. The results show that it outperforms several state-of-the-art methods on both tasks across all three domains. 
We present an approach to the new task of opinion holder and target extraction on opinion compounds. Opinion compounds (e.g. user rating or victim support) are noun compounds whose head is an opinion noun. We do not only examine features known to be effective for noun compound analysis, such as paraphrases and semantic classes of heads and modiÔ¨Åers, but also propose novel features tailored to this new task. Among them, we examine paraphrases that jointly consider holders and targets, a verb detour in which noun heads are replaced by related verbs, a global head constraint allowing inferencing between different compounds, and the categorization of the sentiment view that the head conveys. 
Access to word‚Äìsentiment associations is useful for many applications, including sentiment analysis, stance detection, and linguistic analysis. However, manually assigning Ô¨Ånegrained sentiment association scores to words has many challenges with respect to keeping annotations consistent. We apply the annotation technique of Best‚ÄìWorst Scaling to obtain real-valued sentiment association scores for words and phrases in three different domains: general English, English Twitter, and Arabic Twitter. We show that on all three domains the ranking of words by sentiment remains remarkably consistent even when the annotation process is repeated with a different set of annotators. We also, for the Ô¨Årst time, determine the minimum difference in sentiment association that is perceptible to native speakers of a language. 
In recent years many knowledge bases (KBs) have been constructed, yet there is not yet a verb resource that maps to these growing KB resources. A resource that maps verbs in different languages to KB relations would be useful for extracting facts from text into the KBs, and to aid alignment and integration of knowledge across different KBs and languages. Such a multi-lingual verb resource would also be useful for tasks such as machine translation and machine reading. In this paper, we present a scalable approach to automatically construct such a verb resource using a very large web text corpus as a kind of interlingua to relate verb phrases to KB relations. Given a text corpus in any language and any KB, it can produce a mapping of that language‚Äôs verb phrases to the KB relations. Experiments with the English NELL KB and ClueWeb corpus show that the learned English verb-to-relation mapping is effective for extracting relation instances from English text. When applied to a Portuguese NELL KB and a Portuguese text corpus, the same method automatically constructs a verb resource in Portuguese that is effective for extracting relation instances from Portuguese text. 
We address relation classiÔ¨Åcation in the context of slot Ô¨Ålling, the task of Ô¨Ånding and evaluating Ô¨Ållers like ‚ÄúSteve Jobs‚Äù for the slot X in ‚ÄúX founded Apple‚Äù. We propose a convolutional neural network which splits the input sentence into three parts according to the relation arguments and compare it to state-ofthe-art and traditional approaches of relation classiÔ¨Åcation. Finally, we combine different methods and show that the combination is better than individual approaches. We also analyze the effect of genre differences on performance. 
We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document. Evaluated on a recent large scale dataset (Hermann et al., 2015), our model exhibits better results than previous research, and we Ô¨Ånd that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network 
We address the problem of automatically Ô¨Ånding the parameters of a statistical machine translation system that maximize BLEU scores while ensuring that decoding speed exceeds a minimum value. We propose the use of Bayesian Optimization to efÔ¨Åciently tune the speed-related decoding parameters by easily incorporating speed as a noisy constraint function. The obtained parameter values are guaranteed to satisfy the speed constraint with an associated conÔ¨Ådence margin. Across three language pairs and two speed constraint values, we report overall optimization time reduction compared to grid and random search. We also show that Bayesian Optimization can decouple speed and BLEU measurements, resulting in a further reduction of overall optimization time as speed is measured over a small subset of sentences. 
We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT‚Äô15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model signiÔ¨Åcantly improves the translation quality of low-resource language pairs. 
Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difÔ¨Åcult languages in a low resource setting. 
Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns observed in raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns‚Äô compositional semantics, providing generalization to all possible input text. In response, this paper introduces signiÔ¨Åcant further improvements to the coverage and Ô¨Çexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-Ô¨Ålling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we Ô¨Ånd that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains. 
Can crowdsourced annotation of training data boost performance for relation extraction over methods based solely on distant supervision? While crowdsourcing has been shown effective for many NLP tasks, previous researchers found only minimal improvement when applying the method to relation extraction. This paper demonstrates that a much larger boost is possible, e.g., raising F1 from 0.40 to 0.60. Furthermore, the gains are due to a simple, generalizable technique, Gated Instruction, which combines an interactive tutorial, feedback to correct errors during training, and improved screening. 
This paper proposes a novel translation-based knowledge graph embedding that preserves the logical properties of relations such as transitivity and symmetricity. The embedding space generated by existing translation-based embeddings do not represent transitive and symmetric relations precisely, because they ignore the role of entities in triples. Thus, we introduce a role-speciÔ¨Åc projection which maps an entity to distinct vectors according to its role in a triple. That is, a head entity is projected onto an embedding space by a head projection operator, and a tail entity is projected by a tail projection operator. This idea is applied to TransE, TransR, and TransD to produce lppTransE, lppTransR, and lppTransD, respectively. According to the experimental results on link prediction and triple classiÔ¨Åcation, the proposed logical property preserving embeddings show the state-of-the-art performance at both tasks. These results prove that it is critical to preserve logical properties of relations while embedding knowledge graphs, and the proposed method does it effectively. 
Recurrent neural networks, particularly long short-term memory (LSTM), have recently shown to be very effective in a wide range of sequence modeling problems, core to which is effective learning of distributed representation for subsequences as well as the sequences they form. An assumption in almost all the previous models, however, posits that the learned representation (e.g., a distributed representation for a sentence), is fully compositional from the atomic components (e.g., representations for words), while non-compositionality is a basic phenomenon in human languages. In this paper, we relieve the assumption by extending the chain-structured LSTM to directed acyclic graphs (DAGs), with the aim to endow linear-chain LSTMs with the capability of considering compositionality together with non-compositionality in the same semantic composition framework. From a more general viewpoint, the proposed models incorporate additional prior knowledge into recurrent neural networks, which is interesting to us, considering most NLP tasks have relatively small training data and appropriate prior knowledge could be beneÔ¨Åcial to help cover missing semantics. Our experiments on sentiment composition demonstrate that the proposed models achieve the state-of-the-art performance, outperforming models that lack this ability. 
IdentiÔ¨Åcation of short text similarity (STS) is a high-utility NLP task with applications in a variety of domains. We explore adaptation of STS algorithms to different target domains and applications. A two-level hierarchical Bayesian model is employed for domain adaptation (DA) of a linear STS model to text from different sources (e.g., news, tweets). This model is then further extended for multitask learning (MTL) of three related tasks: STS, short answer scoring (SAS) and answer sentence ranking (ASR). In our experiments, the adaptive model demonstrates better overall cross-domain and crosstask performance over two non-adaptive baselines. 
For many low-resource languages, spoken language resources are more likely to be annotated with translations than transcriptions. This bilingual speech data can be used for word-spotting, spoken document retrieval, and even for documentation of endangered languages. We experiment with the neural, attentional model applied to this data. On phoneto-word alignment and translation reranking tasks, we achieve large improvements relative to several baselines. On the more challenging speech-to-word alignment task, our model nearly matches GIZA++‚Äôs performance on gold transcriptions, but without recourse to transcriptions or to a lexicon. 
This paper introduces information density and machine translation quality estimation inspired features to automatically detect and classify human translated texts. We investigate two settings: discriminating between translations and comparable originally authored texts, and distinguishing two levels of translation professionalism. Our framework is based on delexicalised sentence-level dense feature vector representations combined with a supervised machine learning approach. The results show state-of-the-art performance for mixed-domain translationese detection with information density and quality estimation based features, while results on translation expertise classiÔ¨Åcation are mixed. 
Computational approaches to simultaneous interpretation are stymied by how little we know about the tactics human interpreters use. We produce a parallel corpus of translated and simultaneously interpreted text and study differences between them through a computational approach. Our analysis reveals that human interpreters regularly apply several effective tactics to reduce translation latency, including sentence segmentation and passivization. In addition to these unique, clever strategies, we show that limited human memory also causes other idiosyncratic properties of human interpretation such as generalization and omission of source content. 
ArtiÔ¨Åcial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and their alignment. Further by utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust, and achieves signiÔ¨Åcant improvements over various baseline systems. 
 Dropped Pronouns (DP) in which pronouns are frequently dropped in the source language but should be retained in the target language are challenge in machine translation. In response to this problem, we propose a semisupervised approach to recall possibly missing pronouns in the translation. Firstly, we build training data for DP generation in which the DPs are automatically labelled according to the alignment information from a parallel corpus. Secondly, we build a deep learning-based DP generator for input sentences in decoding when no corresponding references exist. More speciÔ¨Åcally, the generation is two-phase: (1) DP position detection, which is modeled as a sequential labelling task with recurrent neural networks; and (2) DP prediction, which employs a multilayer perceptron with rich features. Finally, we integrate the above outputs into our translation system to recall missing pronouns by both extracting rules from the DP-labelled training data and translating the DP-generated input sentences. Experimental results show that our approach achieves a signiÔ¨Åcant improvement of 1.58 BLEU points in translation performance with 66% F-score for DP generation accuracy. 
There is compelling evidence that coreference prediction would beneÔ¨Åt from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difÔ¨Åculty of crafting informative clusterlevel features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search. 
There is a signiÔ¨Åcant gap between the performance of a coreference resolution system on gold mentions and on system mentions. This gap is due to the large and unbalanced search space in coreference resolution when using system mentions. In this paper we show that search space pruning is a simple but efÔ¨Åcient way of improving coreference resolvers. By incorporating our pruning method in one of the state-of-the-art coreference resolution systems, we achieve the best reported overall score on the CoNLL 2012 English test set. A version of our pruning method is available with the Cort coreference resolution source code. 
Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to overÔ¨Åtting. We present a new model that represents complex lexical features‚Äîcomprised of parts for words, contextual information and labels‚Äîin a tensor that captures conjunction information among these parts. We apply lowrank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include n-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PPattachment, and preposition disambiguation. 
We provide the Ô¨Årst extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for Ô¨Ånding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we Ô¨Ånd that once the beneÔ¨Åt from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words. 
Recently, researchers have demonstrated that both Chinese word and its component characters provide rich semantic information when learning Chinese word embeddings. However, they ignored the semantic similarity across component characters in a word. In this paper, we learn the semantic contribution of characters to a word by exploiting the similarity between a word and its component characters with the semantic knowledge obtained from other languages. We propose a similaritybased method to learn Chinese word and character embeddings jointly. This method is also capable of disambiguating Chinese characters and distinguishing non-compositional Chinese words. Experiments on word similarity and text classiÔ¨Åcation demonstrate the effectiveness of our method. 
Given a pair of sentences, we present computational models to assess if one sentence is simpler to read than the other. While existing models explored the usage of phrase structure features using a non-incremental parser, experimental evidence suggests that the human language processor works incrementally. We empirically evaluate if syntactic features from an incremental CCG parser are more useful than features from a non-incremental phrase structure parser. Our evaluation on Simple and Standard Wikipedia sentence pairs suggests that incremental CCG features are indeed more useful than phrase structure features achieving 0.44 points gain in performance. Incremental CCG parser also gives signiÔ¨Åcant improvements in speed (12 times faster) in comparison to the phrase structure parser. Furthermore, with the addition of psycholinguistic features, we achieve the strongest result to date reported on this task. Our code and data can be downloaded from https://github. com/bharatambati/sent-compl. 
In this paper, we present a straightforward strategy for transferring dependency parsers across languages. The proposed method learns a parser from partially annotated data obtained through the projection of annotations across unambiguous word alignments. It does not rely on any modeling of the reliability of dependency and/or alignment links and is therefore easy to implement and parameter free. Experiments on six languages show that our method is at par with recent algorithmically demanding methods, at a much cheaper computational cost. It can thus serve as a fair baseline for transferring dependencies across languages with the use of parallel corpora. 
We present a fast, simple, and high-accuracy short answer grading system. Given a shortanswer question and its correct answer, key measures of the correctness of a student response can be derived from its semantic similarity with the correct answer. Our supervised model (1) utilizes recent advances in the identiÔ¨Åcation of short-text similarity, and (2) augments text similarity features with key grading-speciÔ¨Åc constructs. We present experimental results where our model demonstrates top performance on multiple benchmarks. 
This paper presents an study of the use of interlocking phrases in phrase-based statistical machine translation. We examine the effect on translation quality when the translation units used in the translation hypotheses are allowed to overlap on the source side, on the target side and on both sides. A large-scale evaluation on 380 language pairs was conducted. Our results show that overall the use of overlapping phrases improved translation quality by 0.3 BLEU points on average. Further analysis revealed that language pairs requiring a larger amount of re-ordering beneÔ¨Åted the most from our approach. When the evaluation was restricted to such pairs, the average improvement increased to up to 0.75 BLEU points with over 97% of the pairs improving. Our approach requires only a simple modiÔ¨Åcation to the decoding algorithm and we believe it should be generally applicable to improve the performance of phrase-based decoders. 
Poorly translated text is often disÔ¨Çuent and difÔ¨Åcult to read. In contrast, well-formed translations require less time to process. In this paper, we model the differences in reading patterns of Machine Translation (MT) evaluators using novel features extracted from their gaze data, and we learn to predict the quality scores given by those evaluators. We test our predictions in a pairwise ranking scenario, measuring Kendall‚Äôs tau correlation with the judgments. We show that our features provide information beyond Ô¨Çuency, and can be combined with BLEU for better predictions. Furthermore, our results show that reading patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 
This work addresses the task of dependency labeling‚Äîassigning labels to an (unlabeled) dependency tree. We employ and extend a feature representation learning approach, optimizing it for both high speed and accuracy. We apply our labeling model on top of state-of-the-art parsers and evaluate its performance on standard benchmarks including the CoNLL-2009 and the English PTB datasets. Our model processes over 1,700 English sentences per second, which is 30 times faster than the sparse-feature method. It improves labeling accuracy over the outputs of top parsers, achieving the best LAS on 5 out of 7 datasets1. 
We explore the consequences of representing token segmentations as hierarchical structures (trees) for the task of Multiword Expression (MWE) recognition, in isolation or in combination with dependency parsing. We propose a novel representation of token segmentation as trees on tokens, resembling dependency trees. Given this new representation, we present and evaluate two different architectures to combine MWE recognition and dependency parsing in the easy-Ô¨Årst framework: a pipeline and a joint system, both taking advantage of lexical and syntactic dimensions. We experimentally validate that MWE recognition signiÔ¨Åcantly helps syntactic parsing.  syntactic trees, in the form of complex subtrees either with Ô¨Çat structures (Nivre and Nilsson, 2004; EryigÀòit et al., 2011; Seddah et al., 2013) or deeper ones (Vincze et al., 2013; Candito and Constant, 2014). However, these representations do not capture deep lexical analyses like nested MWEs. In this paper, we propose a two-dimensional representation that separates lexical and syntactic layers with two distinct dependency trees sharing the same nodes1. This representation facilitates the annotation of complex lexical phenomena like embedding of MWEs (e.g. I will (take a (rain check))). Given this representation, we present two easy-Ô¨Årst dependency parsing systems: one based on a pipeline architecture and another as a joint parser.  
In this paper, we explore sentiment composition in phrases that have at least one positive and at least one negative word‚Äîphrases like happy accident and best winter break. We compiled a dataset of such opposing polarity phrases and manually annotated them with real-valued scores of sentiment association. Using this dataset, we analyze the linguistic patterns present in opposing polarity phrases. Finally, we apply several unsupervised and supervised techniques of sentiment composition to determine their efÔ¨Åcacy on this dataset. Our best system, which incorporates information from the phrase‚Äôs constituents, their parts of speech, their sentiment association scores, and their embedding vectors, obtains an accuracy of over 80% on the opposing polarity phrases. 
Previous work on Automatic Paraphrase IdentiÔ¨Åcation (PI) is mainly based on modeling text similarity between two sentences. In contrast, we study methods for automatically detecting whether a text fragment only appearing in a sentence of the evaluated sentence pair is important or ancillary information with respect to the paraphrase identiÔ¨Åcation task. Engineering features for this new task is rather difÔ¨Åcult, thus, we approach the problem by representing text with syntactic structures and applying tree kernels on them. The results show that the accuracy of our automatic Ancillary Text ClassiÔ¨Åer (ATC) is promising, i.e., 68.6%, and its output can be used to improve the state of the art in PI. 
Part-of-speech (POS) taggers trained on newswire perform much worse on domains such as subtitles, lyrics, or tweets. In addition, these domains are also heterogeneous, e.g., with respect to registers and dialects. In this paper, we consider the problem of learning a POS tagger for subtitles, lyrics, and tweets associated with African-American Vernacular English (AAVE). We learn from a mixture of randomly sampled and manually annotated Twitter data and unlabeled data, which we automatically and partially label using mined tag dictionaries. Our POS tagger obtains a tagging accuracy of 89% on subtitles, 85% on lyrics, and 83% on tweets, with up to 55% error reductions over a state-of-the-art newswire POS tagger, and 15-25% error reductions over a state-of-the-art Twitter POS tagger. 
The Lexical Substitution task involves selecting and ranking lexical paraphrases for a target word in a given sentential context. We present PIC, a simple measure for estimating the appropriateness of substitutes in a given context. PIC outperforms another simple, comparable model proposed in recent work, especially when selecting substitutes from the entire vocabulary. Analysis shows that PIC improves over baselines by incorporating frequency biases into predictions. 
Most work on extracting parallel text from comparable corpora depends on linguistic resources such as seed parallel documents or translation dictionaries. This paper presents a simple baseline approach for bootstrapping a parallel collection. It starts by observing documents published on similar dates and the cooccurrence of a small number of identical tokens across languages. It then uses fast, online inference for a latent variable model to represent multilingual documents in a shared topic space where it can do efÔ¨Åcient nearestneighbor search. Starting from the Gigaword collections in English and Spanish, we train a translation system that outperforms one trained on the WMT‚Äô11 parallel training set. 
Research on grammatical error correction has received considerable attention. For dealing with all types of errors, grammatical error correction methods that employ statistical machine translation (SMT) have been proposed in recent years. An SMT system generates candidates with scores for all candidates and selects the sentence with the highest score as the correction result. However, the 1-best result of an SMT system is not always the best result. Thus, we propose a reranking approach for grammatical error correction. The reranking approach is used to re-score N-best results of the SMT and reorder the results. Our experiments show that our reranking system using parts of speech and syntactic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0.  Figure 1: Flow of reranking.  
Quotations are kernels not just of wisdom but also of beautiful and striking language. While recent studies have characterized the stylistic features of quotations, we characterize the order of stylistic information within quotations. Analyzing a corpus of two-sentence quotations collected from the social network Tumblr, we explore the ways that both low-level features and high-level features tend to occur in either the Ô¨Årst or second sentence. Through analysis of examples, we interpret these tendencies as manifestations of rhetorical patterns. Results from a prediction task suggest that stylistic patterns are more prominent in quotations than in a comparison corpus.  Guerini et al., 2015); this ongoing project can be seen as a contemporary, empirical investigation of the rhetorical arts. Inspired by the way that classical rhetoricians described complicated and highlevel linguistic patterns, the present study takes the novel step of analyzing the way linguistic elements are sequenced within quotations. After describing a minimalistic set of stylistic features designed to capture common patterns in quotations, we demonstrate that some features tend to occur in certain positions within a quotation. We investigate whether quotes are more predictable than other genres in a ‚ÄúQuote Ordering Task‚Äù in which the goal is to distinguish the correct version of a quotation from one whose sentences have been reversed.  
This paper proposes an incremental learning strategy for neural word embedding methods, such as SkipGrams and Global Vectors. Since our method iteratively generates embedding vectors one dimension at a time, obtained vectors equip a unique property. Namely, any right-truncated vector matches the solution of the corresponding lower-dimensional embedding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications. 
Recent work across several AI subdisciplines has focused on automatically solving math word problems. In this paper we introduce MAWPS, an online repository of Math Word Problems, to provide a uniÔ¨Åed testbed to evaluate different algorithms. MAWPS allows for the automatic construction of datasets with particular characteristics, providing tools for tuning the lexical and template overlap of a dataset as well as for Ô¨Åltering ungrammatical problems from web-sourced corpora. The online nature of this repository facilitates easy community contribution. At present, we have amassed 3,320 problems, including the full datasets used in several prominent works. 
The goal of Event extraction is to extract structured information of events that are of interest from unstructured documents. Existing event extractors for social media suffer from two major problems: lack of context and informal nature. In this paper, instead of conducting event extraction solely on each social media message, we incorporate cross-genre knowledge to boost the event extractor performance. Experiment results demonstrate that without any additional annotations, our proposed approach is able to provide 15% absolute F-score improvement over the state-of-the-art. 
We present Emergent, a novel data-set derived from a digital journalism project for rumour debunking. The data-set contains 300 rumoured claims and 2,595 associated news articles, collected and labelled by journalists with an estimation of their veracity (true, false or unveriÔ¨Åed). Each associated article is summarized into a headline and labelled to indicate whether its stance is for, against, or observing the claim, where observing indicates that the article merely repeats the claim. Thus, Emergent provides a real-world data source for a variety of natural language processing tasks in the context of fact-checking. Further to presenting the dataset, we address the task of determining the article headline stance with respect to the claim. For this purpose we use a logistic regression classiÔ¨Åer and develop features that examine the headline and its agreement with the claim. The accuracy achieved was 73% which is 26% higher than the one achieved by the Excitement Open Platform (Magnini et al., 2014). 
Word clusters are useful for many NLP tasks including training neural network language models, but current increases in datasets are outpacing the ability of word clusterers to handle them. Little attention has been paid thus far on inducing high-quality word clusters at a large scale. The predictive exchange algorithm is quite scalable, but sometimes does not provide as good perplexity as other slower clustering algorithms. We introduce the bidirectional, interpolated, reÔ¨Åning, and alternating (BIRA) predictive exchange algorithm. It improves upon the predictive exchange algorithm‚Äôs perplexity by up to 18%, giving it perplexities comparable to the slower two-sided exchange algorithm, and better perplexities than the slower Brown clustering algorithm. Our BIRA implementation is fast, clustering a 2.5 billion token English News Crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available. 
When translating into a morphologically complex language, segmenting the target language can reduce data sparsity, while introducing the complication of desegmenting the system output. We present a method for decoderintegrated desegmentation, allowing features that consider the desegmented target, such as a word-level language model, to be considered throughout the entire search space. Our results on a large-scale, English to Arabic translation task show signiÔ¨Åcant improvement over the 1-best desegmentation baseline. 
INSTANTIATION is a fairly common discourse relation and past work has suggested that it plays special roles in local coherence, in sentiment expression and in content selection in summarization. In this paper we provide the Ô¨Årst systematic corpus analysis of the relation and show that relation-speciÔ¨Åc features can improve considerably the detection of the relation. We show that sentences involved in INSTANTIATION are set apart from other sentences by the use of gradable (subjective) adjectives, the occurrence of rare words and by different patterns in part-of-speech usage. Words across arguments of INSTANTIATION are connected through hypernym and meronym relations signiÔ¨Åcantly more often than in other sentences and that they stand out in context by being signiÔ¨Åcantly less similar to each other than other adjacent sentence pairs. These factors provide substantial predictive power that improves the identiÔ¨Åcation of implicit INSTANTIATION relation by more than 5% F-measure. 
We introduce the task of cross-lingual lexical entailment, which aims to detect whether the meaning of a word in one language can be inferred from the meaning of a word in another language. We construct a gold standard for this task, and propose an unsupervised solution based on distributional word representations. As commonly done in the monolingual setting, we assume a word e entails a word f if the prominent context features of e are a subset of those of f . To address the challenge of comparing contexts across languages, we propose a novel method for inducing sparse bilingual word representations from monolingual and parallel texts. Our approach yields an Fscore of 70%, and signiÔ¨Åcantly outperforms strong baselines based on translation and on existing word representations. 
Given the limited success of medication in reversing the effects of Alzheimer‚Äôs and other dementias, a lot of the neuroscience research has been focused on early detection, in order to slow the progress of the disease through different interventions. We propose a Natural Language Processing approach applied to descriptive writing to attempt to discriminate decline due to normal aging from decline due to pre-dementia conditions. Within the context of a longitudinal study on Alzheimer‚Äôs disease, we created a unique corpus of 201 descriptions of a control image written by subjects of the study. Our classifier, computing linguistic features, was able to discriminate normal from cognitively impaired patients to an accuracy of 86.1% using lexical and semantic irregularities found in their writing. This is a promising result towards elucidating the existence of a general pattern in linguistic deterioration caused by dementia that might be detectable from a subject‚Äôs written descriptive language.  
Current approaches to Information Extraction (IE) are capable of extracting large amounts of facts with associated probabilities. Because no current IE system is perfect, complementary and conÔ¨Çicting facts are obtained when different systems are run over the same data. Knowledge Fusion (KF) is the problem of aggregating facts from different extractors. Existing methods approach KF using supervised learning or deep linguistic knowledge, which either lack sufÔ¨Åcient data or are not robust enough. We propose a semi-supervised application of Consensus Maximization to the KF problem, using a combination of supervised and unsupervised models. Consensus Maximization Fusion (CM Fusion) is able to promote high quality facts and eliminate incorrect ones. We demonstrate the effectiveness of our system on the NIST Slot Filler Validation contest, which seeks to evaluate and aggregate multiple independent information extractors. Our system achieved the highest F1 score relative to other system submissions. 
We present a simple algorithm to efÔ¨Åciently train language models with noise-contrastive estimation (NCE) on graphics processing units (GPUs). Our NCE-trained language models achieve signiÔ¨Åcantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in Chelba et al. (2013). When incorporated into a strong Arabic-English machine translation system they give a strong boost in translation quality. We release a toolkit so that others may also train large-scale, large vocabulary LSTM language models with NCE, parallelizing computation across multiple GPUs. 
A simile is a Ô¨Ågure of speech comparing two fundamentally different things. Sometimes, a simile will explain the basis of a comparison by explicitly mentioning a shared property. For example, ‚Äúmy room is as cold as Antarctica‚Äù gives ‚Äúcold‚Äù as the property shared by the room and Antarctica. But most similes do not give an explicit property (e.g., ‚Äúmy room feels like Antarctica‚Äù) leaving the reader to infer that the room is cold. We tackle the problem of automatically inferring implicit properties evoked by similes. Our approach involves three steps: (1) generating candidate properties from different sources, (2) evaluating properties based on the inÔ¨Çuence of multiple simile components, and (3) aggregated ranking of the properties. We also present an analysis showing that the difÔ¨Åculty of inferring an implicit property for a simile correlates with its interpretive diversity. 
Interactive machine translation (IMT) is a method which uses human-computer interactions to improve the quality of MT. Traditional IMT methods employ a left-to-right order for the interactions, which is difficult to directly modify critical errors at the end of the sentence. In this paper, we propose an IMT framework in which the interaction is decomposed into two simple human actions: picking a critical translation error (Pick) and revising the translation (Revise). The picked phrase could be at any position of the sentence, which improves the efficiency of human computer interaction. We also propose automatic suggestion models for the two actions to further reduce the cost of human interaction. Experiment results demonstrate that by interactions through either one of the actions, the translation quality could be significantly improved. Greater gains could be achieved by iteratively performing both actions. 
Recurrent neural network language models (RNNLM) have recently demonstrated vast potential in modelling long-term dependencies for NLP problems, ranging from speech recognition to machine translation. In this work, we propose methods for conditioning RNNLMs on external side information, e.g., metadata such as keywords, description, document title or topic headline. Our experiments show consistent improvements of RNNLMs using side information over the baselines for two different datasets and genres in two languages. Interestingly, we found that side information in a foreign language can be highly beneÔ¨Åcial in modelling texts in another language, serving as a form of cross-lingual language modelling. 
A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention‚Äôs context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 
We describe a technique for adding contextual distinctions to word embeddings by extending the usual embedding process ‚Äî into two phases. The Ô¨Årst phase resembles existing methods, but also constructs K classiÔ¨Åcations of concepts. The second phase uses these classiÔ¨Åcations in developing reÔ¨Åned K embeddings for words, namely word K-embeddings. The technique is iterative, scalable, and can be combined with other methods (including Word2Vec) in achieving still more expressive representations. Experimental results show consistently large performance gains on a Semantic-Syntactic Word Relationship test set for different K settings. For example, an overall gain of 20% is recorded at K = 5. In addition, we demonstrate that an iterative process can further tune the embeddings and gain an extra 1% (K = 10 in 3 iterations) on the same benchmark. The examples also show that polysemous concepts are meaningfully embedded in our K different conceptual embeddings for words. 
In this paper, we study, compare and combine two state-of-the-art approaches to automatic feature engineering: Convolution Tree Kernels (CTKs) and Convolutional Neural Networks (CNNs) for learning to rank answer sentences in a Question Answering (QA) setting. When dealing with QA, the key aspect is to encode relational information between the constituents of question and answer in learning algorithms. For this purpose, we propose novel CNNs using relational information and combined them with relational CTKs. The results show that (i) both approaches achieve the state of the art on a question answering task, where CTKs produce higher accuracy and (ii) combining such methods leads to unprecedented high results. 
Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for Ô¨Ånding semantically related questions. The task is difÔ¨Åcult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and Ô¨Åne-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs).1 
We present a simple yet powerful approach to non-factoid answer reranking whereby question-answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is used to compute the score for an answer. Despite its simplicity, our approach achieves state-of-the-art performance on a public dataset of How questions, outperforming systems which employ sophisticated feature sets. We attribute this good performance to the use of paragraph instead of word vector representations and to the use of suitable data for training these representations. 
We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than languagespeciÔ¨Åc words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-ofthe-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning ‚Äúfrom scratch‚Äù in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text. 
In the absence of annotations in the target language, multilingual models typically draw on extensive parallel resources. In this paper, we demonstrate that accurate multilingual partof-speech (POS) tagging can be done with just a few (e.g., ten) word translation pairs. We use the translation pairs to establish a coarse linear isometric (orthonormal) mapping between monolingual embeddings. This enables the supervised source model expressed in terms of embeddings to be used directly on the target language. We further reÔ¨Åne the model in an unsupervised manner by initializing and regularizing it to be close to the direct transfer model. Averaged across six languages, our model yields a 37.5% absolute improvement over the monolingual prototypedriven method (Haghighi and Klein, 2006) when using a comparable amount of supervision. Moreover, to highlight key linguistic characteristics of the generated tags, we use them to predict typological properties of languages, obtaining a 50% error reduction relative to the prototype model.1 
As more historical texts are digitized, there is interest in applying natural language processing tools to these archives. However, the performance of these tools is often unsatisfactory, due to language change and genre differences. Spelling normalization heuristics are the dominant solution for dealing with historical texts, but this approach fails to account for changes in usage and vocabulary. In this empirical paper, we assess the capability of domain adaptation techniques to cope with historical texts, focusing on the classic benchmark task of part-of-speech tagging. We evaluate several domain adaptation methods on the task of tagging Early Modern English and Modern British English texts in the Penn Corpora of Historical English. We demonstrate that the Feature Embedding method for unsupervised domain adaptation outperforms word embeddings and Brown clusters, showing the importance of embedding the entire feature space, rather than just individual words. Feature Embeddings also give better performance than spelling normalization, but the combination of the two methods is better still, yielding a 5% raw improvement in tagging accuracy on Early Modern English texts. 
Creole languages do not Ô¨Åt into the traditional tree model of evolutionary history because multiple languages are involved in their formation. In this paper, we present several statistical models to explore the nature of creole genesis. After reviewing quantitative studies on creole genesis, we Ô¨Årst tackle the question of whether creoles are typologically distinct from non-creoles. By formalizing this question as a binary classiÔ¨Åcation problem, we demonstrate that a linear classiÔ¨Åer fails to separate creoles from non-creoles although the two groups have substantially different distributions in the feature space. We then model a creole language as a mixture of source languages plus a special restructurer. We Ô¨Ånd a pervasive inÔ¨Çuence of the restructurer in creole genesis and some statistical universals in it, paving the way for more elaborate statistical models. 
In this study, the problem of shallow parsing of Hindi-English code-mixed social media text (CSMT) has been addressed. We have annotated the data, developed a language identiÔ¨Åer, a normalizer, a part-of-speech tagger and a shallow parser. To the best of our knowledge, we are the Ô¨Årst to attempt shallow parsing on CSMT. The pipeline developed has been made available to the research community with the goal of enabling better text analysis of Hindi English CSMT. The pipeline is accessible at 1. 
We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. 
We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences‚Äîa domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural Ô¨Åt. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually. 
Unsupervised methods for learning distributed representations of words are ubiquitous in today‚Äôs NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We Ô¨Ånd that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-bilinear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. 
Jauhar et al. (2015) recently proposed to learn sense-speciÔ¨Åc word representations by ‚ÄúretroÔ¨Åtting‚Äù standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph deÔ¨Åning word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types signiÔ¨Åcantly improves performance. 
Understanding the argumentative structure of a persuasive essay involves addressing two challenging tasks: identifying the components of the essay‚Äôs argument and identifying the relations that occur between them. We examine the under-investigated task of end-toend argument mining in persuasive student essays, where we (1) present the Ô¨Årst results on end-to-end argument mining in student essays using a pipeline approach; (2) address error propagation inherent in the pipeline approach by performing joint inference over the outputs of the tasks in an Integer Linear Programming (ILP) framework; and (3) propose a novel objective function that enables F-score to be maximized directly by an ILP solver. We evaluate our joint-inference approach with our novel objective function on a publiclyavailable corpus of 90 essays, where it yields an 18.5% relative error reduction in F-score over the pipeline system. 
Argumentation mining is considered as a key technology for future search engines and automated decision making. In such applications, argumentative text segments have to be mined from large and diverse document collections. However, most existing argumentation mining approaches tackle the classiÔ¨Åcation of argumentativeness only for a few manually annotated documents from narrow domains and registers. This limits their practical applicability. We hence propose a distant supervision approach that acquires argumentative text segments automatically from online debate portals. Experiments across domains and registers show that training on such a corpus improves the effectiveness and robustness of mining argumentative text. We freely provide the underlying corpus for research. 
Persuasive communication is the process of shaping, reinforcing and changing others‚Äô responses. In political debates, speakers express their views towards the debated topics by choosing both the content of their discourse and the argumentation process. In this work we study the use of semantic frames for modelling argumentation in speakers‚Äô discourse. We investigate the impact of a speaker‚Äôs argumentation style and their effect in inÔ¨Çuencing an audience in supporting their candidature. We model the inÔ¨Çuence index of each candidate based on their relative standings in the polls released prior to the debate and present a system which ranks speakers in terms of their relative inÔ¨Çuence using a combination of content and persuasive argumentation features. Our results show that although content alone is predictive of a speaker‚Äôs inÔ¨Çuence rank, persuasive argumentation also affects such indices. 
Coherence is established by semantic connections between sentences of a text which can be modeled by lexical relations. In this paper, we introduce the lexical coherence graph (LCG), a new graph-based model to represent lexical relations among sentences. The frequency of subgraphs (coherence patterns) of this graph captures the connectivity style of sentence nodes in this graph. The coherence of a text is encoded by a vector of these frequencies. We evaluate the LCG model on the readability ranking task. The results of the experiments show that the LCG model obtains higher accuracy than state-of-the-art coherence models. Using larger subgraphs yields higher accuracy, because they capture more structural information. However, larger subgraphs can be sparse. We adapt Kneser-Ney smoothing to smooth subgraphs‚Äô frequencies. Smoothing improves performance. 
While there is increasing interest in automatically recognizing the argumentative structure of a text, recognizing the argumentative purpose of revisions to such texts has been less explored. Furthermore, existing revision classiÔ¨Åcation approaches typically ignore contextual information. We propose two approaches for utilizing contextual information when predicting argumentative revision purposes: developing contextual features for use in the classiÔ¨Åcation paradigm of prior work, and transforming the classiÔ¨Åcation problem to a sequence labeling task. Experimental results using two corpora of student essays demonstrate the utility of contextual information for predicting argumentative revision purposes. 
This paper presents a methodology to extract positive interpretations from negated statements. First, we automatically generate plausible interpretations using well-known grammar rules and manipulating semantic roles. Second, we score plausible alternatives according to their likelihood. Manual annotations show that the positive interpretations are intuitive to humans, and experimental results show that the scoring task can be automated. 
Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI). In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neural attention model for NLI but is based on a signiÔ¨Åcantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classiÔ¨Åcation, our solution uses a match-LSTM to perform wordby-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. On the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the state of the art. 
We introduce a latent activity model for workplace emails, positing that communication at work is purposeful and organized by activities. We pose the problem as probabilistic inference in graphical models that jointly capture the interplay between latent activities and the email contexts they govern, such as the recipients, subject and body. The model parameters are learned using maximum likelihood estimation with an expectation maximization algorithm. We present three variants of the model that incorporate the recipients, co-occurrence of the recipients, and email body and subject. We demonstrate the model‚Äôs effectiveness in an email recipient recommendation task and show that it outperforms a state-of-the-art generative model. Additionally, we show that the activity model can be used to identify email senders who engage in similar activities, resulting in further improvements in recipient recommendation. 
Automatically generated databases of English paraphrases have the drawback that they return a single list of paraphrases for an input word or phrase. This means that all senses of polysemous words are grouped together, unlike WordNet which partitions different senses into separate synsets. We present a new method for clustering paraphrases by word sense, and apply it to the Paraphrase Database (PPDB). We investigate the performance of hierarchical and spectral clustering algorithms, and systematically explore different ways of deÔ¨Åning the similarity matrix that they use as input. Our method produces sense clusters that are qualitatively and quantitatively good, and that represent a substantial improvement to the PPDB resource. 
We propose a hierarchical attention network for document classiÔ¨Åcation. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classiÔ¨Åcation tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences. 
Traditional convolutional neural network (CNN) based query classiÔ¨Åcation uses linear feature mapping in its convolution operation. The recurrent neural network (RNN), differs from a CNN in representing word sequence with their ordering information kept explicitly. We propose using a deep long-short-term-memory (DLSTM) based feature mapping to learn feature representation for CNN. The DLSTM, which is a stack of LSTM units, has different order of feature representations at different depth of LSTM unit. The bottom LSTM unit equipped with input and output gates, extracts the Ô¨Årst order feature representation from current word. To extract higher order nonlinear feature representation, the LSTM unit at higher position gets input from two parts. First part is the lower LSTM unit‚Äôs memory cell from previous word. Second part is the lower LSTM unit‚Äôs hidden output from current word. In this way, the DLSTM captures the nonlinear nonconsecutive interaction within n-grams. Using an architecture that combines a stack of the DLSTM layers with a tradition CNN layer, we have observed new state-of-the-art query classiÔ¨Åcation accuracy on benchmark data sets for query classiÔ¨Åcation. 
The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a generalpurpose classiÔ¨Åcation system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long ShortTerm Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentencelevel tasks. Moreover, unlike other CNNbased models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-ofthe-art performance on several tasks, including sentiment analysis, question type classiÔ¨Åcation, and subjectivity classiÔ¨Åcation. 
We introduce a novel, simple convolution neural network (CNN) architecture ‚Äì multi-group norm constraint CNN (MGNC-CNN) ‚Äì that capitalizes on multiple sets of word embeddings for sentence classiÔ¨Åcation. MGNCCNN extracts features from input embedding sets independently and then joins these at the penultimate layer in the network to form a Ô¨Ånal feature vector. We then adopt a group regularization strategy that differentially penalizes weights associated with the subcomponents generated from the respective embedding sets. This model is much simpler than comparable alternative architectures and requires substantially less training time. Furthermore, it is Ô¨Çexible in that it does not require input word embeddings to be of the same dimensionality. We show that MGNC-CNN consistently outperforms baseline models. 
We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches. 
Understanding how a Ô¨Åctional relationship between two characters changes over time (e.g., from best friends to sworn enemies) is a key challenge in digital humanities scholarship. We present a novel unsupervised neural network for this task that incorporates dictionary learning to generate interpretable, accurate relationship trajectories. While previous work on characterizing literary relationships relies on plot summaries annotated with predeÔ¨Åned labels, our model jointly learns a set of global relationship descriptors as well as a trajectory over these descriptors for each relationship in a dataset of raw text from novels. We Ô¨Ånd that our model learns descriptors of events (e.g., marriage or murder) as well as interpersonal states (love, sadness). Our model outperforms topic model baselines on two crowdsourced tasks, and we also Ô¨Ånd interesting correlations to annotations in an existing dataset. 
We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains. 
In this document we report on a user-scenario-based evaluation aiming at assessing the performance of machine translation (MT) systems in a real context of use. We describe a sequel of experiments that has been performed to estimate the usefulness of MT and to test if improvements of MT technology lead to better performance in the usage scenario. One goal is to find the best methodology for evaluating the eventual benefit of a machine translation system in an application. The evaluation is based on the QTLeap corpus, a novel multilingual language resource that was collected through a real-life support service via chat. It is composed of naturally occurring utterances produced by users while interacting with a human technician providing answers. The corpus is available in eight different languages: Basque, Bulgarian, Czech, Dutch, English, German, Portuguese and Spanish.
Out-of-vocabulary words (OOVs) are a ubiquitous and difficult problem in statistical machine translation (SMT). This paper studies different strategies of using BabelNet to alleviate the negative impact brought about by OOVs. BabelNet is a multilingual encyclopedic dictionary and a semantic network, which not only includes lexicographic and encyclopedic terms, but connects concepts and named entities in a very large network of semantic relations. By taking advantage of the knowledge in BabelNet, three different methods ‚Äï using direct training data, domain-adaptation techniques and the BabelNet API ‚Äï are proposed in this paper to obtain translations for OOVs to improve system performance. Experimental results on English‚ÄïPolish and English‚ÄïChinese language pairs show that domain adaptation can better utilize BabelNet knowledge and performs better than other methods. The results also demonstrate that BabelNet is a really useful tool for improving translation performance of SMT systems.
The present work is an overview of the TraMOOC (Translation for Massive Open Online Courses) research and innovation project, a machine translation approach for online educational content. More specifically, videolectures, assignments, and MOOC forum text is automatically translated from English into eleven European and BRIC languages. Unlike previous approaches to machine translation, the output quality in TraMOOC relies on a multimodal evaluation schema that involves crowdsourcing, error type markup, an error taxonomy for translation model comparison, and implicit evaluation via text mining, i.e. entity recognition and its performance comparison between the source and the translated text, and sentiment analysis on the students{'} forum posts. Finally, the evaluation output will result in more and better quality in-domain parallel data that will be fed back to the translation engine for higher quality output. The translation service will be incorporated into the Iversity MOOC platform and into the VideoLectures.net digital library portal.
While an increasing number of (automatic) metrics is available to assess the linguistic quality of machine translations, their interpretation remains cryptic to many users, specifically in the translation community. They are clearly useful for indicating certain overarching trends, but say little about actual improvements for translation buyers or post-editors. However, these metrics are commonly referenced when discussing pricing and models, both with translation buyers and service providers. With the aim of focusing on automatic metrics that are easier to understand for non-research users, we identified Edit Distance (or Post-Edit Distance) as a good fit. While Edit Distance as such does not express cognitive effort or time spent editing machine translation suggestions, we found that it correlates strongly with the productivity tests we performed, for various language pairs and domains. This paper aims to analyse Edit Distance and productivity data on a segment level based on data gathered over some years. Drawing from these findings, we want to then explore how Edit Distance could help in predicting productivity on new content. Some further analysis is proposed, with findings to be presented at the conference.
We present a freely available corpus containing source language texts from different domains along with their automatically generated translations into several distinct morphologically rich languages, their post-edited versions, and error annotations of the performed post-edit operations. We believe that the corpus will be useful for many different applications. The main advantage of the approach used for creation of the corpus is the fusion of post-editing and error classification tasks, which have usually been seen as two independent tasks, although naturally they are not. We also show benefits of coupling automatic and manual error classification which facilitates the complex manual error annotation task as well as the development of automatic error classification tools. In addition, the approach facilitates annotation of language pair related issues.
Existing Arabic sentiment lexicons have low coverage‚Äïwith only a few thousand entries. In this paper, we present several large sentiment lexicons that were automatically generated using two different methods: (1) by using distant supervision techniques on Arabic tweets, and (2) by translating English sentiment lexicons into Arabic using a freely available statistical machine translation system. We compare the usefulness of new and old sentiment lexicons in the downstream application of sentence-level sentiment analysis. Our baseline sentiment analysis system uses numerous surface form features. Nonetheless, the system benefits from using additional features drawn from sentiment lexicons. The best result is obtained using the automatically generated Dialectal Hashtag Lexicon and the Arabic translations of the NRC Emotion Lexicon (accuracy of 66.6{\%}). Finally, we describe a qualitative study of the automatic translations of English sentiment lexicons into Arabic, which shows that about 88{\%} of the automatically translated entries are valid for English as well. Close to 10{\%} of the invalid entries are caused by gross mistranslations, close to 40{\%} by translations into a related word, and about 50{\%} by differences in how the word is used in Arabic.
Sentiment Analysis systems aims at detecting opinions and sentiments that are expressed in texts. Many approaches in literature are based on resources that model the prior polarity of words or multi-word expressions, i.e. a polarity lexicon. Such resources are defined by teams of annotators, i.e. a manual annotation is provided to associate emotional or sentiment facets to the lexicon entries. The development of such lexicons is an expensive and language dependent process, making them often not covering all the linguistic sentiment phenomena. Moreover, once a lexicon is defined it can hardly be adopted in a different language or even a different domain. In this paper, we present several Distributional Polarity Lexicons (DPLs), i.e. large-scale polarity lexicons acquired with an unsupervised methodology based on Distributional Models of Lexical Semantics. Given a set of heuristically annotated sentences from Twitter, we transfer the sentiment information from sentences to words. The approach is mostly unsupervised, and experimental evaluations on Sentiment Analysis tasks in two languages show the benefits of the generated resources. The generated DPLs are publicly available in English and Italian.
In this paper, we analyze the sentiments derived from the conversations that occur in social networks. Our goal is to identify the sentiments of the users in the social network through their conversations. We conduct a study to determine whether users of social networks (twitter in particular) tend to gather together according to the likeness of their sentiments. In our proposed framework, (1) we use ANEW, a lexical dictionary to identify affective emotional feelings associated to a message according to the Russell{'}s model of affection; (2) we design a topic modeling mechanism called Sent{\_}LDA, based on the Latent Dirichlet Allocation (LDA) generative model, which allows us to find the topic distribution in a general conversation and we associate topics with emotions; (3) we detect communities in the network according to the density and frequency of the messages among the users; and (4) we compare the sentiments of the communities by using the Russell{'}s model of affect versus polarity and we measure the extent to which topic distribution strengthen likeness in the sentiments of the users of a community. This works contributes with a topic modeling methodology to analyze the sentiments in conversations that take place in social networks.
A key point in Sentiment Analysis is to determine the polarity of the sentiment implied by a certain word or expression. In basic Sentiment Analysis systems this sentiment polarity of the words is accounted and weighted in different ways to provide a degree of positivity/negativity. Currently words are also modelled as continuous dense vectors, known as word embeddings, which seem to encode interesting semantic knowledge. With regard to Sentiment Analysis, word embeddings are used as features to more complex supervised classification systems to obtain sentiment classifiers. In this paper we compare a set of existing sentiment lexicons and sentiment lexicon generation techniques. We also show a simple but effective technique to calculate a word polarity value for each word in a domain using existing continuous word embeddings generation methods. Further, we also show that word embeddings calculated on in-domain corpus capture the polarity better than the ones calculated on general-domain corpus.
Emotion Recognition (ER) is an important part of dialogue analysis which can be used in order to improve the quality of Spoken Dialogue Systems (SDSs). The emotional hypothesis of the current response of an end-user might be utilised by the dialogue manager component in order to change the SDS strategy which could result in a quality enhancement. In this study additional speaker-related information is used to improve the performance of the speech-based ER process. The analysed information is the speaker identity, gender and age of a user. Two schemes are described here, namely, using additional information as an independent variable within the feature vector and creating separate emotional models for each speaker, gender or age-cluster independently. The performances of the proposed approaches were compared against the baseline ER system, where no additional information has been used, on a number of emotional speech corpora of German, English, Japanese and Russian. The study revealed that for some of the corpora the proposed approach significantly outperforms the baseline methods with a relative difference of up to 11.9{\%}.
We address the task of automatically estimating the missing values of linguistic features by making use of the fact that some linguistic features in typological databases are informative to each other. The questions to address in this work are (i) how much predictive power do features have on the value of another feature? (ii) to what extent can we attribute this predictive power to genealogical or areal factors, as opposed to being provided by tendencies or implicational universals? To address these questions, we conduct a discriminative or predictive analysis on the typological database. Specifically, we use a machine-learning classifier to estimate the value of each feature of each language using the values of the other features, under different choices of training data: all the other languages, or all the other languages except for the ones having the same origin or area with the target language.
We present a study of the adequacy of current methods that are used for POS-tagging historical Dutch texts, as well as an exploration of the influence of employing different techniques to improve upon the current practice. The main focus of this paper is on (unsupervised) methods that are easily adaptable for different domains without requiring extensive manual input. It was found that modernising the spelling of corpora prior to tagging them with a tagger trained on contemporary Dutch results in a large increase in accuracy, but that spelling normalisation alone is not sufficient to obtain state-of-the-art results. The best results were achieved by training a POS-tagger on a corpus automatically annotated by projecting (automatically assigned) POS-tags via word alignments from a contemporary corpus. This result is promising, as it was reached without including any domain knowledge or context dependencies. We argue that the insights of this study combined with semi-supervised learning techniques for domain adaptation can be used to develop a general-purpose diachronic tagger for Dutch.
In this paper we present a language resource for German, composed of a list of 1,021 unique errors extracted from a collection of texts written by people with dyslexia. The errors were annotated with a set of linguistic characteristics as well as visual and phonetic features. We present the compilation and the annotation criteria for the different types of dyslexic errors. This language resource has many potential uses since errors written by people with dyslexia reflect their difficulties. For instance, it has already been used to design language exercises to treat dyslexia in German. To the best of our knowledge, this is first resource of this kind in German.
In this paper, we present the CItA corpus (Corpus Italiano di Apprendenti L1), a collection of essays written by Italian L1 learners collected during the first and second year of lower secondary school. The corpus was built in the framework of an interdisciplinary study jointly carried out by computational linguistics and experimental pedagogists and aimed at tracking the development of written language competence over the years and students{'} background information.
Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Various unsupervised and semi-supervised methods have been proposed to tag an unseen language. However, many of them require some partial understanding of the target language because they rely on dictionaries or parallel corpora such as the Bible. In this paper, we propose a different method named delexicalized tagging, for which we only need a raw corpus of the target language. We transfer tagging models trained on annotated corpora of one or more resource-rich languages. We employ language-independent features such as word length, frequency, neighborhood entropy, character classes (alphabetic vs. numeric vs. punctuation) etc. We demonstrate that such features can, to certain extent, serve as predictors of the part of speech, represented by the universal POS tag.
The SpeDial consortium is sharing two datasets that were used during the SpeDial project. By sharing them with the community we are providing a resource to reduce the duration of cycle of development of new Spoken Dialogue Systems (SDSs). The datasets include audios and several manual annotations, i.e., miscommunication, anger, satisfaction, repetition, gender and task success. The datasets were created with data from real users and cover two different languages: English and Greek. Detectors for miscommunication, anger and gender were trained for both systems. The detectors were particularly accurate in tasks where humans have high annotator agreement such as miscommunication and gender. As expected due to the subjectivity of the task, the anger detector had a less satisfactory performance. Nevertheless, we proved that the automatic detection of situations that can lead to problems in SDSs is possible and can be a promising direction to reduce the duration of SDS{'}s development cycle.
This paper describes a method to automatically create dialogue resources annotated with dialogue act information by reusing existing dialogue corpora. Numerous dialogue corpora are available for research purposes and many of them are annotated with dialogue act information that captures the intentions encoded in user utterances. Annotated dialogue resources, however, differ in various respects: data collection settings and modalities used, dialogue task domains and scenarios (if any) underlying the collection, number and roles of dialogue participants involved and dialogue act annotation schemes applied. The presented study encompasses three phases of data-driven investigation. We, first, assess the importance of various types of features and their combinations for effective cross-domain dialogue act classification. Second, we establish the best predictive model comparing various cross-corpora training settings. Finally, we specify models adaptation procedures and explore late fusion approaches to optimize the overall classification decision taking process. The proposed methodology accounts for empirically motivated and technically sound classification procedures that may reduce annotation and training costs significantly.
In this paper, we present a taxonomy of stories told in dialogue. We based our scheme on prior work analyzing narrative structure and method of telling, relation to storyteller identity, as well as some categories particular to dialogue, such as how the story gets introduced. Our taxonomy currently has 5 major dimensions, with most having sub-dimensions - each dimension has an associated set of dimension-specific labels. We adapted an annotation tool for this taxonomy and have annotated portions of two different dialogue corpora, Switchboard and the Distress Analysis Interview Corpus. We present examples of some of the tags and concepts with stories from Switchboard, and some initial statistics of frequencies of the tags.
PentoRef is a corpus of task-oriented dialogues collected in systematically manipulated settings. The corpus is multilingual, with English and German sections, and overall comprises more than 20000 utterances. The dialogues are fully transcribed and annotated with referring expressions mapped to objects in corresponding visual scenes, which makes the corpus a rich resource for research on spoken referring expressions in generation and resolution. The corpus includes several sub-corpora that correspond to different dialogue situations where parameters related to interactivity, visual access, and verbal channel have been manipulated in systematic ways. The corpus thus lends itself to very targeted studies of reference in spontaneous dialogue.
Spoken conversation corpora often adapt existing Dialogue Act (DA) annotation specifications, such as DAMSL, DIT++, etc., to task specific needs, yielding incompatible annotations; thus, limiting corpora re-usability. Recently accepted ISO standard for DA annotation {--} Dialogue Act Markup Language (DiAML) {--} is designed as domain and application independent. Moreover, the clear separation of dialogue dimensions and communicative functions, coupled with the hierarchical organization of the latter, allows for classification at different levels of granularity. However, re-annotating existing corpora with the new scheme might require significant effort. In this paper we test the utility of the ISO standard through comparative evaluation of the corpus-specific legacy and the semi-automatically transferred DiAML DA annotations on supervised dialogue act classification task. To test the domain independence of the resulting annotations, we perform cross-domain and data aggregation evaluation. Compared to the legacy annotation scheme, on the Italian LUNA Human-Human corpus, the DiAML annotation scheme exhibits better cross-domain and data aggregation classification performance, while maintaining comparable in-domain performance.
This paper presents WikiCoref, an English corpus annotated for anaphoric relations, where all documents are from the English version of Wikipedia. Our annotation scheme follows the one of OntoNotes with a few disparities. We annotated each markable with coreference type, mention type and the equivalent Freebase topic. Since most similar annotation efforts concentrate on very specific types of written text, mainly newswire, there is a lack of resources for otherwise over-used Wikipedia texts. The corpus described in this paper addresses this issue. We present a freely available resource we initially devised for improving coreference resolution algorithms dedicated to Wikipedia texts. Our corpus has no restriction on the topics of the documents being annotated, and documents of various sizes have been considered for annotation.
The aim of distributional semantics is to model the similarity of the meaning of words via the words they occur with. Thereby, it relies on the distributional hypothesis implying that similar words have similar contexts. Deducing meaning from the distribution of words is interesting as it can be done automatically on large amounts of freely available raw text. It is because of this convenience that most current state-of-the-art-models of distributional semantics operate on raw text, although there have been successful attempts to integrate other kinds of‚Äïe.g., syntactic‚Äïinformation to improve distributional semantic models. In contrast, less attention has been paid to semantic information in the research community. One reason for this is that the extraction of semantic information from raw text is a complex, elaborate matter and in great parts not yet satisfyingly solved. Recently, however, there have been successful attempts to integrate a certain kind of semantic information, i.e., co-reference. Two basically different kinds of information contributed by co-reference with respect to the distribution of words will be identified. We will then focus on one of these and examine its general potential to improve distributional semantic models as well as certain more specific hypotheses.
This paper presents the adaptation of an Entity Centric Model for Portuguese coreference resolution, considering 10 named entity categories. The model was evaluated on named e using the HAREM Portuguese corpus and the results are 81.0{\%} of precision and 58.3{\%} of recall overall, the resulting system is freely available
This paper presents a data-driven co-reference resolution system for German that has been adapted from IMS HotCoref, a co-reference resolver for English. It describes the difficulties when resolving co-reference in German text, the adaptation process and the features designed to address linguistic challenges brought forth by German. We report performance on the reference dataset T{\"u
This paper describes a coreference annotation scheme, coreference annotation specific issues and their solutions through our proposed annotation scheme for Hindi. We introduce different co-reference relation types between continuous mentions of the same coreference chain such as {``}Part-of{''}, {``}Function-value pair{''} etc. We used Jaccard similarity based Krippendorff{`}s{'} alpha to demonstrate consistency in annotation scheme, annotation and corpora. To ease the coreference annotation process, we built a semi-automatic Coreference Annotation Tool (CAT). We also provide statistics of coreference annotation on Hindi Dependency Treebank (HDTB).
We present coreference annotation on parallel Czech-English texts of the Prague Czech-English Dependency Treebank (PCEDT). The paper describes innovations made to PCEDT 2.0 concerning coreference, as well as coreference information already present there. We characterize the coreference annotation scheme, give the statistics and compare our annotation with the coreference annotation in Ontonotes and Prague Dependency Treebank for Czech. We also present the experiments made using this corpus to improve the alignment of coreferential expressions, which helps us to collect better statistics of correspondences between types of coreferential relations in Czech and English. The corpus released as PCEDT 2.0 Coref is publicly available.
We describe challenges and advantages unique to coreference resolution in the biomedical domain, and a sieve-based architecture that leverages domain knowledge for both entity and event coreference resolution. Domain-general coreference resolution algorithms perform poorly on biomedical documents, because the cues they rely on such as gender are largely absent in this domain, and because they do not encode domain-specific knowledge such as the number and type of participants required in chemical reactions. Moreover, it is difficult to directly encode this knowledge into most coreference resolution algorithms because they are not rule-based. Our rule-based architecture uses sequentially applied hand-designed {``}sieves{''}, with the output of each sieve informing and constraining subsequent sieves. This architecture provides a 3.2{\%} increase in throughput to our Reach event extraction system with precision parallel to that of the stricter system that relies solely on syntactic patterns for extraction.
Characters form the focus of various studies of literary works, including social network analysis, archetype induction, and plot comparison. The recent rise in the computational modelling of literary works has produced a proportional rise in the demand for character-annotated literary corpora. However, automatically identifying characters is an open problem and there is low availability of literary texts with manually labelled characters. To address the latter problem, this work presents three contributions: (1) a comprehensive scheme for manually resolving mentions to characters in texts. (2) A novel collaborative annotation tool, CHARLES (CHAracter Resolution Label-Entry System) for character annotation and similiar cross-document tagging tasks. (3) The character annotations resulting from a pilot study on the novel Pride and Prejudice, demonstrating the scheme and tool facilitate the efficient production of high-quality annotations. We expect this work to motivate the further production of annotated literary corpora to help meet the demand of the community.
In most international industries, English is the main language of communication for technical documents. These documents are designed to be as unambiguous as possible for their users. For international industries based in non-English speaking countries, the professionals in charge of writing requirements are often non-native speakers of English, who rarely receive adequate training in the use of English for this task. As a result, requirements can contain a relatively large diversity of lexical and grammatical errors, which are not eliminated by the use of guidelines from controlled languages. This article investigates the distribution of errors in a corpus of requirements written in English by native speakers of French. Errors are defined on the basis of grammaticality and acceptability principles, and classified using comparable categories. Results show a high proportion of errors in the Noun Phrase, notably through modifier stacking, and errors consistent with simplification strategies. Comparisons with similar corpora in other genres reveal the specificity of the distribution of errors in requirements. This research also introduces possible applied uses, in the form of strategies for the automatic detection of errors, and in-person training provided by certification boards in requirements authoring.
We present a novel method to automatically improve the accurracy of part-of-speech taggers on learner language. The key idea underlying our approach is to exploit the structure of a typical language learner task and automatically induce POS information for out-of-vocabulary (OOV) words. To evaluate the effectiveness of our approach, we add manual POS and normalization information to an existing language learner corpus. Our evaluation shows an increase in accurracy from 72.4{\%} to 81.5{\%} on OOV words.
We present a new resource for Swedish, SweLL, a corpus of Swedish Learner essays linked to learners{'} performance according to the Common European Framework of Reference (CEFR). SweLL consists of three subcorpora ‚Äï SpIn, SW1203 and Tisus, collected from three different educational establishments. The common metadata for all subcorpora includes age, gender, native languages, time of residence in Sweden, type of written task. Depending on the subcorpus, learner texts may contain additional information, such as text genres, topics, grades. Five of the six CEFR levels are represented in the corpus: A1, A2, B1, B2 and C1 comprising in total 339 essays. C2 level is not included since courses at C2 level are not offered. The work flow consists of collection of essays and permits, essay digitization and registration, meta-data annotation, automatic linguistic annotation. Inter-rater agreement is presented on the basis of SW1203 subcorpus. The work on SweLL is still ongoing with more that 100 essays waiting in the pipeline. This article both describes the resource and the {``}how-to{''} behind the compilation of SweLL.
The paper introduces SVALex, a lexical resource primarily aimed at learners and teachers of Swedish as a foreign and second language that describes the distribution of 15,681 words and expressions across the Common European Framework of Reference (CEFR). The resource is based on a corpus of coursebook texts, and thus describes receptive vocabulary learners are exposed to during reading activities, as opposed to productive vocabulary they use when speaking or writing. The paper describes the methodology applied to create the list and to estimate the frequency distribution. It also discusses some characteristics of the resulting resource and compares it to other lexical resources for Swedish. An interesting feature of this resource is the possibility to separate the wheat from the chaff, identifying the core vocabulary at each level, i.e. vocabulary shared by several coursebook writers at each level, from peripheral vocabulary which is used by the minority of the coursebook writers.
Automated grammatical error detection, which helps users improve their writing, is an important application in NLP. Recently more and more people are learning Chinese, and an automated error detection system can be helpful for the learners. This paper proposes n-gram features, dependency count features, dependency bigram features, and single-character features to determine if a Chinese sentence contains word usage errors, in which a word is written as a wrong form or the word selection is inappropriate. With marking potential errors on the level of sentence segments, typically delimited by punctuation marks, the learner can try to correct the problems without the assistant of a language teacher. Experiments on the HSK corpus show that the classifier combining all sets of features achieves an accuracy of 0.8423. By utilizing certain combination of the sets of features, we can construct a system that favors precision or recall. The best precision we achieve is 0.9536, indicating that our system is reliable and seldom produces misleading results.
We present a light-weight machine learning tool for NLP research. The package supports operations on both discrete and dense vectors, facilitating implementation of linear models as well as neural models. It provides several basic layers which mainly aims for single-layer linear and non-linear transformations. By using these layers, we can conveniently implement linear models and simple neural models. Besides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly.
This study examines two possibilities of using the FLELex graded lexicon for the automated assessment of text complexity in French as a foreign language learning. From the lexical frequency distributions described in FLELex, we derive a single level of difficulty for each word in a parallel corpus of original and simplified texts. We then use this data to automatically address the lexical complexity of texts in two ways. On the one hand, we evaluate the degree of lexical simplification in manually simplified texts with respect to their original version. Our results show a significant simplification effect, both in the case of French narratives simplified for non-native readers and in the case of simplified Wikipedia texts. On the other hand, we define a predictive model which identifies the number of words in a text that are expected to be known at a particular learning level. We assess the accuracy with which these predictions are able to capture actual word knowledge as reported by Dutch-speaking learners of French. Our study shows that although the predictions seem relatively accurate in general (87.4{\%} to 92.3{\%}), they do not yet seem to cover the learners{'} lack of knowledge very well.
Dialogue robots are attractive to people, and in language learning systems, they motivate learners and let them practice conversational skills in more realistic environment. However, automatic speech recognition (ASR) of the second language (L2) learners is still a challenge, because their speech contains not just pronouncing, lexical, grammatical errors, but is sometimes totally disordered. Hence, we propose a novel robot assisted language learning (RALL) system using two robots, one as a teacher and the other as an advanced learner. The system is designed to simulate multiparty conversation, expecting implicit learning and enhancement of predictability of learners{'} utterance through an alignment similar to {``}interactive alignment{''}, which is observed in human-human conversation. We collected a database with the prototypes, and measured how much the alignment phenomenon observed in the database with initial analysis.
We present OSMAN (Open Source Metric for Measuring Arabic Narratives) - a novel open source Arabic readability metric and tool. It allows researchers to calculate readability for Arabic text with and without diacritics. OSMAN is a modified version of the conventional readability formulas such as Flesch and Fog. In our work we introduce a novel approach towards counting short, long and stress syllables in Arabic which is essential for judging readability of Arabic narratives. We also introduce an additional factor called {``}Faseeh{''} which considers aspects of script usually dropped in informal Arabic writing. To evaluate our methods we used Spearman{'}s correlation metric to compare text readability for 73,000 parallel sentences from English and Arabic UN documents. The Arabic sentences were written with the absence of diacritics and in order to count the number of syllables we added the diacritics in using an open source tool called Mishkal. The results show that OSMAN readability formula correlates well with the English ones making it a useful tool for researchers and educators working with Arabic text.
Enabling users of intelligent systems to enhance the system performance by providing feedback on their errors is an important need. However, the ability of systems to learn from user feedback is difficult to evaluate in an objective and comparative way. Indeed, the involvement of real users in the adaptation process is an impediment to objective evaluation. This issue can be solved by using an oracle approach, where users are simulated by oracles having access to the reference test data. Another difficulty is to find a meaningful metric despite the fact that system improvements depend on the feedback provided and on the system itself. A solution is to measure the minimal amount of information needed to correct all system errors. It can be shown that for any well defined non interactive task, the interactively supervised version of the task can be evaluated by combining such an oracle-based approach and a minimum supervision rate metric. This new evaluation protocol for adaptive systems is not only expected to drive progress for such systems, but also to pave the way for a specialisation of actors along the value chain of their technological development.
This paper addresses the problem of quantifying the differences between entity extraction systems, where in general only a small proportion a document should be selected. Comparing overall accuracy is not very useful in these cases, as small differences in accuracy may correspond to huge differences in selections over the target minority class. Conventionally, one may use per-token complementarity to describe these differences, but it is not very useful when the set is heavily skewed. In such situations, which are common in information retrieval and entity recognition, metrics like precision and recall are typically used to describe performance. However, precision and recall fail to describe the differences between sets of objects selected by different decision strategies, instead just describing the proportional amount of correct and incorrect objects selected. This paper presents a method for measuring complementarity for precision, recall and F-score, quantifying the difference between entity extraction approaches.
Opinion Mining is a topic which attracted a lot of interest in the last years. By observing the literature, it is often hard to replicate system evaluation due to the unavailability of the data used for the evaluation or to the lack of details about the protocol used in the campaign. In this paper, we propose an evaluation protocol, called DRANZIERA, composed of a multi-domain dataset and guidelines allowing both to evaluate opinion mining systems in different contexts (Closed, Semi-Open, and Open) and to compare them to each other and to a number of baselines.
Web corpora are often constructed automatically, and their contents are therefore often not well understood. One technique for assessing the composition of such a web corpus is to empirically measure its similarity to a reference corpus whose composition is known. In this paper we evaluate a number of measures of corpus similarity, including a method based on topic modelling which has not been previously evaluated for this task. To evaluate these methods we use known-similarity corpora that have been previously used for this purpose, as well as a number of newly-constructed known-similarity corpora targeting differences in genre, topic, time, and region. Our findings indicate that, overall, the topic modelling approach did not improve on a chi-square method that had previously been found to work well for measuring corpus similarity.
This contribution presents the background, design and results of a study of users of three oral corpus platforms in Germany. Roughly 5.000 registered users of the Database for Spoken German (DGD), the GeWiss corpus and the corpora of the Hamburg Centre for Language Corpora (HZSK) were asked to participate in a user survey. This quantitative approach was complemented by qualitative interviews with selected users. We briefly introduce the corpus resources involved in the study in section 2. Section 3 describes the methods employed in the user studies. Section 4 summarizes results of the studies focusing on selected key topics. Section 5 attempts a generalization of these results to larger contexts.
In this paper we describe a corpus of automatic translations annotated with both error type and quality. The 300 sentences that we have selected were generated by Google Translate, Systran and two in-house Machine Translation systems that use Moses technology. The errors present on the translations were annotated with an error taxonomy that divides errors in five main linguistic categories (Orthography, Lexis, Grammar, Semantics and Discourse), reflecting the language level where the error is located. After the error annotation process, we accessed the translation quality of each sentence using a four point comprehension scale from 1 to 5. Both tasks of error and quality annotation were performed by two different annotators, achieving good levels of inter-annotator agreement. The creation of this corpus allowed us to use it as training data for a translation quality classifier. We concluded on error severity by observing the outputs of two machine learning classifiers: a decision tree and a regression model.
This paper presents an approach for automatic evaluation of the readability of text simplification output for readers with cognitive disabilities. First, we present our work towards the development of the EasyRead corpus, which contains easy-to-read documents created especially for people with cognitive disabilities. We then compare the EasyRead corpus to the simplified output contained in the LocalNews corpus (Feng, 2009), the accessibility of which has been evaluated through reading comprehension experiments including 20 adults with mild intellectual disability. This comparison is made on the basis of 13 disability-specific linguistic features. The comparison reveals that there are no major differences between the two corpora, which shows that the EasyRead corpus is to a similar reading level as the user-evaluated texts. We also discuss the role of Simple Wikipedia (Zhu et al., 2010) as a widely-used accessibility benchmark, in light of our finding that it is significantly more complex than both the EasyRead and the LocalNews corpora.
Word embeddings have been successfully used in several natural language processing tasks (NLP) and speech processing. Different approaches have been introduced to calculate word embeddings through neural networks. In the literature, many studies focused on word embedding evaluation, but for our knowledge, there are still some gaps. This paper presents a study focusing on a rigorous comparison of the performances of different kinds of word embeddings. These performances are evaluated on different NLP and linguistic tasks, while all the word embeddings are estimated on the same training data using the same vocabulary, the same number of dimensions, and other similar characteristics. The evaluation results reported in this paper match those in the literature, since they point out that the improvements achieved by a word embedding in one task are not consistently observed across all tasks. For that reason, this paper investigates and evaluates approaches to combine word embeddings in order to take advantage of their complementarity, and to look for the effective word embeddings that can achieve good performances on all tasks. As a conclusion, this paper provides new perceptions of intrinsic qualities of the famous word embedding families, which can be different from the ones provided by works previously published in the scientific literature.
In this paper, we claim that the CAMOMILE collaborative annotation platform (developed in the framework of the eponymous CHIST-ERA project) eases the organization of multimedia technology benchmarks, automating most of the campaign technical workflow and enabling collaborative (hence faster and cheaper) annotation of the evaluation data. This is demonstrated through the successful organization of a new multimedia task at MediaEval 2015, Multimodal Person Discovery in Broadcast TV.
This paper discusses a methodology to measure the usability of machine translated content by end users, comparing lightly post-edited content with raw output and with the usability of source language content. The content selected consists of Online Help articles from a software company for a spreadsheet application, translated from English into German. Three groups of five users each used either the source text - the English version (EN) -, the raw MT version (DE{\_}MT), or the light PE version (DE{\_}PE), and were asked to carry out six tasks. Usability was measured using an eye tracker and cognitive, temporal and pragmatic measures of usability. Satisfaction was measured via a post-task questionnaire presented after the participants had completed the tasks.
This project assesses the resources necessary to make oral history searchable by means of automatic speech recognition (ASR). There are many inherent challenges in applying ASR to conversational speech: smaller training set sizes and varying demographics, among others. We assess the impact of dataset size, word error rate and term-weighted value on human search capability through an information retrieval task on Mechanical Turk. We use English oral history data collected by StoryCorps, a national organization that provides all people with the opportunity to record, share and preserve their stories, and control for a variety of demographics including age, gender, birthplace, and dialect on four different training set sizes. We show comparable search performance using a standard speech recognition system as with hand-transcribed data, which is promising for increased accessibility of conversational speech and oral history archives.
Odin is an information extraction framework that applies cascades of finite state automata over both surface text and syntactic dependency graphs. Support for syntactic patterns allow us to concisely define relations that are otherwise difficult to express in languages such as Common Pattern Specification Language (CPSL), which are currently limited to shallow linguistic features. The interaction of lexical and syntactic automata provides robustness and flexibility when writing extraction rules. This paper describes Odin{'}s declarative language for writing these cascaded automata.
Breaking news on economic events such as stock splits or mergers and acquisitions has been shown to have a substantial impact on the financial markets. As it is important to be able to automatically identify events in news items accurately and in a timely manner, we present in this paper proof-of-concept experiments for a supervised machine learning approach to economic event detection in newswire text. For this purpose, we created a corpus of Dutch financial news articles in which 10 types of company-specific economic events were annotated. We trained classifiers using various lexical, syntactic and semantic features. We obtain good results based on a basic set of shallow features, thus showing that this method is a viable approach for economic event detection in news text.
Predictive modeling, often called {``}predictive analytics{''} in a commercial context, encompasses a variety of statistical techniques that analyze historical and present facts to make predictions about unknown events. Often the unknown events are in the future, but prediction can be applied to any type of unknown whether it be in the past or future. In our case, we present some experiments applying predictive modeling to the usage of technical terms within the NLP domain.
This paper presents the Gavagai Living Lexicon, which is an online distributional semantic model currently available in 20 different languages. We describe the underlying distributional semantic model, and how we have solved some of the challenges in applying such a model to large amounts of streaming data. We also describe the architecture of our implementation, and discuss how we deal with continuous quality assurance of the lexicon.
Social media outlets are providing new opportunities for harvesting valuable resources. We present a novel approach for mining data from Twitter for the purpose of building transliteration resources and systems. Such resources are crucial in translation and retrieval tasks. We demonstrate the benefits of the approach on Arabic to English transliteration. The contribution of this approach includes the size of data that can be collected and exploited within the span of a limited time; the approach is very generic and can be adopted to other languages and the ability of the approach to cope with new transliteration phenomena and trends. A statistical transliteration system built using this data improved a comparable system built from Wikipedia wikilinks data.
Many emerging documents usually contain temporal information. Because the temporal information is useful for various applications, it became important to develop a system of extracting the temporal information from the documents. Before developing the system, it first necessary to define or design the structure of temporal information. In other words, it is necessary to design a language which defines how to annotate the temporal information. There have been some studies about the annotation languages, but most of them was applicable to only a specific target language (e.g., English). Thus, it is necessary to design an individual annotation language for each language. In this paper, we propose a revised version of Koreain Time Mark-up Language (K-TimeML), and also introduce a dataset, named Korean TimeBank, that is constructed basd on the K-TimeML. We believe that the new K-TimeML and Korean TimeBank will be used in many further researches about extraction of temporal information.
Hypernymy relations (those where an hyponym term shares a {``}isa{''} relationship with his hypernym) play a key role for many Natural Language Processing (NLP) tasks, e.g. ontology learning, automatically building or extending knowledge bases, or word sense disambiguation and induction. In fact, such relations may provide the basis for the construction of more complex structures such as taxonomies, or be used as effective background knowledge for many word understanding applications. We present a publicly available database containing more than 400 million hypernymy relations we extracted from the CommonCrawl web corpus. We describe the infrastructure we developed to iterate over the web corpus for extracting the hypernymy relations and store them effectively into a large database. This collection of relations represents a rich source of knowledge and may be useful for many researchers. We offer the tuple dataset for public download and an Application Programming Interface (API) to help other researchers programmatically query the database.
The objective of this paper was to evaluate the performance of two statistical machine translation (SMT) systems within a cross-language information retrieval (CLIR) architecture and examine if there is a correlation between translation quality and CLIR performance. The SMT systems were KantanMT, a cloud-based machine translation (MT) platform, and Moses, an open-source MT application. First we trained both systems using the same language resources: the EMEA corpus for the translation model and language model and the QTLP corpus for tuning. Then we translated the 63 queries of the OHSUMED test collection from Greek into English using both MT systems. Next, we ran the queries on the document collection using Apache Solr to get a list of the top ten matches. The results were compared to the OHSUMED gold standard. KantanMT achieved higher average precision and F-measure than Moses, while both systems produced the same recall score. We also calculated the BLEU score for each system using the ECDC corpus. Moses achieved a higher BLEU score than KantanMT. Finally, we also tested the IR performance of the original English queries. This work overall showed that CLIR performance can be better even when BLEU score is worse.
This work analyses a corpus made of the titles of research projects belonging to the last four European Commission Framework Programmes (FP4, FP5, FP6, FP7) during a time span of nearly two decades (1994-2012). The starting point is the idea of creating a corpus of titles which would constitute a terminological niche, a sort of {``}cluster map{''} offering an overall vision on the terms used and the links between them. Moreover, by performing a terminological comparison over a period of time it is possible to trace the presence of obsolete words in outdated research areas as well as of neologisms in the most recent fields. Within this scenario, the minimal purpose is to build a corpus of titles of European projects belonging to the several Framework Programmes in order to obtain a terminological mapping of relevant words in the various research areas: particularly significant would be those terms spread across different domains or those extremely tied to a specific domain. A term could actually be found in many fields and being able to acknowledge and retrieve this cross-presence means being able to linking those different domains by means of a process of terminological mapping.
The paper investigates the extent of the support semi-automatic analysis can provide for the specific task of assigning Hohfeldian relations of Duty, using the General Architecture for Text Engineering tool for the automated extraction of Duty instances and the bearers of associated roles. The outcome of the analysis supports scholars in identifying Hohfeldian structures in legal text when performing close reading of the texts. A cyclic workflow involving automated annotation and expert feedback will incrementally increase the quality and coverage of the automatic extraction process, and increasingly reduce the amount of manual work required of the scholar.
The emergence of the web has necessitated the need to detect and correct noisy consumer-generated texts. Most of the previous studies on English spelling-error extraction collected English spelling errors from web services such as Twitter by using the edit distance or from input logs utilizing crowdsourcing. However, in the former approach, it is not clear which word corresponds to the spelling error, and the latter approach requires an annotation cost for the crowdsourcing. One notable exception is Rodrigues and Rytting (2012), who proposed to extract English spelling errors by using a word-typing game. Their approach saves the cost of crowdsourcing, and guarantees an exact alignment between the word and the spelling error. However, they did not assert whether the extracted spelling error corpora reflect the usual writing process such as writing a document. Therefore, we propose a new correctable word-typing game that is more similar to the actual writing process. Experimental results showed that we can regard typing-game logs as a source of spelling errors.
The paper describes automatic definition finding implemented within the leading corpus query and management tool, Sketch Engine. The implementation exploits complex pattern-matching queries in the corpus query language (CQL) and the indexing mechanism of word sketches for finding and storing definition candidates throughout the corpus. The approach is evaluated for Czech and English corpora, showing that the results are usable in practice: precision of the tool ranges between 30 and 75 percent (depending on the major corpus text types) and we were able to extract nearly 2 million definition candidates from an English corpus with 1.4 billion words. The feature is embedded into the interface as a concordance filter, so that users can search for definitions of any query to the corpus, including very specific multi-word queries. The results also indicate that ordinary texts (unlike explanatory texts) contain rather low number of definitions, which is perhaps the most important problem with automatic definition finding in general.
The aim of this paper is to study the effect that the use of Basic English versus common English has on information extraction from online resources. The amount of online information available to the public grows exponentially, and is potentially an excellent resource for information extraction. The problem is that this information often comes in an unstructured format, such as plain text. In order to retrieve knowledge from this type of text, it must first be analysed to find the relevant details, and the nature of the language used can greatly impact the quality of the extracted information. In this paper, we compare triplets that represent definitions or properties of concepts obtained from three online collaborative resources (English Wikipedia, Simple English Wikipedia and Simple English Wiktionary) and study the differences in the results when Basic English is used instead of common English. The results show that resources written in Basic English produce less quantity of triplets, but with higher quality.
In this paper we present PIERINO (PIattaforma per l{'}Estrazione e il Recupero di INformazione Online), a system that was implemented in collaboration with the Italian Ministry of Education, University and Research to analyse the citizens{'} comments given in {\#}labuonascuola survey. The platform includes various levels of automatic analysis such as key-concept extraction and word co-occurrences. Each analysis is displayed through an intuitive view using different types of visualizations, for example radar charts and sunburst. PIERINO was effectively used to support shaping the last Italian school reform, proving the potential of NLP in the context of policy making.
This paper presents the evaluation of the translation quality and Cross-Lingual Information Retrieval (CLIR) performance when using session information as the context of queries. The hypothesis is that previous queries provide context that helps to solve ambiguous translations in the current query. We tested several strategies on the TREC 2010 Session track dataset, which includes query reformulations grouped by generalization, specification, and drifting types. We study the Basque to English direction, evaluating both the translation quality and CLIR performance, with positive results in both cases. The results show that the quality of translation improved, reducing error rate by 12{\%} (HTER) when using session information, which improved CLIR results 5{\%} (nDCG). We also provide an analysis of the improvements across the three kinds of sessions: generalization, specification, and drifting. Translation quality improved in all three types (generalization, specification, and drifting), and CLIR improved for generalization and specification sessions, preserving the performance in drifting sessions.
This paper presents a new Vietnamese text corpus which contains around 4.05 billion words. It is a collection of Wikipedia texts, newspaper articles and random web texts. The paper describes the process of collecting, cleaning and creating the corpus. Processing Vietnamese texts faced several challenges, for example, different from many Latin languages, Vietnamese language does not use blanks for separating words, hence using common tokenizers such as replacing blanks with word boundary does not work. A short review about different approaches of Vietnamese tokenization is presented together with how the corpus has been processed and created. After that, some statistical analysis on this data is reported including the number of syllable, average word length, sentence length and topic analysis. The corpus is integrated into a framework which allows searching and browsing. Using this web interface, users can find out how many times a particular word appears in the corpus, sample sentences where this word occurs, its left and right neighbors.
Text analysis methods for the automatic identification of emerging technologies by analyzing the scientific publications, are gaining attention because of their socio-economic impact. The approaches so far have been mainly focused on retrospective analysis by mapping scientific topic evolution over time. We propose regression based approaches to predict future keyword distribution. The prediction is based on historical data of the keywords, which in our case, are LREC conference proceedings. Considering the insufficient number of data points available from LREC proceedings, we do not employ standard time series forecasting methods. We form a dataset by extracting the keywords from previous year proceedings and quantify their yearly relevance using tf-idf scores. This dataset additionally contains ranked lists of related keywords and experts for each keyword.
Understanding the experimental results of a scientific paper is crucial to understanding its contribution and to comparing it with related work. We introduce a structured, queryable representation for experimental results and a baseline system that automatically populates this representation. The representation can answer compositional questions such as: {``}Which are the best published results reported on the NIST 09 Chinese to English dataset?{''} and {``}What are the most important methods for speeding up phrase-based decoding?{''} Answering such questions usually involves lengthy literature surveys. Current machine reading for academic papers does not usually consider the actual experiments, but mostly focuses on understanding abstracts. We describe annotation work to create an initial hscientific paper; experimental results representationi corpus. The corpus is composed of 67 papers which were manually annotated with a structured representation of experimental results by domain experts. Additionally, we present a baseline algorithm that characterizes the difficulty of the inference task.
Domain-specific annotations for NLP are often centered on real-world applications of text, and incorrect annotations may be particularly unacceptable. In medical text, the process of manual chart review (of a patient{'}s medical record) is error-prone due to its complexity. We propose a staggered NLP-assisted approach to the refinement of clinical annotations, an interactive process that allows initial human judgments to be verified or falsified by means of comparison with an improving NLP system. We show on our internal Asthma Timelines dataset that this approach improves the quality of the human-produced clinical annotations.
This paper presents QUANDHO (QUestion ANswering Data for italian HistOry), an Italian question answering dataset created to cover a specific domain, i.e. the history of Italy in the first half of the XX century. The dataset includes questions manually classified and annotated with Lexical Answer Types, and a set of question-answer pairs. This resource, freely available for research purposes, has been used to retrain a domain independent question answering system so to improve its performances in the domain of interest. Ongoing experiments on the development of a question classifier and an automatic tagger of Lexical Answer Types are also presented.
We present a successfully implemented document repository REST service for flexible SCRUD (search, crate, read, update, delete) storage of social media conversations, using a GATE/TIPSTER-like document object model and providing a query language for document features. This software is currently being used in the SENSEI research project and will be published as open-source software before the project ends. It is, to the best of our knowledge, the first freely available, general purpose data repository to support large-scale multimodal (i.e., speech or text) conversation analytics.
The Dictionaries division at Oxford University Press (OUP) is aiming to model, integrate, and publish lexical content for 100 languages focussing on digitally under-represented languages. While there are multiple ontologies designed for linguistic resources, none had adequate features for meeting our requirements, chief of which was the capability to losslessly capture diverse features of many different languages in a dictionary format, while supplying a framework for inferring relations like translation, derivation, etc., between the data. Building on valuable features of existing models, and working with OUP monolingual and bilingual dictionary datasets, we have designed and implemented a new linguistic ontology. The ontology has been reviewed by a number of computational linguists, and we are working to move more dictionary data into it. We have also developed APIs to surface the linked data to dictionary websites.
Language resources (LR) are indispensable for the development of tools for machine translation (MT) or various kinds of computer-assisted translation (CAT). In particular language corpora, both parallel and monolingual are considered most important for instance for MT, not only SMT but also hybrid MT. The Language Technology Observatory will provide easy access to information about LRs deemed to be useful for MT and other translation tools through its LR Catalogue. In order to determine what aspects of an LR are useful for MT practitioners, a user study was made, providing a guide to the most relevant metadata and the most relevant quality criteria. We have seen that many resources exist which are useful for MT and similar work, but the majority are for (academic) research or educational use only, and as such not available for commercial use. Our work has revealed a list of gaps: coverage gap, awareness gap, quality gap, quantity gap. The paper ends with recommendations for a forward-looking strategy.
The NSF-SI2-funded LAPPS Grid project is a collaborative effort among Brandeis University, Vassar College, Carnegie-Mellon University (CMU), and the Linguistic Data Consortium (LDC), which has developed an open, web-based infrastructure through which resources can be easily accessed and within which tailored language services can be efficiently composed, evaluated, disseminated and consumed by researchers, developers, and students across a wide variety of disciplines. The LAPPS Grid project recently adopted Galaxy (Giardine et al., 2005), a robust, well-developed, and well-supported front end for workflow configuration, management, and persistence. Galaxy allows data inputs and processing steps to be selected from graphical menus, and results are displayed in intuitive plots and summaries that encourage interactive workflows and the exploration of hypotheses. The Galaxy workflow engine provides significant advantages for deploying pipelines of LAPPS Grid web services, including not only means to create and deploy locally-run and even customized versions of the LAPPS Grid as well as running the LAPPS Grid in the cloud, but also access to a huge array of statistical and visualization tools that have been developed for use in genomics research.
After celebrating its 20th anniversary in 2015, ELRA is carrying on its strong involvement in the HLT field. To share ELRA{'}s expertise of those 21 past years, this article begins with a presentation of ELRA{'}s strategic Data and LR Management Plan for a wide use by the language communities. Then, we further report on ELRA{'}s activities and services provided since LREC 2014. When looking at the cataloguing and licensing activities, we can see that ELRA has been active at making the Meta-Share repository move toward new developments steps, supporting Europe to obtain accurate LRs within the Connecting Europe Facility programme, promoting the use of LR citation, creating the ELRA License Wizard web portal.The article further elaborates on the recent LR production activities of various written, speech and video resources, commissioned by public and private customers. In parallel, ELDA has also worked on several EU-funded projects centred on strategic issues related to the European Digital Single Market. The last part gives an overview of the latest dissemination activities, with a special focus on the celebration of its 20th anniversary organised in Dubrovnik (Croatia) and the following up of LREC, as well as the launching of the new ELRA portal.
This paper presents an investigation of mirroring facial expressions and the emotions which they convey in dyadic naturally occurring first encounters. Mirroring facial expressions are a common phenomenon in face-to-face interactions, and they are due to the mirror neuron system which has been found in both animals and humans. Researchers have proposed that the mirror neuron system is an important component behind many cognitive processes such as action learning and understanding the emotions of others. Preceding studies of the first encounters have shown that overlapping speech and overlapping facial expressions are very frequent. In this study, we want to determine whether the overlapping facial expressions are mirrored or are otherwise correlated in the encounters, and to what extent mirroring facial expressions convey the same emotion. The results of our study show that the majority of smiles and laughs, and one fifth of the occurrences of raised eyebrows are mirrored in the data. Moreover some facial traits in co-occurring expressions co-occur more often than it would be expected by chance. Finally, amusement, and to a lesser extent friendliness, are often emotions shared by both participants, while other emotions indicating individual affective states such as uncertainty and hesitancy are never showed by both participants, but co-occur with complementary emotions such as friendliness and support. Whether these tendencies are specific to this type of conversations or are more common should be investigated further.
The New Yorker publishes a weekly captionless cartoon. More than 5,000 readers submit captions for it. The editors select three of them and ask the readers to pick the funniest one. We describe an experiment that compares a dozen automatic methods for selecting the funniest caption. We show that negative sentiment, human-centeredness, and lexical centrality most strongly match the funniest captions, followed by positive sentiment. These results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal (vision+text) systems. As part of this work, a large set of cartoons and captions is being made available to the community.
The paper presents a corpus of text data and its corresponding gaze fixations obtained from autistic and non-autistic readers. The data was elicited through reading comprehension testing combined with eye-tracking recording. The corpus consists of 1034 content words tagged with their POS, syntactic role and three gaze-based measures corresponding to the autistic and control participants. The reading skills of the participants were measured through multiple-choice questions and, based on the answers given, they were divided into groups of skillful and less-skillful readers. This division of the groups informs researchers on whether particular fixations were elicited from skillful or less-skillful readers and allows a fair between-group comparison for two levels of reading ability. In addition to describing the process of data collection and corpus development, we present a study on the effect that word length has on reading in autism. The corpus is intended as a resource for investigating the particular linguistic constructions which pose reading difficulties for people with autism and hopefully, as a way to inform future text simplification research intended for this population.
The ability to efficiently speak in public is an essential asset for many professions and is used in everyday life. As such, tools enabling the improvement of public speaking performance and the assessment and mitigation of anxiety related to public speaking would be very useful. Multimodal interaction technologies, such as computer vision and embodied conversational agents, have recently been investigated for the training and assessment of interpersonal skills. Once central requirement for these technologies is multimodal corpora for training machine learning models. This paper addresses the need of these technologies by presenting and sharing a multimodal corpus of public speaking presentations. These presentations were collected in an experimental study investigating the potential of interactive virtual audiences for public speaking training. This corpus includes audio-visual data and automatically extracted features, measures of public speaking anxiety and personality, annotations of participants{'} behaviors and expert ratings of behavioral aspects and overall performance of the presenters. We hope this corpus will help other research teams in developing tools for supporting public speaking training.
We propose a comparison between various supervised machine learning methods to predict and detect humor in dialogues. We retrieve our humorous dialogues from a very popular TV sitcom: {``}The Big Bang Theory{''}. We build a corpus where punchlines are annotated using the canned laughter embedded in the audio track. Our comparative study involves a linear-chain Conditional Random Field over a Recurrent Neural Network and a Convolutional Neural Network. Using a combination of word-level and audio frame-level features, the CNN outperforms the other methods, obtaining the best F-score of 68.5{\%} over 66.5{\%} by CRF and 52.9{\%} by RNN. Our work is a starting point to developing more effective machine learning and neural network models on the humor prediction task, as well as developing machines capable in understanding humor in general.
This paper aims to implement what is referred to as the collocation of the Arabic keywords approach for extracting formulaic sequences (FSs) in the form of high frequency but semantically regular formulas that are not restricted to any syntactic construction or semantic domain. The study applies several distributional semantic models in order to automatically extract relevant FSs related to Arabic keywords. The data sets used in this experiment are rendered from a new developed corpus-based Arabic wordlist consisting of 5,189 lexical items which represent a variety of modern standard Arabic (MSA) genres and regions, the new wordlist being based on an overlapping frequency based on a comprehensive comparison of four large Arabic corpora with a total size of over 8 billion running words. Empirical n-best precision evaluation methods are used to determine the best association measures (AMs) for extracting high frequency and meaningful FSs. The gold standard reference FSs list was developed in previous studies and manually evaluated against well-established quantitative and qualitative criteria. The results demonstrate that the MI.log{\_}f AM achieved the highest results in extracting significant FSs from the large MSA corpus, while the T-score association measure achieved the worst results.
In this paper we present a rule-based method for multi-word term extraction that relies on extensive lexical resources in the form of electronic dictionaries and finite-state transducers for modelling various syntactic structures of multi-word terms. The same technology is used for lemmatization of extracted multi-word terms, which is unavoidable for highly inflected languages in order to pass extracted data to evaluators and subsequently to terminological e-dictionaries and databases. The approach is illustrated on a corpus of Serbian texts from the mining domain containing more than 600,000 simple word forms. Extracted and lemmatized multi-word terms are filtered in order to reject falsely offered lemmas and then ranked by introducing measures that combine linguistic and statistical information (C-Value, T-Score, LLR, and Keyness). Mean average precision for retrieval of MWU forms ranges from 0.789 to 0.804, while mean average precision of lemma production ranges from 0.956 to 0.960. The evaluation showed that 94{\%} of distinct multi-word forms were evaluated as proper multi-word units, and among them 97{\%} were associated with correct lemmas.
In this paper, we focus on Czech complex predicates formed by a light verb and a predicative noun expressed as the direct object. Although Czech ‚Äï as an inflectional language encoding syntactic relations via morphological cases ‚Äï provides an excellent opportunity to study the distribution of valency complements in the syntactic structure with complex predicates, this distribution has not been described so far. On the basis of a manual analysis of the richly annotated data from the Prague Dependency Treebank, we thus formulate principles governing this distribution. In an automatic experiment, we verify these principles on well-formed syntactic structures from the Prague Dependency Treebank and the Prague Czech-English Dependency Treebank with very satisfactory results: the distribution of 97{\%} of valency complements in the surface structure is governed by the proposed principles. These results corroborate that the surface structure formation of complex predicates is a regular process.
A verb-noun Multi-Word Expression (MWE) is a combination of a verb and a noun with or without other words, in which the combination has a meaning different from the meaning of the words considered separately. In this paper, we present a new lexical resource of Hebrew Verb-Noun MWEs (VN-MWEs). The VN-MWEs of this resource were manually collected and annotated from five different web resources. In addition, we analyze the lexical properties of Hebrew VN-MWEs by classifying them to three types: morphological, syntactic, and semantic. These two contributions are essential for designing algorithms for automatic VN-MWEs extraction. The analysis suggests some interesting features of VN-MWEs for exploration. The lexical resource enables to sample a set of positive examples for Hebrew VN-MWEs. This set of examples can either be used for training supervised algorithms or as seeds in unsupervised bootstrapping algorithms. Thus, this resource is a first step towards automatic identification of Hebrew VN-MWEs, which is important for natural language understanding, generation and translation systems.
This paper reports on an approach and experiments to automatically build a cross-lingual multi-word entity resource. Starting from a collection of millions of acronym/expansion pairs for 22 languages where expansion variants were grouped into monolingual clusters, we experiment with several aggregation strategies to link these clusters across languages. Aggregation strategies make use of string similarity distances and translation probabilities and they are based on vector space and graph representations. The accuracy of the approach is evaluated against Wikipedia{'}s redirection and cross-lingual linking tables. The resulting multi-word entity resource contains 64,000 multi-word entities with unique identifiers and their 600,000 multilingual lexical variants. We intend to make this new resource publicly available.
This paper presents SemLinker, an open source system that discovers named entities, connects them to a reference knowledge base, and clusters them semantically. SemLinker relies on several modules that perform surface form generation, mutual disambiguation, entity clustering, and make use of two annotation engines. SemLinker was evaluated in the English Entity Discovery and Linking track of the Text Analysis Conference on Knowledge Base Population, organized by the US National Institute of Standards and Technology. Along with the SemLinker source code, we release our annotation files containing the discovered named entities, their types, and position across processed documents.
More and more knowledge bases are publicly available as linked data. Since these knowledge bases contain structured descriptions of real-world entities, they can be exploited by entity linking systems that anchor entity mentions from text to the most relevant resources describing those entities. In this paper, we investigate adaptation of the entity linking task using contextual knowledge. The key intuition is that entity linking can be customized depending on the textual content, as well as on the application that would make use of the extracted information. We present an adaptive approach that relies on contextual knowledge from text to enhance the performance of ADEL, a hybrid linguistic and graph-based entity linking system. We evaluate our approach on a domain-specific corpus consisting of annotated WikiNews articles.
Recently, due to the increasing popularity of social media, the necessity for extracting information from informal text types, such as microblog texts, has gained significant attention. In this study, we focused on the Named Entity Recognition (NER) problem on informal text types for Turkish. We utilized a semi-supervised learning approach based on neural networks. We applied a fast unsupervised method for learning continuous representations of words in vector space. We made use of these obtained word embeddings, together with language independent features that are engineered to work better on informal text types, for generating a Turkish NER system on microblog texts. We evaluated our Turkish NER system on Twitter messages and achieved better F-score performances than the published results of previously proposed NER systems on Turkish tweets. Since we did not employ any language dependent features, we believe that our method can be easily adapted to microblog texts in other morphologically rich languages.
The task of Named Entity Linking is to link entity mentions in the document to their correct entries in a knowledge base and to cluster NIL mentions. Ambiguous, misspelled, and incomplete entity mention names are the main challenges in the linking process. We propose a novel approach that combines two state-of-the-art models ‚Äï for entity disambiguation and for paraphrase detection ‚Äï to overcome these challenges. We consider name variations as paraphrases of the same entity mention and adopt a paraphrase model for this task. Our approach utilizes a graph-based disambiguation model based on Personalized Page Rank, and then refines and clusters its output using the paraphrase similarity between entity mention strings. It achieves a competitive performance of 80.5{\%} in B3+F clustering score on diagnostic TAC EDL 2014 data.
In this paper we explain how we created a labelled corpus in English for a Named Entity Recognition (NER) task from multi-source and multi-domain data, for an industrial partner. We explain the specificities of this corpus with examples and describe some baseline experiments. We present some results of domain adaptation on this corpus using a labelled Twitter corpus (Ritter et al., 2011). We tested a semi-supervised method from (Garcia-Fernandez et al., 2014) combined with a supervised domain adaptation approach proposed in (Raymond and Fayolle, 2010) for machine learning experiments with CRFs (Conditional Random Fields). We use the same technique to improve the NER results on the Twitter corpus (Ritter et al., 2011). Our contributions thus consist in an industrial corpus creation and NER performance improvements.
We describe IRIS, a statistical machine translation (SMT) system for translating from English into Irish and vice versa. Since Irish is considered an under-resourced language with a limited amount of machine-readable text, building a machine translation system that produces reasonable translations is rather challenging. As translation is a difficult task, current research in SMT focuses on obtaining statistics either from a large amount of parallel, monolingual or other multilingual resources. Nevertheless, we collected available English-Irish data and developed an SMT system aimed at supporting human translators and enabling cross-lingual language technology tasks.
The present article reports on efforts to improve the translation accuracy of a corpus‚Äïbased Machine Translation (MT) system. In order to achieve that, an error analysis performed on past translation outputs has indicated the likelihood of improving the translation accuracy by augmenting the coverage of the Target-Language (TL) side language model. The method adopted for improving the language model is initially presented, based on the concatenation of consecutive phrases. The algorithmic steps are then described that form the process for augmenting the language model. The key idea is to only augment the language model to cover the most frequent cases of phrase sequences, as counted over a TL-side corpus, in order to maximize the cases covered by the new language model entries. Experiments presented in the article show that substantial improvements in translation accuracy are achieved via the proposed method, when integrating the grown language model to the corpus-based MT system.
An open-source rule-based machine translation system is developed for Scots, a low-resourced minor language closely related to English and spoken in Scotland and Ireland. By concentrating on translation for assimilation (gist comprehension) from Scots to English, it is proposed that the development of dictionaries designed to be used with in the Apertium platform will be sufficient to produce translations that improve non-Scots speakers understanding of the language. Mono- and bilingual Scots dictionaries are constructed using lexical items gathered from a variety of resources across several domains. Although the primary goal of this project is translation for gisting, the system is evaluated for both assimilation and dissemination (publication-ready translations). A variety of evaluation methods are used, including a cloze test undertaken by human volunteers. While evaluation results are comparable to, and in some cases superior to, those of other language pairs within the Apertium platform, room for improvement is identified in several areas of the system.
This paper describes a hybrid machine translation system that explores a parser to acquire syntactic chunks of a source sentence, translates the chunks with multiple online machine translation (MT) system application program interfaces (APIs) and creates output by combining translated chunks to obtain the best possible translation. The selection of the best translation hypothesis is performed by calculating the perplexity for each translated chunk. The goal of this approach is to enhance the baseline multi-system hybrid translation (MHyT) system that uses only a language model to select best translation from translations obtained with different APIs and to improve overall English ‚Äï Latvian machine translation quality over each of the individual MT APIs. The presented syntax-based multi-system translation (SyMHyT) system demonstrates an improvement in terms of BLEU and NIST scores compared to the baseline system. Improvements reach from 1.74 up to 2.54 BLEU points.
In this paper, we address the problem of Machine Translation (MT) for a specialised domain in a language pair for which only a very small domain-specific parallel corpus is available. We conduct a series of experiments using a purely phrase-based SMT (PBSMT) system and a hybrid MT system (TectoMT), testing three different strategies to overcome the problem of the small amount of in-domain training data. Our results show that adding a small size in-domain bilingual terminology to the small in-domain training corpus leads to the best improvements of a hybrid MT system, while the PBSMT system achieves the best results by adding a combination of in-domain bilingual terminology and a larger out-of-domain corpus. We focus on qualitative human evaluation of the output of two best systems (one for each approach) and perform a systematic in-depth error analysis which revealed advantages of the hybrid MT system over the pure PBSMT system for this specific task.
This paper presents CATaLog online, a new web-based MT and TM post-editing tool. CATaLog online is a freeware software that can be used through a web browser and it requires only a simple registration. The tool features a number of editing and log functions similar to the desktop version of CATaLog enhanced with several new features that we describe in detail in this paper. CATaLog online is designed to allow users to post-edit both translation memory segments as well as machine translation output. The tool provides a complete set of log information currently not available in most commercial CAT tools. Log information can be used both for project management purposes as well as for the study of the translation process and translator{'}s productivity.
This paper presents the multimodal Interlingual Map Task Corpus (ILMT-s2s corpus) collected at Trinity College Dublin, and discuss some of the issues related to the collection and analysis of the data. The corpus design is inspired by the HCRC Map Task Corpus which was initially designed to support the investigation of linguistic phenomena, and has been the focus of a variety of studies of communicative behaviour. The simplicity of the task, and the complexity of phenomena it can elicit, make the map task an ideal object of study. Although there are studies that used replications of the map task to investigate communication in computer mediated tasks, this ILMT-s2s corpus is, to the best of our knowledge, the first investigation of communicative behaviour in the presence of three additional {``}filters{''}: Automatic Speech Recognition (ASR), Machine Translation (MT) and Text To Speech (TTS) synthesis, where the instruction giver and the instruction follower speak different languages. This paper details the data collection setup and completed annotation of the ILMT-s2s corpus, and outlines preliminary results obtained from the data.
We propose named entity abstraction methods with fine-grained named entity labels for improving statistical machine translation (SMT). The methods are based on a bilingual named entity recognizer that uses a monolingual named entity recognizer with transliteration. Through experiments, we demonstrate that incorporating fine-grained named entities into statistical machine translation improves the accuracy of SMT with more adequate granularity compared with the standard SMT, which is a non-named entity abstraction method.
In this paper we present our work on the usage of lexical resources for the Machine Translation English and Malayalam. We describe a comparative performance between different Statistical Machine Translation (SMT) systems on top of phrase based SMT system as baseline. We explore different ways of utilizing lexical resources to improve the quality of English Malayalam statistical machine translation. In order to enrich the training corpus we have augmented the lexical resources in two ways (a) additional vocabulary and (b) inflected verbal forms. Lexical resources include IndoWordnet semantic relation set, lexical words and verb phrases etc. We have described case studies, evaluations and have given detailed error analysis for both Malayalam to English and English to Malayalam machine translation systems. We observed significant improvement in evaluations of translation quality. Lexical resources do help uplift performance when parallel corpora are scanty.
Resources for evaluating sentence-level and word-level alignment algorithms are unsatisfactory. Regarding sentence alignments, the existing data is too scarce, especially when it comes to difficult bitexts, containing instances of non-literal translations. Regarding word-level alignments, most available hand-aligned data provide a complete annotation at the level of words that is difficult to exploit, for lack of a clear semantics for alignment links. In this study, we propose new methodologies for collecting human judgements on alignment links, which have been used to annotate 4 new data sets, at the sentence and at the word level. These will be released online, with the hope that they will prove useful to evaluate alignment software and quality estimation tools for automatic alignment. Keywords: Parallel corpora, Sentence Alignments, Word Alignments, Confidence Estimation
Out-of-vocabulary (OOV) word is a crucial problem in statistical machine translation (SMT) with low resources. OOV paraphrasing that augments the translation model for the OOV words by using the translation knowledge of their paraphrases has been proposed to address the OOV problem. In this paper, we propose using word embeddings and semantic lexicons for OOV paraphrasing. Experiments conducted on a low resource setting of the OLYMPICS task of IWSLT 2012 verify the effectiveness of our proposed method.
We introduce the Denoised Web Treebank: a treebank including a normalization layer and a corresponding evaluation metric for dependency parsing of noisy text, such as Tweets. This benchmark enables the evaluation of parser robustness as well as text normalization methods, including normalization as machine translation and unsupervised lexical normalization, directly on syntactic trees. Experiments show that text normalization together with a combination of domain-specific and generic part-of-speech taggers can lead to a significant improvement in parsing accuracy on this test set.
In this paper we propose an approach to predict punctuation marks for unsegmented speech transcript. The approach is purely lexical, with pre-trained Word Vectors as the only input. A training model of Deep Neural Network (DNN) or Convolutional Neural Network (CNN) is applied to classify whether a punctuation mark should be inserted after the third word of a 5-words sequence and which kind of punctuation mark the inserted one should be. TED talks within IWSLT dataset are used in both training and evaluation phases. The proposed approach shows its effectiveness by achieving better result than the state-of-the-art lexical solution which works with same type of data, especially when predicting puncuation position only.
Greedy transition-based parsers are appealing for their very fast speed, with reasonably high accuracies. In this paper, we build a fast shift-reduce neural constituent parser by using a neural network to make local decisions. One challenge to the parsing speed is the large hidden and output layer sizes caused by the number of constituent labels and branching options. We speed up the parser by using a hierarchical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score).
We focus on the improvement of accuracy of raw text parsing, from the viewpoint of language resource addition. In Japanese, the raw text parsing is divided into three steps: word segmentation, part-of-speech tagging, and dependency parsing. We investigate the contribution of language resource addition in each of three steps to the improvement in accuracy for two domain corpora. The experimental results show that this improvement depends on the target domain. For example, when we handle well-written texts of limited vocabulary, white paper, an effective language resource is a word-POS pair sequence corpus for the parsing accuracy. So we conclude that it is important to check out the characteristics of the target domain and to choose a suitable language resource addition strategy for the parsing accuracy improvement.
We present E-TIPSY, a search query corpus annotated with named Entities, Term Importance, POS tags, and SYntactic parses. This corpus contains crowdsourced (gold) annotations of the three most important terms in each query. In addition, it contains automatically produced annotations of named entities, part-of-speech tags, and syntactic parses for the same queries. This corpus comes in two formats: (1) Sober Subset: annotations that two or more crowd workers agreed upon, and (2) Full Glass: all annotations. We analyze the strikingly low correlation between term importance and syntactic headedness, which invites research into effective ways of combining these different signals. Our corpus can serve as a benchmark for term importance methods aimed at improving search engine quality and as an initial step toward developing a dataset of gold linguistic analysis of web search queries. In addition, it can be used as a basis for linguistic inquiries into the kind of expressions used in search.
Compared to well-resourced languages such as English and Dutch, natural language processing (NLP) tools for Afrikaans are still not abundant. In the context of the AfriBooms project, KU Leuven and the North-West University collaborated to develop a first, small treebank, a dependency parser, and an easy to use online linguistic search engine for Afrikaans for use by researchers and students in the humanities and social sciences. The search tool is based on a similar development for Dutch, i.e. GrETEL, a user-friendly search engine which allows users to query a treebank by means of a natural language example instead of a formal search instruction.
The Index Thomisticus Treebank is the largest available treebank for Latin; it contains Medieval Latin texts by Thomas Aquinas. After experimenting on its data with a number of dependency parsers based on different supervised machine learning techniques, we found that DeSR with a multilayer perceptron algorithm, a right-to-left transition, and a tailor-made feature model is the parser providing the highest accuracy rates. We improved the results further by using a technique that combines the output parses of DeSR with those provided by other parsers, outperforming the previous state of the art in parsing the Index Thomisticus Treebank. The key idea behind such improvement is to ensure a sufficient diversity and accuracy of the outputs to be combined; for this reason, we performed an in-depth evaluation of the results provided by the different parsers that we combined. Finally, we assessed that, although the general architecture of the parser is portable to Classical Latin, yet the model trained on Medieval Latin is inadequate for such purpose.
Phrase chunking remains an important natural language processing (NLP) technique for intermediate syntactic processing. This paper describes the development of protocols, annotated phrase chunking data sets and automatic phrase chunkers for ten South African languages. Various problems with adapting the existing annotation protocols of English are discussed as well as an overview of the annotated data sets. Based on the annotated sets, CRF-based phrase chunkers are created and tested with a combination of different features, including part of speech tags and character n-grams. The results of the phrase chunking evaluation show that disjunctively written languages can achieve notably better results for phrase chunking with a limited data set than conjunctive languages, but that the addition of character n-grams improve the results for conjunctive languages.
Continuous word representations appeared to be a useful feature in many natural language processing tasks. Using fixed-dimension pre-trained word embeddings allows avoiding sparse bag-of-words representation and to train models with fewer parameters. In this paper, we use fixed pre-trained word embeddings as additional features for a neural scoring function in the MST parser. With the multi-layer architecture of the scoring function we can avoid handcrafting feature conjunctions. The continuous word representations on the input also allow us to reduce the number of lexical features, make the parser more robust to out-of-vocabulary words, and reduce the total number of parameters of the model. Although its accuracy stays below the state of the art, the model size is substantially smaller than with the standard features set. Moreover, it performs well for languages where only a smaller treebank is available and the results promise to be useful in cross-lingual parsing.
We describe a method for analysing Constraint Grammars (CG) that can detect internal conflicts and redundancies in a given grammar, without the need for a corpus. The aim is for grammar writers to be able to automatically diagnose, and then manually improve their grammars. Our method works by translating the given grammar into logical constraints that are analysed by a SAT-solver. We have evaluated our analysis on a number of non-trivial grammars and found inconsistencies.
The treatment of medieval texts is a particular challenge for parsers. I compare how two dependency parsers, one graph-based, the other transition-based, perform on Old French, facing some typical problems of medieval texts: graphical variation, relatively free word order, and syntactic variation of several parameters over a diachronic period of about 300 years. Both parsers were trained and evaluated on the {``}Syntactic Reference Corpus of Medieval French{''} (SRCMF), a manually annotated dependency treebank. I discuss the relation between types of parsers and types of language, as well as the differences of the analyses from a linguistic point of view.
Parsing Web information, namely parsing content to find relevant documents on the basis of a user{'}s query, represents a crucial step to guarantee fast and accurate Information Retrieval (IR). Generally, an automated approach to such task is considered faster and cheaper than manual systems. Nevertheless, results do not seem have a high level of accuracy, indeed, as also Hjorland (2007) states, using stochastic algorithms entails: {\mbox{$\bullet$}} Low precision due to the indexing of common Atomic Linguistic Units (ALUs) or sentences. {\mbox{$\bullet$}} Low recall caused by the presence of synonyms. {\mbox{$\bullet$}} Generic results arising from the use of too broad or too narrow terms. Usually IR systems are based on invert text index, namely an index data structure storing a mapping from content to its locations in a database file, or in a document or a set of documents. In this paper we propose a system, by means of which we will develop a search engine able to process online documents, starting from a natural language query, and to return information to users. The proposed approach, based on the Lexicon-Grammar (LG) framework and its language formalization methodologies, aims at integrating a semantic annotation process for both query analysis and document retrieval.
The Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE) was created to facilitate the study of challenges posed by rapidly aging societies in developed countries such as Germany. ILSE contains over 8,000 hours of biographic interviews recorded from more than 1,000 participants over the course of 20 years. Investigations on various aspects of aging, such as cognitive decline, often rely on the analysis of linguistic features which can be derived from spoken content like these interviews. However, transcribing speech is a time and cost consuming manual process and so far only 380 hours of ILSE interviews have been transcribed. Thus, it is the aim of our work to establish technical systems to fully automatically transcribe the ILSE interview data. The joint occurrence of poor recording quality, long audio segments, erroneous transcriptions, varying speaking styles {\&} crosstalk, and emotional {\&} dialectal speech in these interviews presents challenges for automatic speech recognition (ASR). We describe our ongoing work towards the fully automatic transcription of all ILSE interviews and the steps we implemented in preparing the transcriptions to meet the interviews{'} challenges. Using a recursive long audio alignment procedure 96 hours of the transcribed data have been made accessible for ASR training.
A speech database has been collected for use to highlight the importance of {``}speaker factor{''} in forensic voice comparison. FABIOLE has been created during the FABIOLE project funded by the French Research Agency (ANR) from 2013 to 2016. This corpus consists in more than 3 thousands excerpts spoken by 130 French native male speakers. The speakers are divided into two categories: 30 target speakers who everyone has 100 excerpts and 100 {``}impostors{''} who everyone has only one excerpt. The data were collected from 10 different French radio and television shows where each utterance turns with a minimum duration of 30s and has a good speech quality. The data set is mainly used for investigating speaker factor in forensic voice comparison and interpreting some unsolved issue such as the relationship between speaker characteristics and system behavior. In this paper, we present FABIOLE database. Then, preliminary experiments are performed to evaluate the effect of the {``}speaker factor{''} and the show on a voice comparison system behavior.
Corpus design for speech synthesis is a well-researched topic in languages such as English compared to Modern Standard Arabic, and there is a tendency to focus on methods to automatically generate the orthographic transcript to be recorded (usually greedy methods). In this work, a study of Modern Standard Arabic (MSA) phonetics and phonology is conducted in order to create criteria for a greedy method to create a speech corpus transcript for recording. The size of the dataset is reduced a number of times using these optimisation methods with different parameters to yield a much smaller dataset with identical phonetic coverage than before the reduction, and this output transcript is chosen for recording. This is part of a larger work to create a completely annotated and segmented speech corpus for MSA.
With emerging conversational data, automated content analysis is needed for better data interpretation, so that it is accurately understood and can be effectively integrated and utilized in various applications. ICSI meeting corpus is a publicly released data set of multi-party meetings in an organization that has been released over a decade ago, and has been fostering meeting understanding research since then. The original data collection includes transcription of participant turns as well as meta-data annotations, such as disfluencies and dialog act tags. This paper presents an extended set of annotations for the ICSI meeting corpus with a goal of deeply understanding meeting conversations, where participant turns are annotated by actionable items that could be performed by an automated meeting assistant. In addition to the user utterances that contain an actionable item, annotations also include the arguments associated with the actionable item. The set of actionable items are determined by aligning human-human interactions to human-machine interactions, where a data annotation schema designed for a virtual personal assistant (human-machine genre) is adapted to the meetings domain (human-human genre). The data set is formed by annotating participants{'} utterances in meetings with potential intents/actions considering their contexts. The set of actions target what could be accomplished by an automated meeting assistant, such as taking a note of action items that a participant commits to, or finding emails or topic related documents that were mentioned during the meeting. A total of 10 defined intents/actions are considered as actionable items in meetings. Turns that include actionable intents were annotated for 22 public ICSI meetings, that include a total of 21K utterances, segmented by speaker turns. Participants{'} spoken turns, possible actions along with associated arguments and their vector representations as computed by convolutional deep structured semantic models are included in the data set for future research. We present a detailed statistical analysis of the data set and analyze the performance of applying convolutional deep structured semantic models for an actionable item detection task. The data is available at http://research.microsoft.com/ projects/meetingunderstanding/.
Current state-of-the-art speech synthesizers for domain-independent systems still struggle with the challenge of generating understandable and natural-sounding speech. This is mainly because the pronunciation of words of foreign origin, inflections and compound words often cannot be handled by rules. Furthermore there are too many of these for inclusion in exception dictionaries. We describe an approach to evaluating text-to-speech synthesizers with a subjective listening experiment. The focus is to differentiate between known problem classes for speech synthesizers. The target language is German but we believe that many of the described phenomena are not language specific. We distinguish the following problem categories: Normalization, Foreign linguistics, Natural writing, Language specific and General. Each of them is divided into five to three problem classes. Word lists for each of the above mentioned categories were compiled and synthesized by both a commercial and an open source synthesizer, both being based on the non-uniform unit-selection approach. The synthesized speech was evaluated by human judges using the Speechalyzer toolkit and the results are discussed. It shows that, as expected, the commercial synthesizer performs much better than the open-source one, and especially words of foreign origin were pronounced badly by both systems.
Recent spoken dialog systems have been able to recognize freely spoken user input in restricted domains thanks to statistical methods in the automatic speech recognition. These methods require a high number of natural language utterances to train the speech recognition engine and to assess the quality of the system. Since human speech offers many variants associated with a single intent, a high number of user utterances have to be elicited. Developers are therefore turning to crowdsourcing to collect this data. This paper compares three different methods to elicit multiple utterances for given semantics via crowd sourcing, namely with pictures, with text and with semantic entities. Specifically, we compare the methods with regard to the number of valid data and linguistic variance, whereby a quantitative and qualitative approach is proposed. In our study, the method with text led to a high variance in the utterances and a relatively low rate of invalid data.
This paper describes the characteristics and structure of a Basque singing voice database of bertsolaritza. Bertsolaritza is a popular singing style from Basque Country sung exclusively in Basque that is improvised and a capella. The database is designed to be used in statistical singing voice synthesis for bertsolaritza style. Starting from the recordings and transcriptions of numerous singers, diarization and phoneme alignment experiments have been made to extract the singing voice from the recordings and create phoneme alignments. This labelling processes have been performed applying standard speech processing techniques and the results prove that these techniques can be used in this specific singing style.
We introduce a unique, comprehensive Austrian German multi-sensor corpus with moving and non-moving speakers to facilitate the evaluation of estimators and detectors that jointly detect a speaker{'}s spatial and temporal parameters. The corpus is suitable for various machine learning and signal processing tasks, linguistic studies, and studies related to a speaker{'}s fundamental frequency (due to recorded glottograms). Available corpora are limited to (synthetically generated/spatialized) speech data or recordings of musical instruments that lack moving speakers, glottograms, and/or multi-channel distant speech recordings. That is why we recorded 24 spatially non-moving and moving speakers, balanced male and female, to set up a two-room and 43-channel Austrian German multi-sensor speech corpus. It contains 8.2 hours of read speech based on phonetically balanced sentences, commands, and digits. The orthographic transcriptions include around 53,000 word tokens and 2,070 word types. Special features of this corpus are the laryngograph recordings (representing glottograms required to detect a speaker{'}s instantaneous fundamental frequency and pitch), corresponding clean-speech recordings, and spatial information and video data provided by four Kinects and a camera.
Auditory voice quality judgements are used intensively for the clinical assessment of pathological voice. Voice quality concepts are fuzzily defined and poorly standardized however, which hinders scientific and clinical communication. The described database documents a wide variety of pathologies and is used to investigate auditory voice quality concepts with regard to phonation mechanisms. The database contains 375 laryngeal high-speed videos and simultaneous high-quality audio recordings of sustained phonations of 80 pathological and 40 non-pathological subjects. Interval wise annotations regarding video and audio quality, as well as voice quality ratings are provided. Video quality is annotated for the visibility of anatomical structures and artefacts such as blurring or reduced contrast. Voice quality annotations include ratings on the presence of dysphonia and diplophonia. The purpose of the database is to aid the formulation of observationally well-founded models of phonation and the development of model-based automatic detectors for distinct types of phonation, especially for clinically relevant nonmodal voice phenomena. Another application is the training of audio-based fundamental frequency extractors on video-based reference fundamental frequencies.
In this paper we introduce a Bulgarian speech database, which was created for the purpose of ASR technology development. The paper describes the design and the content of the speech database. We present also an empirical evaluation of the performance of a LVCSR system for Bulgarian trained on the BulPhonC data. The resource is available free for scientific usage.
In this paper the authors present a speech corpus designed and created for the development and evaluation of dictation systems in Latvian. The corpus consists of over nine hours of orthographically annotated speech from 30 different speakers. The corpus features spoken commands that are common for dictation systems for text editors. The corpus is evaluated in an automatic speech recognition scenario. Evaluation results in an ASR dictation scenario show that the addition of the corpus to the acoustic model training data in combination with language model adaptation allows to decrease the WER by up to relative 41.36{\%} (or 16.83{\%} in absolute numbers) compared to a baseline system without language model adaptation. Contribution of acoustic data augmentation is at relative 12.57{\%} (or 3.43{\%} absolute).
This paper introduces the LetsRead Corpus of European Portuguese read speech from 6 to 10 years old children. The motivation for the creation of this corpus stems from the inexistence of databases with recordings of reading tasks of Portuguese children with different performance levels and including all the common reading aloud disfluencies. It is also essential to develop techniques to fulfill the main objective of the LetsRead project: to automatically evaluate the reading performance of children through the analysis of reading tasks. The collected data amounts to 20 hours of speech from 284 children from private and public Portuguese schools, with each child carrying out two tasks: reading sentences and reading a list of pseudowords, both with varying levels of difficulty throughout the school grades. In this paper, the design of the reading tasks presented to children is described, as well as the collection procedure. Manually annotated data is analyzed according to disfluencies and reading performance. The considered word difficulty parameter is also confirmed to be suitable for the pseudoword reading tasks.
The BAS CLARIN speech data repository is introduced. At the current state it comprises 31 pre-dominantly German corpora of spoken language. It is compliant to the CLARIN-D as well as the OLAC requirements. This enables its embedding into several infrastructures. We give an overview over its structure, its implementation as well as the corpora it contains.
We present a new Dutch dysarthric speech database containing utterances of neurological patients with Parkinson{'}s disease, traumatic brain injury and cerebrovascular accident. The speech content is phonetically and linguistically diversified by using numerous structured sentence and word lists. Containing more than 6 hours of mildly to moderately dysarthric speech, this database can be used for research on dysarthria and for developing and testing speech-to-text systems designed for medical applications. Current activities aimed at extending this database are also discussed.
Language resources, such as corpora, are important for various natural language processing tasks. Urdu has millions of speakers around the world but it is under-resourced in terms of standard evaluation resources. This paper reports the construction of a benchmark corpus for Urdu summaries (abstracts) to facilitate the development and evaluation of single document summarization systems for Urdu language. In Urdu, space does not always mark word boundary. Therefore, we created two versions of the same corpus. In the first version, words are separated by space. In contrast, proper word boundaries are manually tagged in the second version. We further apply normalization, part-of-speech tagging, morphological analysis, lemmatization, and stemming for the articles and their summaries in both versions. In order to apply these annotations, we re-implemented some NLP tools for Urdu. We provide Urdu Summary Corpus, all these annotations and the needed software tools (as open-source) for researchers to run experiments and to evaluate their work including but not limited to single-document summarization task.
In this paper we report our effort to construct the first ever Indonesian corpora for chat summarization. Specifically, we utilized documents of multi-participant chat from a well known online instant messaging application, WhatsApp. We construct the gold standard by asking three native speakers to manually summarize 300 chat sections (152 of them contain images). As result, three reference summaries in extractive and either abstractive form are produced for each chat sections. The corpus is still in its early stage of investigation, yielding exciting possibilities of future works.
Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE{'}s effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.
WordNet represents a cornerstone in the Computational Linguistics field, linking words to meanings (or senses) through a taxonomical representation of synsets, i.e., clusters of words with an equivalent meaning in a specific context often described by few definitions (or glosses) and examples. Most of the approaches to the Word Sense Disambiguation task fully rely on these short texts as a source of contextual information to match with the input text to disambiguate. This paper presents the first attempt to enrich synsets data with common-sense definitions, automatically retrieved from ConceptNet 5, and disambiguated accordingly to WordNet. The aim was to exploit the shared- and immediate-thinking nature of common-sense knowledge to extend the short but incredibly useful contextual information of the synsets. A manual evaluation on a subset of the entire result (which counts a total of almost 600K synset enrichments) shows a very high precision with an estimated good recall.
We present VPS-GradeUp ‚Äï a set of 11,400 graded human decisions on usage patterns of 29 English lexical verbs from the Pattern Dictionary of English Verbs by Patrick Hanks. The annotation contains, for each verb lemma, a batch of 50 concordances with the given lemma as KWIC, and for each of these concordances we provide a graded human decision on how well the individual PDEV patterns for this particular lemma illustrate the given concordance, indicated on a 7-point Likert scale for each PDEV pattern. With our annotation, we were pursuing a pilot investigation of the foundations of human clustering and disambiguation decisions with respect to usage patterns of verbs in context. The data set is publicly available at http://hdl.handle.net/11234/1-1585.
We present an annotation study on a representative dataset of literal and idiomatic uses of German infinitive-verb compounds in newspaper and journal texts. Infinitive-verb compounds form a challenge for writers of German, because spelling regulations are different for literal and idiomatic uses. Through the participation of expert lexicographers we were able to obtain a high-quality corpus resource which offers itself as a testbed for automatic idiomaticity detection and coarse-grained word-sense disambiguation. We trained a classifier on the corpus which was able to distinguish literal and idiomatic uses with an accuracy of 85 {\%}.
We launch the SemDaX corpus which is a recently completed Danish human-annotated corpus available through a CLARIN academic license. The corpus includes approx. 90,000 words, comprises six textual domains, and is annotated with sense inventories of different granularity. The aim of the developed corpus is twofold: i) to assess the reliability of the different sense annotation schemes for Danish measured by qualitative analyses and annotation agreement scores, and ii) to serve as training and test data for machine learning algorithms with the practical purpose of developing sense taggers for Danish. To these aims, we take a new approach to human-annotated corpus resources by double annotating a much larger part of the corpus than what is normally seen: for the all-words task we double annotated 60{\%} of the material and for the lexical sample task 100{\%}. We include in the corpus not only the adjucated files, but also the diverging annotations. In other words, we consider not all disagreement to be noise, but rather to contain valuable linguistic information that can help us improve our annotation schemes and our learning algorithms.
We present a pilot analysis of a new linguistic resource, VPS-GradeUp (available at http://hdl.handle.net/11234/1-1585). The resource contains 11,400 graded human decisions on usage patterns of 29 English lexical verbs, randomly selected from the Pattern Dictionary of English Verbs (Hanks, 2000 2014) based on their frequency and the number of senses their lemmas have in PDEV. This data set has been created to observe the interannotator agreement on PDEV patterns produced using the Corpus Pattern Analysis (Hanks, 2013). Apart from the graded decisions, the data set also contains traditional Word-Sense-Disambiguation (WSD) labels. We analyze the associations between the graded annotation and WSD annotation. The results of the respective annotations do not correlate with the size of the usage pattern inventory for the respective verbs lemmas, which makes the data set worth further linguistic analysis.
Chinese sentences are written as sequences of characters, which are elementary units of syntax and semantics. Characters are highly polysemous in forming words. We present a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings, and explore the usefulness of such character embeddings to Chinese NLP tasks. Evaluation on character similarity shows that multi-prototype embeddings are significantly better than a single-prototype baseline. In addition, used as features in the Chinese NER task, the embeddings result in a 1.74{\%} F-score improvement over a state-of-the-art baseline.
Named Entity Disambiguation (NED) is the task of linking a named-entity mention to an instance in a knowledge-base, typically Wikipedia-derived resources like DBpedia. This task is closely related to word-sense disambiguation (WSD), where the mention of an open-class word is linked to a concept in a knowledge-base, typically WordNet. This paper analyzes the relation between two annotated datasets on NED and WSD, highlighting the commonalities and differences. We detail the methods to construct a NED system following the WSD word-expert approach, where we need a dictionary and one classifier is built for each target entity mention string. Constructing a dictionary for NED proved challenging, and although similarity and ambiguity are higher for NED, the results are also higher due to the larger number of training data, and the more crisp and skewed meaning differences.
The experiments presented here exploit the properties of the Apertium RDF Graph, principally cycle density and nodes{'} degree, to automatically generate new translation relations between words, and therefore to enrich existing bilingual dictionaries with new entries. Currently, the Apertium RDF Graph includes data from 22 Apertium bilingual dictionaries and constitutes a large unified array of linked lexical entries and translations that are available and accessible on the Web (http://linguistic.linkeddata.es/apertium/). In particular, its graph structure allows for interesting exploitation opportunities, some of which are addressed in this paper. Two {`}massive{'} experiments are reported: in the first one, the original EN-ES translation set was removed from the Apertium RDF Graph and a new EN-ES version was generated. The results were compared against the previously removed EN-ES data and against the Concise Oxford Spanish Dictionary. In the second experiment, a new non-existent EN-FR translation set was generated. In this case the results were compared against a converted wiktionary English-French file. The results we got are really good and perform well for the extreme case of correlated polysemy. This lead us to address the possibility to use cycles and nodes degree to identify potential oddities in the source data. If cycle density proves efficient when considering potential targets, we can assume that in dense graphs nodes with low degree may indicate potential errors.
We introduce PreMOn (predicate model for ontologies), a linguistic resource for exposing predicate models (PropBank, NomBank, VerbNet, and FrameNet) and mappings between them (e.g, SemLink) as Linked Open Data. It consists of two components: (i) the PreMOn Ontology, an extension of the lemon model by the W3C Ontology-Lexica Community Group, that enables to homogeneously represent data from the various predicate models; and, (ii) the PreMOn Dataset, a collection of RDF datasets integrating various versions of the aforementioned predicate models and mapping resources. PreMOn is freely available and accessible online in different ways, including through a dedicated SPARQL endpoint.
This paper describes work on incorporating Princenton{'}s WordNet morphosemantics links to the fabric of the Portuguese OpenWordNet-PT. Morphosemantic links are relations between verbs and derivationally related nouns that are semantically typed (such as for tune-tuner ‚Äï in Portuguese {``}afinar-afinador{''} {--} linked through an {``}agent{''} link). Morphosemantic links have been discussed for Princeton{'}s WordNet for a while, but have not been added to the official database. These links are very useful, they help us to improve our Portuguese WordNet. Thus we discuss the integration of these links in our base and the issues we encountered with the integration.
The development of standard models for describing general lexical resources has led to the emergence of numerous lexical datasets of various languages in the Semantic Web. However, equivalent models covering the linguistic domain of morphology do not exist. As a result, there are hardly any language resources of morphemic data available in RDF to date. This paper presents the creation of the Hebrew Morpheme Inventory from a manually compiled tabular dataset comprising around 52.000 entries. It is an ongoing effort of representing the lexemes, word-forms and morphologigal patterns together with their underlying relations based on the newly created Multilingual Morpheme Ontology (MMoOn). It will be shown how segmented Hebrew language data can be granularly described in a Linked Data format, thus, serving as an exemplary case for creating morpheme inventories of any inflectional language with MMoOn. The resulting dataset is described a) according to the structure of the underlying data format, b) with respect to the Hebrew language characteristic of building word-forms directly from roots, c) by exemplifying how inflectional information is realized and d) with regard to its enrichment with external links to sense resources.
We present a new collection of multilingual corpora automatically created from the content available in the Global Voices websites, where volunteers have been posting and translating citizen media stories since 2004. We describe how we crawled and processed this content to generate parallel resources comprising 302.6K document pairs and 8.36M segment alignments in 756 language pairs. For some language pairs, the segment alignments in this resource are the first open examples of their kind. In an initial use of this resource, we discuss how a set of document pair detection algorithms performs on the Greek-English corpus.
High accuracy for automated translation and information retrieval calls for linguistic annotations at various language levels. The plethora of informal internet content sparked the demand for porting state-of-art natural language processing (NLP) applications to new social media as well as diverse language adaptation. Effort launched by the BOLT (Broad Operational Language Translation) program at DARPA (Defense Advanced Research Projects Agency) successfully addressed the internet information with enhanced NLP systems. BOLT aims for automated translation and linguistic analysis for informal genres of text and speech in online and in-person communication. As a part of this program, the Linguistic Data Consortium (LDC) developed valuable linguistic resources in support of the training and evaluation of such new technologies. This paper focuses on methodologies, infrastructure, and procedure for developing linguistic annotation at various language levels, including Treebank (TB), word alignment (WA), PropBank (PB), and co-reference (CoRef). Inspired by the OntoNotes approach with adaptations to the tasks to reflect the goals and scope of the BOLT project, this effort has introduced more annotation types of informal and free-style genres in English, Chinese and Egyptian Arabic. The corpus produced is by far the largest multi-lingual, multi-level and multi-genre annotation corpus of informal text and speech.
Large Web corpora containing full documents with permissive licenses are crucial for many NLP tasks. In this article we present the construction of 12 million-pages Web corpus (over 10 billion tokens) licensed under CreativeCommons license family in 50+ languages that has been extracted from CommonCrawl, the largest publicly available general Web crawl to date with about 2 billion crawled URLs. Our highly-scalable Hadoop-based framework is able to process the full CommonCrawl corpus on 2000+ CPU cluster on the Amazon Elastic Map/Reduce infrastructure. The processing pipeline includes license identification, state-of-the-art boilerplate removal, exact duplicate and near-duplicate document removal, and language detection. The construction of the corpus is highly configurable and fully reproducible, and we provide both the framework (DKPro C4CorpusTools) and the resulting data (C4Corpus) to the research community.
We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.
This paper introduces LexFr, a corpus-based French lexical resource built by adapting the framework LexIt, originally developed to describe the combinatorial potential of Italian predicates. As in the original framework, the behavior of a group of target predicates is characterized by a series of syntactic (i.e., subcategorization frames) and semantic (i.e., selectional preferences) statistical information (a.k.a. distributional profiles) whose extraction process is mostly unsupervised. The first release of LexFr includes information for 2,493 verbs, 7,939 nouns and 2,628 adjectives. In these pages we describe the adaptation process and evaluated the final resource by comparing the information collected for 20 test verbs against the information available in a gold standard dictionary. In the best performing setting, we obtained 0.74 precision, 0.66 recall and 0.70 F-measure.
Polarity lexicons are a basic resource for analyzing the sentiments and opinions expressed in texts in an automated way. This paper explores three methods to construct polarity lexicons: translating existing lexicons from other languages, extracting polarity lexicons from corpora, and annotating sentiments Lexical Knowledge Bases. Each of these methods require a different degree of human effort. We evaluate how much manual effort is needed and to what extent that effort pays in terms of performance improvement. Experiment setup includes generating lexicons for Basque, and evaluating them against gold standard datasets in different domains. Results show that extracting polarity lexicons from corpora is the best solution for achieving a good performance with reasonable human effort.
This paper describes the conversion into LMF, a standard lexicographic digital format of {`}al-q{\=a}m{\=u}s al-mu·∏•{\=\i}·π≠, a Medieval Arabic lexicon. The lexicon is first described, then all the steps required for the conversion are illustrated. The work is will produce a useful lexicographic resource for Arabic NLP, but is also interesting per se, to study the implications of adapting the LMF model to the Arabic language. Some reflections are offered as to the status of roots with respect to previously suggested representations. In particular, roots are, in our opinion are to be not treated as lexical entries, but modeled as lexical metadata for classifying and identifying lexical entries. In this manner, each root connects all entries that are derived from it.
In this work we introduce and describe a language resource composed of lists of simpler synonyms for Spanish. The synonyms are divided in different senses taken from the Spanish OpenThesaurus, where context disambiguation was performed by using statistical information from the Web and Google Books Ngrams. This resource is freely available online and can be used for different NLP tasks such as lexical simplification. Indeed, so far it has been already integrated into four tools.
The National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between 1771 and 1910 (Bremer-Laamanen 2001). This collection contains approximately 1.95 million pages in Finnish and Swedish. Finnish part of the collection consists of about 2.39 billion words. The National Library{'
A trend to digitize historical paper-based archives has emerged in recent years, with the advent of digital optical scanners. A lot of paper-based books, textbooks, magazines, articles, and documents are being transformed into electronic versions that can be manipulated by a computer. For this purpose, Optical Character Recognition (OCR) systems have been developed to transform scanned digital text into editable computer text. However, different kinds of errors in the OCR system output text can be found, but Automatic Error Correction tools can help in performing the quality of electronic texts by cleaning and removing noises. In this paper, we perform a qualitative and quantitative comparison of several error-correction techniques for historical French documents. Experimentation shows that our Machine Translation for Error Correction method is superior to other Language Modelling correction techniques, with nearly 13{\%} relative improvement compared to the initial baseline.
We present further work on evaluation of the fully automatic post-correction of Early Dutch Books Online, a collection of 10,333 18th century books. In prior work we evaluated the new implementation of Text-Induced Corpus Clean-up (TICCL) on the basis of a single book Gold Standard derived from this collection. In the current paper we revisit the same collection on the basis of a sizeable 1020 item random sample of OCR post-corrected strings from the full collection. Both evaluations have their own stories to tell and lessons to teach.
Crowdsourcing approaches for post-correction of OCR output (Optical Character Recognition) have been successfully applied to several historic text collections. We report on our crowd-correction platform Kokos, which we built to improve the OCR quality of the digitized yearbooks of the Swiss Alpine Club (SAC) from the 19th century. This multilingual heritage corpus consists of Alpine texts mainly written in German and French, all typeset in Antiqua font. Finding and engaging volunteers for correcting large amounts of pages into high quality text requires a carefully designed user interface, an easy-to-use workflow, and continuous efforts for keeping the participants motivated. More than 180,000 characters on about 21,000 pages were corrected by volunteers in about 7 month, achieving an OCR gold standard with a systematically evaluated accuracy of 99.7{\%} on the word level. The crowdsourced OCR gold standard and the corresponding original OCR recognition results from Abby FineReader 7 for each page are available as a resource. Additionally, the scanned images (300dpi) of all pages are included in order to facilitate tests with other OCR software.
Given a controversial issue, argument mining from natural language texts (news papers, and any form of text on the Internet) is extremely challenging: domain knowledge is often required together with appropriate forms of inferences to identify arguments. This contribution explores the types of knowledge that are required and how they can be paired with reasoning schemes, language processing and language resources to accurately mine arguments. We show via corpus analysis that the Generative Lexicon, enhanced in different manners and viewed as both a lexicon and a domain knowledge representation, is a relevant approach. In this paper, corpus annotation for argument mining is first developed, then we show how the generative lexicon approach must be adapted and how it can be paired with language processing patterns to extract and specify the nature of arguments. Our approach to argument mining is thus knowledge driven.
In the present paper, we analyse variation of discourse phenomena in two typologically different languages, i.e. in German and Czech. The novelty of our approach lies in the nature of the resources we are using. Advantage is taken of existing resources, which are, however, annotated on the basis of two different frameworks. We use an interoperable scheme unifying discourse phenomena in both frameworks into more abstract categories and considering only those phenomena that have a direct match in German and Czech. The discourse properties we focus on are relations of identity, semantic similarity, ellipsis and discourse relations. Our study shows that the application of interoperable schemes allows an exploitation of discourse-related phenomena analysed in different projects and on the basis of different frameworks. As corpus compilation and annotation is a time-consuming task, positive results of this experiment open up new paths for contrastive linguistics, translation studies and NLP, including machine translation.
In sources used in oral history research (such as interviews with eye witnesses), passages where the degree of personal emotional involvement is found to be high can be of particular interest, as these may give insight into how historical events were experienced, and what moral dilemmas and psychological or religious struggles were encountered. In a pilot study involving a large corpus of interview recordings with Dutch war veterans, we have investigated if it is possible to develop a method for automatically identifying those passages where the degree of personal emotional involvement is high. The method is based on the automatic detection of exceptionally large silences and filled pause segments (using Automatic Speech Recognition), and cues taken from specific n-grams. The first results appear to be encouraging enough for further elaboration of the method.
Existing discourse research only focuses on the monolingual languages and the inconsistency between languages limits the power of the discourse theory in multilingual applications such as machine translation. To address this issue, we design and build a bilingual discource corpus in which we are currently defining and annotating the bilingual elementary discourse units (BEDUs). The BEDUs are then organized into hierarchical structures. Using this discourse style, we have annotated nearly 20K LDC sentences. Finally, we design a bilingual discourse based method for machine translation evaluation and show the effectiveness of our bilingual discourse annotations.
DiMLex is a lexicon of German connectives that can be used for various language understanding purposes. We enhanced the coverage to 275 connectives, which we regard as covering all known German discourse connectives in current use. In this paper, we consider the task of adding the semantic relations that can be expressed by each connective. After discussing different approaches to retrieving semantic information, we settle on annotating each connective with senses from the new PDTB 3.0 sense hierarchy. We describe our new implementation in the extended DiMLex, which will be available for research purposes.
Dispute mediation is a growing activity in the resolution of conflicts, and more and more research emerge to enhance and better understand this (until recently) understudied practice. Corpus analyses are necessary to study discourse in this context; yet, little data is available, mainly because of its confidentiality principle. After proposing hints and avenues to acquire transcripts of mediation sessions, this paper presents the Dispute Mediation Corpus, which gathers annotated excerpts of mediation dialogues. Although developed as part of a project on argumentation, it is freely available and the text data can be used by anyone. This first-ever open corpus of mediation interactions can be of interest to scholars studying discourse, but also conflict resolution, argumentation, linguistics, communication, etc. We advocate for using and extending this resource that may be valuable to a large variety of domains of research, particularly those striving to enhance the study of the rapidly growing activity of dispute mediation.
This paper presents the creation of a corpus of labeled disabilities in scientific papers. The identification of medical concepts in documents and, especially, the identification of disabilities, is a complex task mainly due to the variety of expressions that can make reference to the same problem. Currently there is not a set of documents manually annotated with disabilities with which to evaluate an automatic detection system of such concepts. This is the reason why this corpus arises, aiming to facilitate the evaluation of systems that implement an automatic annotation tool for extracting biomedical concepts such as disabilities. The result is a set of scientific papers manually annotated. For the selection of these scientific papers has been conducted a search using a list of rare diseases, since they generally have associated several disabilities of different kinds.
We present a new corpus, PersonaBank, consisting of 108 personal stories from weblogs that have been annotated with their Story Intention Graphs, a deep representation of the content of a story. We describe the topics of the stories and the basis of the Story Intention Graph representation, as well as the process of annotating the stories to produce the Story Intention Graphs and the challenges of adapting the tool to this new personal narrative domain. We also discuss how the corpus can be used in applications that retell the story using different styles of tellings, co-tellings, or as a content planner.
This paper explores several aspects together for a fine-grained Chinese discourse analysis. We deal with the issues of ambiguous discourse markers, ambiguous marker linkings, and more than one discourse marker. A universal feature representation is proposed. The pair-once postulation, cross-discourse-unit-first rule and word-pair-marker-first rule select a set of discourse markers from ambiguous linkings. Marker-Sum feature considers total contribution of markers and Marker-Preference feature captures the probability distribution of discourse functions of a representative marker by using preference rule. The HIT Chinese discourse relation treebank (HIT-CDTB) is used to evaluate the proposed models. The 25-way classifier achieves 0.57 micro-averaged F-score.
In this article, we present the RATP-DECODA Corpus which is composed by a set of 67 hours of speech from telephone conversations of a Customer Care Service (CCS). This corpus is already available on line at http://sldr.org/sldr000847/fr in its first version. However, many enhancements have been made in order to allow the development of automatic techniques to transcript conversations and to capture their meaning. These enhancements fall into two categories: firstly, we have increased the size of the corpus with manual transcriptions from a new operational day; secondly we have added new linguistic annotations to the whole corpus (either manually or through an automatic processing) in order to perform various linguistic tasks from syntactic and semantic parsing to dialog act tagging and dialog summarization.
We present the first corpus of texts annotated with two alternative approaches to discourse structure, Rhetorical Structure Theory (Mann and Thompson, 1988) and Segmented Discourse Representation Theory (Asher and Lascarides, 2003). 112 short argumentative texts have been analyzed according to these two theories. Furthermore, in previous work, the same texts have already been annotated for their argumentation structure, according to the scheme of Peldszus and Stede (2013). This corpus therefore enables studies of correlations between the two accounts of discourse structure, and between discourse and argumentation. We converted the three annotation formats to a common dependency tree format that enables to compare the structures, and we describe some initial findings.
We propose a scheme for annotating direct speech in literary texts, based on the Text Encoding Initiative (TEI) and the coreference annotation guidelines from the Message Understanding Conference (MUC). The scheme encodes the speakers and listeners of utterances in a text, as well as the quotative verbs that reports the utterances. We measure inter-annotator agreement on this annotation task. We then present statistics on a manually annotated corpus that consists of books from the New Testament. Finally, we visualize the corpus as a conversational network.
This paper presents a method for the normalization of historical texts using a combination of weighted finite-state transducers and language models. We have extended our previous work on the normalization of dialectal texts and tested the method against a 17th century literary work in Basque. This preprocessed corpus is made available in the LREC repository. The performance of this method for learning relations between historical and contemporary word forms is evaluated against resources in three languages. The method we present learns to map phonological changes using a noisy channel model. The model is based on techniques commonly used for phonological inference and producing Grapheme-to-Grapheme conversion systems encoded as weighted transducers and produces F-scores above 80{\%} in the task for Basque. A wider evaluation shows that the approach performs equally well with all the languages in our evaluation suite: Basque, Spanish and Slovene. A comparison against other methods that address the same task is also provided.
In this paper, we present Farasa (meaning insight in Arabic), which is a fast and accurate Arabic segmenter. Segmentation involves breaking Arabic words into their constituent clitics. Our approach is based on SVMrank using linear kernels. The features that we utilized account for: likelihood of stems, prefixes, suffixes, and their combination; presence in lexicons containing valid stems and named entities; and underlying stem templates. Farasa outperforms or equalizes state-of-the-art Arabic segmenters, namely QATARA and MADAMIRA. Meanwhile, Farasa is nearly one order of magnitude faster than QATARA and two orders of magnitude faster than MADAMIRA. The segmenter should be able to process one billion words in less than 5 hours. Farasa is written entirely in native Java, with no external dependencies, and is open-source.
This paper discusses the internal structure of complex Esperanto words (CWs). Using a morphological analyzer, possible affixation and compounding is checked for over 50,000 Esperanto lexemes against a list of 17,000 root words. Morpheme boundaries in the resulting analyses were then checked manually, creating a CW dictionary of 28,000 words, representing 56.4{\%} of the lexicon, or 19.4{\%} of corpus tokens. The error percentage of the EspGram morphological analyzer for new corpus CWs was 4.3{\%} for types and 6.4{\%} for tokens, with a recall of almost 100{\%}, and wrong/spurious boundaries being more common than missing ones. For pedagogical purposes a morpheme frequency dictionary was constructed for a 16 million word corpus, confirming the importance of agglutinative derivational morphemes in the Esperanto lexicon. Finally, as a means to reduce the morphological ambiguity of CWs, we provide POS likelihoods for Esperanto suffixes.
Vietnamese word segmentation (VWS) is a challenging basic issue for natural language processing. This paper addresses the problem of how does dictionary size influence VWS performance, proposes two novel measures: square overlap ratio (SOR) and relaxed square overlap ratio (RSOR), and validates their effectiveness. The SOR measure is the product of dictionary overlap ratio and corpus overlap ratio, and the RSOR measure is the relaxed version of SOR measure under an unsupervised condition. The two measures both indicate the suitable degree between segmentation dictionary and object corpus waiting for segmentation. The experimental results show that the more suitable, neither smaller nor larger, dictionary size is better to achieve the state-of-the-art performance for dictionary-based Vietnamese word segmenters.
D{\'e}monette is a derivational morphological network designed for the description of French. Its original architecture enables its use as a formal framework for the description of morphological analyses and as a repository for existing lexicons. It is fed with a variety of resources, which all are already validated. The harmonization of their content into a unified format provides them a second life, in which they are enriched with new properties, provided these are deductible from their contents. D{\'e}monette is released under a Creative Commons license. It is usable for theoretical and descriptive research in morphology, as a source of experimental material for psycholinguistics, natural language processing (NLP) and information retrieval (IR), where it fills a gap, since French lacks a large-coverage derivational resources database. The article presents the integration of two existing lexicons into D{\'e}monette. The first is Verbaction, a lexicon of deverbal action nouns. The second is Lexeur, a database of agent nouns in -eur derived from verbs or from nouns.
The paper introduces a {``}train once, use many{''} approach for the syntactic analysis of phrasal compounds (PC) of the type XP+N like {``}Would you like to sit on my knee?{''} nonsense. PCs are a challenge for NLP tools since they require the identification of a syntactic phrase within a morphological complex. We propose a method which uses a state-of-the-art dependency parser not only to analyse sentences (the environment of PCs) but also to compound the non-head of PCs in a well-defined particular condition which is the analysis of the non-head spanning from the left boundary (mostly marked by a determiner) to the nominal head of the PC. This method contains the following steps: (a) the use an English state-of-the-art dependency parser with data comprising sentences with PCs from the British National Corpus (BNC), (b) the detection of parsing errors of PCs, (c) the separate treatment of the non-head structure using the same model, and (d) the attachment of the non-head to the compound head. The evaluation of the method showed that the accuracy of 76{\%} could be improved by adding a step in the PC compounder module which specified user-defined contexts being sensitive to the part of speech of the non-head parts and by using TreeTagger, in line with our approach.
Dialectal Arabic (DA) poses serious challenges for Natural Language Processing (NLP). The number and sophistication of tools and datasets in DA are very limited in comparison to Modern Standard Arabic (MSA) and other languages. MSA tools do not effectively model DA which makes the direct use of MSA NLP tools for handling dialects impractical. This is particularly a challenge for the creation of tools to support learning Arabic as a living language on the web, where authentic material can be found in both MSA and DA. In this paper, we present the Dialectal Arabic Linguistic Learning Assistant (DALILA), a Chrome extension that utilizes cutting-edge Arabic dialect NLP research to assist learners and non-native speakers in understanding text written in either MSA or DA. DALILA provides dialectal word analysis and English gloss corresponding to each word.
The CELEX database is one of the standard lexical resources for German. It yields a wealth of data especially for phonological and morphological applications. The morphological part comprises deep-structure morphological analyses of German. However, as it was developed in the Nineties, both encoding and spelling are outdated. About one fifth of over 50,000 datasets contain umlauts and signs such as {\ss}. Changes to a modern version cannot be obtained by simple substitution. In this paper, we shortly describe the original content and form of the orthographic and morphological database for German in CELEX. Then we present our work on modernizing the linguistic data. Lemmas and morphological analyses are transferred to a modern standard of encoding by first merging orthographic and morphological information of the lemmas and their entries and then performing a second substitution for the morphs within their morphological analyses. Changes to modern German spelling are performed by substitution rules according to orthographical standards. We show an example of the use of the data for the disambiguation of morphological structures. The discussion describes prospects of future work on this or similar lexicons. The Perl script is publicly available on our website.
We propose an automatic approach towards determining the relative location of adjectives on a common scale based on their strength. We focus on adjectives expressing different degrees of goodness occurring in French product (perfumes) reviews. Using morphosyntactic patterns, we extract from the reviews short phrases consisting of a noun that encodes a particular aspect of the perfume and an adjective modifying that noun. We then associate each such n-gram with the corresponding product aspect and its related star rating. Next, based on the star scores, we generate adjective scales reflecting the relative strength of specific adjectives associated with a shared attribute of the product. An automatic ordering of the adjectives {``}correct{''} (correct), {``}sympa{''} (nice), {``}bon{''} (good) and {``}excellent{''} (excellent) according to their score in our resource is consistent with an intuitive scale based on human judgments. Our long-term objective is to generate different adjective scales in an empirical manner, which could allow the enrichment of lexical resources.
The automatic analysis of texts containing opinions of users about, e.g., products or political views has gained attention within the last decades. However, previous work on the task of analyzing user reviews about mobile applications in app stores is limited. Publicly available corpora do not exist, such that a comparison of different methods and models is difficult. We fill this gap by contributing the Sentiment Corpus of App Reviews (SCARE), which contains fine-grained annotations of application aspects, subjective (evaluative) phrases and relations between both. This corpus consists of 1,760 annotated application reviews from the Google Play Store with 2,487 aspects and 3,959 subjective phrases. We describe the process and methodology how the corpus was created. The Fleiss Kappa between four annotators reveals an agreement of 0.72. We provide a strong baseline with a linear-chain conditional random field and word-embedding features with a performance of 0.62 for aspect detection and 0.63 for the extraction of subjective phrases. The corpus is available to the research community to support the development of sentiment analysis methods on mobile application reviews.
In this article we describe our method of automatically expanding an existing lexicon of words with affective valence scores. The automatic expansion process was done in English. In addition, we describe our procedure for automatically creating lexicons in languages where such resources may not previously exist. The foreign languages we discuss in this paper are Spanish, Russian and Farsi. We also describe the procedures to systematically validate our newly created resources. The main contributions of this work are: 1) A general method for expansion and creation of lexicons with scores of words on psychological constructs such as valence, arousal or dominance; and 2) a procedure for ensuring validity of the newly constructed resources.
In this paper, we introduce a novel comprehensive dataset of 7,992 German tweets, which were manually annotated by two human experts with fine-grained opinion relations. A rich annotation scheme used for this corpus includes such sentiment-relevant elements as opinion spans, their respective sources and targets, emotionally laden terms with their possible contextual negations and modifiers. Various inter-annotator agreement studies, which were carried out at different stages of work on these data (at the initial training phase, upon an adjudication step, and after the final annotation run), reveal that labeling evaluative judgements in microblogs is an inherently difficult task even for professional coders. These difficulties, however, can be alleviated by letting the annotators revise each other{'}s decisions. Once rechecked, the experts can proceed with the annotation of further messages, staying at a fairly high level of agreement.
This paper discusses the challenges in carrying out fair comparative evaluations of sentiment analysis systems. Firstly, these are due to differences in corpus annotation guidelines and sentiment class distribution. Secondly, different systems often make different assumptions about how to interpret certain statements, e.g. tweets with URLs. In order to study the impact of these on evaluation results, this paper focuses on tweet sentiment analysis in particular. One existing and two newly created corpora are used, and the performance of four different sentiment analysis systems is reported; we make our annotated datasets and sentiment analysis applications publicly available. We see considerable variations in results across the different corpora, which calls into question the validity of many existing annotated datasets and evaluations, and we make some observations about both the systems and the datasets as a result.
This paper describes EmoTweet-28, a carefully curated corpus of 15,553 tweets annotated with 28 emotion categories for the purpose of training and evaluating machine learning models for emotion classification. EmoTweet-28 is, to date, the largest tweet corpus annotated with fine-grained emotion categories. The corpus contains annotations for four facets of emotion: valence, arousal, emotion category and emotion cues. We first used small-scale content analysis to inductively identify a set of emotion categories that characterize the emotions expressed in microblog text. We then expanded the size of the corpus using crowdsourcing. The corpus encompasses a variety of examples including explicit and implicit expressions of emotions as well as tweets containing multiple emotions. EmoTweet-28 represents an important resource to advance the development and evaluation of more emotion-sensitive systems.
Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, based on its constituents. We focus on sentiment composition in phrases formed by at least one positive and at least one negative word ‚Äï phrases like {`}happy accident{'} and {`}best winter break{'}. We refer to such phrases as opposing polarity phrases. We manually annotate a collection of opposing polarity phrases and their constituent single words with real-valued sentiment intensity scores using a method known as Best‚ÄïWorst Scaling. We show that the obtained annotations are consistent. We explore the entries in the lexicon for linguistic regularities that govern sentiment composition in opposing polarity phrases. Finally, we list the current and possible future applications of the lexicon.
Emotions are an important part of the human experience. They are responsible for the adaptation and integration in the environment, offering, most of the time together with the cognitive system, the appropriate responses to stimuli in the environment. As such, they are an important component in decision-making processes. In today{'}s society, the avalanche of stimuli present in the environment (physical or virtual) makes people more prone to respond to stronger affective stimuli (i.e., those that are related to their basic needs and motivations ‚Äï survival, food, shelter, etc.). In media reporting, this is translated in the use of arguments (factual data) that are known to trigger specific (strong, affective) behavioural reactions from the readers. This paper describes initial efforts to detect such arguments from text, based on the properties of concepts. The final system able to retrieve and label this type of data from the news in traditional and social platforms is intended to be integrated Europe Media Monitor family of applications to detect texts that trigger certain (especially negative) reactions from the public, with consequences on citizen safety and security.
This paper presents a framework and methodology for the annotation of perspectives in text. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives. We propose an annotation scheme that integrates these different phenomena. We use a multilayered annotation approach, splitting the annotation of different aspects of perspectives into small subsequent subtasks in order to reduce the complexity of the task and to better monitor interactions between layers. Currently, we have included four layers of perspective annotation: events, attribution, factuality and opinion. The annotations are integrated in a formal model called GRaSP, which provides the means to represent instances (e.g. events, entities) and propositions in the (real or assumed) world in relation to their mentions in text. Then, the relation between the source and target of a perspective is characterized by means of perspective annotations. This enables us to place alternative perspectives on the same entity, event or proposition next to each other.
With the explosive growth of online social media (forums, blogs, and social networks), exploitation of these new information sources has become essential. Our work is based on the sud4science project. The goal of this project is to perform multidisciplinary work on a corpus of authentic SMS, in French, collected in 2011 and anonymised (88milSMS corpus: http://88milsms.huma-num.fr). This paper highlights a new method to integrate opinion detection knowledge from an SMS corpus by combining lexical and semantic information. More precisely, our approach gives more weight to words with a sentiment (i.e. presence of words in a dedicated dictionary) for a classification task based on three classes: positive, negative, and neutral. The experiments were conducted on two corpora: an elongated SMS corpus (i.e. repetitions of characters in messages) and a non-elongated SMS corpus. We noted that non-elongated SMS were much better classified than elongated SMS. Overall, this study highlighted that the integration of semantic knowledge always improves classification.
This paper presents some experiments for specialising Paragraph Vectors, a new technique for creating text fragment (phrase, sentence, paragraph, text, ...) embedding vectors, for text polarity detection. The first extension regards the injection of polarity information extracted from a polarity lexicon into embeddings and the second extension aimed at inserting word order information into Paragraph Vectors. These two extensions, when training a logistic-regression classifier on the combined embeddings, were able to produce a relevant gain in performance when compared to the standard Paragraph Vector methods proposed by Le and Mikolov (2014).
In this article, we propose to evaluate the lexical similarity information provided by word representations against several opinion resources using traditional Information Retrieval tools. Word representation have been used to build and to extend opinion resources such as lexicon, and ontology and their performance have been evaluated on sentiment analysis tasks. We question this method by measuring the correlation between the sentiment proximity provided by opinion resources and the semantic similarity provided by word representations using different correlation coefficients. We also compare the neighbors found in word representations and list of similar opinion words. Our results show that the proximity of words in state-of-the-art word representations is not very effective to build sentiment similarity.
Vector space models and distributional information are widely used in NLP. The models typically rely on complex, high-dimensional objects. We present an interactive visualisation tool to explore salient lexical-semantic features of high-dimensional word objects and word similarities. Most visualisation tools provide only one low-dimensional map of the underlying data, so they are not capable of retaining the local and the global structure. We overcome this limitation by providing an additional trust-view to obtain a more realistic picture of the actual object distances. Additional tool options include the reference to a gold standard classification, the reference to a cluster analysis as well as listing the most salient (common) features for a selected subset of the words.
We present an experimental study making use of a machine learning approach to identify the factors that affect the aspectual value that characterizes verbs under each of their readings. The study is based on various morpho-syntactic and semantic features collected from a French lexical resource and on a gold standard aspectual classification of verb readings designed by an expert. Our results support the tested hypothesis, namely that agentivity and abstractness influence lexical aspect.
This paper presents mwetoolkit+sem: an extension of the mwetoolkit that estimates semantic compositionality scores for multiword expressions (MWEs) based on word embeddings. First, we describe our implementation of vector-space operations working on distributional vectors. The compositionality score is based on the cosine distance between the MWE vector and the composition of the vectors of its member words. Our generic system can handle several types of word embeddings and MWE lists, and may combine individual word representations using several composition techniques. We evaluate our implementation on a dataset of 1042 English noun compounds, comparing different configurations of the underlying word embeddings and word-composition models. We show that our vector-based scores model non-compositionality better than standard association measures such as log-likelihood.
Although meaning is at the core of human cognition, state-of-the-art distributional semantic models (DSMs) are often agnostic to the findings in the area of semantic cognition. In this work, we present a novel type of DSMs motivated by the dual-processing cognitive perspective that is triggered by lexico-semantic activations in the short-term human memory. The proposed model is shown to perform better than state-of-the-art models for computing semantic similarity between words. The fusion of different types of DSMs is also investigated achieving results that are comparable or better than the state-of-the-art. The used corpora along with a set of tools, as well as large repositories of vectorial word representations are made publicly available for four languages (English, German, Italian, and Greek).
This paper describes our independent effort for extending the monolingual semantic textual similarity (STS) task setting to multiple cross-lingual settings involving English, Japanese, and Chinese. So far, we have adopted a {``}monolingual similarity after translation{''} strategy to predict the semantic similarity between a pair of sentences in different languages. With this strategy, a monolingual similarity method is applied after having (one of) the target sentences translated into a pivot language. Therefore, this paper specifically details the required and developed resources to implement this framework, while presenting our current results for English-Japanese-Chinese cross-lingual STS tasks that may exemplify the validity of the framework.
We describe resources aimed at increasing the usability of the semantic representations utilized within the DELPH-IN (Deep Linguistic Processing with HPSG) consortium. We concentrate in particular on the Dependency Minimal Recursion Semantics (DMRS) formalism, a graph-based representation designed for compositional semantic representation with deep grammars. Our main focus is on English, and specifically English Resource Semantics (ERS) as used in the English Resource Grammar. We first give an introduction to ERS and DMRS and a brief overview of some existing resources and then describe in detail a new repository which has been developed to simplify the use of ERS/DMRS. We explain a number of operations on DMRS graphs which our repository supports, with sketches of the algorithms, and illustrate how these operations can be exploited in application building. We believe that this work will aid researchers to exploit the rich and effective but complex DELPH-IN resources.
How-knowledge is indispensable in daily life, but has relatively less quantity and poorer quality than what-knowledge in publicly available knowledge bases. This paper first extracts task-subtask pairs from wikiHow, then mines linguistic patterns from search query logs, and finally applies the mined patterns to extract subtasks to complete given how-to tasks. To evaluate the proposed methodology, we group tasks and the corresponding recommended subtasks into pairs, and evaluate the results automatically and manually. The automatic evaluation shows the accuracy of 0.4494. We also classify the mined patterns based on prepositions and find that the prepositions like {``}on{''}, {``}to{''}, and {``}with{''} have the better performance. The results can be used to accelerate how-knowledge base construction.
In this paper we present a novel application of compositional distributional semantic models (CDSMs): prediction of lexical typology. The paper introduces the notion of typological closeness, which is a novel rigorous formalization of semantic similarity based on comparison of multilingual data. Starting from the Moscow Database of Qualitative Features for adjective typology, we create four datasets of typological closeness, on which we test a range of distributional semantic models. We show that, on the one hand, vector representations of phrases based on data from one language can be used to predict how words within the phrase translate into different languages, and, on the other hand, that typological data can serve as a semantic benchmark for distributional models. We find that compositional distributional models, especially parametric ones, perform way above non-compositional alternatives on the task.
The problem of understanding the stream of messages exchanged on social media such as Facebook and Twitter is becoming a major challenge for automated systems. The tremendous amount of data exchanged on these platforms as well as the specific form of language adopted by social media users constitute a new challenging context for existing argument mining techniques. In this paper, we describe a resource of natural language arguments called DART (Dataset of Arguments and their Relations on Twitter) where the complete argument mining pipeline over Twitter messages is considered: (i) we identify which tweets can be considered as arguments and which cannot, and (ii) we identify what is the relation, i.e., support or attack, linking such tweets to each other.
This paper introduces Port4NooJ v3.0, the latest version of the Portuguese module for NooJ, highlights its main features, and details its three main new components: (i) a lexicon-grammar based dictionary of 5,177 human intransitive adjectives, and a set of local grammars that use the distributional properties of those adjectives for paraphrasing (ii) a polarity dictionary with 9,031 entries for sentiment analysis, and (iii) a set of priority dictionaries and local grammars for named entity recognition. These new components were derived and/or adapted from publicly available resources. The Port4NooJ v3.0 resource is innovative in terms of the specificity of the linguistic knowledge it incorporates. The dictionary is bilingual Portuguese-English, and the semantico-syntactic information assigned to each entry validates the linguistic relation between the terms in both languages. These characteristics, which cannot be found in any other public resource for Portuguese, make it a valuable resource for translation and paraphrasing. The paper presents the current statistics and describes the different complementary and synergic components and integration efforts.
This paper describes corpora collection activity for building large machine translation systems for Latvian e-Government platform. We describe requirements for corpora, selection and assessment of data sources, collection of the public corpora and creation of new corpora from miscellaneous sources. Methodology, tools and assessment methods are also presented along with the results achieved, challenges faced and conclusions made. Several approaches to address the data scarceness are discussed. We summarize the volume of obtained corpora and provide quality metrics of MT systems trained on this data. Resulting MT systems for English-Latvian, Latvian English and Latvian Russian are integrated in the Latvian e-service portal and are freely available on website HUGO.LV. This paper can serve as a guidance for similar activities initiated in other countries, particularly in the context of European Language Resource Coordination action.
The Nederlab project aims to bring together all digitized texts relevant to the Dutch national heritage, the history of the Dutch language and culture (circa 800 {--} present) in one user friendly and tool enriched open access web interface. This paper describes Nederlab halfway through the project period and discusses the collections incorporated, back-office processes, system back-end as well as the Nederlab Research Portal end-user web application.
In most of the research studies on Author Profiling, large quantities of correctly labeled data are used to train the models. However, this does not reflect the reality in forensic scenarios: in practical linguistic forensic investigations, the resources that are available to profile the author of a text are usually scarce. To pay tribute to this fact, we implemented a Semi-Supervised Learning variant of the k nearest neighbors algorithm that uses small sets of labeled data and a larger amount of unlabeled data to classify the authors of texts by gender (man vs woman). We describe the enriched KNN algorithm and show that the use of unlabeled instances improves the accuracy of our gender identification model. We also present a feature set that facilitates the use of a very small number of instances, reaching accuracies higher than 70{\%} with only 113 instances to train the model. It is also shown that the algorithm also performs well using publicly available data.
Classifying research grants into useful categories is a vital task for a funding body to give structure to the portfolio for analysis, informing strategic planning and decision-making. Automating this classification process would save time and effort, providing the accuracy of the classifications is maintained. We employ five classification models to classify a set of BBSRC-funded research grants in 21 research topics based on unigrams, technical terms and Latent Dirichlet Allocation models. To boost precision, we investigate methods for combining their predictions into five aggregate classifiers. Evaluation confirmed that ensemble classification models lead to higher precision.It was observed that there is not a single best-performing aggregate method for all research topics. Instead, the best-performing method for a research topic depends on the number of positive training instances available for this topic. Subject matter experts considered the predictions of aggregate models to correct erroneous or incomplete manual assignments.
In this work, we introduced a corpus for categorizing edit types in Wikipedia. This fine-grained taxonomy of edit types enables us to differentiate editing actions and find editor roles in Wikipedia based on their low-level edit types. To do this, we first created an annotated corpus based on 1,996 edits obtained from 953 article revisions and built machine-learning models to automatically identify the edit categories associated with edits. Building on this automated measurement of edit types, we then applied a graphical model analogous to Latent Dirichlet Allocation to uncover the latent roles in editors{'} edit histories. Applying this technique revealed eight different roles editors play, such as Social Networker, Substantive Expert, etc.
We present new language resources for Moroccan and Sanaani Yemeni Arabic. The resources include corpora for each dialect which have been morphologically annotated, and morphological analyzers for each dialect which are derived from these corpora. These are the first sets of resources for Moroccan and Yemeni Arabic. The resources will be made available to the public.
The paper deals with merging two complementary resources of morphological data previously existing for Czech, namely the inflectional dictionary MorfFlex CZ and the recently developed lexical network DeriNet. The MorfFlex CZ dictionary has been used by a morphological analyzer capable of analyzing/generating several million Czech word forms according to the rules of Czech inflection. The DeriNet network contains several hundred thousand Czech lemmas interconnected with links corresponding to derivational relations (relations between base words and words derived from them). After summarizing basic characteristics of both resources, the process of merging is described, focusing on both rather technical aspects (growth of the data, measuring the quality of newly added derivational relations) and linguistic issues (treating lexical homonymy and vowel/consonant alternations). The resulting resource contains 970 thousand lemmas connected with 715 thousand derivational relations and is publicly available on the web under the CC-BY-NC-SA license. The data were incorporated in the MorphoDiTa library version 2.0 (which provides morphological analysis, generation, tagging and lemmatization for Czech) and can be browsed and searched by two web tools (DeriNet Viewer and DeriNet Search tool).
The goal of a Hungarian research project has been to create an integrated Hungarian natural language processing framework. This infrastructure includes tools for analyzing Hungarian texts, integrated into a standardized environment. The morphological analyzer is one of the core components of the framework. The goal of this paper is to describe a fast and customizable morphological analyzer and its development framework, which synthesizes and further enriches the morphological knowledge implemented in previous tools existing for Hungarian. In addition, we present the method we applied to add semantic knowledge to the lexical database of the morphology. The method utilizes neural word embedding models and morphological and shallow syntactic knowledge.
The Mixer series of speech corpora were collected over several years, principally to support annual NIST evaluations of speaker recognition (SR) technologies. These evaluations focused on conversational speech over a variety of channels and recording conditions. One of the series, Mixer-6, added a new condition, read speech, to support basic scientific research on speaker characteristics, as well as technology evaluation. With read speech it is possible to make relatively precise measurements of phonetic events and features, which can be correlated with the performance of speaker recognition algorithms, or directly used in phonetic analysis of speaker variability. The read speech, as originally recorded, was adequate for large-scale evaluations (e.g., fixed-text speaker ID algorithms) but only marginally suitable for acoustic-phonetic studies. Numerous errors due largely to speaker behavior remained in the corpus, with no record of their locations or rate of occurrence. We undertook the effort to correct this situation with automatic methods supplemented by human listening and annotation. The present paper describes the tools and methods, resulting corrections, and some examples of the kinds of research studies enabled by these enhancements.
In this paper, we describe a new database with audio recordings of non-native (L2) speakers of English, and the perceptual evaluation experiment conducted with native English speakers for assessing the prosody of each recording. These annotations are then used to compute the gold standard using different methods, and a series of regression experiments is conducted to evaluate their impact on the performance of a regression model predicting the degree of naturalness of L2 speech. Further, we compare the relevance of different feature groups modelling prosody in general (without speech tempo), speech rate and pauses modelling speech tempo (fluency), voice quality, and a variety of spectral features. We also discuss the impact of various fusion strategies on performance.Overall, our results demonstrate that the prosody of non-native speakers of English as L2 can be reliably assessed using supra-segmental audio features; prosodic features seem to be the most important ones.
The IFCASL corpus is a French-German bilingual phonetic learner corpus designed, recorded and annotated in a project on individualized feedback in computer-assisted spoken language learning. The motivation for setting up this corpus was that there is no phonetically annotated and segmented corpus for this language pair of comparable of size and coverage. In contrast to most learner corpora, the IFCASL corpus incorporate data for a language pair in both directions, i.e. in our case French learners of German, and German learners of French. In addition, the corpus is complemented by two sub-corpora of native speech by the same speakers. The corpus provides spoken data by about 100 speakers with comparable productions, annotated and segmented on the word and the phone level, with more than 50{\%} manually corrected data. The paper reports on inter-annotator agreement and the optimization of the acoustic models for forced speech-text alignment in exercises for computer-assisted pronunciation training. Example studies based on the corpus data with a phonetic focus include topics such as the realization of /h/ and glottal stop, final devoicing of obstruents, vowel quantity and quality, pitch range, and tempo.
In this paper, we investigate some language acquisition facets of an auto-adaptative system that can automatically acquire most of the relevant lexical knowledge and authoring practices for an application in a given domain. This is the LELIO project: producing customized LELIE solutions. Our goal, within the framework of LELIE (a system that tags language uses that do not follow the Constrained Natural Language principles), is to automate the long, costly and error prone lexical customization of LELIE to a given application domain. Technical texts being relatively restricted in terms of syntax and lexicon, results obtained show that this approach is feasible and relatively reliable. By auto-adaptative, we mean that the system learns from a sample of the application corpus the various lexical terms and uses crucial for LELIE to work properly (e.g. verb uses, fuzzy terms, business terms, stylistic patterns). A technical writer validation method is developed at each step of the acquisition.
The fact that Japanese employs scriptio continua, or a writing system without spaces, complicates the first step of an NLP pipeline. Word segmentation is widely used in Japanese language processing, and lexical knowledge is crucial for reliable identification of words in text. Although external lexical resources like Wikipedia are potentially useful, segmentation mismatch prevents them from being straightforwardly incorporated into the word segmentation task. If we intentionally violate segmentation standards with the direct incorporation, quantitative evaluation will be no longer feasible. To address this problem, we propose to define a separate task that directly links given texts to an external resource, that is, wikification in the case of Wikipedia. By doing so, we can circumvent segmentation mismatch that may not necessarily be important for downstream applications. As the first step to realize the idea, we design the task of Japanese wikification and construct wikification corpora. We annotated subsets of the Balanced Corpus of Contemporary Written Japanese plus Twitter short messages. We also implement a simple wikifier and investigate its performance on these corpora.
This article presents Walenty - a new valence dictionary of Polish predicates, concentrating on its creation process and access via Internet browser. The dictionary contains two layers, syntactic and semantic. The syntactic layer describes syntactic and morphosyntactic constraints predicates put on their dependants. The semantic layer shows how predicates and their arguments are involved in a situation described in an utterance. These two layers are connected, representing how semantic arguments can be realised on the surface. Walenty also contains a powerful phraseological (idiomatic) component. Walenty has been created and can be accessed remotely with a dedicated tool called Slowal. In this article, we focus on most important functionalities of this system. First, we will depict how to access the dictionary and how built-in filtering system (covering both syntactic and semantic phenomena) works. Later, we will describe the process of creating dictionary by Slowal tool that both supports and controls the work of lexicographers.
CEPLEXicon (version 1.1) is a child lexicon resulting from the automatic tagging of two child corpora: the corpus Santos (Santos, 2006; Santos et al. 2014) and the corpus Child ‚Äï Adult Interaction (Freitas et al. 2012), which integrates information from the corpus Freitas (Freitas, 1997). This lexicon includes spontaneous speech produced by seven children (1;02.00 to 3;11.12) during approximately 86h of child-adult interaction. The automatic tagging comprised the lemmatization and morphosyntactic classification of the speech produced by the seven children included in the two child corpora; the lexicon contains information pertaining to lemmas and syntactic categories as well as absolute number of occurrences and frequencies in three age intervals: {\textless} 2 years; {\mbox{$\geq$}} 2 years and {\textless} 3 years; {\mbox{$\geq$}} 3 years. The information included in this lexicon and the format in which it is presented enables research in different areas and allows researchers to obtain measures of lexical growth. CEPLEXicon is available through the ELRA catalogue.
Language models are used in applications as diverse as speech recognition, optical character recognition and information retrieval. They are used to predict word appearance, and to weight the importance of words in these applications. One basic element of language models is the list of words in a language. Another is the unigram frequency of each word. But this basic information is not available for most languages in the world. Since the multilingual Wikipedia project encourages the production of encyclopedic-like articles in many world languages, we can find there an ever-growing source of text from which to extract these two language modelling elements: word list and frequency. Here we present a simple technique for converting this Wikipedia text into lexicons of weighted unigrams for the more than 280 languages present currently present in Wikipedia. The lexicons produced, and the source code for producing them in a Linux-based system are here made available for free on the Web.
GLAWI is a free, large-scale and versatile Machine-Readable Dictionary (MRD) that has been extracted from the French language edition of Wiktionary, called Wiktionnaire. In (Sajous and Hathout, 2015), we introduced GLAWI, gave the rationale behind the creation of this lexicographic resource and described the extraction process, focusing on the conversion and standardization of the heterogeneous data provided by this collaborative dictionary. In the current article, we describe the content of GLAWI and illustrate how it is structured. We also suggest various applications, ranging from linguistic studies, NLP applications to psycholinguistic experimentation. They all can take advantage of the diversity of the lexical knowledge available in GLAWI. Besides this diversity and extensive lexical coverage, GLAWI is also remarkable because it is the only free lexical resource of contemporary French that contains definitions. This unique material opens way to the renewal of MRD-based methods, notably the automated extraction and acquisition of semantic relations.
In recent years, several datasets have been released that include images and text, giving impulse to new methods that combine natural language processing and computer vision. However, there is a need for datasets of images in their natural textual context. The ION corpus contains 300K news articles published between August 2014 - 2015 in five online newspapers from two countries. The 1-year coverage over multiple publishers ensures a broad scope in terms of topics, image quality and editorial viewpoints. The corpus consists of JSON-LD files with the following data about each article: the original URL of the article on the news publisher{'}s website, the date of publication, the headline of the article, the URL of the image displayed with the article (if any), and the caption of that image. Neither the article text nor the images themselves are included in the corpus. Instead, the images are distributed as high-dimensional feature vectors extracted from a Convolutional Neural Network, anticipating their use in computer vision tasks. The article text is represented as a list of automatically generated entity and topic annotations in the form of Wikipedia/DBpedia pages. This facilitates the selection of subsets of the corpus for separate analysis or evaluation.
There are as many sign languages as there are deaf communities in the world. Linguists have been collecting corpora of different sign languages and annotating them extensively in order to study and understand their properties. On the other hand, the field of computer vision has approached the sign language recognition problem as a grand challenge and research efforts have intensified in the last 20 years. However, corpora collected for studying linguistic properties are often not suitable for sign language recognition as the statistical methods used in the field require large amounts of data. Recently, with the availability of inexpensive depth cameras, groups from the computer vision community have started collecting corpora with large number of repetitions for sign language recognition research. In this paper, we present the BosphorusSign Turkish Sign Language corpus, which consists of 855 sign and phrase samples from the health, finance and everyday life domains. The corpus is collected using the state-of-the-art Microsoft Kinect v2 depth sensor, and will be the first in this sign language research field. Furthermore, there will be annotations rendered by linguists so that the corpus will appeal both to the linguistic and sign language recognition research communities.
Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes. In particular, regarding elderly living alone at home, the detection of distress situation after a fall is very important to reassure this kind of population. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The C IRDO corpus is a dataset recorded in realistic conditions in D OMUS , a fully equipped Smart Home with microphones and home automation sensors, in which participants performed scenarios including real falls on a carpet and calls for help. These scenarios were elaborated thanks to a field study involving elderly persons. Experiments related in a first part to distress detection in real-time using audio and speech analysis and in a second part to fall detection using video analysis are presented. Results show the difficulty of the task. The database can be used as standardized database by researchers to evaluate and compare their systems for elderly person{'}s assistance.
Speech data currently receives a growing attention and is an important source of information. We still lack suitable corpora of transcribed speech annotated with semantic roles that can be used for semantic role labeling (SRL), which is not the case for written data. Semantic role labeling in speech data is a challenging and complex task due to the lack of sentence boundaries and the many transcription errors such as insertion, deletion and misspellings of words. In written data, SRL evaluation is performed at the sentence level, but in speech data sentence boundaries identification is still a bottleneck which makes evaluation more complex. In this work, we semi-automatically align the predicates found in transcribed speech obtained with an automatic speech recognizer (ASR) with the predicates found in the corresponding written documents of the OntoNotes corpus and manually align the semantic roles of these predicates thus obtaining annotated semantic frames in the speech data. This data can serve as gold standard alignments for future research in semantic role labeling of speech data.
CORILSE is a computerized corpus of Spanish Sign Language (Lengua de Signos Espa{\~n}ola, LSE). It consists of a set of recordings from different discourse genres by Galician signers living in the city of Vigo. In this paper we describe its annotation system, developed on the basis of pre-existing ones (mostly the model of Auslan corpus). This includes primary annotation of id-glosses for manual signs, annotation of non-manual component, and secondary annotation of grammatical categories and relations, because this corpus is been built for grammatical analysis, in particular argument structures in LSE. Up until this moment the annotation has been basically made by hand, which is a slow and time-consuming task. The need to facilitate this process leads us to engage in the development of automatic or semi-automatic tools for manual and facial recognition. Finally, we also present the web repository that will make the corpus available to different types of users, and will allow its exploitation for research purposes and other applications (e.g. teaching of LSE or design of tasks for signed language assessment).
The OFAI Multimodal Task Description Corpus (OFAI-MMTD Corpus) is a collection of dyadic teacher-learner (human-human and human-robot) interactions. The corpus is multimodal and tracks the communication signals exchanged between interlocutors in task-oriented scenarios including speech, gaze and gestures. The focus of interest lies on the communicative signals conveyed by the teacher and which objects are salient at which time. Data are collected from four different task description setups which involve spatial utterances, navigation instructions and more complex descriptions of joint tasks.
In recent years there has been a surge of interest in the natural language prosessing related to the real world, such as symbol grounding, language generation, and nonlinguistic data search by natural language queries. In order to concentrate on language ambiguities, we propose to use a well-defined {``}real world,{''} that is game states. We built a corpus consisting of pairs of sentences and a game state. The game we focus on is shogi (Japanese chess). We collected 742,286 commentary sentences in Japanese. They are spontaneously generated contrary to natural language annotations in many image datasets provided by human workers on Amazon Mechanical Turk. We defined domain specific named entities and we segmented 2,508 sentences into words manually and annotated each word with a named entity tag. We describe a detailed definition of named entities and show some statistics of our game commentary corpus. We also show the results of the experiments of word segmentation and named entity recognition. The accuracies are as high as those on general domain texts indicating that we are ready to tackle various new problems related to the real world.
In this paper, we describe the organization and the implementation of the CAMOMILE collaborative annotation framework for multimodal, multimedia, multilingual (3M) data. Given the versatile nature of the analysis which can be performed on 3M data, the structure of the server was kept intentionally simple in order to preserve its genericity, relying on standard Web technologies. Layers of annotations, defined as data associated to a media fragment from the corpus, are stored in a database and can be managed through standard interfaces with authentication. Interfaces tailored specifically to the needed task can then be developed in an agile way, relying on simple but reliable services for the management of the centralized annotations. We then present our implementation of an active learning scenario for person annotation in video, relying on the CAMOMILE server; during a dry run experiment, the manual annotation of 716 speech segments was thus propagated to 3504 labeled tracks. The code of the CAMOMILE framework is distributed in open source.
The Frankfurt Image GestURE corpus (FIGURE) is introduced. The corpus data is collected in an experimental setting where 50 naive participants spontaneously produced gestures in response to five to six terms from a total of 27 stimulus terms. The stimulus terms have been compiled mainly from image schemata from psycholinguistics, since such schemata provide a panoply of abstract contents derived from natural language use. The gestures have been annotated for kinetic features. FIGURE aims at finding (sets of) stable kinetic feature configurations associated with the stimulus terms. Given such configurations, they can be used for designing HCI gestures that go beyond pre-defined gesture vocabularies or touchpad gestures. It is found, for instance, that movement trajectories are far more informative than handshapes, speaking against purely handshape-based HCI vocabularies. Furthermore, the mean temporal duration of hand and arm movements associated vary with the stimulus terms, indicating a dynamic dimension not covered by vocabulary-based approaches. Descriptive results are presented and related to findings from gesture studies and natural language dialogue.
The term smart home refers to a living environment that by its connected sensors and actuators is capable of providing intelligent and contextualised support to its user. This may result in automated behaviors that blends into the user{'}s daily life. However, currently most smart homes do not provide such intelligent support. A first step towards such intelligent capabilities lies in learning automation rules by observing the user{'}s behavior. We present a new type of corpus for learning such rules from user behavior as observed from the events in a smart homes sensor and actuator network. The data contains information about intended tasks by the users and synchronized events from this network. It is derived from interactions of 59 users with the smart home in order to solve five tasks. The corpus contains recordings of more than 40 different types of data streams and has been segmented and pre-processed to increase signal quality. Overall, the data shows a high noise level on specific data types that can be filtered out by a simple smoothing approach. The resulting data provides insights into event patterns resulting from task specific user behavior and thus constitutes a basis for machine learning approaches to learn automation rules.
In this paper we describe our work in building an online tool for manually annotating texts in any spoken language with SignWriting in any sign language. The existence of such tool will allow the creation of parallel corpora between spoken and sign languages that can be used to bootstrap the creation of efficient tools for the Deaf community. As an example, a parallel corpus between English and American Sign Language could be used for training Machine Learning models for automatic translation between the two languages. Clearly, this kind of tool must be designed in a way that it eases the task of human annotators, not only by being easy to use, but also by giving smart suggestions as the annotation progresses, in order to save time and effort. By building a collaborative, online, easy to use annotation tool for building parallel corpora between spoken and sign languages we aim at helping the development of proper resources for sign languages that can then be used in state-of-the-art models currently used in tools for spoken languages. There are several issues and difficulties in creating this kind of resource, and our presented tool already deals with some of them, like adequate text representation of a sign and many to many alignments between words and signs.
In South-Asian languages such as Hindi and Urdu, action verbs having compound constructions and serial verbs constructions pose serious problems for natural language processing and other linguistic tasks. Urdu is an Indo-Aryan language spoken by 51, 500, 0001 speakers in India. Action verbs that occur spontaneously in day-to-day communication are highly ambiguous in nature semantically and as a consequence cause disambiguation issues that are relevant and applicable to Language Technologies (LT) like Machine Translation (MT) and Natural Language Processing (NLP). IMAGACT4ALL is an ontology-driven web-based platform developed by the University of Florence for storing action verbs and their inter-relations. This group is currently collaborating with Jawaharlal Nehru University (JNU) in India to connect Indian languages on this platform. Action verbs are frequently used in both written and spoken discourses and refer to various meanings because of their polysemic nature. The IMAGACT4ALL platform stores each 3d animation image, each one of them referring to a variety of possible ontological types, which in turn makes the annotation task for the annotator quite challenging with regard to selecting verb argument structure having a range of probability distribution. The authors, in this paper, discuss the issues and challenges such as complex predicates (compound and conjunct verbs), ambiguously animated video illustrations, semantic discrepancies, and the factors of verb-selection preferences that have produced significant problems in annotating Urdu verbs on the IMAGACT ontology.
Ontologies are powerful to support semantic based applications and intelligent systems. While ontology learning are challenging due to its bottleneck in handcrafting structured knowledge sources and training data. To address this difficulty, many researchers turn to ontology enrichment and population using external knowledge sources such as DBpedia. In this paper, we propose a method using DBpedia in a different manner. We utilize relation instances in DBpedia to supervise the ontology learning procedure from unstructured text, rather than populate the ontology structure as a post-processing step. We construct three language resources in areas of computer science: enriched Wikipedia concept tree, domain ontology, and gold standard from NSFC taxonomy. Experiment shows that the result of ontology learning from corpus of computer science can be improved via the relation instances extracted from DBpedia in the same field. Furthermore, making distinction between the relation instances and applying a proper weighting scheme in the learning procedure lead to even better result.
We present the development of a Norwegian Academic Wordlist (AKA list) for the Norwegian Bokm{\"a
This paper presents the Event and Implied Situation Ontology (ESO), a manually constructed resource which formalizes the pre and post situations of events and the roles of the entities affected by an event. The ontology is built on top of existing resources such as WordNet, SUMO and FrameNet. The ontology is injected to the Predicate Matrix, a resource that integrates predicate and role information from amongst others FrameNet, VerbNet, PropBank, NomBank and WordNet. We illustrate how these resources are used on large document collections to detect information that otherwise would have remained implicit. The ontology is evaluated on two aspects: recall and precision based on a manually annotated corpus and secondly, on the quality of the knowledge inferred by the situation assertions in the ontology. Evaluation results on the quality of the system show that 50{\%} of the events typed and enriched with ESO assertions are correct.
In this paper, we describe experiments on the morphosyntactic annotation of historical language varieties for the example of Middle Low German (MLG), the official language of the German Hanse during the Middle Ages and a dominant language around the Baltic Sea by the time. To our best knowledge, this is the first experiment in automatically producing morphosyntactic annotations for Middle Low German, and accordingly, no part-of-speech (POS) tagset is currently agreed upon. In our experiment, we illustrate how ontology-based specifications of projected annotations can be employed to circumvent this issue: Instead of training and evaluating against a given tagset, we decomponse it into independent features which are predicted independently by a neural network. Using consistency constraints (axioms) from an ontology, then, the predicted feature probabilities are decoded into a sound ontological representation. Using these representations, we can finally bootstrap a POS tagset capturing only morphosyntactic features which could be reliably predicted. In this way, our approach is capable to optimize precision and recall of morphosyntactic annotations simultaneously with bootstrapping a tagset rather than performing iterative cycles.
As part of a human-robot interaction project, we are interested by gestural modality as one of many ways to communicate. In order to develop a relevant gesture recognition system associated to a smart home butler robot. Our methodology is based on an IQ game-like Wizard of Oz experiment to collect spontaneous and implicitly produced gestures in an ecological context. During the experiment, the subject has to use non-verbal cues (i.e. gestures) to interact with a robot that is the referee. The subject is unaware that his gestures will be the focus of our study. In the second part of the experiment, we asked the subjects to do the gestures he had produced in the experiment, those are the explicit gestures. The implicit gestures are compared with explicitly produced ones to determine a relevant ontology. This preliminary qualitative analysis will be the base to build a big data corpus in order to optimize acceptance of the gesture dictionary in coherence with the {``}socio-affective glue{''} dynamics.
In this paper we describe our work in progress in the automatic development of a taxonomy of Spanish nouns, we offer the Perl implementation we have so far, and we discuss the different problems that still need to be addressed. We designed a statistically-based taxonomy induction algorithm consisting of a combination of different strategies not involving explicit linguistic knowledge. Being all quantitative, the strategies we present are however of different nature. Some of them are based on the computation of distributional similarity coefficients which identify pairs of sibling words or co-hyponyms, while others are based on asymmetric co-occurrence and identify pairs of parent-child words or hypernym-hyponym relations. A decision making process is then applied to combine the results of the previous steps, and finally connect lexical units to a basic structure containing the most general categories of the language. We evaluate the quality of the taxonomy both manually and also using Spanish Wordnet as a gold-standard. We estimate an average of 89.07{\%} precision and 25.49{\%} recall considering only the results which the algorithm presents with high degree of certainty, or 77.86{\%} precision and 33.72{\%} recall considering all results.
In this paper, we present a GOLD standard of part-of-speech tagged transcripts of spoken German. The GOLD standard data consists of four annotation layers ‚Äï transcription (modified orthography), normalization (standard orthography), lemmatization and POS tags ‚Äï all of which have undergone careful manual quality control. It comes with guidelines for the manual POS annotation of transcripts of German spoken data and an extended version of the STTS (Stuttgart T{\"u
Part-of-Speech(POS) tagging is a key step in many NLP algorithms. However, tweets are difficult to POS tag because they are short, are not always written maintaining formal grammar and proper spelling, and abbreviations are often used to overcome their restricted lengths. Arabic tweets also show a further range of linguistic phenomena such as usage of different dialects, romanised Arabic and borrowing foreign words. In this paper, we present an evaluation and a detailed error analysis of state-of-the-art POS taggers for Arabic when applied to Arabic tweets. On the basis of this analysis, we combine normalisation and external knowledge to handle the domain noisiness and exploit bootstrapping to construct extra training data in order to improve POS tagging for Arabic tweets. Our results show significant improvements over the performance of a number of well-known taggers for Arabic.
This paper relates to the challenge of morphological tagging and lemmatization in morphologically rich languages by example of German and Latin. We focus on the question what a practitioner can expect when using state-of-the-art solutions out of the box. Moreover, we contrast these with old(er) methods and implementations for POS tagging. We examine to what degree recent efforts in tagger development are reflected by improved accuracies ‚Äï and at what cost, in terms of training and processing time. We also conduct in-domain vs. out-domain evaluation. Out-domain evaluations are particularly insightful because the distribution of the data which is being tagged by a user will typically differ from the distribution on which the tagger has been trained. Furthermore, two lemmatization techniques are evaluated. Finally, we compare pipeline tagging vs. a tagging approach that acknowledges dependencies between inflectional categories.
We present a morphological tagger for Latin, called TTLab Latin Tagger based on Conditional Random Fields (TLT-CRF) which uses a large Latin lexicon. Beyond Part of Speech (PoS), TLT-CRF tags eight inflectional categories of verbs, adjectives or nouns. It utilizes a statistical model based on CRFs together with a rule interpreter that addresses scenarios of sparse training data. We present results of evaluating TLT-CRF to answer the question what can be learnt following the paradigm of 1st order CRFs in conjunction with a large lexical resource and a rule interpreter. Furthermore, we investigate the contigency of representational features and targeted parts of speech to learn about selective features.
Because of the small size of Romanian corpora, the performance of a PoS tagger or a dependency parser trained with the standard supervised methods fall far short from the performance achieved in most languages. That is why, we apply state-of-the-art methods for cross-lingual transfer on Romanian tagging and parsing, from English and several Romance languages. We compare the performance with monolingual systems trained with sets of different sizes and establish that training on a few sentences in target language yields better results than transferring from large datasets in other languages.
In this paper we present a tagger developed for inflectionally rich languages for which both a training corpus and a lexicon are available. We do not constrain the tagger by the lexicon entries, allowing both for lexicon incompleteness and noisiness. By using the lexicon indirectly through features we allow for known and unknown words to be tagged in the same manner. We test our tagger on Slovene data, obtaining a 25{\%} error reduction of the best previous results both on known and unknown words. Given that Slovene is, in comparison to some other Slavic languages, a well-resourced language, we perform experiments on the impact of token (corpus) vs. type (lexicon) supervision, obtaining useful insights in how to balance the effort of extending resources to yield better tagging results.
Treebanks are important resources for researchers in natural language processing, speech recognition, theoretical linguistics, etc. To strengthen the automatic processing of the Vietnamese language, a Vietnamese treebank has been built. However, the quality of this treebank is not satisfactory and is a possible source for the low performance of Vietnamese language processing. We have been building a new treebank for Vietnamese with about 40,000 sentences annotated with three layers: word segmentation, part-of-speech tagging, and bracketing. In this paper, we describe several challenges of Vietnamese language and how we solve them in developing annotation guidelines. We also present our methods to improve the quality of the annotation guidelines and ensure annotation accuracy and consistency. Experiment results show that inter-annotator agreement ratios and accuracy are higher than 90{\%} which is satisfactory.
This paper provides a new method to correct annotation errors in a treebank. The previous error correction method constructs a pseudo parallel corpus where incorrect partial parse trees are paired with correct ones, and extracts error correction rules from the parallel corpus. By applying these rules to a treebank, the method corrects errors. However, this method does not achieve wide coverage of error correction. To achieve wide coverage, our method adopts a different approach. In our method, we consider that an infrequent pattern which can be transformed to a frequent one is an annotation error pattern. Based on a tree mining technique, our method seeks such infrequent tree patterns, and constructs error correction rules each of which consists of an infrequent pattern and a corresponding frequent pattern. We conducted an experiment using the Penn Treebank. We obtained 1,987 rules which are not constructed by the previous method, and the rules achieved good precision.
The question of the type of text used as primary data in treebanks is of certain importance. First, it has an influence at the discourse level: an article is not organized in the same way as a novel or a technical document. Moreover, it also has consequences in terms of semantic interpretation: some types of texts can be easier to interpret than others. We present in this paper a new type of treebank which presents the particularity to answer to specific needs of experimental linguistic. It is made of short texts (book backcovers) that presents a strong coherence in their organization and can be rapidly interpreted. This type of text is adapted to short reading sessions, making it easy to acquire physiological data (e.g. eye movement, electroencepholagraphy). Such a resource offers reliable data when looking for correlations between computational models and human language processing.
This paper presents a new linguistic resource for the study and computational processing of Portuguese. CINTIL DependencyBank PREMIUM is a corpus of Portuguese news text, accurately manually annotated with a wide range of linguistic information (morpho-syntax, named-entities, syntactic function and semantic roles), making it an invaluable resource specially for the development and evaluation of data-driven natural language processing tools. The corpus is under active development, reaching 4,000 sentences in its current version. The paper also reports on the training and evaluation of a dependency parser over this corpus. CINTIL DependencyBank PREMIUM is freely-available for research purposes through META-SHARE.
This paper presents the first version of Estonian Universal Dependencies Treebank which has been semi-automatically acquired from Estonian Dependency Treebank and comprises ca 400,000 words (ca 30,000 sentences) representing the genres of fiction, newspapers and scientific writing. Article analyses the differences between two annotation schemes and the conversion procedure to Universal Dependencies format. The conversion has been conducted by manually created Constraint Grammar transfer rules. As the rules enable to consider unbounded context, include lexical information and both flat and tree structure features at the same time, the method has proved to be reliable and flexible enough to handle most of transformations. The automatic conversion procedure achieved LAS 95.2{\%}, UAS 96.3{\%} and LA 98.4{\%}. If punctuation marks were excluded from the calculations, we observed LAS 96.4{\%}, UAS 97.7{\%} and LA 98.2{\%}. Still the refinement of the guidelines and methodology is needed in order to re-annotate some syntactic phenomena, e.g. inter-clausal relations. Although automatic rules usually make quite a good guess even in obscure conditions, some relations should be checked and annotated manually after the main conversion.
This paper presents the construction of an open-source dependency treebank of spoken Slovenian, the first syntactically annotated collection of spontaneous speech in Slovenian. The treebank has been manually annotated using the Universal Dependencies annotation scheme, a one-layer syntactic annotation scheme with a high degree of cross-modality, cross-framework and cross-language interoperability. In this original application of the scheme to spoken language transcripts, we address a wide spectrum of syntactic particularities in speech, either by extending the scope of application of existing universal labels or by proposing new speech-specific extensions. The initial analysis of the resulting treebank and its comparison with the written Slovenian UD treebank confirms significant syntactic differences between the two language modalities, with spoken data consisting of shorter and more elliptic sentences, less and simpler nominal phrases, and more relations marking disfluencies, interaction, deixis and modality.
This paper introduces the ALT project initiated by the Advanced Speech Translation Research and Development Promotion Center (ASTREC), NICT, Kyoto, Japan. The aim of this project is to accelerate NLP research for Asian languages such as Indonesian, Japanese, Khmer, Laos, Malay, Myanmar, Philippine, Thai and Vietnamese. The original resource for this project was English articles that were randomly selected from Wikinews. The project has so far created a corpus for Myanmar and will extend in scope to include other languages in the near future. A 20000-sentence corpus of Myanmar that has been manually translated from an English corpus has been word segmented, word aligned, part-of-speech tagged and constituency parsed by human annotators. In this paper, we present the implementation steps for creating the treebank in detail, including a description of the ALT web-based treebanking tool. Moreover, we report statistics on the annotation quality of the Myanmar treebank created so far.
This article describes the conversion of the Norwegian Dependency Treebank to the Universal Dependencies scheme. This paper details the mapping of PoS tags, morphological features and dependency relations and provides a description of the structural changes made to NDT analyses in order to make it compliant with the UD guidelines. We further present PoS tagging and dependency parsing experiments which report first results for the processing of the converted treebank. The full converted treebank was made available with the 1.2 release of the UD treebanks.
META-NET is a European network of excellence, founded in 2010, that consists of 60 research centres in 34 European countries. One of the key visions and goals of META-NET is a truly multilingual Europe, which is substantially supported and realised through language technologies. In this article we provide an overview of recent developments around the multilingual Europe topic, we also describe recent and upcoming events as well as recent and upcoming strategy papers. Furthermore, we provide overviews of two new emerging initiatives, the CEF.AT and ELRC activity on the one hand and the Cracking the Language Barrier federation on the other. The paper closes with several suggested next steps in order to address the current challenges and to open up new opportunities.
We present here the context and results of two surveys (a French one and an international one) concerning Ethics and NLP, which we designed and conducted between June and September 2015. These surveys follow other actions related to raising concern for ethics in our community, including a Journ{\'e}e d{'}{\'e}tudes, a workshop and the Ethics and Big Data Charter. The concern for ethics shows to be quite similar in both surveys, despite a few differences which we present and discuss. The surveys also lead to think there is a growing awareness in the field concerning ethical issues, which translates into a willingness to get involved in ethics-related actions, to debate about the topic and to see ethics be included in major conferences themes. We finally discuss the limits of the surveys and the means of action we consider for the future. The raw data from the two surveys are freely available online.
An assessment of the intellectual property requirements for data used in machine-aided translation is provided based on a recent EC-funded legal review. This is compared against the capabilities offered by current linked open data standards from the W3C for publishing and sharing translation memories from translation projects, and proposals for adequately addressing the intellectual property needs of stakeholders in translation projects using open data vocabularies are suggested.
This article presents the latest dissemination activities and technical developments that were carried out for the International Standard Language Resource Number (ISLRN) service. It also recalls the main principle and submission process for providers to obtain their 13-digit ISLRN identifier. Up to March 2016, 2100 Language Resources were allocated an ISLRN number, not only ELRA{'}s and LDC{'}s catalogued Language Resources, but also the ones from other important organisations like the Joint Research Centre (JRC) and the Resource Management Agency (RMA) who expressed their strong support to this initiative.In the research field, not only assigning a unique identification number is important, but also referring to a Language Resource as an object \textit{per se} (like publications) has now become an obvious requirement. The ISLRN could also become an important parameter to be considered to compute a Language Resource Impact Factor (LRIF) in order to recognize the merits of the producers of Language Resources. Integrating the ISLRN number into a LR-oriented bibliographical reference is thus part of the objective. The idea is to make use of a BibTeX entry that would take into account Language Resources items, including ISLRN.The ISLRN being a requested field within the LREC 2016 submission, we expect that several other LRs will be allocated an ISLRN number by the conference date. With this expansion, this number aims to be a spreadly-used LR citation instrument within works referring to LRs.
Since its inception in 2010, the Linguistic Data Consortium{'}s data scholarship program has awarded no cost grants in data to 64 recipients from 26 countries. A survey of the twelve cycles to date ‚Äï two awards each in the Fall and Spring semesters from Fall 2010 through Spring 2016 ‚Äï yields an interesting view into graduate program research trends in human language technology and related fields and the particular data sets deemed important to support that research. The survey also reveals regions in which such activity appears to be on a rise, including in Arabic-speaking regions and portions of the Americas and Asia.
The paper introduces a new annotated French data set for Sentiment Analysis, which is a currently missing resource. It focuses on the collection from Twitter of data related to the socio-political debate about the reform of the bill for wedding in France. The design of the annotation scheme is described, which extends a polarity label set by making available tags for marking target semantic areas and figurative language devices. The annotation process is presented and the disagreement discussed, in particular, in the perspective of figurative language use and in that of the semantic oriented annotation, which are open challenges for NLP systems.
In this paper we present a new corpus of Arabic tweets that mention some form of violent event, developed to support the automatic identification of Human Rights Abuse. The dataset was manually labelled for seven classes of violence using crowdsourcing.
Personality profiling is the task of detecting personality traits of authors based on writing style. Several personality typologies exist, however, the Briggs-Myer Type Indicator (MBTI) is particularly popular in the non-scientific community, and many people use it to analyse their own personality and talk about the results online. Therefore, large amounts of self-assessed data on MBTI are readily available on social-media platforms such as Twitter. We present a novel corpus of tweets annotated with the MBTI personality type and gender of their author for six Western European languages (Dutch, German, French, Italian, Portuguese and Spanish). We outline the corpus creation and annotation, show statistics of the obtained data distributions and present first baselines on Myers-Briggs personality profiling and gender prediction for all six languages.
Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations.
Code-Switching (CS) between two languages is extremely common in communities with societal multilingualism where speakers switch between two or more languages when interacting with each other. CS has been extensively studied in spoken language by linguists for several decades but with the popularity of social-media and less formal Computer Mediated Communication, we now see a big rise in the use of CS in the text form. This poses interesting challenges and a need for computational processing of such code-switched data. As with any Computational Linguistic analysis and Natural Language Processing tools and applications, we need annotated data for understanding, processing, and generation of code-switched language. In this study, we focus on CS between English and Hindi Tweets extracted from the Twitter stream of Hindi-English bilinguals. We present an annotation scheme for annotating the pragmatic functions of CS in Hindi-English (Hi-En) code-switched tweets based on a linguistic analysis and some initial experiments.
We present an attempt to port the international syntactic annotation scheme, Universal Dependencies, to the Japanese language in this paper. Since the Japanese syntactic structure is usually annotated on the basis of unique chunk-based dependencies, we first introduce word-based dependencies by using a word unit called the Short Unit Word, which usually corresponds to an entry in the lexicon UniDic. Porting is done by mapping the part-of-speech tagset in UniDic to the universal part-of-speech tagset, and converting a constituent-based treebank to a typed dependency tree. The conversion is not straightforward, and we discuss the problems that arose in the conversion and the current solutions. A treebank consisting of 10,000 sentences was built by converting the existent resources and currently released to the public.
Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.
The recognition of multiword expressions (MWEs) in a sentence is important for such linguistic analyses as syntactic and semantic parsing, because it is known that combining an MWE into a single token improves accuracy for various NLP tasks, such as dependency parsing and constituency parsing. However, MWEs are not annotated in Penn Treebank. Furthermore, when converting word-based dependency to MWE-aware dependency directly, one could combine nodes in an MWE into a single node. Nevertheless, this method often leads to the following problem: A node derived from an MWE could have multiple heads and the whole dependency structure including MWE might be cyclic. Therefore we converted a phrase structure to a dependency structure after establishing an MWE as a single subtree. This approach can avoid an occurrence of multiple heads and/or cycles. In this way, we constructed an English dependency corpus taking into account compound function words, which are one type of MWEs that serve as functional expressions. In addition, we report experimental results of dependency parsing using a constructed corpus.
TANL is a suite of tools for text analytics based on the software architecture paradigm of data driven pipelines. The strategies for upgrading TANL to the use of Universal Dependencies range from a minimalistic approach consisting of introducing pre/post-processing steps into the native pipeline to revising the whole pipeline. We explore the issue in the context of the Italian Treebank, considering both the efforts involved, how to avoid losing linguistically relevant information and the loss of accuracy in the process. In particular we compare different strategies for parsing and discuss the implications of simplifying the pipeline when detailed part-of-speech and morphological annotations are not available, as it is the case for less resourceful languages. The experiments are relative to the Italian linguistic pipeline, but the use of different parsers in our evaluations and the avoidance of language specific tagging make the results general enough to be useful in helping the transition to UD for other languages.
We present a dependency treebank of the Chinese Buddhist Canon, which contains 1,514 texts with about 50 million Chinese characters. The treebank was created by an automatic parser trained on a smaller treebank, containing four manually annotated sutras (Lee and Kong, 2014). We report results on word segmentation, part-of-speech tagging and dependency parsing, and discuss challenges posed by the processing of medieval Chinese. In a case study, we exploit the treebank to examine verbs frequently associated with Buddha, and to analyze usage patterns of quotative verbs in direct speech. Our results suggest that certain quotative verbs imply status differences between the speaker and the listener.
Polysemy is the capacity for a word to have multiple meanings. Polysemy detection is a first step for Word Sense Induction (WSI), which allows to find different meanings for a term. The polysemy detection is also important for information extraction (IE) systems. In addition, the polysemy detection is important for building/enriching terminologies and ontologies. In this paper, we present a novel approach to detect if a biomedical term is polysemic, with the long term goal of enriching biomedical ontologies. This approach is based on the extraction of new features. In this context we propose to extract features following two manners: (i) extracted directly from the text dataset, and (ii) from an induced graph. Our method obtains an Accuracy and F-Measure of 0.978.
We introduce Cro36WSD, a freely-available medium-sized lexical sample for Croatian word sense disambiguation (WSD).Cro36WSD comprises 36 words: 12 adjectives, 12 nouns, and 12 verbs, balanced across both frequency bands and polysemy levels. We adopt the multi-label annotation scheme in the hope of lessening the drawbacks of discrete sense inventories and obtaining more realistic annotations from human experts. Sense-annotated data is collected through multiple annotation rounds to ensure high-quality annotations: with a 115 person-hours effort we reached an inter-annotator agreement score of 0.877. We analyze the obtained data and perform a correlation analysis between several relevant variables, including word frequency, number of senses, sense distribution skewness, average annotation time, and the observed inter-annotator agreement (IAA). Using the obtained data, we compile multi- and single-labeled dataset variants using different label aggregation schemes. Finally, we evaluate three different baseline WSD models on both dataset variants and report on the insights gained. We make both dataset variants freely available.
Word Sense Disambiguation (WSD) systems tend to have a strong bias towards assigning the Most Frequent Sense (MFS), which results in high performance on the MFS but in a very low performance on the less frequent senses. We addressed the MFS bias in WSD systems by combining the output from a WSD system with a set of mostly static features to create a MFS classifier to decide when to and not to choose the MFS. The output from this MFS classifier, which is based on the Random Forest algorithm, is then used to modify the output from the original WSD system. We applied our classifier to one of the state-of-the-art supervised WSD systems, i.e. IMS, and to of the best state-of-the-art unsupervised WSD systems, i.e. UKB. Our main finding is that we are able to improve the system output in terms of choosing between the MFS and the less frequent senses. When we apply the MFS classifier to fine-grained WSD, we observe an improvement on the less frequent sense cases, whereas we maintain the overall recall.
Linking concepts and named entities to knowledge bases has become a crucial Natural Language Understanding task. In this respect, recent works have shown the key advantage of exploiting textual definitions in various Natural Language Processing applications. However, to date there are no reliable large-scale corpora of sense-annotated textual definitions available to the research community. In this paper we present a large-scale high-quality corpus of disambiguated glosses in multiple languages, comprising sense annotations of both concepts and named entities from a unified sense inventory. Our approach for the construction and disambiguation of the corpus builds upon the structure of a large multilingual semantic network and a state-of-the-art disambiguation system; first, we gather complementary information of equivalent definitions across different languages to provide context for disambiguation, and then we combine it with a semantic similarity-based refinement. As a result we obtain a multilingual corpus of textual definitions featuring over 38 million definitions in 263 languages, and we make it freely available at http://lcl.uniroma1.it/disambiguated-glosses. Experiments on Open Information Extraction and Sense Clustering show how two state-of-the-art approaches improve their performance by integrating our disambiguated corpus into their pipeline.
The Potsdam Commentary Corpus is a collection of 175 German newspaper commentaries annotated on a variety of different layers. This paper introduces a new layer that covers the linguistic notion of information-structural topic (not to be confused with {`}topic{'} as applied to documents in information retrieval). To our knowledge, this is the first larger topic-annotated resource for German (and one of the first for any language). We describe the annotation guidelines and the annotation process, and the results of an inter-annotator agreement study, which compare favourably to the related work. The annotated corpus is freely available for research.
In this paper we present the Corpus of REcommendation STrength (CREST), a collection of HTML-formatted clinical guidelines annotated with the location of recommendations. Recommendations are labelled with an author-provided indicator of their strength of importance. As data was drawn from many disparate authors, we define a unified scheme of importance labels, and provide a mapping for each guideline. We demonstrate the utility of the corpus and its annotations in some initial measurements investigating the type of language constructions associated with strong and weak recommendations, and experiments into promising features for recommendation classification, both with respect to strong and weak labels, and to all labels of the unified scheme. An error analysis indicates that, while there is a strong relationship between lexical choices and strength labels, there can be substantial variance in the choices made by different authors.
Using the Methodius Natural Language Generation (NLG) System, we have created a corpus which consists of a collection of generated texts which describe ancient Greek artefacts. Each text is linked to two representations created as part of the NLG process. The first is a content plan, which uses rhetorical relations to describe the high-level discourse structure of the text, and the second is a logical form describing the syntactic structure, which is sent to the OpenCCG surface realization module to produce the final text output. In recent work, White and Howcroft (2015) have used the SPaRKy restaurant corpus, which contains similar combination of texts and representations, for their research on the induction of rules for the combination of clauses. In the first instance this corpus will be used to test their algorithms on an additional domain, and extend their work to include the learning of referring expression generation rules. As far as we know, the SPaRKy restaurant corpus is the only existing corpus of this type, and we hope that the creation of this new corpus in a different domain will provide a useful resource to the Natural Language Generation community.
The task of recommending relevant scientific literature for a draft academic paper has recently received significant interest. In our effort to ease the discovery of scientific literature and augment scientific writing, we aim to improve the relevance of results based on a shallow semantic analysis of the source document and the potential documents to recommend. We investigate the utility of automatic argumentative and rhetorical annotation of documents for this purpose. Specifically, we integrate automatic Core Scientific Concepts (CoreSC) classification into a prototype context-based citation recommendation system and investigate its usefulness to the task. We frame citation recommendation as an information retrieval task and we use the categories of the annotation schemes to apply different weights to the similarity formula. Our results show interesting and consistent correlations between the type of citation and the type of sentence containing the relevant information.
This paper presents SciCorp, a corpus of full-text English scientific papers of two disciplines, genetics and computational linguistics. The corpus comprises co-reference and bridging information as well as information status labels. Since SciCorp is annotated with both labels and the respective co-referent and bridging links, we believe it is a valuable resource for NLP researchers working on scientific articles or on applications such as co-reference resolution, bridging resolution or information status classification. The corpus has been reliably annotated by independent human coders with moderate inter-annotator agreement (average kappa = 0.71). In total, we have annotated 14 full papers containing 61,045 tokens and marked 8,708 definite noun phrases. The paper describes in detail the annotation scheme as well as the resulting corpus. The corpus is available for download in two different formats: in an offset-based format and for the co-reference annotations in the widely-used, tabular CoNLL-2012 format.
Discourse parsing is a challenging task in NLP and plays a crucial role in discourse analysis. To enable discourse analysis for Hindi, Hindi Discourse Relations Bank was created on a subset of Hindi TreeBank. The benefits of a discourse analyzer in automated discourse analysis, question summarization and question answering domains has motivated us to begin work on a discourse analyzer for Hindi. In this paper, we focus on discourse connective identification for Hindi. We explore various available syntactic features for this task. We also explore the use of dependency tree parses present in the Hindi TreeBank and study the impact of the same on the performance of the system. We report that the novel dependency features introduced have a higher impact on precision, in comparison to the syntactic features previously used for this task. In addition, we report a high accuracy of 96{\%} for this task.
This paper contributes to the limited body of empirical research in the domain of discourse structure of information seeking queries. We describe the development of an annotation schema for coding topic development in information seeking queries and the initial observations from a pilot sample of query sessions. The main idea that we explore is the relationship between constant and variable discourse entities and their role in tracking changes in the topic progression. We argue that the topicalized entities remain stable across development of the discourse and can be identified by a simple mechanism where anaphora resolution is a precursor. We also claim that a corpus annotated in this framework can be used as training data for dialogue management and computational semantics systems.
The PML-Tree Query is a general, powerful and user-friendly system for querying richly linguistically annotated treebanks. The paper shows how the PML-Tree Query can be used for searching for discourse relations in the Penn Discourse Treebank 2.0 mapped onto the syntactic annotation of the Penn Treebank.
This study describes a new corpus of over 60,000 hand-annotated metadiscourse acts from 106 OpenCourseWare lectures, from two different disciplines: Physics and Economics. Metadiscourse is a set of linguistic expressions that signal different functions in the discourse. This type of language is hypothesised to be helpful in finding a structure in unstructured text, such as lectures discourse. A brief summary is provided about the annotation scheme and labelling procedures, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary data that will be distributed with the corpus, and information relating to how to obtain the data. The results provide a deeper understanding of lecture structure and confirm the reliable coding of metadiscursive acts in academic lectures across different disciplines. The next stage of our research will be to build a classification model to automate the tagging process, instead of manual annotation, which take time and efforts. This is in addition to the use of these tags as indicators of the higher level structure of lecture discourse.
We present a large, free, French corpus of online written conversations extracted from the Ubuntu platform{'}s forums, mailing lists and IRC channels. The corpus is meant to support multi-modality and diachronic studies of online written conversations. We choose to build the corpus around a robust metadata model based upon strong principles, such as the {``}stand off{''} annotation principle. We detail the model, we explain how the data was collected and processed - in terms of meta-data, text and conversation - and we detail the corpus{'}contents through a series of meaningful statistics. A portion of the corpus - about 4,700 sentences from emails, forum posts and chat messages sent in November 2014 - is annotated in terms of dialogue acts and sentiment. We discuss how we adapted our dialogue act taxonomy from the DIT++ annotation scheme and how the data was annotated, before presenting our results as well as a brief qualitative analysis of the annotated data.
We present the DUEL corpus, consisting of 24 hours of natural, face-to-face, loosely task-directed dialogue in German, French and Mandarin Chinese. The corpus is uniquely positioned as a cross-linguistic, multimodal dialogue resource controlled for domain. DUEL includes audio, video and body tracking data and is transcribed and annotated for disfluency, laughter and exclamations.
The paper steps outside the comfort-zone of the traditional NLP tasks like automatic speech recognition (ASR) and machine translation (MT) to addresses two novel problems arising in the automated multilingual news monitoring: segmentation of the TV and radio program ASR transcripts into individual stories, and clustering of the individual stories coming from various sources and languages into storylines. Storyline clustering of stories covering the same events is an essential task for inquisitorial media monitoring. We address these two problems jointly by engaging the low-dimensional semantic representation capabilities of the sequence to sequence neural translation models. To enable joint multi-task learning for multilingual neural translation of morphologically rich languages we replace the attention mechanism with the sliding-window mechanism and operate the sequence to sequence neural translation model on the character-level rather than on the word-level. The story segmentation and storyline clustering problem is tackled by examining the low-dimensional vectors produced as a side-product of the neural translation process. The results of this paper describe a novel approach to the automatic story segmentation and storyline clustering problem.
Handling figurative language like irony is currently a challenging task in natural language processing. Since irony is commonly used in user-generated content, its presence can significantly undermine accurate analysis of opinions and sentiment in such texts. Understanding irony is therefore important if we want to push the state-of-the-art in tasks such as sentiment analysis. In this research, we present the construction of a Twitter dataset for two languages, being English and Dutch, and the development of new guidelines for the annotation of verbal irony in social media texts. Furthermore, we present some statistics on the annotated corpora, from which we can conclude that the detection of contrasting evaluations might be a good indicator for recognizing irony.
Inspired by the Oxford Children{'}s Corpus, we have developed a prototype corpus of Arabic texts written and/or selected for children. Our Arabic Children{'}s Corpus of 2950 documents and nearly 2 million words has been collected manually from the web during a 3-month project. It is of high quality, and contains a range of different children{'}s genres based on sources located, including classic tales from The Arabian Nights, and popular fictional characters such as Goha. We anticipate that the current and subsequent versions of our corpus will lead to interesting studies in text classification, language use, and ideology in children{'}s texts.
We introduce a framework for quality assurance of corpora, and apply it to the Reuters Multilingual Corpus (RCV2). The results of this quality assessment of this standard newsprint corpus reveal a significant duplication problem and, to a lesser extent, a problem with corrupted articles. From the raw collection of some 487,000 articles, almost one tenth are trivial duplicates. A smaller fraction of articles appear to be corrupted and should be excluded for that reason. The detailed results are being made available as on-line appendices to this article. This effort also demonstrates the beginnings of a constraint-based methodological framework for quality assessment and quality assurance for corpora. As a first implementation of this framework, we have investigated constraints to verify sample integrity, and to diagnose sample duplication, entropy aberrations, and tagging inconsistencies. To help identify near-duplicates in the corpus, we have employed both entropy measurements and a simple byte bigram incidence digest.
Attribution bias refers to the tendency of people to attribute successes to their own abilities but failures to external factors. In a business context an internal factor might be the restructuring of the firm and an external factor might be an unfavourable change in exchange or interest rates. In accounting research, the presence of an attribution bias has been demonstrated for the narrative sections of the annual financial reports. Previous studies have applied manual content analysis to this problem but in this paper we present novel work to automate the analysis of attribution bias through using machine learning algorithms. Previous studies have only applied manual content analysis on a small scale to reveal such a bias in the narrative section of annual financial reports. In our work a group of experts in accounting and finance labelled and annotated a list of 32,449 sentences from a random sample of UK Preliminary Earning Announcements (PEAs) to allow us to examine whether sentences in PEAs contain internal or external attribution and which kinds of attributions are linked to positive or negative performance. We wished to examine whether human annotators could agree on coding this difficult task and whether Machine Learning (ML) could be applied reliably to replicate the coding process on a much larger scale. Our best machine learning algorithm correctly classified performance sentences with 70{\%} accuracy and detected tone and attribution in financial PEAs with accuracy of 79{\%}.
The paper describes a comparative study of existing and novel text preprocessing and classification techniques for domain detection of user utterances. Two corpora are considered. The first one contains customer calls to a call centre for further call routing; the second one contains answers of call centre employees with different kinds of customer orientation behaviour. Seven different unsupervised and supervised term weighting methods were applied. The collective use of term weighting methods is proposed for classification effectiveness improvement. Four different dimensionality reduction methods were applied: stop-words filtering with stemming, feature selection based on term weights, feature transformation based on term clustering, and a novel feature transformation method based on terms belonging to classes. As classification algorithms we used k-NN and a SVM-based algorithm. The numerical experiments have shown that the simultaneous use of the novel proposed approaches (collectives of term weighting methods and the novel feature transformation method) allows reaching the high classification results with very small number of features.
Paraphrase plagiarism is a significant and widespread problem and research shows that it is hard to detect. Several methods and automatic systems have been proposed to deal with it. However, evaluation and comparison of such solutions is not possible because of the unavailability of benchmark corpora with manual examples of paraphrase plagiarism. To deal with this issue, we present the novel development of a paraphrase plagiarism corpus containing simulated (manually created) examples in the Urdu language - a language widely spoken around the world. This resource is the first of its kind developed for the Urdu language and we believe that it will be a valuable contribution to the evaluation of paraphrase plagiarism detection systems.
Assessing the suitability of an Open Source Software project for adoption requires not only an analysis of aspects related to the code, such as code quality, frequency of updates and new version releases, but also an evaluation of the quality of support offered in related online forums and issue trackers. Understanding the content types of forum messages and issue trackers can provide information about the extent to which requests are being addressed and issues are being resolved, the percentage of issues that are not being fixed, the cases where the user acknowledged that the issue was successfully resolved, etc. These indicators can provide potential adopters of the OSS with estimates about the level of available support. We present a detailed hierarchy of content types of online forum messages and issue tracker comments and a corpus of messages annotated accordingly. We discuss our experiments to classify forum messages and issue tracker comments into content-related classes, i.e.{\textasciitilde}to assign them to nodes of the hierarchy. The results are very encouraging.
The availability of labelled corpus is of great importance for supervised learning in emotion classification tasks. Because it is time-consuming to manually label text, hashtags have been used as naturally annotated labels to obtain a large amount of labelled training data from microblog. However, natural hashtags contain too much noise for it to be used directly in learning algorithms. In this paper, we design a three-stage semi-automatic method to construct an emotion corpus from microblogs. Firstly, a lexicon based voting approach is used to verify the hashtag automatically. Secondly, a SVM based classifier is used to select the data whose natural labels are consistent with the predicted labels. Finally, the remaining data will be manually examined to filter out the noisy data. Out of about 48K filtered Chinese microblogs, 39k microblogs are selected to form the final corpus with the Kappa value reaching over 0.92 for the automatic parts and over 0.81 for the manual part. The proportion of automatic selection reaches 54.1{\%}. Thus, the method can reduce about 44.5{\%} of manual workload for acquiring quality data. Experiment on a classifier trained on this corpus shows that it achieves comparable results compared to the manually annotated NLP{\&}CC2013 corpus.
Social media texts are often fairly informal and conversational, and when produced by bilinguals tend to be written in several different languages simultaneously, in the same way as conversational speech. The recent availability of large social media corpora has thus also made large-scale code-switched resources available for research. The paper addresses the issues of evaluation and comparison these new corpora entail, by defining an objective measure of corpus level complexity of code-switched texts. It is also shown how this formal measure can be used in practice, by applying it to several code-switched corpora.
To attract foreign students is among the goals of the Karlsruhe Institute of Technology (KIT). One obstacle to achieving this goal is that lectures at KIT are usually held in German which many foreign students are not sufficiently proficient in, as, e.g., opposed to English. While the students from abroad are learning German during their stay at KIT, it is challenging to become proficient enough in it in order to follow a lecture. As a solution to this problem we offer our automatic simultaneous lecture translation. It translates German lectures into English in real time. While not as good as human interpreters, the system is available at a price that KIT can afford in order to offer it in potentially all lectures. In order to assess whether the quality of the system we have conducted a user study. In this paper we present this study, the way it was conducted and its results. The results indicate that the quality of the system has passed a threshold as to be able to support students in their studies. The study has helped to identify the most crucial weaknesses of the systems and has guided us which steps to take next.
This paper introduces the ACL Reference Dataset for Terminology Extraction and Classification, version 2.0 (ACL RD-TEC 2.0). The ACL RD-TEC 2.0 has been developed with the aim of providing a benchmark for the evaluation of term and entity recognition tasks based on specialised text from the computational linguistics domain. This release of the corpus consists of 300 abstracts from articles in the ACL Anthology Reference Corpus, published between 1978{--}2006. In these abstracts, terms (i.e., single or multi-word lexical units with a specialised meaning) are manually annotated. In addition to their boundaries in running text, annotated terms are classified into one of the seven categories method, tool, language resource (LR), LR product, model, measures and measurements, and other. To assess the quality of the annotations and to determine the difficulty of this annotation task, more than 171 of the abstracts are annotated twice, independently, by each of the two annotators. In total, 6,818 terms are identified and annotated in more than 1300 sentences, resulting in a specialised vocabulary made of 3,318 lexical forms, mapped to 3,471 concepts. We explain the development of the annotation guidelines and discuss some of the challenges we encountered in this annotation task.
We present our guidelines and annotation procedure to create a human corrected machine translated post-edited corpus for the Modern Standard Arabic. Our overarching goal is to use the annotated corpus to develop automatic machine translation post-editing systems for Arabic that can be used to help accelerate the human revision process of translated texts. The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we created comprehensive and simplified annotation guidelines which were used by a team of five annotators and one lead annotator. In order to ensure a high annotation agreement between the annotators, multiple training sessions were held and regular inter-annotator agreement measures were performed to check the annotation quality. The created corpus of manual post-edited translations of English to Arabic articles is the largest to date for this language pair.
This work addresses the need to aid Machine Translation (MT) development cycles with a complete workflow of MT evaluation methods. Our aim is to assess, compare and improve MT system variants. We hereby report on novel tools and practices that support various measures, developed in order to support a principled and informed approach of MT development. Our toolkit for automatic evaluation showcases quick and detailed comparison of MT system variants through automatic metrics and n-gram feedback, along with manual evaluation via edit-distance, error annotation and task-based feedback.
Automatic Speech recognition (ASR) is one of the most widely used components in spoken language processing applications. ASR errors are of varying importance with respect to the application, making error analysis keys to improving speech processing applications. Knowing the most serious errors for the applicative case is critical to build better systems. In the context of Automatic Speech Recognition (ASR) used as a first step towards Named Entity Recognition (NER) in speech, error seriousness is usually determined by their frequency, due to the use of the WER as metric to evaluate the ASR output, despite the emergence of more relevant measures in the literature. We propose to use a different evaluation metric form the literature in order to classify ASR errors according to their seriousness for NER. Our results show that the ASR errors importance is ranked differently depending on the used evaluation metric. A more detailed analysis shows that the estimation of the error impact given by the ATENE metric is more adapted to the NER task than the estimation based only on the most used frequency metric WER.
The aim of this experiment is to present an easy way to compare fragments of texts in order to detect (supposed) results of copy {\&} paste operations between articles in the domain of Natural Language Processing (NLP). The search space of the comparisons is a corpus labeled as NLP4NLP gathering a large part of the NLP field. The study is centered on LREC papers in both directions, first with an LREC paper borrowing a fragment of text from the collection, and secondly in the reverse direction with fragments of LREC documents borrowed and inserted in the collection.
Motivated by the adage that a {``}picture is worth a thousand words{''} it can be reasoned that automatically enriching the textual content of a document with relevant images can increase the readability of a document. Moreover, features extracted from the additional image data inserted into the textual content of a document may, in principle, be also be used by a retrieval engine to better match the topic of a document with that of a given query. In this paper, we describe our approach of building a ground truth dataset to enable further research into automatic addition of relevant images to text documents. The dataset is comprised of the official ImageCLEF 2010 collection (a collection of images with textual metadata) to serve as the images available for automatic enrichment of text, a set of 25 benchmark documents that are to be enriched, which in this case are children{'}s short stories, and a set of manually judged relevant images for each query story obtained by the standard procedure of depth pooling. We use this benchmark dataset to evaluate the effectiveness of standard information retrieval methods as simple baselines for this task. The results indicate that using the whole story as a weighted query, where the weight of each query term is its tf-idf value, achieves an precision of 0:1714 within the top 5 retrieved images on an average.
Text analysis methods widely used in digital humanities often involve word co-occurrence, e.g. concept co-occurrence networks. These methods provide a useful corpus overview, but cannot determine the predicates that relate co-occurring concepts. Our goal was identifying propositions expressing the points supported or opposed by participants in international climate negotiations. Word co-occurrence methods were not sufficient, and an analysis based on open relation extraction had limited coverage for nominal predicates. We present a pipeline which identifies the points that different actors support and oppose, via a domain model with support/opposition predicates, and analysis rules that exploit the output of semantic role labelling, syntactic dependencies and anaphora resolution. Entity linking and keyphrase extraction are also performed on the propositions related to each actor. A user interface allows examining the main concepts in points supported or opposed by each participant, which participants agree or disagree with each other, and about which issues. The system is an example of tools that digital humanities scholars are asking for, to render rich textual information (beyond word co-occurrence) more amenable to quantitative treatment. An evaluation of the tool was satisfactory.
The task of Relation Extraction from texts is one of the main challenges in the area of Information Extraction, considering the required linguistic knowledge and the sophistication of the language processing techniques employed. This task aims at identifying and classifying semantic relations that occur between entities recognized in a given text. In this paper, we evaluated a Conditional Random Fields classifier for the extraction of any relation descriptor occurring between named entities (Organisation, Person and Place categories), as well as pre-defined relation types between these entities in Portuguese texts.
This work proposes an information retrieval evaluation set for the Slovak language. A set of 80 queries written in the natural language is given together with the set of relevant documents. The document set contains 3980 newspaper articles sorted into 6 categories. Each document in the result set is manually annotated for relevancy with its corresponding query. The evaluation set is mostly compatible with the Cranfield test collection using the same methodology for queries and annotation of relevancy. In addition to that it provides annotation for document title, author, publication date and category that can be used for evaluation of automatic document clustering and categorization.
This paper proposes how to utilize a search engine in order to predict market shares. We propose to compare rates of concerns of those who search for Web pages among several companies which supply products, given a specific products domain. We measure concerns of those who search for Web pages through search engine suggests. Then, we analyze whether rates of concerns of those who search for Web pages have certain correlation with actual market share. We show that those statistics have certain correlations. We finally propose how to predict the market share of a specific product genre based on the rates of concerns of those who search for Web pages.
Keyphrase extraction is the task of finding phrases that represent the important content of a document. The main aim of keyphrase extraction is to propose textual units that represent the most important topics developed in a document. The output keyphrases of automatic keyphrase extraction methods for test documents are typically evaluated by comparing them to manually assigned reference keyphrases. Each output keyphrase is considered correct if it matches one of the reference keyphrases. However, the choice of the appropriate textual unit (keyphrase) for a topic is sometimes subjective and evaluating by exact matching underestimates the performance. This paper presents a dataset of evaluation scores assigned to automatically extracted keyphrases by human evaluators. Along with the reference keyphrases, the manual evaluations can be used to validate new evaluation measures. Indeed, an evaluation measure that is highly correlated to the manual evaluation is appropriate for the evaluation of automatic keyphrase extraction methods.
We present the Royal Society Corpus (RSC) built from the Philosophical Transactions and Proceedings of the Royal Society of London. At present, the corpus contains articles from the first two centuries of the journal (1665‚Äï1869) and amounts to around 35 million tokens. The motivation for building the RSC is to investigate the diachronic linguistic development of scientific English. Specifically, we assume that due to specialization, linguistic encodings become more compact over time (Halliday, 1988; Halliday and Martin, 1993), thus creating a specific discourse type characterized by high information density that is functional for expert communication. When building corpora from uncharted material, typically not all relevant meta-data (e.g. author, time, genre) or linguistic data (e.g. sentence/word boundaries, words, parts of speech) is readily available. We present an approach to obtain good quality meta-data and base text data adopting the concept of Agile Software Development.
Common people often experience difficulties in accessing relevant, correct, accurate and understandable health information online. Developing search techniques that aid these information needs is challenging. In this paper we present the datasets created by CLEF eHealth Lab from 2013-2015 for evaluation of search solutions to support common people finding health information online. Specifically, the CLEF eHealth information retrieval (IR) task of this Lab has provided the research community with benchmarks for evaluating consumer-centered health information retrieval, thus fostering research and development aimed to address this challenging problem. Given consumer queries, the goal of the task is to retrieve relevant documents from the provided collection of web pages. The shared datasets provide a large health web crawl, queries representing people{'}s real world information needs, and relevance assessment judgements for the queries.
This article presents a corpus for development and testing of event schema induction systems in English. Schema induction is the task of learning templates with no supervision from unlabeled texts, and to group together entities corresponding to the same role in a template. Most of the previous work on this subject relies on the MUC-4 corpus. We describe the limits of using this corpus (size, non-representativeness, similarity of roles across templates) and propose a new, partially-annotated corpus in English which remedies some of these shortcomings. We make use of Wikinews to select the data inside the category Laws {\&} Justice, and query Google search engine to retrieve different documents on the same events. Only Wikinews documents are manually annotated and can be used for evaluation, while the others can be used for unsupervised learning. We detail the methodology used for building the corpus and evaluate some existing systems on this new data.
The current study focuses on optimization of Levenshtein algorithm for the purpose of computing the optimal alignment between two phoneme transcriptions of spoken utterance containing sequences of phonetic symbols. The alignment is computed with the help of a confusion matrix in which costs for phonetic symbol deletion, insertion and substitution are defined taking into account various phonological processes that occur in fluent speech, such as anticipatory assimilation, phone elision and epenthesis. The corpus containing about 30 hours of Russian read speech was used to evaluate the presented algorithms. The experimental results have shown significant reduction of misalignment rate in comparison with the baseline Levenshtein algorithm: the number of errors has been reduced from 1.1 {\%} to 0.28 {\%}
This paper describes speech data recording, processing and annotation of a new speech corpus CoRuSS (Corpus of Russian Spontaneous Speech), which is based on connected communicative speech recorded from 60 native Russian male and female speakers of different age groups (from 16 to 77). Some Russian speech corpora available at the moment contain plain orthographic texts and provide some kind of limited annotation, but there are no corpora providing detailed prosodic annotation of spontaneous conversational speech. This corpus contains 30 hours of high quality recorded spontaneous Russian speech, half of it has been transcribed and prosodically labeled. The recordings consist of dialogues between two speakers, monologues (speakers{'} self-presentations) and reading of a short phonetically balanced text. Since the corpus is labeled for a wide range of linguistic - phonetic and prosodic - information, it provides basis for empirical studies of various spontaneous speech phenomena as well as for comparison with those we observe in prepared read speech. Since the corpus is designed as a open-access resource of speech data, it will also make possible to advance corpus-based analysis of spontaneous speech data across languages and speech technology development as well.
Recently, there has been an explosion in the availability of large, good-quality cross-linguistic databases such as WALS (Dryer {\&} Haspelmath, 2013), Glottolog (Hammarstrom et al., 2015) and Phoible (Moran {\&} McCloy, 2014). Databases such as Phoible contain the actual segments used by various languages as they are given in the primary language descriptions. However, this segment-level representation cannot be used directly for analyses that require generalizations over classes of segments that share theoretically interesting features. Here we present a method and the associated R (R Core Team, 2014) code that allows the flexible definition of such meaningful classes and that can identify the sets of segments falling into such a class for any language inventory. The method and its results are important for those interested in exploring cross-linguistic patterns of phonetic and phonological diversity and their relationship to extra-linguistic factors and processes such as climate, economics, history or human genetics.
SEA{\_}AP (Segmentador e Etiquetador Autom{\'a}tico para An{\'a}lise Pros{\'o}dica, Automatic Segmentation and Labelling for Prosodic Analysis) toolkit is an application that performs audio segmentation and labelling to create a TextGrid file which will be used to launch a prosodic analysis using Praat. In this paper, we want to describe the improved functionality of the tool achieved by adding a dialectometric analysis module using R scripts. The dialectometric analysis includes computing correlations among F0 curves and it obtains prosodic distances among the different variables of interest (location, speaker, structure, etc.). The dialectometric analysis requires large databases in order to be adequately computed, and automatic segmentation and labelling can create them thanks to a procedure less costly than the manual alternative. Thus, the integration of these tools into the SEA{\_}AP allows to propose a distribution of geoprosodic areas by means of a quantitative method, which completes the traditional dialectological point of view. The current version of the SEA{\_}AP toolkit is capable of analysing Galician, Spanish and Brazilian Portuguese data, and hence the distances between several prosodic linguistic varieties can be measured at present.
In this paper, we present a music retrieval and recommendation system using machine learning techniques. We propose a query by humming system for music retrieval that uses deep neural networks for note transcription and a note-based retrieval system for retrieving the correct song from the database. We evaluate our query by humming system using the standard MIREX QBSH dataset. We also propose a similar artist recommendation system which recommends similar artists based on acoustic features of the artists{'} music, online text descriptions of the artists and social media data. We use supervised machine learning techniques over all our features and compare our recommendation results to those produced by a popular similar artist recommendation website.
Vocal User Interfaces in domestic environments recently gained interest in the speech processing community. This interest is due to the opportunity of using it in the framework of Ambient Assisted Living both for home automation (vocal command) and for call for help in case of distress situations, i.e. after a fall. C IRDO X, which is a modular software, is able to analyse online the audio environment in a home, to extract the uttered sentences and then to process them thanks to an ASR module. Moreover, this system perfoms non-speech audio event classification; in this case, specific models must be trained. The software is designed to be modular and to process on-line the audio multichannel stream. Some exemples of studies in which C IRDO X was involved are described. They were operated in real environment, namely a Living lab environment.
Computer-assisted transcription promises high-quality speech transcription at reduced costs. This is achieved by limiting human effort to transcribing parts for which automatic transcription quality is insufficient. Our goal is to improve the human transcription quality via appropriate user interface design. We focus on iterative interfaces that allow humans to solve tasks based on an initially given suggestion, in this case an automatic transcription. We conduct a user study that reveals considerable quality gains for three variations of iterative interfaces over a non-iterative from-scratch transcription interface. Our iterative interfaces included post-editing, confidence-enhanced post-editing, and a novel retyping interface. All three yielded similar quality on average, but we found that the proposed retyping interface was less sensitive to the difficulty of the segment, and superior when the automatic transcription of the segment contained relatively many errors. An analysis using mixed-effects models allows us to quantify these and other factors and draw conclusions over which interface design should be chosen in which circumstance.
This paper introduces a new British English speech database, named the homeService corpus, which has been gathered as part of the homeService project. This project aims to help users with speech and motor disabilities to operate their home appliances using voice commands. The audio recorded during such interactions consists of realistic data of speakers with severe dysarthria. The majority of the homeService corpus is recorded in real home environments where voice control is often the normal means by which users interact with their devices. The collection of the corpus is motivated by the shortage of realistic dysarthric speech corpora available to the scientific community. Along with the details on how the data is organised and how it can be accessed, a brief description of the framework used to make the recordings is provided. Finally, the performance of the homeService automatic recogniser for dysarthric speech trained with single-speaker data from the corpus is provided as an initial baseline. Access to the homeService corpus is provided through the dedicated web page at http://mini.dcs.shef.ac.uk/resources/homeservice-corpus/. This will also have the most updated description of the data. At the time of writing the collection process is still ongoing.
Perceptive evaluation of speech disorders is still the standard method in clinical practice for the diagnosing and the following of the condition progression of patients. Such methods include different tasks such as read speech, spontaneous speech, isolated words, sustained vowels, etc. In this context, automatic speech processing tools have proven pertinence in speech quality evaluation and assistive technology-based applications. Though, a very few studies have investigated the use of automatic tools on spontaneous speech. This paper investigates the behavior of an automatic phone-based anomaly detection system when applied on read and spontaneous French dysarthric speech. The behavior of the automatic tool reveals interesting inter-pathology differences across speech styles.
We present a text-to-speech (TTS) system designed for the dialect of Bengali spoken in Bangladesh. This work is part of an ongoing effort to address the needs of under-resourced languages. We propose a process for streamlining the bootstrapping of TTS systems for under-resourced languages. First, we use crowdsourcing to collect the data from multiple ordinary speakers, each speaker recording small amount of sentences. Second, we leverage an existing text normalization system for a related language (Hindi) to bootstrap a linguistic front-end for Bangla. Third, we employ statistical techniques to construct multi-speaker acoustic models using Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) and Hidden Markov Model (HMM) approaches. We then describe our experiments that show that the resulting TTS voices score well in terms of their perceived quality as measured by Mean Opinion Score (MOS) evaluations.
With the increasing amount of audiovisual and digital data deriving from televisual and radiophonic sources, professional archives such as INA, France{'}s national audiovisual institute, acknowledge a growing need for efficient indexing tools. In this paper, we describe the Speech Trax system that aims at analyzing the audio content of TV and radio documents. In particular, we focus on the speaker tracking task that is very valuable for indexing purposes. First, we detail the overall architecture of the system and show the results obtained on a large-scale experiment, the largest to our knowledge for this type of content (about 1,300 speakers). Then, we present the Speech Trax demonstrator that gathers the results of various automatic speech processing techniques on top of our speaker tracking system (speaker diarization, speech transcription, etc.). Finally, we provide insight on the obtained performances and suggest hints for future improvements.
In this article we propose a descriptive study of a chat conversations corpus from an assistance contact center. Conversations are described from several view points, including interaction analysis, language deviation analysis and typographic expressivity marks analysis. We provide in particular a detailed analysis of language deviations that are encountered in our corpus of 230 conversations, corresponding to 6879 messages and 76839 words. These deviations may be challenging for further syntactic and semantic parsing. Analysis is performed with a distinction between Customer messages and Agent messages. On the overall only 4{\%} of the observed words are misspelled but 26{\%} of the messages contain at least one erroneous word (rising to 40{\%} when focused on Customer messages). Transcriptions of telephone conversations from an assistance call center are also studied, allowing comparisons between these two interaction modes to be drawn. The study reveals significant differences in terms of conversation flow, with an increased efficiency for chat conversations in spite of longer temporal span.
Monitoring social media has been shown to be an interesting approach for the early detection of drug adverse effects. In this paper, we describe a system which extracts medical entities in French drug reviews written by users. We focus on the identification of medical conditions, which is based on the concept of post-coordination: we first extract minimal medical-related entities (pain, stomach) then we combine them to identify complex ones (It was the worst [pain I ever felt in my stomach]). These two steps are respectively performed by two classifiers, the first being based on Conditional Random Fields and the second one on Support Vector Machines. The overall results of the minimal entity classifier are the following: P=0.926; R=0.849; F1=0.886. A thourough analysis of the feature set shows that, when combined with word lemmas, clusters generated by word2vec are the most valuable features. When trained on the output of the first classifier, the second classifier{'}s performances are the following: p=0.683;r=0.956;f1=0.797. The addition of post-processing rules did not add any significant global improvement but was found to modify the precision/recall ratio.
Data acquisition in dialectology is typically a tedious task, as dialect samples of spoken language have to be collected via questionnaires or interviews. In this article, we suggest to use the {``}web as a corpus{''} approach for dialectology. We present a case study that demonstrates how authentic language data for the Bavarian dialect (ISO 639-3:bar) can be collected automatically from the social network Facebook. We also show that Facebook can be used effectively as a crowdsourcing platform, where users are willing to translate dialect words collaboratively in order to create a common lexicon of their Bavarian dialect. Key insights from the case study are summarized as {``}lessons learned{''}, together with suggestions for future enhancements of the lexicon creation approach.
In order to gain a deep understanding of how social context manifests in interactions, we need data that represents interactions from a large community of people over a long period of time, capturing different aspects of social context. In this paper, we present a large corpus of Wikipedia Talk page discussions that are collected from a broad range of topics, containing discussions that happened over a period of 15 years. The dataset contains 166,322 discussion threads, across 1236 articles/topics that span 15 different topic categories or domains. The dataset also captures whether the post is made by an registered user or not, and whether he/she was an administrator at the time of making the post. It also captures the Wikipedia age of editors in terms of number of months spent as an editor, as well as their gender. This corpus will be a valuable resource to investigate a variety of computational sociolinguistics research questions regarding online social interactions.
Natural Language Engineering tasks require large and complex annotated datasets to build more advanced models of language. Corpora are typically annotated by several experts to create a gold standard; however, there are now compelling reasons to use a non-expert crowd to annotate text, driven by cost, speed and scalability. Phrase Detectives Corpus 1.0 is an anaphorically-annotated corpus of encyclopedic and narrative text that contains a gold standard created by multiple experts, as well as a set of annotations created by a large non-expert crowd. Analysis shows very good inter-expert agreement (kappa=.88-.93) but a more variable baseline crowd agreement (kappa=.52-.96). Encyclopedic texts show less agreement (and by implication are harder to annotate) than narrative texts. The release of this corpus is intended to encourage research into the use of crowds for text annotation and the development of more advanced, probabilistic language models, in particular for anaphoric coreference.
Despite the popularity of coreference resolution as a research topic, the overwhelming majority of the work in this area focused so far on single antecedence coreference only. Multiple antecedent coreference (MAC) has been largely neglected. This can be explained by the scarcity of the phenomenon of MAC in generic discourse. However, in specialized discourse such as patents, MAC is very dominant. It seems thus unavoidable to address the problem of MAC resolution in the context of tasks related to automatic patent material processing, among them abstractive summarization, deep parsing of patents, construction of concept maps of the inventions, etc. We present the first version of an operational rule-based MAC resolution strategy for patent material that covers the three major types of MAC: (i) nominal MAC, (ii) MAC with personal / relative pronouns, and MAC with reflexive / reciprocal pronouns. The evaluation shows that our strategy performs well in terms of precision and recall.
This paper presents a second release of the ARRAU dataset: a multi-domain corpus with thorough linguistically motivated annotation of anaphora and related phenomena. Building upon the first release almost a decade ago, a considerable effort had been invested in improving the data both quantitatively and qualitatively. Thus, we have doubled the corpus size, expanded the selection of covered phenomena to include referentiality and genericity and designed and implemented a methodology for enforcing the consistency of the manual annotation. We believe that the new release of ARRAU provides a valuable material for ongoing research in complex cases of coreference as well as for a variety of related tasks. The corpus is publicly available through LDC.
We describe a method for identifying and performing functional analysis of structured regions that are embedded in natural language documents, such as tables or key-value lists. Such regions often encode information according to ad hoc schemas and avail themselves of visual cues in place of natural language grammar, presenting problems for standard information extraction algorithms. Unlike previous work in table extraction, which assumes a relatively noiseless two-dimensional layout, our aim is to accommodate a wide variety of naturally occurring structure types. Our approach has three main parts. First, we collect and annotate a a diverse sample of {``}naturally{''} occurring structures from several sources. Second, we use probabilistic text segmentation techniques, featurized by skip bigrams over spatial and token category cues, to automatically identify contiguous regions of structured text that share a common schema. Finally, we identify the records and fields within each structured region using a combination of distributional similarity and sequence alignment methods, guided by minimal supervision in the form of a single annotated record. We evaluate the last two components individually, and conclude with a discussion of further work.
In distributional semantics words are represented by aggregated context features. The similarity of words can be computed by comparing their feature vectors. Thus, we can predict whether two words are synonymous or similar with respect to some other semantic relation. We will show on six different datasets of pairs of similar and non-similar words that a supervised learning algorithm on feature vectors representing pairs of words outperforms cosine similarity between vectors representing single words. We compared different methods to construct a feature vector representing a pair of words. We show that simple methods like pairwise addition or multiplication give better results than a recently proposed method that combines different types of features. The semantic relation we consider is relatedness of terms in thesauri for intellectual document classification. Thus our findings can directly be applied for the maintenance and extension of such thesauri. To the best of our knowledge this relation was not considered before in the field of distributional semantics.
We present a proposal for the annotation of factuality of event mentions in Spanish texts and a free available annotated corpus. Our factuality model aims to capture a pragmatic notion of factuality, trying to reflect a casual reader judgements about the realis / irrealis status of mentioned events. Also, some learning experiments (SVM and CRF) have been held, showing encouraging results.
Lately, with the success of Deep Learning techniques in some computational linguistics tasks, many researchers want to explore new models for their linguistics applications. These models tend to be very different from what standard Neural Networks look like, limiting the possibility to use standard Neural Networks frameworks. This work presents NNBlocks, a new framework written in Python to build and train Neural Networks that are not constrained by a specific kind of architecture, making it possible to use it in computational linguistics.
This paper presents some preliminary results of the OPLON project. It aimed at identifying early linguistic symptoms of cognitive decline in the elderly. This pilot study was conducted on a corpus composed of spontaneous speech sample collected from 39 subjects, who underwent a neuropsychological screening for visuo-spatial abilities, memory, language, executive functions and attention. A rich set of linguistic features was extracted from the digitalised utterances (at phonetic, suprasegmental, lexical, morphological and syntactic levels) and the statistical significance in pinpointing the pathological process was measured. Our results show remarkable trends for what concerns both the linguistic traits selection and the automatic classifiers building.
This paper describes the recording of a speech corpus focused on prosody of people with intellectual disabilities. To do this, a video game is used with the aim of improving the user{'}s motivation. Moreover, the player{'}s profiles and the sentences recorded during the game sessions are described. With the purpose of identifying the main prosodic troubles of people with intellectual disabilities, some prosodic features are extracted from recordings, like fundamental frequency, energy and pauses. After that, a comparison is made between the recordings of people with intellectual disabilities and people without intellectual disabilities. This comparison shows that pauses are the best discriminative feature between these groups. To check this, a study has been done using machine learning techniques, with a classification rate superior to 80{\%}.
Autism spectrum disorder (ASD) is a complex neurodevelopmental condition that would benefit from low-cost and reliable improvements to screening and diagnosis. Human language technologies (HLTs) provide one possible route to automating a series of subjective decisions that currently inform {``}Gold Standard{''} diagnosis based on clinical judgment. In this paper, we describe a new resource to support this goal, comprised of 100 20-minute semi-structured English language samples labeled with child age, sex, IQ, autism symptom severity, and diagnostic classification. We assess the feasibility of digitizing and processing sensitive clinical samples for data sharing, and identify areas of difficulty. Using the methods described here, we propose to join forces with researchers and clinicians throughout the world to establish an international repository of annotated language samples from individuals with ASD and related disorders. This project has the potential to improve the lives of individuals with ASD and their families by identifying linguistic features that could improve remote screening, inform personalized intervention, and promote advancements in clinically-oriented HLTs.
We propose in this work a novel acoustic phonetic study for Arabic people suffering from language disabilities and non-native learners of Arabic language to classify Arabic continuous speech to pathological or healthy and to identify phonemes that pose pronunciation problems (case of pathological speeches). The main idea can be summarized in comparing between the phonetic model reference to Arabic spoken language and that proper to concerned speaker. For this task, we use techniques of automatic speech processing like forced alignment and artificial neural network (ANN) (Basheer, 2000). Based on a test corpus containing 100 speech sequences, recorded by different speakers (healthy/pathological speeches and native/foreign speakers), we attain 97{\%} as classification rate. Algorithms used in identifying phonemes that pose pronunciation problems show high efficiency: we attain an identification rate of 100{\%}.
In this paper, we investigate a covert labeling cue, namely the probability that a title (by example of the Wikipedia titles) is a noun. If this probability is very large, any list such as or comparable to the Wikipedia titles can be used as a reliable word-class (or part-of-speech tag) predictor or noun lexicon. This may be especially useful in the case of Low Resource Languages (LRL) where labeled data is lacking and putatively for Natural Language Processing (NLP) tasks such as Word Sense Disambiguation, Sentiment Analysis and Machine Translation. Profitting from the ease of digital publication on the web as opposed to print, LRL speaker communities produce resources such as Wikipedia and Wiktionary, which can be used for an assessment. We provide statistical evidence for a strong noun bias for the Wikipedia titles from 2 corpora (English, Persian) and a dictionary (Japanese) and for a typologically balanced set of 17 languages including LRLs. Additionally, we conduct a small experiment on predicting noun tags for out-of-vocabulary items in part-of-speech tagging for English.
Although some words carry strong associations with specific colors (e.g., the word danger is associated with the color red), few studies have investigated these relationships. This may be due to the relative rarity of databases that contain large quantities of such information. Additionally, these resources are often limited to particular languages, such as English. Moreover, the existing resources often do not consider the possible contexts of words in assessing the associations between a word and a color. As a result, the influence of context on word‚Äïcolor associations is not fully understood. In this study, we constructed a novel language resource for word‚Äïcolor associations. The resource has two characteristics: First, our resource is the first to include Japanese word‚Äïcolor associations, which were collected via crowdsourcing. Second, the word‚Äïcolor associations in the resource are linked to contexts. We show that word‚Äïcolor associations depend on language and that associations with certain colors are affected by context information.
This paper presents a collection of annotations (tags or keywords) for a set of 2,133 environmental sounds taken from the Freesound database (www.freesound.org). The annotations are acquired through an open-ended crowd-labeling task, in which participants were asked to provide keywords for each of three sounds. The main goal of this study is to find out (i) whether it is feasible to collect keywords for a large collection of sounds through crowdsourcing, and (ii) how people talk about sounds, and what information they can infer from hearing a sound in isolation. Our main finding is that it is not only feasible to perform crowd-labeling for a large collection of sounds, it is also very useful to highlight different aspects of the sounds that authors may fail to mention. Our data is freely available, and can be used to ground semantic models, improve search in audio databases, and to study the language of sound.
We present a new large dataset of 12403 context-sensitive verb relations manually annotated via crowdsourcing. These relations capture fine-grained semantic information between verb-centric propositions, such as temporal or entailment relations. We propose a novel semantic verb relation scheme and design a multi-step annotation approach for scaling-up the annotations using crowdsourcing. We employ several quality measures and report on agreement scores. The resulting dataset is available under a permissive CreativeCommons license at www.ukp.tu-darmstadt.de/data/verb-relations/. It represents a valuable resource for various applications, such as automatic information consolidation or automatic summarization.
We describe an experiment for the acquisition of opposition relations among Italian verb senses, based on a crowdsourcing methodology. The goal of the experiment is to discuss whether the types of opposition we distinguish (i.e. complementarity, antonymy, converseness and reversiveness) are actually perceived by the crowd. In particular, we collect data for Italian by using the crowdsourcing platform CrowdFlower. We ask annotators to judge the type of opposition existing among pairs of sentences -previously judged as opposite- that differ only for a verb: the verb in the first sentence is opposite of the verb in second sentence. Data corroborate the hypothesis that some opposition relations exclude each other, while others interact, being recognized as compatible by the contributors.
We announce the release of the CROWDED CORPUS: a pair of speech corpora collected via crowdsourcing, containing a native speaker corpus of English (CROWDED{\_}ENGLISH), and a corpus of German/English bilinguals (CROWDED{\_}BILINGUAL). Release 1 of the CROWDED CORPUS contains 1000 recordings amounting to 33,400 tokens collected from 80 speakers and is freely available to other researchers. We recruited participants via the Crowdee application for Android. Recruits were prompted to respond to business-topic questions of the type found in language learning oral tests. We then used the CrowdFlower web application to pass these recordings to crowdworkers for transcription and annotation of errors and sentence boundaries. Finally, the sentences were tagged and parsed using standard natural language processing tools. We propose that crowdsourcing is a valid and economical method for corpus collection, and discuss the advantages and disadvantages of this approach.
Our interest is in people{'}s capacity to efficiently and effectively describe geographic objects in urban scenes. The broader ambition is to develop spatial models capable of equivalent functionality able to construct such referring expressions. To that end we present a newly crowd-sourced data set of natural language references to objects anchored in complex urban scenes (In short: The REAL Corpus ‚Äï Referring Expressions Anchored Language). The REAL corpus contains a collection of images of real-world urban scenes together with verbal descriptions of target objects generated by humans, paired with data on how successful other people were able to identify the same object based on these descriptions. In total, the corpus contains 32 images with on average 27 descriptions per image and 3 verifications for each description. In addition, the corpus is annotated with a variety of linguistically motivated features. The paper highlights issues posed by collecting data using crowd-sourcing with an unrestricted input format, as well as using real-world urban scenes.
Crowdsourcing is an arising collaborative approach applicable among many other applications to the area of language and speech processing. In fact, the use of crowdsourcing was already applied in the field of speech processing with promising results. However, only few studies investigated the use of crowdsourcing in computational paralinguistics. In this contribution, we propose a novel evaluator for crowdsourced-based ratings termed Weighted Trustability Evaluator (WTE) which is computed from the rater-dependent consistency over the test questions. We further investigate the reliability of crowdsourced annotations as compared to the ones obtained with traditional labelling procedures, such as constrained listening experiments in laboratories or in controlled environments. This comparison includes an in-depth analysis of obtainable classification performances. The experiments were conducted on the Speaker Likability Database (SLD) already used in the INTERSPEECH Challenge 2012, and the results lend further weight to the assumption that crowdsourcing can be applied as a reliable annotation source for computational paralinguistics given a sufficient number of raters and suited measurements of their reliability.
In online computer-mediated communication, speakers were considered to have experienced difficulties in catching their partner{'}s emotions and in conveying their own emotions. To explain why online emotional communication is so difficult and to investigate how this problem should be solved, multimodal online emotional communication corpus was constructed by recording approximately 100 speakers{'} emotional expressions and reactions in a modality-controlled environment. Speakers communicated over the Internet using video chat, voice chat or text chat; their face-to-face conversations were used for comparison purposes. The corpora incorporated emotional labels by evaluating the speaker{'}s dynamic emotional states and the measurements of the speaker{'}s facial expression, vocal expression and autonomic nervous system activity. For the initial study of this project, which used a large-scale emotional communication corpus, the accuracy of online emotional understanding was assessed to demonstrate the emotional labels evaluated by the speakers and to summarize the speaker{'}s answers on the questionnaire regarding the difference between an online chat and face-to-face conversations in which they actually participated. The results revealed that speakers have difficulty communicating their emotions in online communication environments, regardless of the type of communication modality and that inaccurate emotional understanding occurs more frequently in online computer-mediated communication than in face-to-face communication.
This paper presents a quantitative description of laughter in height 1-hour French spontaneous conversations. The paper includes the raw figures for laughter as well as more details concerning inter-individual variability. It firstly describes to what extent the amount of laughter and their durations varies from speaker to speaker in all dialogs. In a second suite of analyses, this paper compares our corpus with previous analyzed corpora. In a final set of experiments, it presents some facts about overlapping laughs. This paper have quantified these all effects in free-style conversations, for the first time.
It has been shown that adding expressivity and emotional expressions to an agent{'}s communication systems would improve the interaction quality between this agent and a human user. In this paper we present a multimodal database of affect bursts, which are very short non-verbal expressions with facial, vocal, and gestural components that are highly synchronized and triggered by an identifiable event. This database contains motion capture and audio data of affect bursts representing disgust, startle and surprise recorded at three different levels of arousal each. This database is to be used for synthesis purposes in order to generate affect bursts of these emotions on a continuous arousal level scale.
Emotional aspects play a vital role in making human communication a rich and dynamic experience. As we introduce more automated system in our daily lives, it becomes increasingly important to incorporate emotion to provide as natural an interaction as possible. To achieve said incorporation, rich sets of labeled emotional data is prerequisite. However, in Japanese, existing emotion database is still limited to unimodal and bimodal corpora. Since emotion is not only expressed through speech, but also visually at the same time, it is essential to include multiple modalities in an observation. In this paper, we present the first audio-visual emotion corpora in Japanese, collected from 14 native speakers. The corpus contains 100 minutes of annotated and transcribed material. We performed preliminary emotion recognition experiments on the corpus and achieved an accuracy of 61.42{\%} for five classes of emotion.
In this paper we compare different context selection approaches to improve the creation of Emotive Vector Space Models (VSMs). The system is based on the results of an existing approach that showed the possibility to create and update VSMs by exploiting crowdsourcing and human annotation. Here, we introduce a method to manipulate the contexts of the VSMs under the assumption that the emotive connotation of a target word is a function of both its syntagmatic and paradigmatic association with the various emotions. To study the differences among the proposed spaces and to confirm the reliability of the system, we report on two experiments: in the first one we validated the best candidates extracted from each model, and in the second one we compared the models{'} performance on a random sample of target words. Both experiments have been implemented as crowdsourcing tasks.
Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair. This paper describes a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy on languages like Chinese with uncertain sentence boundaries. It relies on the definition of hard (certain) and soft (uncertain) punctuation delimiters, the latter being possibly ignored to optimize the alignment result. The alignment method is used in combination with lexicons automatically generated from the input article pairs using pivot-based MT, achieving better coverage of the input words with fewer entries than pre-existing dictionaries. Pivot-based MT makes it possible to build dictionaries for language pairs that have scarce parallel data. The alignment method is implemented in a tool that will be freely available in the near future.
Parallel corpora are often injected with bilingual lexical resources for improved Indian language machine translation (MT). In absence of such lexical resources, multilingual topic models have been used to create coarse lexical resources in the past, using a Cartesian product approach. Our results show that for morphologically rich languages like Hindi, the Cartesian product approach is detrimental for MT. We then present a novel {`}sentential{'} approach to use this coarse lexical resource from a multilingual topic model. Our coarse lexical resource when injected with a parallel corpus outperforms a system trained using parallel corpus and a good quality lexical resource. As demonstrated by the quality of our coarse lexical resource and its benefit to MT, we believe that our sentential approach to create such a resource will help MT for resource-constrained languages.
In this paper, we describe the details of the ASPEC (Asian Scientific Paper Excerpt Corpus), which is the first large-size parallel corpus of scientific paper domain. ASPEC was constructed in the Japanese-Chinese machine translation project conducted between 2006 and 2010 using the Special Coordination Funds for Promoting Science and Technology. It consists of a Japanese-English scientific paper abstract corpus of approximately 3 million parallel sentences (ASPEC-JE) and a Chinese-Japanese scientific paper excerpt corpus of approximately 0.68 million parallel sentences (ASPEC-JC). ASPEC is used as the official dataset for the machine translation evaluation workshop WAT (Workshop on Asian Translation).
This paper presents how an state-of-the-art SMT system is enriched by using an extra in-domain parallel corpora extracted from Wikipedia. We collect corpora from parallel titles and from parallel fragments in comparable articles from Wikipedia. We carried out an evaluation with a double objective: evaluating the quality of the extracted data and evaluating the improvement due to the domain-adaptation. We think this can be very useful for languages with limited amount of parallel corpora, where in-domain data is crucial to improve the performance of MT sytems. The experiments on the Spanish-English language pair improve a baseline trained with the Europarl corpus in more than 2 points of BLEU when translating in the Computer Science domain.
This paper presents ProphetMT, a tree-based SMT-driven Controlled Language (CL) authoring and post-editing tool. ProphetMT employs the source-side rules in a translation model and provides them as auto-suggestions to users. Accordingly, one might say that users are writing in a Controlled Language that is understood by the computer. ProphetMT also allows users to easily attach structural information as they compose content. When a specific rule is selected, a partial translation is promptly generated on-the-fly with the help of the structural information. Our experiments conducted on English-to-Chinese show that our proposed ProphetMT system can not only better regularise an author{'}s writing behaviour, but also significantly improve translation fluency which is vital to reduce the post-editing time. Additionally, when the writing and translation process is over, ProphetMT can provide an effective colour scheme to further improve the productivity of post-editors by explicitly featuring the relations between the source and target rules.
Bilingual lexica are the basis for many cross-lingual natural language processing tasks. Recent works have shown success in learning bilingual dictionary by taking advantages of comparable corpora and a diverse set of signals derived from monolingual corpora. In the present work, we describe an approach to automatically learn bilingual lexica by training a supervised classifier using word embedding-based vectors of only a few hundred translation equivalent word pairs. The word embedding representations of translation pairs were obtained from source and target monolingual corpora, which are not necessarily related. Our classifier is able to predict whether a new word pair is under a translation relation or not. We tested it on two quite distinct language pairs Chinese-Spanish and English-Spanish. The classifiers achieved more than 0.90 precision and recall for both language pairs in different evaluation scenarios. These results show a high potential for this method to be used in bilingual lexica production for language pairs with reduced amount of parallel or comparable corpora, in particular for phrase table expansion in Statistical Machine Translation systems.
In this paper, we introduce a coverage-based scoring function that discriminates between parallel and non-parallel sentences. When plugged into Bleualign, a state-of-the-art sentence aligner, our function improves both precision and recall of alignments over the originally proposed BLEU score. Furthermore, since our scoring function uses Moses phrase tables directly we avoid the need to translate the texts to be aligned, which is time-consuming and a potential source of alignment errors.
This paper presents a solution to evaluate spoken post-editing of imperfect machine translation output by a human translator. We compare two approaches to the combination of machine translation (MT) and automatic speech recognition (ASR): a heuristic algorithm and a machine learning method. To obtain a data set with spoken post-editing information, we use the French version of TED talks as the source texts submitted to MT, and the spoken English counterparts as their corrections, which are submitted to an ASR system. We experiment with various levels of artificial ASR noise and also with a state-of-the-art ASR system. The results show that the combination of MT with ASR improves over both individual outputs of MT and ASR in terms of BLEU scores, especially when ASR performance is low.
This paper presents our work towards a novel approach for Quality Estimation (QE) of machine translation based on sequences of adjacent words, the so-called phrases. This new level of QE aims to provide a natural balance between QE at word and sentence-level, which are either too fine grained or too coarse levels for some applications. However, phrase-level QE implies an intrinsic challenge: how to segment a machine translation into sequence of words (contiguous or not) that represent an error. We discuss three possible segmentation strategies to automatically extract erroneous phrases. We evaluate these strategies against annotations at phrase-level produced by humans, using a new dataset collected for this purpose.
In this paper, we present a freely available corpus of human and automatic translations of subtitles. The corpus comprises, the original English subtitles (SRC), both human (HT) and machine translations (MT) into German, as well as post-editions (PE) of the MT output. HT and MT are annotated with errors. Moreover, human evaluation is included in HT, MT, and PE. Such a corpus is a valuable resource for both human and machine translation communities, enabling the direct comparison {--} in terms of errors and evaluation {--} between human and machine translations and post-edited machine translations.
This paper describes a pilot study in lexical encoding of multi-word expressions (MWEs) in 4 Latin American dialects of Spanish: Costa Rican, Colombian, Mexican and Peruvian. We describe the variability of MWE usage across dialects. We adapt an existing data model to a dialect-aware encoding, so as to represent dialect-related specificities, while avoiding redundancy of the data common for all dialects. A dozen of linguistic properties of MWEs can be expressed in this model, both on the level of a whole MWE and of its individual components. We describe the resulting lexical resource containing several dozens of MWEs in four dialects and we propose a method for constructing a web corpus as a support for crowdsourcing examples of MWE occurrences. The resource is available under an open license and paves the way towards a large-scale dialect-aware language resource construction, which should prove useful in both traditional and novel NLP applications.
Automatic Term Extraction (ATE) or Recognition (ATR) is a fundamental processing step preceding many complex knowledge engineering tasks. However, few methods have been implemented as public tools and in particular, available as open-source freeware. Further, little effort is made to develop an adaptable and scalable framework that enables customization, development, and comparison of algorithms under a uniform environment. This paper introduces JATE 2.0, a complete remake of the free Java Automatic Term Extraction Toolkit (Zhang et al., 2008) delivering new features including: (1) highly modular, adaptable and scalable ATE thanks to integration with Apache Solr, the open source free-text indexing and search platform; (2) an extended collection of state-of-the-art algorithms. We carry out experiments on two well-known benchmarking datasets and compare the algorithms along the dimensions of effectiveness (precision) and efficiency (speed and memory consumption). To the best of our knowledge, this is by far the only free ATE library offering a flexible architecture and the most comprehensive collection of algorithms.
Synaesthesia is a type of metaphor associating linguistic expressions that refer to two different sensory modalities. Previous studies, based on the analysis of poetic texts, have shown that synaesthetic transfers tend to go from the lower toward the higher senses (e.g., sweet music vs. musical sweetness). In non-literary language synaesthesia is rare, and finding a sufficient number of examples manually would be too time-consuming. In order to verify whether the directionality also holds for conventional synaesthesia found in non-literary texts, an automatic procedure for the identification of instances of synaesthesia is therefore highly desirable. In this paper, we first focus on the preliminary step of this procedure, that is, the creation of a controlled lexicon of perception. Next, we present the results of a small pilot study that applies the extraction procedure to English and Italian corpus data.
The purpose of this paper is to introduce the TermoPL tool created to extract terminology from domain corpora in Polish. The program extracts noun phrases, term candidates, with the help of a simple grammar that can be adapted for user{'}s needs. It applies the C-value method to rank term candidates being either the longest identified nominal phrases or their nested subphrases. The method operates on simplified base forms in order to unify morphological variants of terms and to recognize their contexts. We support the recognition of nested terms by word connection strength which allows us to eliminate truncated phrases from the top part of the term list. The program has an option to convert simplified forms of phrases into correct phrases in the nominal case. TermoPL accepts as input morphologically annotated and disambiguated domain texts and creates a list of terms, the top part of which comprises domain terminology. It can also compare two candidate term lists using three different coefficients showing asymmetry of term occurrences in this data.
This paper presents a novel gold standard of German noun-noun compounds (Ghost-NN) including 868 compounds annotated with corpus frequencies of the compounds and their constituents, productivity and ambiguity of the constituents, semantic relations between the constituents, and compositionality ratings of compound-constituent pairs. Moreover, a subset of the compounds containing 180 compounds is balanced for the productivity of the modifiers (distinguishing low/mid/high productivity) and the ambiguity of the heads (distinguishing between heads with 1, 2 and {\textgreater}2 senses
We introduce DeQue, a lexicon covering French complex prepositions (CPRE) like {``}{\`a} partir de{''} (from) and complex conjunctions (CCONJ) like {``}bien que{''} (although). The lexicon includes fine-grained linguistic description based on empirical evidence. We describe the general characteristics of CPRE and CCONJ in French, with special focus on syntactic ambiguity. Then, we list the selection criteria used to build the lexicon and the corpus-based methodology employed to collect entries. Finally, we quantify the ambiguity of each construction by annotating around 100 sentences randomly taken from the FRWaC. In addition to its theoretical value, the resource has many potential practical applications. We intend to employ DeQue for treebank annotation and to train a dependency parser that can takes complex constructions into account.
This paper summarizes the preliminary results of an ongoing survey on multiword resources carried out within the IC1207 Cost Action PARSEME (PARSing and Multi-word Expressions). Despite the availability of language resource catalogs and the inventory of multiword datasets on the SIGLEX-MWE website, multiword resources are scattered and difficult to find. In many cases, language resources such as corpora, treebanks, or lexical databases include multiwords as part of their data or take them into account in their annotations. However, these resources need to be centralized to make them accessible. The aim of this survey is to create a portal where researchers can easily find multiword(-aware) language resources for their research. We report on the design of the survey and analyze the data gathered so far. We also discuss the problems we have detected upon examination of the data as well as possible ways of enhancing the survey.
The goal of this work is to introduce CHILDES-MWE, which contains English CHILDES corpora automatically annotated with Multiword Expressions (MWEs) information. The result is a resource with almost 350,000 sentences annotated with more than 70,000 distinct MWEs of various types from both longitudinal and latitudinal corpora. This resource can be used for large scale language acquisition studies of how MWEs feature in child language. Focusing on compound nouns (CN), we then verify in a longitudinal study if there are differences in the distribution and compositionality of CNs in child-directed and child-produced sentences across ages. Moreover, using additional latitudinal data, we investigate if there are further differences in CN usage and in compositionality preferences. The results obtained for the child-produced sentences reflect CN distribution and compositionality in child-directed sentences.
While measuring the readability of texts has been a long-standing research topic, assessing the technicality of terms has only been addressed more recently and mostly for the English language. In this paper, we train a learning-to-rank model to determine a specialization degree for each term found in a given list. Since no training data for this task exist for French, we train our system with non-lexical features on English data, namely, the Consumer Health Vocabulary, then apply it to French. The features include the likelihood ratio of the term based on specialized and lay language models, and tests for containing morphologically complex words. The evaluation of this approach is conducted on 134 terms from the UMLS Metathesaurus and 868 terms from the Eugloss thesaurus. The Normalized Discounted Cumulative Gain obtained by our system is over 0.8 on both test sets. Besides, thanks to the learning-to-rank approach, adding morphological features to the language model features improves the results on the Eugloss thesaurus.
Collocations such as {``}heavy rain{''} or {``}make [a] decision{''}, are combinations of two elements where one (the base) is freely chosen, while the choice of the other (collocate) is restricted, depending on the base. Collocations present difficulties even to advanced language learners, who usually struggle to find the right collocate to express a particular meaning, e.g., both {``}heavy{''} and {``}strong{''} express the meaning {`}intense{'}, but while {``}rain{''} selects {``}heavy{''}, {``}wind{''} selects {``}strong{''}. Lexical Functions (LFs) describe the meanings that hold between the elements of collocations, such as {`}intense{'}, {`}perform{'}, {`}create{'}, {`}increase{'}, etc. Language resources with semantically classified collocations would be of great help for students, however they are expensive to build, since they are manually constructed, and scarce. We present an unsupervised approach to the acquisition and semantic classification of collocations according to LFs, based on word embeddings in which, given an example of a collocation for each of the target LFs and a set of bases, the system retrieves a list of collocates for each base and LF.
By means of an online survey, we have investigated ways in which various types of multiword expressions are annotated in existing treebanks. The results indicate that there is considerable variation in treatments across treebanks and thereby also, to some extent, across languages and across theoretical frameworks. The comparison is focused on the annotation of light verb constructions and verbal idioms. The survey shows that the light verb constructions either get special annotations as such, or are treated as ordinary verbs, while VP idioms are handled through different strategies. Based on insights from our investigation, we propose some general guidelines for annotating multiword expressions in treebanks. The recommendations address the following application-based needs: distinguishing MWEs from similar but compositional constructions; searching distinct types of MWEs in treebanks; awareness of literal and nonliteral meanings; and normalization of the MWE representation. The cross-lingually and cross-theoretically focused survey is intended as an aid to accessing treebanks and an aid for further work on treebank annotation.
Multiword Expressions (MWEs) are used frequently in natural languages, but understanding the diversity in MWEs is one of the open problem in the area of Natural Language Processing. In the context of Indian languages, MWEs play an important role. In this paper, we present MWEs annotation dataset created for Indian languages viz., Hindi and Marathi. We extract possible MWE candidates using two repositories: 1) the POS-tagged corpus and 2) the IndoWordNet synsets. Annotation is done for two types of MWEs: compound nouns and light verb constructions. In the process of annotation, human annotators tag valid MWEs from these candidates based on the standard guidelines provided to them. We obtained 3178 compound nouns and 2556 light verb constructions in Hindi and 1003 compound nouns and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems.
The question of how to compare languages and more generally the domain of linguistic typology, relies on the study of different linguistic properties or phenomena. Classically, such a comparison is done semi-manually, for example by extracting information from databases such as the WALS. However, it remains difficult to identify precisely regular parameters, available for different languages, that can be used as a basis towards modeling. We propose in this paper, focusing on the question of syntactic typology, a method for automatically extracting such parameters from treebanks, bringing them into a typology perspective. We present the method and the tools for inferring such information and navigating through the treebanks. The approach has been applied to 10 languages of the Universal Dependencies Treebank. We approach is evaluated by showing how automatic classification correlates with language families.
This paper introduces EasyTree, a dynamic graphical tool for dependency tree annotation. Built in JavaScript using the popular D3 data visualization library, EasyTree allows annotators to construct and label trees entirely by manipulating graphics, and then export the corresponding data in JSON format. Human users are thus able to annotate in an intuitive way without compromising the machine-compatibility of the output. EasyTree has a number of features to assist annotators, including color-coded part-of-speech indicators and optional translation displays. It can also be customized to suit a wide range of projects; part-of-speech categories, edge labels, and many other settings can be edited from within the GUI. The system also utilizes UTF-8 encoding and properly handles both left-to-right and right-to-left scripts. By providing a user-friendly annotation tool, we aim to reduce time spent transforming data or learning to use the software, to improve the user experience for annotators, and to make annotation approachable even for inexperienced users. Unlike existing solutions, EasyTree is built entirely with standard web technologies{--}JavaScript, HTML, and CSS{--}making it ideal for web-based annotation efforts, including crowdsourcing efforts.
Wikipedia, the well known internet encyclopedia, is nowadays a widely used source of information. To leverage its rich information, already parsed versions of Wikipedia have been proposed. We present an annotated dump of the English Wikipedia. This dump draws upon previously released Wikipedia parsed dumps. Still, we head in a different direction. In this parse we focus more into the syntactical characteristics of words: aside from the classical Part-of-Speech (PoS) tags and dependency parsing relations, we provide the full constituent parse branch for each word in a succinct way. Additionally, we propose a hypergraph network representation of the extracted linguistic information. The proposed modelization aims to take advantage of the information stocked within our parsed Wikipedia dump. We hope that by releasing these resources, researchers from the concerned communities will have a ready-to-experiment Wikipedia corpus to compare and distribute their work. We render public our parsed Wikipedia dump as well as the tool (and its source code) used to perform the parse. The hypergraph network and its related metadata is also distributed.
To ensure portability of NLP systems across multiple domains, existing treebanks are often extended by adding trees from interesting domains that were not part of the initial annotation effort. In this paper, we will argue that it is both useful from an application viewpoint and enlightening from a linguistic viewpoint to detect and reduce divergence in annotation schemes between extant and new parts in a set of treebanks that is to be used in evaluation experiments. The results of our correction and harmonization efforts will be made available to the public as a test suite for the evaluation of constituent parsing.
The Persian Universal Dependency Treebank (Persian UD) is a recent effort of treebanking Persian with Universal Dependencies (UD), an ongoing project that designs unified and cross-linguistically valid grammatical representations including part-of-speech tags, morphological features, and dependency relations. The Persian UD is the converted version of the Uppsala Persian Dependency Treebank (UPDT) to the universal dependencies framework and consists of nearly 6,000 sentences and 152,871 word tokens with an average sentence length of 25 words. In addition to the universal dependencies syntactic annotation guidelines, the two treebanks differ in tokenization. All words containing unsegmented clitics (pronominal and copula clitics) annotated with complex labels in the UPDT have been separated from the clitics and appear with distinct labels in the Persian UD. The treebank has its original syntactic annotation scheme based on Stanford Typed Dependencies. In this paper, we present the approaches taken in the development of the Persian UD.
We present the French Question Bank, a treebank of 2600 questions. We show that classical parsing model performance drop while the inclusion of this data set is highly beneficial without harming the parsing of non-question data. when facing out-of- domain data with strong structural diver- gences. Two thirds being aligned with the QB (Judge et al., 2006) and being freely available, this treebank will prove useful to build robust NLP systems.
Many shallow natural language understanding tasks use dependency trees to extract relations between content words. However, strict surface-structure dependency trees tend to follow the linguistic structure of sentences too closely and frequently fail to provide direct relations between content words. To mitigate this problem, the original Stanford Dependencies representation also defines two dependency graph representations which contain additional and augmented relations that explicitly capture otherwise implicit relations between content words. In this paper, we revisit and extend these dependency graph representations in light of the recent Universal Dependencies (UD) initiative and provide a detailed account of an enhanced and an enhanced++ English UD representation. We further present a converter from constituency to basic, i.e., strict surface structure, UD trees, and a converter from basic UD trees to enhanced and enhanced++ English UD graphs. We release both converters as part of Stanford CoreNLP and the Stanford Parser.
This paper describes our efforts for the development of a Proposition Bank for Urdu, an Indo-Aryan language. Our primary goal is the labeling of syntactic nodes in the existing Urdu dependency Treebank with specific argument labels. In essence, it involves annotation of predicate argument structures of both simple and complex predicates in the Treebank corpus. We describe the overall process of building the PropBank of Urdu. We discuss various statistics pertaining to the Urdu PropBank and the issues which the annotators encountered while developing the PropBank. We also discuss how these challenges were addressed to successfully expand the PropBank corpus. While reporting the Inter-annotator agreement between the two annotators, we show that the annotators share similar understanding of the annotation guidelines and of the linguistic phenomena present in the language. The present size of this Propbank is around 180,000 tokens which is double-propbanked by the two annotators for simple predicates. Another 100,000 tokens have been annotated for complex predicates of Urdu.
We introduce a new member of the family of Prague dependency treebanks. The Czech Legal Text Treebank 1.0 is a morphologically and syntactically annotated corpus of 1,128 sentences. The treebank contains texts from the legal domain, namely the documents from the Collection of Laws of the Czech Republic. Legal texts differ from other domains in several language phenomena influenced by rather high frequency of very long sentences. A manual annotation of such sentences presents a new challenge. We describe a strategy and tools for this task. The resulting treebank can be explored in various ways. It can be downloaded from the LINDAT/CLARIN repository and viewed locally using the TrEd editor or it can be accessed on-line using the KonText and TreeQuery tools.
We present an attempt to link the large amount of different concept lists which are used in the linguistic literature, ranging from Swadesh lists in historical linguistics to naming tests in clinical studies and psycholinguistics. This resource, our Concepticon, links 30 222 concept labels from 160 conceptlists to 2495 concept sets. Each concept set is given a unique identifier, a unique label, and a human-readable definition. Concept sets are further structured by defining different relations between the concepts. The resource can be used for various purposes. Serving as a rich reference for new and existing databases in diachronic and synchronic linguistics, it allows researchers a quick access to studies on semantic change, cross-linguistic polysemies, and semantic associations.
In this study we elaborate a road map for the conversion of a traditional lexical syntactico-semantic resource for French into a linguistic linked open data (LLOD) model. Our approach uses current best-practices and the analyses of earlier similar undertakings (lemonUBY and PDEV-lemon) to tease out the most appropriate representation for our resource.
In knowledge bases where concepts have associated properties, there is a large amount of comparative information that is implicitly encoded in the values of the properties these concepts share. Although there have been previous approaches to generating riddles, none of them seem to take advantage of structured information stored in knowledge bases such as Thesaurus Rex, which organizes concepts according to the fine grained ad-hoc categories they are placed into by speakers in everyday language, along with associated properties or modifiers. Taking advantage of these shared properties, we have developed a riddle generator that creates riddles about concepts represented as common nouns. The base of these riddles are comparisons between the target concept and other entities that share some of its properties. In this paper, we describe the process we have followed to generate the riddles starting from the target concept and we show the results of the first evaluation we have carried out to test the quality of the resulting riddles.
The paper presents the strategy and results of mapping adjective synsets between plWordNet (the wordnet of Polish, cf. Piasecki et al. 2009, Maziarz et al. 2013) and Princeton WordNet (cf. Fellbaum 1998). The main challenge of this enterprise has been very different synset relation structures in the two networks: horizontal, dumbbell-model based in PWN and vertical, hyponymy-based in plWN. Moreover, the two wordnets display differences in the grouping of adjectives into semantic domains and in the size of the adjective category. The handle the above contrasts, a series of automatic prompt algorithms and a manual mapping procedure relying on corresponding synset and lexical unit relations as well as on inter-lingual relations between noun synsets were proposed in the pilot stage of mapping (Rudnicka et al. 2015). In the paper we discuss the final results of the mapping process as well as explain example mapping choices. Suggestions for further development of mapping are also given.
Recent research shows the importance of linking linguistic knowledge resources for the creation of large-scale linguistic data. We describe our approach for combining two English resources, FrameNet and sar-graphs, and illustrate the benefits of the linked data in a relation extraction setting. While FrameNet consists of schematic representations of situations, linked to lexemes and their valency patterns, sar-graphs are knowledge resources that connect semantic relations from factual knowledge graphs to the linguistic phrases used to express instances of these relations. We analyze the conceptual similarities and differences of both resources and propose to link sar-graphs and FrameNet on the levels of relations/frames as well as phrases. The former alignment involves a manual ontology mapping step, which allows us to extend sar-graphs with new phrase patterns from FrameNet. The phrase-level linking, on the other hand, is fully automatic. We investigate the quality of the automatically constructed links and identify two main classes of errors.
This paper presents the process of enriching the verb frame database of a Hungarian natural language parser to enable the assignment of semantic roles. We accomplished this by linking the parser{'}s verb frame database to existing linguistic resources such as VerbNet and WordNet, and automatically transferring back semantic knowledge. We developed OWL ontologies that map the various constraint description formalisms of the linked resources and employed a logical reasoning device to facilitate the linking procedure. We present results and discuss the challenges and pitfalls that arose from this undertaking.
Wikipedia has been increasingly used as a knowledge base for open-domain Named Entity Linking and Disambiguation. In this task, a dictionary with entity surface forms plays an important role in finding a set of candidate entities for the mentions in text. Existing dictionaries mostly rely on the Wikipedia link structure, like anchor texts, redirect links and disambiguation links. In this paper, we introduce a dictionary for Entity Linking that includes name variations extracted from Wikipedia article text, in addition to name variations derived from the Wikipedia link structure. With this approach, we show an increase in the coverage of entities and their mentions in the dictionary in comparison to other Wikipedia based dictionaries.
The Open Linguistics Working Group (OWLG) brings together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections. A major outcome of our work is the Linguistic Linked Open Data (LLOD) cloud, an LOD (sub-)cloud of linguistic resources, which covers various linguistic databases, lexicons, corpora, terminologies, and metadata repositories. We present and summarize five years of progress on the development of the cloud and of advancements in open data in linguistics, and we describe recent community activities. The paper aims to serve as a guideline to orient and involve researchers with the community and/or Linguistic Linked Open Data.
Various lexical resources are being published in RDF. To enhance the usability of these resources, identical resources in different data sets should be linked. If lexical resources are described in different natural languages, then techniques to deal with multilinguality are required for interlinking. In this paper, we evaluate machine translation for interlinking concepts, i.e., generic entities named with a common noun or term. In our previous work, the evaluated method has been applied on named entities. We conduct two experiments involving different thesauri in different languages. The first experiment involves concepts from the TheSoz multilingual thesaurus in three languages: English, French and German. The second experiment involves concepts from the EuroVoc and AGROVOC thesauri in English and Chinese respectively. Our results demonstrate that machine translation can be beneficial for cross-lingual thesauri interlinking independently of a dataset structure.
Language Resources (LRs) are an essential ingredient of current approaches in Linguistics, Computational Linguistics, Language Technology and related fields. LRs are collections of spoken or written language data, typically annotated with linguistic analysis information. Different types of LRs exist, for example, corpora, ontologies, lexicons, collections of spoken language data (audio), or collections that also include video (multimedia, multimodal). Often, LRs are distributed with specific tools, documentation, manuals or research publications. The different phases that involve creating and distributing an LR can be conceptualised as a life cycle. While the idea of handling the LR production and maintenance process in terms of a life cycle has been brought up quite some time ago, a best practice model or common approach can still be considered a research gap. This article wants to help fill this gap by proposing an initial version of a generic Language Resource Life Cycle that can be used to inform, direct, control and evaluate LR research and development activities (including description, management, production, validation and evaluation workflows).
Everyday meals are an important part of our daily lives and, currently, there are many Internet sites that help us plan these meals. Allied to the growth in the amount of food data such as recipes available on the Internet is an increase in the number of studies on these data, such as recipe analysis and recipe search. However, there are few publicly available resources for food research; those that do exist do not include a wide range of food data or any meal data (that is, likely combinations of recipes). In this study, we construct a large-scale recipe and meal data collection as the underlying infrastructure to promote food research. Our corpus consists of approximately 1.7 million recipes and 36000 meals in cookpad, one of the largest recipe sites in the world. We made the corpus available to researchers in February 2015 and as of February 2016, 82 research groups at 56 universities have made use of it to enhance their studies.
Although there are many tools for natural language processing tasks in Estonian, these tools are very loosely interoperable, and it is not easy to build practical applications on top of them. In this paper, we introduce a new Python library for natural language processing in Estonian, which provides unified programming interface for various NLP components. The EstNLTK toolkit provides utilities for basic NLP tasks including tokenization, morphological analysis, lemmatisation and named entity recognition as well as offers more advanced features such as a clause segmentation, temporal expression extraction and normalization, verb chain detection, Estonian Wordnet integration and rule-based information extraction. Accompanied by a detailed API documentation and comprehensive tutorials, EstNLTK is suitable for a wide range of audience. We believe EstNLTK is mature enough to be used for developing NLP-backed systems both in industry and research. EstNLTK is freely available under the GNU GPL version 2+ license, which is standard for academic software.
This presentation introduces the imminent establishment of a new language resource infrastructure focusing on languages spoken in Southern Africa, with an eventual aim to become a hub for digital language resources within Sub-Saharan Africa. The Constitution of South Africa makes provision for 11 official languages all with equal status. The current language Resource Management Agency will be merged with the new Centre, which will have a wider focus than that of data acquisition, management and distribution. The Centre will entertain two main programs: Digitisation and Digital Humanities. The digitisation program will focus on the systematic digitisation of relevant text, speech and multi-modal data across the official languages. Relevancy will be determined by a Scientific Advisory Board. This will take place on a continuous basis through specified projects allocated to national members of the Centre, as well as through open-calls aimed at the academic as well as local communities. The digital resources will be managed and distributed through a dedicated web-based portal. The development of the Digital Humanities program will entail extensive academic support for projects implementing digital language based data. The Centre will function as an enabling research infrastructure primarily supported by national government and hosted by the North-West University.
In this paper, we report on the design and development of an online search platform for the MERLIN corpus of learner texts in Czech, German and Italian. It was created in the context of the MERLIN project, which aims at empirically illustrating features of the Common European Framework of Reference (CEFR) for evaluating language competences based on authentic learner text productions compiled into a learner corpus. Furthermore, the project aims at providing access to the corpus through a search interface adapted to the needs of multifaceted target groups involved with language learning and teaching. This article starts by providing a brief overview on the project ambition, the data resource and its intended target groups. Subsequently, the main focus of the article is on the design and development process of the platform, which is carried out in a user-centred fashion. The paper presents the user studies carried out to collect requirements, details the resulting decisions concerning the platform design and its implementation, and reports on the evaluation of the platform prototype and final adjustments.
Language resources are valuable assets, both for institutions and researchers. To safeguard these resources requirements for repository systems and data management have been specified by various branch organizations, e.g., CLARIN and the Data Seal of Approval. This paper describes these and some additional ones posed by the authors{'} home institutions. And it shows how they are met by FLAT, to provide a new home for language resources. The basis of FLAT is formed by the Fedora Commons repository system. This repository system can meet many of the requirements out-of-the box, but still additional configuration and some development work is needed to meet the remaining ones, e.g., to add support for Handles and Component Metadata. This paper describes design decisions taken in the construction of FLAT{'}s system architecture via a mix-and-match strategy, with a preference for the reuse of existing solutions. FLAT is developed and used by the Meertens Institute and The Language Archive, but is also freely available for anyone in need of a CLARIN-compliant repository for their language resources.
I introduce CLARIAH in the Netherlands, which aims to contribute the Netherlands part of a Europe-wide humanities research infrastructure. I describe the digital turn in the humanities, the background and context of CLARIAH, both nationally and internationally, its relation to the CLARIN and DARIAH infrastructures, and the rationale for joining forces between CLARIN and DARIAH in the Netherlands. I also describe the first results of joining forces as achieved in the CLARIAH-SEED project, and the plans of the CLARIAH-CORE project, which is currently running
The Component MetaData Infrastructure (CMDI) is a framework for the creation and usage of metadata formats to describe all kinds of resources in the CLARIN world. To better connect to the library world, and to allow librarians to enter metadata for linguistic resources into their catalogues, a crosswalk from CMDI-based formats to bibliographic standards is required. The general and rather fluid nature of CMDI, however, makes it hard to map arbitrary CMDI schemas to metadata standards such as Dublin Core (DC) or MARC 21, which have a mature, well-defined and fixed set of field descriptors. In this paper, we address the issue and propose crosswalks between CMDI-based profiles originating from the NaLiDa project and DC and MARC 21, respectively.
Data management plans, data sharing plans and the like are now required by funders worldwide as part of research proposals. Concerned with promoting the notion of open scientific data, funders view such plans as the framework for satisfying the generally accepted requirements for data generated in funded research projects, among them that it be accessible, usable, standardized to the degree possible, secure and stable. This paper examines the origins of data management plans, their requirements and issues they raise for data centers and HLT resource development in general.
We introduce JCoRe 2.0, the relaunch of a UIMA-based open software repository for full-scale natural language processing originating from the Jena University Language {\&} Information Engineering (JULIE) Lab. In an attempt to put the new release of JCoRe on firm software engineering ground, we uploaded it to GitHub, a social coding platform, with an underlying source code versioning system and various means to support collaboration for software development and code modification management. In order to automate the builds of complex NLP pipelines and properly represent and track dependencies of the underlying Java code, we incorporated Maven as part of our software configuration management efforts. In the meantime, we have deployed our artifacts on Maven Central, as well. JCoRe 2.0 offers a broad range of text analytics functionality (mostly) for English-language scientific abstracts and full-text articles, especially from the life sciences domain.
The issue for CLARIN archives at the metadata level is to facilitate the user{'}s possibility to describe their data, even with their own standard, and at the same time make these metadata meaningful for a variety of users with a variety of resource types, and ensure that the metadata are useful for search across all resources both at the national and at the European level. We see that different people from different research communities fill in the metadata in different ways even though the metadata was defined and documented. This has impacted when the metadata are harvested and displayed in different environments. A loss of information is at stake. In this paper we view the challenges of ensuring metadata interoperability through examples of propagation of metadata values from the CLARIN-DK archive to the VLO. We see that the CLARIN Community in many ways support interoperability, but argue that agreeing upon standards, making clear definitions of the semantics of the metadata and their content is inevitable for the interoperability to work successfully. The key points are clear and freely available definitions, accessible documentation and easily usable facilities and guidelines for the metadata creators.
The article describes the current status of a large national project, CoRoLa, aiming at building a reference corpus for the contemporary Romanian language. Unlike many other national corpora, CoRoLa contains only - IPR cleared texts and speech data, obtained from some of the country{'}s most representative publishing houses, broadcasting agencies, editorial offices, newspapers and popular bloggers. For the written component 500 million tokens are targeted and for the oral one 300 hours of recordings. The choice of texts is done according to their functional style, domain and subdomain, also with an eye to the international practice. A metadata file (following the CMDI model) is associated to each text file. Collected texts are cleaned and transformed in a format compatible with the tools for automatic processing (segmentation, tokenization, lemmatization, part-of-speech tagging). The paper also presents up-to-date statistics about the structure of the corpus almost two years before its official launching. The corpus will be freely available for searching. Users will be able to download the results of their searches and those original files when not against stipulations in the protocols we have with text providers.
The paper concentrates on the design, composition and annotation of SYN2015, a new 100-million representative corpus of contemporary written Czech. SYN2015 is a sequel of the representative corpora of the SYN series that can be described as traditional (as opposed to the web-crawled corpora), featuring cleared copyright issues, well-defined composition, reliability of annotation and high-quality text processing. At the same time, SYN2015 is designed as a reflection of the variety of written Czech text production with necessary methodological and technological enhancements that include a detailed bibliographic annotation and text classification based on an updated scheme. The corpus has been produced using a completely rebuilt text processing toolchain called SynKorp. SYN2015 is lemmatized, morphologically and syntactically annotated with state-of-the-art tools. It has been published within the framework of the Czech National Corpus and it is available via the standard corpus query interface KonText at http://kontext.korpus.cz as well as a dataset in shuffled format.
This proposal describes a new way to visualise resources in the LREMap, a community-built repository of language resource descriptions and uses. The LREMap is represented as a force-directed graph, where resources, papers and authors are nodes. The analysis of the visual representation of the underlying graph is used to study how the community gathers around LRs and how LRs are used in research.
Researchers in Natural Language Processing rely on availability of data and software, ideally under open licenses, but little is done to actively encourage it. In fact, the current Copyright framework grants exclusive rights to authors to copy their works, make them available to the public and make derivative works (such as annotated language corpora). Moreover, in the EU databases are protected against unauthorized extraction and re-utilization of their contents. Therefore, proper public licensing plays a crucial role in providing access to research data. A public license is a license that grants certain rights not to one particular user, but to the general public (everybody). Our article presents a tool that we developed and whose purpose is to assist the user in the licensing process. As software and data should be licensed under different licenses, the tool is composed of two separate parts: Data and Software. The underlying logic as well as elements of the graphic interface are presented below.
The Information System for Syntactic and Semantic Analysis of the Lithuanian language (lith. Lietuvi{\k{u}} kalbos sintaksin{\.e}s ir semantin{\.e}s analiz{\.e}s informacin{\.e} sistema, LKSSAIS) is the first infrastructure for the Lithuanian language combining Lithuanian language tools and resources for diverse linguistic research and applications tasks. It provides access to the basic as well as advanced natural language processing tools and resources, including tools for corpus creation and management, text preprocessing and annotation, ontology building, named entity recognition, morphosyntactic and semantic analysis, sentiment analysis, etc. It is an important platform for researchers and developers in the field of natural language technology.
We introduce CODE ALLTAG, a text corpus composed of German-language e-mails. It is divided into two partitions: the first of these portions, CODE ALLTAG{\_}XL, consists of a bulk-size collection drawn from an openly accessible e-mail archive (roughly 1.5M e-mails), whereas the second portion, CODE ALLTAG{\_}S+d, is much smaller in size (less than thousand e-mails), yet excels with demographic data from each author of an e-mail. CODE ALLTAG, thus, currently constitutes the largest E-Mail corpus ever built. In this paper, we describe, for both parts, the solicitation process for gathering e-mails, present descriptive statistical properties of the corpus, and, for CODE ALLTAG{\_}S+d, reveal a compilation of demographic features of the donors of e-mails.
The Low Resource Language research conducted under DARPA{'}s Broad Operational Language Translation (BOLT) program required the rapid creation of text corpora of typologically diverse languages (Turkish, Hausa, and Uzbek) which were annotated with morphological information, along with other types of annotation. Since the output of morphological analyzers is a significant aid to morphological annotation, we developed a morphological analyzer for each language in order to support the annotation task, and also as a deliverable by itself. Our framework for analyzer creation results in tables similar to those used in the successful SAMA analyzer for Arabic, but with a more abstract linguistic level, from which the tables are derived. A lexicon was developed from available resources for integration with the analyzer, and given the speed of development and uncertain coverage of the lexicon, we assumed that the analyzer would necessarily be lacking in some coverage for the project annotation. Our analyzer framework was therefore focused on rapid implementation of the key structures of the language, together with accepting {``}wildcard{''} solutions as possible analyses for a word with an unknown stem, building upon our similar experiences with morphological annotation with Modern Standard Arabic and Egyptian Arabic.
We propose a novel neural lemmatization model which is language independent and supervised in nature. To handle the words in a neural framework, word embedding technique is used to represent words as vectors. The proposed lemmatizer makes use of contextual information of the surface word to be lemmatized. Given a word along with its contextual neighbours as input, the model is designed to produce the lemma of the concerned word as output. We introduce a new network architecture that permits only dimension specific connections between the input and the output layer of the model. For the present work, Bengali is taken as the reference language. Two datasets are prepared for training and testing purpose consisting of 19,159 and 2,126 instances respectively. As Bengali is a resource scarce language, these datasets would be beneficial for the respective research community. Evaluation method shows that the neural lemmatizer achieves 69.57{\%} accuracy on the test dataset and outperforms the simple cosine similarity based baseline strategy by a margin of 1.37{\%}.
{\textasciitilde}This paper describes the development of free/open-source finite-state morphological transducers for Tuvan, a Turkic language spoken in and around the Tuvan Republic in Russia. The finite-state toolkit used for the work is the Helsinki Finite-State Toolkit (HFST), we use the lexc formalism for modelling the morphotactics and twol formalism for modelling morphophonological alternations. We present a novel description of the morphological combinatorics of pseudo-derivational morphemes in Tuvan. An evaluation is presented which shows that the transducer has a reasonable coverage‚Äïaround 93{\%}‚Äïon freely-available corpora of the languages, and high precision‚Äïover 99{\%}‚Äïon a manually verified test set.
We describe an extensive and versatile lexical resource for Latvian, an under-resourced Indo-European language, which we call Tezaurs (Latvian for {`}thesaurus{'}). It comprises a large explanatory dictionary of more than 250,000 entries that are derived from more than 280 external sources. The dictionary is enriched with phonetic, morphological, semantic and other annotations, as well as augmented by various language processing tools allowing for the generation of inflectional forms and pronunciation, for on-the-fly selection of corpus examples, for suggesting synonyms, etc. Tezaurs is available as a public and widely used web application for end-users, as an open data set for the use in language technology (LT), and as an API ‚Äï a set of web services for the integration into third-party applications. The ultimate goal of Tezaurs is to be the central computational lexicon for Latvian, bringing together all Latvian words and frequently used multi-word units and allowing for the integration of other LT resources and tools.
Morphological analysis is a fundamental task in natural-language processing, which is used in other NLP applications such as part-of-speech tagging, syntactic parsing, information retrieval, machine translation, etc. In this paper, we present our work on the development of free/open-source finite-state morphological analyser for Sindhi. We have used Apertium{'}s lttoolbox as our finite-state toolkit to implement the transducer. The system is developed using a paradigm-based approach, wherein a paradigm defines all the word forms and their morphological features for a given stem (lemma). We have evaluated our system on the Sindhi Wikipedia corpus and achieved a reasonable coverage of 81{\%} and a precision of over 97{\%}.
This paper presents a semi-automatic method to derive morphological analyzers from a limited number of example inflections suitable for languages with alphabetic writing systems. The system we present learns the inflectional behavior of morphological paradigms from examples and converts the learned paradigms into a finite-state transducer that is able to map inflected forms of previously unseen words into lemmas and corresponding morphosyntactic descriptions. We evaluate the system when provided with inflection tables for several languages collected from the Wiktionary.
We report on the implementation of a morphological analyzer for the Sahidic dialect of Coptic, a now extinct Afro-Asiatic language. The system is developed in the finite-state paradigm. The main purpose of the project is provide a method by which scholars and linguists can semi-automatically gloss extant texts written in Sahidic. Since a complete lexicon containing all attested forms in different manuscripts requires significant expertise in Coptic spanning almost 1,000 years, we have equipped the analyzer with a core lexicon and extended it with a {``}guesser{''} ability to capture out-of-vocabulary items in any inflection. We also suggest an ASCII transliteration for the language. A brief evaluation is provided.
We present the new online edition of a dictionary of Polish inflection ‚Äï the Grammatical Dictionary of Polish (http://sgjp.pl). The dictionary is interesting for several reasons: it is comprehensive (over 330,000 lexemes corresponding to almost 4,300,000 different textual words; 1116 handcrafted inflectional patterns), the inflection is presented in an explicit manner in the form of carefully designed tables, the user interface facilitates advanced queries by several features (lemmas, forms, applicable grammatical categories, types of inflection). Moreover, the data of the dictionary is used in morphological analysers, including our product Morfeusz (http://sgjp. pl/morfeusz). From the start, the dictionary was meant to be comfortable for the human reader as well as to be ready for use in NLP applications. In the paper we briefly discuss both aspects of the resource.
This paper presents a collection of 350,000 German lemmatised words, rated on four psycholinguistic affective attributes. All ratings were obtained via a supervised learning algorithm that can automatically calculate a numerical rating of a word. We applied this algorithm to abstractness, arousal, imageability and valence. Comparison with human ratings reveals high correlation across all rating types. The full resource is publically available at: http://www.ims.uni-stuttgart.de/data/affective{\_}norms/
Despite a centuries-long tradition in lexicography, Latin lacks state-of-the-art computational lexical resources. This situation is strictly related to the still quite limited amount of linguistically annotated textual data for Latin, which can help the building of new lexical resources by supporting them with empirical evidence. However, projects for creating new language resources for Latin have been launched over the last decade to fill this gap. In this paper, we present Latin Vallex, a valency lexicon for Latin built in mutual connection with the semantic and pragmatic annotation of two Latin treebanks featuring texts of different eras. On the one hand, such a connection between the empirical evidence provided by the treebanks and the lexicon allows to enhance each frame entry in the lexicon with its frequency in real data. On the other hand, each valency-capable word in the treebanks is linked to a frame entry in the lexicon.
Given lexical-semantic resources in different languages, it is useful to establish cross-lingual correspondences, preferably with semantic relation labels, between the concept nodes in these resources. This paper presents a framework for enabling a cross-lingual/node-wise alignment of lexical-semantic resources, where cross-lingual correspondence candidates are first discovered and ranked, and then classified by a succeeding module. Indeed, we propose that a two-tier classifier configuration is feasible for the second module: the first classifier filters out possibly irrelevant correspondence candidates and the second classifier assigns a relatively fine-grained semantic relation label to each of the surviving candidates. The results of Japanese-to-English alignment experiments using EDR Electronic Dictionary and Princeton WordNet are described to exemplify the validity of the proposal.
The last two decades have seen the development of various semantic lexical resources such as WordNet (Miller, 1995) and the USAS semantic lexicon (Rayson et al., 2004), which have played an important role in the areas of natural language processing and corpus-based studies. Recently, increasing efforts have been devoted to extending the semantic frameworks of existing lexical knowledge resources to cover more languages, such as EuroWordNet and Global WordNet. In this paper, we report on the construction of large-scale multilingual semantic lexicons for twelve languages, which employ the unified Lancaster semantic taxonomy and provide a multilingual lexical knowledge base for the automatic UCREL semantic annotation system (USAS). Our work contributes towards the goal of constructing larger-scale and higher-quality multilingual semantic lexical resources and developing corpus annotation tools based on them. Lexical coverage is an important factor concerning the quality of the lexicons and the performance of the corpus annotation tools, and in this experiment we focus on evaluating the lexical coverage achieved by the multilingual lexicons and semantic annotation tools based on them. Our evaluation shows that some semantic lexicons such as those for Finnish and Italian have achieved lexical coverage of over 90{\%} while others need further expansion.
We present the dict{\_}to{\_}4lang tool for processing entries of three monolingual dictionaries of English and mapping definitions to concept graphs following the 4lang principles of semantic representation introduced by (Kornai, 2010). 4lang representations are domain- and language-independent, and make use of only a very limited set of primitives to encode the meaning of all utterances. Our pipeline relies on the Stanford Dependency Parser for syntactic analysis, the dep to 4lang module then builds directed graphs of concepts based on dependency relations between words in each definition. Several issues are handled by construction-specific rules that are applied to the output of dep{\_}to{\_}4lang. Manual evaluation suggests that ca. 75{\%} of graphs built from the Longman Dictionary are either entirely correct or contain only minor errors. dict{\_}to{\_}4lang is available under an MIT license as part of the 4lang library and has been used successfully in measuring Semantic Textual Similarity (Recski and {\'A}cs, 2015). An interactive demo of core 4lang functionalities is available at http://4lang.hlt.bme.hu.
This article presents the semantic layer of Walenty‚Äïa new valence dictionary of Polish predicates, with a number of novel features, as compared to other such dictionaries. The dictionary contains two layers, syntactic and semantic. The syntactic layer describes syntactic and morphosyntactic constraints predicates put on their dependants. In particular, it includes a comprehensive and powerful phraseological component. The semantic layer shows how predicates and their arguments are involved in a described situation in an utterance. These two layers are connected, representing how semantic arguments can be realised on the surface. Each syntactic schema and each semantic frame are illustrated by at least one exemplary sentence attested in linguistic reality. The semantic layer consists of semantic frames represented as lists of pairs and connected with PlWordNet lexical units. Semantic roles have a two-level representation (basic roles are provided with an attribute) enabling representation of arguments in a flexible way. Selectional preferences are based on PlWordNet structure as well.
This paper proposes a new method for Italian verb classification -and a preliminary example of resulting classes- inspired by Levin (1993) and VerbNet (Kipper-Schuler, 2005), yet partially independent from these resources; we achieved such a result by integrating Levin and VerbNet{'}s models of classification with other theoretic frameworks and resources. The classification is rooted in the constructionist framework (Goldberg, 1995; 2006) and is distribution-based. It is also semantically characterized by a link to FrameNet{'}ssemanticframesto represent the event expressed by a class. However, the new Italian classes maintain the hierarchic {``}tree{''} structure and monotonic nature of VerbNet{'}s classes, and, where possible, the original names (e.g.: Verbs of Killing, Verbs of Putting, etc.). We therefore propose here a taxonomy compatible with VerbNet but at the same time adapted to Italian syntax and semantics. It also addresses a number of problems intrinsic to the original classifications, such as the role of argument alternations, here regarded simply as epiphenomena, consistently with the constructionist approach.
Patients are often exposed to medical terms, such as anosognosia, myelodysplastic, or hepatojejunostomy, that can be semantically complex and hardly understandable by non-experts in medicine. Hence, it is important to assess which words are potentially non-understandable and require further explanations. The purpose of our work is to build specific lexicon in which the words are rated according to whether they are understandable or non-understandable. We propose to work with medical words in French such as provided by an international medical terminology. The terms are segmented in single words and then each word is manually processed by three annotators. The objective is to assign each word into one of the three categories: I can understand, I am not sure, I cannot understand. The annotators do not have medical training nor they present specific medical problems. They are supposed to represent an average patient. The inter-annotator agreement is then computed. The content of the categories is analyzed. Possible applications in which this lexicon can be helpful are proposed and discussed. The rated lexicon is freely available for the research purposes. It is accessible online at http://natalia.grabar.perso.sfr.fr/rated-lexicon.html
Word sense embeddings represent a word sense as a low-dimensional numeric vector. While this representation is potentially useful for NLP applications, its interpretability is inherently limited. We propose a simple technique that improves interpretability of sense vectors by mapping them to synsets of a lexical resource. Our experiments with AdaGram sense embeddings and BabelNet synsets show that it is possible to retrieve synsets that correspond to automatically learned sense vectors with Precision of 0.87, Recall of 0.42 and AUC of 0.78.
This paper presents a lexical resource developed for Portuguese. The resource contains sentences annotated with semantic roles. The sentences were extracted from two domains: Cardiology research papers and newspaper articles. Both corpora were analyzed with the PALAVRAS parser and subsequently processed with a subcategorization frames extractor, so that each sentence that contained at least one main verb was stored in a database together with its syntactic organization. The annotation was manually carried out by a linguist using an annotation interface. Both the annotated and non-annotated data were exported to an XML format, which is readily available for download. The reason behind exporting non-annotated data is that there is syntactic information collected from the parser annotation in the non-annotated data, and this could be useful for other researchers. The sentences from both corpora were annotated separately, so that it is possible to access sentences either from the Cardiology or from the newspaper corpus. The full resource presents more than seven thousand semantically annotated sentences, containing 192 different verbs and more than 15 thousand individual arguments and adjuncts.
This paper presents the Predicate Matrix 1.3, a lexical resource resulting from the integration of multiple sources of predicate information including FrameNet, VerbNet, PropBank and WordNet. This new version of the Predicate Matrix has been extended to cover nominal predicates by adding mappings to NomBank. Similarly, we have integrated resources in Spanish, Catalan and Basque. As a result, the Predicate Matrix 1.3 provides a multilingual lexicon to allow interoperable semantic analysis in multiple languages.
We present a gold standard for evaluating scale membership and the order of scalar adjectives. In addition to evaluating existing methods of ordering adjectives, this knowledge will aid in studying the organization of adjectives in the lexicon. This resource is the result of two elicitation tasks conducted with informants from Amazon Mechanical Turk. The first task is notable for gathering open-ended lexical data from informants. The data is analyzed using Cultural Consensus Theory, a framework from anthropology, to not only determine scale membership but also the level of consensus among the informants (Romney et al., 1986). The second task gathers a culturally salient ordering of the words determined to be members. We use this method to produce 12 scales of adjectives for use in evaluation.
In this paper we describe VerbCROcean, a broad-coverage repository of fine-grained semantic relations between Croatian verbs. Adopting the methodology of Chklovski and Pantel (2004) used for acquiring the English VerbOcean, we first acquire semantically related verb pairs from a web corpus hrWaC by relying on distributional similarity of subject-verb-object paths in the dependency trees. We then classify the semantic relations between each pair of verbs as similarity, intensity, antonymy, or happens-before, using a number of manually-constructed lexico-syntatic patterns. We evaluate the quality of the resulting resource on a manually annotated sample of 1000 semantic verb relations. The evaluation revealed that the predictions are most accurate for the similarity relation, and least accurate for the intensity relation. We make available two variants of VerbCROcean: a coverage-oriented version, containing about 36k verb pairs at a precision of 41{\%}, and a precision-oriented version containing about 5k verb pairs, at a precision of 56{\%}.
In this article we present an exploratory approach to enrich a WordNet-like lexical ontology with the synonyms present in a standard monolingual Portuguese dictionary. The dictionary was converted from PDF into XML and senses were automatically identified and annotated. This allowed us to extract them, independently of definitions, and to create sets of synonyms (synsets). These synsets were then aligned with WordNet synsets, both in the same language (Portuguese) and projecting the Portuguese terms into English, Spanish and Galician. This process allowed both the addition of new term variants to existing synsets, as to create new synsets for Portuguese.
Collecting data for sentiment analysis in resource-limited languages carries a significant risk of sample selection bias, since the small quantities of available data are most likely not representative of the whole population. Ignoring this bias leads to less robust machine learning classifiers and less reliable evaluation results. In this paper we present a dataset balancing algorithm that minimizes the sample selection bias by eliminating irrelevant systematic differences between the sentiment classes. We prove its superiority over the random sampling method and we use it to create the Serbian movie review dataset ‚Äï SerbMR ‚Äï the first balanced and topically uniform sentiment analysis dataset in Serbian. In addition, we propose an incremental way of finding the optimal combination of simple text processing options and machine learning features for sentiment classification. Several popular classifiers are used in conjunction with this evaluation approach in order to establish strong but reliable baselines for sentiment analysis in Serbian.
This paper introduces the augmented NTU sentiment dictionary, abbreviated as ANTUSD, which is constructed by collecting sentiment stats of words in several sentiment annotation work. A total of 26,021 words were collected in ANTUSD. For each word, the CopeOpi numerical sentiment score and the number of positive annotation, neutral annotation, negative annotation, non-opinionated annotation, and not-a-word annotation are provided. Words and their sentiment information in ANTUSD have been linked to the Chinese ontology E-HowNet to provide rich semantic information. We demonstrate the usage of ANTUSD in polarity classification of words, and the results show that a superior f-score 98.21 is achieved, which supports the usefulness of the ANTUSD. ANTUSD can be freely obtained through application from NLPSA lab, Academia Sinica: http://academiasinicanlplab.github.io/
Due to the phenomenal growth of online product reviews, sentiment analysis (SA) has gained huge attention, for example, by online service providers. A number of benchmark datasets for a wide range of domains have been made available for sentiment analysis, especially in resource-rich languages. In this paper we assess the challenges of SA in Hindi by providing a benchmark setup, where we create an annotated dataset of high quality, build machine learning models for sentiment analysis in order to show the effective usage of the dataset, and finally make the resource available to the community for further advancement of research. The dataset comprises of Hindi product reviews crawled from various online sources. Each sentence of the review is annotated with aspect term and its associated sentiment. As classification algorithms we use Conditional Random Filed (CRF) and Support Vector Machine (SVM) for aspect term extraction and sentiment analysis, respectively. Evaluation results show the average F-measure of 41.07{\%} for aspect term extraction and accuracy of 54.05{\%} for sentiment classification.
This paper deals with building linguistic resources for Gulf Arabic, one of the Arabic variations, for sentiment analysis task using machine learning. To our knowledge, no previous works were done for Gulf Arabic sentiment analysis despite the fact that it is present in different online platforms. Hence, the first challenge is the absence of annotated data and sentiment lexicons. To fill this gap, we created these two main linguistic resources. Then we conducted different experiments: use Naive Bayes classifier without any lexicon; add a sentiment lexicon designed basically for MSA; use only the compiled Gulf Arabic sentiment lexicon and finally use both MSA and Gulf Arabic sentiment lexicons. The Gulf Arabic lexicon gives a good improvement of the classifier accuracy (90.54 {\%}) over a baseline that does not use the lexicon (82.81{\%}), while the MSA lexicon causes the accuracy to drop to (76.83{\%}). Moreover, mixing MSA and Gulf Arabic lexicons causes the accuracy to drop to (84.94{\%}) compared to using only Gulf Arabic lexicon. This indicates that it is useless to use MSA resources to deal with Gulf Arabic due to the considerable differences and conflicting structures between these two languages.
Sentiment shifters, i.e., words and expressions that can affect text polarity, play an important role in opinion mining. However, the limited ability of current automated opinion mining systems to handle shifters represents a major challenge. The majority of existing approaches rely on a manual list of shifters; few attempts have been made to automatically identify shifters in text. Most of them just focus on negating shifters. This paper presents a novel and efficient semi-automatic method for identifying sentiment shifters in drug reviews, aiming at improving the overall accuracy of opinion mining systems. To this end, we use weighted association rule mining (WARM), a well-known data mining technique, for finding frequent dependency patterns representing sentiment shifters from a domain-specific corpus. These patterns that include different kinds of shifter words such as shifter verbs and quantifiers are able to handle both local and long-distance shifters. We also combine these patterns with a lexicon-based approach for the polarity classification task. Experiments on drug reviews demonstrate that extracted shifters can improve the precision of the lexicon-based approach for polarity classification 9.25 percent.
This paper describes the STAC resource, a corpus of multi-party chats annotated for discourse structure in the style of SDRT (Asher and Lascarides, 2003; Lascarides and Asher, 2009). The main goal of the STAC project is to study the discourse structure of multi-party dialogues in order to understand the linguistic strategies adopted by interlocutors to achieve their conversational goals, especially when these goals are opposed. The STAC corpus is not only a rich source of data on strategic conversation, but also the first corpus that we are aware of that provides full discourse structures for multi-party dialogues. It has other remarkable features that make it an interesting resource for other topics: interleaved threads, creative language, and interactions between linguistic and extra-linguistic contexts.
This paper presents an automatic corpus-based process to author an open-domain conversational strategy usable both in chatterbot systems and as a fallback strategy for out-of-domain human utterances. Our approach is implemented on a corpus of television drama subtitles. This system is used as a chatterbot system to collect a corpus of 41 open-domain textual dialogues with 27 human participants. The general capabilities of the system are studied through objective measures and subjective self-reports in terms of understandability, repetition and coherence of the system responses selected in reaction to human utterances. Subjective evaluations of the collected dialogues are presented with respect to amusement, engagement and enjoyability. The main factors influencing those dimensions in our chatterbot experiment are discussed.
In this study, we describe the use of back-channelling patterns extracted from a dialogue corpus as a mean to characterising text-based dialogue systems. Our goal was to provide system users with the feeling that they are interacting with distinct individuals rather than artificially created characters. An analysis of the corpus revealed that substantial difference exists among speakers regarding the usage patterns of back-channelling. The patterns consist of back-channelling frequency, types, and expressions. They were used for system characterisation. Implemented system characters were tested by asking users of the dialogue system to identify the source speakers in the corpus. Experimental results suggest that possibility of using back-channelling patterns alone to characterize the dialogue system in some cases even among the same age and gender groups.
Team word-guessing games where one player, the clue-giver, gives clues attempting to elicit a target-word from another player, the receiver, are a popular form of entertainment and also used for educational purposes. Creating an engaging computational agent capable of emulating a talented human clue-giver in a timed word-guessing game depends on the ability to provide effective clues (clues able to elicit a correct guess from a human receiver). There are many available web resources and databases that can be mined for the raw material for clues for target-words; however, a large number of those clues are unlikely to be able to elicit a correct guess from a human guesser. In this paper, we propose a method for automatically filtering a clue corpus for effective clues for an arbitrary target-word from a larger set of potential clues, using machine learning on a set of features of the clues, including point-wise mutual information between a clue{'}s constituent words and a clue{'}s target-word. The results of the experiments significantly improve the average clue quality over previous approaches, and bring quality rates in-line with measures of human clue quality derived from a corpus of human-human interactions. The paper also introduces the data used to develop this method; audio recordings of people making guesses after having heard the clues being spoken by a synthesized voice.
In this paper, a novel approach is proposed to automatically construct parallel discourse corpus for dialogue machine translation. Firstly, the parallel subtitle data and its corresponding monolingual movie script data are crawled and collected from Internet. Then tags such as speaker and discourse boundary from the script data are projected to its subtitle data via an information retrieval approach in order to map monolingual discourse to bilingual texts. We not only evaluate the mapping results, but also integrate speaker information into the translation. Experiments show our proposed method can achieve 81.79{\%} and 98.64{\%} accuracy on speaker and dialogue boundary annotation, and speaker-based language model adaptation can obtain around 0.5 BLEU points improvement in translation qualities. Finally, we publicly release around 100K parallel discourse data with manual speaker and dialogue boundary annotation.
Automatic evaluation of Machine Translation (MT) is typically approached by measuring similarity between the candidate MT and a human reference translation. An important limitation of existing evaluation systems is that they are unable to distinguish candidate-reference differences that arise due to acceptable linguistic variation from the differences induced by MT errors. In this paper we present a new metric, UPF-Cobalt, that addresses this issue by taking into consideration the syntactic contexts of candidate and reference words. The metric applies a penalty when the words are similar but the contexts in which they occur are not equivalent. In this way, Machine Translations (MTs) that are different from the human translation but still essentially correct are distinguished from those that share high number of words with the reference but alter the meaning of the sentence due to translation errors. The results show that the method proposed is indeed beneficial for automatic MT evaluation. We report experiments based on two different evaluation tasks with various types of manual quality assessment. The metric significantly outperforms state-of-the-art evaluation systems in varying evaluation settings.
The usual concern when opting for a rule-based or a hybrid machine translation (MT) system is how much effort is required to adapt the system to a different language pair or a new domain. In this paper, we describe a way of adapting an existing hybrid MT system to a new language pair, and show that such a system can outperform a standard phrase-based statistical machine translation system with an average of 10 persons/month of work. This is specifically important in the case of domain-specific MT for which there is not enough parallel data for training a statistical machine translation system.
Word translations arise in dictionary-like organization as well as via machine learning from corpora. The former is exemplified by Wiktionary, a crowd-sourced dictionary with editions in many languages. {\'A}cs et al. (2013) obtain word translations from Wiktionary with the pivot-based method, also called triangulation, that infers word translations in a pair of languages based on translations to other, typically better resourced ones called pivots. Triangulation may introduce noise if words in the pivot are polysemous. The reliability of each triangulated translation is basically estimated by the number of pivot languages (Tanaka et al 1994). Mikolov et al (2013) introduce a method for generating or scoring word translations. Translation is formalized as a linear mapping between distributed vector space models (VSM) of the two languages. VSMs are trained on monolingual data, while the mapping is learned in a supervised fashion, using a seed dictionary of some thousand word pairs. The mapping can be used to associate existing translations with a real-valued similarity score. This paper exploits human labor in Wiktionary combined with distributional information in VSMs. We train VSMs on gigaword corpora, and the linear translation mapping on direct (non-triangulated) Wiktionary pairs. This mapping is used to filter triangulated translations based on scores. The motivation is that scores by the mapping may be a smoother measure of merit than considering only the number of pivot for the triangle. We evaluate the scores against dictionaries extracted from parallel corpora (Tiedemann 2012). We show that linear translation really provides a more reliable method for triangle scoring than pivot count. The methods we use are language-independent, and the training data is easy to obtain for many languages. We chose the German-Hungarian pair for evaluation, in which the filtered triangles resulting from our experiments are the greatest freely available list of word translations we are aware of.
This paper reports on an experiment where 795 human participants answered to the questions taken from second language proficiency tests that were translated to their native language. The output of three machine translation systems and two different human translations were used as the test material. We classified the translation errors in the questions according to an error taxonomy and analyzed the participants{'} response on the basis of the type and frequency of the translation errors. Through the analysis, we identified several types of errors that deteriorated most the accuracy of the participants{'} answers, their confidence on the answers, and their overall evaluation of the translation quality.
Although it is commonly assumed that word sense disambiguation (WSD) should help to improve lexical choice and improve the quality of machine translation systems, how to successfully integrate word senses into such systems remains an unanswered question. Some successful approaches have involved reformulating either WSD or the word senses it produces, but work on using traditional word senses to improve machine translation have met with limited success. In this paper, we build upon previous work that experimented on including word senses as contextual features in maxent-based translation models. Training on a large, open-domain corpus (Europarl), we demonstrate that this aproach yields significant improvements in machine translation from English to Portuguese.
This paper reports SuperCAT, a corpus analysis toolkit. It is a radical extension of SubCAT, the Sublanguage Corpus Analysis Toolkit, from sublanguage analysis to corpus analysis in general. The idea behind SuperCAT is that representative corpora have no tendency towards closure‚Äïthat is, they tend towards infinity. In contrast, non-representative corpora have a tendency towards closure‚Äïroughly, finiteness. SuperCAT focuses on general techniques for the quantitative description of the characteristics of any corpus (or other language sample), particularly concerning the characteristics of lexical distributions. Additionally, SuperCAT features a complete re-engineering of the previous SubCAT architecture.
The web data contains immense amount of data, hundreds of billion words are waiting to be extracted and used for language research. In this work we introduce our tool LanguageCrawl which allows NLP researchers to easily construct web-scale corpus from Common Crawl Archive: a petabyte scale, open repository of web crawl information. Three use-cases are presented: filtering Polish websites, building an N-gram corpora and training continuous skip-gram language model with hierarchical softmax. Each of them has been implemented within the LanguageCrawl toolkit, with the possibility to adjust specified language and N-gram ranks. Special effort has been put on high computing efficiency, by applying highly concurrent multitasking. We make our tool publicly available to enrich NLP resources. We strongly believe that our work will help to facilitate NLP research, especially in under-resourced languages, where the lack of appropriately sized corpora is a serious hindrance to applying data-intensive methods, such as deep neural networks.
The availability of large corpora for more and more languages enforces generic querying and standard interfaces. This development is especially relevant in the context of integrated research environments like CLARIN or DARIAH. The paper focuses on several applications and implementation details on the basis of a unified corpus format, a unique POS tag set, and prepared data for word similarities. All described data or applications are already or will be in the near future accessible via well-documented RESTful Web services. The target group are all kinds of interested persons with varying level of experience in programming or corpus query languages.
Several parallel corpora built from European Union language resources are presented here. They were processed by state-of-the-art tools and made available for researchers in the corpus manager Sketch Engine. A completely new resource is introduced: EUR-Lex Corpus, being one of the largest parallel corpus available at the moment, containing 840 million English tokens and the largest language pair English-French has more than 25 million aligned segments (paragraphs).
The present paper describes Corpus Query Lingua Franca (ISO CQLF), a specification designed at ISO Technical Committee 37 Subcommittee 4 {``}Language resource management{''} for the purpose of facilitating the comparison of properties of corpus query languages. We overview the motivation for this endeavour and present its aims and its general architecture. CQLF is intended as a multi-part specification; here, we concentrate on the basic metamodel that provides a frame that the other parts fit in.
The present paper describes the current release of the Bochum English Countability Lexicon (BECL 2.1), a large empirical database consisting of lemmata from Open ANC (http://www.anc.org) with added senses from WordNet (Fellbaum 1998). BECL 2.1 contains {\mbox{$\approx$}} 11,800 annotated noun-sense pairs, divided in four major countability classes and 18 fine-grained subclasses. In the current version, BECL also provides information on nouns whose senses occur in more than one class allowing a closer look on polysemy and homonymy with regard to countability. Further included are sets of similar senses using the Leacock and Chodorow (LCH) score for semantic similarity (Leacock {\&} Chodorow 1998), information on orthographic variation, on the completeness of all WordNet senses in the database and an annotated representation of different types of proper names. The further development of BECL will investigate the different countability classes of proper names and the general relation between semantic similarity and countability as well as recurring syntactic patterns for noun-sense pairs. The BECL 2.1 database is also publicly available via http://count-and-mass.org.
We propose a novel method for detecting optional arguments of Hungarian verbs using only positive data. We introduce a custom variant of collexeme analysis that explicitly models the noise in verb frames. Our method is, for the most part, unsupervised: we use the spectral clustering algorithm described in Brew and Schulte in Walde (2002) to build a noise model from a short, manually verified seed list of verbs. We experimented with both raw count- and context-based clusterings and found their performance almost identical. The code for our algorithm and the frame list are freely available at http://hlt.bme.hu/en/resources/tade.
We address the task of automatically correcting preposition errors in learners{'} Dutch by modelling preposition usage in native language. Specifically, we build two models exploiting a large corpus of Dutch. The first is a binary model for detecting whether a preposition should be used at all in a given position or not. The second is a multiclass model for selecting the appropriate preposition in case one should be used. The models are tested on native as well as learners data. For the latter we exploit a crowdsourcing strategy to elicit native judgements. On native test data the models perform very well, showing that we can model preposition usage appropriately. However, the evaluation on learners{'} data shows that while detecting that a given preposition is wrong is doable reasonably well, detecting the absence of a preposition is a lot more difficult. Observing such results and the data we deal with, we envisage various ways of improving performance, and report them in the final section of this article.
In this paper we describe 1) the process of converting a corpus of Dante Alighieri from a TEI XML format in to a pseudo-CoNLL format; 2) how a pos-tagger trained on modern Italian performs on Dante{'}s Italian 3) the performances of two different pos-taggers trained on the given corpus. We are making our conversion scripts and models available to the community. The two other models trained on the corpus performs reasonably well. The tool used for the conversion process might turn useful for bridging the gap between traditional digital humanities and modern NLP applications since the TEI original format is not usually suitable for being processed with standard NLP tools. We believe our work will serve both communities: the DH community will be able to tag new documents and the NLP world will have an easier way in converting existing documents to a standardized machine-readable format.
In this paper, we present a corpus of news blog conversations in Italian annotated with gold standard agreement/disagreement relations at message and sentence levels. This is the first resource of this kind in Italian. From the analysis of ADRs at the two levels emerged that agreement annotated at message level is consistent and generally reflected at sentence level, moreover, the argumentation structure of disagreement is more complex than agreement. The manual error analysis revealed that this resource is useful not only for the analysis of argumentation, but also for the detection of irony/sarcasm in online debates. The corpus and annotation tool are available for research purposes on request.
We introduce an approach based on using the dependency grammar representations of sentences to compute sentence similarity for extractive multi-document summarization. We adapt and investigate the effects of two untyped dependency tree kernels, which have originally been proposed for relation extraction, to the multi-document summarization problem. In addition, we propose a series of novel dependency grammar based kernels to better represent the syntactic and semantic similarities among the sentences. The proposed methods incorporate the type information of the dependency relations for sentence similarity calculation. To our knowledge, this is the first study that investigates using dependency tree based sentence similarity for multi-document summarization.
In this paper, we focus on the verb-particle (V-Prt) split construction in English and German and its difficulty for parsing and Machine Translation (MT). For German, we use an existing test suite of V-Prt split constructions, while for English, we build a new and comparable test suite from raw data. These two data sets are then used to perform an analysis of errors in dependency parsing, word-level alignment and MT, which arise from the discontinuous order in V-Prt split constructions. In the automatic alignments of parallel corpora, most of the particles align to NULL. These mis-alignments and the inability of phrase-based MT system to recover discontinuous phrases result in low quality translations of V-Prt split constructions both in English and German. However, our results show that the V-Prt split phrases are correctly parsed in 90{\%} of cases, suggesting that syntactic-based MT should perform better on these constructions. We evaluate a syntactic-based MT system on German and compare its performance to the phrase-based system.
We report on the creation of a lexical resource for the identification of potentially unspecific or imprecise constructions in German requirements documentation from the car manufacturing industry. In requirements engineering, such expressions are called {``}weak words{''}: they are not sufficiently precise to ensure an unambiguous interpretation by the contractual partners, who for the definition of their cooperation, typically rely on specification documents (Melchisedech, 2000); an example are dimension adjectives, such as kurz or lang ({`}short{'}, {`}long{'}) which need to be modified by adverbials indicating the exact duration, size etc. Contrary to standard practice in requirements engineering, where the identification of such weak words is merely based on stopword lists, we identify weak uses in context, by querying annotated text. The queries are part of the resource, as they define the conditions when a word use is weak. We evaluate the recognition of weak uses on our development corpus and on an unseen evaluation corpus, reaching stable F1-scores above 0.95.
The granularity of PolNet (Polish Wordnet) is the main theoretical issue discussed in the paper. We describe the latest extension of PolNet including valency information of simple verbs and noun-verb collocations using manual and machine-assisted methods. Valency is defined to include both semantic and syntactic selectional restrictions. We assume the valency structure of a verb to be an index of meaning. Consistently we consider it an attribute of a synset. Strict application of this principle results in fine granularity of the verb section of the wordnet. Considering valency as a distinctive feature of synsets was an essential step to transform the initial PolNet (first intended as a lexical ontology) into a lexicon-grammar. For the present refinement of PolNet we assume that the category of language register is a part of meaning. The totality of PolNet 2.0 synsets is being revised in order to split the PolNet 2.0 synsets that contain different register words into register-uniform sub-synsets. We completed this operation for synsets that were used as values of semantic roles. The operation augmented the number of considered synsets by 29{\%}. In the paper we report an extension of the class of collocation-based verb synsets.
This paper presents C-WEP, the Collection of Writing Errors by Professionals Writers of German. It currently consists of 245 sentences with grammatical errors. All sentences are taken from published texts. All authors are professional writers with high skill levels with respect to German, the genres, and the topics. The purpose of this collection is to provide seeds for more sophisticated writing support tools as only a very small proportion of those errors can be detected by state-of-the-art checkers. C-WEP is annotated on various levels and freely available.
In this paper, we describe an addition to the corpus query system Kontext that enables to enhance the search using syntactic attributes in addition to the existing features, mainly lemmas and morphological categories. We present the enhancements of the corpus query system itself, the attributes we use to represent syntactic structures in data, and some examples of querying the syntactically annotated corpora, such as treebanks in various languages as well as an automatically parsed large corpus.
Starting from the English affective lexicon ANEW (Bradley and Lang, 1999a) we have created the first Greek affective lexicon. It contains human ratings for the three continuous affective dimensions of valence, arousal and dominance for 1034 words. The Greek affective lexicon is compared with affective lexica in English, Spanish and Portuguese. The lexicon is automatically expanded by selecting a small number of manually annotated words to bootstrap the process of estimating affective ratings of unknown words. We experimented with the parameters of the semantic-affective model in order to investigate their impact to its performance, which reaches 85{\%} binary classification accuracy (positive vs. negative ratings). We share the Greek affective lexicon that consists of 1034 words and the automatically expanded Greek affective lexicon that contains 407K words.
In this paper we present a Hungarian sentiment corpus manually annotated at aspect level. Our corpus consists of Hungarian opinion texts written about different types of products. The main aim of creating the corpus was to produce an appropriate database providing possibilities for developing text mining software tools. The corpus is a unique Hungarian database: to the best of our knowledge, no digitized Hungarian sentiment corpus that is annotated on the level of fragments and targets has been made so far. In addition, many language elements of the corpus, relevant from the point of view of sentiment analysis, got distinct types of tags in the annotation. In this paper, on the one hand, we present the method of annotation, and we discuss the difficulties concerning text annotation process. On the other hand, we provide some quantitative and qualitative data on the corpus. We conclude with a description of the applicability of the corpus.
Sentiment analysis has so far focused on the detection of explicit opinions. However, of late implicit opinions have received broader attention, the key idea being that the evaluation of an event type by a speaker depends on how the participants in the event are valued and how the event itself affects the participants. We present an annotation scheme for adding relevant information, couched in terms of so-called effect functors, to German lexical items. Our scheme synthesizes and extends previous proposals. We report on an inter-annotator agreement study. We also present results of a crowdsourcing experiment to test the utility of some known and some new functors for opinion inference where, unlike in previous work, subjects are asked to reason from event evaluation to participant evaluation.
In this paper, a German verb resource for verb-centered sentiment inference is introduced and evaluated. Our model specifies verb polarity frames that capture the polarity effects on the fillers of the verb{'}s arguments given a sentence with that verb frame. Verb signatures and selectional restrictions are also part of the model. An algorithm to apply the verb resource to treebank sentences and the results of our first evaluation are discussed.
In this paper we present the TWitterBuonaScuola corpus (TW-BS), a novel Italian linguistic resource for Sentiment Analysis, developed with the main aim of analyzing the online debate on the controversial Italian political reform {``}Buona Scuola{''} (Good school), aimed at reorganizing the national educational and training systems. We describe the methodologies applied in the collection and annotation of data. The collection has been driven by the detection of the hashtags mainly used by the participants to the debate, while the annotation has been focused on sentiment polarity and irony, but also extended to mark the aspects of the reform that were mainly discussed in the debate. An in-depth study of the disagreement among annotators is included. We describe the collection and annotation stages, and the in-depth analysis of disagreement made with Crowdflower, a crowdsourcing annotation platform.
This paper presents NileULex, which is an Arabic sentiment lexicon containing close to six thousands Arabic words and compound phrases. Forty five percent of the terms and expressions in the lexicon are Egyptian or colloquial while fifty five percent are Modern Standard Arabic. While the collection of many of the terms included in the lexicon was done automatically, the actual addition of any term was done manually. One of the important criterions for adding terms to the lexicon, was that they be as unambiguous as possible. The result is a lexicon with a much higher quality than any translated variant or automatically constructed one. To demonstrate that a lexicon such as this can directly impact the task of sentiment analysis, a very basic machine learning based sentiment analyser that uses unigrams, bigrams, and lexicon based features was applied on two different Twitter datasets. The obtained results were compared to a baseline system that only uses unigrams and bigrams. The same lexicon based features were also generated using a publicly available translation of a popular sentiment lexicon. The experiments show that usage of the developed lexicon improves the results over both the baseline and the publicly available lexicon.
The paper contains a description of OPFI: Opinion Finder for the Polish Language, a freely available tool for opinion target extraction. The goal of the tool is opinion finding: a task of identifying tuples composed of sentiment (positive or negative) and its target (about what or whom is the sentiment expressed). OPFI is not dependent on any particular method of sentiment identification and provides a built-in sentiment dictionary as a convenient option. Technically, it contains implementations of three different modes of opinion tuple generation: one hybrid based on dependency parsing and CRF, the second based on shallow parsing and the third on deep learning, namely GRU neural network. The paper also contains a description of related language resources: two annotated treebanks and one set of tweets.
The fine-grained task of automatically detecting all sentiment expressions within a given document and the aspects to which they refer is known as aspect-based sentiment analysis. In this paper we present the first full aspect-based sentiment analysis pipeline for Dutch and apply it to customer reviews. To this purpose, we collected reviews from two different domains, i.e. restaurant and smartphone reviews. Both corpora have been manually annotated using newly developed guidelines that comply to standard practices in the field. For our experimental pipeline we perceive aspect-based sentiment analysis as a task consisting of three main subtasks which have to be tackled incrementally: aspect term extraction, aspect category classification and polarity classification. First experiments on our Dutch restaurant corpus reveal that this is indeed a feasible approach that yields promising results.
We construct a case-based English-to-Chinese semantic constituent parallel Treebank for a Statistical Machine Translation (SMT) task by labelling each node of the Deep Syntactic Tree (DST) with our refined semantic cases. Since subtree span-crossing is harmful in tree-based SMT, DST is adopted to alleviate this problem. At the same time, we tailor an existing case set to represent bilingual shallow semantic relations more precisely. This Treebank is a part of a semantic corpus building project, which aims to build a semantic bilingual corpus annotated with syntactic, semantic cases and word senses. Data in our Treebank is from the news domain of Datum corpus. 4,000 sentence pairs are selected to cover various lexicons and part-of-speech (POS) n-gram patterns as much as possible. This paper presents the construction of this case Treebank. Also, we have tested the effect of adopting DST structure in alleviating subtree span-crossing. Our preliminary analysis shows that the compatibility between Chinese and English trees can be significantly increased by transforming the parse-tree into the DST. Furthermore, the human agreement rate in annotation is found to be acceptable (90{\%} in English nodes, 75{\%} in Chinese nodes).
Morphologically-rich languages pose problems for machine translation (MT) systems, including word-alignment errors, data sparsity and multiple affixes. Current alignment models at word-level do not distinguish words and morphemes, thus yielding low-quality alignment and subsequently affecting end translation quality. Models using morpheme-level alignment can reduce the vocabulary size of morphologically-rich languages and overcomes data sparsity. The alignment data based on smallest units reveals subtle language features and enhances translation quality. Recent research proves such morpheme-level alignment (MA) data to be valuable linguistic resources for SMT, particularly for languages with rich morphology. In support of this research trend, the Linguistic Data Consortium (LDC) created Uzbek-English and Turkish-English alignment data which are manually aligned at the morpheme level. This paper describes the creation of MA corpora, including alignment and tagging process and approaches, highlighting annotation challenges and specific features of languages with rich morphology. The light tagging annotation on the alignment layer adds extra value to the MA data, facilitating users in flexibly tailoring the data for various MT model training.
Parallel corpora are crucial for machine translation (MT), however they are quite scarce for most language pairs and domains. As comparable corpora are far more available, many studies have been conducted to extract parallel sentences from them for MT. In this paper, we exploit the neural network features acquired from neural MT for parallel sentence extraction. We observe significant improvements for both accuracy in sentence extraction and MT performance.
The biomedical scientific literature is a rich source of information not only in the English language, for which it is more abundant, but also in other languages, such as Portuguese, Spanish and French. We present the first freely available parallel corpus of scientific publications for the biomedical domain. Documents from the {''}Biological Sciences{''} and {''}Health Sciences{''} categories were retrieved from the Scielo database and parallel titles and abstracts are available for the following language pairs: Portuguese/English (about 86,000 documents in total), Spanish/English (about 95,000 documents) and French/English (about 2,000 documents). Additionally, monolingual data was also collected for all four languages. Sentences in the parallel corpus were automatically aligned and a manual analysis of 200 documents by native experts found that a minimum of 79{\%} of sentences were correctly aligned in all language pairs. We demonstrate the utility of the corpus by running baseline machine translation experiments. We show that for all language pairs, a statistical machine translation system trained on the parallel corpora achieves performance that rivals or exceeds the state of the art in the biomedical domain. Furthermore, the corpora are currently being used in the biomedical task in the First Conference on Machine Translation (WMT{'}16).
This paper presents an approach for building large monolingual corpora and, at the same time, extracting parallel data by crawling the top-level domain of a given language of interest. For gathering linguistically relevant data from top-level domains we use the SpiderLing crawler, modified to crawl data written in multiple languages. The output of this process is then fed to Bitextor, a tool for harvesting parallel data from a collection of documents. We call the system combining these two tools Spidextor, a blend of the names of its two crucial parts. We evaluate the described approach intrinsically by measuring the accuracy of the extracted bitexts from the Croatian top-level domain {``}.hr{''} and the Slovene top-level domain {``}.si{''}, and extrinsically on the English-Croatian language pair by comparing an SMT system built from the crawled data with third-party systems. We finally present parallel datasets collected with our approach for the English-Croatian, English-Finnish, English-Serbian and English-Slovene language pairs.
We describe a strategy for the acquisition of training data necessary to build a social-media-driven early detection system for individuals at risk for (preventable) type 2 diabetes mellitus (T2DM). The strategy uses a game-like quiz with data and questions acquired semi-automatically from Twitter. The questions are designed to inspire participant engagement and collect relevant data to train a public-health model applied to individuals. Prior systems designed to use social media such as Twitter to predict obesity (a risk factor for T2DM) operate on entire communities such as states, counties, or cities, based on statistics gathered by government agencies. Because there is considerable variation among individuals within these groups, training data on the individual level would be more effective, but this data is difficult to acquire. The approach proposed here aims to address this issue. Our strategy has two steps. First, we trained a random forest classifier on data gathered from (public) Twitter statuses and state-level statistics with state-of-the-art accuracy. We then converted this classifier into a 20-questions-style quiz and made it available online. In doing so, we achieved high engagement with individuals that took the quiz, while also building a training set of voluntarily supplied individual-level data for future classification.
We set out to investigate whether TV ratings and mentions of TV programmes on the Twitter social media platform are correlated. If such a correlation exists, Twitter may be used as an alternative source for estimating viewer popularity. Moreover, the Twitter-based rating estimates may be generated during the programme, or even before. We count the occurrences of programme-specific hashtags in an archive of Dutch tweets of eleven popular TV shows broadcast in the Netherlands in one season, and perform correlation tests. Overall we find a strong correlation of 0.82; the correlation remains strong, 0.79, if tweets are counted a half hour before broadcast time. However, the two most popular TV shows account for most of the positive effect; if we leave out the single and second most popular TV shows, the correlation drops to being moderate to weak. Also, within a TV show, correlations between ratings and tweet counts are mostly weak, while correlations between TV ratings of the previous and next shows are strong. In absence of information on previous shows, Twitter-based counts may be a viable alternative to classic estimation methods for TV ratings. Estimates are more reliable with more popular TV shows.
In this paper we consider the problem of out-of-vocabulary term classification in web forum text from the automotive domain. We develop a set of nine domain- and application-specific categories for out-of-vocabulary terms. We then propose a supervised approach to classify out-of-vocabulary terms according to these categories, drawing on features based on word embeddings, and linguistic knowledge of common properties of out-of-vocabulary terms. We show that the features based on word embeddings are particularly informative for this task. The categories that we predict could serve as a preliminary, automatically-generated source of lexical knowledge about out-of-vocabulary terms. Furthermore, we show that this approach can be adapted to give a semi-automated method for identifying out-of-vocabulary terms of a particular category, automotive named entities, that is of particular interest to us.
Many people post about their daily life on social media. These posts may include information about the purchase activity of people, and insights useful to companies can be derived from them: e.g. profile information of a user who mentioned something about their product. As a further advanced analysis, we consider extracting users who are likely to buy a product from the set of users who mentioned that the product is attractive. In this paper, we report our methodology for building a corpus for Twitter user purchase behavior prediction. First, we collected Twitter users who posted a want phrase + product name: e.g. {``}want a Xperia{''} as candidate want users, and also candidate bought users in the same way. Then, we asked an annotator to judge whether a candidate user actually bought a product. We also annotated whether tweets randomly sampled from want/bought user timelines are relevant or not to purchase. In this annotation, 58{\%} of want user tweets and 35{\%} of bought user tweets were annotated as relevant. Our data indicate that information embedded in timeline tweets can be used to predict purchase behavior of tweeted products.
Hashtags, which are commonly composed of multiple words, are increasingly used to convey the actual messages in tweets. Understanding what tweets are saying is getting more dependent on understanding hashtags. Therefore, identifying the individual words that constitute a hashtag is an important, yet a challenging task due to the abrupt nature of the language used in tweets. In this study, we introduce a feature-rich approach based on using supervised machine learning methods to segment hashtags. Our approach is unsupervised in the sense that instead of using manually segmented hashtags for training the machine learning classifiers, we automatically create our training data by using tweets as well as by automatically extracting hashtag segmentations from a large corpus. We achieve promising results with such automatically created noisy training data.
Language varies not only between countries, but also along regional and socio-demographic lines. This variation is one of the driving factors behind language change. However, investigating language variation is a complex undertaking: the more factors we want to consider, the more data we need. Traditional qualitative methods are not well-suited to do this, an therefore restricted to isolated factors. This reduction limits the potential insights, and risks attributing undue importance to easily observed factors. While there is a large interest in linguistics to increase the quantitative aspect of such studies, it requires training in both variational linguistics and computational methods, a combination that is still not common. We take a first step here to alleviating the problem by providing an interface, www.languagevariation.com, to explore large-scale language variation along multiple socio-demographic factors {--} without programming knowledge. It makes use of large amounts of data and provides statistical analyses, maps, and interactive features that will enable scholars to explore language variation in a data-driven way.
Much research has focused on detecting trends on Twitter, including health-related trends such as mentions of Influenza-like illnesses or their symptoms. The majority of this research has been conducted using Twitter{'}s public feed, which includes only about 1{\%} of all public tweets. It is unclear if, when, and how using Twitter{'}s 1{\%} feed has affected the evaluation of trend detection methods. In this work we use a larger feed to investigate the effects of sampling on Twitter trend detection. We focus on using health-related trends to estimate the prevalence of Influenza-like illnesses based on tweets. We use ground truth obtained from the CDC and Google Flu Trends to explore how the prevalence estimates degrade when moving from a 100{\%} to a 1{\%} sample. We find that using the 1{\%} sample is unlikely to substantially harm ILI estimates made at the national level, but can cause poor performance when estimates are made at the city level.
In this paper, we present a study on tweet classification which aims to define the communication behavior of the 103 French museums that participated in 2014 in the Twitter operation: MuseumWeek. The tweets were automatically classified in four communication categories: sharing experience, promoting participation, interacting with the community, and promoting-informing about the institution. Our classification is multi-class. It combines Support Vector Machines and Naive Bayes methods and is supported by a selection of eighteen subtypes of features of four different kinds: metadata information, punctuation marks, tweet-specific and lexical features. It was tested against a corpus of 1,095 tweets manually annotated by two experts in Natural Language Processing and Information Communication and twelve Community Managers of French museums. We obtained an state-of-the-art result of F1-score of 72{\%} by 10-fold cross-validation. This result is very encouraging since is even better than some state-of-the-art results found in the tweet classification literature.
Word sense induction (WSI) seeks to induce senses of words from unannotated corpora. In this paper, we address the WSI task for the Croatian language. We adopt the word clustering approach based on co-occurrence graphs, in which senses are taken to correspond to strongly inter-connected components of co-occurring words. We experiment with a number of graph construction techniques and clustering algorithms, and evaluate the sense inventories both as a clustering problem and extrinsically on a word sense disambiguation (WSD) task. In the cluster-based evaluation, Chinese Whispers algorithm outperformed Markov Clustering, yielding a normalized mutual information score of 64.3. In contrast, in WSD evaluation Markov Clustering performed better, yielding an accuracy of about 75{\%}. We are making available two induced sense inventories of 10,000 most frequent Croatian words: one coarse-grained and one fine-grained inventory, both obtained using the Markov Clustering algorithm.
We describe the word sense annotation layer in \textit{Eukalyptus}, a freely available five-domain corpus of contemporary Swedish with several annotation layers. The annotation uses the SALDO lexicon to define the sense inventory, and allows word sense annotation of compound segments and multiword units. We give an overview of the new annotation tool developed for this project, and finally present an analysis of the inter-annotator agreement between two annotators.
This work presents parallel corpora automatically annotated with several NLP tools, including lemma and part-of-speech tagging, named-entity recognition and classification, named-entity disambiguation, word-sense disambiguation, and coreference. The corpora comprise both the well-known Europarl corpus and a domain-specific question-answer troubleshooting corpus on the IT domain. English is common in all parallel corpora, with translations in five languages, namely, Basque, Bulgarian, Czech, Portuguese and Spanish. We describe the annotated corpora and the tools used for annotation, as well as annotation statistics for each language. These new resources are freely available and will help research on semantic processing for machine translation and cross-lingual transfer.
We present a VerbNet-based annotation scheme for semantic roles that we explore in an annotation study on German language data that combines word sense and semantic role annotation. We reannotate a substantial portion of the SALSA corpus with GermaNet senses and a revised scheme of VerbNet roles. We provide a detailed evaluation of the interaction between sense and role annotation. The resulting corpus will allow us to compare VerbNet role annotation for German to FrameNet and PropBank annotation by mapping to existing role annotations on the SALSA corpus. We publish the annotated corpus and detailed guidelines for the new role annotation scheme.
Word Sense Disambiguation (WSD) is one of the open problems in the area of natural language processing. Various supervised, unsupervised and knowledge based approaches have been proposed for automatically determining the sense of a word in a particular context. It has been observed that such approaches often find it difficult to beat the WordNet First Sense (WFS) baseline which assigns the sense irrespective of context. In this paper, we present our work on creating the WFS baseline for Hindi language by manually ranking the synsets of Hindi WordNet. A ranking tool is developed where human experts can see the frequency of the word senses in the sense-tagged corpora and have been asked to rank the senses of a word by using this information and also his/her intuition. The accuracy of WFS baseline is tested on several standard datasets. F-score is found to be 60{\%}, 65{\%} and 55{\%} on Health, Tourism and News datasets respectively. The created rankings can also be used in other NLP applications viz., Machine Translation, Information Retrieval, Text Summarization, etc.
In this paper, a new approach towards semantic clustering of the results of ambiguous search queries is presented. We propose using distributed vector representations of words trained with the help of prediction-based neural embedding models to detect senses of search queries and to cluster search engine results page according to these senses. The words from titles and snippets together with semantic relationships between them form a graph, which is further partitioned into components related to different query senses. This approach to search engine results clustering is evaluated against a new manually annotated evaluation data set of Russian search queries. We show that in the task of semantically clustering search results, prediction-based models slightly but stably outperform traditional count-based ones, with the same training corpora.
This paper describes the evaluation methodology followed to measure the impact of using a machine learning algorithm to automatically segment intralingual subtitles. The segmentation quality, productivity and self-reported post-editing effort achieved with such approach are shown to improve those obtained by the technique based in counting characters, mainly employed for automatic subtitle segmentation currently. The corpus used to train and test the proposed automated segmentation method is also described and shared with the community, in order to foster further research in this area.
Images naturally appear alongside text in a wide variety of media, such as books, magazines, newspapers, and in online articles. This type of multi-modal data offers an interesting basis for vision and language research but most existing datasets use crowdsourced text, which removes the images from their original context. In this paper, we introduce the KBK-1M dataset of 1.6 million images in their original context, with co-occurring texts found in Dutch newspapers from 1922 - 1994. The images are digitally scanned photographs, cartoons, sketches, and weather forecasts; the text is generated from OCR scanned blocks. The dataset is suitable for experiments in automatic image captioning, image‚Äïarticle matching, object recognition, and data-to-text generation for weather forecasting. It can also be used by humanities scholars to analyse photographic style changes, the representation of people and societal issues, and new tools for exploring photograph reuse via image-similarity-based search.
The task of automatically generating sentential descriptions of image content has become increasingly popular in recent years, resulting in the development of large-scale image description datasets and the proposal of various metrics for evaluating image description generation systems. However, not much work has been done to analyse and understand both datasets and the metrics. In this paper, we propose using a leave-one-out cross validation (LOOCV) process as a means to analyse multiply annotated, human-authored image description datasets and the various evaluation metrics, i.e. evaluating one image description against other human-authored descriptions of the same image. Such an evaluation process affords various insights into the image description datasets and evaluation metrics, such as the variations of image descriptions within and across datasets and also what the metrics capture. We compute and analyse (i) human upper-bound performance; (ii) ranked correlation between metric pairs across datasets; (iii) lower-bound performance by comparing a set of descriptions describing one image to another sentence not describing that image. Interesting observations are made about the evaluation metrics and image description datasets, and we conclude that such cross-validation methods are extremely useful for assessing and gaining insights into image description datasets and evaluation metrics for image descriptions.
In American Sign Language (ASL) as well as other signed languages, different classes of signs (e.g., lexical signs, fingerspelled signs, and classifier constructions) have different internal structural properties. Continuous sign recognition accuracy can be improved through use of distinct recognition strategies, as well as different training datasets, for each class of signs. For these strategies to be applied, continuous signing video needs to be segmented into parts corresponding to particular classes of signs. In this paper we present a multiple instance learning-based segmentation system that accurately labels 91.27{\%} of the video frames of 500 continuous utterances (including 7 different subjects) from the publicly accessible NCSLGR corpus (Neidle and Vogler, 2012). The system uses novel feature descriptors derived from both motion and shape statistics of the regions of high local motion. The system does not require a hand tracker.
Lexical Simplification is the task of replacing complex words in a text with simpler alternatives. A variety of strategies have been devised for this challenge, yet there has been little effort in comparing their performance. In this contribution, we present a benchmarking of several Lexical Simplification systems. By combining resources created in previous work with automatic spelling and inflection correction techniques, we introduce BenchLS: a new evaluation dataset for the task. Using BenchLS, we evaluate the performance of solutions for various steps in the typical Lexical Simplification pipeline, both individually and jointly. This is the first time Lexical Simplification systems are compared in such fashion on the same data, and the findings introduce many contributions to the field, revealing several interesting properties of the systems evaluated.
Scientific literature records the research process with a standardized structure and provides the clues to track the progress in a scientific field. Understanding its internal structure and content is of paramount importance for natural language processing (NLP) technologies. To meet this requirement, we have developed a multi-layered annotated corpus of scientific papers in the domain of Computer Graphics. Sentences are annotated with respect to their role in the argumentative structure of the discourse. The purpose of each citation is specified. Special features of the scientific discourse such as advantages and disadvantages are identified. In addition, a grade is allocated to each sentence according to its relevance for being included in a summary.To the best of our knowledge, this complex, multi-layered collection of annotations and metadata characterizing a set of research papers had never been grouped together before in one corpus and therefore constitutes a newer, richer resource with respect to those currently available in the field.
In this paper we report a comparison of various techniques for single-document extractive summarization under strict length budgets, which is a common commercial use case (e.g. summarization of news articles by news aggregators). We show that, evaluated using ROUGE, numerous algorithms from the literature fail to beat a simple lead-based baseline for this task. However, a supervised approach with lightweight and efficient features improves over the lead-based baseline. Additional human evaluation demonstrates that the supervised approach also performs competitively with a commercial system that uses more sophisticated features.
Automatic summarization of reader comments in on-line news is an extremely challenging task and a capability for which there is a clear need. Work to date has focussed on producing extractive summaries using well-known techniques imported from other areas of language processing. But are extractive summaries of comments what users really want? Do they support users in performing the sorts of tasks they are likely to want to perform with reader comments? In this paper we address these questions by doing three things. First, we offer a specification of one possible summary type for reader comment, based on an analysis of reader comment in terms of issues and viewpoints. Second, we define a task-based evaluation framework for reader comment summarization that allows summarization systems to be assessed in terms of how well they support users in a time-limited task of identifying issues and characterising opinion on issues in comments. Third, we describe a pilot evaluation in which we used the task-based evaluation framework to evaluate a prototype reader comment clustering and summarization system, demonstrating the viability of the evaluation framework and illustrating the sorts of insight such an evaluation affords.
Unsupervised learning of morphological segmentation of words in a language, based only on a large corpus of words, is a challenging task. Evaluation of the learned segmentations is a challenge in itself, due to the inherent ambiguity of the segmentation task. There is no way to posit unique {``}correct{''} segmentation for a set of data in an objective way. Two models may arrive at different ways of segmenting the data, which may nonetheless both be valid. Several evaluation methods have been proposed to date, but they do not insist on consistency of the evaluated model. We introduce a new evaluation methodology, which enforces correctness of segmentation boundaries while also assuring consistency of segmentation decisions across the corpus.
Bilingual lexicon extraction from comparable corpora is usually based on distributional methods when dealing with single word terms (SWT). These methods often treat SWT as single tokens without considering their compositional property. However, many SWT are compositional (composed of roots and affixes) and this information, if taken into account can be very useful to match translational pairs, especially for infrequent terms where distributional methods often fail. For instance, the English compound \textit{xenograft} which is composed of the root \textit{xeno} and the lexeme \textit{graft} can be translated into French compositionally by aligning each of its elements (\textit{xeno} with \textit{x{\'e}no} and \textit{graft} with \textit{greffe}) resulting in the translation: \textit{x{\'e}nogreffe}. In this paper, we experiment several distributional modellings at the morpheme level that we apply to perform compositional translation to a subset of French and English compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level.
Structured, complete inflectional paradigm data exists for very few of the world{'}s languages, but is crucial to training morphological analysis tools. We present methods inspired by linguistic fieldwork for gathering inflectional paradigm data in a machine-readable, interoperable format from remotely-located speakers of any language. Informants are tasked with completing language-specific paradigm elicitation templates. Templates are constructed by linguists using grammatical reference materials to ensure completeness. Each cell in a template is associated with contextual prompts designed to help informants with varying levels of linguistic expertise (from professional translators to untrained native speakers) provide the desired inflected form. To facilitate downstream use in interoperable NLP/HLT applications, each cell is also associated with a language-independent machine-readable set of morphological tags from the UniMorph Schema. This data is useful for seeding morphological analysis and generation software, particularly when the data is representative of the range of surface morphological variation in the language. At present, we have obtained 792 lemmas and 25,056 inflected forms from 15 languages.
Wiktionary is a large-scale resource for cross-lingual lexical information with great potential utility for machine translation (MT) and many other NLP tasks, especially automatic morphological analysis and generation. However, it is designed primarily for human viewing rather than machine readability, and presents numerous challenges for generalized parsing and extraction due to a lack of standardized formatting and grammatical descriptor definitions. This paper describes a large-scale effort to automatically extract and standardize the data in Wiktionary and make it available for use by the NLP research community. The methodological innovations include a multidimensional table parsing algorithm, a cross-lexeme, token-frequency-based method of separating inflectional form data from grammatical descriptors, the normalization of grammatical descriptors to a unified annotation scheme that accounts for cross-linguistic diversity, and a verification and correction process that exploits within-language, cross-lexeme table format consistency to minimize human effort. The effort described here resulted in the extraction of a uniquely large normalized resource of nearly 1,000,000 inflectional paradigms across 350 languages. Evaluation shows that even though the data is extracted using a language-independent approach, it is comparable in quantity and quality to data extracted using hand-tuned, language-specific approaches.
Users will interact with an individual app on smart devices (e.g., phone, TV, car) to fulfill a specific goal (e.g. find a photographer), but users may also pursue more complex tasks that will span multiple domains and apps (e.g. plan a wedding ceremony). Planning and executing such multi-app tasks are typically managed by users, considering the required global context awareness. To investigate how users arrange domains/apps to fulfill complex tasks in their daily life, we conducted a user study on 14 participants to collect such data from their Android smart phones. This document 1) summarizes the techniques used in the data collection and 2) provides a brief statistical description of the data. This data guilds the future direction for researchers in the fields of conversational agent and personal assistant, etc. This data is available at http://AppDialogue.com.
The paper describes experimental dialogue data collection activities, as well semantically annotated corpus creation undertaken within EU-funded METALOGUE project(www.metalogue.eu). The project aims to develop a dialogue system with flexible dialogue management to enable system{'}s adaptive, reactive, interactive and proactive dialogue behavior in setting goals, choosing appropriate strategies and monitoring numerous parallel interpretation and management processes. To achieve these goals negotiation (or more precisely multi-issue bargaining) scenario has been considered as the specific setting and application domain. The dialogue corpus forms the basis for the design of task and interaction models of participants negotiation behavior, and subsequently for dialogue system development which would be capable to replace one of the negotiators. The METALOGUE corpus will be released to the community for research purposes.
Annotated in-domain corpora are crucial to the successful development of dialogue systems of automated agents, and in particular for developing natural language understanding (NLU) components of such systems. Unfortunately, such important resources are scarce. In this work, we introduce an annotated natural language human-agent dialogue corpus in the negotiation domain. The corpus was collected using Amazon Mechanical Turk following the {`}Wizard-Of-Oz{'} approach, where a {`}wizard{'} human translates the participants{'} natural language utterances in real time into a semantic language. Once dialogue collection was completed, utterances were annotated with intent labels by two independent annotators, achieving high inter-annotator agreement. Our initial experiments with an SVM classifier show that automatically inferring such labels from the utterances is far from trivial. We make our corpus publicly available to serve as an aid in the development of dialogue systems for negotiation agents, and suggest that analogous corpora can be created following our methodology and using our available source code. To the best of our knowledge this is the first publicly available negotiation dialogue corpus.
Dialogue breakdown detection is a promising technique in dialogue systems. To promote the research and development of such a technique, we organized a dialogue breakdown detection challenge where the task is to detect a system{'}s inappropriate utterances that lead to dialogue breakdowns in chat. This paper describes the design, datasets, and evaluation metrics for the challenge as well as the methods and results of the submitted runs of the participants.
This paper presents the DialogBank, a new language resource consisting of dialogues with gold standard annotations according to the ISO 24617-2 standard. Some of these dialogues have been taken from existing corpora and have been re-annotated according to the ISO standard; others have been annotated directly according to the standard. The ISO 24617-2 annotations have been designed according to the ISO principles for semantic annotation, as formulated in ISO 24617-6. The DialogBank makes use of three alternative representation formats, which are shown to be interoperable.
The Artwalk Corpus is a collection of 48 mobile phone conversations between 24 pairs of friends and 24 pairs of strangers performing a novel, naturalistically-situated referential communication task. This task produced dialogues which, on average, are just under 40 minutes. The task requires the identification of public art while walking around and navigating pedestrian routes in the downtown area of Santa Cruz, California. The task involves a Director on the UCSC campus with access to maps providing verbal instructions to a Follower executing the task. The task provides a setting for real-world situated dialogic language and is designed to: (1) elicit entrainment and coordination of referring expressions between the dialogue participants, (2) examine the effect of friendship on dialogue strategies, and (3) examine how the need to complete the task while negotiating myriad, unanticipated events in the real world ‚Äï such as avoiding cars and other pedestrians ‚Äï affects linguistic coordination and other dialogue behaviors. Previous work on entrainment and coordinating communication has primarily focused on similar tasks in laboratory settings where there are no interruptions and no need to navigate from one point to another in a complex space. The corpus provides a general resource for studies on how coordinated task-oriented dialogue changes when we move outside the laboratory and into the world. It can also be used for studies of entrainment in dialogue, and the form and style of pedestrian instruction dialogues, as well as the effect of friendship on dialogic behaviors.
We introduce a dialogue task between a virtual patient and a doctor where the dialogue system, playing the patient part in a simulated consultation, must reconcile a specialized level, to understand what the doctor says, and a lay level, to output realistic patient-language utterances. This increases the challenges in the analysis and generation phases of the dialogue. This paper proposes methods to manage linguistic and terminological variation in that situation and illustrates how they help produce realistic dialogues. Our system makes use of lexical resources for processing synonyms, inflectional and derivational variants, or pronoun/verb agreement. In addition, specialized knowledge is used for processing medical roots and affixes, ontological relations and concept mapping, and for generating lay variants of terms according to the patient{'}s non-expert discourse. We also report the results of a first evaluation carried out by 11 users interacting with the system. We evaluated the non-contextual analysis module, which supports the Spoken Language Understanding step. The annotation of task domain entities obtained 91.8{\%} of Precision, 82.5{\%} of Recall, 86.9{\%} of F-measure, 19.0{\%} of Slot Error Rate, and 32.9{\%} of Sentence Error Rate.
We present a corpus of virtual patient dialogues to which we have added manually annotated gold standard word alignments. Since each question asked by a medical student in the dialogues is mapped to a canonical, anticipated version of the question, the corpus implicitly defines a large set of paraphrase (and non-paraphrase) pairs. We also present a novel process for selecting the most useful data to annotate with word alignments and for ensuring consistent paraphrase status decisions. In support of this process, we have enhanced the earlier Edinburgh alignment tool (Cohn et al., 2008) and revised and extended the Edinburgh guidelines, in particular adding guidance intended to ensure that the word alignments are consistent with the overall paraphrase status decision. The finished corpus and the enhanced alignment tool are made freely available.
There have been several attempts to annotate communicative functions to utterances of verbal feedback in English previously. Here, we suggest an annotation scheme for verbal and non-verbal feedback utterances in French including the categories base, attitude, previous and visual. The data comprises conversations, maptasks and negotiations from which we extracted ca. 13,000 candidate feedback utterances and gestures. 12 students were recruited for the annotation campaign of ca. 9,500 instances. Each instance was annotated by between 2 and 7 raters. The evaluation of the annotation agreement resulted in an average best-pair kappa of 0.6. While the base category with the values acknowledgement, evaluation, answer, elicit achieve good agreement, this is not the case for the other main categories. The data sets, which also include automatic extractions of lexical, positional and acoustic features, are freely available and will further be used for machine learning classification experiments to analyse the form-function relationship of feedback.
We developed a web application for crowdsourcing transcriptions of Dutch words spoken by Spanish L2 learners. In this paper we discuss the design of the application and the influence of metadata and various forms of feedback. Useful data were obtained from 159 participants, with an average of over 20 transcriptions per item, which seems a satisfactory result for this type of research. Informing participants about how many items they still had to complete, and not how many they had already completed, turned to be an incentive to do more items. Assigning participants a score for their performance made it more attractive for them to carry out the transcription task, but this seemed to influence their performance. We discuss possible advantages and disadvantages in connection with the aim of the research and consider possible lessons for designing future experiments.
The Uppsala Corpus of Student Writings consists of Swedish texts produced as part of a national test of students ranging in age from nine (in year three of primary school) to nineteen (the last year of upper secondary school) who are studying either Swedish or Swedish as a second language. National tests have been collected since 1996. The corpus currently consists of 2,500 texts containing over 1.5 million tokens. Parts of the texts have been annotated on several linguistic levels using existing state-of-the-art natural language processing tools. In order to make the corpus easy to interpret for scholars in the humanities, we chose the CoNLL format instead of an XML-based representation. Since spelling and grammatical errors are common in student writings, the texts are automatically corrected while keeping the original tokens in the corpus. Each token is annotated with part-of-speech and morphological features as well as syntactic structure. The main purpose of the corpus is to facilitate the systematic and quantitative empirical study of the writings of various student groups based on gender, geographic area, age, grade awarded or a combination of these, synchronically or diachronically. The intention is for this to be a monitor corpus, currently under development.
This paper describes the collection of the H1 Corpus of children{'}s weekly writing over the course of 3 months in 2nd and 3rd grades, aged 7-11. The texts were collected within the normal classroom setting by the teacher. Texts of children whose parents signed the permission to donate the texts to science were collected and transcribed. The corpus consists of the elicitation techniques, an overview of the data collected and the transcriptions of the texts both with and without spelling errors, aligned on a word by word basis, as well as the scanned in texts. The corpus is available for research via Linguistic Data Consortium (LDC). Researchers are strongly encouraged to make additional annotations and improvements and return it to the public domain via LDC.
We present the COPLE2 corpus, a learner corpus of Portuguese that includes written and spoken texts produced by learners of Portuguese as a second or foreign language. The corpus includes at the moment a total of 182,474 tokens and 978 texts, classified according to the CEFR scales. The original handwritten productions are transcribed in TEI compliant XML format and keep record of all the original information, such as reformulations, insertions and corrections made by the teacher, while the recordings are transcribed and aligned with EXMARaLDA. The TEITOK environment enables different views of the same document (XML, student version, corrected version), a CQP-based search interface, the POS, lemmatization and normalization of the tokens, and will soon be used for error annotation in stand-off format. The corpus has already been a source of data for phonological, lexical and syntactic interlanguage studies and will be used for a data-informed selection of language features for each proficiency level.
The French Learners Audio Corpus of German Speech (FLACGS) was created to compare German speech production of German native speakers (GG) and French learners of German (FG) across three speech production tasks of increasing production complexity: repetition, reading and picture description. 40 speakers, 20 GG and 20 FG performed each of the three tasks, which in total leads to approximately 7h of speech. The corpus was manually transcribed and automatically aligned. Analysis that can be performed on this type of corpus are for instance segmental differences in the speech production of L2 learners compared to native speakers. We chose the realization of the velar nasal consonant engma. In spoken French, engma does not appear in a VCV context which leads to production difficulties in FG. With increasing speech production complexity (reading and picture description), engma is realized as engma + plosive by FG in over 50{\%} of the cases. The results of a two way ANOVA with unequal sample sizes on the durations of the different realizations of engma indicate that duration is a reliable factor to distinguish between engma and engma + plosive in FG productions compared to the engma productions in GG in a VCV context. The FLACGS corpus allows to study L2 production and perception.
In the paper authors present the Croatian corpus of non-professional written language. Consisting of two subcorpora, i.e. the clinical subcorpus, consisting of written texts produced by speakers with various types of language disorders, and the healthy speakers subcorpus, as well as by the levels of its annotation, it offers an opportunity for different lines of research. The authors present the corpus structure, describe the sampling methodology, explain the levels of annotation, and give some very basic statistics. On the basis of data from the corpus, existing language technologies for Croatian are adapted in order to be implemented in a platform facilitating text production to speakers with language disorders. In this respect, several analyses of the corpus data and a basic evaluation of the developed technologies are presented.
We are presenting our work on the creation of the first optical character recognition (OCR) model for Northern Haida, also known as Masset or Xaad Kil, a nearly extinct First Nations language spoken in the Haida Gwaii archipelago in British Columbia, Canada. We are addressing the challenges of training an OCR model for a language with an extensive, non-standard Latin character set as follows: (1) We have compared various training approaches and present the results of practical analyses to maximize recognition accuracy and minimize manual labor. An approach using just one or two pages of Source Images directly performed better than the Image Generation approach, and better than models based on three or more pages. Analyses also suggest that a character{'}s frequency is directly correlated with its recognition accuracy. (2) We present an overview of current OCR accuracy analysis tools available. (3) We have ported the once de-facto standardized OCR accuracy tools to be able to cope with Unicode input. Our work adds to a growing body of research on OCR for particularly challenging character sets, and contributes to creating the largest electronic corpus for this severely endangered language.
An online tool based on dialectometric methods, DistGraph, is applied to a group of Kru languages of C{\^o}te d{'}Ivoire, Liberia and Burkina Faso. The inputs to this resource consist of tables of languages x linguistic features (e.g. phonological, lexical or grammatical), and statistical and graphical outputs are generated which show similarities and differences between the languages in terms of the features as virtual distances. In the present contribution, attention is focussed on the consonant systems of the languages, a traditional starting point for language comparison. The data are harvested from a legacy language data resource based on fieldwork in the 1970s and 1980s, a language atlas of the Kru languages. The method on which the online tool is based extends beyond documentation of individual languages to the documentation of language groups, and supports difference-based prioritisation in education programmes, decisions on language policy and documentation and conservation funding, as well as research on language typology and heritage documentation of history and migration.
This is a report of findings from on-going language documentation research based on three consecutive projects from 2008 to 2016. In the light of this research, we propose that (1) we should stand on the side of language resource producers to enhance the research of language processing. We support personal data management in addition to social data sharing. (2) This support leads to adopting simple data formats instead of the multi-link-path data models proposed as international standards up to the present. (3) We should set up a framework for total language resource study that includes not only pivotal data formats such as standard formats, but also the surroundings of data formation to capture a wider range of language activities, e.g. annotation, hesitant language formation, and reference-referent relations. A study of this framework is expected to be a foundation of rebuilding man-machine interface studies in which we seek to observe generative processes of informational symbols in order to establish a high affinity interface in regard to documentation.
This paper describes the process of semi-automatically converting dictionaries from paper to structured text (database) and the integration of these into the CLARIN infrastructure in order to make the dictionaries accessible and retrievable for the research community. The case study at hand is that of the curation of 42 fascicles of the Dictionaries of the Brabantic and Limburgian dialects, and 6 fascicles of the Dictionary of dialects in Gelderland.
Poor digital representation of minority languages further prevents their usability on digital media and devices. The Digital Language Diversity Project, a three-year project funded under the Erasmus+ programme, aims at addressing the problem of low digital representation of EU regional and minority languages by giving their speakers the intellectual an practical skills to create, share, and reuse online digital content. Availability of digital content and technical support to use it are essential prerequisites for the development of language-based digital applications, which in turn can boost digital usage of these languages. In this paper we introduce the project, its aims, objectives and current activities for sustaining digital usability of minority languages through adult education.
This paper describes the use of a free, on-line language spelling and grammar checking aid as a vehicle for the collection of a significant (31 million words and rising) corpus of text for academic research in the context of less resourced languages where such data in sufficient quantities are often unavailable. It describes two versions of the corpus: the texts as submitted, prior to the correction process, and the texts following the user{'}s incorporation of any suggested changes. An overview of the corpus{'} contents is given and an analysis of use including usage statistics is also provided. Issues surrounding privacy and the anonymization of data are explored as is the data{'}s potential use for linguistic analysis, lexical research and language modelling. The method used for gathering this corpus is believed to be unique, and is a valuable addition to corpus studies in a minority language.
In this paper, we illustrate the integration of an online dialectometric tool, Gabmap, together with an online dialect atlas, the Atlante Lessicale Toscano (ALT-Web). By using a newly created url-based interface to Gabmap, ALT-Web is able to take advantage of the sophisticated dialect visualization and exploration options incorporated in Gabmap. For example, distribution maps showing the distribution in the Tuscan dialect area of a specific dialectal form (selected via the ALT-Web website) are easily obtainable. Furthermore, the complete ALT-Web dataset as well as subsets of the data (selected via the ALT-Web website) can be automatically uploaded and explored in Gabmap. By combining these two online applications, macro- and micro-analyses of dialectal data (respectively offered by Gabmap and ALT-Web) are effectively and dynamically combined.
In this paper, we describe the textual linguistic resources in nearly 3 dozen languages being produced by Linguistic Data Consortium for DARPA{'}s LORELEI (Low Resource Languages for Emergent Incidents) Program. The goal of LORELEI is to improve the performance of human language technologies for low-resource languages and enable rapid re-training of such technologies for new languages, with a focus on the use case of deployment of resources in sudden emergencies such as natural disasters. Representative languages have been selected to provide broad typological coverage for training, and surprise incident languages for testing will be selected over the course of the program. Our approach treats the full set of language packs as a coherent whole, maintaining LORELEI-wide specifications, tagsets, and guidelines, while allowing for adaptation to the specific needs created by each language. Each representative language corpus, therefore, both stands on its own as a resource for the specific language and forms part of a large multilingual resource for broader cross-language technology development.
In this paper we conduct an initial study on the dialects of Romanian. We analyze the differences between Romanian and its dialects using the Swadesh list. We analyze the predictive power of the orthographic and phonetic features of the words, building a classification problem for dialect identification.
This paper describes a repository of example sentences in three endangered Athabascan languages: Koyukon, Upper Tanana, Lower Tanana. The repository allows researchers or language teachers to browse the example sentence corpus to either investigate the languages or to prepare teaching materials. The originally heterogeneous text collection was imported into a SOLR store via the POIO bridge. This paper describes the requirements, implementation, advantages and drawbacks of this approach and discusses the potential to apply it for other languages of the Athabascan family or beyond.
The lack or absence of parallel and comparable corpora makes bilingual lexicon extraction becomes a difficult task for low-resource languages. Pivot language and cognate recognition approach have been proven useful to induce bilingual lexicons for such languages. We analyze the features of closely related languages and define a semantic constraint assumption. Based on the assumption, we propose a constraint-based bilingual lexicon induction for closely related languages by extending constraints and translation pair candidates from recent pivot language approach. We further define three constraint sets based on language characteristics. In this paper, two controlled experiments are conducted. The former involves four closely related language pairs with different language pair similarities, and the latter focuses on sense connectivity between non-pivot words and pivot words. We evaluate our result with F-measure. The result indicates that our method works better on voluminous input dictionaries and high similarity languages. Finally, we introduce a strategy to use proper constraint sets for different goals and language characteristics.
This paper introduces the Web TextFull linkage to Linked Open Data (WTF-LOD) dataset intended for large-scale evaluation of named entity recognition (NER) systems. First, we present the process of collecting data from the largest publically-available textual corpora, including Wikipedia dumps, monthly runs of the CommonCrawl, and ClueWeb09/12. We discuss similarities and differences of related initiatives such as WikiLinks and WikiReverse. Our work primarily focuses on links from {``}textfull{''} documents (links surrounded by a text that provides a useful context for entity linking), de-duplication of the data and advanced cleaning procedures. Presented statistics demonstrate that the collected data forms one of the largest available resource of its kind. They also prove suitability of the result for complex NER evaluation campaigns, including an analysis of the most ambiguous name mentions appearing in the data.
For publishing sign language corpus data on the web, anonymization is crucial even if it is impossible to hide the visual appearance of the signers: In a small community, even vague references to third persons may be enough to identify those persons. In the case of the DGS Korpus (German Sign Language corpus) project, we want to publish data as a contribution to the cultural heritage of the sign language community while annotation of the data is still ongoing. This poses the question how well anonymization can be achieved given that no full linguistic analysis of the data is available. Basically, we combine analysis of all data that we have, including named entity recognition on translations into German. For this, we use the WebLicht language technology infrastructure. We report on the reliability of these methods in this special context and also illustrate how the anonymization of the video data is technically achieved in order to minimally disturb the viewer.
In this paper, we present a crowdsourced dataset which adds entity salience (importance) annotations to the Reuters-128 dataset, which is subset of Reuters-21578. The dataset is distributed under a free license and publish in the NLP Interchange Format, which fosters interoperability and re-use. We show the potential of the dataset on the task of learning an entity salience classifier and report on the results from several experiments.
In this paper we present a gold standard dataset for Entity Linking (EL) in the Music Domain. It contains thousands of musical named entities such as Artist, Song or Record Label, which have been automatically annotated on a set of artist biographies coming from the Music website and social network Last.fm. The annotation process relies on the analysis of the hyperlinks present in the source texts and in a voting-based algorithm for EL, which considers, for each entity mention in text, the degree of agreement across three state-of-the-art EL systems. Manual evaluation shows that EL Precision is at least 94{\%}, and due to its tunable nature, it is possible to derive annotations favouring higher Precision or Recall, at will. We make available the annotated dataset along with evaluation data and the code.
In Sorani Kurdish, one of the most useful orthographic features in named-entity recognition {--} capitalization {--} is absent, as the language{'}s Perso-Arabic script does not make a distinction between uppercase and lowercase letters. We describe a system for deriving an inferred capitalization value from closely related languages by phonological similarity, and illustrate the system using several related Western Iranian languages.
We describe a corpus of consumer health questions annotated with named entities. The corpus consists of 1548 de-identified questions about diseases and drugs, written in English. We defined 15 broad categories of biomedical named entities for annotation. A pilot annotation phase in which a small portion of the corpus was double-annotated by four annotators was followed by a main phase in which double annotation was carried out by six annotators, and a reconciliation phase in which all annotations were reconciled by an expert. We conducted the annotation in two modes, manual and assisted, to assess the effect of automatic pre-annotation and calculated inter-annotator agreement. We obtained moderate inter-annotator agreement; assisted annotation yielded slightly better agreement and fewer missed annotations than manual annotation. Due to complex nature of biomedical entities, we paid particular attention to nested entities for which we obtained slightly lower inter-annotator agreement, confirming that annotating nested entities is somewhat more challenging. To our knowledge, the corpus is the first of its kind for consumer health text and is publicly available.
This paper presents a German corpus for Named Entity Linking (NEL) and Knowledge Base Population (KBP) tasks. We describe the annotation guideline, the annotation process, NIL clustering techniques and conversion to popular NEL formats such as NIF and TAC that have been used to construct this corpus based on news transcripts from the German regional broadcaster RBB (Rundfunk Berlin Brandenburg). Since creating such language resources requires significant effort, the paper also discusses how to derive additional evaluation resources for tasks like named entity contextualization or ontology enrichment by exploiting the links between named entities from the annotated corpus. The paper concludes with an evaluation that shows how several well-known NEL tools perform on the corpus, a discussion of the evaluation results, and with suggestions on how to keep evaluation corpora and datasets up to date.
The ever increasing importance of machine learning in Natural Language Processing is accompanied by an equally increasing need in large-scale training and evaluation corpora. Due to its size, its openness and relative quality, the Wikipedia has already been a source of such data, but on a limited scale. This paper introduces the DBpedia Abstract Corpus, a large-scale, open corpus of annotated Wikipedia texts in six languages, featuring over 11 million texts and over 97 million entity links. The properties of the Wikipedia texts are being described, as well as the corpus creation process, its format and interesting use-cases, like Named Entity Linking training and evaluation.
This paper describes the named entity language resources developed as part of a development project for the South African languages. The development efforts focused on creating protocols and annotated data sets with at least 15,000 annotated named entity tokens for ten of the official South African languages. The description of the protocols and annotated data sets provide an overview of the problems encountered during the annotation of the data sets. Based on these annotated data sets, CRF named entity recognition systems are developed that leverage existing linguistic resources. The newly created named entity recognisers are evaluated, with F-scores of between 0.64 and 0.77, and error analysis is performed to identify possible avenues for improving the quality of the systems.
Recognition of real-world entities is crucial for most NLP applications. Since its introduction some twenty years ago, named entity processing has undergone a significant evolution with, among others, the definition of new tasks (e.g. entity linking) and the emergence of new types of data (e.g. speech transcriptions, micro-blogging). These pose certainly new challenges which affect not only methods and algorithms but especially linguistic resources. Where do we stand with respect to named entity resources? This paper aims at providing a systematic overview of named entity resources, accounting for qualities such as multilingualism, dynamicity and interoperability, and to identify shortfalls in order to guide future developments.
This paper explores the incorporation of lexico-semantic heuristics into a deterministic Coreference Resolution (CR) system for classifying named entities at document-level. The highest precise sieves of a CR tool are enriched with both a set of heuristics for merging named entities labeled with different classes and also with some constraints that avoid the incorrect merging of similar mentions. Several tests show that this strategy improves both NER labeling and CR. The CR tool can be applied in combination with any system for named entity recognition using the CoNLL format, and brings benefits to text analytics tasks such as Information Extraction. Experiments were carried out in Spanish, using three different NER tools.
In this paper we investigate the usefulness of neural word embeddings in the process of translating Named Entities (NEs) from a resource-rich language to a language low on resources relevant to the task at hand, introducing a novel, yet simple way of obtaining bilingual word vectors. Inspired by observations in (Mikolov et al., 2013b), which show that training their word vector model on comparable corpora yields comparable vector space representations of those corpora, reducing the problem of translating words to finding a rotation matrix, and results in (Zou et al., 2013), which showed that bilingual word embeddings can improve Chinese Named Entity Recognition (NER) and English to Chinese phrase translation, we use the sentence-aligned English-French EuroParl corpora and show that word embeddings extracted from a merged corpus (corpus resulted from the merger of the two aligned corpora) can be used to NE translation. We extrapolate that word embeddings trained on merged parallel corpora are useful in Named Entity Recognition and Translation tasks for resource-poor languages.
The task of relation extraction is to recognize and extract relations between entities or concepts in texts. Dependency parse trees have become a popular source for discovering extraction patterns, which encode the grammatical relations among the phrases that jointly express relation instances. State-of-the-art weakly supervised approaches to relation extraction typically extract thousands of unique patterns only potentially expressing the target relation. Among these patterns, some are semantically equivalent, but differ in their morphological, lexical-semantic or syntactic form. Some express a relation that entails the target relation. We propose a new approach to structuring extraction patterns by utilizing entailment graphs, hierarchical structures representing entailment relations, and present a novel resource of gold-standard entailment graphs based on a set of patterns automatically acquired using distant supervision. We describe the methodology used for creating the dataset and present statistics of the resource as well as an analysis of inference types underlying the entailment decisions.
Bar exams provide a key watershed by which legal professionals demonstrate their knowledge of the law and its application. Passing the bar entitles one to practice the law in a given jurisdiction. The bar provides an excellent benchmark for the performance of legal information systems since passing the bar would arguably signal that the system has acquired key aspects of legal reason on a par with a human lawyer. The paper provides a corpus and experimental results with material derived from a real bar exam, treating the problem as a form of textual entailment from the question to an answer. The providers of the bar exam material set the Gold Standard, which is the answer key. The experiments carried out using the {`}out of the box{'} the Excitement Open Platform for textual entailment. The results and evaluation show that the tool can identify wrong answers (non-entailment) with a high F1 score, but it performs poorly in identifying the correct answer (entailment). The results provide a baseline performance measure against which to evaluate future improvements. The reasons for the poor performance are examined, and proposals are made to augment the tool in the future. The corpus facilitates experimentation by other researchers.
In this paper we present the creation of a corpora annotated with both semantic relatedness (SR) scores and textual entailment (TE) judgments. In building this corpus we aimed at discovering, if any, the relationship between these two tasks for the mutual benefit of resolving one of them by relying on the insights gained from the other. We considered a corpora already annotated with TE judgments and we proceed to the manual annotation with SR scores. The RTE 1-4 corpora used in the PASCAL competition fit our need. The annotators worked independently of one each other and they did not have access to the TE judgment during annotation. The intuition that the two annotations are correlated received major support from this experiment and this finding led to a system that uses this information to revise the initial estimates of SR scores. As semantic relatedness is one of the most general and difficult task in natural language processing we expect that future systems will combine different sources of information in order to solve it. Our work suggests that textual entailment plays a quantifiable role in addressing it.
This paper proposes a new topic model that exploits word sense information in order to discover less redundant and more informative topics. Word sense information is obtained from WordNet and the discovered topics are groups of synsets, instead of mere surface words. A key feature is that all the known senses of a word are considered, with their probabilities. Alternative configurations of the model are described and compared to each other and to LDA, the most popular topic model. However, the obtained results suggest that there are no benefits of enriching LDA with word sense information.
Health support forums have become a rich source of data that can be used to improve health care outcomes. A user profile, including information such as age and gender, can support targeted analysis of forum data. But users might not always disclose their age and gender. It is desirable then to be able to automatically extract this information from users{'} content. However, to the best of our knowledge there is no such resource for author profiling of health forum data. Here we present a large corpus, with close to 85,000 users, for profiling and also outline our approach and benchmark results to automatically detect a user{'}s age and gender from their forum posts. We use a mix of features from a user{'}s text as well as forum specific features to obtain accuracy well above the baseline, thus showing that both our dataset and our method are useful and valid.
In this paper we explore and compare a speech and text classification approach on a corpus of native and non-native English speakers. We experiment on a subset of the International Corpus Network of Asian Learners of English containing the recorded speeches and the equivalent text transcriptions. Our results suggest a high correlation between the spoken and written classification results, showing that native accent is highly correlated with grammatical structures found in text.
Timeliness and precision for detection of infectious animal disease outbreaks from the information published on the web is crucial for prevention against their spread. We propose a generic method to enrich and extend the use of different expressions as queries in order to improve the acquisition of relevant disease related pages on the web. Our method combines a text mining approach to extract terms from corpora of relevant disease outbreak documents, and domain expert elicitation (Delphi method) to propose expressions and to select relevant combinations between terms obtained with text mining. In this paper we evaluated the performance as queries of a number of expressions obtained with text mining and validated by a domain expert and expressions proposed by a panel of 21 domain experts. We used African swine fever as an infectious animal disease model. The expressions obtained with text mining outperformed as queries the expressions proposed by domain experts. However, domain experts proposed expressions not extracted automatically. Our method is simple to conduct and flexible to adapt to any other animal infectious disease and even in the public health domain.
Privacy concerns have often served as an insurmountable barrier for the production of research and resources in clinical information retrieval (IR). We believe that both clinical IR research innovation and legitimate privacy concerns can be served by the creation of intra-institutional, fully protected resources. In this paper, we provide some principles and tools for IR resource-building in the unique problem setting of patient-level IR, following the tradition of the Cranfield paradigm.
Early detection and treatment of diseases that onset after a patient is admitted to a hospital, such as pneumonia, is critical to improving and reducing costs in healthcare. Previous studies (Tepper et al., 2013) showed that change-of-state events in clinical notes could be important cues for phenotype detection. In this paper, we extend the annotation schema proposed in (Klassen et al., 2014) to mark change-of-state events, diagnosis events, coordination, and negation. After we have completed the annotation, we build NLP systems to automatically identify named entities and medical events, which yield an f-score of 94.7{\%} and 91.8{\%}, respectively.
Most Text to Speech (TTS) systems today assume that the input text is in a single language and is written in the same language that the text needs to be synthesized in. However, in bilingual and multilingual communities, code mixing or code switching occurs in speech, in which speakers switch between languages in the same utterance. Due to the popularity of social media, we now see code-mixing even in text in these multilingual communities. TTS systems capable of synthesizing such text need to be able to handle text that is written in multiple languages and scripts. Code-mixed text poses many challenges to TTS systems, such as language identification, spelling normalization and pronunciation modeling. In this work, we describe a preliminary framework for synthesizing code-mixed text. We carry out experiments on synthesizing code-mixed Hindi and English text. We find that there is a significant user preference for TTS systems that can correctly identify and pronounce words in different languages.
This paper describes the development and evaluation of a chatbot platform designed for the teaching/learning of Irish. The chatbot uses synthetic voices developed for the dialects of Irish. Speech-enabled chatbot technology offers a potentially powerful tool for dealing with the challenges of teaching/learning an endangered language where learners have limited access to native speaker models of the language and limited exposure to the language in a truly communicative setting. The sociolinguistic context that motivates the present development is explained. The evaluation of the chatbot was carried out in 13 schools by 228 pupils and consisted of two parts. Firstly, learners{'} opinions of the overall chatbot platform as a learning environment were elicited. Secondly, learners evaluated the intelligibility, quality, and attractiveness of the synthetic voices used in this platform. Results were overwhelmingly positive to both the learning platform and the synthetic voices and indicate that the time may now be ripe for language learning applications which exploit speech and language technologies. It is further argued that these technologies have a particularly vital role to play in the maintenance of the endangered language.
This paper reports the preservation of an old speech synthesis website as a corpus. CHATR was a revolutionary technique developed in the mid nineties for concatenative speech synthesis. The method has since become the standard for high quality speech output by computer although much of the current research is devoted to parametric or hybrid methods that employ smaller amounts of data and can be more easily tunable to individual voices. The system was first reported in 1994 and the website was functional in 1996. The ATR labs where this system was invented no longer exist, but the website has been preserved as a corpus containing 1537 samples of synthesised speech from that period (118 MB in aiff format) in 211 pages under various finely interrelated themes The corpus can be accessed from www.speech-data.jp as well as www.tcd-fastnet.com, where the original code and samples are now being maintained.
In order to explore intuitive verbal and non-verbal interfaces in smart environments we recorded user interactions with an intelligent apartment. Besides offering various interactive capabilities itself, the apartment is also inhabited by a social robot that is available as a humanoid interface. This paper presents a multi-modal corpus that contains goal-directed actions of naive users in attempts to solve a number of predefined tasks. Alongside audio and video recordings, our data-set consists of large amount of temporally aligned sensory data and system behavior provided by the environment and its interactive components. Non-verbal system responses such as changes in light or display contents, as well as robot and apartment utterances and gestures serve as a rich basis for later in-depth analysis. Manual annotations provide further information about meta data like the current course of study and user behavior including the incorporated modality, all literal utterances, language features, emotional expressions, foci of attention, and addressees.
Story-telling is a fundamental and prevalent aspect of human social behavior. In the wild, stories are told conversationally in social settings, often as a dialogue and with accompanying gestures and other nonverbal behavior. This paper presents a new corpus, the Story Dialogue with Gestures (SDG) corpus, consisting of 50 personal narratives regenerated as dialogues, complete with annotations of gesture placement and accompanying gesture forms. The corpus includes dialogues generated by human annotators, gesture annotations on the human generated dialogues, videos of story dialogues generated from this representation, video clips of each gesture used in the gesture annotations, and annotations of the original personal narratives with a deep representation of story called a Story Intention Graph. Our long term goal is the automatic generation of story co-tellings as animated dialogues from the Story Intention Graph. We expect this corpus to be a useful resource for researchers interested in natural language generation, intelligent virtual agents, generation of nonverbal behavior, and story and narrative representations.
This paper reports on work related to the modelling of Human-Robot Communication on the basis of multimodal and multisensory human behaviour analysis. A primary focus in this framework of analysis is the definition of semantics of human actions in interaction, their capture and their representation in terms of behavioural patterns that, in turn, feed a multimodal human-robot communication system. Semantic analysis encompasses both oral and sign languages, as well as both verbal and non-verbal communicative signals to achieve an effective, natural interaction between elderly users with slight walking and cognitive inability and an assistive robotic platform.
We present a corpus of 44 human-agent verbal and gestural story retellings designed to explore whether humans would gesturally entrain to an embodied intelligent virtual agent. We used a novel data collection method where an agent presented story components in installments, which the human would then retell to the agent. At the end of the installments, the human would then retell the embodied animated agent the story as a whole. This method was designed to allow us to observe whether changes in the agent{'}s gestural behavior would result in human gestural changes. The agent modified its gestures over the course of the story, by starting out the first installment with gestural behaviors designed to manifest extraversion, and slowly modifying gestures to express introversion over time, or the reverse. The corpus contains the verbal and gestural transcripts of the human story retellings. The gestures were coded for type, handedness, temporal structure, spatial extent, and the degree to which the participants{'} gestures match those produced by the agent. The corpus illustrates the variation in expressive behaviors produced by users interacting with embodied virtual characters, and the degree to which their gestures were influenced by the agent{'}s dynamic changes in personality-based expressive style.
This paper presents a new corpus, the Personality Dyads Corpus, consisting of multimodal data for three conversations between three personality-matched, two-person dyads (a total of 9 separate dialogues). Participants were selected from a larger sample to be 0.8 of a standard deviation above or below the mean on the Big-Five Personality extraversion scale, to produce an Extravert-Extravert dyad, an Introvert-Introvert dyad, and an Extravert-Introvert dyad. Each pair carried out conversations for three different tasks. The conversations were recorded using optical motion capture for the body and data gloves for the hands. Dyads{'} speech was transcribed and the gestural and postural behavior was annotated with ANVIL. The released corpus includes personality profiles, ANVIL files containing speech transcriptions and the gestural annotations, and BVH files containing body and hand motion in 3D.
In order to make the growing amount of conceptual knowledge available through ontologies and datasets accessible to humans, NLP applications need access to information on how this knowledge can be verbalized in natural language. One way to provide this kind of information are ontology lexicons, which apart from the actual verbalizations in a given target language can provide further, rich linguistic information about them. Compiling such lexicons manually is a very time-consuming task and requires expertise both in Semantic Web technologies and lexicon engineering, as well as a very good knowledge of the target language at hand. In this paper we present an alternative approach to generating ontology lexicons by means of crowdsourcing: We use CrowdFlower to generate a small Japanese ontology lexicon for ten exemplary ontology elements from the DBpedia ontology according to a two-stage workflow, the main underlying idea of which is to turn the task of generating lexicon entries into a translation task; the starting point of this translation task is a manually created English lexicon for DBpedia. Comparison of the results to a manually created Japanese lexicon shows that the presented workflow is a viable option if an English seed lexicon is already available.
This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.
Scripts are standardized event sequences describing typical everyday activities, which play an important role in the computational modeling of cognitive abilities (in particular for natural language processing). We present a large-scale crowdsourced collection of explicit linguistic descriptions of script-specific event sequences (40 scenarios with 100 sequences each). The corpus is enriched with crowdsourced alignment annotation on a subset of the event descriptions, to be used in future work as seed data for automatic alignment of event descriptions (for example via clustering). The event descriptions to be aligned were chosen among those expected to have the strongest corrective effect on the clustering algorithm. The alignment annotation was evaluated against a gold standard of expert annotators. The resulting database of partially-aligned script-event descriptions provides a sound empirical basis for inducing high-quality script knowledge, as well as for any task involving alignment and paraphrase detection of events.
This paper describes two sets of crowdsourcing experiments on temporal information annotation conducted on two languages, i.e., English and Italian. The first experiment, launched on the CrowdFlower platform, was aimed at classifying temporal relations given target entities. The second one, relying on the CrowdTruth metric, consisted in two subtasks: one devoted to the recognition of events and temporal expressions and one to the detection and classification of temporal relations. The outcomes of the experiments suggest a valuable use of crowdsourcing annotations also for a complex task like Temporal Processing.
We present an approach to creating corpora for use in detecting deception in text, including a discussion of the challenges peculiar to this task. Our approach is based on soliciting several types of reviews from writers and was implemented using Amazon Mechanical Turk. We describe the multi-dimensional corpus of reviews built using this approach, available free of charge from LDC as the Boulder Lies and Truth Corpus (BLT-C). Challenges for both corpus creation and the deception detection include the fact that human performance on the task is typically at chance, that the signal is faint, that paid writers such as turkers are sometimes deceptive, and that deception is a complex human behavior; manifestations of deception depend on details of domain, intrinsic properties of the deceiver (such as education, linguistic competence, and the nature of the intention), and specifics of the deceptive act (e.g., lying vs. fabricating.) To overcome the inherent lack of ground truth, we have developed a set of semi-automatic techniques to ensure corpus validity. We present some preliminary results on the task of deception detection which suggest that the BLT-C is an improvement in the quality of resources available for this task.
OpenSubtitles.org provides a large collection of user contributed subtitles in various languages for movies and TV programs. Subtitle translations are valuable resources for cross-lingual studies and machine translation research. A less explored feature of the collection is the inclusion of alternative translations, which can be very useful for training paraphrase systems or collecting multi-reference test suites for machine translation. However, differences in translation may also be due to misspellings, incomplete or corrupt data files, or wrongly aligned subtitles. This paper reports our efforts in recognising and classifying alternative subtitle translations with language independent techniques. We use time-based alignment with lexical re-synchronisation techniques and BLEU score filters and sort alternative translations into categories using edit distance metrics and heuristic rules. Our approach produces large numbers of sentence-aligned translation alternatives for over 50 languages provided via the OPUS corpus collection.
This article describes a large comparable corpus for Basque and Spanish and the methods employed to build a parallel resource from the original data. The EITB corpus, a strongly comparable corpus in the news domain, is to be shared with the research community, as an aid for the development and testing of methods in comparable corpora exploitation, and as basis for the improvement of data-driven machine translation systems for this language pair. Competing approaches were explored for the alignment of comparable segments in the corpus, resulting in the design of a simple method which outperformed a state-of-the-art method on the corpus test sets. The method we present is highly portable, computationally efficient, and significantly reduces deployment work, a welcome result for the exploitation of comparable corpora.
This paper describes the creation process and statistics of the official United Nations Parallel Corpus, the first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish. The corpus is freely available for download under a liberal license. Apart from the pairwise aligned documents, a fully aligned subcorpus for the six official UN languages is distributed. We provide baseline BLEU scores of our Moses-based SMT systems trained with the full data of language pairs involving English and for all possible translation directions of the six-way subcorpus.
This paper presents WAGS (Word Alignment Gold Standard), a novel benchmark which allows extensive evaluation of WA tools on out-of-vocabulary (OOV) and rare words. WAGS is a subset of the Common Test section of the Europarl English-Italian parallel corpus, and is specifically tailored to OOV and rare words. WAGS is composed of 6,715 sentence pairs containing 11,958 occurrences of OOV and rare words up to frequency 15 in the Europarl Training set (5,080 English words and 6,878 Italian words), representing almost 3{\%} of the whole text. Since WAGS is focused on OOV/rare words, manual alignments are provided for these words only, and not for the whole sentences. Two off-the-shelf word aligners have been evaluated on WAGS, and results have been compared to those obtained on an existing benchmark tailored to full text alignment. The results obtained confirm that WAGS is a valuable resource, which allows a statistically sound evaluation of WA systems{'} performance on OOV and rare words, as well as extensive data analyses. WAGS is publicly released under a Creative Commons Attribution license.
Paraphrasing of reference translations has been shown to improve the correlation with human judgements in automatic evaluation of machine translation (MT) outputs. In this work, we present a new dataset for evaluating English-Czech translation based on automatic paraphrases. We compare this dataset with an existing set of manually created paraphrases and find that even automatic paraphrases can improve MT evaluation. We have also propose and evaluate several criteria for selecting suitable reference translations from a larger set.
We present Poly-GrETEL, an online tool which enables syntactic querying in parallel treebanks, based on the monolingual GrETEL environment. We provide online access to the Europarl parallel treebank for Dutch and English, allowing users to query the treebank using either an XPath expression or an example sentence in order to look for similar constructions. We provide automatic alignments between the nodes. By combining example-based query functionality with node alignments, we limit the need for users to be familiar with the query language and the structure of the trees in the source and target language, thus facilitating the use of parallel corpora for comparative linguistics and translation studies.
We present NorGramBank, a treebank for Norwegian with highly detailed LFG analyses. It is one of many treebanks made available through the INESS treebanking infrastructure. NorGramBank was constructed as a parsebank, i.e. by automatically parsing a corpus, using the wide coverage grammar NorGram. One part consisting of 350,000 words has been manually disambiguated using computer-generated discriminants. A larger part of 50 M words has been stochastically disambiguated. The treebank is dynamic: by global reparsing at certain intervals it is kept compatible with the latest versions of the grammar and the lexicon, which are continually further developed in interaction with the annotators. A powerful query language, INESS Search, has been developed for search across formalisms in the INESS treebanks, including LFG c- and f-structures. Evaluation shows that the grammar provides about 85{\%} of randomly selected sentences with good analyses. Agreement among the annotators responsible for manual disambiguation is satisfactory, but also suggests desirable simplifications of the grammar.
Parsing predicate-argument structures in a deep syntax framework requires graphs to be predicted. Argument structures represent a higher level of abstraction than the syntactic ones and are thus more difficult to predict even for highly accurate parsing models on surfacic syntax. In this paper we investigate deep syntax parsing, using a French data set (Ribeyre et al., 2014a). We demonstrate that the use of topologically different types of syntactic features, such as dependencies, tree fragments, spines or syntactic paths, brings a much needed context to the parser. Our higher-order parsing model, gaining thus up to 4 points, establishes the state of the art for parsing French deep syntactic structures.
Idafa in traditional Arabic grammar is an umbrella construction that covers several phenomena including what is expressed in English as noun-noun compounds and Saxon and Norman genitives. Additionally, Idafa participates in some other constructions, such as quantifiers, quasi-prepositions, and adjectives. Identifying the various types of the Idafa construction (IC) is of importance to Natural Language processing (NLP) applications. Noun-Noun compounds exhibit special behavior in most languages impacting their semantic interpretation. Hence distinguishing them could have an impact on downstream NLP applications. The most comprehensive syntactic representation of the Arabic language is the LDC Arabic Treebank (ATB). In the ATB, ICs are not explicitly labeled and furthermore, there is no distinction between ICs of noun-noun relations and other traditional ICs. Hence, we devise a detailed syntactic and semantic typification process of the IC phenomenon in Arabic. We target the ATB as a platform for this classification. We render the ATB annotated with explicit IC labels but with the further semantic characterization which is useful for syntactic, semantic and cross language processing. Our typification of IC comprises 3 main syntactic IC types: FIC, GIC, and TIC, and they are further divided into 10 syntactic subclasses. The TIC group is further classified into semantic relations. We devise a method for automatic IC labeling and compare its yield against the CATiB treebank. Our evaluation shows that we achieve the same level of accuracy, but with the additional fine-grained classification into the various syntactic and semantic types.
The specialised lexicon belongs to the most prominent attributes of specialised writing: Terms function as semantically dense encodings of specialised concepts, which, in the absence of terms, would require lengthy explanations and descriptions. In this paper, we argue that terms are the result of diachronic processes on both the semantic and the morpho-syntactic level. Very little is known about these processes. We therefore present a corpus annotation project aiming at revealing how terms are coined and how they evolve to fit their function as semantically and morpho-syntactically dense encodings of specialised knowledge. The scope of this paper is two-fold: Firstly, we outline our methodology for annotating terminology in a diachronic corpus of scientific publications. Moreover, we provide a detailed analysis of our annotation results and suggest methods for improving the accuracy of annotations in a setting as difficult as ours. Secondly, we present results of a pilot study based on the annotated terms. The results suggest that terms in older texts are linguistically relatively simple units that are hard to distinguish from the lexicon of general language. We believe that this supports our hypothesis that terminology undergoes diachronic processes of densification and specialisation.
KorAP is a corpus search and analysis platform, developed at the Institute for the German Language (IDS). It supports very large corpora with multiple annotation layers, multiple query languages, and complex licensing scenarios. KorAP{'}s design aims to be scalable, flexible, and sustainable to serve the German Reference Corpus DeReKo for at least the next decade. To meet these requirements, we have adopted a highly modular microservice-based architecture. This paper outlines our approach: An architecture consisting of small components that are easy to extend, replace, and maintain. The components include a search backend, a user and corpus license management system, and a web-based user frontend. We also describe a general corpus query protocol used by all microservices for internal communications. KorAP is open source, licensed under BSD-2, and available on GitHub.
In this paper, we present the experiments we made to recover the original page layout structure into two columns from layout damaged digitized files. We designed several CRF-based approaches, either to identify column separator or to classify each token from each line into left or right columns. We achieved our best results with a model trained on homogeneous corpora (only files composed of 2 columns) when classifying each token into left or right columns (overall F-measure of 0.968). Our experiments show it is possible to recover the original layout in columns of digitized documents with results of quality.
This study primarily aims to build a Turkish psycholinguistic database including three variables: word frequency, age of acquisition (AoA), and imageability, where AoA and imageability information are limited to nouns. We used a corpus-based approach to obtain information about the AoA variable. We built two corpora: a child literature corpus (CLC) including 535 books written for 3-12 years old children, and a corpus of transcribed children{'}s speech (CSC) at ages 1;4-4;8. A comparison between the word frequencies of CLC and CSC gave positive correlation results, suggesting the usability of the CLC to extract AoA information. We assumed that frequent words of the CLC would correspond to early acquired words whereas frequent words of a corpus of adult language would correspond to late acquired words. To validate AoA results from our corpus-based approach, a rated AoA questionnaire was conducted on adults. Imageability values were collected via a different questionnaire conducted on adults. We conclude that it is possible to deduce AoA information for high frequency words with the corpus-based approach. The results about low frequency words were inconclusive, which is attributed to the fact that corpus-based AoA information is affected by the strong negative correlation between corpus frequency and rated AoA.
This work presents a straightforward method for extending or creating in-domain web corpora by focused webcrawling. The focused webcrawler uses statistical N-gram language models to estimate the relatedness of documents and weblinks and needs as input only N-grams or plain texts of a predefined domain and seed URLs as starting points. Two experiments demonstrate that our focused crawler is able to stay focused in domain and language. The first experiment shows that the crawler stays in a focused domain, the second experiment demonstrates that language models trained on focused crawls obtain better perplexity scores on in-domain corpora. We distribute the focused crawler as open source software.
In computer-mediated communication, Latin-based scripts users often omit diacritics when writing. Such text is typically easily understandable to humans but very difficult for computational processing because many words become ambiguous or unknown. Letter-level approaches to diacritic restoration generalise better and do not require a lot of training data but word-level approaches tend to yield better results. However, they typically rely on a lexicon which is an expensive resource, not covering non-standard forms, and often not available for less-resourced languages. In this paper we present diacritic restoration models that are trained on easy-to-acquire corpora. We test three different types of corpora (Wikipedia, general web, Twitter) for three South Slavic languages (Croatian, Serbian and Slovene) and evaluate them on two types of text: standard (Wikipedia) and non-standard (Twitter). The proposed approach considerably outperforms charlifter, so far the only open source tool available for this task. We make the best performing systems freely available.
This paper introduces a toolkit used for the purpose of detecting replacements of different grammatical and semantic structures in ongoing text production logged as a chronological series of computer interaction events (so-called keystroke logs). The specific case we use involves human translations where replacements can be indicative of translator behaviour that leads to specific features of translations that distinguish them from non-translated texts. The toolkit uses a novel CCG chart parser customised so as to recognise grammatical words independently of space and punctuation boundaries. On the basis of the linguistic analysis, structures in different versions of the target text are compared and classified as potential equivalents of the same source text segment by {`}equivalence judges{'}. In that way, replacements of grammatical and semantic structures can be detected. Beyond the specific task at hand the approach will also be useful for the analysis of other types of spaceless text such as Twitter hashtags and texts in agglutinative or spaceless languages like Finnish or Chinese.
As data-driven approaches started to make their way into the Natural Language Generation (NLG) domain, the need for automation of corpus building and extension became apparent. Corpus creation and extension in data-driven NLG domain traditionally involved manual paraphrasing performed by either a group of experts or with resort to crowd-sourcing. Building the training corpora manually is a costly enterprise which requires a lot of time and human resources. We propose to automate the process of corpus extension by integrating automatically obtained synonyms and paraphrases. Our methodology allowed us to significantly increase the size of the training corpus and its level of variability (the number of distinct tokens and specific syntactic structures). Our extension solutions are fully automatic and require only some initial validation. The human evaluation results confirm that in many cases native speakers favor the outputs of the model built on the extended corpus.
In this paper, we present the automatic annotation of bibliographical references{'} zone in papers and articles of XML/TEI format. Our work is applied through two phases: first, we use machine learning technology to classify bibliographical and non-bibliographical paragraphs in papers, by means of a model that was initially created to differentiate between the footnotes containing or not containing bibliographical references. The previous description is one of BILBO{'}s features, which is an open source software for automatic annotation of bibliographic reference. Also, we suggest some methods to minimize the margin of error. Second, we propose an algorithm to find the largest list of bibliographical references in the article. The improvement applied on our model results an increase in the model{'}s efficiency with an Accuracy equal to 85.89. And by testing our work, we are able to achieve 72.23{\%} as an average for the percentage of success in detecting bibliographical references{'} zone.
This paper presents the annotation guidelines developed as part of an effort to create a large scale manually diacritized corpus for various Arabic text genres. The target size of the annotated corpus is 2 million words. We summarize the guidelines and describe issues encountered during the training of the annotators. We also discuss the challenges posed by the complexity of the Arabic language and how they are addressed. Finally, we present the diacritization annotation procedure and detail the quality of the resulting annotations.
The goal of the cognitive machine translation (MT) evaluation approach is to build classifiers which assign post-editing effort scores to new texts. The approach helps estimate fair compensation for post-editors in the translation industry by evaluating the cognitive difficulty of post-editing MT output. The approach counts the number of errors classified in different categories on the basis of how much cognitive effort they require in order to be corrected. In this paper, we present the results of applying an existing cognitive evaluation approach to Modern Standard Arabic (MSA). We provide a comparison of the number of errors and categories of errors in three MSA texts of different MT quality (without any language-specific adaptation), as well as a comparison between MSA texts and texts from three Indo-European languages (Russian, Spanish, and Bulgarian), taken from a previous experiment. The results show how the error distributions change passing from the MSA texts of worse MT quality to MSA texts of better MT quality, as well as a similarity in distinguishing the texts of better MT quality for all four languages.
Effectively assessing Natural Language Processing output tasks is a challenge for research in the area. In the case of Machine Translation (MT), automatic metrics are usually preferred over human evaluation, given time and budget constraints.However, traditional automatic metrics (such as BLEU) are not reliable for absolute quality assessment of documents, often producing similar scores for documents translated by the same MT system.For scenarios where absolute labels are necessary for building models, such as document-level Quality Estimation, these metrics can not be fully trusted. In this paper, we introduce a corpus of reading comprehension tests based on machine translated documents, where we evaluate documents based on answers to questions by fluent speakers of the target language. We describe the process of creating such a resource, the experiment design and agreement between the test takers. Finally, we discuss ways to convert the reading comprehension test into document-level quality scores.
Resources such as WordNet are useful for NLP applications, but their manual construction consumes time and personnel, and frequently results in low coverage. One alternative is the automatic construction of large resources from corpora like distributional thesauri, containing semantically associated words. However, as they may contain noise, there is a strong need for automatic ways of evaluating the quality of the resulting resource. This paper introduces a gold standard that can aid in this task. The BabelNet-Based Semantic Gold Standard (B2SG) was automatically constructed based on BabelNet and partly evaluated by human judges. It consists of sets of tests that present one target word, one related word and three unrelated words. B2SG contains 2,875 validated relations: 800 for verbs and 2,075 for nouns; these relations are divided among synonymy, antonymy and hypernymy. They can be used as the basis for evaluating the accuracy of the similarity relations on distributional thesauri by comparing the proximity of the target word with the related and unrelated options and observing if the related word has the highest similarity value among them. As a case study two distributional thesauri were also developed: one using surface forms from a large (1.5 billion word) corpus and the other using lemmatized forms from a smaller (409 million word) corpus. Both distributional thesauri were then evaluated against B2SG, and the one using lemmatized forms performed slightly better.
In this paper we introduce MoBiL, a hybrid Monolingual, Bilingual and Language modelling feature set and feature selection and evaluation framework. The set includes translation quality indicators that can be utilized to automatically predict the quality of human translations in terms of content adequacy and language fluency. We compare MoBiL with the QuEst baseline set by using them in classifiers trained with support vector machine and relevance vector machine learning algorithms on the same data set. We also report an experiment on feature selection to opt for fewer but more informative features from MoBiL. Our experiments show that classifiers trained on our feature set perform consistently better in predicting both adequacy and fluency than the classifiers trained on the baseline feature set. MoBiL also performs well when used with both support vector machine and relevance vector machine algorithms.
We present Marmot{\textasciitilde}‚Äï a new toolkit for quality estimation (QE) of machine translation output. Marmot contains utilities targeted at quality estimation at the word and phrase level. However, due to its flexibility and modularity, it can also be extended to work at the sentence level. In addition, it can be used as a framework for extracting features and learning models for many common natural language processing tasks. The tool has a set of state-of-the-art features for QE, and new features can easily be added. The tool is open-source and can be downloaded from https://github.com/qe-team/marmot/
Ranking is used for a wide array of problems, most notably information retrieval (search). Kendall{'}s œÑ, Average Precision, and nDCG are a few popular approaches to the evaluation of ranking. When dealing with problems such as user ranking or recommendation systems, all these measures suffer from various problems, including the inability to deal with elements of the same rank, inconsistent and ambiguous lower bound scores, and an inappropriate cost function. We propose a new measure, a modification of the popular nDCG algorithm, named rankDCG, that addresses these problems. We provide a number of criteria for any effective ranking algorithm and show that only rankDCG satisfies them all. Results are presented on constructed and real data sets. We release a publicly available rankDCG evaluation package.
Contents analisys from text data requires semantic representations that are difficult to obtain automatically, as they may require large handcrafted knowledge bases or manually annotated examples. Unsupervised autonomous methods for generating semantic representations are of greatest interest in face of huge volumes of text to be exploited in all kinds of applications. In this work we describe the generation and validation of semantic representations in the vector space paradigm for Spanish. The method used is GloVe (Pennington, 2014), one of the best performing reported methods , and vectors were trained over Spanish Wikipedia. The learned vectors evaluation is done in terms of word analogy and similarity tasks (Pennington, 2014; Baroni, 2014; Mikolov, 2013a). The vector set and a Spanish version for some widely used semantic relatedness tests are made publicly available.
Preprocessing is a preliminary step in many fields including IR and NLP. The effect of basic preprocessing settings on English for text summarization is well-studied. However, there is no such effort found for the Urdu language (with the best of our knowledge). In this study, we analyze the effect of basic preprocessing settings for single-document text summarization for Urdu, on a benchmark corpus using various experiments. The analysis is performed using the state-of-the-art algorithms for extractive summarization and the effect of stopword removal, lemmatization, and stemming is analyzed. Results showed that these pre-processing settings improve the results.
This paper describes the process of creating a corpus annotated for concepts and semantic relations in the scientific domain. A part of the ACL Anthology Corpus was selected for annotation, but the annotation process itself is not specific to the computational linguistics domain and could be applied to any scientific corpora. Concepts were identified and annotated fully automatically, based on a combination of terminology extraction and available ontological resources. A typology of semantic relations between concepts is also proposed. This typology, consisting of 18 domain-specific and 3 generic relations, is the result of a corpus-based investigation of the text sequences occurring between concepts in sentences. A sample of 500 abstracts from the corpus is currently being manually annotated with these semantic relations. Only explicit relations are taken into account, so that the data could serve to train or evaluate pattern-based semantic relation classification systems.
GATE is a widely used open-source solution for text processing with a large user community. It contains components for several natural language processing tasks. However, temporal information extraction functionality within GATE has been rather limited so far, despite being a prerequisite for many application scenarios in the areas of natural language processing and information retrieval. This paper presents an integrated approach to temporal information processing. We take state-of-the-art tools in temporal expression and event recognition and bring them together to form an openly-available resource within the GATE infrastructure. GATE-Time provides annotation in the form of TimeML events and temporal expressions complying with this mature ISO standard for temporal semantic annotation of documents. Major advantages of GATE-Time are (i) that it relies on HeidelTime for temporal tagging, so that temporal expressions can be extracted and normalized in multiple languages and across different domains, (ii) it includes a modern, fast event recognition and classification tool, and (iii) that it can be combined with different linguistic pre-processing annotations, and is thus not bound to license restricted preprocessing components.
Distributional thesauri are useful in many tasks of Natural Language Processing. In this paper, we address the problem of building and evaluating such thesauri with the help of Information Retrieval (IR) concepts. Two main contributions are proposed. First, following the work of [8], we show how IR tools and concepts can be used with success to build a thesaurus. Through several experiments and by evaluating directly the results with reference lexicons, we show that some IR models outperform state-of-the-art systems. Secondly, we use IR as an applicative framework to indirectly evaluate the generated thesaurus. Here again, this task-based evaluation validates the IR approach used to build the thesaurus. Moreover, it allows us to compare these results with those from the direct evaluation framework used in the literature. The observed differences bring these evaluation habits into question.
This paper introduces the parallel Chinese-English Entities, Relations and Events (ERE) corpora developed by Linguistic Data Consortium under the DARPA Deep Exploration and Filtering of Text (DEFT) Program. Original Chinese newswire and discussion forum documents are annotated for two versions of the ERE task. The texts are manually translated into English and then annotated for the same ERE tasks on the English translation, resulting in a rich parallel resource that has utility for performers within the DEFT program, for participants in NIST{'}s Knowledge Base Population evaluations, and for cross-language projection research more generally.
We present the first version of a corpus annotated for psychiatric disorders and their etiological factors. The paper describes the choice of text, annotated entities and events/relations as well as the annotation scheme and procedure applied. The corpus is featuring a selection of focus psychiatric disorders including depressive disorder, anxiety disorder, obsessive-compulsive disorder, phobic disorders and panic disorder. Etiological factors for these focus disorders are widespread and include genetic, physiological, sociological and environmental factors among others. Etiological events, including annotated evidence text, represent the interactions between their focus disorders and their etiological factors. Additionally to these core events, symptomatic and treatment events have been annotated. The current version of the corpus includes 175 scientific abstracts. All entities and events/relations have been manually annotated by domain experts and scores of inter-annotator agreement are presented. The aim of the corpus is to provide a first gold standard to support the development of biomedical text mining applications for the specific area of mental disorders which belong to the main contributors to the contemporary burden of disease.
News sources frame issues in different ways in order to appeal or control the perception of their readers. We present a large scale study of news articles from partisan sources in the US across a variety of different issues. We first highlight that differences between sides exist by predicting the political leaning of articles of unseen political bias. Framing can be driven by different types of morality that each group values. We emphasize differences in framing of different news building on the moral foundations theory quantified using hand crafted lexicons. Our results show that partisan sources frame political issues differently both in terms of words usage and through the moral foundations they relate to.
Just as industrialization matured from mass production to customization and personalization, so has the Web migrated from generic content to public disclosures of one{'}s most intimately held thoughts, opinions and beliefs. This relatively new type of data is able to represent finer and more narrowly defined demographic slices. If until now researchers have primarily focused on leveraging personalized content to identify latent information such as gender, nationality, location, or age of the author, this study seeks to establish a structured way of extracting possessions, or items that people own or are entitled to, as a way to ultimately provide insights into people{'}s behaviors and characteristics. In order to promote more research in this area, we are releasing a set of 798 possessions extracted from blog genre, where possessions are marked at different confidence levels, as well as a detailed set of guidelines to help in future annotation studies.
The DARPA BOLT Information Retrieval evaluations target open-domain natural-language queries over a large corpus of informal text in English, Chinese and Egyptian Arabic. We outline the goals of BOLT IR, comparing it with the prior GALE Distillation task. After discussing the properties of the BOLT IR corpus, we provide a detailed description of the query creation process, contrasting the summary query format presented to systems at run time with the full query format created by annotators. We describe the relevance criteria used to assess BOLT system responses, highlighting the evolution of the procedures used over the three evaluation phases. We provide a detailed review of the decision points model for relevance assessment introduced during Phase 2, and conclude with information about inter-assessor consistency achieved with the decision points assessment model.
In this article, we present a method to validate a multi-lingual (English, Spanish, Russian, and Farsi) corpus on imageability ratings automatically expanded from MRCPD (Liu et al., 2014). We employed the corpus (Brysbaert et al., 2014) on concreteness ratings for our English MRCPD+ validation because of lacking human assessed imageability ratings and high correlation between concreteness ratings and imageability ratings (e.g. r = .83). For the same reason, we built a small corpus with human imageability assessment for the other language corpus validation. The results show that the automatically expanded imageability ratings are highly correlated with human assessment in all four languages, which demonstrate our automatic expansion method is valid and robust. We believe these new resources can be of significant interest to the research community, particularly in natural language processing and computational sociolinguistics.
In this paper, we put forward a strategy that supplements Hindi WordNet entries with information on the temporality of its word senses. Each synset of Hindi WordNet is automatically annotated to one of the five dimensions: past, present, future, neutral and atemporal. We use semi-supervised learning strategy to build temporal classifiers over the glosses of manually selected initial seed synsets. The classification process is iterated based on the repetitive confidence based expansion strategy of the initial seed list until cross-validation accuracy drops. The resource is unique in its nature as, to the best of our knowledge, still no such resource is available for Hindi.
Our work addresses automatic detection of enunciations and segments with reformulations in French spoken corpora. The proposed approach is syntagmatic. It is based on reformulation markers and specificities of spoken language. The reference data are built manually and have gone through consensus. Automatic methods, based on rules and CRF machine learning, are proposed in order to detect the enunciations and segments that contain reformulations. With the CRF models, different features are exploited within a window of various sizes. Detection of enunciations with reformulations shows up to 0.66 precision. The tests performed for the detection of reformulated segments indicate that the task remains difficult. The best average performance values reach up to 0.65 F-measure, 0.75 precision, and 0.63 recall. We have several perspectives to this work for improving the detection of reformulated segments and for studying the data from other points of view.
Negation is often found more frequent in dialogue than commonly written texts, such as literary texts. Furthermore, the scope and focus of negation depends on context in dialogues than other forms of texts. Existing negation datasets have focused on non-dialogue texts such as literary texts where the scope and focus of negation is normally present within the same sentence where the negation is located and therefore are not the most appropriate to inform the development of negation handling algorithms for dialogue-based systems. In this paper, we present DT -Neg corpus (DeepTutor Negation corpus) which contains texts extracted from tutorial dialogues where students interacted with an Intelligent Tutoring System (ITS) to solve conceptual physics problems. The DT -Neg corpus contains annotated negations in student responses with scope and focus marked based on the context of the dialogue. Our dataset contains 1,088 instances and is available for research purposes at http://language.memphis.edu/dt-neg.
This paper discusses the creation of a semantically annotated corpus of questions about patient data in electronic health records (EHRs). The goal is provide the training data necessary for semantic parsers to automatically convert EHR questions into a structured query. A layered annotation strategy is used which mirrors a typical natural language processing (NLP) pipeline. First, questions are syntactically analyzed to identify multi-part questions. Second, medical concepts are recognized and normalized to a clinical ontology. Finally, logical forms are created using a lambda calculus representation. We use a corpus of 446 questions asking for patient-specific information. From these, 468 specific questions are found containing 259 unique medical concepts and requiring 53 unique predicates to represent the logical forms. We further present detailed characteristics of the corpus, including inter-annotator agreement results, and describe the challenges automatic NLP systems will face on this task.
We present a new annotation scheme for normalizing time expressions, such as {``}three days ago{''}, to computer-readable forms, such as 2016-03-07. The annotation scheme addresses several weaknesses of the existing TimeML standard, allowing the representation of time expressions that align to more than one calendar unit (e.g., {``}the past three summers{''}), that are defined relative to events (e.g., {``}three weeks postoperative{''}), and that are unions or intersections of smaller time expressions (e.g., {``}Tuesdays and Thursdays{''}). It achieves this by modeling time expression interpretation as the semantic composition of temporal operators like UNION, NEXT, and AFTER. We have applied the annotation scheme to 34 documents so far, producing 1104 annotations, and achieving inter-annotator agreement of 0.821.
Proverbs are commonly metaphoric in nature and the mapping across domains is commonly established in proverbs. The abundance of proverbs in terms of metaphors makes them an extremely valuable linguistic resource since they can be utilized as a gold standard for various metaphor related linguistic tasks such as metaphor identification or interpretation. Besides, a collection of proverbs fromvarious languages annotated with metaphors would also be essential for social scientists to explore the cultural differences betweenthose languages. In this paper, we introduce PROMETHEUS, a dataset consisting of English proverbs and their equivalents in Italian.In addition to the word-level metaphor annotations for each proverb, PROMETHEUS contains other types of information such as the metaphoricity degree of the overall proverb, its meaning, the century that it was first recorded in and a pair of subjective questions responded by the annotators. To the best of our knowledge, this is the first multi-lingual and open-domain corpus of proverbs annotated with word-level metaphors.
This paper reports on the development of a French FrameNet, within the ASFALDA project. While the first phase of the project focused on the development of a French set of frames and corresponding lexicon (Candito et al., 2014), this paper concentrates on the subsequent corpus annotation phase, which focused on four notional domains (commercial transactions, cognitive stances, causality and verbal communication). Given full coverage is not reachable for a relatively {``}new{''} FrameNet project, we advocate that focusing on specific notional domains allowed us to obtain full lexical coverage for the frames of these domains, while partially reflecting word sense ambiguities. Furthermore, as frames and roles were annotated on two French Treebanks (the French Treebank (Abeill{\'e} and Barrier, 2004) and the Sequoia Treebank (Candito and Seddah, 2012), we were able to extract a syntactico-semantic lexicon from the annotated frames. In the resource{'}s current status, there are 98 frames, 662 frame evoking words, 872 senses, and about 13000 annotated frames, with their semantic roles assigned to portions of text. The French FrameNet is freely available at alpage.inria.fr/asfalda.
This paper reports a critical analysis of the ISO TimeML standard, in the light of several experiences of temporal annotation that were conducted on spoken French. It shows that the norm suffers from weaknesses that should be corrected to fit a larger variety of needs inNLP and in corpus linguistics. We present our proposition of some improvements of the norm before it will be revised by the ISO Committee in 2017. These modifications concern mainly (1) Enrichments of well identified features of the norm: temporal function of TIMEX time expressions, additional types for TLINK temporal relations; (2) Deeper modifications concerning the units or features annotated: clarification between time and tense for EVENT units, coherence of representation between temporal signals (the SIGNAL unit) and TIMEX modifiers (the MOD feature); (3) A recommendation to perform temporal annotation on top of a syntactic (rather than lexical) layer (temporal annotation on a treebank).
We present here a general set of semantic frames to annotate causal expressions, with a rich lexicon in French and an annotated corpus of about 5000 instances of causal lexical items with their corresponding semantic frames. The aim of our project is to have both the largest possible coverage of causal phenomena in French, across all parts of speech, and have it linked to a general semantic framework such as FN, to benefit in particular from the relations between other semantic frames, e.g., temporal ones or intentional ones, and the underlying upper lexical ontology that enable some forms of reasoning. This is part of the larger ASFALDA French FrameNet project, which focuses on a few different notional domains which are interesting in their own right (Djemma et al., 2016), including cognitive positions and communication frames. In the process of building the French lexicon and preparing the annotation of the corpus, we had to remodel some of the frames proposed in FN based on English data, with hopefully more precise frame definitions to facilitate human annotation. This includes semantic clarifications of frames and frame elements, redundancy elimination, and added coverage. The result is arguably a significant improvement of the treatment of causality in FN itself.
This paper presents a two-step methodology to annotate spatial knowledge on top of OntoNotes semantic roles. First, we manipulate semantic roles to automatically generate potential additional spatial knowledge. Second, we crowdsource annotations with Amazon Mechanical Turk to either validate or discard the potential additional spatial knowledge. The resulting annotations indicate whether entities are or are not located somewhere with a degree of certainty, and temporally anchor this spatial information. Crowdsourcing experiments show that the additional spatial knowledge is ubiquitous and intuitive to humans, and experimental results show that it can be inferred automatically using standard supervised machine learning techniques.
This article describes SPACEREF, a corpus of street-level geographic descriptions. Pedestrians are walking a route in a (real) urban environment, describing their actions. Their position is automatically logged, their speech is manually transcribed, and their references to objects are manually annotated with respect to a crowdsourced geographic database. We describe how the data was collected and annotated, and how it has been used in the context of creating resources for an automatic pedestrian navigation system.
This paper describes the procedure of semantic role labeling and the development of the first manually annotated Persian Proposition Bank (PerPB) which added a layer of predicate-argument information to the syntactic structures of Persian Dependency Treebank (known as PerDT). Through the process of annotating, the annotators could see the syntactic information of all the sentences and so they annotated 29982 sentences with more than 9200 unique verbs. In the annotation procedure, the direct syntactic dependents of the verbs were the first candidates for being annotated. So we did not annotate the other indirect dependents unless their phrasal heads were propositional and had their own arguments or adjuncts. Hence besides the semantic role labeling of verbs, the argument structure of 1300 unique propositional nouns and 300 unique propositional adjectives were annotated in the sentences, too. The accuracy of annotation process was measured by double annotation of the data at two separate stages and finally the data was prepared in the CoNLL dependency format.
We describe our ongoing effort to establish an annotation scheme for describing the semantic structures of research articles in the computer science domain, with the intended use of developing search systems that can refine their results by the roles of the entities denoted by the query keys. In our scheme, mentions of entities are annotated with ontology-based types, and the roles of the entities are annotated as relations with other entities described in the text. So far, we have annotated 400 abstracts from the ACL anthology and the ACM digital library. In this paper, the scheme and the annotated dataset are described, along with the problems found in the course of annotation. We also show the results of automatic annotation and evaluate the corpus in a practical setting in application to topic extraction.
We propose a way of enriching the TimeML annotations of TimeBank by adding information about the Topic Time in terms of Klein (1994). The annotations are partly automatic, partly inferential and partly manual. The corpus was converted into the native format of the annotation software GraphAnno and POS-tagged using the Stanford bidirectional dependency network tagger. On top of each finite verb, a FIN-node with tense information was created, and on top of any FIN-node, a TOPICTIME-node, in accordance with Klein{'}s (1994) treatment of finiteness as the linguistic correlate of the Topic Time. Each TOPICTIME-node is linked to a MAKEINSTANCE-node representing an (instantiated) event in TimeML (Pustejovsky et al. 2005), the markup language used for the annotation of TimeBank. For such links we introduce a new category, ELINK. ELINKs capture the relationship between the Topic Time (TT) and the Time of Situation (TSit) and have an aspectual interpretation in Klein{'}s (1994) theory. In addition to these automatic and inferential annotations, some TLINKs were added manually. Using an example from the corpus, we show that the inclusion of the Topic Time in the annotations allows for a richer representation of the temporal structure than does TimeML. A way of representing this structure in a diagrammatic form similar to the T-Box format (Verhagen, 2007) is proposed.
Out-Of-Vocabulary (OOV) words missed by Large Vocabulary Continuous Speech Recognition (LVCSR) systems can be recovered with the help of topic and semantic context of the OOV words captured from a diachronic text corpus. In this paper we investigate how the choice of documents for the diachronic text corpora affects the retrieval of OOV Proper Names (PNs) relevant to an audio document. We first present our diachronic French broadcast news datasets, which highlight the motivation of our study on OOV PNs. Then the effect of using diachronic text data from different sources and a different time span is analysed. With OOV PN retrieval experiments on French broadcast news videos, we conclude that a diachronic corpus with text from different sources leads to better retrieval performance than one relying on text from single source or from a longer time span.
This paper reports our work on building up a Cantonese Speech-to-Text (STT) system with a syllable based acoustic model. This is a part of an effort in building a STT system to aid dyslexic students who have cognitive deficiency in writing skills but have no problem expressing their ideas through speech. For Cantonese speech recognition, the basic unit of acoustic models can either be the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC) syllables where finals are further split into nucleus and coda to reflect the intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system is trained using the stochastic gradient descent optimization model with the aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model (DNN-HMM) with and without I-vector based speaker adaptive training technique. The input features of the same Gaussian Mixture Model with speaker adaptive training (GMM-SAT) to DNN are used in all cases. Experiments show that the ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the best performance with the word error rate (WER) of 9.66{\%} and the real time factor (RTF) of 1.38812.
This article presents the data collected and ASR systems developped for 4 sub-saharan african languages (Swahili, Hausa, Amharic and Wolof). To illustrate our methodology, the focus is made on Wolof (a very under-resourced language) for which we designed the first ASR system ever built in this language. All data and scripts are available online on our github repository.
In this paper we present SCALE, a new Python toolkit that contains two extensions to n-gram language models. The first extension is a novel technique to model compound words called Semantic Head Mapping (SHM). The second extension, Bag-of-Words Language Modeling (BagLM), bundles popular models such as Latent Semantic Analysis and Continuous Skip-grams. Both extensions scale to large data and allow the integration into first-pass ASR decoding. The toolkit is open source, includes working examples and can be found on http://github.com/jorispelemans/scale.
Text-to-speech has long been centered on the production of an intelligible message of good quality. More recently, interest has shifted to the generation of more natural and expressive speech. A major issue of existing approaches is that they usually rely on a manual annotation in expressive styles, which tends to be rather subjective. A typical related issue is that the annotation is strongly influenced ‚Äï and possibly biased ‚Äï by the semantic content of the text (e.g. a shot or a fault may incite the annotator to tag that sequence as expressing a high degree of excitation, independently of its acoustic realization). This paper investigates the assumption that human annotation of basketball commentaries in excitation levels can be automatically improved on the basis of acoustic features. It presents two techniques for label correction exploiting a Gaussian mixture and a proportional-odds logistic regression. The automatically re-annotated corpus is then used to train HMM-based expressive speech synthesizers, the performance of which is assessed through subjective evaluations. The results indicate that the automatic correction of the annotation with Gaussian mixture helps to synthesize more contrasted excitation levels, while preserving naturalness.
In 2012 the Bavarian Archive for Speech Signals started providing some of its tools from the field of spoken language in the form of Software as a Service (SaaS). This means users access the processing functionality over a web browser and therefore do not have to install complex software packages on a local computer. Amongst others, these tools include segmentation {\&} labeling, grapheme-to-phoneme conversion, text alignment, syllabification and metadata generation, where all but the last are available for a variety of languages. Since its creation the number of available services and the web interface have changed considerably. We give an overview and a detailed description of the system architecture, the available web services and their functionality. Furthermore, we show how the number of files processed over the system developed in the last four years.
This paper presents SPA, a web-based Speech Analytics platform that integrates several speech processing modules and that makes it possible to use them through the web. It was developed with the aim of facilitating the usage of the modules, without the need to know about software dependencies and specific configurations. Apart from being accessed by a web-browser, the platform also provides a REST API for easy integration with other applications. The platform is flexible, scalable, provides authentication for access restrictions, and was developed taking into consideration the time and effort of providing new services. The platform is still being improved, but it already integrates a considerable number of audio and text processing modules, including: Automatic transcription, speech disfluency classification, emotion detection, dialog act recognition, age and gender classification, non-nativeness detection, hyper-articulation detection, dialog act recognition, and two external modules for feature extraction and DTMF detection. This paper describes the SPA architecture, presents the already integrated modules, and provides a detailed description for the ones most recently integrated.
The {``}Corpus Oral Informatizado da Lingua Galega (CORILGA){''} project aims at building a corpus of oral language for Galician, primarily designed to study the linguistic variation and change. This project is currently under development and it is periodically enriched with new contributions. The long-term goal is that all the speech recordings will be enriched with phonetic, syllabic, morphosyntactic, lexical and sentence ELAN-complaint annotations. A way to speed up the process of annotation is to use automatic speech-recognition-based tools tailored to the application. Therefore, CORILGA repository has been enhanced with an automatic alignment tool, available to the administrator of the repository, that aligns speech with an orthographic transcription. In the event that no transcription, or just a partial one, were available, a speech recognizer for Galician is used to generate word and phonetic segmentations. These recognized outputs may contain errors that will have to be manually corrected by the administrator. For assisting this task, the tool also provides an ELAN tier with the confidence measure of each recognized word. In this paper, after the description of the main facts of the CORILGA corpus, the speech alignment and recognition tools are described. Both have been developed using the Kaldi toolkit.
Governments are increasingly utilising online platforms in order to engage with, and ascertain the opinions of, their citizens. Whilst policy makers could potentially benefit from such enormous feedback from society, they first face the challenge of making sense out of the large volumes of data produced. This creates a demand for tools and technologies which will enable governments to quickly and thoroughly digest the points being made and to respond accordingly. By determining the argumentative and dialogical structures contained within a debate, we are able to determine the issues which are divisive and those which attract agreement. This paper proposes a method of graph-based analytics which uses properties of graphs representing networks of arguments pro- {\&} con- in order to automatically analyse issues which divide citizens about new regulations. By future application of the most recent advances in argument mining, the results reported here will have a chance to scale up to enable sense-making of the vast amount of feedback received from citizens on directions that policy should take.
This paper describes metaTED ‚Äï a freely available corpus of metadiscursive acts in spoken language collected via crowdsourcing. Metadiscursive acts were annotated on a set of 180 randomly chosen TED talks in English, spanning over different speakers and topics. The taxonomy used for annotation is composed of 16 categories, adapted from Adel(2010). This adaptation takes into account both the material to annotate and the setting in which the annotation task is performed. The crowdsourcing setup is described, including considerations regarding training and quality control. The collected data is evaluated in terms of quantity of occurrences, inter-annotator agreement, and annotation related measures (such as average time on task and self-reported confidence). Results show different levels of agreement among metadiscourse acts (Œ± ‚àà [0.15; 0.49]). To further assess the collected material, a subset of the annotations was submitted to expert appreciation, who validated which of the marked occurrences truly correspond to instances of the metadiscursive act at hand. Similarly to what happened with the crowd, experts revealed different levels of agreement between categories (Œ± ‚àà [0.18; 0.72]). The paper concludes with a discussion on the applicability of metaTED with respect to each of the 16 categories of metadiscourse.
Quotation and opinion extraction, discourse and factuality have all partly addressed the annotation and identification of Attribution Relations. However, disjoint efforts have provided a partial and partly inaccurate picture of attribution and generated small or incomplete resources, thus limiting the applicability of machine learning approaches. This paper presents PARC 3.0, a large corpus fully annotated with Attribution Relations (ARs). The annotation scheme was tested with an inter-annotator agreement study showing satisfactory results for the identification of ARs and high agreement on the selection of the text spans corresponding to its constitutive elements: source, cue and content. The corpus, which comprises around 20k ARs, was used to investigate the range of structures that can express attribution. The results show a complex and varied relation of which the literature has addressed only a portion. PARC 3.0 is available for research use and can be used in a range of different studies to analyse attribution and validate assumptions as well as to develop supervised attribution extraction models.
We introduce improved guidelines for annotation of sentence specificity, addressing the issues encountered in prior work. Our annotation provides judgements of sentences in context. Rather than binary judgements, we introduce a specificity scale which accommodates nuanced judgements. Our augmented annotation procedure also allows us to define where in the discourse context the lack of specificity can be resolved. In addition, the cause of the underspecification is annotated in the form of free text questions. We present results from a pilot annotation with this new scheme and demonstrate good inter-annotator agreement. We found that the lack of specificity distributes evenly among immediate prior context, long distance prior context and no prior context. We find that missing details that are not resolved in the the prior context are more likely to trigger questions about the reason behind events, {``}why{''} and {``}how{''}. Our data is accessible at http://www.cis.upenn.edu/{\textasciitilde}nlp/corpora/lrec16spec.html
While the formal pragmatic concepts in information structure, such as the focus of an utterance, are precisely defined in theoretical linguistics and potentially very useful in conceptual and practical terms, it has turned out to be difficult to reliably annotate such notions in corpus data. We present a large-scale focus annotation effort designed to overcome this problem. Our annotation study is based on the tasked-based corpus CREG, which consists of answers to explicitly given reading comprehension questions. We compare focus annotation by trained annotators with a crowd-sourcing setup making use of untrained native speakers. Given the task context and an annotation process incrementally making the question form and answer type explicit, the trained annotators reach substantial agreement for focus annotation. Interestingly, the crowd-sourcing setup also supports high-quality annotation ‚Äï for specific subtypes of data. Finally, we turn to the question whether the relevance of focus annotation can be extrinsically evaluated. We show that automatic short-answer assessment significantly improves for focus annotated data. The focus annotated CREG corpus is freely available and constitutes the largest such resource for German.
Twitter-related studies often need to geo-locate Tweets or Twitter users, identifying their real-world geographic locations. As tweet-level geotagging remains rare, most prior work exploited tweet content, timezone and network information to inform geolocation, or else relied on off-the-shelf tools to geolocate users from location information in their user profiles. However, such user location metadata is not consistently structured, causing such tools to fail regularly, especially if a string contains multiple locations, or if locations are very fine-grained. We argue that user profile location (UPL) and tweet location need to be treated as distinct types of information from which differing inferences can be drawn. Here, we apply geoparsing to UPLs, and demonstrate how task performance can be improved by adapting our Edinburgh Geoparser, which was originally developed for processing English text. We present a detailed evaluation method and results, including inter-coder agreement. We demonstrate that the optimised geoparser can effectively extract and geo-reference multiple locations at different levels of granularity with an F1-score of around 0.90. We also illustrate how geoparsed UPLs can be exploited for international information trade studies and country-level sentiment analysis.
We can often detect from a person{'}s utterances whether he/she is in favor of or against a given target entity (a product, topic, another person, etc.). Here for the first time we present a dataset of tweets annotated for whether the tweeter is in favor of or against pre-chosen targets of interest‚Äïtheir stance. The targets of interest may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. The data pertains to six targets of interest commonly known and debated in the United States. Apart from stance, the tweets are also annotated for whether the target of interest is the target of opinion in the tweet. The annotations were performed by crowdsourcing. Several techniques were employed to encourage high-quality annotations (for example, providing clear and simple instructions) and to identify and discard poor annotations (for example, using a small set of check questions annotated by the authors). This Stance Dataset, which was subsequently also annotated for sentiment, can be used to better understand the relationship between stance, sentiment, entity relationships, and textual inference.
In this paper, we present an experiment to detect emotions in tweets. Unlike much previous research, we draw the important distinction between the tasks of emotion detection in a closed world assumption (i.e. every tweet is emotional) and the complicated task of identifying emotional versus non-emotional tweets. Given an apparent lack of appropriately annotated data, we created two corpora for these tasks. We describe two systems, one symbolic and one based on machine learning, which we evaluated on our datasets. Our evaluation shows that a machine learning classifier performs best on emotion detection, while a symbolic approach is better for identifying relevant (i.e. emotional) tweets.
The increasing streams of information pose challenges to both humans and machines. On the one hand, humans need to identify relevant information and consume only the information that lies at their interests. On the other hand, machines need to understand the information that is published in online data streams and generate concise and meaningful overviews. We consider events as prime factors to query for information and generate meaningful context. The focus of this paper is to acquire empirical insights for identifying salience features in tweets and news about a target event, i.e., the event of {``}whaling{''}. We first derive a methodology to identify such features by building up a knowledge space of the event enriched with relevant phrases, sentiments and ranked by their novelty. We applied this methodology on tweets and we have performed preliminary work towards adapting it to news articles. Our results show that crowdsourcing text relevance, sentiments and novelty (1) can be a main step in identifying salient information, and (2) provides a deeper and more precise understanding of the data at hand compared to state-of-the-art approaches.
Emojis allow us to describe objects, situations and even feelings with small images, providing a visual and quick way to communicate. In this paper, we analyse emojis used in Twitter with distributional semantic models. We retrieve 10 millions tweets posted by USA users, and we build several skip gram word embedding models by mapping in the same vectorial space both words and emojis. We test our models with semantic similarity experiments, comparing the output of our models with human assessment. We also carry out an exhaustive qualitative evaluation, showing interesting results.
Despite the recent success of distributional semantic models (DSMs) in various semantic tasks they remain disconnected with real-world perceptual cues since they typically rely on linguistic features. Text data constitute the dominant source of features for the majority of such models, although there is evidence from cognitive science that cues from other modalities contribute to the acquisition and representation of semantic knowledge. In this work, we propose the crossmodal extension of a two-tier text-based model, where semantic representations are encoded in the first layer, while the second layer is used for computing similarity between words. We exploit text- and image-derived features for performing computations at each layer, as well as various approaches for their crossmodal fusion. It is shown that the crossmodal model performs better (from 0.68 to 0.71 correlation coefficient) than the unimodal one for the task of similarity computation between words.
Recent efforts have focused on expanding the annotation coverage of PropBank from verb relations to adjective and noun relations, as well as light verb constructions (e.g., make an offer, take a bath). While each new relation type has presented unique annotation challenges, ensuring consistent and comprehensive annotation of light verb constructions has proved particularly challenging, given that light verb constructions are semi-productive, difficult to define, and there are often borderline cases. This research describes the iterative process of developing PropBank annotation guidelines for light verb constructions, the current guidelines, and a comparison to related resources.
Inconsistencies are part of any manually annotated corpus. Automatically finding these inconsistencies and correcting them (even manually) can increase the quality of the data. Past research has focused mainly on detecting inconsistency in syntactic annotation. This work explores new approaches to detecting inconsistency in semantic annotation. Two ranking methods are presented in this paper: a discrepancy ranking and an entropy ranking. Those methods are then tested and evaluated on multiple corpora annotated with multiword expressions and supersense labels. The results show considerable improvements in detecting inconsistency candidates over a random baseline. Possible applications of methods for inconsistency detection are improving the annotation procedure as well as the guidelines and correcting errors in completed annotations.
We announce a new language resource for research on semantic parsing, a large, carefully curated collection of semantic dependency graphs representing multiple linguistic traditions. This resource is called SDP{\textasciitilde}2016 and provides an update and extension to previous versions used as Semantic Dependency Parsing target representations in the 2014 and 2015 Semantic Evaluation Exercises. For a common core of English text, this third edition comprises semantic dependency graphs from four distinct frameworks, packaged in a unified abstract format and aligned at the sentence and token levels. SDP 2016 is the first general release of this resource and available for licensing from the Linguistic Data Consortium in May 2016. The data is accompanied by an open-source SDP utility toolkit and system results from previous contrastive parsing evaluations against these target representations.
Multi-pass sieve approaches have been successfully applied to entity coreference resolution and many other tasks in natural language processing (NLP), owing in part to the ease of designing high-precision rules for these tasks. However, the same is not true for event coreference resolution: typically lying towards the end of the standard information extraction pipeline, an event coreference resolver assumes as input the noisy outputs of its upstream components such as the trigger identification component and the entity coreference resolution component. The difficulty in designing high-precision rules makes it challenging to successfully apply a multi-pass sieve approach to event coreference resolution. In this paper, we investigate this challenge, proposing the first multi-pass sieve approach to event coreference resolution. When evaluated on the version of the KBP 2015 corpus available to the participants of EN Task 2 (Event Nugget Detection and Coreference), our approach achieves an Avg F-score of 40.32{\%}, outperforming the best participating system by 0.67{\%} in Avg F-score.
This project approaches the problem of language documentation and revitalization from a rather untraditional angle. To improve and facilitate language documentation of endangered languages, we attempt to use corpus linguistic methods and speech and language technologies to reduce the time needed for transcription and annotation of audio and video language recordings. The paper demonstrates this approach on the example of the endangered and seriously under-resourced variety of Eastern Chatino (CTP). We show how initial speech corpora can be created that can facilitate the development of speech and language technologies for under-resourced languages by utilizing Forced Alignment tools to time align transcriptions. Time-aligned transcriptions can be used to train speech corpora and utilize automatic speech recognition tools for the transcription and annotation of untranscribed data. Speech technologies can be used to reduce the time and effort necessary for transcription and annotation of large collections of audio and video recordings in digital language archives, addressing the transcription bottleneck problem that most language archives and many under-documented languages are confronted with. This approach can increase the availability of language resources from low-resourced and endangered languages to speech and language technology research and development.
In this paper, we describe a new corpus -named DIRHA-L2F RealCorpus- composed of typical home automation speech interactions in European Portuguese that has been recorded by the INESC-ID{'}s Spoken Language Systems Laboratory (L2F) to support the activities of the Distant-speech Interaction for Robust Home Applications (DIRHA) EU-funded project. The corpus is a multi-microphone and multi-room database of real continuous audio sequences containing read phonetically rich sentences, read and spontaneous keyword activation sentences, and read and spontaneous home automation commands. The background noise conditions are controlled and randomly recreated with noises typically found in home environments. Experimental validation on this corpus is reported in comparison with the results obtained on a simulated corpus using a fully automated speech processing pipeline for two fundamental automatic speech recognition tasks of typical {`}always-listening{'} home-automation scenarios: system activation and voice command recognition. Attending to results on both corpora, the presence of overlapping voice-like noise is shown as the main problem: simulated sequences contain concurrent speakers that result in general in a more challenging corpus, while real sequences performance drops drastically when TV or radio is on.
There exists a major incompatibility in emotion labeling framework among emotional speech corpora, that is, category-based and dimension-based. Commonizing these requires inter-corpus emotion labeling according to both frameworks, but doing this by human annotators is too costly for most cases. This paper examines the possibility of automatic cross-corpus emotion labeling. In order to evaluate the effectiveness of the automatic labeling, a comprehensive emotion annotation for two conversational corpora, UUDB and OGVC, was performed. With a state-of-the-art machine learning technique, dimensional and categorical emotion estimation models were trained and tested against the two corpora. For the emotion dimension estimation, the automatic cross-corpus emotion labeling for the different corpus was effective for the dimensions of aroused-sleepy, dominant-submissive and interested-indifferent, showing only slight performance degradation against the result for the same corpus. On the other hand, the performance for the emotion category estimation was not sufficient.
Speech-enabled interfaces have the potential to become one of the most efficient and ergonomic environments for human-computer interaction and for text production. However, not much research has been carried out to investigate in detail the processes and strategies involved in the different modes of text production. This paper introduces and evaluates a corpus of more than 55 hours of English-to-Japanese user activity data that were collected within the ENJA15 project, in which translators were observed while writing and speaking translations (translation dictation) and during machine translation post-editing. The transcription of the spoken data, keyboard logging and eye-tracking data were recorded with Translog-II, post-processed and integrated into the CRITT Translation Process Research-DB (TPR-DB), which is publicly available under a creative commons license. The paper presents the ENJA15 data as part of a large multilingual Chinese, Danish, German, Hindi and Spanish translation process data collection of more than 760 translation sessions. It compares the ENJA15 data with the other language pairs and reviews some of its particularities.
In the design of controlled experiments with language stimuli, researchers from psycholinguistic, neurolinguistic, and related fields, require language resources that isolate variables known to affect language processing. This article describes a freely available database that provides word level statistics for words and nonwords of Mandarin, Chinese. The featured lexical statistics include subtitle corpus frequency, phonological neighborhood density, neighborhood frequency, and homophone density. The accompanying word descriptors include pinyin, ascii phonetic transcription (sampa), lexical tone, syllable structure, dominant PoS, and syllable, segment and pinyin lengths for each phonological word. It is designed for researchers particularly concerned with language processing of isolated words and made to accommodate multiple existing hypotheses concerning the structure of the Mandarin syllable. The database is divided into multiple files according to the desired search criteria: 1) the syllable segmentation schema used to calculate density measures, and 2) whether the search is for words or nonwords. The database is open to the research community at https://github.com/karlneergaard/Mandarin-Neighborhood-Statistics.
TEITOK is a web-based framework for corpus creation, annotation, and distribution, that combines textual and linguistic annotation within a single TEI based XML document. TEITOK provides several built-in NLP tools to automatically (pre)process texts, and is highly customizable. It features multiple orthographic transcription layers, and a wide range of user-defined token-based annotations. For searching, TEITOK interfaces with a local CQP server. TEITOK can handle various types of additional resources including Facsimile images and linked audio files, making it possible to have a combined written/spoken corpus. It also has additional modules for PSDX syntactic annotation and several types of stand-off annotation.
We present texigt, a command-line tool for the extraction of structured linguistic data from LaTeX source documents, and a language resource that has been generated using this tool: a corpus of interlinear glossed text (IGT) extracted from open access books published by Language Science Press. Extracted examples are represented in a simple XML format that is easy to process and can be used to validate certain aspects of interlinear glossed text. The main challenge involved is the parsing of TeX and LaTeX documents. We review why this task is impossible in general and how the texhs Haskell library uses a layered architecture and selective early evaluation (expansion) during lexing and parsing in order to provide access to structured representations of LaTeX documents at several levels. In particular, its parsing modules generate an abstract syntax tree for LaTeX documents after expansion of all user-defined macros and lexer-level commands that serves as an ideal interface for the extraction of interlinear glossed text by texigt. This architecture can easily be adapted to extract other types of linguistic data structures from LaTeX source documents.
Natural language processing applications are frequently integrated to solve complex linguistic problems, but the lack of interoperability between these tools tends to be one of the main issues found in that process. That is often caused by the different linguistic formats used across the applications, which leads to attempts to both establish standard formats to represent linguistic information and to create conversion tools to facilitate this integration. Pepper is an example of the latter, as a framework that helps the conversion between different linguistic annotation formats. In this paper, we describe the use of Pepper to convert a corpus linguistically annotated by the annotation scheme AWA into the relANNIS format, with the ultimate goal of interacting with AWA documents through the ANNIS interface. The experiment converted 40 megabytes of AWA documents, allowed their use on the ANNIS interface, and involved making architectural decisions during the mapping from AWA into relANNIS using Pepper. The main issues faced during this process were due to technical issues mainly caused by the integration of the different systems and projects, namely AWA, Pepper and ANNIS.
Text preprocessing is an important and necessary task for all NLP applications. A simple variation in any preprocessing step may drastically affect the final results. Moreover replicability and comparability, as much as feasible, is one of the goals of our scientific enterprise, thus building systems that can ensure the consistency in our various pipelines would contribute significantly to our goals. The problem has become quite pronounced with the abundance of NLP tools becoming more and more available yet with different levels of specifications. In this paper, we present a dynamic unified preprocessing framework and tool, SPLIT, that is highly configurable based on user requirements which serves as a preprocessing tool for several tools at once. SPLIT aims to standardize the implementations of the most important preprocessing steps by allowing for a unified API that could be exchanged across different researchers to ensure complete transparency in replication. The user is able to select the required preprocessing tasks among a long list of preprocessing steps. The user is also able to specify the order of execution which in turn affects the final preprocessing output.
Swiss dialects of German are, unlike most dialects of well standardised languages, widely used in everyday communication. Despite this fact, automatic processing of Swiss German is still a considerable challenge due to the fact that it is mostly a spoken variety rarely recorded and that it is subject to considerable regional variation. This paper presents a freely available general-purpose corpus of spoken Swiss German suitable for linguistic research, but also for training automatic tools. The corpus is a result of a long design process, intensive manual work and specially adapted computational processing. We first describe how the documents were transcribed, segmented and aligned with the sound source, and how inconsistent transcriptions were unified through an additional normalisation layer. We then present a bootstrapping approach to automatic normalisation using different machine-translation-inspired methods. Furthermore, we evaluate the performance of part-of-speech taggers on our data and show how the same bootstrapping approach improves part-of-speech tagging by 10{\%} over four rounds. Finally, we present the modalities of access of the corpus as well as the data format.
We present experiments on word segmentation for Akkadian cuneiform, an ancient writing system and a language used for about 3 millennia in the ancient Near East. To our best knowledge, this is the first study of this kind applied to either the Akkadian language or the cuneiform writing system. As a logosyllabic writing system, cuneiform structurally resembles Eastern Asian writing systems, so, we employ word segmentation algorithms originally developed for Chinese and Japanese. We describe results of rule-based algorithms, dictionary-based algorithms, statistical and machine learning approaches. Our results may indicate possible promising steps in cuneiform word segmentation that can create and improve natural language processing in this area.
In this paper, we presented the annotation propagation tool we designed to be used in conjunction with the BRAT rapid annotation tool. We designed two experiments to annotate a corpus of 60 files, first not using our tool, second using our propagation tool. We evaluated the annotation time and the quality of annotations. We shown that using the annotation propagation tool reduces by 31.7{\%} the time spent to annotate the corpus with a better quality of results.
A potential work item (PWI) for ISO standard (MAP) about linguistic annotation concerning syntax-semantics mapping is discussed. MAP is a framework for graphical linguistic annotation to specify a mapping (set of combinations) between possible syntactic and semantic structures of the annotated linguistic data. Just like a UML diagram, a MAP diagram is formal, in the sense that it accurately specifies such a mapping. MAP provides a diagrammatic sort of concrete syntax for linguistic annotation far easier to understand than textual concrete syntax such as in XML, so that it could better facilitate collaborations among people involved in research, standardization, and practical use of linguistic data. MAP deals with syntactic structures including dependencies, coordinations, ellipses, transsentential constructions, and so on. Semantic structures treated by MAP are argument structures, scopes, coreferences, anaphora, discourse relations, dialogue acts, and so forth. In order to simplify explicit annotations, MAP allows partial descriptions, and assumes a few general rules on correspondence between syntactic and semantic compositions.
When designing Natural Language Processing (NLP) applications that use Machine Learning (ML) techniques, feature extraction becomes a significant part of the development effort, whether developing a new application or attempting to reproduce results reported for existing NLP tasks. We present EDISON, a Java library of feature generation functions used in a suite of state-of-the-art NLP tools, based on a set of generic NLP data structures. These feature extractors populate simple data structures encoding the extracted features, which the package can also serialize to an intuitive JSON file format that can be easily mapped to formats used by ML packages. EDISON can also be used programmatically with JVM-based (Java/Scala) NLP software to provide the feature extractor input. The collection of feature extractors is organised hierarchically and a simple search interface is provided. In this paper we include examples that demonstrate the versatility and ease-of-use of the EDISON feature extraction suite to show that this can significantly reduce the time spent by developers on feature extraction design for NLP systems. The library is publicly hosted at https://github.com/IllinoisCogComp/illinois-cogcomp-nlp/, and we hope that other NLP researchers will contribute to the set of feature extractors. In this way, the community can help simplify reproduction of published results and the integration of ideas from diverse sources when developing new and improved NLP applications.
This paper introduces MADAD, a general-purpose annotation tool for Arabic text with focus on readability annotation. This tool will help in overcoming the problem of lack of Arabic readability training data by providing an online environment to collect readability assessments on various kinds of corpora. Also the tool supports a broad range of annotation tasks for various linguistic and semantic phenomena by allowing users to create their customized annotation schemes. MADAD is a web-based tool, accessible through any web browser; the main features that distinguish MADAD are its flexibility, portability, customizability and its bilingual interface (Arabic/English).
This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8{\%} accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change.
Gender differences in language use have long been of interest in linguistics. The task of automatic gender attribution has been considered in computational linguistics as well. Most research of this type is done using (usually English) texts with authorship metadata. In this paper, we propose a new method of male/female corpus creation based on gender-specific first-person expressions. The method was applied on CommonCrawl Web corpus for Polish (language, in which gender-revealing first-person expressions are particularly frequent) to yield a large (780M words) and varied collection of men{'}s and women{'}s texts. The whole procedure for building the corpus and filtering out unwanted texts is described in the present paper. The quality check was done on a random sample of the corpus to make sure that the majority (84{\%}) of texts are correctly attributed, natural texts. Some preliminary (socio)linguistic insights (websites and words frequently occurring in male/female fragments) are given as well.
We describe COHERE, our coherence toolkit which incorporates various complementary models for capturing and measuring different aspects of text coherence. In addition to the traditional entity grid model (Lapata, 2005) and graph-based metric (Guinaudeau and Strube, 2013), we provide an implementation of a state-of-the-art syntax-based model (Louis and Nenkova, 2012), as well as an adaptation of this model which shows significant performance improvements in our experiments. We benchmark these models using the standard setting for text coherence: original documents and versions of the document with sentences in shuffled order.
With the constant growth of the scientific literature, automated processes to enable access to its contents are increasingly in demand. Several functional discourse annotation schemes have been proposed to facilitate information extraction and summarisation from scientific articles, the most well known being argumentative zoning. Core Scientific concepts (CoreSC) is a three layered fine-grained annotation scheme providing content-based annotations at the sentence level and has been used to index, extract and summarise scientific publications in the biomedical literature. A previously developed CoreSC corpus on which existing automated tools have been trained contains a single annotation for each sentence. However, it is the case that more than one CoreSC concept can appear in the same sentence. Here, we present the Multi-CoreSC CRA corpus, a text corpus specific to the domain of cancer risk assessment (CRA), consisting of 50 full text papers, each of which contains sentences annotated with one or more CoreSCs. The full text papers have been annotated by three biology experts. We present several inter-annotator agreement measures appropriate for multi-label annotation assessment. Employing several inter-annotator agreement measures, we were able to identify the most reliable annotator and we built a harmonised consensus (gold standard) from the three different annotators, while also taking concept priority (as specified in the guidelines) into account. We also show that the new Multi-CoreSC CRA corpus allows us to improve performance in the recognition of CoreSCs. The updated guidelines, the multi-label CoreSC CRA corpus and other relevant, related materials are available at the time of publication at http://www.sapientaproject.com/.
The growth of social networking platforms has drawn a lot of attentions to the need for social computing. Social computing utilises human insights for computational tasks as well as design of systems that support social behaviours and interactions. One of the key aspects of social computing is the ability to attribute responsibility such as blame or praise to social events. This ability helps an intelligent entity account and understand other intelligent entities{'} social behaviours, and enriches both the social functionalities and cognitive aspects of intelligent agents. In this paper, we present an approach with a model for blame and praise detection in text. We build our model based on various theories of blame and include in our model features used by humans determining judgment such as moral agent causality, foreknowledge, intentionality and coercion. An annotated corpus has been created for the task of blame and praise detection from text. The experimental results show that while our model gives similar results compared to supervised classifiers on classifying text as blame, praise or others, it outperforms supervised classifiers on more finer-grained classification of determining the direction of blame and praise, i.e., self-blame, blame-others, self-praise or praise-others, despite not using labelled training data.
Word embeddings have recently seen a strong increase in interest as a result of strong performance gains on a variety of tasks. However, most of this research also underlined the importance of benchmark datasets, and the difficulty of constructing these for a variety of language-specific tasks. Still, many of the datasets used in these tasks could prove to be fruitful linguistic resources, allowing for unique observations into language use and variability. In this paper we demonstrate the performance of multiple types of embeddings, created with both count and prediction-based architectures on a variety of corpora, in two language-specific tasks: relation evaluation, and dialect identification. For the latter, we compare unsupervised methods with a traditional, hand-crafted dictionary. With this research, we provide the embeddings themselves, the relation evaluation task benchmark for use in further research, and demonstrate how the benchmarked embeddings prove a useful unsupervised linguistic resource, effectively used in a downstream task.
In this paper we introduce the Satirical Language Resource: a dataset containing a balanced collection of satirical and non satirical news texts from various domains. This is the first dataset of this magnitude and scope in the domain of satire. We envision this dataset will facilitate studies on various aspects of of sat- ire in news articles. We test the viability of our data on the task of classification of satire.
Wikipedia has become one of the most popular resources in natural language processing and it is used in quantities of applications. However, Wikipedia requires a substantial pre-processing step before it can be used. For instance, its set of nonstandardized annotations, referred to as the wiki markup, is language-dependent and needs specific parsers from language to language, for English, French, Italian, etc. In addition, the intricacies of the different Wikipedia resources: main article text, categories, wikidata, infoboxes, scattered into the article document or in different files make it difficult to have global view of this outstanding resource. In this paper, we describe WikiParq, a unified format based on the Parquet standard to tabulate and package the Wikipedia corpora. In combination with Spark, a map-reduce computing framework, and the SQL query language, WikiParq makes it much easier to write database queries to extract specific information or subcorpora from Wikipedia, such as all the first paragraphs of the articles in French, or all the articles on persons in Spanish, or all the articles on persons that have versions in French, English, and Spanish. WikiParq is available in six language versions and is potentially extendible to all the languages of Wikipedia. The WikiParq files are downloadable as tarball archives from this location: http://semantica.cs.lth.se/wikiparq/.
Code-switching texts are those that contain terms in two or more different languages, and they appear increasingly often in social media. The aim of this paper is to provide a resource to the research community to evaluate the performance of sentiment classification techniques on this complex multilingual environment, proposing an English-Spanish corpus of tweets with code-switching (EN-ES-CS CORPUS). The tweets are labeled according to two well-known criteria used for this purpose: SentiStrength and a trinary scale (positive, neutral and negative categories). Preliminary work on the resource is already done, providing a set of baselines for the research community.
Semantic relations play an important role in linguistic knowledge representation. Although their role is relevant in the context of written text, there is no approach or dataset that makes use of contextuality of classic semantic relations beyond the boundary of one sentence. We present the SemRelData dataset that contains annotations of semantic relations between nominals in the context of one paragraph. To be able to analyse the universality of this context notion, the annotation was performed on a multi-lingual and multi-genre corpus. To evaluate the dataset, it is compared to large, manually created knowledge resources in the respective languages. The comparison shows that knowledge bases not only have coverage gaps; they also do not account for semantic relations that are manifested in particular contexts only, yet still play an important role for text cohesion.
In this paper we describe our effort to create a dataset for the evaluation of cross-language textual similarity detection. We present preexisting corpora and their limits and we explain the various gathered resources to overcome these limits and build our enriched dataset. The proposed dataset is multilingual, includes cross-language alignment for different granularities (from chunk to document), is based on both parallel and comparable corpora and contains human and machine translated texts. Moreover, it includes texts written by multiple types of authors (from average to professionals). With the obtained dataset, we conduct a systematic and rigorous evaluation of several state-of-the-art cross-language textual similarity detection methods. The evaluation results are reviewed and discussed. Finally, dataset and scripts are made publicly available on GitHub: http://github.com/FerreroJeremy/Cross-Language-Dataset.
In this paper, we describe our effort in the development and annotation of a large scale corpus containing code-switched data. Until recently, very limited effort has been devoted to develop computational approaches or even basic linguistic resources to support research into the processing of Moroccan Darija.
In the recent years, Linked Data and Language Technology solutions gained popularity. Nevertheless, their coupling in real-world business is limited due to several issues. Existing products and services are developed for a particular domain, can be used only in combination with already integrated datasets or their language coverage is limited. In this paper, we present an innovative solution FREME - an open framework of e-Services for multilingual and semantic enrichment of digital content. The framework integrates six interoperable e-Services. We describe the core features of each e-Service and illustrate their usage in the context of four business cases: i) authoring and publishing; ii) translation and localisation; iii) cross-lingual access to data; and iv) personalised Web content recommendations. Business cases drive the design and development of the framework.
There is a rich flora of word space models that have proven their efficiency in many different applications including information retrieval (Dumais, 1988), word sense disambiguation (Schutze, 1992), various semantic knowledge tests (Lund et al., 1995; Karlgren, 2001), and text categorization (Sahlgren, 2005). Based on the assumption that each model captures some aspects of word meanings and provides its own empirical evidence, we present in this paper a systematic exploration of the principal corpus-based word space models for bilingual terminology extraction from comparable corpora. We find that, once we have identified the best procedures, a very simple combination approach leads to significant improvements compared to individual models.
We present MultiVec, a new toolkit for computing continuous representations for text at different granularity levels (word-level or sequences of words). MultiVec includes word2vec{'}s features, paragraph vector (batch and online) and bivec for bilingual distributed representations. MultiVec also includes different distance measures between words and sequences of words. The toolkit is written in C++ and is aimed at being fast (in the same order of magnitude as word2vec), easy to use, and easy to extend. It has been evaluated on several NLP tasks: the analogical reasoning task, sentiment analysis, and crosslingual document classification.
Statistical Machine Translation (SMT) relies on the availability of rich parallel corpora. However, in the case of under-resourced languages or some specific domains, parallel corpora are not readily available. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data. However, most efforts have been related to European languages and less in middle-east languages. In this study, we report comparable corpora created from news articles for the pair English ‚Äï{Arabic, Persian, Urdu} languages. The data has been collected over a period of a year, entails Arabic, Persian and Urdu languages. Furthermore using the English as a pivot language, comparable corpora that involve more than one language can be created, e.g. English- Arabic - Persian, English - Arabic - Urdu, English ‚Äï Urdu - Persian, etc. Upon request the data can be provided for research purposes.
We describe a monolingual English corpus of original and (human) translated texts, with an accurate annotation of speaker properties, including the original language of the utterances and the speaker{'}s country of origin. We thus obtain three sub-corpora of texts reflecting native English, non-native English, and English translated from a variety of European languages. This dataset will facilitate the investigation of similarities and differences between these kinds of sub-languages. Moreover, it will facilitate a unified comparative study of translations and language produced by (highly fluent) non-native speakers, two closely-related phenomena that have only been studied in isolation so far.
In an intercomprehension scenario, typically a native speaker of language L1 is confronted with output from an unknown, but related language L2. In this setting, the degree to which the receiver recognizes the unfamiliar words greatly determines communicative success. Despite exhibiting great string-level differences, cognates may be recognized very successfully if the receiver is aware of regular correspondences which allow to transform the unknown word into its familiar form. Modeling L1-L2 intercomprehension then requires the identification of all the regular correspondences between languages L1 and L2. We here present a set of linguistic orthographic correspondences manually compiled from comparative linguistics literature along with a set of statistically-inferred suggestions for correspondence rules. In order to do statistical inference, we followed the Minimum Description Length principle, which proposes to choose those rules which are most effective at describing the data. Our statistical model was able to reproduce most of our linguistic correspondences (88.5{\%} for Czech-Polish and 75.7{\%} for Bulgarian-Russian) and furthermore allowed to easily identify many more non-trivial correspondences which also cover aspects of morphology.
This paper describes the project called Axolotl which comprises a Spanish-Nahuatl parallel corpus and its search interface. Spanish and Nahuatl are distant languages spoken in the same country. Due to the scarcity of digital resources, we describe the several problems that arose when compiling this corpus: most of our sources were non-digital books, we faced errors when digitizing the sources and there were difficulties in the sentence alignment process, just to mention some. The documents of the parallel corpus are not homogeneous, they were extracted from different sources, there is dialectal, diachronical, and orthographical variation. Additionally, we present a web search interface that allows to make queries through the whole parallel corpus, the system is capable to retrieve the parallel fragments that contain a word or phrase searched by a user in any of the languages. To our knowledge, this is the first Spanish-Nahuatl public available digital parallel corpus. We think that this resource can be useful to develop language technologies and linguistic studies for this language pair.
Bilingual communities often alternate between languages both in spoken and written communication. One such community, Germany residents of Turkish origin produce Turkish-German code-switching, by heavily mixing two languages at discourse, sentence, or word level. Code-switching in general, and Turkish-German code-switching in particular, has been studied for a long time from a linguistic perspective. Yet resources to study them from a more computational perspective are limited due to either small size or licence issues. In this work we contribute the solution of this problem with a corpus. We present a Turkish-German code-switching corpus which consists of 1029 tweets, with a majority of intra-sentential switches. We share different type of code-switching we have observed in our collection and describe our processing steps. The first step is data collection and filtering. This is followed by manual tokenisation and normalisation. And finally, we annotate data with word-level language identification information. The resulting corpus is available for research purposes.
In this work, we present the Language Computer Corporation (LCC) annotated metaphor datasets, which represent the largest and most comprehensive resource for metaphor research to date. These datasets were produced over the course of three years by a staff of nine annotators working in four languages (English, Spanish, Russian, and Farsi). As part of these datasets, we provide (1) metaphoricity ratings for within-sentence word pairs on a four-point scale, (2) scored links to our repository of 114 source concept domains and 32 target concept domains, and (3) ratings for the affective polarity and intensity of each pair. Altogether, we provide 188,741 annotations in English (for 80,100 pairs), 159,915 annotations in Spanish (for 63,188 pairs), 99,740 annotations in Russian (for 44,632 pairs), and 137,186 annotations in Farsi (for 57,239 pairs). In addition, we are providing a large set of likely metaphors which have been independently extracted by our two state-of-the-art metaphor detection systems but which have not been analyzed by our team of annotators.
We present our effort to create a large Multi-Layered representational repository of Linguistic Code-Switched Arabic data. The process involves developing clear annotation standards and Guidelines, streamlining the annotation process, and implementing quality control measures. We used two main protocols for annotation: in-lab gold annotations and crowd sourcing annotations. We developed a web-based annotation tool to facilitate the management of the annotation process. The current version of the repository contains a total of 886,252 tokens that are tagged into one of sixteen code-switching tags. The data exhibits code switching between Modern Standard Arabic and Egyptian Dialectal Arabic representing three data genres: Tweets, commentaries, and discussion fora. The overall Inter-Annotator Agreement is 93.1{\%}.
The overarching objective underlying this research is to develop an online tool, based on a parallel corpus of French Belgian Sign Language (LSFB) and written Belgian French. This tool is aimed to assist various set of tasks related to the comparison of LSFB and French, to the benefit of general users as well as teachers in bilingual schools, translators and interpreters, as well as linguists. These tasks include (1) the comprehension of LSFB or French texts, (2) the production of LSFB or French texts, (3) the translation between LSFB and French in both directions and (4) the contrastive analysis of these languages. The first step of investigation aims at creating an unidirectional French-LSFB concordancer, able to align a one- or multiple-word expression from the French translated text with its corresponding expressions in the videotaped LSFB productions. We aim at testing the efficiency of this concordancer for the extraction of a dictionary of meanings in context. In this paper, we will present the modelling of the different data sources at our disposal and specifically the way they interact with one another.
In this paper we present the newly created parallel corpus of two under-resourced languages, namely, Macedonian-Croatian Parallel Corpus (mk-hr{\_}pcorp) that has been collected during 2015 at the Faculty of Humanities and Social Sciences, University of Zagreb. The mk-hr{\_}pcorp is a unidirectional (mk‚Üíhr) parallel corpus composed of synchronic fictional prose texts received already in digital form with over 500 Kw in each language. The corpus was sentence segmented and provides 39,735 aligned sentences. The alignment was done automatically and then post-corrected manually. The alignments order was shuffled and this enabled the corpus to be available under CC-BY license through META-SHARE. However, this prevents the research in language units over the sentence level.
The Aranea Project is targeted at creation of a family of Gigaword web-corpora for a dozen of languages that could be used for teaching language- and linguistics-related subjects at Slovak universities, as well as for research purposes in various areas of linguistics. All corpora are being built according to a standard methodology and using the same set of tools for processing and annotation, which ‚Äï together with their standard size and‚Äï makes them also a valuable resource for translators and contrastive studies. All our corpora are freely available either via a web interface or in a source form in an annotated vertical format.
The listener{'}s gazing activities during utterances were analyzed in a face-to-face three-party conversation setting. The function of each utterance was categorized according to the Grounding Acts defined by Traum (Traum, 1994) so that gazes during utterances could be analyzed from the viewpoint of grounding in communication (Clark, 1996). Quantitative analysis showed that the listeners were gazing at the speakers more in the second language (L2) conversation than in the native language (L1) conversation during the utterances that added new pieces of information, suggesting that they are using visual information to compensate for their lack of linguistic proficiency in L2 conversation.
The Multi-language Speech (MLS) Corpus supports NIST{'}s Language Recognition Evaluation series by providing new conversational telephone speech and broadcast narrowband data in 20 languages/dialects. The corpus was built with the intention of testing system performance in the matter of distinguishing closely related or confusable linguistic varieties, and careful manual auditing of collected data was an important aspect of this work. This paper lists the specific data requirements for the collection and provides both a commentary on the rationale for those requirements as well as an outline of the various steps taken to ensure all goals were met as specified. LDC conducted a large-scale recruitment effort involving the implementation of candidate assessment and interview techniques suitable for hiring a large contingent of telecommuting workers, and this recruitment effort is discussed in detail. We also describe the telephone and broadcast collection infrastructure and protocols, and provide details of the steps taken to pre-process collected data prior to auditing. Finally, annotation training, procedures and outcomes are presented in detail.
We present FlexTag, a highly flexible PoS tagging framework. In contrast to monolithic implementations that can only be retrained but not adapted otherwise, FlexTag enables users to modify the feature space and the classification algorithm. Thus, FlexTag makes it easy to quickly develop custom-made taggers exactly fitting the research problem.
In this paper we present newly developed inflectional lexcions and manually annotated corpora of Croatian and Serbian. We introduce hrLex and srLex - two freely available inflectional lexicons of Croatian and Serbian - and describe the process of building these lexicons, supported by supervised machine learning techniques for lemma and paradigm prediction. Furthermore, we introduce hr500k, a manually annotated corpus of Croatian, 500 thousand tokens in size. We showcase the three newly developed resources on the task of morphosyntactic annotation of both languages by using a recently developed CRF tagger. We achieve best results yet reported on the task for both languages, beating the HunPos baseline trained on the same datasets by a wide margin.
TGermaCorp is a German text corpus whose primary sources are collected from German literature texts which date from the sixteenth century to the present. The corpus is intended to represent its target language (German) in syntactic, lexical, stylistic and chronological diversity. For this purpose, it is hand-annotated on several linguistic layers, including POS, lemma, named entities, multiword expressions, clauses, sentences and paragraphs. In order to introduce TGermaCorp in comparison to more homogeneous corpora of contemporary everyday language, quantitative assessments of syntactic and lexical diversity are provided. In this respect, TGermaCorp contributes to establishing characterising features for resource descriptions, which is needed for keeping track of a meaningful comparison of the ever-growing number of natural language resources. The assessments confirm the special role of proper names, whose propagation in text may influence lexical and syntactic diversity measures in rather trivial ways. TGermaCorp will be made available via hucompute.org.
In this work we present the open source hunvec framework for sequential tagging, built upon Theano and Pylearn2. The underlying statistical model, which connects linear CRF-s with neural networks, was used by Collobert and co-workers, and several other researchers. For demonstrating the flexibility of our tool, we describe a set of experiments on part-of-speech and named-entity-recognition tasks, using English and Hungarian datasets, where we modify both model and training parameters, and illustrate the usage of custom features. Model parameters we experiment with affect the vectorial word representations used by the model; we apply different word vector initializations, defined by Word2vec and GloVe embeddings and enrich the representation of words by vectors assigned trigram features. We extend training methods by using their regularized (l2 and dropout) version. When testing our framework on a Hungarian named entity corpus, we find that its performance reaches the best published results on this dataset, with no need for language-specific feature engineering. Our code is available at http://github.com/zseder/hunvec
Most Arabic natural language processing tools and resources are developed to serve Modern Standard Arabic (MSA), which is the official written language in the Arab World. Some Dialectal Arabic varieties, notably Egyptian Arabic, have received some attention lately and have a growing collection of resources that include annotated corpora and morphological analyzers and taggers. Gulf Arabic, however, lags behind in that respect. In this paper, we present the Gumar Corpus, a large-scale corpus of Gulf Arabic consisting of 110 million words from 1,200 forum novels. We annotate the corpus for sub-dialect information at the document level. We also present results of a preliminary study in the morphological annotation of Gulf Arabic which includes developing guidelines for a conventional orthography. The text of the corpus is publicly browsable through a web interface we developed for it.
Automatic natural language processing of large texts often presents recurring challenges in multiple languages: even for most advanced tasks, the texts are first processed by basic processing steps {--} from tokenization to parsing. We present an extremely simple-to-use tool consisting of one binary and one model (per language), which performs these tasks for multiple languages without the need for any other external data. UDPipe, a pipeline processing CoNLL-U-formatted files, performs tokenization, morphological analysis, part-of-speech tagging, lemmatization and dependency parsing for nearly all treebanks of Universal Dependencies 1.2 (namely, the whole pipeline is currently available for 32 out of 37 treebanks). In addition, the pipeline is easily trainable with training data in CoNLL-U format (and in some cases also with additional raw corpora) and requires minimal linguistic knowledge on the users{'} part. The training code is also released.
We present a novel technique for Arabic morphological annotation. The technique utilizes diacritization to produce morphological annotations of quality comparable to human annotators. Although Arabic text is generally written without diacritics, diacritization is already available for large corpora of Arabic text in several genres. Furthermore, diacritization can be generated at a low cost for new text as it does not require specialized training beyond what educated Arabic typists know. The basic approach is to enrich the input to a state-of-the-art Arabic morphological analyzer with word diacritics (full or partial) to enhance its performance. When applied to fully diacritized text, our approach produces annotations with an accuracy of over 97{\%} on lemma, part-of-speech, and tokenization combined.
Part-of-speech tagging is a basic step in Natural Language Processing that is often essential. Labeling the word forms of a text with fine-grained word-class information adds new value to it and can be a prerequisite for downstream processes like a dependency parser. Corpus linguists and lexicographers also benefit greatly from the improved search options that are available with tagged data. The Albanian language has some properties that pose difficulties for the creation of a part-of-speech tagset. In this paper, we discuss those difficulties and present a proposal for a part-of-speech tagset that can adequately represent the underlying linguistic phenomena.
Like most of the languages which have only recently started being investigated for the Natural Language Processing (NLP) tasks, Amazigh lacks annotated corpora and tools and still suffers from the scarcity of linguistic tools and resources. The main aim of this paper is to present a new part-of-speech (POS) tagger based on a new Amazigh tag set (AMTS) composed of 28 tags. In line with our goal we have trained Conditional Random Fields (CRFs) to build a POS tagger for the Amazigh language. We have used the 10-fold technique to evaluate and validate our approach. The CRFs 10 folds average level is 87.95{\%} and the best fold level result is 91.18{\%}. In order to improve this result, we have gathered a set of about 8k words with their POS tags. The collected lexicon was used with CRFs confidence measure in order to have a more accurate POS-tagger. Hence, we have obtained a better performance of 93.82{\%}.
In this paper, we investigate unsupervised and semi-supervised methods for part-of-speech (PoS) tagging in the context of historical German text. We locate our research in the context of Digital Humanities where the non-canonical nature of text causes issues facing an Natural Language Processing world in which tools are mainly trained on standard data. Data deviating from the norm requires tools adjusted to this data. We explore to which extend the availability of such training material and resources related to it influences the accuracy of PoS tagging. We investigate a variety of algorithms including neural nets, conditional random fields and self-learning techniques in order to find the best-fitted approach to tackle data sparsity. Although methods using resources from related languages outperform weakly supervised methods using just a few training examples, we can still reach a promising accuracy with methods abstaining additional resources.
In this paper we present the ongoing efforts to expand the depth and breath of the Open Multilingual Wordnet coverage by introducing two new classes of non-referential concepts to wordnet hierarchies: interjections and numeral classifiers. The lexical semantic hierarchy pioneered by Princeton Wordnet has traditionally restricted its coverage to referential and contentful classes of words: such as nouns, verbs, adjectives and adverbs. Previous efforts have been employed to enrich wordnet resources including, for example, the inclusion of pronouns, determiners and quantifiers within their hierarchies. Following similar efforts, and motivated by the ongoing semantic annotation of the NTU-Multilingual Corpus, we decided that the four traditional classes of words present in wordnets were too restrictive. Though non-referential, interjections and classifiers possess interesting semantics features that can be well captured by lexical resources like wordnets. In this paper, we will further motivate our decision to include non-referential concepts in wordnets and give an account of the current state of this expansion.
We present a WordNet like structured resource for slang words and neologisms on the internet. The dynamism of language is often an indication that current language technology tools trained on today{'}s data, may not be able to process the language in the future. Our resource could be (1) used to augment the WordNet, (2) used in several Natural Language Processing (NLP) applications which make use of noisy data on the internet like Information Retrieval and Web Mining. Such a resource can also be used to distinguish slang word senses from conventional word senses. To stimulate similar innovations widely in the NLP community, we test the efficacy of our resource for detecting slang using standard bag of words Word Sense Disambiguation (WSD) algorithms (Lesk and Extended Lesk) for English data on the internet.
Although represented as such in wordnets, word senses are not discrete. To handle word senses as fuzzy objects, we exploit the graph structure of synonymy pairs acquired from different sources to discover synsets where words have different membership degrees that reflect confidence. Following this approach, a wide-coverage fuzzy thesaurus was discovered from a synonymy network compiled from seven Portuguese lexical-semantic resources. Based on a crowdsourcing evaluation, we can say that the quality of the obtained synsets is far from perfect but, as expected in a confidence measure, it increases significantly for higher cut-points on the membership and, at a certain point, reaches 100{\%} correction rate.
We present the Hebrew FrameNet project, describe the development and annotation processes and enumerate the challenges we faced along the way. We have developed semi-automatic tools to help speed the annotation and data collection process. The resource currently covers 167 frames, 3,000 lexical units and about 500 fully annotated sentences. We have started training and testing automatic SRL tools on the seed data.
The availability of openly available textual datasets ({``}corpora{''}) with highly accurate manual annotations ({``}gold standard{''}) of named entities (e.g. persons, locations, organizations, etc.) is crucial in the training and evaluation of named entity recognition systems. Currently there are only few such datasets available on the web, and even less for texts containing historical spelling variation. The production and subsequent release into the public domain of four such datasets with 100 pages each for the languages Dutch, French, German (including Austrian) as part of the Europeana Newspapers project is expected to contribute to the further development and improvement of named entity recognition systems with a focus on historical content. This paper describes how these datasets were produced, what challenges were encountered in their creation and informs about their final quality and availability.
Among all researches dedicating to terminology and word sense disambiguation, little attention has been devoted to the ambiguity of term occurrences. If a lexical unit is indeed a term of the domain, it is not true, even in a specialised corpus, that all its occurrences are terminological. Some occurrences are terminological and other are not. Thus, a global decision at the corpus level about the terminological status of all occurrences of a lexical unit would then be erroneous. In this paper, we propose three original methods to characterise the ambiguity of term occurrences in the domain of social sciences for French. These methods differently model the context of the term occurrences: one is relying on text mining, the second is based on textometry, and the last one focuses on text genre properties. The experimental results show the potential of the proposed approaches and give an opportunity to discuss about their hybridisation.
In order to analyze metrical and semantics aspects of poetry in Spanish with computational techniques, we have developed a large corpus annotated with metrical information. In this paper we will present and discuss the development of this corpus: the formal representation of metrical patterns, the semi-automatic annotation process based on a new automatic scansion system, the main annotation problems, and the evaluation, in which an inter-annotator agreement of 96{\%} has been obtained. The corpus is open and available.
This paper poses the question, how linguistic corpus-based research may be enriched by the exploitation of conceptual text structures and layout as provided via TEI annotation. Examples for possible areas of research and usage scenarios are provided based on the German historical corpus of the Deutsches Textarchiv (DTA) project, which has been consistently tagged accordant to the TEI Guidelines, more specifically to the DTA ‚Ä∫Base Format‚Äπ (DTABf). The paper shows that by including TEI-XML structuring in corpus-based analyses significances can be observed for different linguistic phenomena, as e.g. the development of conceptual text structures themselves, the syntactic embedding of terms in certain conceptual text structures, and phenomena of language change which become obvious via the layout of a text. The exemplary study carried out here shows some of the potential for the exploitation of TEI annotation for linguistic research, which might be kept in mind when making design decisions for new corpora as well when working with existing TEI corpora.
Entity linking has become a popular task in both natural language processing and semantic web communities. However, we find that the benchmark datasets for entity linking tasks do not accurately evaluate entity linking systems. In this paper, we aim to chart the strengths and weaknesses of current benchmark datasets and sketch a roadmap for the community to devise better benchmark datasets.
Streaming media provides a number of unique challenges for computational linguistics. This paper studies the temporal variation in word co-occurrence statistics, with application to event detection. We develop a spectral clustering approach to find groups of mutually informative terms occurring in discrete time frames. Experiments on large datasets of tweets show that these groups identify key real world events as they occur in time, despite no explicit supervision. The performance of our method rivals state-of-the-art methods for event detection on F-score, obtaining higher recall at the expense of precision.
Joint inference approaches such as Integer Linear Programming (ILP) and Markov Logic Networks (MLNs) have recently been successfully applied to many natural language processing (NLP) tasks, often outperforming their pipeline counterparts. However, MLNs are arguably much less popular among NLP researchers than ILP. While NLP researchers who desire to employ these joint inference frameworks do not necessarily have to understand their theoretical underpinnings, it is imperative that they understand which of them should be applied under what circumstances. With the goal of helping NLP researchers better understand the relative strengths and weaknesses of MLNs and ILP; we will compare them along different dimensions of interest, such as expressiveness, ease of use, scalability, and performance. To our knowledge, this is the first systematic comparison of ILP and MLNs on an NLP task.
A significant portion of data generated on blogging and microblogging websites is non-credible as shown in many recent studies. To filter out such non-credible information, machine learning can be deployed to build automatic credibility classifiers. However, as in the case with most supervised machine learning approaches, a sufficiently large and accurate training data must be available. In this paper, we focus on building a public Arabic corpus of blogs and microblogs that can be used for credibility classification. We focus on Arabic due to the recent popularity of blogs and microblogs in the Arab World and due to the lack of any such public corpora in Arabic. We discuss our data acquisition approach and annotation process, provide rigid analysis on the annotated data and finally report some results on the effectiveness of our data for credibility classification.
Active learning (AL) is often used in corpus construction (CC) for selecting {``}informative{''} documents for annotation. This is ideal for focusing annotation efforts when all documents cannot be annotated, but has the limitation that it is carried out in a closed-loop, selecting points that will improve an existing model. For phenomena-driven and exploratory CC, the lack of existing-models and specific task(s) for using it make traditional AL inapplicable. In this paper we propose a novel method for model-free AL utilising characteristics of phenomena for applying AL to select documents for annotation. The method can also supplement traditional closed-loop AL-based CC to extend the utility of the corpus created beyond a single task. We introduce our tool, MOVE, and show its potential with a real world case-study.
This paper presents some work on direct and indirect speech in Portuguese using corpus-based methods: we report on a study whose aim was to identify (i) Portuguese verbs used to introduce reported speech and (ii) syntactic patterns used to convey reported speech, in order to enhance the performance of a quotation extraction system, dubbed QUEMDISSE?. In addition, (iii) we present a Portuguese corpus annotated with reported speech, using the lexicon and rules provided by (i) and (ii), and discuss the process of their annotation and what was learned.
One of the most pressing questions in cognitive science remains unanswered: what cognitive mechanisms enable children to learn any of the world{'}s 7000 or so languages? Much discovery has been made with regard to specific learning mechanisms in specific languages, however, given the remarkable diversity of language structures (Evans and Levinson 2009, Bickel 2014) the burning question remains: what are the underlying processes that make language acquisition possible, despite substantial cross-linguistic variation in phonology, morphology, syntax, etc.? To investigate these questions, a comprehensive cross-linguistic database of longitudinal child language acquisition corpora from maximally diverse languages has been built.
Annotating and predicting behavioural aspects in conversations is becoming critical in the conversational analytics industry. In this paper we look into inter-annotator agreement of agent behaviour dimensions on two call center corpora. We find that the task can be annotated consistently over time, but that subjectivity issues impacts the quality of the annotation. The reformulation of some of the annotated dimensions is suggested in order to improve agreement.
In 2016, we set about building a large-scale corpus of everyday Japanese conversation‚Äïa collection of conversations embedded in naturally occurring activities in daily life. We will collect more than 200 hours of recordings over six years,publishing the corpus in 2022. To construct such a huge corpus, we have conducted a pilot project, one of whose purposes is to establish a corpus design for collecting various kinds of everyday conversations in a balanced manner. For this purpose, we conducted a survey of everyday conversational behavior, with about 250 adults, in order to reveal how diverse our everyday conversational behavior is and to build an empirical foundation for corpus design. The questionnaire included when, where, how long,with whom, and in what kind of activity informants were engaged in conversations. We found that ordinary conversations show the following tendencies: i) they mainly consist of chats, business talks, and consultations; ii) in general, the number of participants is small and the duration of the conversation is short; iii) many conversations are conducted in private places such as homes, as well as in public places such as offices and schools; and iv) some questionnaire items are related to each other. This paper describes an overview of this survey study, and then discusses how to design a large-scale corpus of everyday Japanese conversation on this basis.
This papers describes a data collection setup and a newly recorded dataset. The main purpose of this dataset is to explore patterns in the focus of visual attention of humans under three different conditions - two humans involved in task-based interaction with a robot; same two humans involved in task-based interaction where the robot is replaced by a third human, and a free three-party human interaction. The dataset contains two parts - 6 sessions with duration of approximately 3 hours and 9 sessions with duration of approximately 4.5 hours. Both parts of the dataset are rich in modalities and recorded data streams - they include the streams of three Kinect v2 devices (color, depth, infrared, body and face data), three high quality audio streams, three high resolution GoPro video streams, touch data for the task-based interactions and the system state of the robot. In addition, the second part of the dataset introduces the data streams from three Tobii Pro Glasses 2 eye trackers. The language of all interactions is English and all data streams are spatially and temporally aligned.
Large scale corpora have benefited many areas of research in natural language processing, but until recently, resources for dialogue have lagged behind. Now, with the emergence of large scale social media websites incorporating a threaded dialogue structure, content feedback, and self-annotation (such as stance labeling), there are valuable new corpora available to researchers. In previous work, we released the INTERNET ARGUMENT CORPUS, one of the first larger scale resources available for opinion sharing dialogue. We now release the INTERNET ARGUMENT CORPUS 2.0 (IAC 2.0) in the hope that others will find it as useful as we have. The IAC 2.0 provides more data than IAC 1.0 and organizes it using an extensible, repurposable SQL schema. The database structure in conjunction with the associated code facilitates querying from and combining multiple dialogically structured data sources. The IAC 2.0 schema provides support for forum posts, quotations, markup (bold, italic, etc), and various annotations, including Stanford CoreNLP annotations. We demonstrate the generalizablity of the schema by providing code to import the ConVote corpus.
Casual multiparty conversation is an understudied but very common genre of spoken interaction, whose analysis presents a number of challenges in terms of data scarcity and annotation. We describe the annotation process used on the d64 and DANS multimodal corpora of multiparty casual talk, which have been manually segmented, transcribed, annotated for laughter and disfluencies, and aligned using the Penn Aligner. We also describe a visualization tool, STAVE, developed during the annotation process, which allows long stretches of talk or indeed entire conversations to be viewed, aiding preliminary identification of features and patterns worthy of analysis. It is hoped that this tool will be of use to other researchers working in this field.
In order to develop its full potential, global communication needs linguistic support systems such as Machine Translation (MT). In the past decade, free online MT tools have become available to the general public, and the quality of their output is increasing. However, the use of such tools may entail various legal implications, especially as far as processing of personal data is concerned. This is even more evident if we take into account that their business model is largely based on providing translation in exchange for data, which can subsequently be used to improve the translation model, but also for commercial purposes. The purpose of this paper is to examine how free online MT tools fit in the European data protection framework, harmonised by the EU Data Protection Directive. The perspectives of both the user and the MT service provider are taken into account.
This paper introduces a novel research tool for the field of linguistics: The Lin|gu|is|tik web portal provides a virtual library which offers scientific information on every linguistic subject. It comprises selected internet sources and databases as well as catalogues for linguistic literature, and addresses an interdisciplinary audience. The virtual library is the most recent outcome of the Special Subject Collection Linguistics of the German Research Foundation (DFG), and also integrates the knowledge accumulated in the Bibliography of Linguistic Literature. In addition to the portal, we describe long-term goals and prospects with a special focus on ongoing efforts regarding an extension towards integrating language resources and Linguistic Linked Open Data.
Since mobile devices have feature-rich configurations and provide diverse functions, the use of mobile devices combined with the language resources of cloud environments is high promising for achieving a wide range communication that goes beyond the current language barrier. However, there are mismatches between using resources of mobile devices and services in the cloud such as the different communication protocol and different input and output methods. In this paper, we propose a language service infrastructure for mobile environments to combine these services. The proposed language service infrastructure allows users to use and mashup existing language resources on both cloud environments and their mobile devices. Furthermore, it allows users to flexibly use services in the cloud or services on mobile devices in their composite service without implementing several different composite services that have the same functionality. A case study of Mobile Shopping Translation System using both a service in the cloud (translation service) and services on mobile devices (Bluetooth low energy (BLE) service and text-to-speech service) is introduced.
In this paper, we discuss the requirements that a long lasting linguistic database should have in order to meet the needs of the linguists together with the aim of durability and sharing of data. In particular, we discuss the generalizability of the Syntactic Atlas of Italy, a linguistic project that builds on a long standing tradition of collecting and analyzing linguistic corpora, on a more recent project that focuses on the synchronic and diachronic analysis of the syntax of Italian and Portuguese relative clauses. The results that are presented are in line with the FLaReNet Strategic Agenda that highlighted the most pressing needs for research areas, such as Natural Language Processing, and presented a set of recommendations for the development and progress of Language resources in Europe.
The infrastructure Global Open Resources and Information for Language and Linguistic Analysis (GORILLA) was created as a resource that provides a bridge between disciplines such as documentary, theoretical, and corpus linguistics, speech and language technologies, and digital language archiving services. GORILLA is designed as an interface between digital language archive services and language data producers. It addresses various problems of common digital language archive infrastructures. At the same time it serves the speech and language technology communities by providing a platform to create and share speech and language data from low-resourced and endangered languages. It hosts an initial collection of language models for speech and natural language processing (NLP), and technologies or software tools for corpus creation and annotation. GORILLA is designed to address the Transcription Bottleneck in language documentation, and, at the same time to provide solutions to the general Language Resource Bottleneck in speech and language technologies. It does so by facilitating the cooperation between documentary and theoretical linguistics, and speech and language technologies research and development, in particular for low-resourced and endangered languages.
This paper introduces an open source, interoperable generic software tool set catering for the entire workflow of creation, migration, annotation, query and analysis of multi-layer linguistic corpora. It consists of four components: Salt, a graph-based meta model and API for linguistic data, the common data model for the rest of the tool set; Pepper, a conversion tool and platform for linguistic data that can be used to convert many different linguistic formats into each other; Atomic, an extensible, platform-independent multi-layer desktop annotation software for linguistic corpora; ANNIS, a search and visualization architecture for multi-layer linguistic corpora with many different visualizations and a powerful native query language. The set was designed to solve the following issues in a multi-layer corpus workflow: Lossless data transition between tools through a common data model generic enough to allow for a potentially unlimited number of different types of annotation, conversion capabilities for different linguistic formats to cater for the processing of data from different sources and/or with existing annotations, a high level of extensibility to enhance the sustainability of the whole tool set, analysis capabilities encompassing corpus and annotation query alongside multi-faceted visualizations of all annotation layers.
In this paper, I describe a method of creating massively huge web corpora from the CommonCrawl data sets and redistributing the resulting annotations in a stand-off format. Current EU (and especially German) copyright legislation categorically forbids the redistribution of downloaded material without express prior permission by the authors. Therefore, such stand-off annotations (or other derivates) are the only format in which European researchers (like myself) are allowed to re-distribute the respective corpora. In order to make the full corpora available to the public despite such restrictions, the stand-off format presented here allows anybody to locally reconstruct the full corpora with the least possible computational effort.
This paper presents a new Web-based annotation tool, the {``}CLARIN-EL Web-based Annotation Tool{''}. Based on an existing annotation infrastructure offered by the {``}Ellogon{''} language enginneering platform, this new tool transfers a large part of Ellogon{'}s features and functionalities to a Web environment, by exploiting the capabilities of cloud computing. This new annotation tool is able to support a wide range of annotation tasks, through user provided annotation schemas in XML. The new annotation tool has already been employed in several annotation tasks, including the anotation of arguments, which is presented as a use case. The CLARIN-EL annotation tool is compared to existing solutions along several dimensions and features. Finally, future work includes the improvement of integration with the CLARIN-EL infrastructure, and the inclusion of features not currently supported, such as the annotation of aligned documents.
This paper presents two alternative NLP architectures to analyze massive amounts of documents, using parallel processing. The two architectures focus on different processing scenarios, namely batch-processing and streaming processing. The batch-processing scenario aims at optimizing the overall throughput of the system, i.e., minimizing the overall time spent on processing all documents. The streaming architecture aims to minimize the time to process real-time incoming documents and is therefore especially suitable for live feeds. The paper presents experiments with both architectures, and reports the overall gain when they are used for batch as well as for streaming processing. All the software described in the paper is publicly available under free licenses.
The Trove Newspaper Corpus is derived from the National Library of Australia{'}s digital archive of newspaper text. The corpus is a snapshot of the NLA collection taken in 2015 to be made available for language research as part of the Alveo Virtual Laboratory and contains 143 million articles dating from 1806 to 2007. This paper describes the work we have done to make this large corpus available as a research collection, facilitating access to individual documents and enabling large scale processing of the newspaper text in a cloud-based environment.
In this paper we describe the new developments brought to LRE Map, especially in terms of the user interface of the Web application, of the searching of the information therein, and of the data model updates.
In 2014, the Swedish government tasked a Swedish agency, The Swedish Post and Telecom Authority (PTS), with investigating how to best create and populate an infrastructure for spoken language resources (Ref N2014/2840/ITP). As a part of this work, the department of Speech, Music and Hearing at KTH Royal Institute of Technology have taken inventory of existing potential spoken language resources, mainly in Swedish national archives and other governmental or public institutions. In this position paper, key priorities, perspectives, and strategies that may be of general, rather than Swedish, interest are presented. We discuss broad types of potential spoken language resources available; to what extent these resources are free to use; and thirdly the main contribution: strategies to ensure the continuous acquisition of spoken language resources in a manner that facilitates speech and speech technology research.
To allow an easy understanding of the various licenses that exist for the use of Language Resources (ELRA{'}s, META-SHARE{'}s, Creative Commons{'}, etc.), ELRA has developed a License Wizardto help the right-holders share/distribute their resources under the appropriate license. It also aims to be exploited by users to better understand the legal obligations that apply in various licensing situations. The present paper elaborates on the License Wizard functionalities of this web configurator, which enables to select a number of legal features and obtain the user license adapted to the users selection, to define which user licenses they would like to select in order to distribute their Language Resources, to integrate the user license terms into a Distribution Agreement that could be proposed to ELRA or META-SHARE for further distribution through the ELRA Catalogue of Language Resources. Thanks to a flexible back office, the structure of the legal feature selection can easily be reviewed to include other features that may be relevant for other licenses. Integrating contributions from other initiatives thus aim to be one of the obvious next steps, with a special focus on CLARIN and Linked Data experiences.
With the support of the DGLFLF, ELDA conducted an inventory of existing language resources for the regional languages of France. The main aim of this inventory was to assess the exploitability of the identified resources within technologies. A total of 2,299 Language Resources were identified. As a second step, a deeper analysis of a set of three language groups (Breton, Occitan, overseas languages) was carried out along with a focus of their exploitability within three technologies: automatic translation, voice recognition/synthesis and spell checkers. The survey was followed by the organisation of the TLRF2015 Conference which aimed to present the state of the art in the field of the Technologies for Regional Languages of France. The next step will be to activate the network of specialists built up during the TLRF conference and to begin the organisation of a second TLRF conference. Meanwhile, the French Ministry of Culture continues its actions related to linguistic diversity and technology, in particular through a project with Wikimedia France related to contributions to Wikipedia in regional languages, the upcoming new version of the {``}Corpus de la Parole{''} and the reinforcement of the DGLFLF{'}s Observatory of Linguistic Practices.
This paper documents and describes the criteria used to select languages for study within programs that include low resource languages whether given that label or another similar one. It focuses on five US common task, Human Language Technology research and development programs in which the authors have provided information or consulting related to the choice of language. The paper does not describe the actual selection process which is the responsibility of program management and highly specific to a program{'}s individual goals and context. Instead it concentrates on the data and criteria that have been considered relevant previously with the thought that future program managers and their consultants may adapt these and apply them with different prioritization to future programs.
This paper discusses the role that statistical machine translation (SMT) can play in the development of cross-border EU e-commerce,by highlighting extant obstacles and identifying relevant technologies to overcome them. In this sense, it firstly proposes a typology of e-commerce static and dynamic textual genres and it identifies those that may be more successfully targeted by SMT. The specific challenges concerning the automatic translation of user-generated content are discussed in detail. Secondly, the paper highlights the risk of data sparsity inherent to e-commerce and it explores the state-of-the-art strategies to achieve domain adequacy via adaptation. Thirdly, it proposes a robust workflow for the development of SMT systems adapted to the e-commerce domain by relying on inexpensive methods. Given the scarcity of user-generated language corpora for most language pairs, the paper proposes to obtain monolingual target-language data to train language models and aligned parallel corpora to tune and evaluate MT systems by means of crowdsourcing.
ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7{\%}, against a baseline of 57.2{\%} (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline. hypernyms-co-hyponyms 95.7{\%} vs. 69.8{\%}, hypernyms-random 91.8{\%} vs. 64.1{\%} and co-hyponyms-random 97.8{\%} vs. 79.4{\%}. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.
In this paper, we claim that Vector Cosine ‚Äï which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models ‚Äï can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that ‚Äï independently of the adopted parameters ‚Äï outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50{\%}) and several state-of-the-art approaches.
The paper investigates the relation between metaphoricity and distributional characteristics of verbs, introducing POM, a corpus-derived index that can be used to define the upper bound of metaphoricity of any expression in which a given verb occurs. The work moves from the observation that while some verbs can be used to create highly metaphoric expressions, others can not. We conjecture that this fact is related to the number of contexts in which a verb occurs and to the frequency of each context. This intuition is modelled by introducing a method in which each context of a verb in a corpus is assigned a vector representation, and a clustering algorithm is employed to identify similar contexts. Eventually, the Standard Deviation of the relative frequency values of the clusters is computed and taken as the POM of the target verb. We tested POM in two experimental settings obtaining values of accuracy of 84{\%} and 92{\%}. Since we are convinced, along with (Shutoff, 2015), that metaphor detection systems should be concerned only with the identification of highly metaphoric expressions, we believe that POM could be profitably employed by these systems to a priori exclude expressions that, due to the verb they include, can only have low degrees of metaphoricity
This work presents a practical system for indexing terms and relations from French radiology reports, called IMAIOS. In this paper, we present how semantic relations (causes, consequences, symptoms, locations, parts...) between medical terms can be extracted. For this purpose, we handcrafted some linguistic patterns from on a subset of our radiology report corpora. As semantic patterns (de (of)) may be too general or ambiguous, semantic constraints have been added. For instance, in the sentence n{\'e}oplasie du sein (neoplasm of breast) the system knowing neoplasm as a disease and breast as an anatomical location, identify the relation as being a location: neoplasm r-lieu breast. An evaluation of the effect of semantic constraints is proposed.
Distributional semantic models (DSMs) are currently being used in the measurement of word relatedness and word similarity. One shortcoming of DSMs is that they do not provide a principled way to discriminate different semantic relations. Several approaches have been adopted that rely on annotated data either in the training of the model or later in its evaluation. In this paper, we introduce a dataset for training and evaluating DSMs on semantic relations discrimination between words, in Mandarin, Chinese. The construction of the dataset followed EVALution 1.0, which is an English dataset for the training and evaluating of DSMs. The dataset contains 360 relation pairs, distributed in five different semantic relations, including antonymy, synonymy, hypernymy, meronymy and nearsynonymy. All relation pairs were checked manually to estimate their quality. In the 360 word relation pairs, there are 373 relata. They were all extracted and subsequently manually tagged according to their semantic type. The relatas{'} frequency was calculated in a combined corpus of Sinica and Chinese Gigaword. To the best of our knowledge, EVALution-MAN is the first of its kind for Mandarin, Chinese.
We present a statistical system for identifying the semantic relationships or semantic roles for two major Indian Languages, Hindi and Urdu. Given an input sentence and a predicate/verb, the system first identifies the arguments pertaining to that verb and then classifies it into one of the semantic labels which can either be a DOER, THEME, LOCATIVE, CAUSE, PURPOSE etc. The system is based on 2 statistical classifiers trained on roughly 130,000 words for Urdu and 100,000 words for Hindi that were hand-annotated with semantic roles under the PropBank project for these two languages. Our system achieves an accuracy of 86{\%} in identifying the arguments of a verb for Hindi and 75{\%} for Urdu. At the subsequent task of classifying the constituents into their semantic roles, the Hindi system achieved 58{\%} precision and 42{\%} recall whereas Urdu system performed better and achieved 83{\%} precision and 80{\%} recall. Our study also allowed us to compare the usefulness of different linguistic features and feature combinations in the semantic role labeling task. We also examine the use of statistical syntactic parsing as feature in the role labeling task.
Verb aspect is a grammatical and lexical category that encodes temporal unfolding and duration of events described by verbs. It is a potentially interesting source of information for various computational tasks, but has so far not been studied in much depth from the perspective of automatic processing. Slavic languages are particularly interesting in this respect, as they encode aspect through complex and not entirely consistent lexical derivations involving prefixation and suffixation. Focusing on Croatian and Serbian, in this paper we propose a novel framework for automatic classification of their verb types into a number of fine-grained aspectual classes based on the observable morphology of verb forms. In addition, we provide a set of around 2000 verbs classified based on our framework. This set can be used for linguistic research as well as for testing automatic classification on a larger scale. With minor adjustments the approach is also applicable to other Slavic languages
Entailment recognition approaches are useful for application domains such as information extraction, question answering or summarisation, for which evidence from multiple sentences needs to be combined. We report on a new 3-way judgement Recognizing Textual Entailment (RTE) resource that originates in the Social Media domain, and explain our semi-automatic creation method for the special purpose of information verification, which draws on manually established rumourous claims reported during crisis events. From about 500 English tweets related to 70 unique claims we compile and evaluate 5.4k RTE pairs, while continue automatizing the workflow to generate similar-sized datasets in other languages.
We present the specification for a modeling language, VoxML, which encodes semantic knowledge of real-world objects represented as three-dimensional models, and of events and attributes related to and enacted over these objects.VoxML is intended to overcome the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a variety of systems and platforms, leading to multimodal simulations of real-world scenarios using conceptual objects that represent their semantic values
Metonymy is a figure of speech in which one item{'}s name represents another item that usually has a close relation with the first one. Metonymic expressions need to be correctly detected and interpreted because sentences including such expressions have different mean- ings from literal ones; computer systems may output inappropriate results in natural language processing. In this paper, an associative approach for analyzing metonymic expressions is proposed. By using associative information and two conceptual distances between words in a sentence, a previous method is enhanced and a decision tree is trained to detect metonymic expressions. After detecting these expressions, they are interpreted as metonymic understanding words by using associative information. This method was evaluated by comparing it with two baseline methods based on previous studies on the Japanese language that used case frames and co-occurrence information. As a result, the proposed method exhibited significantly better accuracy (0.85) of determining words as metonymic or literal expressions than the baselines. It also exhibited better accuracy (0.74) of interpreting the detected metonymic expressions than the baselines.
Our ability to understand language often relies on common-sense knowledge ‚Äï background information the speaker can assume is known by the reader. Similarly, our comprehension of the language used in complex domains relies on access to domain-specific knowledge. Capturing common-sense and domain-specific knowledge can be achieved by taking advantage of recent advances in open information extraction (IE) techniques and, more importantly, of knowledge embeddings, which are multi-dimensional representations of concepts and relations. Building a knowledge graph for representing common-sense knowledge in which concepts discerned from noun phrases are cast as vertices and lexicalized relations are cast as edges leads to learning the embeddings of common-sense knowledge accounting for semantic compositionality as well as implied knowledge. Common-sense knowledge is acquired from a vast collection of blogs and books as well as from WordNet. Similarly, medical knowledge is learned from two large sets of electronic health records. The evaluation results of these two forms of knowledge are promising: the same knowledge acquisition methodology based on learning knowledge embeddings works well both for common-sense knowledge and for medical knowledge Interestingly, the common-sense knowledge that we have acquired was evaluated as being less neutral than than the medical knowledge, as it often reflected the opinion of the knowledge utterer. In addition, the acquired medical knowledge was evaluated as more plausible than the common-sense knowledge, reflecting the complexity of acquiring common-sense knowledge due to the pragmatics and economicity of language.
In recent years, we have seen an increasing amount of interest in low-dimensional vector representations of words. Among other things, these facilitate computing word similarity and relatedness scores. The most well-known example of algorithms to produce representations of this sort are the word2vec approaches. In this paper, we investigate a new model to induce such vector spaces for medical concepts, based on a joint objective that exploits not only word co-occurrences but also manually labeled documents, as available from sources such as PubMed. Our extensive experimental analysis shows that our embeddings lead to significantly higher correlations with human similarity and relatedness assessments than previous work. Due to the simplicity and versatility of vector representations, these findings suggest that our resource can easily be used as a drop-in replacement to improve any systems relying on medical concept similarity measures.
We present a corpus and a knowledge database aiming at developing Question-Answering in a new context, the open world of a video game. We chose a popular game called {`}Minecraft{'}, and created a QA corpus with a knowledge database related to this game and the ontology of a meaning representation that will be used to structure this database. We are interested in the logic rules specific to the game, which may not exist in the real world. The ultimate goal of this research is to build a QA system that can answer natural language questions from players by using inference on these game-specific logic rules. The QA corpus is partially composed of online quiz questions and partially composed of manually written variations of the most relevant ones. The knowledge database is extracted from several wiki-like websites about Minecraft. It is composed of unstructured data, such as text, that will be structured using the meaning representation we defined, and already structured data such as infoboxes. A preliminary examination of the data shows that players are asking creative questions about the game, and that the QA corpus can be used for clustering verbs and linking them to predefined actions in the game.
We present a corpus of time-aligned spoken data of Wikipedia articles as well as the pipeline that allows to generate such corpora for many languages. There are initiatives to create and sustain spoken Wikipedia versions in many languages and hence the data is freely available, grows over time, and can be used for automatic corpus creation. Our pipeline automatically downloads and aligns this data. The resulting German corpus currently totals 293h of audio, of which we align 71h in full sentences and another 86h of sentences with some missing words. The English corpus consists of 287h, for which we align 27h in full sentence and 157h with some missing words. Results are publically available.
In this Paper we present a corpus named SXUCorpus which contains read and spontaneous speech of the Upper Saxon German dialect. The data has been collected from eight archives of local television stations located in the Free State of Saxony. The recordings include broadcasted topics of news, economy, weather, sport, and documentation from the years 1992 to 1996 and have been manually transcribed and labeled. In the paper, we report the methodology of collecting and processing analog audiovisual material, constructing the corpus and describe the properties of the data. In its current version, the corpus is available to the scientific community and is designed for automatic speech recognition (ASR) evaluation with a development set and a test set. We performed ASR experiments with the open-source framework sphinx-4 including a configuration for Standard German on the dataset. Additionally, we show the influence of acoustic model and language model adaptation by the utilization of the development set.
This paper presents the TYPALOC corpus of French Dysarthric and Healthy speech and the rationale underlying its constitution. The objective is to compare phonetic variation in the speech of dysarthric vs. healthy speakers in different speech conditions (read and unprepared speech). More precisely, we aim to compare the extent, types and location of phonetic variation within these different populations and speech conditions. The TYPALOC corpus is constituted of a selection of 28 dysarthric patients (three different pathologies) and of 12 healthy control speakers recorded while reading the same text and in a more natural continuous speech condition. Each audio signal has been segmented into Inter-Pausal Units. Then, the corpus has been manually transcribed and automatically aligned. The alignment has been corrected by an expert phonetician. Moreover, the corpus benefits from an automatic syllabification and an Automatic Detection of Acoustic Phone-Based Anomalies. Finally, in order to interpret phonetic variations due to pathologies, a perceptual evaluation of each patient has been conducted. Quantitative data are provided at the end of the paper.
We present a new speech database containing 18.5 hours of annotated radio broadcasts in the Frisian language. Frisian is mostly spoken in the province Fryslan and it is the second official language of the Netherlands. The recordings are collected from the archives of Omrop Fryslan, the regional public broadcaster of the province Fryslan. The database covers almost a 50-year time span. The native speakers of Frisian are mostly bilingual and often code-switch in daily conversations due to the extensive influence of the Dutch language. Considering the longitudinal and code-switching nature of the data, an appropriate annotation protocol has been designed and the data is manually annotated with the orthographic transcription, speaker identities, dialect information, code-switching details and background noise/music information.
This paper presents a new Slovenian spoken language resource built from TEDx Talks. The speech database contains 242 talks in total duration of 54 hours. The annotation and transcription of acquired spoken material was generated automatically, applying acoustic segmentation and automatic speech recognition. The development and evaluation subset was also manually transcribed using the guidelines specified for the Slovenian GOS corpus. The manual transcriptions were used to evaluate the quality of unsupervised transcriptions. The average word error rate for the SI TEDx-UM evaluation subset was 50.7{\%}, with out of vocabulary rate of 24{\%} and language model perplexity of 390. The unsupervised transcriptions contain 372k tokens, where 32k of them were different.
We have constructed a new speech data corpus, using the utterances of 100 elderly Japanese people, to improve speech recognition accuracy of the speech of older people. Humanoid robots are being developed for use in elder care nursing homes. Interaction with such robots is expected to help maintain the cognitive abilities of nursing home residents, as well as providing them with companionship. In order for these robots to interact with elderly people through spoken dialogue, a high performance speech recognition system for speech of elderly people is needed. To develop such a system, we recorded speech uttered by 100 elderly Japanese, most of them are living in nursing homes, with an average age of 77.2. Previously, a seniors{'} speech corpus named S-JNAS was developed, but the average age of the participants was 67.6 years, but the target age for nursing home care is around 75 years old, much higher than that of the S-JNAS samples. In this paper we compare our new corpus with an existing Japanese read speech corpus, JNAS, which consists of adult speech, and with the above mentioned S-JNAS, the senior version of JNAS.
This paper reports on a new database ‚Äï Polish rhythmic database and tools developed with the aim of investigating timing phenomena and rhythmic structure in Polish including topics such as, inter alia, the effect of speaking style and tempo on timing patterns, phonotactic and phrasal properties of speech rhythm and stability of rhythm metrics. So far, 19 native and 12 non-native speakers with different first languages have been recorded. The collected speech data (5 h 14 min.) represents five different speaking styles and five different tempi. For the needs of speech corpus management, annotation and analysis, a database was developed and integrated with Annotation Pro (Klessa et al., 2013, Klessa, 2016). Currently, the database is the only resource for Polish which allows for a systematic study of a broad range of phenomena related to speech timing and rhythm. The paper also introduces new tools and methods developed to facilitate the database annotation and analysis with respect to various timing and rhythm measures. In the end, the results of an ongoing research and first experimental results using the new resources are reported and future work is sketched.
In this paper, we introduce an extension of our previously released TUKE-BNews-SK corpus based on a semi-automatic annotation scheme. It firstly relies on the automatic transcription of the BN data performed by our Slovak large vocabulary continuous speech recognition system. The generated hypotheses are then manually corrected and completed by trained human annotators. The corpus is composed of 25 hours of fully-annotated spontaneous and prepared speech. In addition, we have acquired 900 hours of another BN data, part of which we plan to annotate semi-automatically. We present a preliminary corpus evaluation that gives very promising results.
To create automatic transcription and annotation tools for the AHEYM corpus of recorded interviews with Yiddish speakers in Eastern Europe we develop initial Yiddish language resources that are used for adaptations of speech and language technologies. Our project aims at the development of resources and technologies that can make the entire AHEYM corpus and other Yiddish resources more accessible to not only the community of Yiddish speakers or linguists with language expertise, but also historians and experts from other disciplines or the general public. In this paper we describe the rationale behind our approach, the procedures and methods, and challenges that are not specific to the AHEYM corpus, but apply to all documentary language data that is collected in the field. To the best of our knowledge, this is the first attempt to create a speech corpus and speech technologies for Yiddish. This is also the first attempt to work out speech and language technologies to transcribe and translate a large collection of Yiddish spoken language resources.
Text Complexity Analysis is an useful task in Education. For example, it can help teachers select appropriate texts for their students according to their educational level. This task requires the analysis of several text features that people do mostly manually (e.g. syntactic complexity, words variety, etc.). In this paper, we present a tool useful for Complexity Analysis, called Coh-Metrix-Esp. This is the Spanish version of Coh-Metrix and is able to calculate 45 readability indices. We analyse how these indices behave in a corpus of {``}simple{''} and {``}complex{''} documents, and also use them as features in a complexity binary classifier for texts in Spanish. After some experiments with machine learning algorithms, we got 0.9 F-measure for a corpus that contains tales for kids and adults and 0.82 F-measure for a corpus with texts written for students of Spanish as a foreign language.
In this paper, we present an improvement of the last year architecture for identifying shallow discourse relations in texts. In the Ô¨Årst phase, the system will detect the connective words and both of arguments by performing the Conditional Random Fields (CRFs) learning algorithm with models that are trained based on a set of features such as words, part-of-speech (POS) and pattern based features extracted from parsing trees of sentences. The second phase will classify arguments and explicit connectives into one of thirteen types of senses by using the Sequential Minimal Optimization (SMO) and Random Forest classiÔ¨Åers with a set of features extracted from arguments and connective along with a set of given resources. The evaluation results of the whole system on the development, test and blind data set are 29.65%, 24.67% and 20.37% in terms of F1 scores. The results are competitive with other top baseline systems in recognition of explicit discourse relations. 
This speech celebrates the 20th anniversary of the CoNLL conference and looks back 20 years before CoNLL and 20 years into the future in an attempt to paint a longterm roadmap of Computational Natural Language Learning. The founders of CoNLL agonized hard and long over what to call our nascent Ô¨Åeld, and how to ensure that we kept all the interdisciplinary diversity that we had in those early days, including preserving the richness of views in a Ô¨Åeld that encompassed many controversies. We will explore this diversity with a focus on new directions that are developing; we will reÔ¨Çect on the changing nature of our technology including the deceleration of Moore‚Äôs Law and the emergence of Big Data; and we will consider the impact of and on ubiquitous technologies ranging from wearables to multimedia, from intelligent phones to driverless cars. 
The standard recurrent neural network language model (rnnlm) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the diÔ¨Écult learning problem presented by this model, demonstrate its eÔ¨Äectiveness in imputing missing words, explore many interesting properties of the model‚Äôs latent sentence space, and present negative results on the use of the model in language modeling. 
The ability to capture time information is essential to many natural language processing and information retrieval applications. Therefore, a lexical resource associating word senses to their temporal orientation might be crucial for the computational tasks aiming at the interpretation of language of time in texts. In this paper, we propose a semi-supervised minimum cuts strategy that makes use of WordNet glosses and semantic relations to supplement WordNet entries with temporal information. Intrinsic and extrinsic evaluations show that our approach outperforms prior semi-supervised non-graph classiÔ¨Åers. 
In this work, we propose a semisupervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the kmeans clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) re-estimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments Ô¨Åxed. Experimental results on four datasets show that our method works signiÔ¨Åcantly better than several other text clustering methods. 
Context representations are central to various NLP tasks, such as word sense disambiguation, named entity recognition, coreference resolution, and many more. In this work we present a neural model for efÔ¨Åciently learning a generic context embedding function from large corpora, using bidirectional LSTM. With a very simple application of our context representations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks. 
Semantic scripts is a conceptual representation which deÔ¨Ånes how events are organized into higher level activities. Practically all the previous approaches to inducing script knowledge from text relied on count-based techniques (e.g., generative models) and have not attempted to compositionally model events. In this work, we introduce a neural network model which relies on distributed compositional representations of events. The model captures statistical dependencies between events in a scenario, overcomes some of the shortcomings of previous approaches (e.g., by more effectively dealing with data sparsity) and outperforms count-based counterparts on the narrative cloze task. 
Most automatic text summarization systems proposed to date rely on centrality and structural features as indicators for information importance. In this paper, we argue that these features cannot reliably detect important information in heterogeneous document collections. Instead, we propose CPSum, a summarizer that learns the importance of information objects from a background source. Our hypothesis is tested on a multi-document corpus where we remove centrality and structural features. CPSum proves to be able to perform well in this challenging scenario whereas reference systems fail. 
Verb prediction is important in human sentence processing and, practically, in simultaneous machine translation. In verb-Ô¨Ånal languages, speakers select the Ô¨Ånal verb before it is uttered, and listeners predict it before it is uttered. Simultaneous interpreters must do the same to translate in real-time. Motivated by the problem of SOV-SVO simultaneous machine translation, we provide a study of incremental verb prediction in verb-Ô¨Ånal languages. As a basis of comparison, we examine incremental verb prediction with human participants in a multiple choice setting using crowdsourcing to gain insight into incremental human performance in a constrained setting. We then examine a computational approach to incremental verb prediction using discriminative classiÔ¨Åcation with shallow features. Both humans and machines predict verbs more accurately as more of a sentence becomes available, and case markers‚Äîwhen available‚Äîhelp humans and sometimes machines predict Ô¨Ånal verbs. 
We investigate implicit corrections in the form of contrastive discourse in childadult interaction, which have been argued to contribute to language learning. In contrast to previous work in psycholinguistics, we adopt a data-driven methodology, using comparably large amounts of data and leveraging computational methods. We conduct a corpus study on the use of parental corrective feedback and show that its presence in child directed speech is associated with a reduction of child subject omission errors in English. 
This paper studies how word embeddings trained on the British National Corpus interact with part of speech boundaries. Our work targets the Universal PoS tag set, which is currently actively being used for annotation of a range of languages. We experiment with training classiÔ¨Åers for predicting PoS tags for words based on their embeddings. The results show that the information about PoS afÔ¨Åliation contained in the distributional vectors allows us to discover groups of words with distributional patterns that differ from other words of the same part of speech. This data often reveals hidden inconsistencies of the annotation process or guidelines. At the same time, it supports the notion of ‚Äòsoft‚Äô or ‚Äògraded‚Äô part of speech afÔ¨Åliations. Finally, we show that information about PoS is distributed among dozens of vector components, not limited to only one or two features. 
In this work, we explore how learners can infer second-language noun meanings in the context of their native language. Motivated by an interest in building interactive tools for language learning, we collect data on three word-guessing tasks, analyze their difÔ¨Åculty, and explore the types of errors that novice learners make. We train a log-linear model for predicting our subjects‚Äô guesses of word meanings in varying kinds of contexts. The model‚Äôs predictions correlate well with subject performance, and we provide quantitative and qualitative analyses of both human and model performance. 
We present methods for investigating processes of evolution in a language family by modeling relationships among the observed languages. The models aim to Ô¨Ånd regularities‚Äîregular correspondences in lexical data. We present an algorithm which codes the data using phonetic features of sounds, and learns longrange contextual rules that condition recurrent sound correspondences between languages. This gives us a measure of model quality: better models Ô¨Ånd more regularity in the data. We also present a procedure for imputing unseen data, which provides another method of model comparison. Our experiments demonstrate improvements in performance compared to prior work. 
This paper is a novel study that views sarcasm detection in dialogue as a sequence labeling task, where a dialogue is made up of a sequence of utterances. We create a manuallylabeled dataset of dialogue from TV series ‚ÄòFriends‚Äô annotated with sarcasm. Our goal is to predict sarcasm in each utterance, using sequential nature of a scene. We show performance gain using sequence labeling as compared to classiÔ¨Åcation-based approaches. Our experiments are based on three sets of features, one is derived from information in our dataset, the other two are from past works. Two sequence labeling algorithms (SVM-HMM and SEARN) outperform three classiÔ¨Åcation algorithms (SVM, Naive Bayes) for all these feature sets, with an increase in F-score of around 4%. Our observations highlight the viability of sequence labeling techniques for sarcasm detection of dialogue. 
Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers. Statistical classiÔ¨Åcation using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features. We perform feature signiÔ¨Åcance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment analyzers to handle complex constructs. 
 We introduce a deep neural network for automated sarcasm detection. Recent work has emphasized the need for models to capitalize on contextual features, beyond lexical and syntactic cues present in utterances. For example, different speakers will tend to employ sarcasm regarding different subjects and, thus, sarcasm detection models ought to encode such speaker information. Current methods have achieved this by way of laborious feature engineering. By contrast, we propose to automatically learn and then exploit user embeddings, to be used in concert with lexical signals to recognize sarcasm. Our approach does not require elaborate feature engineering (and concomitant data scraping); Ô¨Åtting user embeddings requires only the text from their previous posts. The experimental results show that the our model outperforms a state-of-the-art approach leveraging an extensive set of carefully crafted features. 
Cross lingual projection of linguistic annotation suffers from many sources of bias and noise, leading to unreliable annotations that cannot be used directly. In this paper, we introduce a novel approach to sequence tagging that learns to correct the errors from cross-lingual projection using an explicit debiasing layer. This is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags. We evaluated with only 1,000 tokens tagged with gold standard tags, along with more plentiful parallel data. Our system equals or exceeds the state-of-the-art on eight simulated lowresource settings, as well as two real lowresource languages, Malagasy and Kinyarwanda. 
Current automatic machine translation systems require heavy human proofreading to produce high-quality translations. We present a new interactive machine translation approach aimed at providing a natural collaboration between humans and translation systems. As such, we grant the user complete freedom to validate and correct any part of the translations suggested by the system. Our approach is then designed according to the requirements placed by this unrestricted proofreading protocol. In particular, the ability of the system to suggest new translations coherent with the set of potentially disjoint translation segments validated by the user. We evaluate our approach in a usersimulated setting where reference translations are considered the output desired by a human expert. Results show important reductions in the number of edits in comparison to decoupled post-editing and conventional preÔ¨Åx-based interactive translation prediction. Additionally, we provide evidence that it can also reduce the cognitive overload reported for interactive translation systems in previous user studies. 
Machine Translation Quality Estimation is a notoriously difÔ¨Åcult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workÔ¨Çows. 
Named Entity Recognition (NER) models for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on another, source, language (or multiple source languages). We introduce a language independent method for NER, building on cross-lingual wikiÔ¨Åcation, a technique that grounds words and phrases in nonEnglish text into English Wikipedia entries. Thus, mentions in any language can be described using a set of categories and FreeBase types, yielding, as we show, strong language-independent features. With this insight, we propose an NER model that can be applied to all languages in Wikipedia. When trained on English, our model outperforms comparable approaches on the standard CoNLL datasets (Spanish, German, and Dutch) and also performs very well on lowresource languages (e.g., Turkish, Tagalog, Yoruba, Bengali, and Tamil) that have signiÔ¨Åcantly smaller Wikipedia. Moreover, our method allows us to train on multiple source languages, typically improving NER results on the target languages. Finally, we show that our languageindependent features can be used also to enhance monolingual NER systems, yielding improved results for all 9 languages. 
Wikipedia is a resource of choice exploited in many NLP applications, yet we are not aware of recent attempts to adapt coreference resolution to this resource. In this work, we revisit a seldom studied task which consists in identifying in a Wikipedia article all the mentions of the main concept being described. We show that by exploiting the Wikipedia markup of a document, as well as links to external knowledge bases such as Freebase, we can acquire useful information on entities that helps to classify mentions as coreferent or not. We designed a classiÔ¨Åer which drastically outperforms fair baselines built on top of state-of-the-art coreference resolution systems. We also measure the beneÔ¨Åts of this classiÔ¨Åer in a full coreference resolution pipeline applied to Wikipedia texts. 
Coreference resolution for event mentions enables extraction systems to process document-level information. Current systems in this area base their decisions on rich semantic features from various knowledge bases, thus restricting them to domains where such external sources are available. We propose a model for this task which does not rely on such features but instead utilizes sentential features coming from convolutional neural networks. Two such networks Ô¨Årst process coreference candidates and their respective context, thereby generating latent-feature representations which are tuned towards event aspects relevant for a linking decision. These representations are augmented with lexicallevel and pairwise features, and serve as input to a trainable similarity function producing a coreference score. Our model achieves state-of-the-art performance on two datasets, one of which is publicly available. An error analysis points out directions for further research. 
Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method speciÔ¨Åcally designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-theart accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset. 
For most entity disambiguation systems, the secret recipes are feature representations for mentions and entities, most of which are based on Bag-of-Words (BoW) representations. Commonly, BoW has several drawbacks: (1) It ignores the intrinsic meaning of words/entities; (2) It often results in high-dimension vector spaces and expensive computation; (3) For different applications, methods of designing handcrafted representations may be quite different, lacking of a general guideline. In this paper, we propose a different approach named EDKate. We Ô¨Årst learn low-dimensional continuous vector representations for entities and words by jointly embedding knowledge base and text in the same vector space. Then we utilize these embeddings to design simple but effective features and build a two-layer disambiguation model. Extensive experiments on real-world data sets show that (1) The embedding-based features are very effective. Even a single one embedding-based feature can beat the combination of several BoW-based features. (2) The superiority is even more promising in a difÔ¨Åcult set where the mention-entity prior cannot work well. (3) The proposed embedding method is much better than trivial implementations of some off-the-shelf embedding algorithms. (4) We compared our EDKate with existing methods/systems and the results are also positive.  
We propose an unsupervised approach for substring-based transliteration which incorporates two new sources of knowledge in the learning process: (i) context by learning substring mappings, as opposed to single character mappings, and (ii) phonetic features which capture cross-lingual character similarity via prior distributions. Our approach is a two-stage iterative, boot-strapping solution, which vastly outperforms Ravi and Knight (2009)‚Äôs state-of-the-art unsupervised transliteration method and outperforms a rule-based baseline by up to 50% for top-1 accuracy on multiple language pairs. We show that substring-based models are superior to character-based models, and observe that their top-10 accuracy is comparable to the top-1 accuracy of supervised systems. Our method only requires a phonemic representation of the words. This is possible for many language-script combinations which have a high grapheme-to-phoneme correspondence e.g. scripts of Indian languages derived from the Brahmi script. Hence, Indian languages were the focus of our experiments. For other languages, a grapheme-to-phoneme converter would be required. 
In this work, we model abstractive text summarization using Attentional EncoderDecoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-toword structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research. 
Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efÔ¨Åcacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT‚Äô14 EnglishGerman translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model. 
Discourse relations can either be implicit or explicitly expressed by markers, such as ‚Äôtherefore‚Äô and ‚Äôbut‚Äô. How a speaker makes this choice is a question that is not well understood. We propose a psycholinguistic model that predicts whether a speaker will produce an explicit marker given the discourse relation s/he wishes to express. Based on the framework of the Rational Speech Acts model, we quantify the utility of producing a marker based on the information-theoretic measure of surprisal, the cost of production, and a bias to maintain uniform information density throughout the utterance. Experiments based on the Penn Discourse Treebank show that our approach outperforms stateof-the-art approaches, while giving an explanatory account of the speaker‚Äôs choice. 
In this paper, we propose a method which uses semi-supervised convolutional neural networks (CNNs) to select in-domain training data for statistical machine translation. This approach is particularly effective when only tiny amounts of in-domain data are available. The in-domain data and randomly sampled general-domain data are used to train a data selection model with semi-supervised CNN, then this model computes domain relevance scores for all the sentences in the generaldomain data set. The sentence pairs with top scores are selected to train the system. We carry out experiments on 4 language directions with three test domains. Compared with strong baseline systems trained with large amount of data, this method can improve the performance up to 3.1 BLEU. Its performances are signiÔ¨Åcant better than three state-of-the-art language model based data selection methods. We also show that the in-domain data used to train the selection model could be as few as 100 sentences, which makes Ô¨Ånegrained topic-dependent translation adaptation possible. 
My talk will recount the path I have taken from the linguistic garden into the bush. 1. MIT Years: Into the Garden I came into linguistics with a bachelor‚Äôs degree in philosophy from Reed College, and after a couple of false starts I ended up in grad school at MIT studying formal grammar in the Department of Linguistics and Philosophy with Chomsky and Halle in the heyday of generative grammar. At MIT, Chomsky was my doctoral advisor, and my mentor was Morris Halle, who ran the Department at that time. ‚àó This article contains the text of my acceptance speech for the ACL Lifetime Achievement Award in 2016. It uses with permission some material from Bresnan (2011). I prepared the plots of wild data with R (Sarkar 2008; R Core Team 2015). E-mail: Bresnan@stanford.edu. doi:10.1162/COLI a 00260 ¬© 2017 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 4  The exciting goal was to infer the nature of the mind‚Äôs capacity for language from the structure of human language, viewed as a purely combinatorial set of formal patterns, like the formulas of symbolic logic. It was apparently exciting even to Fred Jelinek as an MIT doctoral student in information theory ten years before me (Jelinek 2009). In his Lifetime Award speech he recounts how as a grad student he attended some of Chomsky‚Äôs lectures with his wife, got the ‚Äúcrazy notion‚Äù that he should switch from information theory to linguistics, and went as far as discussing it with Chomsky when his advisor Fano got wind of it and said he had to complete his Ph.D. in Information Theory. He had no choice. The rest is history.1 The MIT epistemology held that the structure of language could not be learned inductively from what we hear; it had to be deduced from innate, universal cognitive structures speciÔ¨Åc to human language. This approach had methodological advantages for a philosophical linguist: First, a limitless profusion of data in our own minds came from our intuitions about sentences that we had never heard before; second, a sustained and messy relationship to the world of facts and data was not required; and third, (with the proper training) scientiÔ¨Åc research could conveniently be done from an armchair using introspection. 2. Psychological (Un)reality I got my Ph.D. from MIT in 1972 and taught brieÔ¨Çy at Stanford and at UMass, Amherst, before joining the MIT faculty in 1975 as an Associate Professor of Linguistics. Very early on in my career as a linguist I had become aware of discrepancies between the MIT transformational grammar models and the Ô¨Åndings of psycholinguists. For example, the theory that more highly transformed syntactic structures would require more complex processing during language comprehension and development did not work. With a year off on a Guggenheim fellowship (1975‚Äì1976), I began to think about designing a more psychologically realistic system of transformational grammar that made much less use of syntactic transformations in favor of an enriched lexicon and pragmatics. The occasion was a 1975 symposium jointly sponsored by MIT and AT&T to assess the past and future impact of telecommunications technology on society, in celebration of the centennial of the invention of the telephone. What did I know about any of this? Absolutely nothing. I was invited to participate by Morris Halle. From Harvard Psychology, George Miller invited Eric Wanner, Mike Maratsos, and Ron Kaplan. Ron Kaplan and I developed our common interests in relating formal grammar to computational psycholinguistics, and we began to collaborate. In 1977 we each taught courses at the IV International Summer School in Computational and Mathematical Linguistics, organized by Antonio Zampoli at the Scuola Normale Superiore, Pisa. In 1978 Kaplan visited MIT and we taught a joint graduate course in computational psycholinguistics. From 1978 to 1983, I consulted at the Computer Science Laboratory, Xerox Corporation Palo Alto Research Center (1978‚Äì1980) and the Cognitive and Instructional Sciences Group, Xerox PARC (1981‚Äì1983).  
Fifty years later, Formal Semantics (FS) and vectorial models of meaning‚Äî commonly referred to as ‚ÄúDistributional Semantics‚Äù (DS)‚Äîhave made substantial progress. Large machine-readable corpora are available and computing power has grown exponentially. These developments, together with the advent of improved ‚àó CIMeC (Universita` di Trento), Palazzo Fedrigotti, C.so Bettini 31, 38068 Rovereto, Italy. E-mail: gemma.boleda@unitn.it. ‚àó‚àó CIMeC (Universita` di Trento), Palazzo Fedrigotti, C.so Bettini 31, 38068 Rovereto, Italy. E-mail: aurelie.herbelot@unitn.it. Submission received: 20 June 2016; revised version received: 10 September 2016; accepted for publication: 15 September 2016. doi:10.1162/COLI a 00261 ¬© 2017 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 4  machine learning techniques (LeCun, Bengio, and Hinton 2015), have brought back the idea that Computational Linguistics should work on general language understanding, that is, on theories and models that account both for language use as a whole, and for the associated conceptual apparatus (Collobert and Weston 2008; Mikolov, Joulin, and Baroni 2015; Goodman, Tenenbaum, and Gerstenberg 2015; Erk 2016). This special issue looks at this goal from the point of view of developing a semantic framework that, ideally, would encompass a wide range of the phenomena we might subsume under the term ‚Äúunderstanding.‚Äù This framework, Formal Distributional Semantics (FDS), takes up the challenge from a particular angle, which involves integrating Formal Semantics and Distributional Semantics in a theoretically and computationally sound fashion. To show why the integration is desirable, and, more generally speaking, what we mean by general understanding, let us consider the following discourse: (1) The new postdoc doesn‚Äôt work for Kim: she writes papers on semantics. And... uh... on those neural nets that have loops. Processing the meaning of those sentences requires a number of complex linguistic abilities. Hearers must be able to retrieve the descriptive content of words such as postdoc or write (i.e., have knowledge of the properties of the involved individuals and events). They must also correctly process function words and structural markers, such as the, and, and bare singular/plural constructions to understand which entities are involved in the discourse: (a) a unique postdoc, (b) a person named Kim, (c) a (probably relatively small) number of entities that are papers, and (d) abstract concepts referred to generically (semantics, neural nets). They must carry out some form of composition over the constituents of each sentence, for example, shifting the meanings of write and paper towards ‚Äòauthoring‚Äô and ‚ÄòscientiÔ¨Åc article.‚Äô Finally, they must be prepared to perform a range of inferences, for instance: the new postdoc is knowledgeable about semantics and what is probably Recurrent Neural Networks; she cannot be working for Kim, who is a syntactician, but she perhaps works for Sandy, who does Machine Learning. To this day, no single semantic framework has been proposed that would naturally cater to all of these phenomena. Instead, Formal and Distributional Semantics have focused on‚Äîand been extremely successful in‚Äîmodeling particular aspects of meaning. Formal Semantics provides an account of the inferential properties of language and of compositionality based on the formalization of the relations between the distinct entities and events referred to in a linguistic expression (Montague 1974; Partee 2008). The various strands of FS also offer philosophically grounded theories of meaning. However, the framework struggles with descriptive content, despite the large amount of work done on lexical semantics and formal ontology (Dowty 1991; Pustejovsky 1995; Pinkal 1995; Guarino, Pribbenow, and Vieu 1996; Kennedy and McNally 2005; Ro√üdeutscher and Kamp 2010; Asher 2011, among others). This comes from the fact that, being focused on a particular type of logical entailment, FS naturally limits the type of phenomena that it covers, especially at the lexical level. This, in turn, affects its psychological plausibility. Distributional Semantics, on the other hand, has made good progress in modeling the descriptive content of linguistic expressions in a cognitively plausible way (Lund, Burgess, and Atchley 1995; Landauer and Dumais 1997), but faces serious difÔ¨Åculties with many of the phenomena that Formal Semantics excels at, such as quantiÔ¨Åcation and logical inference.  620  Boleda and Herbelot  Formal Distributional Semantics  Because of the complementary strengths of the two approaches, it has been suggested that much could be gained by developing an overarching framework (Coecke, Sadrzadeh, and Clark 2011; Beltagy et al. 2013; Erk 2013; Garrette, Erk, and Mooney 2014; Grefenstette 2013; Lewis and Steedman 2013; Baroni, Bernardi, and Zamparelli 2015). A Formal Distributional Semantics thus holds the promise of developing a more comprehensive model of meaning. However, given the fundamentally different natures of FS and DS, building an integrative framework poses theoretical and engineering challenges. This introductory article provides the necessary background to understand those challenges and to situate the articles that follow in the broader research context. 2. Formal Semantics Formal Semantics is a broad term that covers a range of approaches to the study of meaning, from model-theoretic (Montague 1974; Partee 2008) to proof-theoretic semantics (Gentzen 1935). Although a comprehensive overview of those different strands is beyond the scope of this introduction, we will present here the various formal concepts that have been discussed as being desirable in the FDS context. Formal Semantics is so-called because it has adopted some of the tools standardly used to describe formal languages: Notably, Montague proposed a way to express semantic composition with respect to a model using intensional logic (Montague 1974). Relying on a well-honed logical apparatus, FS has developed prominent models of many linguistic phenomena, from quantiÔ¨Åcation to modality (see Dowty, Wall, and Peters 1981 and Cann 1993 for overviews). It caters to ontological matters (what there is in the world), reference to entities (how we talk about things), meaning at the higher constituent level (composition), interpretation at the sentential level (e.g., by giving propositions a truth value), and‚Äîcrucially‚Äîsophisticated accounts of the logical inferences that can be drawn from a particular sentence. One intuitive way to describe the world in a logic is via a model, usually provided in terms of sets (Dowty, Wall, and Peters 1981). For instance, a world with three postdocs will contain a set of three entities sharing the property of being a postdoc. This set may be a subset of the larger set of humans. Generally, models are provided for miniworlds corresponding to a particular situation or ‚Äústate-of-affairs,‚Äù as it would clearly be impractical to describe the world in its entirety, even with a limited vocabulary. With a model at our disposal, we must explain how words come to relate to its elements. The two notions of extension and intension are key to this explanation. The extension, or denotation, of a linguistic expression are the entities it refers to in the model. For instance, the extension of postdoc is the set of all postdocs in the model under consideration. This simple correspondence is complicated by the fact that there is not a straightforward one-to-one relation between words of a language and sets of a model. Some expressions may refer to the same entity but have different semantic content. For instance, although the new postdoc and the author of the neural net paper may refer to the same entity in a particular universe of discourse, they encapsulate different properties (Frege 1892). Further, knowledge of the properties of a named entity does not imply knowledge of its extension, and vice versa (Jones 1911): It is possible to know that someone wrote a particular paper about neural networks and not be able to recognize them at a conference. The logical notion of intension contributes to solving this issue by providing a function mapping possible worlds to extensions. The availability of such a function allows us to posit worlds where two linguistic expressions have the same extension, and others where they do not, clearly separating extensions from linguistic expressions. 621  Computational Linguistics  Volume 42, Number 4  Beyond providing reference, natural languages are compositional. The meaning of a complex expression is derivable from the meaning of its parts in a systematic and productive way. A compositional formal semantics framework provides semantic representations of linguistic expressions in a logic, and rules for combining them. So for any expression, it is possible to identify a number of potentially distinct individuals (or sets thereof), and the relationships between them. For instance, the sentence The new postdoc has written several articles can be transformed (simplifying somewhat) into the following logical form: (2) ‚àÉx, y[new(postdoc(x)) ‚àß article‚àó(y) ‚àß write(x, y)] showing that for this state of affairs, the extension of the relevant subsets of postdocs and articles (x and y; the ‚àó sign is used to indicate a plurality) are linked through a writing relation. A complete account of compositionality relies heavily on being able to interpret function words in the sentence. FS gives a sophisticated formalization of quantiÔ¨Åers (‚àÉ in our example) that lets us select subsets of entities and assign them properties: For example, some entities in the set of articles, denoted by the variable y, are written by x. FS, in virtue of having access to a detailed model of a world, has generally been very successful in formalizing the meaning of logical operators, particularly quantiÔ¨Åers (including matters of plurality and genericity), negation, modals, and their application to entities and events. Beyond its descriptive power, FS also has tools to interpret the meaning of words and sentences with respect to a particular world. In truth-theoretic semantics, the meaning of a sentence is a function from possible worlds to truth values. Obtaining the truth value of a proposition with respect to a given world relies on the notion of satisfaction (a predicate can be truthfully applied to a term if the corresponding property in a world applies to the referent of the term): In Tarski‚Äôs words, the proposition snow is white is true if snow is white (Tarski 1944). In contrast to truth-theoretic approaches, probabilistic logic approaches assume that a speaker assigns a probability distribution to a set of possible worlds (Nilsson 1994; Pinkal 1995; van Benthem, Gerbrandy, and Kooi 2009; van Eijck and Lappin 2012). Being able to give an interpretation to a sentence with respect to either truth or speaker belief is an essential part of explaining the implicit aspects of meaning, in particular inference. Consider the following example. If the set of postdocs is in the set of human beings, then the sentence a postdoc is writing, if true, will entail the truth of the sentence a human is writing. Speakers of a language routinely infer many facts from the explicit information that is given to them: This ability is in fact crucial to achieving communication efÔ¨Åciency. By logically relating parts of language to parts of a model, model theory ensures that inference is properly subsumed by the theory: If the set of postdocs is fully included in the set of humans, it follows from the deÔ¨Ånition of extension that we can refer to a set of postdocs as ‚Äúhumans.‚Äù In practice, though, inference in model-theoretic semantics is intractable, as it often relies on a full search over the model. This is where proof theory helps, by providing classes of inference that are algorithmically decidable. For instance, whereas it is necessary to search through an entire model to Ô¨Ånd out what it knows about the concept postdoc, proof theory has this information readily stored in, for instance, a type (e.g., a postdoc may be cast as an individual entity which is a human, holds a doctorate, etc.). This decomposition of content words into formal 622  Boleda and Herbelot  Formal Distributional Semantics  type representations allows for a range of functions to be directly applied to those types. However, formal approaches fail to represent content words in all their richness (and by extension, the kind of inferences that can be made over lexical information). A formal ontology may tell us that the set of postdocs is fully included in the set of humans (model theory), or that the type postdoc encapsulates a logical relation to the type human in its deÔ¨Ånition (proof theory), but this falls short of giving us a full representation of the concept. For instance, imagine formalizing, in such a system, the distinction between near-synonyms such as man/gentleman/chap/lad/guy/dude/bloke (Edmonds and Hirst 2002; Boleda and Erk 2015). Although all these words refer to male humans, they are clearly not equivalent: For instance, man is a general, ‚Äúneutral‚Äù word whereas chap, lad, and others have an informal connotation as well as links to particular varieties of English (e.g., British vs. American). This is not the type of information that can naturally be represented in either a model- or proof-theoretic structure. But a more fundamental problem is that, by being a logical system geared towards explaining a particular type of inference, FS naturally limits what it encodes of human linguistic experience. For instance, analogical reasoning is outside of its remit, despite it being a core feature of the human predictive apparatus. Knowing a complex attribute of postdoc (e.g., that a postdoc is more likely to answer an e-mail at midnight on a Sunday than 8 am on a Monday) can warrant the belief that third-year Ph.D. students work on Sunday evenings. This belief does not derive from a strict operation over truth values, but is still a reasonable abductive inference to make. In relation to this issue, it should also be clear that, at least in its simplest incarnation, model theory lacks cognitive plausibility: It is doubtful that speakers hold a detailed, permanent model of the world ‚Äúin their head.‚Äù It is similarly doubtful that they all hold the same model of the world (see Labov [1978] on how two individuals might disagree on the extension of cup vs. mug; or Herbelot and Vecchi [2016] on quantiÔ¨Åcational disagreements). Still, people are able to talk to each other about a wide variety of topics, including some which they are not fully familiar with. In order to explain this, we must account for the way humans deal with partial or vague knowledge, inconsistencies, and uncertainties, and for the way that, in the Ô¨Årst place, they acquire their semantic knowledge. Although some progress has been done on the formalization side‚Äîfor instance, with tools such as supervaluation (Fine 1975), update semantics (Veltman 1996), and probabilistic models (Nilsson 1994)‚Äîmuch work remains to be done. 3. Distributional Semantics Distributional Semantics (Turney and Pantel 2010; Clark 2012; Erk 2012) has a radically different view of language, based on the hypothesis that the meaning of a linguistic expression can be induced from the contexts in which it is used (Harris 1954; Firth 1957), because related expressions, such as postdoc and student, are used in similar contexts (a poor , the struggled through the deadline). In contrast with Formal Semantics, this provides an operational learning procedure for semantic representations that has been proÔ¨Åtably used in computational semantics, and more broadly in ArtiÔ¨Åcial Intelligence (Mikolov, Yih, and Zweig 2013, for instance) and Cognitive Science (Lund, Burgess, and Atchley 1995; Landauer and Dumais 1997, and subsequent work). The two key points that underlie the success of Distributional Semantics are (1) the fact that DS is able to acquire semantic representations directly from natural language data, and (2) the fact that those representations suit the properties of lexical or conceptual aspects of meaning, 623  Computational Linguistics  Volume 42, Number 4  thus accounting well for descriptive content both at the word level and in composition (as we will see next). Both aspects strengthen its cognitive plausibility. In Distributional Semantics, the meaning representation for a given linguistic expression is a function of the contexts in which it occurs. Context can be deÔ¨Åned in various ways; the most usual one is the linguistic environment in which a word appears (typically, simply the words surrounding the target word, but some approaches use more sophisticated linguistic representations encoding, e.g., syntactic relations; Pado¬¥ and Lapata [2007]). Figure 1(a) shows an example. Recently, researchers have started exploring other modalities, using, for instance, visual and auditory information extracted from images and sound Ô¨Åles (Feng and Lapata 2010; Bruni et al. 2012; Roller and Schulte Im Walde 2013; Kiela and Clark 2015; Lopopolo and van Miltenburg 2015). Distributional representations are vectors (Figure 1(c)) or more complex algebraic objects such as matrices and tensors, where numerical values are abstractions on the contexts of use obtained from large amounts of natural language data (large corpora, image data sets, and so forth). The Ô¨Ågure only shows values for two dimensions, but standard distributional representations range from a few dozen to hundreds of thousands of dimensions (cf. the dots in the Ô¨Ågure). The semantic information is distributed across all the dimensions of the vector, and it is encoded in the form of continuous values, which allows for very rich and nuanced information to be expressed (Landauer and Dumais 1997; Baroni and Lenci 2010). One of the key strengths of Distributional Semantics is its use of well-deÔ¨Åned algebraic techniques to manipulate semantic representations, which yield useful information about the semantics of the involved expressions. For instance, the collection of words in a lexicon forms a vector space or semantic space, in which semantic relations can be modeled as geometric relations: In a typical semantic space, postdoc is near student, and far from less related words such as wealth, as visualized in Figure 2. The words (represented with two dimensions dim1 and dim2; see Figure 2, left) can be plotted as vectors from the origin to their coordinates in the dimensions (Figure 2, right). The visually clear vector relationships can be quantiÔ¨Åed with standard measures such as cosine similarity, which ranges (for positive-valued vectors) between 0 and 1: The cosine similarity between postdoc and student in our example is 0.99, and that of postdoc and wealth is 0.37. The same techniques used for two dimensions work for any number of dimensions, and thus we can interpret a cosine of 0.99 for two 300-dimensional vectors as signalling that the vectors have very similar values along almost all the dimensions. Also note that distributional representations are naturally graded: Two vectors can be more or less similar, or similar in certain dimensions but not others. This  The postdoc shook her head.  Any grad student or postdoc he‚Äôd have would be a  =‚áí clonal copy of himself. That tattoo is actually a custom-made cartoon gen-  0.43679 1.93841 . . .  erated by the postdoc who led this work.  During that postdoc, I didn‚Äôt publish anything.  (a)  (b)  (c)  Figure 1 Distributional Semantics: The linguistic contexts in which an expression appears, for example, the words in the postdoc sentences in (a), are mapped to an algebraic representation (see the vector in (c)) through a function, represented by the arrow in (b). The resulting representations (in the example, the vector for postdoc) are thus abstractions over contexts of use. Examples adapted from the COCA corpus.  624  Boleda and Herbelot  Formal Distributional Semantics  2.5  postdoc student wealth  dim1 0.43679 0.71038 1.77337  dim2 1.93841 1.76058 0.00012  dim2  1.0  1.5  2.0  postdoc (0.43679,1.93841) student (0.71038,1.76058)  0.5  wealth (1.77337,0.00012)  0.0  0.0  0.5  1.0  1.5  2.0  2.5  dim1  Figure 2 Semantic distance as geometric distance: A toy lexicon (left) and its representation in semantic space (right). The distance (see arcs) between the vectors for postdoc and student is smaller than the distance between the vectors for postdoc and wealth. The converse is true for similarity as opposed to distance.  is in accordance with what is known about conceptual knowledge and its interaction with language (Murphy 2004). How are distributional representations obtained? There are in fact many different versions of the function that maps contexts into distributional representations (represented as an arrow in Figure 1(b)). Traditional distributional models are count-based (Baroni, Dinu, and Kruszewski 2014): They are statistics over the observed contexts of use, corresponding for instance to how many times words occur with other words (like head, student, researcher, etc.) in a given sentence.1 More recent neural network‚Äìbased models involve predicting the contexts instead (Collobert and Weston 2008; Socher et al. 2012; Mikolov, Yih, and Zweig 2013). In this type of approach, semantic representations are a by-product of solving a linguistic prediction task. Tasks that are general enough lead to general-purpose semantic representations; for instance, Mikolov, Yih, and Zweig (2013) used language modeling, the task of predicting words in a sentence (e.g., After graduating, Barbara kept working in the same institute, this time as a ). In a predictive setup, word vectors (called embeddings in the neural network literature) are typically initialized randomly, and iteratively reÔ¨Åned as the model goes through the data and improves its predictions. Because similar words appear in similar contexts, they end up with similar embeddings; those are essentially part of the internal representation of the model for the prediction of a particular word. Although predictive models can outperform count models by a large margin (Baroni, Dinu, and Kruszewski 2014), those improvements can be replicated with Ô¨Ånely tuned count models (Levy, Goldberg, and Dagan 2015). Distributional models have been shown to reliably correlate with human judgments on semantic similarity, as well as a broad range of other linguistic and psycholinguistic phenomena (Lund, Burgess, and Atchley 1995; Landauer and Dumais 1997; Baroni and Lenci 2010; Erk, Pado¬¥ , and Pado¬¥ 2010). For instance, Baroni and Lenci (2010) explore synonymy (puma‚Äìcougar), noun categorization (car IS-A vehicle; banana IS-A fruit),  
Center for Mind/Brain Sciences, University of Trento Logical negation is a challenge for distributional semantics, because predicates and their negations tend to occur in very similar contexts, and consequently their distributional vectors are very similar. Indeed, it is not even clear what properties a ‚Äúnegated‚Äù distributional vector should possess. However, when linguistic negation is considered in its actual discourse usage, it often performs a role that is quite different from straightforward logical negation. If someone states, in the middle of a conversation, that ‚ÄúThis is not a dog,‚Äù the negation strongly suggests a restricted set of alternative predicates that might hold true of the object being talked about. In particular, other canids and middle-sized mammals are plausible alternatives, birds are less likely, skyscrapers and other large buildings virtually impossible. Conversational negation acts like a graded similarity function, of the sort that distributional semantics might be good at capturing. In this article, we introduce a large data set of alternative plausibility ratings for conversationally negated nominal predicates, and we show that simple similarity in distributional semantic space provides an excellent Ô¨Åt to subject data. On the one hand, this Ô¨Ålls a gap in the literature on conversational negation, proposing distributional semantics as the right tool to make explicit predictions about potential alternatives of negated predicates. On ‚àó Center for Mind/Brain Sciences, University of Trento, C.so Bettini 31, 38068 Rovereto (TN), Italy. E-mail: german.kruszewski@unitn.it. Submission received: 10 April, 2015; revised version received: 25 November, 2015; accepted for publication: 5 April, 2016. doi:10.1162/COLI a 00262 ¬© 2017 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 4  the other hand, the results suggest that negation, when addressed from a broader pragmatic perspective, far from being a nuisance, is an ideal application domain for distributional semantic methods. 1. Introduction Distributional semantics (DS) derives vector-based representations of the meaning of words and other linguistic expressions by generalizing over the contexts in which such expressions occur in large text corpora (Turney and Pantel 2010; Erk 2012). By exploiting the rich commonsense knowledge encoded in corpora and a continuous notion of relatedness that is well-suited to capture the fuzzy nature of content-word semantics, DS representations can successfully model lexical aspects of meaning such as synonymy (Landauer and Dumais 1997), word analogy (Mikolov, Yih, and Zweig 2013), selectional preferences (Erk, Pado¬¥ , and Pado¬¥ 2010), and, to a certain extent, hypernymy (Roller, Erk, and Boleda 2014). There is, however, virtually no evidence that DS can capture the semantic properties of grammatical terms such as conjunctions, determiners, or adverbial particles. The very notion of continuous similarity that is so powerful in modeling lexical phenomena is problematic when it comes to capturing the discrete logical operations that are typically associated with the meaning of grammatical terms. Adverbs and determiners expressing negation, such as English no and not, receive a very elegant treatment in logic-based approaches: if dog denotes the (appropriately indexed) set of all dogs, then no dog denotes the complement of the set. However, there is no straightforward ‚Äúnegation‚Äù operation that, when applied to the DS vector of dog, would derive a no dog vector capturing the same complement intuition in vector space. Moreover, negated elements tend to occur in the same contexts of their afÔ¨Årmative counterparts (cf.: A dog was barking, No dog was barking). Consequently, corpus-induced vectors of predicates and their negations are very similar. This ‚Äúcontextual invariance‚Äù of negation is indeed a well-known problem also in lexical semantics, where it has been observed that vectors of words and their (lexicalized) opposites tend to be extremely similar (Mohammad et al. 2013). In this article, we argue that the problems with negation in DS arise because we are trying to capture a purely logical kind of negation that is neither well-suited to DS nor particularly useful for modeling real-life language usage. If we isolate dogs and nondogs in the lab, the logical approach is very appealing: non-dogs include anything that is not a dog. However, consider which of the following two sentences is more likely to be uttered in a natural conversational context: (1) a. This is not a dog. . . it is a wolf. b. This is not a dog. . . it is a screwdriver. If the negation of a predicate is just the complement of the corresponding set, then Examples (1a) and (1b) should be equally plausible. However, Example (1a) is clearly more natural than (1b). Looking beyond the purely logical aspects of negation, a long tradition in formal semantics, pragmatics and psycholinguistics has stressed that, in actual conversational contexts, negation is not just excluding possible denotata of the predicates it takes scope over, but also suggesting the truth of an alternative assertion. Alternativehood (the possibility of an expression to constitute an alternative to a negated item) seems very well-suited to be modeled in DS. It is obviously similarity-based. It is, more speciÔ¨Åcally, tied to a contextual notion of similarity: We expect plausible alternatives to be objects  638  Kruszewski et al.  Conversational Negation with Distributional Semantics  or events that tend to occur in contexts that are typical of the negated ones. Finally, alternativehood, just like many lexical properties successfully modeled in DS, is an inherently graded property. Consider: (2) a. This is not a dog. . . it is a tarantula. b. This is not a dog. . . it is a conference call. Sentence (2a) is more surprising than (1a), but arguably less so than (1b). In turn, the contingencies in which the latter might be uttered, although undoubtedly bizarre, are still easier to conceive than those that would justify uttering Example (2b). The Ô¨Årst goal of the current article is then to introduce the computational linguistics community to the pragmatic, alternative-licensing view of negation, that we will call conversational negation. Second, we illustrate how DS can contribute, from a new angle, to the literature on alternativehood under conversational negation. Thanks to its ability to automatically identify potential alternatives in a large vocabulary of linguistic expressions, DS allows us to make predictions about which elements fall into the (fuzzy) alternative set of a negated expression. This is a new contribution to studies on the semantics of alternatives (in negation or other domains), where authors rely instead on their intuition to pick a small number of candidate alternatives. Our main empirical contributions are to provide a set of subject-rated negatedpredicate/alternative statements, and to predict these ratings with DS. We collect alternativehood judgments in two (minimal) sentential contexts, and study how both sentential context and the negated-item/alternative relation affect the judgments. The most striking result of the computational simulations is how good simple distributional similarity is at predicting the plausibility of an alternative. This measure comes so close to an estimated upper bound that we can only improve over it by a small margin when we use compositional methods and supervision to take sentential context and the speciÔ¨Åcs of negation into account. Finally, we present some conjectures on what a DS-based theory accounting for conversational negation could look like. We argue that negation should not be modeled as part of the static distributional representation of a single statement, but as a function that, given the negated predicate, produces a probability distribution over the predicates that are most likely to follow. This approach suggests, more generally, adopting a dynamic view of DS, not unlike the one that has been prominent for decades in other areas of semantics. The rest of this article is structured as follows. Section 2 reviews attempts to model (logical) negation in DS. Section 3 surveys the literature on alternative-licensing conversational negation. Our data set containing subject plausibility ratings for negateditem/alternative pairs is introduced and analyzed in Section 4. In Section 5, we use DS to model the ratings in the data set. We conclude in Section 6 by looking at the theoretical implications of our work, as well as suggesting directions for further study. 2. Negation in Distributional Semantics Because distributional semantics has traditionally focused on lexical aspects of meaning, negation has mostly been tackled, implicitly, as part of the study of opposites (hot and cold), or, more generally, ‚Äúcontrasting‚Äù words (warm and cold). A survey of the relevant DS literature is provided by Mohammad et al. (2013). The consensus view is that contrasting words tend to occur in similar contexts (Mohammad et al. even 639  Computational Linguistics  Volume 42, Number 4  propose a ‚Äúdistributional hypothesis of highly contrasting pairs‚Äù stating that highly contrasting pairs occur in similar contexts more often than non-contrasting word pairs). Thus, it is impossible to distinguish them from non-contrasting related words (e.g., synonyms) using standard distributional similarity measures, and ad hoc strategies must be devised. Widdows (2003) presented a pioneering study of explicit negation in DS. The assumption of that work is that negated word meanings should be orthogonal, which is to say that they should not share any common feature. SpeciÔ¨Åcally, Widdows proposes a binary negation operator, NOT(A, B), which projects the vector representing A onto the orthogonal space of the B vector. In logical terms, this can be seen as conjunction with a negated predicate (A ‚àß ¬¨B). The orthogonality assumption makes perfect sense for the information retrieval applications envisioned by Widdows (web NOT internet), but it is too strict to characterize the linguistic predicates of the relevant form in general (Italian but not Roman refers to somebody who shares many properties with Romans, such as that of speaking Italian). Interest in grammatical words in general, and negation in particular, has recently risen thanks to the development of compositional DS models (Mitchell and Lapata 2010; Baroni 2013). A shared assumption within this framework is that the operation performed by negation on vectors should be deÔ¨Åned a priori, attempting to mimic the logical properties of negation, rather than being induced from distributional data. Clark, Coecke, and Sadrzadeh (2008) explore the idea that sentences live in a space spanned by a single ‚Äútruth-denoting‚Äù basis (1), with the origin (0) corresponding to ‚Äúfalse.‚Äù A sentence like John likes Mary is represented by 1 if the sentence is true, 0 otherwise. In this framework, further elaborated by Coecke, Sadrzadeh, and Clark (2010), negation is elegantly modeled as a swap matrix. Related approaches have been presented by Preller and Sadrzadeh (2011) and Grefenstette (2013). All this work, however, is purely theoretical, and it is not clear how the proposed models would be implemented in practice. Moreover, treating negation as a swapping operator only makes sense in the abstract scenario of a vector space representing truth values. If vectors of sentences and other expressions are instead distributional in nature (e.g., representing distributions over possible contexts), it is far from clear that swapped vectors would capture linguistic negation. Indeed, Hermann, Grefenstette, and Blunsom (2013) argue that negation should not affect all dimensions in a vector, because a word, when negated, does not change its domain: The vector representation of not blue should still be close to that of other colors (Hovy [2010] also defends a similar view). Hermann and colleagues propose that vectors should include distinct ‚Äúdomain‚Äù and ‚Äúvalue‚Äù features, and negation would modify (change sign and possibly rescale) only the latter. In this way, not blue would still be in the color domain, but its chromatic values would differ from those of blue. Extrapolating to nouns, under this proposal we might expect not dog to still belong to the canid domain, but with different values from those that speciÔ¨Åcally characterize dogs. This would capture the intuition we spelled out in the introduction that a wolf is a better non-dog than a screwdriver. However, the proposal of Hermann and colleagues is, again, purely theoretical, and we do not see how domain and value features with the desired properties could be induced from corpora on a large scale. Such is the difÔ¨Åculty to model negation and other logical terms in DS that Garrette, Erk, and Mooney (2013) have proposed a division of labor between DS, handling lexical relations between content words, and Ô¨Årst-order logic, accounting for the semantics of grammatical terms. In this framework, the issue of the distributional meaning of negation does not arise.  640  Kruszewski et al.  Conversational Negation with Distributional Semantics  Finally, because of the theoretical nature of most of the relevant work, we lack benchmarks to evaluate empirical models of negation in DS. 3. Conversational Negation and Alternative Sets As already pointed out by Horn (1972), all human languages have negation and no other animal communication system includes negative utterances. This alone makes linguistic negation intriguing and justiÔ¨Åes the huge amount of literature dedicated to it. Furthermore, linguistic negation seems to play different roles and therefore constitutes a challenge for any formal theory. On the one hand, negation works as a truth-functional operator, and as such it has attracted philosophers and formal semanticists, for example, for its scope ambiguity behavior. However, linguistic negation also works as a conversational illocutionary marker, causing non-literal interpretations of propositions, and as such it has attracted the attention of linguists, psychologists, and epistemologists. Conversational negation is something different from a logical truth-functional operator that Ô¨Çips the values of its argument. In particular, in actual linguistic usage, the negation of a predicate often suggests that one of a set of alternatives might be holding. The alternative set view of negation traces back, on the one hand, to Grice‚Äôs conversational maxims (Grice 1975) and Horn‚Äôs principle of alternate implicatures (Horn 1972), and, on the other, to the ‚Äúalternative semantics‚Äù theory of focus (Rooth 1985). Implicatures as studied in pragmatics are part of sentence interpretation on top of the strictly logical meaning. A commonly assumed mechanism for generating implicatures to a statement is to take alternatives of the statement as false. For instance, Some dogs bark implicates that the alternative All dogs bark is false, even though it is compatible with the literal interpretation of the sentence (‚Äúthere are some dogs barking‚Äù). This derives pragmatic strengthening of a class of scalar elements (some > some but not all, can > can but does not have to, etc.). Under the alternative semantics theory of focus (Rooth 1985), sentences are assigned, in addition to their usual semantic values, an alternative set. For a sentence John likes JANE with focus on the individual liked by the subject, the alternative set is the set of all propositions of the form {[[john likes x]]|x an individual}. Another analytical option is the so-called ‚Äústructured meaning approach to focus,‚Äù which considers alternatives only to parts of the sentence, for example, the noun Jane, but not to whole propositions (Krifka 1992). The two theories formalize the meaning of focus-sensitive operators differently, but, importantly for us, both assume the notion of semantic alternatives to be crucial for focus interpretation. Perhaps the most widely known use of focus alternatives is the semantic analysis of focus particles such as even or only. But students of focus noticed that negation is also sensitive to them: Not A implies the truth of an alternative to A, for example, John doesn‚Äôt like JANE suggests that John likes someone else. This is not a boy suggests an alternative assertion (This is a girl; This is a man); see Kratzer (1989, p. 646) for an explicit intensional analysis of negation in terms of focus alternatives. Negation, seen as a focus operator, is similar but logically opposite to only: whereas John only likes JANE means that John likes Jane but does not like anyone else, John doesn‚Äôt like JANE means that John does not like Jane but likes someone else. Non-trivial interaction with focus alternatives is very typical for usage of negation in natural language, so negation is rarely used in a purely logical sense. As usually assumed, the alternative set for a sentence is the set of semantic values resulting from replacing the negated element with arbitrary values of the right semantic type. However, it is clear that not all alternatives are created equal. The most plausible 641  Computational Linguistics  Volume 42, Number 4  ones are relevant across many varied contexts, whereas others require a heavy contextual pressure to become acceptable. For example, boy and submarine are of the same semantic type (that of unary predicates), but it requires a very unusual context for This is not a boy to suggest This is a submarine.1 In most contexts a limited set of predicates (girl, man, etc.), all related to boy, constitute viable alternatives, and submarine is not one of them. So, although it is true that context affects the set of relevant alternatives, the most prominent ones are largely predictable from the propositional content of the utterance. The plausibility of alternatives has been studied from a psychological angle, using reasoning tasks such as the construction of settings that verify or falsify a given rule (Evans 1972) or selection of information critical for determining the truth of a rule (Wason 1966). The alternative possibilities primed by negation have been said to be ‚Äúthe most likely or relevant members of the complement set‚Äù (Oaksford 2002, page 140). In particular, Oaksford and Stenning (1992) identify several mechanisms used for constraining contrast classes (viz. alternative sets): focus/intonation, high-level relations, and world knowledge. They take the sentence Johnny didn‚Äôt travel to Manchester by train as an example. Different contrast classes will be built based on where the focus is put (Johnny vs. Manchester vs. train.). The high-level traveling schema relation imposes constraints for each slot (travelers, destinations, or mode of transportation). Other constraints are imposed by world knowledge. For example, if you are traveling from London, the vehicle of choice is unlikely to be a ship. In short, Oaksford and Stenning conclude that (emphasis ours) ‚Äúthe contrast-class members should be as similar as possible, in the relevant respects, to the negated constituent‚Äù (page 849). Psychologists have also extensively investigated the issue of whether the presence of negation automatically evokes a set of alternatives (‚Äúsearch for alternatives‚Äù view) (Oaksford and Stenning 1992; Oaksford 2002) or whether the direct effect of negation is just one of information denial (‚Äúquasi-literal‚Äù view) (Evans, Clibbens, and Rood 1996). Even under this second view, alternatives can still be evoked and explored at a later interpretation stage. Indeed, based on behavioral evidence, Prado and Novek (2006, page 327) propose to reconcile the two views by concluding that ‚Äúan initial reading of a negation will be narrow [viz. literal] and in some scenarios this might be enough. [..] A search for alternatives arises, but as part of a secondary effort to interpret the negation in the proposition.‚Äù More recently, Ferguson, Sanford, and Leuthold (2008) presented eye-movement and ERP evidence in favor of the search for alternative view. Neither side of this debate denies that alternatives play an important role in the interpretation of negated statements, and we do not take a stand on it. A phenomenon that is intuitively related to alternativehood is the degree of plausibility of a negated identity. Wason (1965, 1971) has claimed that the interpretation of negative statements of this kind is easier when the sentence negates a presupposition, something that is believed to be true. In this view, the sentence The whale is not a Ô¨Åsh is easier to interpret than The whale is not a motorcycle. While negation underlines the difference between two terms, at the same time it presupposes that they are similar: It is pragmatically reasonable to negate that two terms are the same thing when they can be confused. An alternative approach to this view is proposed by Clark (1974), who claims that comprehending negation relies on detecting differences between the proposition negated and the actual state of affairs. This predicts that, when two things are dissimilar, it should be easier to perceive one of them as a negation of the other: The  
CNRS In this article, we explore an integration of a formal semantic approach to lexical meaning and an approach based on distributional methods. First, we outline a formal semantic theory that aims to combine the virtues of both formal and distributional frameworks. We then proceed to develop an algebraic interpretation of that formal semantic theory and show how at least two kinds of distributional models make this interpretation concrete. Focusing on the case of adjective‚Äìnoun composition, we compare several distributional models with respect to the semantic information that a formal semantic theory would need, and we show how to integrate the information provided by distributional models back into the formal semantic framework. 1. Introduction Formal semantics (FS) has provided insightful models of composition and recently has addressed issues of how composition may in turn affect the original meanings of lexical items (Pustejovsky 1995; Partee 2010; Asher 2011). Type Composition Logic (TCL; Asher 2011) provides a detailed formal model of the interaction between composition and lexical meaning in which the composition of two words w and w may shift the original meanings of w and w . For example, consider the case of an adjective like heavy and a noun like trafÔ¨Åc. TCL assigns a logical form to the adjective‚Äìnoun combination heavy trafÔ¨Åc, Œªx.(O(heavy)(x) ‚àß M(trafÔ¨Åc)(x)), where O is a functor induced by the noun that outputs a meaning paraphrased as heavy for trafÔ¨Åc. The M functor does something ‚àó IRIT, Universite¬¥ Paul Sabatier, 118 Route de Narbonne, F-31062 Toulouse Cedex 9. E-mail: {nicholas.asher, tim.vandecruys, antoine.bride, marta.abrusan}@irit.fr. Submission received: 10 April 2015; revised version received: 26 July 2016; accepted for publication: 8 August 2016. doi:10.1162/COLI a 00264 ¬© 2017 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 4  similar for the noun. Different types of adjectives will interact differently with the meaning of the noun; for example, non-subsective adjectives like fake in fake dollar bill output a meaning whose denotation has an empty intersection with the denotation of the original noun but shares surface properties with things that are (e.g., [real] dollar bills). TCL thus decomposes an adjective‚Äìnoun combination into a conjunction of two properties representing the contextual contributions of the noun and adjective. This decomposition property allows TCL to predict non-trivial logical entailments just from the form of adjective‚Äìnoun compositions (in contrast to Montague‚Äôs higher order approach, which requires meaning postulates), while also capturing the shiftiness of lexical meaning, something that most formal semantic theories do not consider. However, neither TCL nor the other FS theories mentioned provide a method for constructing such functors or lexical meanings. In this article we develop two distributional semantic (DS) models able to provide such a method, in virtue of (i) TCL‚Äôs distinction between internal or conceptual content and external or referential content and (ii) the close correspondence between the way TCL and these models treat composition‚Äî in particular, the fact that these models share with TCL the decomposition property we just mentioned. We show that such methods can furnish the appropriate TCL functors, provided we take one big step: We identify TCL‚Äôs internal content with vectors, which distributional methods use to represent word meaning. Functors introduced by TCL for composition then correspond to vector transformations within distributional models. We also show how to translate the results of these transformations back into TCL logical forms. TCL logical forms will then entail non-trivial inferences based on DS lexical information, while keeping the structural and conceptual advantages of a FS based logical form. We illustrate our approach with adjective‚Äìnoun compositions because they are simpler and better understood than other compositions. Such compositions do not typically introduce scope-bearing elements like quantiÔ¨Åers, unlike the construction of verb phrases, for instance. Also, the range of variation in adjective‚Äìnoun composition is better understood, than, say, the effects of composition in verbal predications, which also involve more parameters that can potentially affect the composition. 2. Towards an Integration of DS and FS: The Formal Framework TCL TCL Asher (2011) has three advantages compared with other FS theories for studying the interactions between FS and DS: its use of types and its notion of internal content, its commitment to the actuality of meaning shifts during composition, and its formal model of meaning shift. However, TCL does not supply detailed information about particular types, which is crucial to determining meaning shifts. This is where we turn to DS for help. 2.1 Types and TCL‚Äôs Notion of Internal Content In TCL, each word has a model-theoretic meaning that determines appropriate extensions for expressions (at points of evaluation). This is TCL‚Äôs notion of external content, which is the usual notion of content in FS theories. In addition, however, each word in TCL has a type. Types are semantic objects and encode the ‚Äúinternal meaning‚Äù of the expression associated with it. So, for instance, the external semantics or extension of the word wine is a set of wine portions at some world and time, while the type or internal meaning of wine is given by the features we associate with wine (e.g., it is a liquid, a beverage, has alcohol, and has a particular taste). Internal semantics can also make use 704  Asher et al.  Integrating Type Theory and Distributional Semantics  of multi-modal information; thus olfactory and gustatory features can also play a role. These features enable speakers to correctly judge in normal circumstances whether an entity they experience falls under the extension of a term, though these judgments are not always completely reliable. The notions of internal and external meaning and the particular conception of how they interact are unique to TCL. Types and internal content play an important role in TCL. They model selectional restrictions and are used to guide composition. An irresolvable clash between the type of a predicate and the type of its argument implies that the predication is semantically anomalous. Types also guide TCL‚Äôs account of meaning shifts in predication. In TCL types encode the correct usage of a term. This is very similar to what DS methods do. However, TCL makes use of the lambda calculus for composition, a well-known and well-understood formalism; and doing so depends upon a particular interpretation of internal content based on a notion of justiÔ¨Åcation. The correct usage and a speaker‚Äôs mastery of the content of a term involves, among other things, an ability to justify, when asked, the use of that term in a particular context, and mutatis mutandis, an ability to justify the assertion of a predication in which a predicate applies to an individual characterized in a certain way. In the case of a speaker‚Äôs assertion of a predication, such a justiÔ¨Åcation explains why the speaker takes the assertion to be true. Such justiÔ¨Åcations encode the features that speakers use to identify extensions of terms. The reason these justiÔ¨Åcations are a part of linguistic mastery of expressions is that they are a reliable guide to determining extensions. Such justiÔ¨Åcations constitute internal content in TCL. Modern type theories like TCL exploit a deep relation between proofs and types, known as the Curry-Howard correspondence (Howard 1980). The Curry-Howard correspondence shows that the notions of proof and types in the lambda calculus are isomorphic, allowing one to identify types with proofs or proof schemas. TCL exploits this correspondence with justiÔ¨Åcations and types for natural language expressions: Types and justiÔ¨Åcations are structurally isomorphic and so types are formally indistinguishable from justiÔ¨Åcations. In light of such a correspondence, the particular type assigned to a sentence like this is wine is identical to its justiÔ¨Åcation, which is a defeasible proof of the truth of the proposition that the object the speaker demonstrates is wine. This identiÔ¨Åcation of types with justiÔ¨Åcations not only theoretically clariÔ¨Åes the internal content of terms, but it also allows us to exploit the notion of composition in the typed lambda calculus as a method for composing internal contents without modiÔ¨Åcation. To clarify this core concept of TCL, we sketch a recursive deÔ¨Ånition of internal meanings . for a language fragment with just nouns (N) and adjectives (A) and assuming, for illustrative purposes, a Montague-like composition rule for the two. We assume each individual, recognizable object has a name e; and to each such name we assign an individual type. We will take as primitive the set I of individual types. We identify these with an individual justiÔ¨Åcation rule r that can be used to recognize the object denoted by e. To give an example of an individual justiÔ¨Åcation rule, if a speaker A, who has used the demonstrative this to refer to a particular object in her environment, is asked to justify her use of the demonstrative (e.g., another speaker B asks, what do you mean ‚Äòthis‚Äô?), the speaker will resort to an individual justiÔ¨Åcation rule for determining the referent of this. Linguistically, such a rule is expressed as a deÔ¨Ånite description for the denotation‚Äîfor example, to explain her use of this, speaker A might say: the stuff in the glass that I‚Äôm holding. Besides I, we will also take as basic the type PROP, the type of closed formulas or sentences. PROP is a set of justiÔ¨Åcations, which are, given the Curry-Howard correspondence, defeasible proofs for the truth of formulas. Note that 705  Computational Linguistics  Volume 42, Number 4  PROP also contains the empty set ‚àÖ in case a formula has no justiÔ¨Åcation. We specify the types and internal contents . for nouns and adjectives as shown here: r N : I ‚Üí PROP. That is, noun types are functions from individual justiÔ¨Åcation rules r ‚àà I into a set of justiÔ¨Åcations that an individual satisfying r is of the type N (or ‚àÖ, if there is no such justiÔ¨Åcation). r For adjectives A , A : N ‚Üí N . That is, an adjective meaning takes a noun meaning and returns another noun meaning. To illustrate, a justiÔ¨Åcation rule for wine must provide particular features such that if something satisfying a particular individual justiÔ¨Åcation type r has these features, then we can defeasibly conclude it is wine. The justiÔ¨Åcation rule for wine will appeal to olefactory, gustatory, and visual features (clear liquid of either yellow, red, or pink color) that are typical of wine. As an example of an adjectival meaning, white is a function from a justiÔ¨Åcation rule like that of the noun type wine to a noun type justiÔ¨Åcation rule for something being white wine . As the internal content of a noun N is a function from individuals to propositions, it is of the right type to be assigned as a meaning to the Œª term ŒªxNx, the usual representation for a common noun in formal semantics. The internal content of an adjective also has the requisite structure to reÔ¨Çect the standard type of adjectives, and this enables composition using the lambda calculus. This means we can compose internal contents using the same method with which we compose external contents. TCL‚Äôs characterization of internal content yields a natural link between internal content and external, model-theoretic content. The internal semantics ‚Äútracks‚Äù the external semantics, in that in the majority of cases or in normal circumstances, the internal semantics determines appropriate truth conditions for sentences. The internal content given by the types does not determine the expression‚Äôs extension in all cases, as philosophical, externalist arguments show (Putnam 1975; Kripke 1980). But assuming speaker competence, internal content should normally yield the correct extensions for expressions. For instance, Nicholas‚Äôs olfactory and gustatory capabilities are reasonably good at distinguishing different kinds of white wine. They are not infallible; and so they cannot determine the extension of the predicate Chardonnay from the Corbie`res. But they do often work correctly and would constitute his justiÔ¨Åcation for his asserting that something is a Chardonnay from the Corbie`res. A justiÔ¨Åcation for a predicative expression should in normal circumstances identify elements in that predicate‚Äôs extension; otherwise it would not be a justiÔ¨Åcation. Similarly, an individual justiÔ¨Åcation rule rt for using a referring term t would not be a justiÔ¨Åcation if rt did not pick out normally what t refers to. Composing these justiÔ¨Åcations and similar ones for other parts of speech together to get a justiÔ¨Åcation for a whole sentence will then also normally deliver the correct truth value in a circumstance of evaluation. Because these justiÔ¨Åcations tell us what the truth conditions of the sentence would be in the normal case, they are in effect a modal characterization of those truth conditions. 2.2 TCL and Meaning Shifts Meaning shifts occur often when composition occurs. We call meaning shifting compositions co-compositions, following Pustejovsky (1995). There are several kinds of cocomposition. One kind is easily explained using TCL‚Äôs system of types. An ambiguous word may be made less ambiguous when it combines with other words. Consider for  706  Asher et al.  Integrating Type Theory and Distributional Semantics  instance the word trafÔ¨Åc. It is ambiguous at least between the senses of denoting a Ô¨Çow of vehicles or information. However, when combined with a modiÔ¨Åer like Internet or New York City in the phrases Internet trafÔ¨Åc and New York City trafÔ¨Åc, this ambiguity vanishes: The modiÔ¨Åer selects or at least prefers one of the senses of trafÔ¨Åc. TCL and other type theoretic lexical theories represent the different senses of ambiguous words with the use of disjoint types. For example, trafÔ¨Åc would have the disjoint type INFORMATION ‚à® VEHICLE. TCL models the disambiguation in the phrases above with an inference that is logically sound: The predicate, by selecting one of the disjoint types to satisfy its selectional restrictions, makes the other types in the disjoint union non-applicable, thus conferring a more specialized meaning to the argument. But the composition of a predicate and its argument can exhibit other sorts of meaning shifts as well, which pose challenges for type theories other than TCL. Consider the following adjective‚Äìnoun compositions:  (1)  a. heavy appliance  b. heavy rain  c. heavy sea  d. heavy bleeding  e. heavy smoker  In these examples the head noun affects the meaning of the modiÔ¨Åer. If these data are well known, formal analyses for them are not. We could assume that adjectives are wildly ambiguous, roughly one sense for each noun with which they can combine. And we could model their internal content in terms of the disjoint union of their possible precisiÔ¨Åcations (the unambiguous senses). But that would miss or obscure certain logical relations. For instance, a heavy physical object does have something in common with heavy rain, and even with heavy smoker and heavy bleeding; in each case some dimension of the denotation of the head noun is modiÔ¨Åed towards an extreme, saturated end of the scale (Mel‚Äôcuk 2006). A disjoint union type is right for homonymously ambiguous expressions (such as bank) but not for logically polysemous ones‚Äîexpressions whose senses have some logical or metaphysical connection. To analyze logical polysemy, TCL appeals to functors that shift the meaning of the predicational relation itself. Although TCL motivates the functor view based on an analysis of coercion, it also uses it for co-composition. In TCL an expression has a type presupposition which must be satisÔ¨Åed in the predicational environment; a failure to satisfy such a type either leads to semantic anomaly or coercion effects. Type presuppositions are very general types like EVENTUALITY, PHYSICAL-OBJECT, or INFORMATIONAL-OBJECT. But an expression also has a more speciÔ¨Åc, ‚ÄúÔ¨Åne-grained‚Äù type that encapsulates the internal content speciÔ¨Åc to the term, the sort of content we discussed before. It is this Ô¨Åne-grained content that TCL exploits in co-composition. TCL‚Äôs approach to adjective‚Äìnoun co-composition is quite different from a standard Montagovian approach. In standard semantic treatments, an adjectival meaning is a functor taking a noun meaning as an argument and returning a noun phrase meaning; composition is a matter of applying the adjective meaning as a higher-order property to the noun meaning. In TCL the noun and adjective meanings affect each other, and the output of an adjective‚Äìnoun composition is the conjunction of a modiÔ¨Åed adjectival meaning and a modiÔ¨Åed noun meaning, which are both Ô¨Årst order properties and apply to individuals, as in Schema (2). It introduces functors that potentially modify both the adjective and the noun‚Äôs internal content in co-composition and then conjoins the modiÔ¨Åed contents. In the adjective‚Äìnoun composition Schema (2), A is the adjective, N the  707  Computational Linguistics  Volume 42, Number 4  noun, OA the functor on the noun given by the adjective, and MN the functor on the adjective induced by the noun:  (2)  Œªx (OA(N(x)) ‚àß MN(A(x)))  For subsective adjectives,1 which include the vast majority of adjectives in most languages, OA selects a subtype or constituent type of N, if they shift the meaning of N at all. Thus, the individuals satisfying OA(N(x)) will necessarily be a subset of the denotation of N at any point of evaluation. TCL thus predicts an inÔ¨Çuence of the adjective on the noun‚Äôs denotation when we have a subsective modiÔ¨Åer‚Äîin particular, an ambiguous noun may be disambiguated by the modiÔ¨Åer‚Äôs meaning. Those few adjectives that are not subsective, like former in former prisoner, support inferences that subsective adjectives do under the scope of some sort of modal or temporal operator; for example, former prisoners were once prisoners, possible difÔ¨Åculties are difÔ¨Åculties in some epistemic alternative, and fake guns and stone lions appear to be or look like guns and lions. TCL also predicts an often non-trivial shift in the meaning of a modiÔ¨Åer as it combines with various nouns or vice versa. This coincides with our Ô¨Åndings in distributional semantics for adjective‚Äìnoun compositions in Section 4. For instance, non-intersective adjectives are predicted to undergo a modiÔ¨Åcation that relativizes their denotation. Consider a non-intersective adjective like small in the sentence that‚Äôs a small elephant. The functor Melephant should shift to select those things in the denotation of elephant that are small on a scale suitable for elephants. Adjective‚Äìnoun compositions analyzed with functors thus immediately yield interesting inferences; that‚Äôs a small elephant entails that that is an elephant and that it was small for an elephant. According to TCL, adjective‚Äìnoun compositions should thus be decomposable, in the sense that it should entail that there is an object of type N as modiÔ¨Åed by the adjective and that it has some properties given by the modiÔ¨Åed sense of the adjective. This formalizes observations made by other researchers as well (Kamp and Partee 1995; Partee 2010).  2.3 Types as Algebraic Objects  TCL tells us about the general form of composition, and the TCL equation in Example (2) imposes useful constraints on the functors essential to this process. But to build appropriate functors for individual words like heavy in the context of storm, for instance, TCL does not provide any method. DS offers us the promise of giving us the functors we want in a systematic and automatic way. We will model each word‚Äôs type or TCL internal content with a suitable algebraic object from DS. In most versions of DS, each basic word meaning is a vector in some space V whose dimensions are contextual features or a more abstract set of latent features. This is not quite the sort of defeasible justiÔ¨Åcations discussed in Section 2.1, but it is a place to start and it contains information pertinent to justiÔ¨Åcation. Thus, individual word types will be modeled as vectors in a Ô¨Ånite dimensional space V, whose dimensions reÔ¨Çect aspects of the context of use. The DS counterpart of a TCL functor is a transformation of v ‚àà V into a vector v ‚àà V, where v‚Äôs values  
University of Sussex We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization. 1. Introduction This article addresses a central unresolved issue in distributional semantics: how to model semantic composition. Although there has recently been considerable interest in this problem, it remains unclear what distributional composition actually means. Our view is that distributional composition is a matter of contextualizing the lexemes being composed. This goes well beyond traditional word sense disambiguation, where each lexeme is assigned one of a Ô¨Åxed number of senses. Our proposal is that composition involves deriving a Ô¨Åne-grained characterization of the distributional meaning of each lexeme in the phrase, where the meaning that is associated with each lexeme is bespoke to that particular context. Distributional composition is, therefore, a matter of integrating the meaning of each of the lexemes in the phrase. To achieve this we need a structure within which all of the lexemes‚Äô semantics can be overlaid. Once this is done, the lexemes can collectively agree on the semantics of the phrase, and in so doing, determine the semantics that they have ‚àó Department of Informatics, University of Sussex. Falmer, Brighton BN1 9QH, UK. E-mail: {d.j.weir, j.e.weeds, j.p.reffin, t.kober}@sussex.ac.uk. Submission received: 10 April 2015; revised version received: 17 April 2016; accepted for publication: 20 April 2016. doi:10.1162/COLI a 00265 ¬© 2017 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 4  in the context of that phrase. Our process of composition thus creates a single structure that encodes contextualized representations of every lexeme in the phrase. The (uncontextualized) distributional knowledge of a lexeme is typically formed by aggregating distributional features across all uses of the lexeme found within the corpus, where distributional features arise from co-occurrences found in the corpus. The distributional features of a lexeme are associated with weights that encode the strength of that feature. Contextualization involves inferring adjustments to these weights to reÔ¨Çect the context in which the lexeme is being used. The weights of distributional features that don‚Äôt Ô¨Åt the context are reduced, while the weight of those features that are compatible with the context can be boosted. As an example, consider how we contextualize the distributional features of the word wooden in the context of the phrase wooden Ô¨Çoor. The uncontextualized representation of wooden presumably includes distributional features associated with different uses, for example, The director Ô¨Åred the wooden actor and I sat on the wooden chair. So, although we may have observed in a corpus that it is plausible for the adjective wooden to modify Ô¨Çoor, table, toy, actor, and voice, in the speciÔ¨Åc context of the phrase wooden Ô¨Çoor, we need to Ô¨Ånd a way to down-weight the distributional features of being something that can modify actor and voice, while up-weighting the distributional features of being something that can modify table and toy. In this example we considered so-called Ô¨Årst-order distributional features; these involve a single dependency relation (e.g., an adjective modifying a noun). Similar inferences can also be made with respect to distributional features that involve higherorder grammatical dependencies.1 For example, suppose that we have observed that a noun that wooden modiÔ¨Åes (e.g., actor) can be the direct object of the verb Ô¨Åred, as in The director Ô¨Åred the wooden actor. We want this distributional feature of wooden to be downweighted in the distributional representation of wooden in the context of wooden table, since things made of wood do not typically lose their job. In addition to specializing the distributional representation of wood to reÔ¨Çect the context wooden Ô¨Çoor, the distributional representation of Ô¨Çoor should also be reÔ¨Åned, down-weighting distributional features arising in contexts such as Prices fell through the Ô¨Çoor, while up-weighting distributional features arising in contexts such as I polished the concrete Ô¨Çoor. In our example, some of the distributional features of wooden‚Äîin particular, those to do with the noun that this sense of wooden could modify‚Äîare internal to the phrase wooden Ô¨Çoor in the sense that they are alternatives to one of the words in the phrase. Although it is speciÔ¨Åcally a Ô¨Çoor that is wooden, our proposal is that the contextualized representation of wooden should recognize that it is plausible that nouns such as chair and toy could be modiÔ¨Åed by the particular sense of wooden that is being used. The remaining distributional features are external to the phrase. For example, the verb mop could be an external feature, because things that can be modiÔ¨Åed by wooden can be the direct object of mop. The external features of wooden and Ô¨Çoor with respect to the phrase wooden Ô¨Çoor provide something akin to the traditional interpretation of the distributional semantics of the phrase, namely, a representation of those (external) contexts in which this phrase can occur. Although internal features are, in a sense, inconsistent with the speciÔ¨Åc semantics of the phrase, they provide a way to embellish the characterization of the distributional  
¬© 2017 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 4  parsing. The need for each task is discussed, together with an overview of work that has developed specialized methods for social media‚Äìtype texts. Also included are useful discussions about how each of these modules is typically evaluated. The methods are discussed rather brieÔ¨Çy; they are pointers to work rather than descriptions of the models or techniques. An exception is a detailed example given for dialect identiÔ¨Åcation, where the model, data, experiments, and comprehensive results are presented. Similar detailed work is presented at a few other chosen points in the book. Newcomers to NLP technology will appreciate a listing of NLP tools in this chapter, and those new to social media processing will Ô¨Ånd pointers to tools developed speciÔ¨Åcally for social media texts. Chapter 3 focuses on information extraction problems such as geolocation identiÔ¨Åcation, opinion mining, event extraction, and NLP applications such as machine translation and automatic summarization. Although this chapter is called ‚Äúsemantic analysis‚Äù and the term ‚Äúsemantics‚Äù is used at several points in the book, the authors acknowledge that the term refers to any intelligent processing of the text, rather than an attempt to create a representation of the text‚Äôs meaning. Again, detailed examples are presented for geolocation, opinion classiÔ¨Åcation, and machine translation. The usefulness of these examples is mixed, and will be discussed later in this review. Otherwise, the methods and techniques are motivated and described brieÔ¨Çy, similar to the previous chapter. The second part of the book, Chapters 4 and 5, discusses applications that beneÔ¨Åt from processing of social media texts, and how data can be obtained for research. A wide spectrum of applications are described including Ô¨Ånancial, political, and defense interests. Chapter 5 summarizes existing data sets and evaluation methods together with discussions about how useful data can be gleaned from large volumes, and how privacy can be maintained during social media research. The conclusion chapter reiterates the key challenges of social media processing, and brings up questions that are yet to be addressed by the NLP community. The book ends with a brief case study on a social media‚Äìbased event-monitoring application that combines many of the tasks and technologies discussed earlier in the book. The authors should be commended for taking up a subject in which NLP has a huge impact. At the same time, as a nascent area, this topic is worthy of discussion within our community to identify the main challenges, the techniques that are consistently useful, as well as the scope for further work. In this regard, this book provides a valuable introduction to the Ô¨Åeld. For a short book, it covers a number of dimensions to social media language analysis‚Äîpreprocessing, information extraction, as well as applications. Newcomers to the social media domain will Ô¨Ånd the book a good reference point to research done in the area. The highlighting of challenges in this domain and differences from NLP work typically carried out on news and other single-author and edited text are very useful. Another handy element is a continuous focus on explaining how methods for various NLP problems are evaluated. At the same time, although a broad set of topics is discussed, some key areas of social media processing are ignored. One is the use of social media as a tool for language understanding. On one side, researchers are excited about using social media as a large-scale source of conversation data on which questions about language can be asked. Recent work in this area has created new insights into language variation (Doyle 2014; Eisenstein 2015b), phonology (Eisenstein 2013, 2015a), and entrainment (DanescuNiculescu-Mizil, Gamon, and Dumais 2011; Doyle, Yurovsky, and Frank 2016), to name a few. Another line of work is richer conversation understanding through language processing beyond lexical and syntactic levels. Work on dialog act tagging (Kim,  834  Book Reviews Cavedon, and Baldwin 2010; Ritter, Cherry, and Dolan 2010), topic structure (Ramage, Dumais, and Liebling 2010; Peng, Wang, and Dredze 2014), as well as phrasing (Niculae and Danescu-Niculescu-Mizil 2014; Tan, Lee, and Pang 2014) on social media conversations fall under this category and yet are not covered in this book. Even on the word- and sentence-level, the book does not cover research carried out in the related web, social media, and social network communities. Many of the models in these areas carry out similar word-level processing but ask different sets of research questions. There are some references to this line of work but the majority of the content remains unexplored. Finally, there is heavy focus on Twitter or microblogs in comparison with other forms of social media. Some sections such as summarization or machine translation entirely focus on Twitter. Although it may be possible that there are logical divisions of social media text types and that tweets represent a class of short texts, such reÔ¨Çections and explanations are not provided in the book. Going beyond coverage, a more fundamental question that newcomers as well as current NLP researchers want to ask is what is the role of natural language processing for social media. It is clear that the tasks involved in preprocessing content (normalization, POS tagging, and parsing) are clearly in the realm of natural language processing, and so are tasks such as named entity recognition, opinion mining, and event extraction. However, the book does not do a good job of distinguishing what new techniques are required due to distinguishing properties of the social media domain. Some of the tasks appear to be solvable by adding annotated data from this domain to existing NLP corpora, and retraining the models. Other problems appear to be solved by supervised techniques with word-level features, sometimes combined with social media and social network metadata. It is not even clear from the book whether syntactic parsing, and so forth, are useful in current NLP work on social media. A reÔ¨Çection on the types of techniques that work well so far, and insights into new types of models proposed for social media, and an emphasis on methods addressing special properties, for example, streaming algorithms, would have been worthwhile additions. Clarifying these questions would have been an important desired property of a book on NLP and social media, and this is the main place where the book falls short. The book is also not self-sufÔ¨Åcient for understanding the details of social media processing. There is no clear discussion of particular models, especially the ones speciÔ¨Åc to social media. For example, normalization of non-dictionary words is a task clearly new and motivated by social media texts, but only a very brief section is given to its discussion and devoid of any models or techniques. Often multiple paragraph descriptions of a study are given but without enough details, equations, or technical explanations to actually understand the approach. As a result, not much assimilation of the topic is possible at the end of many sections of the book. Rather, a reader may only use these descriptions as a pointer for Ô¨Ånding work to read further and understand. Another undesirable property of this book is a heavy focus on research carried out by the authors of the book themselves. The detailed examples in the book where comprehensive explanations of data, models, and results are given are all the authors‚Äô own work with their collaborators. At least from the book, it is not clear why these models should be described in detail compared with other tasks and the work of other researchers. The case study is also on a company associated with one of the authors. For a book focusing on such a wide topic, a biased focus could have been avoided. Still, overall, this book will be useful as a reference point for work currently done, and a starting point for further discussions on social media and NLP work. 835  Computational Linguistics  Volume 42, Number 4  References Danescu-Niculescu-Mizil, Cristian, Michael Gamon, and Susan Dumais. 2011. Mark my words!: Linguistic style accommodation in social media. In Proceedings of the 20th International Conference on World Wide Web (WWW), pages 745‚Äì754, Hyderabad. Doyle, Gabriel. 2014. Mapping dialectal variation by querying social media. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 98‚Äì106, Gothenburg. 
doi:10.1162/COLI r 00271 ¬© 2017 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 4  presentations of these tools that I have seen, with the caveat that it presupposes a certain Ô¨Çuency with the underlying mathematical notation and concepts. It concludes with an application of these tools to reÔ¨Çexive pronouns and intersective and nonintersective attributive adjectives, as well as propositional vs. predicate negation and conjunction. Chapter 4 is concerned with the interpretation of quantiÔ¨Åers in natural language. Generalized quantiÔ¨Åer theory is discussed, Ô¨Årst with descriptions of quantiÔ¨Åers as relations between sets, followed by discussion of quantiÔ¨Åers as sets of predicates. This is followed by clear and rather standard exposition of monotonicity, negative polarity item licensing, the denotation of quantiÔ¨Åcational determiners, and conservativity. Despite my saying that Chapters 3 and 4 are very clear‚Äîor maybe because of this‚ÄîI want to reiterate again that some hand-holding will be required for students without backgrounds in, for example, mathematics or computer science. Simple familiarity with the basic concepts and notation of sets and functions is in fact insufÔ¨Åcient; the reader must be comfortable moving quickly between these characterizations and translating between them. Take for example the discussion of quantiÔ¨Åcational determiner meanings, where in the span of just three pages (p. 116ff), the text switches between (a) a Curried, higher-order function (a function from predicates to functions from predicates to truth values), (b) a binary function (a function mapping two sets of individuals to truth values), and (c) a relation (a set of pairs of sets of individuals), choosing each formulation where convenient. Typographic conventions do not help the reader either. Sometimes the variables A and B are functions (as in [4.47a]) and sometimes they are sets (as in [4.47b] right below!). I Ô¨Ånd these issues unfortunate because it would not have taken an inordinate effort to make these (otherwise great) chapters friendlier towards readers who are less mathematically inclined. I think of Chapter 5, entitled ‚ÄúLong-Distance Meaning Relationships,‚Äù as logically two chapters and will discuss them in turn. The Ô¨Årst half introduces the process of function abstraction through hypothetical reasoning in the tradition here called the Lambek-Van Benthem Calculus, motivated by the discussion of non-subject quantiÔ¨Åers and relativization. A brief detour introduces the idea of introducing a hypothetical premise œï into a proof of conclusion œà and then ‚Äúdischarging‚Äù œï later, resulting in the conclusion œï ‚Üí œà. By analogy, in the computation of many sentences with quantiÔ¨Åers and relativization, we introduce a hypothetical argument (variable) u to yield the u-dependent meaning z, and then later ‚Äúdischarge‚Äù the hypothesis of u to yield the function Œªu . z, abstracting over the value of u. This presentation is friendly and successful. There is no mention of the relation of this process of variable introduction and abstraction to syntactic movement as discussed in much contemporary work on the syntax/semantics interface, save for one brief mention later (p. 175). The second half of Chapter 5 uses questions raised in the Ô¨Årst half regarding the relationship between semantics and surface word order as motivation to introduce the framework known as Abstract Categorial Grammar (ACG) or also Lambda Grammar. Under this approach, lexical items (signs) have perceptual denotations that can be (higher-order) functions over strings, not just strings themselves. For example, the transitive verb praise has the perceptual representation Œªx . Œªy . y ¬∑ praised ¬∑ x, where ¬∑ is concatenation; this encodes that the Ô¨Årst argument of praise will follow the verb in surface word order and its second argument will precede the verb. This treatment of the syntax of word order will strike readers with any previous training to the intricacies and  838  Book Reviews regularities of natural language syntax as terribly naive. But to be fair, these limitations are readily acknowledged by Winter himself: ...when we talk about ‚ÄòAbstract Categorial Grammar,‚Äô we use the term ‚Äògrammar‚Äô in the sense of a formal grammatical framework, with no pretensions to the wide empirical coverage that we normally expect from grammatical descriptions of natural languages... at present there is no commonly accepted way of embedding ACG within linguistic theory. (p. 167) The chapter ends with discussion and treatment of scope ambiguities. Chapter 6 is an introduction to intensional semantics, motivated by classic problems of sense versus reference. After a brief and approachable survey of intensional contexts, a domain of possible worlds (or indices) is added to our models and individual concepts are introduced, deriving the desired result that Lewis Carroll is C. L. Dodgson and John believes Lewis Carroll wrote Alice does not entail John believes C. L. Dodgson wrote Alice. This is followed by a more general discussion of intensionalization using the ACG type system introduced in Chapter 5. The chapter concludes with de re/de dicto ambiguities and their analysis as scope ambiguities. Finally, Chapter 7 suggests directions for further study, devoting a paragraph each to a range of additional topics in formal semantics. This is followed by a brief appendix to Chapter 3 with formal deÔ¨Ånitions for the basic typing and interpretation system. Each of the core chapters ends with pointers to recommended further reading, at both introductory and advanced levels, which include key works in linguistics, computer science, and philosophy of language. This is followed by a set of exercises, immediately followed by selected solutions. Some exercises emphasize mechanical practice in engaging with technical notions, some are more conceptual, and some introduce new linguistic phenomena to consider. All are clear and of appropriate difÔ¨Åculty. Some are downright clever, like problems 5 and 6 of Chapter 2 (p. 38ff). The inclusion of selected solutions‚Äîjust Ô¨Ånal results, not intermediate steps or detailed justiÔ¨Åcation‚Äî will be appreciated by conscientious students and those using the text for self-study. In reading the book as a potential text for students, a few gaps stand out. In Chapter 2, where entailment is given a central role in the empirical investigation of meaning, there is no discussion of presupposition. In fact, the deÔ¨Ånition of ‚Äúentailment‚Äù (p. 16) as any indefeasible consequence will include both presuppositions and entailments proper. Also missing is discussion of motivation and diagnostics for constituency. Readers with little or no linguistics background would have to simply accept the bracketings and trees given in the text, without reliable tools to work out novel sentences on their own. There is also no discussion of languages other than English. Of course, the methods presented here are broadly applicable to other languages, but in my experience it is worth making this point. Still, at the end of the day, I generally think the choice of concepts and topics covered is good, as is their depth. The physical book (I reviewed the paperback) is relatively compact, with nice, large text. The prose feels mature and well edited, with just a small handful of typos. Overall, Elements is a generally clear, precise, and friendly introduction to formal natural language semantics for students with mathematical maturity. I recommend it. Michael Yoshitaka Erlewine is an assistant professor at the National University of Singapore. He received his Ph.D. in Linguistics from MIT in 2014. His research interests lie in the syntax/semantics interface and linguistic theory, with a particular interest in long-distance dependencies including movement and focus. In previous work, he has investigated Mandarin Chinese, English, and Japanese, as well as a number of understudied Mayan and Austronesian languages. He thanks Brian Buccola for valuable discussion and detailed comments on this review. Erlewine‚Äôs e-mail address is mitcho@nus.edu.sg. 839 
In a major breakthrough, Valiant (1975) showed that context-free grammar recognition is no more complex than Boolean matrix multiplication for a matrix of size m √ó m where m is linear in the length of the sentence, n. With current state-of-the-art results in matrix multiplication, this means that CFG recognition can be done with an asymptotic complexity of O(n2.38). ‚àó School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, United Kingdom. E-mail: scohen@inf.ed.ac.uk. ‚àó‚àó Department of Computer Science, University of Rochester, Rochester, NY 14627, United States. E-mail: gildea@cs.rochester.edu. Submission received: 26 October 2016; revised version received: 19 January 2016; accepted for publication: 15 February 2016. doi:10.1162/COLI a 00254 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 3  In this article, we show that the problem of linear context-free rewriting system (LCFRS) recognition can also be reduced to Boolean matrix multiplication. Current chart-parsing algorithms for binary LCFRS have an asymptotic complexity of O(n3f ), where f is the maximal fan-out of the grammar.1 Our algorithm takes time O(nœâd), for a constant d which is a function of the grammar (and not the input string), and where the complexity of n √ó n matrix multiplication is M(n) = O(nœâ ). The parameter d can be as small as f , meaning that we reduce parsing complexity from O(n3f ) to O(nœâf ), and that, in general, the savings in the exponent is larger for more complex grammars. LCFRS is a broad family of grammars. As such, we are able to support the Ô¨Åndings of Rajasekaran and Yooseph (1998), who showed that tree-adjoining grammar (TAG) recognition can be done in time O(M(n2)) = O(n4.76) (TAG can be reduced to LCFRS with d = 2). As a result, combinatory categorial grammars, head grammars, and linear indexed grammars can be recognized in time O(M(n2)). In addition, we show that inversion transduction grammars (ITGs; Wu 1997) can be parsed in time O(nM(n2)) = O(n5.76), improving the best asymptotic complexity previously known for ITGs. 1.1 Matrix Multiplication State of the Art Our algorithm reduces the problem of LCFRS parsing to Boolean matrix multiplication. Let M(n) be the complexity of multiplying two such n √ó n matrices. These matrices can be na¬®ƒ±vely multiplied in O(n3) time by computing for each output cell the dot product between the corresponding row and column in the input matrices (each such product is an O(n) operation). Strassen (1969) discovered a way to do the same multiplication in O(n2.8704) time‚Äîhis algorithm is a divide and conquer algorithm that eventually uses only seven operations (instead of eight) to multiply 2 √ó 2 matrices. With this discovery, there have been many attempts to further reduce the complexity of matrix multiplication, relying on principles similar to Strassen‚Äôs method: a reduction in the number of operations it takes to multiply sub-matrices of the original matrices to be multiplied. Coppersmith and Winograd (1987) identiÔ¨Åed an algorithm that has the asymptotic complexity of O(n2.375477). Others have slightly improved that algorithm, and currently there is an algorithm for matrix multiplication with M(n) = O(nœâ ) such that œâ = 2.3728639 (Le Gall 2014). It is known that M(n) = ‚Ñ¶(n2 log n) (Raz 2002). Although the asymptotically best matrix multiplication algorithms have large constant factors lurking in the O-notation, Strassen‚Äôs algorithm does not, and is widely used in practice. Bened¬¥ƒ± and Sa¬¥nchez (2007) show speed improvement when parsing natural language sentences using Strassen‚Äôs algorithm as the matrix multiplication subroutine for Valiant‚Äôs algorithm for CFG parsing. This indicates that similar speed-ups may be possible in practice using our algorithm for LCFRS parsing. 1.2 Main Result Our main result is a matrix multiplication algorithm for unbalanced, single-initial binary LCFRS with asymptotic complexity M(nd) = O(nœâd) where d is the maximal  
‚àó LT3, Faculty of Arts and Philosophy, Groot-Brittanni√´laan 45, 9000 Ghent, Belgium. E-mail: orphee.declercq@ugent.be. ‚àó‚àó LT3, Faculty of Arts and Philosophy, Groot-Brittani√´laan 45, 9000 Ghent, Belgium. E-mail: veronique.hoste@ugent.be. Submission received: 11 September 2014; revised version received: 2 November 2015; accepted for publication: 28 February 2016. doi:10.1162/COLI_a_00255 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 3  example, the 2003 National Assessment Adult Literacy showed that only 13% of adults were maximally proÔ¨Åcient in understanding texts they encounter in their daily life. The European Commission has also been involved in extensive investigations of literacy after research had revealed that almost one in Ô¨Åve adults in the European society lack the literacy skills to successfully function in a modern society (Wolf 2005). Every day we are confronted with all sorts of texts, some of which are easier to process than others. Moreover, it seems that the documents that are potentially the most important for adult readers are also the more difÔ¨Åcult ones to process, such as mortgage Ô¨Åles, legal texts, or patient information leaÔ¨Çets. According to a recent OECD study where the literacy of adults from 23 Western countries or regions was rated on a Ô¨Åve-point scale, these speciÔ¨Åc texts genres all require a literacy level of at least four. The Ô¨Åndings of this study for participants from the Dutch language area show that only 12.4% of adults in Flanders and 18.2% in the Netherlands reach the two highest levels of proÔ¨Åciency (OECD 2013). Readability research and the automatic prediction of readability has a very long and rich tradition (see surveys by Klare 1976; DuBay 2004; Benjamin 2012; and CollinsThompson 2014). Whereas superÔ¨Åcial text characteristics leading to on-the-spot readability formulas were popular until the last decade of the previous century (Flesch 1948; Gunning 1952; Kincaid et al. 1975), recent advances in the Ô¨Åeld of computer science and natural language processing have triggered the inclusion of more intricate characteristics in present-day readability research (Si and Callan 2001; Collins-Thompson and Callan 2005; Schwarm and Ostendorf 2005; Heilman, Collins-Thompson, and Eskenazi 2008; Feng et al. 2010). The bulk of these studies, however, have focused on readability as perceived by speciÔ¨Åc groups of people, such as children (Schwarm and Ostendorf 2005), second language learners (Fran√ßois 2009), or people with intellectual disabilities (Feng et al. 2010), and on the readability of texts in speciÔ¨Åc domains, such as the medical one (Leroy and Endicott 2011). The investigation of the readability of a wide variety of texts without targeting a speciÔ¨Åc audience has not received much attention (Benjamin 2012). Moreover, when it comes to current state-of-the art systems, it can be observed that even though more complex features trained on various levels of complexity have proven quite successful when implemented in a readability prediction system (Pitler and Nenkova 2008; Feng et al. 2010; Kate et al. 2010), there is still no consensus on which features are actually the best predictors of readability. As a consequence, when institutions, companies, or other research disciplines wish to use readability prediction techniques, they still rely on the more outdated superÔ¨Åcial characteristics and formulas (see, for example, the recent work by van Boom [2014] on the readability of mortgage terms). In this article, we investigate the creation of a fully automatic readability assessment system that can assess generic text material in two languages, English and Dutch. We use a supervised machine learning approach and investigate both a regression and classiÔ¨Åcation set-up reÔ¨Çecting the two possible readability prediction tasks: scoring individual texts or comparing two texts. This requires general evaluation corpora of English and Dutch generic text comprising various text genres and levels of readability. As well as a suitable corpus, the investigation also requires a methodology to assess readability: In this respect, we were the Ô¨Årst to explore crowdsourcing as an alternative to using expensive expert labels (De Clercq et al. 2014). In our system various text characteristics have been implemented ranging from easy-to-compute superÔ¨Åcial text features to features requiring deep linguistic processing. We investigate to what extent automatically derived features can be considered  458  De Clercq and Hoste  Finding the Optimal Feature Set for General Readability Prediction  optimal for predicting readability in both languages under consideration. We envisage Ô¨Ånding the optimal mix of these readability predictors by exploiting a wrapperbased approach to feature selection using a genetic algorithm. We will show that going beyond correlation calculations for readability optimization using genetic algorithms is a promising task that provides considerable insights in which feature combinations contribute to the overall readability prediction. Another aspect of this research is to investigate in closer detail the contribution of those features requiring deep linguistic processing. Though many advances have been made in NLP, the more difÔ¨Åcult text-understanding tasks still achieve moderate performance rates. Think, for example, of coreference resolution where a combined F-measure of 60% is considered state-of-the-art.1 Implementing such features in a fullÔ¨Çedged readability prediction system is thus risky as the automatically derived features might not truly represent the information at hand. Because we have gold standard deep syntactic and semantic information available for our Dutch readability data set, we were able to investigate in close detail its added value in predicting readability. Interestingly, we will observe that the performance of our fully automatic readability prediction pipeline is on par with the pipeline using gold-standard deep syntactic and semantic information. The remainder of this article is organized as follows. After describing the related research with a speciÔ¨Åc focus on features that have been used in previous readability research (Section 2), we explain in Section 3 how the English and Dutch data were collected and assessed. Section 4 describes the methods used to perform the actual optimization experiments, the results of which are described and analyzed in Section 5. We end with a concluding general discussion in Section 6. 2. Related Work What makes a particular text easy or difÔ¨Åcult to read has been the central question in reading research over the past century. There seems to be a consensus that readability depends on complex language comprehension processes between a reader and a text (Davison and Kantor 1982; Feng et al. 2010). This implies that reading ease can be determined by looking at both intrinsic text properties as well as aspects of the reader. Since the Ô¨Årst half of the 20th century, however, readability formulas have been developed to automatically predict the readability of an unseen text based only on superÔ¨Åcial text characteristics such as the average word or sentence length. Over the years, many objections have been raised against these traditional formulas: their lack of absolute value (Bailin and Grafstein 2001), the fact that they are solely based on superÔ¨Åcial text characteristics (Davison and Kantor 1982; DuBay 2004, 2007; Feng, Elhadad, and Huenerfauth 2009; Kraf and Pander Maat 2009), the underlying assumption of a regression between readability and the modeled text characteristics (Heilman, Collins-Thompson, and Eskenazi 2008), and so forth. Furthermore, there seems to be a remarkably strong correspondence between the readability formulas themselves, even across different languages (van Oosten, Tanghe, and Hoste 2010). These objections have led to new quantitative approaches of doing readability prediction that adopt a machine learning perspective to the task. Advancements in these Ô¨Åelds have introduced more intricate prediction methods such as naive Bayes classiÔ¨Åers (Collins-Thompson and Callan 2004), logistic regression (Fran√ßois 2009) and  
‚àó University of Groningen, the Netherlands. E-mail: johan.bos@rug.nl. Submission received: 9 October 2015; accepted for publication: 9 December 2015. doi:10.1162/COLI a 00257 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 3  build upon automated deduction methods (Blackburn and Bos 2005). Hence, the goal of this squib is to Ô¨Ånd answers to the following questions: 1. Are AMRs any different from traditional meaning representations? 2. Can we provide a model-theoretic semantics for AMRs? 3. Is there a systematic translation to Ô¨Årst-order logic? 4. Is universal quantiÔ¨Åcation expressible in AMRs? 5. Does the AMR language need to be extended? To answer these questions, we will formulate a systematic translation function from AMR into classical logic. First, we will consider the simplest of AMRs. Then, we will add negation and investigate the role of recurrent variables. Finally, an extension to AMR to cover scope phenomena will be proposed. Throughout, some basic knowledge of Ô¨Årst-order logic (FOL) and the Œª-calculus is assumed. 2. Basic AMRs In this section we will take a closer look at AMRs without polarity and recurrent variables. We will provide a deÔ¨Ånition of its syntax, discuss viewing AMRs as triples, its unique ability to display information structure by inversing roles, and Ô¨Ånally provide a model-theoretic deÔ¨Ånition. Figure 1 shows example AMRs represented as trees (tense is ignored in the AMR analyses throughout this article). Every AMR has a unique root, displayed as the top node in the tree. What we further see here are variables (e, x, y, etc.), events and concepts (moan-01, child, etc.), constants (‚ÄúMs Ribble‚Äù), and roles (ARG0, etc.). The slash indicates an instance: x/child means that x is an instance of the concept child. The colon is a punctuation symbol to display roles. The brackets indicate which role belongs to which concept, and, as line breaks are optional, AMRs can also be given in linear format, for example, (e/read-01 :ARG0 (x/girl) :ARG1 (y/book)) for a girl read a book. A remarkable property of AMRs is the ability to invert roles. Role inversion swaps the arguments of a relation: R(x,y) ‚â° R-of(y,x). Role inversion will lead (for polarity-free AMRs) to AMRs that are semantically equivalent, but are structured differently. For the given example we can apply role inversion in two ways, as Figure 2 shows. The Ô¨Årst AMR in Figure 2 puts the focus on girl, paraphrased as a girl read a book. In the second AMR we have placed book in the foreground, corresponding to a book that was read by a girl. Role inversion has its limitations: we cannot ‚Äúpull out‚Äù both book and girl, because we would be left with an ill-formed AMR equipped with two roots. Although AMRs are usually visualized as tree structures, they can also be viewed as directed acyclic graphs with a unique root (with vertices denoting variables and edges denoting roles and instances). As a consequence, AMRs can be transformed into sets of triples, which is convenient for evaluation purposes (Cai and Knight 2013). For semantic  (e / moan-01 :ARG0 (x / child))  (e / give-01 :ARG0 (x / person :named "Ms Ribble") :ARG2 (y / child) :ARG1 (z / envelope))  Figure 1 AMRs for The children moaned (left) and Ms Ribble handed out envelopes to the children (right).  528  Johan Bos  Expressive Power of Abstract Meaning Representations  (x / girl  (y / book  :ARG0-of (e / read-01  :ARG1-of (e / read-01  :ARG1 (y / book)))  :ARG0 (x / girl)  Figure 2 AMR‚Äôs feature to display information structure by role inversion.  interpretation, the tree structure is handier, because for operators like negation we need to be able to assign scope. In order to give a semantic, model-theoretic interpretation, we could do this in a fairly straightforward way by converting concepts and events to one-place predicates, the roles to two-place predicates, and by existentially quantifying over all variables introduced by concepts and events. But this would not allow us to include scopal operators, such as negation, quantiÔ¨Åcation, and projection in a systematic way. What we will do instead is provide a formal deÔ¨Ånition of the syntax of AMRs and then deÔ¨Åne a recursive translation function from AMR to FOL. This function bears strong similarities with the conversion from AMR to Œª-calculus presented in Artzi, Lee, and Zettlemoyer (2015). We will use the following notational conventions: Ai to denote AMRs, x for variables, c for constants, P for properties, and R for roles. DeÔ¨Ånition 1 (Syntax of Basic AMRs) A ::= c | (x/P) | (x/P :R1A1 . . . :RnAn) This deÔ¨Ånition says that constants, instance assignments, and instance assignments decorated with ‚Äúout-going‚Äù roles are all AMRs. This is perhaps slightly counterintuitive, because semantic objects of different types are placed in the same equation. The translation function that we will deÔ¨Åne next will clarify this issue, because it will translate all AMR-constructs into something of a propositional type. The best way to conceive this is by viewing an AMR c as the proposition ‚Äúthere exists an entity denoted by the constant c‚Äù and an AMR (x/P) as ‚Äúthere exists an x with property P,‚Äù and so on. To deal with correct assignment of scope (relevant when we introduce polarity in AMRs), we delay the translation of roles by converting them Ô¨Årst into Œª-expressions abstracting over role players. For instance, if the already-translated x is connected to the not-yet-translated AMR A via a role R, we get the expression Œªy.R(x,y). The resulting recursive translation function maps an AMR paired with a Œª-expression for roles to an FOL formula (œÜ denotes a Œª-expression for roles): DeÔ¨Ånition 2 (Semantics of Basic AMRs) c,œÜ = œÜ(c) (x/P),œÜ = ‚àÉx(P(x)‚àßœÜ(x)) (x/P :R1A1 . . . :RnAn),œÜ = ‚àÉx(P(x) ‚àß A1,Œªy.R1(x,y) ‚àß . . . ‚àß An,Œªy.Rn(x,y) ‚àß œÜ(x)) When translating a concept that is related to other concepts, we do not know what semantic material these concepts will introduce, to which we can bind the roles that connect them. The translation function deals with this by postponing this decision with the help of Œª-bound formulas representing roles. However, when we start translating a fresh AMR, we start with the root node. The root is not connected to other concepts by out-going roles, so we need to give it a ‚Äúdummy‚Äù formula: Œªx. . Here stands for a 529  Computational Linguistics  Volume 42, Number 3  formula that is always true (so for any formula œÜ, (œÜ ‚àß ) is logically equivalent to œÜ). Figure 3 shows an example derivation. The resulting structure is a closed formula (all its variables are bound), since the translation ensures that no free occurrences of variables can appear. Interestingly, basic AMRs are equivalent to the controlled DRT-fragment introduced by Bos (2010). As the latter are in the two-variable fragment of FOL, so are basic AMRs. Recall that Ô¨Årstorder logic is undecidable. The two-variable fragment, however, is a decidable Ô¨Årstorder language where formulas have maximally two differently named variables, no function symbols, but possibly do have equality. It has the Ô¨Ånite model property, which means that if a formula of this fragment is satisÔ¨Åable, it is satisÔ¨Åable in a Ô¨Ånite model (Mortimer 1975). 3. AMRs with Polarity Negation is expressed in AMRs with a polarity relation. The relation is between the concept that is negated and the constant ‚Äú‚Äì‚Äù. Needless to say this is a special kind of relation: It cannot be inverted, and it needs a distinctive treatment during semantic interpretation, because it is propositions that are negated, not concepts. One could see the polarity relation as a Ô¨Çag, indicating that an AMR is negated. In order to accommodate polarity in AMR, we Ô¨Årst need to extend its syntax, and then add a clause in the translation function. This is done in the following two deÔ¨Ånitions (from here on we abbreviate :R1A1 . . . :RnAn as :RiAi for convenience): DeÔ¨Ånition 3 (Syntax of AMRs with Polarity) A ::= c | (x/P) | (x/P :RiAi) | (x/P :RiAi :polarity‚Äì) DeÔ¨Ånition 4 (Semantics of AMRs with Polarity) c,œÜ = œÜ(c) (x/P),œÜ = ‚àÉx(P(x)‚àßœÜ(x)) (x/P :RiAi),œÜ = ‚àÉx(P(x)‚àß Ai,Œªy.Ri(x,y) ‚àßœÜ(x)) (x/P :RiAi :polarity‚Äì),œÜ = ¬¨‚àÉx(P(x)‚àß Ai,Œªy.Ri(x,y) ‚àßœÜ(x)) This translation function maps the polar AMR given in Figure 4 to the following formula: ¬¨‚àÉe(giggle-01(e)‚àß‚àÉx(boy(x)‚àß ARG0(e,x))). As can be seen here, negation outscopes all other existential quantiÔ¨Åers because the polarity is assigned to the root concept of the AMR. If we assign the negative polarity to x instead, the AMR would be translated as ‚àÉe(giggle-01(e)‚àß¬¨‚àÉx(boy(x)‚àßARG0(e,x))). This formula could be true in a situation where there was someone giggling, but not a boy: It was not a boy who was giggling. As we observed in the previous section, promoting concepts by inversing  (e / shout-01 :ARG0 (x / teacher)), Œªu. = [2.c] ‚àÉe(shout-01(e)‚àß (x/teacher), Œªy.ARG0(e,y) ‚àßŒªu. (e)) = [2.b] ‚àÉe(shout-01(e)‚àß ‚àÉx(teacher(x)‚àß Œªy.ARG0(e,y)(x))‚àßŒªu. (e)) = [Œ≤-conv] 
University of Twente/ Erasmus University Rotterdam Language is a social phenomenon and variation is inherent to its social nature. Recently, there has been a surge of interest within the computational linguistics (CL) community in the social dimension of language. In this article we present a survey of the emerging Ô¨Åeld of ‚Äùcomputational sociolinguistics" that reÔ¨Çects this increased interest. We aim to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics such as the relation between language and social identity, language use in social interaction, and multilingual communication. Moreover, we demonstrate the potential for synergy between the research communities involved, by showing how the large-scale data-driven methods that are widely used in CL can complement existing sociolinguistic studies, and how sociolinguistics can inform and challenge the methods and assumptions used in CL studies. We hope to convey the possible beneÔ¨Åts of a closer collaboration between the two communities and conclude with a discussion of open challenges. 1. Introduction Science has experienced a paradigm shift along with the increasing availability of large amounts of digital research data (Hey, Tansley, and Tolle 2009). In addition to the traditional focus on the description of natural phenomena, theory development, and ‚àó Department EWI Researchgroup Human Media Interaction (HMI), PO Box 217, 7500 AE, Enschede, The Netherlands. E-mail: dong.p.ng@gmail.com. A. Seza DogÀò ru√∂z: a.s.dogruoz@gmail.com; Carolyn P. Ros√©: cprose@cs.cmu.edu; Franciska de Jong: f.m.g.dejong@utwente.nl. Submission received: 25 August 2015; accepted for publication: 18 February 2016. doi:10.1162/COLI_a_00258 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 3  computational science, data-driven exploration and discovery have become a dominant ingredient of many methodological frameworks. In line with these developments, the Ô¨Åeld of computational linguistics (CL) has also evolved. Human communication occurs in both verbal and nonverbal form. Research on computational linguistics has primarily focused on capturing the informational dimension of language and the structure of verbal information transfer. In the words of Krishnan and Eisenstein (2015), computational linguistics has made great progress in modeling language‚Äôs informational dimension, but with a few notable exceptions, computation has had little to contribute to our understanding of language‚Äôs social dimension. The recent increase in interest of computational linguists to study language in social contexts is partly driven by the ever increasing availability of social media data. Data from social media platforms provide a strong incentive for innovation in the CL research agenda and the surge in relevant data opens up methodological possibilities for studying text as social data. Textual resources, like many other language resources, can be seen as a data type that is signaling all kinds of social phenomena. This is related to the fact that language is one of the instruments by which people construct their online identity and manage their social network. Of course, there are challenges as well. For example, social media language is more colloquial and contains more linguistic variation, such as the use of slang and dialects, than the language in data sets that have been commonly used in CL research (e.g., scientiÔ¨Åc articles, newswire text, and the Wall Street Journal) (Eisenstein 2013b). However, an even greater challenge is that the relation between social variables and language is typically Ô¨Çuid and tenuous, whereas the CL Ô¨Åeld commonly focuses on the level of literal meaning and language structure, which is more stable. The tenuous connection between social variables and language arises because of the symbolic nature of the relation between them. With the language chosen a social identity is signaled, which may buy a speaker1 something in terms of footing within a conversation; or, in other words, for speakers there is room for choice in how to use their linguistic repertoire in order to achieve social goals. This freedom of choice is often referred to as the agency of speakers and the linguistic symbols chosen can be thought of as a form of social currency. Speakers may thus make use of speciÔ¨Åc words or stylistic elements to represent themselves in a certain way. However, because of this agency, social variables cease to have an essential connection with language use. It may be the case, for example, that on average female speakers display certain characteristics in their language more frequently than their male counterparts. Nevertheless, in speciÔ¨Åc circumstances, females may choose to de-emphasize their identity as females by modulating their language usage to sound more male. Thus, although this exception serves to highlight rather than challenge the commonly accepted symbolic association between gender and language, it nevertheless means that it is less feasible to predict how a woman will sound in a randomly selected context. Speaker agency also enables creative violations of conventional language patterns. Just as with any violation of expectations, these creative violations communicate indirect meanings. As these violations become conventionalized, they may be one vehicle towards language change. Thus, agency plays a role in explaining the variation in and dynamic nature of language practices, both within individual speakers and across speakers. This variation is manifested at various levels of expression‚Äîthe choice  
2. This book not only presents the main sub-tasks of sentiment analysis, such as sentiment classiÔ¨Åcation at different discourse levels, opinion summarization, opinion search, and emotion identiÔ¨Åcation, but also covers doi:10.1162/COLI r 00259 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 3  many emerging sentiment-related topics, such as sentiment analysis of debates and discussions, mining of intentions, and deceptive opinion detection. This shows that Liu has extensive research experience as well as knowledge of industry applications and knows the importance of these emerging tasks, which are in demand from the industry. Furthermore, this book includes the latest methods and technologies for each related task published in the mainstream conference proceedings and journals. What is especially valuable is that it describes Ô¨Årst-hand experience in building a sentiment analysis system. Liu thus brings to readers a global view of the whole area from research to practice, and meets the needs of different readerships, from experienced scholars in natural language processing to graduate students and industry practitioners interested in sentiment analysis and social media analysis. This book thus serves as a comprehensive yet in-depth survey of references in sentiment analysis. 3. It is also commendable that the book gives a balanced treatment of both linguistic approaches and machine learning approaches to solving the sentiment analysis problem. In recent years, the mainstream methods in the research literature have been based on supervised learning with elaborate feature engineering. State-of-the-art techniques also use deep learning models to learn effective features directly from raw data as an alternative to manual feature engineering. However, as mentioned in this book, supervised learning methods provide no linguistic interpretations and do not generate knowledge for linguists or industry developers to gain insights of the problem. Practically, when errors occur in an application, it is hard to know what is wrong and how to Ô¨Åx it. Fortunately, this book provides a comprehensive list of linguistic constructs and perspectives that are instrumental for sentiment analysis, which make up for the deÔ¨Åciency of black-box approaches using pure machine learning. Moreover, it also lists and elaborates on many speciÔ¨Åc linguistic phenomena that are critical for effective classiÔ¨Åcation of sentiment such as negation (Chapter 5), modality (Chapter 5), and comparison (Chapter 8). We believe that this book will enable the reader to gain not just a comprehensive understanding of the computation methods but also deep linguistic insights of the sentiment analysis problem and its possible solutions.  The book is organized into 13 chapters. The Ô¨Årst two chapters introduce the basics and deÔ¨Åne the sentiment analysis problem. Chapters 3‚Äì9 discuss the core sentiment analysis tasks (e.g., sentiment classiÔ¨Åcation, aspect analysis, and opinion summarization) and their current solution methods. Chapters 10‚Äì13 investigate the emerging themes from recent research and applications (e.g., analysis of debates, intentions, fake opinions, and review quality). SpeciÔ¨Åcally, Chapter 1 motivates and gives an overview of the whole book. It describes the expression of sentiment as one of the most important and complicated phenomena of human language. The goal of sentiment analysis is to computationally extract sentiments, opinions, and emotions expressed in text, which is different from the goal of traditional linguistic studies aimed at understanding the human language. Technically, analysis of sentiment can be divided into several levels according to different 596  Book Review discourse granularity, such as document, sentence, and aspect or sentiment-and-target levels. Liu took a structured approach to write this book. 
1. Introduction Statistical machine translation (SMT) is a data-driven approach to the translation of text from one natural language into another. It emerged in the 1990s and matured in the 2000s to become widespread today; the core SMT methods (Brown et al. 1990, 1993; Berger et al. 1996; Koehn, Och, and Marcu 2003) learn direct correspondences between source and target language from collections of translated sentences, without the need for ‚àó Informatics Institute, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, The Netherlands. E-mail: a.bisazza@uva.nl. ‚àó‚àó Fondazione Bruno Kessler, Via Sommarive 18, 38123 Povo, Trento, Italy. E-mail: federico@fbk.eu. Submission received: 5 February 2015; revised version received: 3 November 2015; accepted for publication: 12 January 2016. doi:10.1162/COLI_a_00245 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 2  SRC verb  subject  object  complement  jdd AlEAhl Almgrby Almlk mHmd AlsAds dEm -h l- m$rwE Alr}ys Alfrnsy  renewed the-monarch the-Moroccan the-King Mohamed the-sixth support his to project the-president the-French  REF The Moroccan monarch King Mohamed VI renewed his support to the project of the French President  Figure 1 Arabic source sentence (right-to-left) and English reference translation, taken from the NIST-MT09 benchmark. The Arabic sentence is morphologically segmented by AMIRA (Diab, Hacioglu, and Jurafsky 2004) according to the Arabic Treebank scheme, and provided with Buckwalter transliteration (left-to-right) and English glosses.  abstract linguistic representations. The main advantages of SMT are versatility and costeffectiveness: In principle, the same modeling framework can be applied to any pair of languages with minimal engineering effort, given sufÔ¨Åcient amounts of translation data. However, experience in a diverse range of language pairs has revealed that this form of modeling is highly sensitive to structural differences between source and target language, particularly at the level of word order. Indeed, natural languages vary greatly in how they arrange sentence components, and translating words in the correct order is essential to preserving meaning across languages. In English, for instance, the role of different predicate arguments is determined precisely by their relative position within the sentence. Consider the translation example in Figure 1: Looking at the English glosses of the Arabic sentence, one can see that corresponding words in the two languages are placed in overall similar orders with the notable exception of the verb (jdd/renewed), which occurs at the beginning of the Arabic sentence but in the middle of the English one‚Äîmore speciÔ¨Åcally, between the subject and the object. To reach the correct English order, three other reorderings are required between pairs of adjacent Arabic words: (AlEAhl/the-monarch, Almgrby/the-Moroccan), (dEm/support, -h/his), and (Alr}ys/the-president, Alfrnsy/the-French). This example suggests a simple division of reordering patterns into long range, or global, and short range, or local. However, other language pairs display more complex, hierarchical patterns. Word reordering phenomena are naturally handled by human translators1 but are a major source of complexity for SMT. In very general terms, the task of SMT consists of breaking the input sentence into smaller units, selecting an optimal translation for each unit, and placing them in the correct order. Searching for the overall best translation throughout the space of all possible reorderings is, however, computationally intractable (Knight 1999). This crucial fact has motivated an impressive amount of research around two inter-related questions: namely, how to effectively restrict the set of allowed word permutations and how to detect the best permutation among them. Existing solutions to these problems range from heuristic constraints, based on word-to-word distances and completely agnostic about the sentence content, to linguistically motivated SMT frameworks where the entire translation process is guided by syntactic structure. The research in word reordering has advanced together with core SMT research and has sometimes directed it, being one of the main motivations for the development of tree-based SMT. At the same time, the variety of word orders existing in world languages has pressed the SMT community to admit the importance of  
Despite the fact that SCFGs are a very natural extension of CFGs, and that the parsing problem for CFGs is rather well understood nowadays, our knowledge of the parsing problem for SCFGs is quite limited, with many questions still left unanswered. In this article we tackle one of these open problems. Unlike CFGs, SCFGs do not admit any canonical form in which rules are bounded in length (Aho and Ullman 1972), as for instance in the well-known Chomsky normal form for CFGs. A consequence of this fact is that the computational complexity of parsing with SCFG depends on the grammar. More precisely, for a Ô¨Åxed SCFG, parsing is polynomial in the length of the string. The degree of the polynomial depends on the ‚àó Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu. ‚àó‚àó Dipartimento di Ingegneria dell‚ÄôInformazione, Universita` di Padova, Via Gradenigo 6/A, 35131 Padova, Italy. E-mail: satta@dei.unipd.it. Submission received: 28 July 2015; revised version received: 4 February 2016; accepted for publication: 8 February 2016. doi:10.1162/COLI a 00246 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 2  length of the grammar‚Äôs rules, the re-ordering patterns represented by these rules, as well as the strategy used to parse each rule. The complexity of Ô¨Ånding the best parsing strategy for a Ô¨Åxed SCFG rule has remained open. This article shows that it is NP-hard to Ô¨Ånd the binary parsing strategy having the lowest space complexity. We also consider the problem of Ô¨Ånding the parsing strategy with the lowest time complexity, and we show that it would require progress on long-standing open problems in graph theory either to Ô¨Ånd a polynomial algorithm or to show NP-hardness. The parsing complexity of SCFG rules increases with the increase of the number of nonterminals in the rule itself. Practical machine translation systems usually conÔ¨Åne themselves to binary rules, that is, rules having no more than two right-hand side nonterminals, because of the complexity issues and because binary rules seem to be adequate empirically (Huang et al. 2009). Longer rules are of theoretical interest because of the naturalness and generality of the SCFG formalism. Longer rules may also be of practical interest as machine translation systems improve. For a Ô¨Åxed SCFG, complexity can be reduced by factoring the parsing of a grammar rule into a sequence of smaller steps, which we refer to as a parsing strategy. Each step of a parsing strategy collects nonterminals from the right-hand side of an SCFG rule into a subset, indicating that a portion of the SCFG rule has been matched to a subsequence of the two input strings, as we explain precisely in Section 2. A parsing strategy is binary if it combines two subsets of nonterminals at each step to create a new subset. We consider the two problems of Ô¨Ånding the binary parsing strategy with optimal time and space complexity. For time complexity, there is no beneÔ¨Åt to considering parsing strategies that combine more than two subsets at a time. For space complexity, the lowest complexity can be achieved with no factorization whatsoever; however, considering binary parsing strategies can provide an important trade-off between time and space complexity. Our results generalize those of Crescenzi et al. (2015), who show NPcompleteness for decision problems related to both time and space complexity for linear parsing strategies, which are deÔ¨Åned to be strategies that add one nonterminal at a time. Our approach constructs a graph from the permutation of nonterminals in a given SCFG rule, and relates the parsing problem to properties of the graph. Our results for space complexity are based on the graph theoretic concept of carving width, whose decision problem is NP-complete for general graphs. Section 3 relates the space complexity of the SCFG parsing problem to the carving width of a graph constructed from the SCFG rule. In Section 4, we show that any polynomial time algorithm for optimizing the space complexity of binary SCFG parsing strategies would imply a polynomial time algorithm for carving width of general graphs. Our results for time complexity are based on the graph theoretic concept of treewidth. In Section 5, we show that any polynomial time algorithm for the decision problem of the time complexity of binary SCFG strategies would imply a polynomial time constant‚Äìfactor approximation algorithm for the treewidth of general graphs. In the other direction, NP-completeness of the decision problem of the time complexity for SCFG would imply the NP-completeness of treewidth of general graphs of degree six. These are both long-standing open problems in graph theory. 2. Synchronous Context-Free Grammars and Parsing Strategies In this section we informally introduce the notion of synchronous context-free grammar and deÔ¨Åne the computational problem that we investigate in this article. We assume the reader to be familiar with the notion of context-free grammar, and only brieÔ¨Çy 208  Gildea and Satta  Synchronous Context-Free Grammars and Optimal Parsing Strategies  summarize the adopted notation. Throughout this article, for any positive integer n we write [n] to denote the set {1, . . . , n}. 2.1 Synchronous Context-Free Grammars As usual, a CFG has a Ô¨Ånite set of rules having the general form A ‚Üí Œ±, where A is a nonterminal symbol to be rewritten and Œ± is a string of nonterminal and terminal symbols. A synchronous context-free grammar is a rewriting system based on a Ô¨Ånite set of so-called synchronous rules. Each synchronous rule has the general form [A1 ‚Üí Œ±1, A2 ‚Üí Œ±2], where A1 ‚Üí Œ±1 and A2 ‚Üí Œ±2 are CFG rules. By convention, we refer to A1 ‚Üí Œ±1 and A2 ‚Üí Œ±2 as the Chinese and English components of the synchronous rule, respectively. Furthermore, Œ±1, Œ±2 must be synchronous strings. This means that there exists a bijection between the occurrences of nonterminals in Œ±1 and the occurrences of nonterminals in Œ±2, and that this bijection is explicitly provided by the synchronous rule. Nonterminal occurrences that correspond under the given bijection are called linked nonterminal occurrences, or just linked nonterminals when no ambiguity arises. We assume that linked nonterminals always have the same label. As a simple example, consider the synchronous rule [A ‚Üí aA 1 bB 2 d, A ‚Üí bB 2 cA 1 ] where A, B are nonterminal symbols and a, b, c, d are terminal symbols. We have indicated the bijection associated with the synchronous rule by annotating the nonterminal occurrences with natural numbers, with the intended meaning that linked nonterminals get the same number. We refer to this number as a nonterminal‚Äôs index. The bijection associated with a synchronous rule plays a major role in the derivation of a pair of strings by the SCFG. In fact, in an SCFG we can only apply a synchronous rule to linked nonterminals. To illustrate this, we use our running example and consider the synchronous strings [A 1 , A 1 ]. We can construct a derivation by applying our synchronous rule to the linked nonterminals, written [A 1 , A 1 ] ‚áí [aA 2 bB 3 d, bB 3 cA 2 ] Note that we have renamed the indices in the synchronous rule to make them disjoint from the indices already in use in the synchronous string to be rewritten. Although this is unnecessary in this Ô¨Årst derivation step, this strategy will always be adopted to avoid conÔ¨Çicts in more complex derivations. We can move on with our derivation by applying once more our synchronous rule to rewrite the linked A nonterminals, obtaining [A 1 , A 1 ] ‚áí [aA 2 bB 3 d, bB 3 cA 2 ] ‚áí [aaA 4 bB 5 dbB 3 d, bB 3 cbB 5 cA 4 ] In this derivation, the renaming of the indices is crucial to avoid conÔ¨Çicts. Let S be a special nonterminal of our SCFG, which we call the starting nonterminal. Using our notion of derivation, one can start from [S 1 , S 1 ] and attempt to derive a pair of strings [u, v] entirely composed of terminal symbols. Whenever this is possible, we say that [u, v] is in the translation generated by the SCFG, meaning that v is one of the possible translations of u. 209  Computational Linguistics  Volume 42, Number 2  Example 1 Consider the following list of synchronous rules, where symbols si are rule labels to be used later as references s1 : [S ‚Üí A 1 B 2 , S ‚Üí B 2 A 1 ] s2 : [A ‚Üí aA 1 b, A ‚Üí bA 1 a] s3 : [A ‚Üí ab, A ‚Üí ba] , s4 : [B ‚Üí cB 1 d, B ‚Üí dB 1 c] s5 : [B ‚Üí cd, B ‚Üí dc] What follows is a valid derivation of the SCFG, obtained by rewriting at each step the linked nonterminals with the Chinese component occurring at the left-most position in the sentential form  [S 1 , S 1 ] ‚áís1 [A 2 B 3 , B 3 A 2 ] ‚áís2 [aA 4 bB 3 , B 3 bA 4 a] ‚áís2 [aaA 5 bbB 3 , B 3 bbA 5 aa] ‚áís3 [aaabbbB 3 , B 3 bbbaaa] ‚áís4 [aaabbbcB 6 d, dB 6 cbbbaaa] ‚áís5 [aaabbbccdd, ddccbbbaaa]  It is not difÔ¨Åcult to see that the given SCFG provides the translation {[apbpcqdq, dqcqbpap] | p, q ‚â• 1}. One can also associate SCFG derivations with parse trees, in a way very similar to what we do with CFG derivations. More precisely, an SCFG derivation is represented as a pair of parse trees, each generating one component in the derived string pair. Furthermore, linked nonterminals in the SCFG derivation are annotated in the parse trees to indicate the place of application of some synchronous rule.  Example 2 The SCFG derivation for string pair [aaabbbccdd, ddccbbbaaa] in Example (1) is associated with the following tree pair:  S1  S1  A2  B3  B3  A2  a A4 b  c B6 d d B6 c  b A4 a  a A5 b  cd  dc  b A5 a  ab  dc  210  Gildea and Satta  Synchronous Context-Free Grammars and Optimal Parsing Strategies  One common view of SCFGs is that each synchronous rule implements a permutation of the (occurrences of the) nonterminal symbols in the two rule components. More precisely, the nonterminals in the right-hand side of the Chinese rule component are re-used in the right-hand side of the English component, but with a different ordering, as deÔ¨Åned by the bijection associated with the rule. This reordering is at the core of the translation schema implemented by the SCFG. We will often use the permutation view of a synchronous rule in later sections. As already mentioned, we assume that linked nonterminals in a synchronous rule have the same label. In natural language processing applications, this is by far the most common assumption, but one might also drop this requirement. The results presented in this article do not rest on such a restriction. 2.2 Parsing Strategies The recognition and the parsing problems for SCFGs are natural generalizations of the same problems deÔ¨Åned for CFGs. Given an SCFG of interest, in the recognition problem one has to decide whether an input pair [u, v] can be generated. In addition, for the parsing problem one has to construct a parse forest, in some compact representation, with all tree pairs that the SCFG produces when generating [u, v]. When dynamic programming techniques are used, it turns out that recognition and parsing algorithms are closely related, since the elementary steps that are performed during recognition can be memoized and later used to construct the desired parse forest. Despite the similarity between CFGs and SCFGs, it turns out that there is a considerable gap in the space and time complexity for recognition and parsing based on these two classes. More speciÔ¨Åcally, for a Ô¨Åxed CFG, we can solve the recognition problem in time cubic in the length of the input string, using standard dynamic programming techniques. In contrast, for a Ô¨Åxed SCFG we can still recognize the input pair [u, v] in polynomial time, but the degree of the polynomial depends on the speciÔ¨Åc structure of the synchronous rules of the grammar, and can be much larger than in the CFG case. A similar scenario also holds for the space complexity. The reason for this gap is informally explained in what follows. In the investigation of the recognition problem for CFGs and SCFGs, a crucial notion is the one of parsing strategy. Recognition algorithms can be seen as sequences of elementary computational steps in which two parse trees that generate non-overlapping portions of the input are combined into a larger parse tree, according to the rules of the grammar. We call parsing strategy any general prescription indicating the exact order in which these elementary steps should be carried out. Generally speaking, parsing strategies represent the order in which each parsing tree of the input is constructed. For the CFG case, let us consider the standard top‚Äìdown chart parsing algorithm of Kay (1980). The algorithm adopts a parsing strategy that visits the nonterminals in the right-hand side of a rule one at a time and in a left-to-right order, combining the associated parse trees accordingly. Parse trees are then compacted into so-called chart edges, which record the two endpoints of the generated substring along with other grammar information. Chart parsing can be generalized to SCFGs. However, in the latter case parse trees no longer generate single substrings: They rather generate tuples with several substrings. This can be ascribed to the added component in synchronous rules and to the re-ordering of the nonterminals across the two components. Let us call fan-out the number of substrings generated by a parse tree (this notion will be formally deÔ¨Åned later). It is well-known among parsing practitioners that the 211  Computational Linguistics  Volume 42, Number 2  fan-out affects the number of stored edges for a given input string, and is directly connected to the space and time performance of the algorithm. A binary parsing strategy of fan-out œï has space complexity O(n2œï) and time complexity at most O(n3œï) where n is the sentence length (Seki et al. 1991; Gildea 2010). If we adopt the appropriate parsing strategy, we can reduce the fan-out, resulting in asymptotic improvement in the space and time complexity of our algorithm. To illustrate this claim we discuss a simple example.  Example 3 Consider the synchronous rule  s : [A ‚Üí B 1 B 2 B 3 B 4 B 5 B 6 B 7 B 8  A‚ÜíB5B7B3B1B8B6B2B4]  (1)  The permutation associated with this rule is schematically visualized in Figure 1. A naive parsing strategy for rule s would be to collect the nonterminals B k one at a time and in ascending order of k. For instance, at the Ô¨Årst step we combine B 1 and B 2 , constructing a parse tree with fan-out 3, as seen from Figure 1. The worst case is attested when we construct the parse tree consisting of occurrences B 1 , . . . , B 5 , which has a fan-out of 4. Alternatively, we could explore more Ô¨Çexible parsing strategies. An example is depicted in Figure 2, where each elementary step is represented as an internal node of a tree. This time, at the Ô¨Årst step (node n1) we combine B 3 and B 4 , as depicted in Figure 3a. At the second step (node n3), we combine the result of node n1 and B 2 , again constructing a parse tree with fan-out three, as depicted in Figure 3b, and so on. This strategy is non-linear, because it might combine parses containing more than one nonterminal occurrence each; see Figure 3c. From Figure 1 it is not difÔ¨Åcult to check that at each node nk the constructed parse has fan-out bounded by 3. We therefore conclude that our second strategy is more efÔ¨Åcient than the left-to-right one.  B8 B7 B6 B5 B4 B3 B2 B1 A Figure 1 Graphical representation of the permutation associated with synchronous rule s of Equation (1). 212  Gildea and Satta  Synchronous Context-Free Grammars and Optimal Parsing Strategies  n7  n5  n6  B1  n3  B5  n4  B2  n1  B6  n2  B3 B4  B7 B8  Figure 2 A bidirectional parsing strategy for rule s of Equation (1).  This example shows that the maximum fan-out needed to parse a synchronous rule depends on the adopted parsing strategy. In order to optimize the computational resources of a parsing algorithm, we need to search for a parsing strategy that minimizes the maximum fan-out of the intermediate parse trees. The problem that we investigate in this article is therefore the one of Ô¨Ånding optimal parsing strategies for synchronous rules, where in this context optimal means with a value of the maximal fan-out as small as possible. 2.3 Fan-out and Parsing Optimization In this section we provide a mathematical deÔ¨Ånition for the concepts that we have informally introduced in the previous section. We need to do so in order to be able to precisely deÔ¨Åne the computational problem that is investigated in this article.  (a) B 4 B3 n1 (b) B 2 n1 n3 (c) n5 n6 A Figure 3 First step (a), second step (b), and last step (c) of the parsing strategy in Figure 2. 213  Computational Linguistics  Volume 42, Number 2  Assume a synchronous context-free rule s with r ‚â• 2 linked nonterminals. We need to address occurrences of nonterminal symbols within s. We write 1, i to represent the ith occurrence (from left to right) in the right-hand side of the Chinese component of s. Similarly, 2, i represents the ith occurrence in the right-hand side of the English component of s. Each pair h, i , h ‚àà [2] and i ‚àà [r], is called an occurrence. We assume without loss of generality that the nonterminals of the Chinese component are indexed sequentially. That is, the index of 1, i is i. Let œÄ be the permutation over set [r] implemented by s, meaning that 2, i is annotated with index œÄ(i), and therefore is co-indexed with 1, œÄ(i) . As an example, in rule (1) we have r = 8 and œÄ(1) = 5, œÄ(2) = 7, œÄ(3) = 3, etc. Under this convention, each pair ( 1, œÄ(i) , 2, i ) and, equivalently, each pair ( 1, i , 2, œÄ‚àí1(i) ), i ‚àà [r] is called a linked pair. A parsing strategy for s is a rooted, binary tree œÑs with r leaves, where each leaf is a linked pair ( 1, œÄ(i) , 2, i ). As already explained, the intended meaning is that each internal node of œÑs represents the operation of combining the linked pairs below the left node with the linked pairs below the right node. These operations must be performed in a bottom‚Äìup fashion. Let n be an internal node of œÑs, and let œÑn be the subtree of œÑs rooted at n. We write y(n) to denote the set of all occurrences h, i appearing in the linked pair of some leaf of œÑn. We say that occurrence h, i ‚àà y(n) is a right boundary of n if i = r or if i < r and h, i + 1 ‚àà y(n). Symmetrically, we say that h, i ‚àà y(n) is a left boundary if i = 1 or if i > 1 and h, i ‚àí 1 ‚àà y(n). Note that the occurrences 1, 1 and 2, 1 are always left boundaries, and 1, r and 2, r are always right boundaries. We let bd(n) be the total number of right and left boundaries in y(n). Intuitively, the number of boundaries in y(n) provides the number of endpoints of the substrings in the rule components of s that are spanned by the occurrences in y(n). Dividing this total number by two provides the number of substrings spanned by the occurrences in y(n). We therefore deÔ¨Åne the fan-out at node n as  fo(n)  =  
We propose to operationalize partitionability as clusterability, a measure of how easy the occurrences of a lemma are to cluster. We test two ways of measuring clusterability: (1) existing measures from the machine learning literature that aim to measure the goodness of optimal k-means clusterings, and (2) the idea that if a lemma is more clusterable, two clusterings based on two different ‚Äúviews‚Äù of the same data points will be more congruent. The two views that we use are two different sets of manually constructed lexical substitutes for the target lemma, on the one hand monolingual paraphrases, and on the other hand translations. We apply automatic clustering to the manual annotations. We use manual annotations because we want the representations of the instances that we cluster to be as informative and ‚Äúclean‚Äù as possible. We show that when we control for polysemy, our measures of clusterability tend to correlate with partitionability, in particular some of the type-(1) clusterability measures, and that these measures outperform a baseline that relies on the amount of overlap in a soft clustering. ‚àó Department of Theoretical and Applied Linguistics, University of Cambridge, UK. E-mail: diana@dianamccarthy.co.uk. ‚àó‚àó LIMSI, CNRS, Universite¬¥ Paris-Saclay, France. E-mail: marianna.apidianaki@limsi.fr. ‚Ä† Department of Linguistics, University of Texas at Austin, USA. E-mail: katrin.erk@mail.utexas.edu. Submission received: 13 June 2014; revised version received: 3 August 2015; accepted for publication: 25 January 2016. doi:10.1162/COLI a 00247 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 2  1. Introduction In computational linguistics, the Ô¨Åeld of word sense disambiguation (WSD)‚Äîwhere a computer selects the appropriate sense from an inventory for a word in a given context‚Äîhas received considerable attention.1 Initially, most work focused on manually constructed inventories such as WordNet (Fellbaum 1998) but there has subsequently been a great deal of work on the related Ô¨Åeld of word sense induction (WSI) (Pedersen 2006; Manandhar et al. 2010; Jurgens and Klapaftis 2013) prior to disambiguation. This article concerns the phenomenon of word meaning and current practice in the Ô¨Åelds of WSD and WSI. Computational approaches to determining word meaning in context have traditionally relied on a Ô¨Åxed sense inventory produced by humans or by a WSI system that groups token instances into hard clusters. Either sense inventory can then be applied to tag sentences on the premise that there will be one best-Ô¨Åtting sense for each token instance. However, word meanings do not always take the form of discrete senses but vary on a continuum between clear-cut ambiguity and vagueness (Tuggy 1993). For example, the noun crane is a clear-cut case of ambiguity between lifting device and bird, whereas the exact meaning of the noun thing can only be retrieved via the context of use rather than via a representation in the mental lexicon of speakers. Cases of polysemy such as the verb paint, which can mean painting a picture, decorating a room, or painting a mural on a house, lie somewhere between these two poles. Tuggy highlights the fact that boundaries between these different categories are blurred. Although speciÔ¨Åc context clearly plays a role (Copestake and Briscoe 1995; Passonneau et al. 2010) some lemmas are inherently much harder to partition than others (Kilgarriff 1998; Cruse 2000). There are recent attempts to address some of these issues by using alternative characterizations of word meaning that do not involve creating a partition of usages into senses (McCarthy and Navigli 2009; Erk, McCarthy, and Gaylord 2013), and by asking WSI systems to produce soft or graded clusterings (Jurgens and Klapaftis 2013) where tokens can belong to a mixture of the clusters. However, these approaches do not overtly consider the location of a lemma on the continuum, but doing so should help in determining an appropriate representation. Whereas the broad senses of the noun crane could easily be represented by a hard clustering, this would not make any sense for the noun thing; meanwhile, the verb paint might beneÔ¨Åt from a more graded representation. In this article, we propose the notion of partitionability of a lemma, that is, the ease with which usages can be grouped into senses. We exploit data from annotation studies to explore the partitionability of different lemmas and see where on the ambiguity‚Äì vagueness cline a lemma is. This should be useful in helping to determine the appropriate computational representation for a word‚Äôs meanings‚Äîfor example, whether a hard clustering will sufÔ¨Åce, whether a soft clustering would be more appropriate, or whether a clustering representation does not make sense. To our knowledge, there has been no study on detecting partitionability of word senses. We operationalize partitionability as clusterability, a measure of how much structure there is in the data and therefore how easy it is to cluster (Ackerman and Ben-David 2009a), and test to what extent clusterability can predict partitionability. For deriving a gold estimate of partitionability, we turn to the Usage Similarity (hereafter Usim) data set (Erk, McCarthy, and Gaylord 2009), for which annotators have rated the similarity of  
Our experiments for Indonesian/Malay‚ÄìEnglish translation show that using the large adapted resource-rich bitext yields 7.26 BLEU points of improvement over the unadapted one and 3.09 BLEU points over the original small bitext. Moreover, combining the small POOR‚ÄìTGT bitext with the adapted bitext outperforms the corresponding combinations with the unadapted bitext by 1.93‚Äì3.25 BLEU points. We also demonstrate the applicability of our approaches to other languages and domains. 1. Introduction Contemporary statistical machine translation (SMT) systems learn how to translate from large sentence-aligned bilingual corpora of human-generated translations, called ‚àó 2225 East Bayshore Road, Suite 200, Palo Alto, CA 94303. E-mail: pwang@machinezone.com. The work reported in this article was part of the Ô¨Årst author‚Äôs Ph.D. thesis research in the Department of Computer Science, National University of Singapore. ‚àó‚àó Tornado Tower, Ô¨Çoor 10, P.O. 5825, Doha, Qatar. E-mail: pnakov@qf.org.qa. ‚Ä† 13 Computing Drive, Singapore 117417. E-mail: nght@comp.nus.edu.sg. Submission received: 23 May 2015; revised version received: 10 January 2016; accepted for publication: 15 February 2016. doi:10.1162/COLI a 00248 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 2  bitexts. Unfortunately, collecting sufÔ¨Åciently large, high-quality bitexts is difÔ¨Åcult, and thus most of the 6,500+ world languages are resource-poor for SMT. Fortunately, many of these resource-poor languages are related to some resource-rich language, with whom they overlap in vocabulary and share cognates, which offers opportunities for bitext reuse. Example pairs of such resource rich‚Äìpoor languages include Spanish‚ÄìCatalan, Finnish‚ÄìEstonian, Swedish‚ÄìNorwegian, Russian‚ÄìUkrainian, Irish‚ÄìGaelic Scottish, Standard German‚ÄìSwiss German, Modern Standard Arabic‚ÄìDialectical Arabic (e.g., Gulf, Egyptian), Turkish‚ÄìAzerbaijani, and so on. Previous work has already demonstrated the beneÔ¨Åts of using a bitext for a related resource-rich language to X (e.g., X = English) to improve machine translation from a resource-poor language to X (Nakov and Ng 2009, 2012). Here we take a different, orthogonal approach: We adapt the resource-rich language to get closer to the resourcepoor one. We assume two bitexts: (1) a small bitext for a resource-poor source language S1 and some target language T, and (2) a large bitext for a related resource-rich source language S2 and the same target language T. We use these bitexts to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the resource-poor and resource-rich languages, S1 and S2. We propose three approaches to adapt (the source side of) the large bitext for S2‚ÄìT: word-level paraphrasing, phraselevel paraphrasing, and text rewriting using a specialized decoder. The Ô¨Årst two approaches were proposed in our previous work (Wang, Nakov, and Ng 2012), and the third approach is novel and outperforms the other two in our experiments. Training on the adapted large bitext S2‚ÄìT yields very signiÔ¨Åcant improvements in translation quality compared with both training on the unadapted large bitext S2‚ÄìT, and training on the small bitext for the resource-poor language S1‚ÄìT. We further achieve very sizable improvements when combining the small bitext S1‚ÄìT with the large adapted bitext S2‚ÄìT, compared with combining the former with the unadapted bitext S2‚ÄìT. Although here we focus on adapting Malay to look like Indonesian, we also demonstrate the applicability of our approach to another language pair, Bulgarian‚Äì Macedonian, which is also from a different domain. The remainder of this article is organized as follows. Section 2 presents an overview of related work. Section 3 introduces our target resource rich‚Äìpoor language pair: Malay‚ÄìIndonesian. Then, Section 4 presents our three approaches for source language adaptation. Section 5 describes the experimental set-up, after which we present the experimental results and discussions in Section 6. Section 7 contains deeper analysis of the obtained results. Finally, Section 8 concludes and points to possible directions for future work. 2. Related Work One relevant line of research is on machine translation between closely related languages, which is arguably simpler than general SMT, and thus can be handled using word-for-word translation, manual language-speciÔ¨Åc rules that take care of the necessary morphological and syntactic transformations, or character-level translation/ transliteration. This has been tried for a number of language pairs including Czech‚Äì Slovak (HajicÀá, Hric, and KubonÀá 2000), Turkish‚ÄìCrimean Tatar (Altintas and Cicekli 2002), Irish‚ÄìScottish Gaelic (Scannell 2006), and Macedonian‚ÄìBulgarian (Nakov and Tiedemann 2012). In contrast, we have a different objective: We do not carry out full  278  Wang, Nakov, and Ng  Source Language Adaptation for Resource-Poor MT  translation but rather adaptation (since our ultimate goal is to translate into a third language X). A special case of this same line of research is the translation between dialects of the same language, for example, between Cantonese and Mandarin (Zhang 1998), or between a dialect of a language and a standard version of that language, for example, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr, Shaalan, and Ziedan 2008; Sawaf 2010; Salloum and Habash 2011; Sajjad, Darwish, and Belinkov 2013). Here again, manual rules and/or language-speciÔ¨Åc tools and resources are typically used. In the case of Arabic dialects, a further complication arises due to the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online media such as social networks, chats, forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. Sajjad, Darwish, and Belinkov (2013) Ô¨Årst normalized a dialectal Egyptian Arabic to look like Modern Standard Arabic, and then translated the transformed text to English. In fact, this is a more general problem, which arises with informal sources such as SMS messages and Tweets for just any language (Aw et al. 2006; Han and Baldwin 2011; Wang and Ng 2013; Bojja, Nedunchezhian, and Wang 2015). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language to another. A second relevant line of research is on language adaptation and normalization, when done speciÔ¨Åcally for improving SMT into another language. For example, Marujo et al. (2011) described a rule-based system for adapting Brazilian Portuguese (BP) to European Portuguese (EP), which they used to adapt BP‚ÄìEnglish bitexts to EP‚ÄìEnglish. They report small improvements in BLEU for EP‚ÄìEnglish translation when training on the adapted ‚ÄúEP‚Äù‚ÄìEnglish bitext compared with using the unadapted BP‚ÄìEnglish (38.55 vs. 38.29 BLEU points), or when an EP‚ÄìEnglish bitext is used in addition to the adapted/unadapted one (41.07 vs. 40.91 BLEU points). Unlike that work, which heavily relied on language-speciÔ¨Åc rules, our approach is statistical, and largely languageindependent; moreover, our improvements are much more sizable. A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages. For example, our previous work (Nakov and Ng 2009, 2012) experimented with various techniques for combining a small bitext for a resource-poor language (Indonesian or Spanish) with a much larger bitext for a related resource-rich language (Malay or Portuguese), pretending that Spanish is resource-poor; the target language of all bitexts was English. However, that work did not attempt language adaptation, except for very simple transliteration for Portuguese‚ÄìSpanish that ignored context entirely; because it does not substitute a word with a completely different word, transliteration did not help much for Malay‚ÄìIndonesian, which use uniÔ¨Åed spelling. Still, once we have language-adapted the large bitext, it makes sense to try to combine it further with the small bitext; thus, in the following we will directly compare and combine these two approaches. One alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Cohn and Lapata 2007; Utiyama and Isahara 2007; Wu and Wang 2009). Unfortunately, using the resource-rich language as a pivot (poor‚Üírich‚ÜíX) would require an additional parallel poor‚Äìrich bitext, which we do not have. Pivoting 279  Computational Linguistics  Volume 42, Number 2  over the target X (rich‚ÜíX‚Üípoor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the Ô¨Årst step would ask an SMT system to translate its own training data (we only have one rich‚ÄìX bitext). Yet another alternative approach for improving resource-poor MT is to mine translation bitexts from comparable corpora (Munteanu, Fraser, and Marcu 2004; Snover, Dorr, and Schwartz 2008). This is orthogonal to our efforts here, as our focus is on adapting resources for a related resource-rich language, rather than directly mining source‚Äìtarget translation pairs from comparable corpora. 3. Malay and Indonesian Malay and Indonesian are closely related, mutually intelligible Austronesian languages with 180 million speakers combined. They have a uniÔ¨Åed spelling, with occasional differences, for example, kerana vs. karena (‚Äòbecause‚Äô), Inggeris vs. Inggris (‚ÄòEnglish‚Äô), and wang vs. uang (‚Äòmoney‚Äô). They differ more substantially in vocabulary, mostly because of loan words, where Malay typically follows the English pronunciation, whereas Indonesian tends to follow Dutch, for example, televisyen vs. televisi, Julai vs. Juli, and Jordan vs. Yordania. Although there are many cognates between the two languages, there are also many false friends, for example, polisi means policy in Malay but police in Indonesian. There are also many partial cognates, for example, nanti means both will (future tense marker) and later in Malay but only later in Indonesian. Thus, Ô¨Çuent Malay and Ô¨Çuent Indonesian can differ substantially. Consider, for example, Article 1 of the Universal Declaration of Human Rights:1 r Semua manusia dilahirkan bebas dan samarata dari segi kemuliaan dan hak-hak. Mereka mempunyai pemikiran dan perasaan hati dan hendaklah bertindak di antara satu sama lain dengan semangat persaudaraan. (Malay) r Semua orang dilahirkan merdeka dan mempunyai martabat dan hak-hak yang sama. Mereka dikaruniai akal dan hati nurani dan hendaknya bergaul satu sama lain dalam semangat persaudaraan. (Indonesian) r All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. (English) There is only 50% overlap at the word level, but the actual vocabulary overlap is much higher‚Äîfor example, there is only one word in the Malay text that does not exist in Indonesian: samarata (‚Äòequal‚Äô). Other differences are due to the use of different morphological forms, for example, hendaklah vs. hendaknya (‚Äòconscience‚Äô), derivational variants of hendak (‚Äòwant‚Äô). To quantify the similarity between some pairs of languages, we calculated the cosine similarity between them based on the Universal Declaration of Human Rights.2 The results are shown in Table 1. We can see that the average similarity between English 
Carnegie Mellon University Isabel Trancoso¬ß Instituto Superior Te¬¥cnico University of Lisbon INESC-ID Microblogs such as Twitter, Facebook, and Sina Weibo (China‚Äôs equivalent of Twitter) are a remarkable linguistic resource. In contrast to content from edited genres such as newswire, microblogs contain discussions of virtually every topic by numerous individuals in different languages and dialects and in different styles. In this work, we show that some microblog users post ‚Äúself-translated‚Äù messages targeting audiences who speak different languages, either by writing the same message in multiple languages or by retweeting translations of their original posts in a second language. We introduce a method for Ô¨Ånding and extracting this naturally occurring parallel data. Identifying the parallel content requires solving an alignment problem, and we give an optimally efÔ¨Åcient dynamic programming algorithm for this. Using our method, we extract nearly 3M Chinese‚ÄìEnglish parallel segments from Sina Weibo using a targeted crawl of Weibo users who post in multiple languages. Additionally, from a random sample of Twitter, we obtain substantial amounts of parallel data in multiple language pairs. Evaluation is performed by assessing the accuracy of our extraction approach relative to a manual annotation as well as ‚àó Google DeepMind, London, N1 0AE. E-mail: lingwang@google.com. ‚àó‚àó Feedzai Research, Lisbon, 1990-095. E-mail: luis.marujo@feedzai.com. ‚Ä† School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: cdyer@cs.cmu.edu. ‚Ä° School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: awb@cs.cmu.edu. ¬ß Instituto Superior Te¬¥cnico, Lisbon, 1000-029. E-mail: isabel.trancoso@inesc-id.pt. Submission received: 10 July 2014; accepted for publication: 25 January 2016. doi:10.1162/COLI a 00249 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 2  in terms of utility as training data for a Chinese‚ÄìEnglish machine translation system. Relative to traditional parallel data resources, the automatically extracted parallel data yield substantial translation quality improvements in translating microblog text and modest improvements in translating edited news content.  1. Introduction  In the span of about two decades, the Web has evolved from a collection of mostly static  Web pages created, to dynamic, interactive content, and to content created by users  themselves. The advent of microblogs, such as Facebook and Twitter, has particularly  transformed the kind of information that is published, since traditional barriers to  publication (e.g., expertise in Web development) have been virtually eliminated by  making publication easy for anyone with Web access. Microblog posts are created in  the service of a variety of communicative and identity-building goals (Marwick and  Boyd 2010), and the diversity of users and their intentions is reÔ¨Çected in an equally  diverse array of writing styles and linguistic conventions. In contrast to traditional  print media, user-generated content on social media can be informal, colloquial, and  is in particular marked by innovative and varied use of orthography. For example, we  can readily Ô¨Ånd tweets like (R U still with me or what?) and nonstandard abbreviations  (idk! smh).  Automated language processing tools (e.g., those that perform linguistic analysis or  translation) face particular difÔ¨Åculty with this new kind of content. On one hand, these  have been developed with the conventions of more edited genres in mind. For example,  they often make strong assumptions about orthographic and lexical uniformity (e.g.,  that there is just one way to spell you, and that cool, cooool, and cooooool represent  completely unrelated lexical items). While modeling innovations are helping to relax  these assumptions (Han and Baldwin 2011; Ritter et al. 2012; Owoputi et al. 2013; Ling  et al. 2015), a second serious challenge is that many of the annotated text resources  that our tools are learned from are drawn from edited genres, and poor generalization  from edited to user-generated genres is a major source of errors (Gimpel et al. 2011;  Kong et al. 2014).  In this work, we present methods for Ô¨Ånding naturally occurring parallel data on  social media sites that is suitable for training machine translation (MT) systems. In MT,  the domain mismatch problem is quite acute because most existing sources of parallel  data are governmental, religious, or commercial, which are quite different from user-  generated content, both in language use and in topic. The extracted parallel data can  then be used to create systems designed to translate user-generated content. Addition-  ally, because microblogs host discussions of virtually limitless topics, they are also a  potential source of information about how to translate names and words associated  with breaking events, and, as such, may be useful for translation of texts from more  traditional domains. Apart from machine translation, parallel data in this domain can  improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang,  and Dredze 2014; Wang et al. 2014).  Our method is inspired by the (perhaps surprising) observation that a reason-  able number of microblog users tweet ‚Äúin parallel‚Äù in two or more languages. For  instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo  that regularly posts English‚ÄìChinese parallel messages, for example, watup Kenny  Mayne!! - Kenny Mayne  !!, where an English message and its Chinese  translation are in the same post, separated by a dash. It is not only celebrities (or  308  Ling et al.  Mining Parallel Corpora from Sina Weibo and Twitter  their publicists) who tweet in multiple languages; we see this with casual users as  well. For example, on Twitter, we found  | I don‚Äôt like this Ehomaki!!:  http://t.co/9FTXgV0w, which contains a Japanese to English translation, separated by  | and followed by an image link, which is displayed as an image when viewed on  the Twitter Web site. Other examples of parallel posts are shown in Figure 1. We  note that these parallel posts contain information that is valuable, since they contain  elements that are rare in standard edited genres. In the examples we have given  here, the terms watup and Ehomaki are not correctly or not translated by online MT  systems.  We offer brief speculation on the reasons that users translate their own posts,  although later in the article we will provide an analysis of the content that provides  further insight into the question. One class of parallel posts are found in public celebri-  ties‚Äô proÔ¨Åles (e.g., Snoop Dogg‚Äôs Sina Weibo posts), done to enhance their popularity in  non-English markets. Another class is posted by people who live or have lived abroad  and wish to communicate with people who speak different languages. In fact, our  Ô¨Årst contact with such posts was with the parallel messages that Chinese students at  Carnegie Mellon University were posting on Facebook so their friends both in China  and in Pittsburgh could read them.  Extracting parallel data from such posts poses many challenges to current NLP  and MT methods. As part of the elaboration of this work, we made the following  contributions to the Ô¨Åeld of NLP and MT.  Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309  Computational Linguistics  Volume 42, Number 2  r A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. r EfÔ¨Åcient multilingual document detection - Parallel tweets are a relative rarity, but because of the volumes of data involved, we can Ô¨Ånd a large amount of content. We must efÔ¨Åciently detect multilingual tweets, as monolingual tweets do not contain translations. Although language identiÔ¨Åcation is a well-studied problem (Zissman 1996; Gottron and Lipka 2010; Greenhill 2011), even in the microblog domain (Bergsma et al. 2012), these assume that only one language is present within a document and cannot be directly applied to this problem. Furthermore, because of the magnitude of tweets that must be processed, many of the proposed solutions cannot be applied due to their computational complexity. In our work, we propose an efÔ¨Åcient implementation for large-scale detection of multilingual documents. r Crowdsourcing for parallel data extraction - To tune the parameters of our parallel data extractor and perform MT experiments, user-veriÔ¨Åed annotations must be obtained. To obtain this, we propose a simple crowdsourcing method for extracting parallel data from tweets. At a high level, our algorithm proceeds in three steps. First, we identify candidate multilingual tweets using a fast heuristic classiÔ¨Åer. Second, we extract the candidate parallel segments from each tweet using a dynamic programming algorithm. Third, a classiÔ¨Åer determines if the proposed parallel segment is correct. If so, the material is extracted, otherwise it is discarded. This research is an extension of the preliminary work described in Ling et al. (2013), in which we obtained over 1 million Chinese‚ÄìEnglish parallel segments from Sina Weibo, using only their public application program interface (API). This automatically extracted parallel data yielded substantial translation quality improvements in translating microblog text and modest improvements in translating edited news. Following this work, we developed a method for crowdsourcing judgments about parallel segments (Ling et al. 2014), which was then used to build gold standard data for other language pairs and for the Twitter domain. This article extends these two papers in several ways: r Improved language pair detection - The previous work assumes that the language pair is formed by two languages with different unicode ranges, such as English‚ÄìChinese, and does not support the extraction of parallel data if the languages share the same unicode range (such as English‚ÄìPortuguese). This issue is addressed in this article, where we present a novel approach for Ô¨Ånding multilingual tweets.  310  Ling et al.  Mining Parallel Corpora from Sina Weibo and Twitter  r More language pairs considered - The previous architecture only allowed one language pair to be considered during extraction. Thus, for instance, only English‚ÄìChinese parallel sentences were extracted from Sina Weibo, and English‚ÄìArabic sentence pairs from Twitter. In this work, we present a general architecture that allows multiple language pairs to be considered simultaneously. Using this architecture, we extracted data for nine language pairs from Twitter.1 r MT experiments with more data - As we are actively collecting more data, our resulting parallel data sets are naturally larger. In fact, our current data set amounts to nearly 3 million sentence pairs for the English‚ÄìChinese language pair alone. Furthermore, we perform cross-domain experiments (training using Weibo data and testing on Twitter data) and consider other language pairs. The article is organized as follows. Section 2 describes the related work in parallel data extraction. Section 3 presents our model to align segments within one single document and the metrics used to evaluate the quality of the segments. Section 4 describes the extraction pipeline used to extract the parallel data from Sina Weibo and Twitter. Section 5 describes the method used to obtain gold standards for translation in different languages. We then present, in Section 7, the experiments showing that our harvested data not only substantially improve translations of microblog text with existing (and arguably inappropriate) translation models, but that they improve the translation of more traditional MT genres, such as newswire. We conclude in Section 8. 2. Related Work The automatic identiÔ¨Åcation and retrieval of translated text (bitext) is a well-studied problem in natural language processing. In particular, there has been a long tradition of exploiting the multilingual Web as a source of bitext (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012). In general, existing approaches extract bitext in two steps. In the Ô¨Årst step, candidate pairs of Web sites that are likely to be parallel are retrieved from a set of documents using an efÔ¨Åcient retrieval mechanism. This step is necessary to narrow down the potential candidates for parallel Web sites, since considering all possible pairs in a given set of Web pages is intractable (a direct comparison of O(n2) documents would be impractical for any modestly sized collection). Resnik and Smith (2003) used features from URLs to build this set of pairs, by checking for Web sites with patterns that may indicate the presence of a parallel Web site. For instance, the pattern lang=en generally indicates that by changing en to another language, such as pt, we may Ô¨Ånd another Web site, which is the translation of the original one. However, this method has a low recall, because it ignores the content within the Web document. Uszkoreit et al. (2010) proposed a content-based approach to improve retrieval that executes this Ô¨Årst step by translating all documents into English using a baseline translation system, and then Ô¨Ånds rare (i.e., informative) n-grams that are common across Web documents with different original languages using an inverted indexing approach.  
1. Introduction  Dimensions of a word vector in distributional semantic models contain a function  of the co-occurrence counts of the word with contexts of interest. A popular and  effective option (Bullinaria and Levy 2012) is to transform counts into pointwise  mutual information (PMI) scores, which are given, for any word a and context c, by  PMI(a,  c)  =  log(  P(a|c) P(a)  ).1  There  are  various  proposals  on  deriving  phrase  representations  ‚àó Center for Mind/Brain Sciences, University of Trento, Palazzo Fedrigotti, corso Bettini 31, 38068 Rovereto  (TN), Italy. E-mails: denis.paperno@unitn.it; marco.baroni@unitn.it.  
‚àó 8916-5 Takayama-cho, Ikoma, Nara, Japan. E-mail: neubig@is.naist.jp. ‚àó‚àó 6-10-1 Roppongi, Minato-ku, Tokyo, Japan. E-mail: tarow@google.com. This work was mostly done while the second author was afÔ¨Åliated with the National Institute of Information and Communications Technology, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan. Submission received: 3 June 2014; revised version received: 18 March 2015; accepted for publication: 11 October 2015. doi:10.1162/COLI a 00241 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 1  Within the SMT framework, there have been two revolutions in the way we mathematically model the translation process. The Ô¨Årst was the pioneering work of Brown et al. (1993), who proposed the idea of SMT, and described methods for estimation of the parameters used in translation. In that work, the parameters of a word-based generative translation model were optimized to maximize the conditional likelihood of the training corpus. The second major advance in SMT is the discriminative training framework proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus. By describing the scoring function for MT as a Ô¨Çexibly parameterizable loglinear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing (Collins 2002). However, within the general framework of structured prediction, MT stands apart in many ways, and as a result requires a number of unique design decisions not necessary in other frameworks (as summarized in Table 1). The Ô¨Årst is the search space that must be considered. The search space in MT is generally too large to expand exhaustively, so it is necessary to decide which subset of all the possible hypotheses should be used in optimization. In addition, the evaluation of MT accuracy is not straightforward, with automatic evaluation measures for MT still being researched to this day. From the optimization perspective, even once we have chosen an automatic evaluation metric, it is not necessarily the case that it can be decomposed for straightforward integration with structured learning algorithms. Given this evaluation measure, it is necessary to incorporate it into a loss function to target. The loss function should be closely related to the Ô¨Ånal evaluation objective, while allowing for the use of efÔ¨Åcient optimization algorithms. Finally, it is necessary to choose an optimization algorithm. In many cases it is possible to choose a standard algorithm from other Ô¨Åelds, but there are also algorithms that have been tailored towards the unique challenges posed by MT.  Table 1 A road map of the various elements that affect MT optimization.  Which Loss Functions? Error (¬ß3.1) Softmax (¬ß3.2) Risk (¬ß3.3) Margin, Perceptron (¬ß3.4) Ranking (¬ß3.5) Minimum Squared Error (¬ß3.6)  Which Optimization Algorithm? Minimum Error Rate Training (¬ß5.1) Gradient-based Methods (¬ß5.2, ¬ß6.5) Margin-based Methods (¬ß5.3) Linear Regression (¬ß5.4) Perceptron (¬ß6.2) MIRA (¬ß6.3) AROW (¬ß6.4)  Which Evaluation Measure? Corpus-level, Sentence Level (¬ß2.5) BLEU and Approximations (¬ß2.5.1, ¬ß2.5.2) Other Measures (¬ß8.3)  Which Hypotheses to Target? k-best vs. Lattice vs. Forest (¬ß2.4) Merged k-bests (¬ß5) Forced Decoding (¬ß2.4), Oracles (¬ß4)  Other Topics: Large Data Sets (¬ß7), Non-linear Models (¬ß8.1), Domain Adaptation (¬ß8.2), Search and Optimization (¬ß8.4)  2  Neubig and Watanabe  Optimization for Statistical Machine Translation  In this article, we survey the state of the art in machine translation optimization in a comprehensive and systematic fashion, covering a wide variety of topics, with a uniÔ¨Åed set of terminology. In Section 2, we Ô¨Årst provide deÔ¨Ånitions of the problem of machine translation, describe brieÔ¨Çy how models are built, how features are deÔ¨Åned, and how translations are evaluated, and Ô¨Ånally deÔ¨Åne the optimization setting. In Section 3, we next describe a variety of loss functions that have been targeted in machine translation optimization. In Section 4, we explain the selection of oracle translations, a non-trivial process that directly affects the optimization results. In Section 5, we describe batch optimization algorithms, starting with the popular minimum error rate training, and continuing with other approaches using likelihood, margin, rank loss, or risk as objectives. In Section 6, we describe online learning algorithms, Ô¨Årst explaining the relationship between corpus-level optimization and sentence-level optimization, and then moving on to algorithms based on perceptron, margin, or likelihood-based objectives. In Section 7, we describe the recent advances in scaling training of MT systems up to large amounts of data through parallel computing, and in Section 8, we cover a number of other topics in MT optimization such as non-linear models, domain adaptation, and the relationship between MT evaluation and optimization. Finally, we conclude in Section 9, overviewing the methods described, making a brief note about which methods see the most use in actual systems, and outlining some of the unsolved problems in the optimization of MT systems. 2. Machine Translation Preliminaries and DeÔ¨Ånitions Before delving into the details of actual optimization algorithms, we Ô¨Årst introduce preliminaries and deÔ¨Ånitions regarding MT in general and the MT optimization problem in particular. We focus mainly on the aspects of MT that are relevant to optimization, and readers may refer to Koehn (2010) or Lopez (2008) for more details about MT in general. 2.1 Machine Translation Machine translation is the problem of automatically translating from one natural language to another. Formally, we deÔ¨Åne this problem by specifying F to be the collection of all source sentences to be translated, f ‚àà F as one of the sentences, and E ( f ) as the collection of all possible target language sentences that can be obtained by translating f . Machine translation systems perform this translation process by dividing the translation of a full sentence into the translation and recombination of smaller parts, which are represented as hidden variables, which together form a derivation. For example, in phrase-based translation (Koehn, Och, and Marcu 2003), the hidden variables will be the alignment between the phrases of the source and target sentences, and in tree-based translation models (Yamada and Knight 2001; Chiang 2007), the hidden variables will represent the latent tree structure used to generate the translation. We will deÔ¨Åne D( f ) to be the space of possible derivations that can be acquired from source sentence f , and d ‚àà D( f ) to be one of those derivations. Any particular derivation d will correspond to exactly one e ‚àà E ( f ), although the opposite is not true (the derivation uniquely determines the translation, but there can be multiple derivations corresponding to a particular translation). We also deÔ¨Åne tuple e, d consisting of a target sentence and its corresponding derivation, and T ( f ) ‚äÜ E ( f ) √ó D( f ) as the set of all of these tuples. Because the set of all possible translations E ( f ) will contain both good and bad translations, it is necessary to have a method to identify and output the good 3  Computational Linguistics  Volume 42, Number 1  translations. In order to do so, in machine translation it is common to deÔ¨Åne a linear model that determines the score of each translation candidate. In this linear model we Ô¨Årst deÔ¨Åne an M-dimensional feature vector for each output and its derivation as h( f , e, d) : F √ó E √ó D ‚Üí RM. For each feature, we also deÔ¨Åne a corresponding weight, resulting in an M-dimensional weight vector w ‚àà RM. Based on these feature and weight vectors, we proceed to deÔ¨Åne the problem of selecting the best e, d as the following maximization problem  eÀÜ, dÀÜ = arg max w h( f , e, d)  (1)  e,d ‚ààT ( f )  where the dot product of the parameters and features is equivalent to the score assigned to a particular translation. The optimization problem that we will be surveying in this article is generally concerned with Ô¨Ånding the most effective weight vector w from the set of possible weight vectors RM.1 Optimization is also widely called tuning in the SMT literature. In addition, because of the exponentially large number of possible translations in E ( f ) that must be considered, it is necessary to take advantage of the problem structure, making MT optimization an instance of structured learning.  2.2 Model Construction  The Ô¨Årst step of creating a machine translation system is model construction, in which translation models (TMs) are extracted from a large parallel corpus. The TM is usually created by Ô¨Årst aligning the parallel text (Och and Ney 2003), using this text to extract multi-word phrase pairs or synchronous grammar rules (Koehn, Och, and Marcu 2003; Chiang 2007), and scoring these rules according to several features explained in more detail in Section 2.3. The construction of the TM is generally performed Ô¨Årst in a manner that does not directly consider the optimization of translation accuracy, followed by an optimization step that explicitly considers the accuracy achieved by the system.2 In this survey, we focus on the optimization step, and thus do not cover elements of model construction that do not directly optimize an objective function related to translation accuracy, but interested readers can reference Koehn (2010) for more details. In the context of this article, however, the TM is particularly important in the role it plays in deÔ¨Åning our derivation space D( f ). For example, in the case of phrase-based translation, only phrase pairs included in the TM will be expanded during the process of searching for the best translation (explained in Section 2.4). This has major implications from the point of view of optimization, the most important of which being that we must use separate data for training the TM and optimizing the parameters w. The reason for this lies in the fact that the TM is constructed in such a way that allows it to ‚Äúmemorize‚Äù long multi-word phrases included in the training data. Using the same data to train the model parameters will result in overÔ¨Åtting, learning parameters that heavily favor using these memorized multi-word phrases, which will not be present in a separate test set.  
The fundamental problem we address in this article was formulated by Joshi (1985) as the following question: How much context-sensitivity is required to provide reasonable structural descriptions? His answer to this question was the extended domain of locality principle of Tree Adjoining Grammars, which allows us to represent in a single structure (an elementary tree1) a predicate and its arguments. ‚àó Computer Engineering Department, Faculty of Engineering, University of Guilan, Rasht, Iran. E-mail: mirroshandel@guilan.ac.ir. ‚àó‚àó Laboratoire d‚ÄôInformatique Fondamentale, CNRS - Universite¬¥ Aix-Marseille, France. E-mail: alexis.nasr@univ-amu.fr. 
Aalto University Mikko Kurimo‚àó Aalto University Sami Virpioja‚àó‚àó Aalto University This article presents a comparative study of a subÔ¨Åeld of morphology learning referred to as minimally supervised morphological segmentation. In morphological segmentation, word forms are segmented into morphs, the surface forms of morphemes. In the minimally supervised data-driven learning setting, segmentation models are learned from a small number of manually annotated word forms and a large set of unannotated word forms. In addition to providing a literature survey on published methods, we present an in-depth empirical comparison on three diverse model families, including a detailed error analysis. Based on the literature survey, we conclude that the existing methodology contains substantial work on generative morph lexicon-based approaches and methods based on discriminative boundary detection. As for which approach has been more successful, both the previous work and the empirical evaluation presented here strongly imply that the current state of the art is yielded by the discriminative boundary detection methodology. ‚àó Department of Signal Processing and Acoustics, Otakaari 5 A, FI-02150 Espoo, Finland. E-mail: {teemu.ruokolainen, stig-arne.gronroos, mikko.kurimo}@aalto.Ô¨Å. ‚àó‚àó Department of Information and Computer Science, Konemiehentie 2, FI-02150 Espoo, Finland. E-mail: {oskar.kohonen, sami.virpioja}@aalto.Ô¨Å. ‚Ä† Institute of Cybernetics, Akadeemia tee 21 EE-12618 Tallin, Estonia. E-mail: sirts@phon.ioc.ee. Submission received: 15 September 2014; revised version received: 7 October 2015; accepted for publication: 15 November 2015 doi:10.1162/COLI_a_00243 ¬© 2016 Association for Computational Linguistics  Computational Linguistics  Volume 42, Number 1  1. Introduction This article discusses a subÔ¨Åeld of morphology learning referred to as morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. For example, consider the English word houses with a corresponding segmentation house+s, where the segment house corresponds to the word stem and the sufÔ¨Åx -s marks the plural number. Although this is a major simpliÔ¨Åcation of the diverse morphological phenomena present in languages, this type of analysis has nevertheless been of substantial interest to computational linguistics, beginning with the pioneering work on morphological learning by Harris (1955). As for automatic language processing, such segmentations have been found useful in a wide range of applications, including speech recognition (Hirsim√§ki et al. 2006; Narasimhan et al. 2014), information retrieval (Turunen and Kurimo 2011), machine translation (de Gispert et al. 2009; Green and DeNero 2012), and word representation learning (Luong, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufÔ¨Åciently large set of unannotated words in electronic form. Consequently, such methods provide an inexpensive means of acquiring a type of morphological analysis for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning setting has received further popularity because of its close relationship with the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards semisupervised learning (Poon, Cherry, and Toutanova 2009; Kohonen, Virpioja, and Lagus 2010; Sirts and Goldwater 2013; Gr√∂nroos et al. 2014; Ruokolainen et al. 2014). In general, the aim of semi-supervised learning is to acquire high-performing models utilizing both unannotated as well as annotated data (Zhu and Goldberg 2009). In morphological segmentation, the annotated data sets are commonly small, on the order of a few hundred word forms. We refer to this learning setting with such a small amount of supervision as minimally supervised learning. In consequence, similar to the unsupervised methods, the minimally supervised techniques can be seen as a means of acquiring a type of morphological analysis for under-resourced languages. Individual articles describing novel methods typically contain a comparative discussion and empirical evaluation between one or two preceding approaches. Therefore, what is currently lacking from the literature is a summarizing comparative study on the published methodology as a whole. Moreover, the literature currently lacks discussion on error analysis. A study on the error patterns produced by varying approaches could inform us about their potential utility in different tasks. For example, if an application requires high-accuracy compound splitting, one could choose to apply a model with a good compound-splitting capability even if its afÔ¨Åx accuracy does not reach state of the art. The purpose of this work is to address these issues. Our main contributions are as follows. First, we present a literature survey on morphological segmentation methods applicable in the minimally supervised learning setting. The considered methods include unsupervised techniques that learn solely from  92  Ruokolainen et al.  Minimally Supervised Morphological Segmentation  unannotated data, supervised methods that utilize solely annotated data, and semisupervised approaches that utilize both unannotated and annotated data. Second, we perform an extensive empirical evaluation of three diverse method families, including a detailed error analysis. The approaches considered in this comparison are variants of the Morfessor algorithm (Creutz and Lagus 2002, 2005, 2007; Kohonen, Virpioja, and Lagus 2010; Gr√∂nroos et al. 2014), the adaptor grammar framework (Sirts and Goldwater 2013), and the conditional random Ô¨Åeld method (Ruokolainen et al. 2013, 2014). We hope the presented discussion and empirical evaluation will be of help for future research on the considered task. The rest of the article is organized as follows. In Section 2, we provide an overview of related studies. We then provide a literature survey of published morphological segmentation methodology in Section 3. Experimental work is presented in Section 4. Finally, we provide a discussion on potential directions for future work and conclusions on the current work in Sections 5 and 6, respectively. 2. Related Work Hammarstr√∂m and Borin (2011) presented a literature survey on unsupervised learning of morphology, including methods for learning morphological segmentation. Whereas the discussion provided by Hammarstr√∂m and Borin focuses mainly on linguistic aspects of morphology learning, our work is strongly rooted in machine learning methodology and empirical evaluation. In addition, whereas Hammarstr√∂m and Borin focus entirely on unsupervised learning, our work considers a broader range of learning paradigms. Therefore, although related, Hammarstr√∂m and Borin and our current presentation are complementary in that they have different focus areas. In addition to the work of Hammarstr√∂m and Borin (2011), we note that there exists some established forums on morphology learning. First, we mention the ACL Special Interest Group on Computational Morphology and Phonology (SIGMORPHON), which has regularly organized workshops on the subject since 2002. As for speciÔ¨Åcally morphology learning, we refer to the Morpho Challenge competitions organized since 2005 at Aalto University (formerly known as Helsinki University of Technology). Although these events have been successful in providing a publication and discussion venue for researchers interested in the topic, they have not given birth to comparative studies or survey literature. For example, whereas the publications on the Morpho Challenge (Kurimo et al. 2009; Kurimo, Virpioja, and Turunen 2010) discuss the competition results, they nevertheless do not attempt to provide any insight on the fundamental differences and similarities of the participating methods. 3. Methods This section provides a detailed review of our methodology. We begin by describing varying morphological representations, including segmentation, and the minimally supervised learning setting in Sections 3.1 and 3.2, respectively. We then provide a literature survey and comparative discussion on a range of methods in Section 3.3. 3.1 On Learning Morphological Representations In what follows, we brieÔ¨Çy characterize morphological segmentation with respect to alternative morphological representations, particularly the full morphological analysis. To this end, consider the exemplar segmentations and full analyses for Finnish 93  Computational Linguistics  Volume 42, Number 1  Table 1 Morphological segmentation versus full morphological analysis for exemplar Finnish word forms. The full analysis consists of word lemma (basic form), part-of-speech, and Ô¨Åne-grained labels.  word form  full analysis  segmentation  auto (car) autossa (in car) autoilta (from cars) autoilta (car evening) maantie (highway) s√§hk√∂auto (electric car)  auto+N+Sg+Nom auto+N+Sg+Ine auto+N+Pl+Abl auto+N+Sg+Nom+# ilta+N+Sg+Nom maantie+N+Sg+Nom maa+N+Sg+Gen+# tie+N+Sg+Nom s√§hk√∂auto+N+Sg+Nom s√§hk√∂+N+Sg+Nom+# auto+N+Sg+Nom  auto auto+ssa auto+i+lta auto+ilta maantie maa+n+tie s√§hk√∂auto s√§hk√∂+auto  word forms in Table 1, where the full analyses are provided by the rule-based OMorFi analyzer developed by Pirinen (2008). Note that it is typical for word forms to have alternative analyses and/or meanings that cannot be disambiguated without sentential context. Evidently, the level of detail in the full analysis is substantially higher compared with the segmentation, as it contains lemmatization as well as morphological tagging, whereas the segmentation consists of only segment boundary positions. Consequently, because of this simpliÔ¨Åcation, morphological segmentation has been amenable to unsupervised machine learning methodology, beginning with the work of Harris (1955). Meanwhile, the majority of work on learning of full morphological analysis has used supervised methodology (Chrupala, Dinu, and van Genabith 2008). Lastly, there have been numerous studies on statistical learning of intermediate forms of segmentation and full analysis (Lignos 2010; Virpioja, Kohonen, and Lagus 2010) as well as alternative morphological representations (Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001; Neuvel and Fulop 2002; Johnson and Martin 2003). As for language processing, learning segmentation can be advantageous compared with learning full analyses. In particular, learning full analysis in a supervised manner typically requires up to tens of thousands of manually annotated sentences. A low-cost alternative, therefore, could be to learn morphological segmentation from unannotated word lists and a handful of annotated examples. Importantly, segmentation analysis has been found useful in a range of applications, such as speech recognition (Hirsim√§ki et al. 2006; Narasimhan et al. 2014), information retrieval (Turunen and Kurimo 2011), machine translation (de Gispert et al. 2009; Green and DeNero 2012), and word representation learning (Luong, Socher, and Manning 2013; Qiu et al. 2014). Despite its intuitiveness, it should be noted that the segmentation representation is not equally applicable to all languages. To this end, consider the terms isolative and synthetic languages. In languages with a high amount of isolating morphological properties, word forms tend to comprise their own morphemes. Meanwhile, in heavily synthetic languages, words tend to contain multiple morphemes. Synthetic languages can be described further according to their agglutinative (concatenative) and fusional properties. In the former, the morphs tend to have clear boundaries between them whereas in the latter, the morphs tend to be indistinguishable. For examples of agglutinative and fusional word formation, consider the English verbs played (past tense of play) and sang (past tense of sing). Where the previous can be effortlessly 94  Ruokolainen et al.  Minimally Supervised Morphological Segmentation  divided into two segments as play+ed (STEM + PAST TENSE), there are no such distinct boundaries in the latter. Generally, languages with synthetic properties mix concatenative and fusional schemes and contain agglutinative properties to varying degrees. Morphological segmentation can be most naturally applied to highly agglutinative languages. Morphologically ambiguous word forms are common especially in highly synthetic languages. Even without disambiguation based on sentential context, providing all correct alternatives could be useful for some downstream applications, such as information retrieval. Statistical methods can usually provide n-best segmentations; for example, Morfessor (Creutz and Lagus 2007) and CRFs (Ruokolainen et al. 2013) by using n-best Viterbi algorithm and adaptor grammar (Sirts and Goldwater 2013) by collecting the variations in the posterior distribution samples. Although there is no evident way to decide the correct number of alternatives for a particular word form, n-best lists might be useful whenever recall (including the correct answers) is more important than precision (excluding any incorrect answers). The Morpho Challenge competitions have allowed providing alternative segmentations for the submitted methods, but no clear developments have been reported. In fact, even in the reference results based on the gold standard segmentations, selecting all alternative segmentations has performed slightly worse in the information retrieval tasks than taking only the Ô¨Årst segmentation (Kurimo, Virpioja, and Turunen 2010).  3.2 Minimally Supervised Learning Settings In data-driven morphological segmentation, our aim is to learn segmentation models from training data. Subsequent to training, the models provide segmentations for given word forms. In the minimally supervised learning setting, as deÔ¨Åned here, the models are estimated from annotated and unannotated word forms. We denote the annotated data set comprising word forms with their corresponding segmentation as D and the unannotated data set comprising raw word forms as U. Typically, the raw word forms can be obtained easily and, consequently, U can contain millions of word forms. Meanwhile, acquiring the annotated data D requires manual labor and, therefore, typically contains merely hundreds or thousands of word forms. For an illustration of D and U, see Table 2.  Table 2 Examples of annotated and unannotated data, D and U, respectively. Typically, U can contain hundreds of thousands or millions of word forms, whereas D contains merely hundreds or thousands of word forms.  D  U  anarch + ist + s bound + ed conting + ency de + fame entitle + ment fresh + man ...  actions bilinguals community disorders equipped faster ...  95  Computational Linguistics  Volume 42, Number 1  We consider three machine learning approaches applicable in the minimally supervised learning setting, namely, unsupervised, supervised, and semi-supervised learning. In unsupervised learning, the segmentation models are trained on solely unannotated data U. Meanwhile, supervised models are trained from solely the annotated data D. Finally, the aim of semi-supervised learning is to utilize both the available unannotated and annotated data. Because the semi-supervised approach utilizes the largest amount of data, it is expected to be most suitable for acquiring high segmentation accuracy in the minimally supervised learning setting. Lastly, we note that the unsupervised learning framework can be understood in a strict or non-strict sense, depending on whether the applied methods are allowed to use annotated data D for hyperparameter tuning. Although the term unsupervised learning itself suggests that such adjusting is infeasible, this type of tuning is nevertheless common (Creutz et al. 2007; √á√∂ltekin 2010; Spiegler and Flach 2010; Sirts and Goldwater 2013). In addition, the minimally supervised learning setting explicitly assumes a small amount of available annotated word forms. Consequently, in the remainder of this article, all discussion on unsupervised methods refers to unsupervised learning in the non-strict sense. 3.3 Algorithms Here we provide a literature survey on proposed morphological segmentation methods applicable in the minimally supervised learning setting. We place particular emphasis on three method families, namely, the Morfessor algorithm (Creutz and Lagus 2002, 2005, 2007; Kohonen, Virpioja, and Lagus 2010; Gr√∂nroos et al. 2014), the adaptor grammar framework (Sirts and Goldwater 2013), and conditional random Ô¨Åelds (Ruokolainen et al. 2013, 2014). These approaches are the subject of the empirical evaluation presented in Section 4. We present individual method descriptions in Section 3.3.1. Subsequently, Section 3.3.2 provides a summarizing discussion, the purpose of which is to gain insight on the fundamental differences and similarities between the varying approaches. 3.3.1 Descriptions Morfessor. We begin by describing the original, unsupervised Morfessor method family (Creutz and Lagus 2002, 2005, 2007). We then discuss the later, semi-supervised extensions (Kohonen, Virpioja, and Lagus 2010; Gr√∂nroos et al. 2014). In particular, we review the extension of Morfessor Baseline to semi-supervised learning by using a weighted generative model (Kohonen, Virpioja, and Lagus 2010), and then discuss the most recent Morfessor variant, FlatCat (Gr√∂nroos et al. 2014). Finally, we discuss some general results from the literature on semi-supervised learning with generative models. The unsupervised Morfessor methods are based on a generative probabilistic model that generates the observed word forms xi ‚àà U by concatenating morphs xi = mi1 ‚ó¶ mi2 ‚ó¶ ¬∑ ¬∑ ¬∑ ‚ó¶ min. The morphs are stored in a morph lexicon, which deÔ¨Ånes the probability of each morph P(m | Œ∏) given some parameters Œ∏. The Morfessor learning problem is to Ô¨Ånd a morph lexicon that strikes an optimal balance between encoding the observed word forms concisely and, at the same time, having a concise morph lexicon. To this end, Morfessor utilizes a prior distribution P(Œ∏) over morph lexicons, derived from the Minimum Description Length principle (Rissanen 1989), that favors lexicons that contain fewer, shorter morphs. This leads to the following minimization problem  96  Ruokolainen et al.  Minimally Supervised Morphological Segmentation  that seeks to balance the conciseness of the lexicon with the conciseness of the observed corpus encoded with the lexicon:  Œ∏‚àó = arg min L(Œ∏, U ) = arg min{‚àí ln P(Œ∏) ‚àí ln P(U | Œ∏)} ,  (1)  Œ∏  Œ∏  The optimization problem in Equation (1) is complicated by the fact that each word in the corpus U can be generated by different combinations of morphs, deÔ¨Åned by the set of segmentations of that word. This introduces a nuisance parameter z for the segmentation of each word form, where P(U | Œ∏) = Z P(U | z, Œ∏)P(z). Because of this summation, the expression cannot be solved analytically, and iterative optimization must be used instead. The unsupervised Morfessor variants differ in the following ways: Ô¨Årst, whether all morphs belong to a single category or the categories PREFIX, STEM, and SUFFIX are used; secondly, if the model utilizes a lexicon that is Ô¨Çat or hierarchical. In a Ô¨Çat lexicon, morphs can only be encoded by combining letters, whereas in a hierarchical lexicon pre-existing morphs can be used for storing longer morphs. Thirdly, the parameter estimation and inference methods differ. Parameters are estimated using greedy local search or iterative batch procedures while inference is performed with either Viterbi decoding or heuristic procedures. The earliest Morfessor method, referred to as Morfessor Baseline, has been extended to semi-supervised learning by Kohonen, Virpioja, and Lagus (2010). In contrast, the later methods, namely, Categories-ML and Categories-MAP, have not been extended, as they use either hierarchical lexicons or training procedures that make them less amenable to semi-supervised learning. However, recently Gr√∂nroos et al. (2014) proposed a new Morfessor variant that uses morph categories in combination with a Ô¨Çat lexicon, and can therefore apply the semi-supervised learning technique of Kohonen, Virpioja, and Lagus. We begin the description of the semi-supervised extension to Morfessor Baseline (Creutz and Lagus 2002, 2007) by reviewing its generative model. Morfessor Baseline utilizes a model in which word forms are generated by concatenating morphs, all of which belong to the same category. It utilizes a Ô¨Çat morph lexicon P(m | Œ∏) that is simply a multinomial distribution over morphs m, according to the probabilities given by the parameter vector Œ∏. The utilized prior penalizes storing long morphs in the lexicon by assigning each stored morph a cost that depends most strongly on the morph length in letters. A morph is considered to be stored if the lexicon assigns it a nonzero probability. The parameter estimation for Œ∏ Ô¨Ånds a local optimum utilizing greedy local search. The search procedure approximates the optimization problem in Equation (1) by assuming that, for each word form xi, its corresponding segmentation distribution P(zi) has all its mass concentrated to a single segmentation zi. The parameter estimation is then performed by locally searching each word for the segmentation that yields the best value of the cost function in Equation (1). The process is repeated for all words in random order until convergence. Subsequent to learning, the method predicts the segmentation of a word form by selecting the segmentation with the most probable sequence of morphs using an extension of the Viterbi algorithm. Semi-supervised learning is in principle trivial for a generative model: For the labeled word forms D, the segmentation is Ô¨Åxed to its correct value, and for the unlabeled forms U the standard parameter estimation procedure is applied. However, Kohonen,  97  Computational Linguistics  Volume 42, Number 1  Virpioja, and Lagus (2010) failed to achieve notable improvements in this fashion, and consequently replaced the minimized function L in Equation (1) with  L(Œ∏, z, U, D) = ‚àí ln P(Œ∏) ‚àí Œ± √ó ln P(U | Œ∏) ‚àí Œ≤ √ó ln P(D | Œ∏).  (2)  Such weighted objectives were used earlier in combination with generative models by,  for example, Nigam et al. (2000). The semi-supervised training procedure then adjusts  the weight values Œ± and Œ≤. The absolute values of the weights control the cost of  encoding a morph in the training data with respect to the cost of adding a new morph to  the lexicon, and their ratio controls how much weight is placed on the annotated data  with respect to the unannotated data. When the hyperparameters Œ± and Œ≤ are Ô¨Åxed,  the lexicon parameters Œ∏ can be optimized with the same greedy local search procedure  as in the unsupervised Morfessor Baseline. The weights can then be optimized with a  grid search and by choosing the model with the best evaluation score on a held-out  development set. Although this modiÔ¨Åcation is difÔ¨Åcult to justify from the perspective  of generative modeling, Kohonen, Virpioja, and Lagus show that in practice it can  yield performance improvements. From a theoretical point of view, it can be seen  as incorporating discriminative training techniques when working with a generative  model by optimizing for segmentation performance rather than maximum a posteriori  probability. However, only the hyperparameters are optimized in this fashion, whereas  the lexicon parameters are still learned within the generative model framework.  The semi-supervised learning strategy described here is simple to apply if the  objective function in Equation (1) can be factored to parts that encode the morphs  using letters and encode the training corpus using the morphs. For some models of the  Morfessor family this is not possible because of the use of a hierarchical lexicon, where  morphs can be generated from other morphs as well as from individual letters. In par-  ticular, this includes the well-performing Categories-MAP variant (Creutz et al. 2007).  In contrast to Morfessor Baseline, the Categories-MAP and the preceding Categories-  ML method use a hidden Markov model to produce the observed words, where the  states are given by STEM, PREFIX, SUFFIX categories as well as an internal non-morpheme  category. A recent development is Morfessor FlatCat by Gr√∂nroos et al. (2014), which  uses the hidden Markov model structure and morph categories in combination with a  Ô¨Çat lexicon, thus allowing semi-supervised learning in the same fashion as for Morfessor  Baseline.  In general, the key idea behind using the weighted objective function in Equation  (2) for semi-supervised learning is that the hyperparameters Œ± and Œ≤ can be used to  explicitly control the inÔ¨Çuence of the unannotated data on the learning. Similar semi-  supervised learning strategies have also been used in other problems. For classiÔ¨Åcation  with generative models, it is known that adding unlabeled data to a model trained with  labeled data can degrade performance (Cozman et al. 2003; Cozman and Cohen 2006).  In particular, this can be the case if the generative model does not match the generating  process, something that is difÔ¨Åcult to ensure in practice. Recently, this phenomenon was  analyzed in more detail by Fox-Roberts and Rosten (2014), who show that, although the  unlabeled data can introduce a bias, the bias can be removed by optimizing a weighted  likelihood  function  where  the  unlabeled  data  is  raised  to  the  power  NL N  ,  where  NL  is  the  number of labeled samples and N is the number of all samples. This corresponds to the  weighting  scheme  used  in  Morfessor  when  setting  the  ratio  Œ± Œ≤  =  NL N  .  98  Ruokolainen et al.  Minimally Supervised Morphological Segmentation  Adaptor Grammars. Recently, Sirts and Goldwater (2013) presented work on minimally supervised morphological segmentation using the adaptor grammar (AG) approach (Johnson, GrifÔ¨Åths, and Goldwater 2006). The AGs are a non-parametric Bayesian modeling framework applicable for learning latent tree structures over an input corpus of strings. They can be used to deÔ¨Åne morphological grammars of different complexity, starting from the simplest grammar where each word is just a sequence of morphs and extending to more complex grammars, where each word consists, for example, of zero or more preÔ¨Åxes, a stem, and zero or more sufÔ¨Åxes. The actual forms of the morphs are learned from the data and, subsequent to learning, used to generate segmentations for new word forms. In this general approach, AGs are similar to the Morfessor family (Creutz and Lagus 2007). A major difference, however, is that the morphological grammar is not hard-coded but instead speciÔ¨Åed as an input to the algorithm. This allows different grammars to be explored in a Ô¨Çexible manner. Prior to the work by Sirts and Goldwater, the AGs were successfully applied in a related task of segmenting utterances into words (Johnson 2008; Johnson and Goldwater 2009; Johnson and Demuth 2010). The second major difference between the Morfessor family and the AG framework is the contrast between the MAP and fully Bayesian estimation approaches. Whereas the search procedure of the Morfessor method discussed earlier returns a single model corresponding to the MAP point-estimate, AGs instead operate with full posterior distributions over all possible models. Because acquiring the posteriors analytically is intractable, inference is performed utilizing Markov chain Monte Carlo algorithms to obtain samples from the posterior distributions of interest (Johnson 2008; Johnson and Goldwater 2009; Johnson and Demuth 2010; Sirts and Goldwater 2013). However, as sampling-based models are costly to train on large amounts of data, we adopt the parsing-based method proposed in Sirts and Goldwater (2013) to use the trained AG model inductively on test data. One of the byproducts of training the AG model is the posterior grammar, which in addition to all the initial grammar rules, also contains the cached subtrees learned by the system. This grammar can be used in any standard parser to obtain segmentations for new data. The AG framework was originally designed for the unsupervised learning setting, but Sirts and Goldwater (2013) introduced two approaches for semi-supervised learning they call the semi-supervised AG and AG Select methods. The semi-supervised AG approach is an extension to unsupervised AG, in which the annotated data D is exploited in a straightforward manner by keeping the annotated parts of parse trees Ô¨Åxed while inferring latent structures for the unannotated parts. For unannotated word forms, inference is performed on full trees. For example, the grammar may specify that words are sequences of morphs and each morph is a sequence of submorphs. Typically, the annotated data only contain morpheme boundaries and submorphs are latent in this context. In this situation the inference for annotated data is performed over submorph structures only. Similarly to unsupervised learning, semi-supervised AG requires the morphological grammar to be deÔ¨Åned manually. Meanwhile, the AG Select approach aims to automate the grammar development process by systematically evaluating a range of grammars and Ô¨Ånding the best one. AG Select is trained using unsupervised AG with an uninformative metagrammar so that the resulting parse-trees contain many possible segmentation templates. To Ô¨Ånd out which template works the best for any given language or data set, each of these templates are evaluated using the annotated data set D. In this sense, AG Select can be characterized as more of a model selection method than semi-supervised learning.  99  Computational Linguistics  Volume 42, Number 1  Conditional Random Fields. The Morfessor and AG algorithms discussed earlier, although different in several respects, operate in a similar manner in that they both learn lexicons. For Morfessor, the lexicon consists of morphs, whereas for AG, the lexical units are partial parse-trees. Subsequent to learning, new word forms are segmented either by generating the most likely morph sequences (Morfessor) or by sampling parse trees from the posterior distribution (AG). In what follows, we consider a different approach to segmentation using sequence labeling methodology. The key idea in this approach is to focus the modeling effort to morph boundaries instead of the whole segments. Following the presentation of Ruokolainen et al. (2013, 2014), the morphological segmentation task can be represented as a sequence labeling problem by assigning each character in a word form to one of three classes, namely, B beginning of a multi-character morph M middle of a multi-character morph S single-character morph Using this label set, one can represent the segmentation of the Finnish word autoilta (from cars) (auto+i+lta) as au t oil t a ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì‚Üì ‚Üì ‚Üì BMMMSBMM Naturally, one can also use other label sets. Essentially, by deÔ¨Åning more Ô¨Åne-grained labels, one captures increasingly eloquent structure but begins to overÔ¨Åt model to the training data because of increasingly sparser statistics. Subsequent to deÔ¨Åning the label set, one can learn a segmentation model using general sequence labeling methods, such as the well-known conditional random Ô¨Åeld (CRF) framework (Lafferty, McCallum, and Pereira 2001). Denoting the word form and the corresponding label sequence as x and y, respectively, the CRFs directly model the conditional probability of the segmentation given the word form, that is, p(y | x; w). The model parameters w are estimated discriminatively from the annotated data set D using iterative learning algorithms (Lafferty, McCallum, and Pereira 2001; Collins 2002). Subsequent to estimation, the CRF model segments word forms x by using maximum a posteriori (MAP) graph inference, that is, solving an optimization problem  z = arg max p (u | x; w)  (3)  u  using the standard Viterbi search (Lafferty, McCallum, and Pereira 2001). As it turns out, the CRF model can learn to segment words with a surprisingly high accuracy from a relatively small D, that is, without utilizing any of the available unannotated word forms U. Particularly, Ruokolainen et al. (2013) showed that it is sufÔ¨Åcient to use simple left and right substring context features that are naturally accommodated by the discriminative parameter estimation procedure. Moreover, Ruokolainen et al. (2014) showed that the CRF-based approach can be successfully extended to semi-supervised learning settings in a straightforward manner via feature set expansion by utilizing predictions of unsupervised segmentation algorithms. By utilizing this approach, the CRF model learns to associate the output of the unsupervised algorithms, such as the Morfessor and adaptor grammar methods, in relation to the surrounding substring context.  100  Ruokolainen et al.  Minimally Supervised Morphological Segmentation  Other Work. In addition to the algorithms discussed here, there exist numerous other segmentation approaches applicable in the minimally supervised learning setting. As the earliest example of work in this line, consider obtaining segmentations using the classic letter successor variety (LSV) method of Harris (1955). The LSV method utilizes the insight that the predictability of successive letters should be high within morph segments, and low at the boundaries. Consequently, a high variety of letters following a preÔ¨Åx indicates a high probability of a boundary. Whereas LSV score tracks predictability given preÔ¨Åxes, the same idea can be utilized for sufÔ¨Åxes, providing the letter predecessor variety (LPV) method. As for the minimally supervised learning setting, the LSV/LPV method can be used most straightforwardly by counting the LSV/LPV scores from unannotated data and, subsequently, tuning the necessary threshold values on the annotated data (√á√∂ltekin 2010). On the other hand, one could also use the LSV/LPV values as features for a classiÔ¨Åcation model, in which case the threshold values can be learned discriminatively based on the available annotated data. The latter approach is essentially realized in the event the LSV/PSV scores are provided for the CRF model discussed earlier (Ruokolainen et al. 2014). As for more recent work, we Ô¨Årst refer to the generative log-linear model of Poon, Cherry, and Toutanova (2009). Similarly to the Morfessor model family, this approach is based on deÔ¨Åning a joint probability distribution over the unannotated word forms U and the corresponding segmentations S. The distribution is log-linear in form and is denoted as p(U, S; Œ∏), where Œ∏ is the model parameter vector. Again, similarly to the Morfessor framework, Poon, Cherry, and Toutanova (2009) learn a morph lexicon that is subsequently used to generate segmentations for new word forms. The learning is controlled using prior distributions on both corpus and lexicon, which penalize exceedingly complex morph lexicon (similarly to Morfessor) and exceedingly segmented corpus, respectively. The log-linear form of p(U, S; Œ∏) enables the approach to use a wide range of overlapping features. Particularly, Poon, Cherry, and Toutanova (2009) utilize a morph-context feature set with individual features deÔ¨Åned for each morph and morph substring contexts. In addition to unsupervised learning, they present experiments in the semi-supervised setting. SpeciÔ¨Åcally, they accomplish this by Ô¨Åxing the segmentations of annotated words in D, according to their gold standard segmentation. Note, however, that this approach of extending a generative model does not necessarily utilize the supervision efÔ¨Åciently, as discussed previously regarding the Morfessor method family. Finally, we brieÔ¨Çy mention a range of recently published methods (Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Kƒ±lƒ±√ß and Bozs¬∏ahin 2012; Eger 2013). The Paramor approach presented by Monson, Hollingshead, and Roark (2010) deÔ¨Ånes a rule-based system for unsupervised learning of morphological paradigms. The Promodes system of Spiegler and Flach (2010) deÔ¨Ånes a family of generative probabilistic models for recovering segment boundaries in an unsupervised fashion. The algorithm of Kƒ±lƒ±√ß and Bozs¬∏ahin (2012) is based on a generative hidden Markov model (HMM), in which the HMM learns to generate morph sequences for given word forms in a semi-supervised fashion. Finally, Eger (2013) presents work on fully supervised segmentation by exhaustive enumeration and a generative Markov model on morphs. As for the minimally supervised learning setting, the Paramor system learns mainly from unannotated data U and utilizes annotated data D to adjust the required threshold value. The Promodes models can be trained either in an unsupervised manner on U or in a supervised manner on D. The algorithm of Kƒ±lƒ±√ß and Bozs¬∏ahin (2012) learns mainly from unannotated data U and incorporates supervision from the annotated corpus in the form of manually selected statistics: the inclusion of the statistics yields a large  101  Computational Linguistics  Volume 42, Number 1  improvement in performance. Lastly, in their work with the supervised enumeration approach, Eger (2013) assumes a large (on the order of tens of thousands) amount of annotated word forms available for learning. Thus, it is left for future work to determine if the approach could be applied successfully in the minimally supervised learning setting. 3.3.2 Summary. Here we aim to summarize the fundamental differences and similarities between the varying learning approaches discussed in the previous section. Learning Lexicons versus Detecting Boundaries. We begin by dividing the methods described earlier into two‚Äîlexicon-based (Creutz et al. 2007; Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Kƒ±lƒ±√ß and Bozs¬∏ahin 2012; Eger 2013; Sirts and Goldwater 2013) and boundary detection (Harris 1955; Spiegler and Flach 2010; Ruokolainen et al. 2013)‚Äîcategories. In the former, the model learns lexical units, whereas in the latter the model learns properties of morph boundaries. For example, in the case of Morfessor (Creutz et al. 2007) the lexical units correspond to morphs whereas in AGs (Sirts and Goldwater 2013) the units are parse trees. Meanwhile, consider the CRF approach of Ruokolainen et al. (2013) and the classical approach of Harris (1955), which identify morph boundary positions using substring contexts and letter successor varieties, respectively. In general, whether it is easier to discover morphs or morph boundaries is largely an empirical question. So far, only the method of Poon, Cherry, and Toutanova (2009) has explicitly modeled both a morph lexicon and features describing character n-grams at morpheme boundaries. Generative versus Discriminative Learning. The second main distinction divides the models into generative and discriminative approaches. The generative approaches (Creutz et al. 2007; Poon, Cherry, and Toutanova 2009; Spiegler and Flach 2010; Monson, Hollingshead, and Roark 2010; Kƒ±lƒ±√ß and Bozs¬∏ahin 2012; Eger 2013; Sirts and Goldwater 2013) model the joint distribution of word forms and their corresponding segmentations, whereas discriminative (Harris 1955; Ruokolainen et al. 2013) approaches directly estimate a conditional distribution of segmentation given a word form. In other words, whereas generative methods generate both word forms and segmentations, the discriminative methods generate only segmentations given word forms. The generative models are naturally applicable for unsupervised learning. Meanwhile, discriminative modeling always requires some annotated data, thus excluding the possibility of unsupervised learning. Lastly, it appears that most lexicon-based methods are generative and most boundary detection methods are discriminative. However, note that this is a trend rather than a rule, as exempliÔ¨Åed by generative boundary detection method of Spiegler and Flach (2010). Semi-Supervised Learning Approaches. Both generative and discriminative models can be extended to utilize annotated as well as unannotated data in a semi-supervised manner. However, the applicable techniques differ. For generative models, semi-supervised learning is in principle trivial: For the labeled word forms D, the segmentation is Ô¨Åxed to its correct value, as exempliÔ¨Åed by the approaches of Poon, Cherry, and Toutanova (2009), Spiegler and Flach (2010), and Sirts and Goldwater (2013). On the other hand, the semi-supervised setting also makes it possible to apply discriminative techniques to generative models. In particular, model hyperparameters can be selected to optimize segmentation performance, rather than some generative objective, such as likelihood. Special cases of hyperparameter selection include the weighted objective function 102  Ruokolainen et al.  Minimally Supervised Morphological Segmentation  (Kohonen, Virpioja, and Lagus 2010), data selection (Virpioja, Kohonen, and Lagus 2011; Sirts and Goldwater 2013), and grammar template selection (Sirts and Goldwater 2013). As for the weighted objective function and grammar template selection, the weights and templates are optimized to maximize segmentation accuracy. Meanwhile, data selection is based on the observation that omitting some of the training data can improve segmentation accuracy (Virpioja, Kohonen, and Lagus 2011; Sirts and Goldwater 2013). For discriminative models, the possibly most straightforward semi-supervised learning technique is adding features derived from the unlabeled data, as exempliÔ¨Åed by the CRF approach of Ruokolainen et al. (2014). However, discriminative, semi-supervised learning is in general a much researched Ô¨Åeld with numerous diverse techniques (Zhu and Goldberg 2009). For example, merely for the CRF model alone, there exist several proposed semi-supervised learning approaches (Jiao et al. 2006; Mann and McCallum 2008; Wang et al. 2009). On Local Search. In what follows, we will discuss a potential pitfall of some algorithms that utilize local search procedures in the parameter estimation process, as exempliÔ¨Åed by the Morfessor model family (Creutz et al. 2007). As discussed in Section 3.3.1, the Morfessor algorithm Ô¨Ånds a local optimum of the objective function using a local search procedure. This complicates model development because if two model variants perform differently empirically, it is uncertain whether it is because of a truly better model or merely better Ô¨Åt with the utilized parameter estimation method, as discussed also by Goldwater (2006, Section 4.2.2.3). Therefore, in contrast, within the adaptor grammar framework (Johnson, GrifÔ¨Åths, and Goldwater 2006; Sirts and Goldwater 2013), the focus has not been on Ô¨Ånding a single best model, but rather on Ô¨Ånding the posterior distribution over segmentations of the words. Another approach to the problem of bad local optima is to start a local search near some known good solution. This approach is taken in Morfessor FlatCat, for which it was found that initializing the model with the segmentations produced by the supervised CRF model (with a convex objective function) yields improved results (Gr√∂nroos et al. 2014). 4. Experiments In this section, we perform an empirical comparison of segmentation algorithms in the semi-supervised learning setting. The purpose of the presented experiments is to extend the current literature by considering a wider range of languages compared with previous work, and by providing an in-depth error analysis. 4.1 Data We perform the experiments on four languages, namely, English, Estonian, Finnish, and Turkish. The English, Finnish, and Turkish data are from the Morpho Challenge 2009/2010 data set (Kurimo et al. 2009; Kurimo, Virpioja, and Turunen 2010). The annotated Estonian data set is acquired from a manually annotated, morphologically disambiguated corpus,1 and the unannotated word forms are gathered from the Estonian Reference Corpus (Kaalep et al. 2010). Table 3 shows the total number of instances available for model estimation and testing.  
This tutorial aims to bring NLP researchers up to speed with the current techniques in deep learning and neural networks, and show them how they can turn their ideas into practical implementations. We will start with simple classification models (logistic regression and multilayer perceptrons) and cover more advanced patterns that come up in NLP such as recurrent networks for sequence tagging and prediction problems, structured networks (e.g., compositional architectures based on syntax trees), structured output spaces (sequences and trees), attention for sequence-to-sequence transduction, and feature induction for complex algorithm states. A particular emphasis will be on learning to represent complex objects as recursive compositions of simpler objects. This representation will reflect characterize standard objects in NLP, such as the composition of characters and morphemes into words, and words into sentences and documents. In addition, new opportunities such as learning to embed ``algorithm states'' such as those used in transition-based parsing and other sequential structured prediction models (for which effective features may be difficult to engineer by hand) will be covered.Everything in the tutorial will be grounded in code {---} we will show how to program seemingly complex neural-net models using toolkits based on the computation-graph formalism. Computation graphs decompose complex computations into a DAG, with nodes representing inputs, target outputs, parameters, or (sub)differentiable functions (e.g., ``tanh'', ``matrix multiply'', and ``softmax''), and edges represent data dependencies. These graphs can be run ``forward'' to make predictions and compute errors (e.g., log loss, squared error) and then ``backward'' to compute derivatives with respect to model parameters. In particular we'll cover the Python bindings of the CNN library. CNN has been designed from the ground up for NLP applications, dynamically structured NNs, rapid prototyping, and a transparent data and execution model.
In the early days of the statistical NLP era, many language processing tasks were tackled using the so-called pipeline architecture: the given task is broken into a series of sub-tasks such that the output of one sub-task is an input to the next sub-task in the sequence. The pipeline architecture is appealing for various reasons, including modularity, modeling convenience, and manageable computational complexity. However, it suffers from the error propagation problem: errors made in one sub-task are propagated to the next sub-task in the sequence, leading to poor accuracy on that sub-task, which in turn leads to more errors downstream. Another disadvantage associated with it is lack of feedback: errors made in a sub-task are often not corrected using knowledge uncovered while solving another sub-task down the pipeline.Realizing these weaknesses, researchers have turned to joint inference approaches in recent years. One such approach involves the use of Markov logic, which is defined as a set of weighted first-order logic formulas and, at a high level, unifies first-order logic with probabilistic graphical models. It is an ideal modeling language (knowledge representation) for compactly representing relational and uncertain knowledge in NLP. In a typical use case of MLNs in NLP, the application designer describes the background knowledge using a few first-order logic sentences and then uses software packages such as Alchemy, Tuffy, and Markov the beast to perform learning and inference (prediction) over the MLN. However, despite its obvious advantages, over the years, researchers and practitioners have found it difficult to use MLNs effectively in many NLP applications. The main reason for this is that it is hard to scale inference and learning algorithms for MLNs to large datasets and complex models, that are typical in NLP.In this tutorial, we will introduce the audience to recent advances in scaling up inference and learning in MLNs as well as new approaches to make MLNs a ``black-box'' for NLP applications (with only minor tuning required on the part of the user). Specifically, we will introduce attendees to a key idea that has emerged in the MLN research community over the last few years, lifted inference , which refers to inference techniques that take advantage of symmetries (e.g., synonyms), both exact and approximate, in the MLN . We will describe how these next-generation inference techniques can be used to perform effective joint inference. We will also present our new software package for inference and learning in MLNs, Alchemy 2.0, which is based on lifted inference, focusing primarily on how it can be used to scale up inference and learning in large models and datasets for applications such as semantic similarity determination, information extraction and question answering.
Machine learning (ML) has been successfully used as a prevalent approach to solving numerous NLP problems. However, the classic ML paradigm learns in isolation. That is, given a dataset, an ML algorithm is executed on the dataset to produce a model without using any related or prior knowledge. Although this type of isolated learning is very useful, it also has serious limitations as it does not accumulate knowledge learned in the past and use the knowledge to help future learning, which is the hallmark of human learning and human intelligence. Lifelong machine learning (LML) aims to achieve this capability. Specifically, it aims to design and develop computational learning systems and algorithms that learn as humans do, i.e., retaining the results learned in the past, abstracting knowledge from them, and using the knowledge to help future learning. In this tutorial, we will introduce the existing research of LML and to show that LML is very suitable for NLP tasks and has potential to help NLP make major progresses.
Sentiment analysis has been a major research topic in natural language processing (NLP). Traditionally, the problem has been attacked using discrete models and manually-defined sparse features. Over the past few years, neural network models have received increased research efforts in most sub areas of sentiment analysis, giving highly promising results. A main reason is the capability of neural models to automatically learn dense features that capture subtle semantic information over words, sentences and documents, which are difficult to model using traditional discrete features based on words and ngram patterns. This tutorial gives an introduction to neural network models for sentiment analysis, discussing the mathematics of word embeddings, sequence models and tree structured models and their use in sentiment analysis on the word, sentence and document levels, and fine-grained sentiment analysis. The tutorial covers a range of neural network models (e.g. CNN, RNN, RecNN, LSTM) and their extensions, which are employed in four main subtasks of sentiment analysis:Sentiment-oriented embeddings;Sentence-level sentiment;Document-level sentiment;Fine-grained sentiment.The content of the tutorial is divided into 3 sections of 1 hour each. We assume that the audience is familiar with linear algebra and basic neural network structures, introduce the mathematical details of the most typical models. First, we will introduce the sentiment analysis task, basic concepts related to neural network models for sentiment analysis, and show detail approaches to integrate sentiment information into embeddings. Sentence-level models will be described in the second section. Finally, we will discuss neural network models use for document-level and fine-grained sentiment.
The mathematical metaphor offered by the geometric concept of distance in vector spaces with respect to semantics and meaning has been proven to be useful in many monolingual natural language processing applications. There is also some recent and strong evidence that this paradigm can also be useful in the cross-language setting. In this tutorial, we present and discuss some of the most recent advances on exploiting the vector space model paradigm in specific cross-language natural language processing applications, along with a comprehensive review of the theoretical background behind them.First, the tutorial introduces some fundamental concepts of distributional semantics and vector space models. More specifically, the concepts of distributional hypothesis and term-document matrices are revised, followed by a brief discussion on linear and non-linear dimensionality reduction techniques and their implications to the parallel distributed approach to semantic cognition. Next, some classical examples of using vector space models in monolingual natural language processing applications are presented. Specific examples in the areas of information retrieval, related term identification and semantic compositionality are described.Then, the tutorial focuses its attention on the use of the vector space model paradigm in cross-language applications. To this end, some recent examples are presented and discussed in detail, addressing the specific problems of cross-language information retrieval, cross-language sentence matching, and machine translation. Some of the most recent developments in the area of Neural Machine Translation are also discussed.Finally, the tutorial concludes with a discussion about current and future research problems related to the use of vector space models in cross-language settings. Future avenues for scientific research are described, with major emphasis on the extension from vector and matrix representations to tensors, as well as the problem of encoding word position information into the vector-based representations.
Many important NLP tasks are casted as structured prediction problems, and try to predict certain forms of structured output from the input. Examples of structured prediction include POS tagging, named entity recognition, PCFG parsing, dependency parsing, machine translation, and many others. When apply structured prediction to a specific NLP task, there are the following challenges:1. Model selection: Among various models/algorithms with different characteristics, which one should we choose for a specific NLP task?2. Training: How to train the model parameters effectively and efficiently?3. Overfitting: To achieve good accuracy on test data, it is important to control the overfitting from the training data. How to control the overfitting risk for structured prediction?This tutorial will provide a clear overview of recent advances in structured prediction methods and theories, and address the above issues when we apply structured prediction to NLP tasks. We will introduce large margin methods (e.g., perceptrons, MIRA), graphical models (e.g., CRFs), and deep learning methods (e.g., RNN, LSTM), and show the respective advantages and disadvantages for NLP applications. For the training algorithms, we will introduce online/ stochastic training methods, and we will introduce parallel online/stochastic learning algorithms and theories to speed up the training (e.g., the Hogwild algorithm). For controlling the overfitting from training data, we will introduce the weight regularization methods, structure regularization, and implicit regularization methods.
Parsing accuracy using efÔ¨Åcient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed stateof-the-art approaches in constituency parsing. To remedy this, we introduce a new shiftreduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the Ô¨Årst provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n3) oracles for standard dependency parsing. Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data. 
Tree transducers that model expressive linguistic phenomena often require wordalignments and a heuristic rule extractor to induce their grammars. However, when the corpus of tree/string pairs is small compared to the size of the vocabulary or the complexity of the grammar, word-alignments are unreliable. We propose a general rule extraction algorithm that uses cost functions over tree fragments, and formulate the extraction as a cost minimization problem. As a by-product, we are able to introduce back-off states at which some cost functions generate right-hand-sides of previously unseen lefthand-sides, thus creating transducer rules ‚Äúon-the-Ô¨Çy‚Äù. We test the generalization power of our induced tree transducers on a QA task over a large Knowledge Base, obtaining a reasonable syntactic accuracy and effectively overcoming the typical lack of rule coverage. 
We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus. 
Center-embedding is difÔ¨Åcult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited centerembedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative model, the dependency model with valence (Klein and Manning, 2004). Cross-linguistic experiments on Universal Dependencies show that often our method boosts the performance from the baseline, and competes with the current state-ofthe-art model in a number of languages. 
Determining whether a major societal event has already happened, is still on-going, or may occur in the future is crucial for event prediction, timeline generation, and news summarization. We introduce a new task and a new corpus, EventStatus, which has 4500 English and Spanish articles about civil unrest events labeled as PAST, ON-GOING, or FUTURE. We show that the temporal status of these events is difÔ¨Åcult to classify because local tense and aspect cues are often lacking, time expressions are insufÔ¨Åcient, and the linguistic contexts have rich semantic compositionality. We explore two approaches for event status classiÔ¨Åcation: (1) a feature-based SVM classiÔ¨Åer augmented with a novel induced lexicon of future-oriented verbs, such as ‚Äúthreatened‚Äù and ‚Äúplanned‚Äù, and (2) a convolutional neural net. Both types of classiÔ¨Åers improve event status recognition over a state-of-the-art TempEval model, and our analysis offers linguistic insights into the semantic compositionality challenges for this new task. 
The challenges of Machine Reading and Knowledge Extraction at a web scale require a system capable of extracting diverse information from large, heterogeneous corpora. The Open Information Extraction (OIE) paradigm aims at extracting assertions from large corpora without requiring a vocabulary or relation-speciÔ¨Åc training data. Most systems built on this paradigm extract binary relations from arbitrary sentences, ignoring the context under which the assertions are correct and complete. They lack the expressiveness needed to properly represent and extract complex assertions commonly found in the text. To address the lack of representation power, we propose NESTIE, which uses a nested representation to extract higher-order relations, and complex, interdependent assertions. Nesting the extracted propositions allows NESTIE to more accurately reÔ¨Çect the meaning of the original sentence. Our experimental study on real-world datasets suggests that NESTIE obtains comparable precision with better minimality and informativeness than existing approaches. NESTIE produces 1.7-1.8 times more minimal extractions and achieves 1.1-1.2 times higher informativeness than CLAUSIE. 
With the renaissance of neural network in recent years, relation classiÔ¨Åcation has again become a research hotspot in natural language processing, and leveraging parse trees is a common and effective method of tackling this problem. In this work, we offer a new perspective on utilizing syntactic information of dependency parse tree and present a position encoding convolutional neural network (PECNN) based on dependency parse tree for relation classiÔ¨Åcation. First, treebased position features are proposed to encode the relative positions of words in dependency trees and help enhance the word representations. Then, based on a redeÔ¨Ånition of ‚Äúcontext‚Äù, we design two kinds of tree-based convolution kernels for capturing the semantic and structural information provided by dependency trees. Finally, the features extracted by convolution module are fed to a classiÔ¨Åer for labelling the semantic relations. Experiments on the benchmark dataset show that PECNN outperforms state-of-the-art approaches. We also compare the effect of different position features and visualize the inÔ¨Çuence of treebased position feature by tracing back the convolution process. 
This paper focuses on the study of recognizing discontiguous entities. Motivated by a previous work, we propose to use a novel hypergraph representation to jointly encode discontiguous entities of unbounded length, which can overlap with one another. To compare with existing approaches, we Ô¨Årst formally introduce the notion of model ambiguity, which deÔ¨Ånes the difÔ¨Åculty level of interpreting the outputs of a model, and then formally analyze the theoretical advantages of our model over previous existing approaches based on linearchain CRFs. Our empirical results also show that our model is able to achieve signiÔ¨Åcantly better results when evaluated on standard data with many discontiguous entities. 
When humans read text, they Ô¨Åxate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g., using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (Ô¨Åxating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading. 
Natural language acquisition relies on appropriate generalization: the ability to produce novel sentences, while learning to restrict productions to acceptable forms in the language. Psycholinguists have proposed various properties that might play a role in guiding appropriate generalizations, looking at learning of verb alternations as a testbed. Several computational cognitive models have explored aspects of this phenomenon, but their results are hard to compare given the high variability in the linguistic properties represented in their input. In this paper, we directly compare two recent approaches, a Bayesian model and a connectionist model, in their ability to replicate human judgments of appropriate generalizations. We Ô¨Ånd that the Bayesian model more accurately mimics the judgments due to its richer learning mechanism that can exploit distributional properties of the input in a manner consistent with human behaviour. 
Prediction without justiÔ¨Åcation has limited applicability. As a remedy, we learn to extract pieces of input text as justiÔ¨Åcations ‚Äì rationales ‚Äì that are tailored to be short and coherent, yet sufÔ¨Åcient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator speciÔ¨Åes a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.1 
Neural network based models have achieved impressive results on various speciÔ¨Åc tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufÔ¨Åcient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More speciÔ¨Åcally, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classiÔ¨Åcation tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 
We present EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model‚Äôs response to the questions. EpiReader is an end-to-end neural model comprising two components: the Ô¨Årst component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that EpiReader sets a new state-of-the-art on the CNN and Children‚Äôs Book Test benchmarks, outperforming previous neural models by a signiÔ¨Åcant margin. 
A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using generalpurpose lexical models such as word embeddings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-speciÔ¨Åc embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-speciÔ¨Åc contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, signiÔ¨Åcantly outperforming a strong baseline by 7.7% (relative). 
In this work, we show the possibility of inferring the answer type before solving a factoid question and leveraging the type information to improve semantic parsing. By replacing the topic entity in a question with its type, we are able to generate an abstract form of the question, whose answer corresponds to the answer type of the original question. A bidirectional LSTM model is built to train over the abstract form of questions and infer their answer types. It is also observed that if we convert a question into a statement form, our LSTM model achieves better accuracy. Using the predicted type information to rerank the logical forms returned by AgendaIL, one of the leading semantic parsers, we are able to improve the F1-score from 49.7% to 52.6% on the WEBQUESTIONS data. 
 Situated question answering is the problem of answering questions about an environment such as an image or diagram. This problem requires jointly interpreting a question and an environment using background knowledge to select the correct answer. We present Parsing to Probabilistic Programs (P 3), a novel situated question answering model that can use background knowledge and global features of the question/environment interpretation while retaining efÔ¨Åcient approximate inference. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, outperforming several competitive classical and neural baselines. 
A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing Ô¨Çute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be used to provide a probability distribution over Ô¨Ållers for a thematic role which is not mentioned in the text at all. To this end, we train two neural network models (an incremental one and a non-incremental one) on large amounts of automatically rolelabelled text. Our models are probabilistic and can handle several roles at once, which also enables them to learn interactions between different role Ô¨Ållers. Evaluation shows a drastic improvement over current state-of-the-art systems on modelling human thematic Ô¨Åt judgements, and we demonstrate via a sentence similarity task that the system learns highly useful embeddings. 
Word embedding has been widely studied and proven helpful in solving many natural language processing tasks. However, the ambiguity of natural language is always a problem on learning high quality word embeddings. A possible solution is sense embedding which trains embedding for each sense of words instead of each word. Some recent work on sense embedding uses context clustering methods to determine the senses of words, which is heuristic in nature. Other work creates a probabilistic model and performs word sense disambiguation and sense embedding iteratively. However, most of the previous work has the problems of learning sense embeddings based on imperfect word embeddings as well as ignoring the dependency between sense choices of neighboring words. In this paper, we propose a novel probabilistic model for sense embedding that is not based on problematic word embedding of polysemous words and takes into account the dependency between sense choices. Based on our model, we derive a dynamic programming inference algorithm and an ExpectationMaximization style unsupervised learning algorithm. The empirical studies show that our model outperforms the state-of-the-art model on a word sense induction task by a 13% relative gain. 
Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest. Most existing methods perform the embedding task using only fact triples. Logical rules, although containing rich background information, have not been well studied in this task. This paper proposes a novel method of jointly embedding knowledge graphs and logical rules. The key idea is to represent and model triples and rules in a uniÔ¨Åed framework. SpeciÔ¨Åcally, triples are represented as atomic formulae and modeled by the translation assumption, while rules represented as complex formulae and modeled by t-norm fuzzy logics. Embedding then amounts to minimizing a global loss over both atomic and complex formulae. In this manner, we learn embeddings compatible not only with triples but also with rules, which will certainly be more predictive for knowledge acquisition and inference. We evaluate our method with link prediction and triple classiÔ¨Åcation tasks. Experimental results show that joint embedding brings significant and consistent improvements over stateof-the-art methods. Particularly, it enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of our method to learn more predictive embeddings. 
We introduce a simple semi-supervised approach to improve implicit discourse relation identiÔ¨Åcation. This approach harnesses large amounts of automatically extracted discourse connectives along with their arguments to construct new distributional word representations. SpeciÔ¨Åcally, we represent words in the space of discourse connectives as a way to directly encode their rhetorical function. Experiments on the Penn Discourse Treebank demonstrate the effectiveness of these task-tailored representations in predicting implicit discourse relations. Our results indeed show that, despite their simplicity, these connective-based representations outperform various off-the-shelf word embeddings, and achieve state-of-the-art performance on this problem. 
We introduce a deep memory network for aspect level sentiment classiÔ¨Åcation. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation. 
It is well-known that opinions have targets. Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use. So far many algorithms have been proposed to extract opinion targets. However, an opinion target can be an entity or an aspect (part or attribute) of an entity. An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that speciÔ¨Åc attribute or aspect of an entity. Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions. This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling. Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly. 
In this paper, we study cross-domain sentiment classiÔ¨Åcation with neural network architectures. We borrow the idea from Structural Correspondence Learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classiÔ¨Åcation. We also propose to jointly learn this sentence embedding together with the sentiment classiÔ¨Åer itself. Experiment results demonstrate that our proposed joint model outperforms several state-of-theart methods on Ô¨Åve benchmark datasets. 
Most of the state-of-the-art sentiment classiÔ¨Åcation methods are based on supervised learning algorithms which require large amounts of manually labeled data. However, the labeled resources are usually imbalanced in different languages. Cross-lingual sentiment classiÔ¨Åcation tackles the problem by adapting the sentiment resources in a resource-rich language to resource-poor languages. In this study, we propose an attention-based bilingual representation learning model which learns the distributed semantics of the documents in both the source and the target languages. In each language, we use Long Short Term Memory (LSTM) network to model the documents, which has been proved to be very effective for word sequences. Meanwhile, we propose a hierarchical attention mechanism for the bilingual LSTM network. The sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive. The proposed model achieves good results on a benchmark dataset using English as the source language and Chinese as the target language. 
Within the Ô¨Åeld of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the Ô¨Årst technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-ofthe-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural vs. phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the Ô¨Årst time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models ‚Äì such as the reordering of verbs ‚Äì while pointing out other aspects that remain to be improved. 
In this paper, we propose a novel Ô¨Ånetuning algorithm for the recently introduced multiway, multilingual neural machine translate that enables zero-resource machine translation. When used together with novel manyto-one translation strategies, we empirically show that this Ô¨Ånetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters. 
We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called MEMDEC. At each time during decoding, MEMDEC will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with predetermined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by 4.8 BLEU upon Groundhog and 5.3 BLEU upon on Moses, yielding the best performance achieved with the same training set. 
We propose a fast and scalable method for semi-supervised learning of sequence models, based on anchor words and moment matching. Our method can handle hidden Markov models with feature-based log-linear emissions. Unlike other semi-supervised methods, no decoding passes are necessary on the unlabeled data and no graph needs to be constructed‚Äî only one pass is necessary to collect moment statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 
Automatically solving algebra word problems has raised considerable interest recently. Existing state-of-the-art approaches mainly rely on learning from human annotated equations. In this paper, we demonstrate that it is possible to efÔ¨Åciently mine algebra problems and their numerical solutions with little to no manual effort. To leverage the mined dataset, we propose a novel structured-output learning algorithm that aims to learn from both explicit (e.g., equations) and implicit (e.g., solutions) supervision signals jointly. Enabled by this new algorithm, our model gains 4.6% absolute improvement in accuracy on the ALG514 benchmark compared to the one without using implicit supervision. The Ô¨Ånal model also outperforms the current state-of-the-art approach by 3%. 
 We describe TweeTIME, a temporal tagger for recognizing and normalizing time expressions in Twitter. Most previous work in social media analysis has to rely on temporal resolvers that are designed for well-edited text, and therefore suffer from reduced performance due to domain mismatch. We present a minimally supervised method that learns from large quantities of unlabeled data and requires no hand-engineered rules or hand-annotated training corpora. TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date expressions, outperforming a broad range of state-of-the-art systems.1 
In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model Ô¨Årst draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data. 
Recurrent neural networks can generate locally coherent text but often have difÔ¨Åculties representing what has already been generated and what still needs to be said ‚Äì especially when constructing long texts. We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output. The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items. Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda. 
We introduce a manually-created, multireference dataset for abstractive sentence and short paragraph compression. First, we examine the impact of single- and multi-sentence level editing operations on human compression quality as found in this corpus. We observe that substitution and rephrasing operations are more meaning preserving than other operations, and that compressing in context improves quality. Second, we systematically explore the correlations between automatic evaluation metrics and human judgments of meaning preservation and grammaticality in the compression task, and analyze the impact of the linguistic units used and precision versus recall measures on the quality of the metrics. Multi-reference evaluation metrics are shown to offer signiÔ¨Åcant advantage over single reference-based metrics. 
In this paper we present PaCCSS‚ÄìIT, a Parallel Corpus of Complex‚ÄìSimple Sentences for ITalian. To build the resource we develop a new method for automatically acquiring a corpus of complex‚Äìsimple paired sentences able to intercept structural transformations and particularly suitable for text simpliÔ¨Åcation. The method requires a wide amount of texts that can be easily extracted from the web making it suitable also for less‚Äìresourced languages. We test it on the Italian language making available the biggest Italian corpus for automatic text simpliÔ¨Åcation. 
RST-style document-level discourse parsing remains a difÔ¨Åcult task and efÔ¨Åcient deep learning models on this task have rarely been presented. In this paper, we propose an attention-based hierarchical neural network model for discourse parsing. We also incorporate tensor-based transformation function to model complicated feature interactions. Experimental results show that our approach obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering. 
In this paper, we study the task of response selection for multi-turn human-computer conversation. Previous approaches take word as a unit and view context and response as sequences of words. This kind of approaches do not explicitly take each utterance as a unit, therefore it is difÔ¨Åcult to catch utterancelevel discourse information and dependencies. In this paper, we propose a multi-view response selection model that integrates information from two different views, i.e., word sequence view and utterance sequence view. We jointly model the two views via deep neural networks. Experimental results on a public corpus for context-sensitive response selection demonstrate the effectiveness of the proposed multi-view model, which signiÔ¨Åcantly outperforms other single-view baselines. 
Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efÔ¨Åcient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a reparameterized variational lower bound. This allows VarNDRR to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VarNDRR can achieve comparable results against stateof-the-art baselines without using any manual features. 
An important aspect of natural language understanding involves recognizing and categorizing events and the relations among them. However, these tasks are quite subtle and annotating training data for machine learning based approaches is an expensive task, resulting in supervised systems that attempt to learn complex models from small amounts of data, which they over-Ô¨Åt. This paper addresses this challenge by developing an event detection and co-reference system with minimal supervision, in the form of a few event examples. We view these tasks as semantic similarity problems between event mentions or event mentions and an ontology of types, thus facilitating the use of large amounts of out of domain text data. Notably, our semantic relatedness function exploits the structure of the text by making use of a semantic-role-labeling based representation of an event. We show that our approach to event detection is competitive with the top supervised methods. More signiÔ¨Åcantly, we outperform stateof-the-art supervised methods for event coreference on benchmark data sets, and support signiÔ¨Åcantly better transfer across domains. 
Taxonomic relation identiÔ¨Åcation aims to recognize the ‚Äòis-a‚Äô relation between two terms. Previous works on identifying taxonomic relations are mostly based on statistical and linguistic approaches, but the accuracy of these approaches is far from satisfactory. In this paper, we propose a novel supervised learning approach for identifying taxonomic relations using term embeddings. For this purpose, we Ô¨Årst design a dynamic weighting neural network to learn term embeddings based on not only the hypernym and hyponym terms, but also the contextual information between them. We then apply such embeddings as features to identify taxonomic relations using a supervised method. The experimental results show that our proposed approach signiÔ¨Åcantly outperforms other state-of-the-art methods by 9% to 13% in terms of accuracy for both general and speciÔ¨Åc domain datasets. 
Given a set of documents from a speciÔ¨Åc domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identiÔ¨Åcation of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important Ô¨Årst step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the Ô¨Årst application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we Ô¨Ånd that SICTF is not only more accurate than state-of-the-art baselines, but also signiÔ¨Åcantly faster (about 14x faster). 
Lexical taxonomies are graph-like hierarchical structures that provide a formal representation of knowledge. Most knowledge graphs to date rely on is-a (hypernymic) relations as the backbone of their semantic structure. In this paper, we propose a supervised distributional framework for hypernym discovery which operates at the sense level, enabling large-scale automatic acquisition of disambiguated taxonomies. By exploiting semantic regularities between hyponyms and hypernyms in embeddings spaces, and integrating a domain clustering algorithm, our model becomes sensitive to the target data. We evaluate several conÔ¨Ågurations of our approach, training with information derived from a manually created knowledge base, along with hypernymic relations obtained from Open Information Extraction systems. The integration of both sources of knowledge yields the best overall results according to both automatic and manual evaluation on ten different domains. 
In this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles. The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling. We introduce two algorithms to infer a tree for a given sentence. The Ô¨Årst one is based on Gibbs sampling. It is fast, but does not guarantee to Ô¨Ånd the most probable tree. The second one is based on dynamic programming. It is slower, but guarantees to Ô¨Ånd the most probable tree. We provide comparison of both algorithms. We combine LTLM with 4-gram ModiÔ¨Åed Kneser-Ney language model via linear interpolation. Our experiments with English and Czech corpora show signiÔ¨Åcant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram ModiÔ¨Åed Kneser-Ney language model. 
Multi-modal distributional models learn grounded representations for improved performance in semantics. Deep visual representations, learned using convolutional neural networks, have been shown to achieve particularly high performance. In this study, we systematically compare deep visual representation learning techniques, experimenting with three well-known network architectures. In addition, we explore the various data sources that can be used for retrieving relevant images, showing that images from search engines perform as well as, or better than, those from manually crafted resources such as ImageNet. Furthermore, we explore the optimal number of images and the multi-lingual applicability of multi-modal semantics. We hope that these Ô¨Åndings can serve as a guide for future research in the Ô¨Åeld. 
Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efÔ¨Åciently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the beneÔ¨Åt of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge. 
We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a new structured prediction algorithm that generalizes the Collins Structured Perceptron (CSP, (Collins, 2002)). Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels. We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results. 
Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overÔ¨Åtting. In some Ô¨Åelds like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.1 
Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difÔ¨Åcult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a uniÔ¨Åed probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging. 
Structural isomorphism between languages beneÔ¨Åts the performance of cross-lingual applications. We propose an automatic algorithm for cross-lingual similarization of dependency grammars, which automatically learns grammars with high cross-lingual similarity. The algorithm similarizes the annotation styles of the dependency grammars for two languages in the level of classiÔ¨Åcation decisions, and gradually improves the cross-lingual similarity without losing linguistic knowledge resorting to iterative crosslingual cooperative learning. The dependency grammars given by cross-lingual similarization have much higher cross-lingual similarity while maintaining non-triviality. As applications, the cross-lingually similarized grammars signiÔ¨Åcantly improve the performance of dependency tree-based machine translation. 
Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations. However, crowdsourced judgments are often biased and inaccurate. In this paper, we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system‚Äôs performance. Our method applies graded response model from item response theory (IRT), which was originally developed for academic tests. We conducted experiments on a public dataset from the Workshop on Statistical Machine Translation 2013, and found that our approach resulted in highly interpretable estimates and was less affected by noisy judges than previously proposed methods. 
Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efÔ¨Åcient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and EnglishGerman translation tasks show that the proposed variational neural machine translation achieves signiÔ¨Åcant improvements over the vanilla neural machine translation baselines. 
Among the alignment models used in statistical machine translation (SMT), the hidden Markov model (HMM) is arguably the most elegant: it performs consistently better than IBM Model 3 and is very close in performance to the much more complex IBM Model 4. In this paper we discuss a model which combines the structure of the HMM and IBM Model 2. Using this surrogate, our experiments show that we can attain a similar level of alignment quality as the HMM model implemented in GIZA++ (Och and Ney, 2003). For this model, we derive its convex relaxation and show that it too has strong performance despite not having the local optima problems of non-convex objectives. In particular, the word alignment quality of this new convex model is signiÔ¨Åcantly above that of the standard IBM Models 2 and 3, as well as the popular (and still non-convex) IBM Model 2 variant of (Dyer et al., 2013). 
Verbal comprehension questions appear very frequently in Intelligence Quotient (IQ) tests, which measure human‚Äôs verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by the deep learning technologies for text data. We found that the task was quite challenging, and simply applying existing technologies like word embedding could not achieve a good performance, due to the multiple senses of words and the complex relations among words. To tackle these challenges, we propose a novel framework to automatically solve the verbal IQ questions by leveraging improved word embedding by jointly considering the multi-sense nature of words and the relational information among words. Experimental results have shown that the proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study. 
In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art. 
We present a semi-automated framework for constructing factoid question answering (QA) datasets, where an array of question characteristics are formalized, including structure complexity, function, commonness, answer cardinality, and paraphrasing. Instead of collecting questions and manually characterizing them, we employ a reverse procedure, Ô¨Årst generating a kind of graph-structured logical forms from a knowledge base, and then converting them into questions. Our work is the Ô¨Årst to generate questions with explicitly speciÔ¨Åed characteristics for QA evaluation. We construct a new QA dataset with over 5,000 logical form-question pairs, associated with answers from the knowledge base, and show that datasets constructed in this way enable Ô¨Ånegrained analyses of QA systems. The dataset can be found in https://github.com/ ysu1989/GraphQuestions. 
In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, contextbased, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p < 0.05): translating all text into English, then training a classiÔ¨Åer based only on English (original or translated) text. 
We study the problem of identifying individuals based on their characteristic gaze patterns during reading of arbitrary text. The motivation for this problem is an unobtrusive biometric setting in which a user is observed during access to a document, but no speciÔ¨Åc challenge protocol requiring the user‚Äôs time and attention is carried out. Existing models of individual differences in gaze control during reading are either based on simple aggregate features of eye movements, or rely on parametric density models to describe, for instance, saccade amplitudes or word Ô¨Åxation durations. We develop Ô¨Çexible semiparametric models of eye movements during reading in which densities are inferred under a Gaussian process prior centered at a parametric distribution family that is expected to approximate the true distribution well. An empirical study on reading data from 251 individuals shows signiÔ¨Åcant improvements over the state of the art. 
A word‚Äôs sentiment depends on the domain in which it is used. Computational social science research thus requires sentiment lexicons that are speciÔ¨Åc to the domains being studied. We combine domain-speciÔ¨Åc word embeddings with a label propagation framework to induce accurate domain-speciÔ¨Åc sentiment lexicons using small sets of seed words, achieving state-of-the-art performance competitive with approaches that rely on hand-curated resources. Using our framework we perform two large-scale empirical studies to quantify the extent to which sentiment varies across time and between communities. We induce and release historical sentiment lexicons for 150 years of English and community-speciÔ¨Åc sentiment lexicons for 250 online communities from the social media forum Reddit. The historical lexicons show that more than 5% of sentiment-bearing (non-neutral) English words completely switched polarity during the last 150 years, and the community-speciÔ¨Åc lexicons highlight how sentiment varies drastically between different communities. 
Opinionated expression extraction is a central problem in Ô¨Åne-grained sentiment analysis. Most existing works focus on either generic subjective expression or aspect expression extraction. However, in opinion mining, it is often desirable to mine the aspect speciÔ¨Åc opinion expressions (or aspectsentiment phrases) containing both the aspect and the opinion. This paper proposes a hybrid generative-discriminative framework for extracting such expressions. The hybrid model consists of (i) an unsupervised generative component for modeling the semantic coherence of terms (words/phrases) based on their collocations across different documents, and (ii) a supervised discriminative sequence modeling component for opinion phrase extraction. Experimental results using Amazon.com reviews demonstrate the effectiveness of the approach that signiÔ¨Åcantly outperforms several state-of-the-art baselines. 
The advent of social media and its prosperity enable users to share their opinions and views. Understanding users‚Äô emotional states might provide the potential to create new business opportunities. Automatically identifying users‚Äô emotional states from their texts and classifying emotions into Ô¨Ånite categories such as joy, anger, disgust, etc., can be considered as a text classiÔ¨Åcation problem. However, it introduces a challenging learning scenario where multiple emotions with different intensities are often found in a single sentence. Moreover, some emotions co-occur more often while other emotions rarely coexist. In this paper, we propose a novel approach based on emotion distribution learning in order to address the aforementioned issues. The key idea is to learn a mapping function from sentences to their emotion distributions describing multiple emotions and their respective intensities. Moreover, the relations of emotions are captured based on the Plutchik‚Äôs wheel of emotions and are subsequently incorporated into the learning algorithm in order to improve the accuracy of emotion detection. Experimental results show that the proposed approach can effectively deal with the emotion distribution detection problem and perform remarkably better than both the state-of-theart emotion detection method and multi-label learning methods. 
Evaluation of NLP methods requires testing against a previously vetted gold-standard test set and reporting standard metrics (accuracy/precision/recall/F1). The current assumption is that all items in a given test set are equal with regards to difÔ¨Åculty and discriminating power. We propose Item Response Theory (IRT) from psychometrics as an alternative means for gold-standard test-set generation and NLP system evaluation. IRT is able to describe characteristics of individual items their difÔ¨Åculty and discriminating power - and can account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we demonstrate IRT by generating a gold-standard test set for Recognizing Textual Entailment. By collecting a large number of human responses and Ô¨Åtting our IRT model, we show that our IRT model compares NLP systems with the performance in a human population and is able to provide more insight into system performance than standard evaluation metrics. We show that a high accuracy score does not always imply a high IRT score, which depends on the item characteristics and the response pattern.1 
Embedding words in a vector space has gained a lot of attention in recent years. While stateof-the-art methods provide efÔ¨Åcient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efÔ¨Åciently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a signiÔ¨Åcant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage. 
In this paper, we utilize distributed word representations (i.e., word embeddings) to analyse the representation of semantics in brain activity. The brain activity data were recorded using functional magnetic resonance imaging (fMRI) when subjects were viewing words. First, we analysed the functional selectivity of different cortex areas by calculating the correlations between neural responses and several types of word representations, including skipgram word embeddings, visual semantic vectors, and primary visual features. The results demonstrated consistency with existing neuroscientiÔ¨Åc knowledge. Second, we utilized behavioural data as the semantic ground truth to measure their relevance with brain activity. A method to estimate word embeddings under the constraints of brain activity similarities is further proposed based on the semantic word embedding (SWE) model. The experimental results show that the brain activity data are signiÔ¨Åcantly correlated with the behavioural data of human judgements on semantic similarity. The correlations between the estimated word embeddings and the semantic ground truth can be effectively improved after integrating the brain activity data for learning, which implies that semantic patterns in neural representations may exist that have not been fully captured by state-of-the-art word embeddings derived from text corpora. 
To alleviate the error propagation in the traditional pipelined models for Abstract Meaning Representation (AMR) parsing, we formulate AMR parsing as a joint task that performs the two subtasks: concept identiÔ¨Åcation and relation identiÔ¨Åcation simultaneously. To this end, we Ô¨Årst develop a novel componentwise beam search algorithm for relation identiÔ¨Åcation in an incremental fashion, and then incorporate the decoder into a uniÔ¨Åed framework based on multiple-beam search, which allows for the bi-directional information Ô¨Çow between the two subtasks in a single incremental model. Experiments on the public datasets demonstrate that our joint model signiÔ¨Åcantly outperforms the previous pipelined counterparts, and also achieves better or comparable performance than other approaches to AMR parsing, without utilizing external semantic resources. 
We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-conÔ¨Ådence), Ô¨Ånding they have predictive power. We also Ô¨Ånd evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we Ô¨Ånd evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves. 
Search personalization that considers the social dimension of the web has attracted a significant volume of research in recent years. A user profile is usually needed to represent a user‚Äôs interests in order to tailor future searches. Previous research has typically constructed a profile solely from a user‚Äôs usage information. When the user has only limited activities in the system, the effect of the user profile on search is also constrained. This research addresses the setting where a user has only a limited amount of usage information. We build enhanced user profiles from a set of annotations and resources that users have marked, together with an external knowledge base constructed according to usage histories. We present two probabilistic latent topic models to simultaneously incorporate social annotations, documents and the external knowledge base. Our web search strategy is achieved using personalized social query expansion. We introduce a topical query expansion model to enhance the search by utilizing individual user profiles. The proposed approaches have been intensively evaluated on a large public social annotation dataset. Results show that our models significantly outperformed existing personalized query expansion methods which use user profiles solely built from past usage information in personalized search. 
Chinese named entities occur frequently in formal and informal environments. Various approaches have been formalized the problem as a sequence labelling task and utilize a character-based methodology, in which character is treated as the basic classiÔ¨Åcation unit. One of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of Chinese. To address this problem, we propose a novel neural network architecture to perform task. It combines recurrent neural network (RNN) with an architecture determining whether a given sequence of characters can be a word or not. For demonstrating the effectiveness of the proposed method, we evaluate it on Chinese named entity generation and opinion target extraction tasks. Experimental results show that the proposed method can achieve better performance than state-ofthe-art methods. 
Multiple treebanks annotated under heterogeneous standards give rise to the research question of best utilizing multiple resources for improving statistical models. Prior research has focused on discrete models, leveraging stacking and multi-view learning to address the problem. In this paper, we empirically investigate heterogeneous annotations using neural network models, building a neural network counterpart to discrete stacking and multiview learning, respectively, Ô¨Ånding that neural models have their unique advantages thanks to the freedom from manual feature engineering. Neural model achieves not only better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 
This paper introduces STEM and LAMB, embeddings trained for stems and lemmata instead of for surface forms. For morphologically rich languages, they perform significantly better than standard embeddings on word similarity and polarity evaluations. On a new WordNet-based evaluation, STEM and LAMB are up to 50% better than standard embeddings. We show that both embeddings have high quality even for small dimensionality and training corpora. 
The recently proposed coupled sequence labeling is shown to be able to effectively exploit multiple labeled data with heterogeneous annotations but suffer from severe inefficiency problem due to the large bundled tag space (Li et al., 2015). In their case study of part-ofspeech (POS) tagging, Li et al. (2015) manually design context-free tag-to-tag mapping rules with a lot of effort to reduce the tag space. This paper proposes a context-aware pruning approach that performs token-wise constraints on the tag space based on contextual evidences, making the coupled approach efficient enough to be applied to the more complex task of joint word segmentation (WS) and POS tagging for the first time. Experiments show that using the large-scale People Daily as auxiliary heterogeneous data, the coupled approach can improve F-score by 95.55 ‚àí 94.88 = 0.67% on WS, and by 90.58 ‚àí 89.49 = 1.09% on joint WS&POS on Penn Chinese Treebank. All codes are released at http://hlt.suda.edu.cn/~zhli. 
Unsupervised dependency parsing aims to learn a dependency grammar from text annotated with only POS tags. Various features and inductive biases are often used to incorporate prior knowledge into learning. One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags. Previous work employed manually designed features or special prior distributions to encode such information. In this paper, we propose a novel approach to unsupervised dependency parsing that uses a neural model to predict grammar rule probabilities based on distributed representation of POS tags. The distributed representation is automatically learned from data and captures the correlations between POS tags. Our experiments show that our approach outperforms previous approaches utilizing POS correlations and is competitive with recent state-of-the-art approaches on nine different languages. 
Previous work on automatic summarization does not thoroughly consider coherence while generating the summary. We introduce a graph-based approach to summarize scientiÔ¨Åc articles. We employ coherence patterns to ensure that the generated summaries are coherent. The novelty of our model is twofold: we mine coherence patterns in a corpus of abstracts, and we propose a method to combine coherence, importance and non-redundancy to generate the summary. We optimize these factors simultaneously using Mixed Integer Programming. Our approach signiÔ¨Åcantly outperforms baseline and state-of-the-art systems in terms of coherence (summary coherence assessment) and relevance (ROUGE scores). 
This paper studies summarizing key information from news streams. We propose simple yet effective models to solve the problem based on a novel and promising representation of text streams ‚Äì Burst Information Networks (BINets). A BINet can be aware of redundant information, allows global analysis of a text stream, and can be efÔ¨Åciently built and dynamically updated, which perfectly Ô¨Åts the demands of text stream summarization. Extensive experiments show that the BINet-based approaches are not only efÔ¨Åcient and can be used in a real-time online summarization setting, but also can generate high-quality summaries, outperforming the state-of-the-art approach. 
We present a new Convolutional Neural Network (CNN) model for text classiÔ¨Åcation that jointly exploits labels on documents and their constituent sentences. SpeciÔ¨Åcally, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on Ô¨Åve classiÔ¨Åcation datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions. 
In this work, we investigate the possibility of cross-website transfer learning for tackling the cold-start problem. To address the coldstart issues commonly present in a collaborative ltering (CF) system, most existing crossdomain CF models require auxiliary rating data from another domain; nevertheless, under the cross-website scenario, such data is often unobtainable. Therefore, we propose the nearest-neighbor transfer matrix factorization (NT-MF) model, where a topic model is applied to the unstructured user-generated content in the source domain, and the similarity between users in the latent topic space is utilized to guide the target-domain CF model. Specically, the latent factors of the nearestneighbors are regarded as a set of pseudo observations, which can be used to estimate the unknown parameters in the model. Improvement over previous methods, especially for the cold-start users, is demonstrated with experiments on a real-world cross-website dataset. 
Speculation and negation are important information to identify text factuality. In this paper, we propose a Convolutional Neural Network (CNN)-based model with probabilistic weighted average pooling to address speculation and negation scope detection. In particular, our CNN-based model extracts those meaningful features from various syntactic paths between the cues and the candidate tokens in both constituency and dependency parse trees. Evaluation on BioScope shows that our CNN-based model significantly outperforms the state-ofthe-art systems on Abstracts, a sub-corpus in BioScope, and achieves comparable performances on Clinical Records, another subcorpus in BioScope. 
Sentence modelling is a fundamental topic in computational linguistics. Recently, deep learning-based sequential models of sentence, such as recurrent neural network, have proved to be effective in dealing with the non-sequential properties of human language. However, little is known about how a recurrent neural network captures linguistic knowledge. Here we propose to correlate the neuron activation pattern of a LSTM language model with rich language features at sequential, lexical and compositional level. Qualitative visualization as well as quantitative analysis under multilingual perspective reveals the effectiveness of gate neurons and indicates that LSTM learns to allow different neurons selectively respond to linguistic knowledge at different levels. Cross-language evidence shows that the model captures different aspects of linguistic properties for different languages due to the variance of syntactic complexity. Additionally, we analyze the inÔ¨Çuence of modelling strategy on linguistic knowledge encoded implicitly in different sequential models. 
Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas. The task of automatically extracting them have received considerable attention in recent decades. Different from previous studies, which are usually focused on automatically extracting keyphrases from documents or articles, in this study, we considered the problem of automatically extracting keyphrases from tweets. Because of the length limitations of Twitter-like sites, the performances of existing methods usually drop sharply. We proposed a novel deep recurrent neural network (RNN) model to combine keywords and context information to perform this problem. To evaluate the proposed method, we also constructed a large-scale dataset collected from Twitter. The experimental results showed that the proposed method performs signiÔ¨Åcantly better than previous methods. 
Chinese character riddle is a riddle game in which the riddle solution is a single Chinese character. It is closely connected with the shape, pronunciation or meaning of Chinese characters. The riddle description (sentence) is usually composed of phrases with rich linguistic phenomena (such as pun, simile, and metaphor), which are associated to different parts (namely radicals) of the solution character. In this paper, we propose a statistical framework to solve and generate Chinese character riddles. SpeciÔ¨Åcally, we learn the alignments and rules to identify the metaphors between phrases in riddles and radicals in characters. Then, in the solving phase, we utilize a dynamic programming method to combine the identiÔ¨Åed metaphors to obtain candidate solutions. In the riddle generation phase, we use a template-based method and a replacement-based method to obtain candidate riddle descriptions. We then use Ranking SVM to rerank the candidates both in the solving and generation process. Experimental results in the solving task show that the proposed method outperforms baseline methods. We also get very promising results in the generation task according to human judges.  and a corresponding solution. The character riddle is one of the most popular forms of various riddles in which the riddle solution is a single Chinese character. While English words are strings of letters together, Chinese characters are composed of radicals that associate with meaning or metaphor. In other words, Chinese characters are usually positioned into some common structures, such as upperlower structure, left-right structure, inside-outside structure, which means they can be decomposed into other characters or radicals. For example, ‚ÄúÂ•Ω‚Äù (good), a character with left-right structure, can be decomposed into ‚ÄúÂ•≥‚Äù (daughter) and ‚ÄúÂ≠ê‚Äù (son). As illustrated in Figure 1(a), the left part of ‚ÄúÂ•Ω‚Äù is ‚ÄúÂ•≥‚Äù and the right part is ‚ÄúÂ≠ê‚Äù. ‚ÄúÂ•≥‚Äù and ‚ÄúÂ≠ê‚Äù are called the ‚Äúradical‚Äù of ‚ÄúÂ•Ω‚Äù. Figure 1(b) is another example of the character ‚ÄúÊÄù‚Äù (miss) with an upper-lower structure.  Â•Ω good  Â•≥Â≠ê  daughter  son  Áî∞ ÊÄù field  miss  ÂøÉ  heart  (a) Left-Right Structure  (b) Upper-Lower Structure  Figure 1: Examples of the structure of Chinese characters  
Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major application of sequence labeling involves extraction of relevant entities such as medication, indication, and side-effects from Electronic Health Record Narratives. Sequence labeling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recurrent Neural Networks. We extend the previously studied CRF-LSTM model with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methods1 for structured prediction in order to improve the exact phrase detection of clinical entities. 
Review spam detection is a key task in opinion mining. To accomplish this type of detection, previous work has focused mainly on effectively representing fake and non-fake reviews with discriminative features, which are discovered or elaborately designed by experts or developers. This paper proposes a novel review spam detection method that learns the representation of reviews automatically instead of heavily relying on experts‚Äô knowledge in a data-driven manner. More speciÔ¨Åcally, according to 11 relations (generated automatically from two basic patterns) between reviewers and products, we employ tensor decomposition to learn the embeddings of the reviewers and products in a vector space. We collect relations between any two entities (reviewers and products), which results in much useful and global information. We concatenate the review text, the embeddings of the reviewer and the reviewed product as the representation of a review. Based on such representations, the classiÔ¨Åer could identify the opinion spam more precisely. Experimental results on an open Yelp dataset show that our method could effectively enhance the spam detection accuracy compared with the stateof-the-art methods. 
Convolutional neural networks (CNN) have achieved the top performance for event detection due to their capacity to induce the underlying structures of the k-grams in the sentences. However, the current CNN-based event detectors only model the consecutive k-grams and ignore the non-consecutive kgrams that might involve important structures for event detection. In this work, we propose to improve the current CNN models for ED by introducing the non-consecutive convolution. Our systematic evaluation on both the general setting and the domain adaptation setting demonstrates the effectiveness of the nonconsecutive CNN model, leading to the significant performance improvement over the current state-of-the-art systems. 
Many downstream NLP tasks can beneÔ¨Åt from Open Information Extraction (Open IE) as a semantic representation. While Open IE systems are available for English, many other languages lack such tools. In this paper, we present a straightforward approach for adapting PropS, a rule-based predicate-argument analysis for English, to a new language, German. With this approach, we quickly obtain an Open IE system for German covering 89% of the English rule set. It yields 1.6 n-ary extractions per sentence at 60% precision, making it comparable to systems for English and readily usable in downstream applications.1 
In named entity recognition, we often don‚Äôt have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-speciÔ¨Åc NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch. 
We present our pilot research on automatically extracting subevents from a domain-speciÔ¨Åc corpus, focusing on the type of subevents that describe physical actions composing an event. We decompose the challenging problem and propose a two-phase approach that effectively captures sentential and local cues that describe subevents. We extracted a rich set of over 600 novel subevent phrases. Evaluation shows the automatically learned subevents help to discover 10% additional main events (of which the learned subevents are a part) and improve event detection performance. 
An exciting outcome of research at the intersection of language and vision is that of zeroshot learning (ZSL). ZSL promises to scale visual recognition by borrowing distributed semantic models learned from linguistic corpora and turning them into visual recognition models. However the popular word-vector DSM embeddings are relatively impoverished in their expressivity as they model each word as a single vector point. In this paper we explore word-distribution embeddings for ZSL. We present a visual-linguistic mapping for ZSL in the case where words and visual categories are both represented by distributions. Experiments show improved results on ZSL benchmarks due to this better exploiting of intra-concept variability in each modality 
 Non-Visual Visual False-Premise Visual True-Premise  Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g., What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we Ô¨Årst determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outperform strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent, reasonable, and human-like. 
 Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing ‚Äì given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense. 
We conduct large-scale studies on ‚Äòhuman attention‚Äô in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current VQA attention models do not seem to be looking at the same regions as humans. 
In this paper, we explore the possibility of leveraging Residual Networks (ResNet), a powerful structure in constructing extremely deep neural network for image understanding, to improve recurrent neural networks (RNN) for modeling sequential data. We show that for sequence classiÔ¨Åcation tasks, incorporating residual connections into recurrent structures yields similar accuracy to Long Short Term Memory (LSTM) RNN with much fewer model parameters. In addition, we propose two novel models which combine the best of both residual learning and LSTM. Experiments show that the new models signiÔ¨Åcantly outperform LSTM. 
In this work we present a generalisation of the ModiÔ¨Åed Kneser-Ney interpolative smoothing for richer smoothing via additional discount parameters. We provide mathematical underpinning for the estimator of the new discount parameters, and showcase the utility of our rich MKN language models on several European languages. We further explore the interdependency among the training data size, language model order, and number of discount parameters. Our empirical results illustrate that larger number of discount parameters, i) allows for better allocation of mass in the smoothing process, particularly on small data regime where statistical sparsity is severe, and ii) leads to signiÔ¨Åcant reduction in perplexity, particularly for out-of-domain test sets which introduce higher ratio of out-ofvocabulary words.1 
We propose a domain adaptation framework, and formally prove that it generalizes the feature augmentation technique in (Daume¬¥ III, 2007) and the multi-task regularization framework in (Evgeniou and Pontil, 2004). We show that our framework is strictly more general than these approaches and allows practitioners to tune hyper-parameters to encourage transfer between close domains and avoid negative transfer between distant ones. 
In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality signiÔ¨Åcantly on various test sets over the strong large vocabulary NMT system. 
Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoderdecoder model for this task. Additionally, we extend our model to include morphemelevel and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian. 
We investigate mutual beneÔ¨Åts between syntax and semantic roles using neural network models, by studying a parsing‚ÜíSRL pipeline, a SRL‚Üíparsing pipeline, and a simple joint model by embedding sharing. The integration of syntactic and semantic features gives promising results in a Chinese Semantic Treebank, demonstrating large potentials of neural models for joint parsing and semantic role labeling. 
This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model. 
This paper considers the problem of learning Chinese word embeddings. In contrast to English, a Chinese word is usually composed of characters, and most of the characters themselves can be further divided into components such as radicals. While characters and radicals contain rich information and are capable of indicating semantic meanings of words, they have not been fully exploited by existing word embedding methods. In this work, we propose multi-granularity embedding (MGE) for Chinese words. The key idea is to make full use of such word-character-radical composition, and enrich word embeddings by further incorporating Ô¨Åner-grained semantics from characters and radicals. Quantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify Ô¨Åner-grained semantic meanings of words. 
 Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements. 
Annotation projection based on parallel corpora has shown great promise in inexpensively creating Proposition Banks for languages for which high-quality parallel corpora and syntactic parsers are available. In this paper, we present an experimental study where we apply this approach to three languages that lack such resources: Tamil, Bengali and Malayalam. We Ô¨Ånd an average quality difference of 6 to 20 absolute F-measure points vis-avis high-resource languages, which indicates that annotation projection alone is insufÔ¨Åcient in low-resource scenarios. Based on these results, we explore the possibility of using annotation projection as a starting point for inexpensive data curation involving both experts and non-experts. We give an outline of what such a process may look like and present an initial study to discuss its potential and challenges. 
Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classiÔ¨Åed independently, even though they form part of a review‚Äôs argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classiÔ¨Åcation of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on Ô¨Åve multilingual, multi-domain datasets without any handengineered features or external resources. 
This paper makes a simple increment to state-ofthe-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their beneÔ¨Åt to sarcasm detection. 
Supervised stance classiÔ¨Åcation, in such domains as Congressional debates and online forums, has been a topic of interest in the past decade. Approaches have evolved from text classiÔ¨Åcation to structured output prediction, including collective classiÔ¨Åcation and sequence labeling. In this work, we investigate collective classiÔ¨Åcation of stances on Twitter, using hinge-loss Markov random Ô¨Åelds (HLMRFs). Given the graph of all posts, users, and their relationships, we constrain the predicted post labels and latent user labels to correspond with the network structure. We focus on a weakly supervised setting, in which only a small set of hashtags or phrases is labeled. Using our relational approach, we are able to go beyond the stance-indicative patterns and harvest more stance-indicative tweets, which can also be used to train any linear text classiÔ¨Åer when the network structure is not available or is costly. 
We argue that NLP researchers are especially well-positioned to contribute to the national discussion about gun violence. Reasoning about the causes and outcomes of gun violence is typically dominated by politics and emotion, and data-driven research on the topic is stymied by a shortage of data and a lack of federal funding. However, data abounds in the form of unstructured text from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem. 
When considering a social media corpus, we often have access to structural information about how messages are Ô¨Çowing between people or organizations. This information is particularly useful when the linguistic evidence is sparse, incomplete, or of dubious quality. In this paper we construct a simple model to leverage the structure of Twitter data to help determine the set of languages each user is Ô¨Çuent in. Our results demonstrate that imposing several intuitive constraints leads to improvements in performance and stability. We release the Ô¨Årst annotated data set for exploring this task, and discuss how our approach may be extended to other applications. 
This work investigates style and topic aspects of language in online communities: looking at both utility as an identiÔ¨Åer of the community and correlation with community reception of content. Style is characterized using a hybrid word and part-of-speech tag n-gram language model, while topic is represented using Latent Dirichlet Allocation. Experiments with several Reddit forums show that style is a better indicator of community identity than topic, even for communities organized around speciÔ¨Åc topics. Further, there is a positive correlation between the community reception to a contribution and the style similarity to that community, but not so for topic similarity. 
Joint dependency parsing with disÔ¨Çuency detection is an important task in speech language processing. Recent methods show high performance for this task, although most authors make the unrealistic assumption that input texts are transcribed by human annotators. In real-world applications, the input text is typically the output of an automatic speech recognition (ASR) system, which implies that the text contains not only disÔ¨Çuency noises but also recognition errors from the ASR system. In this work, we propose a parsing method that handles both disÔ¨Çuency and ASR errors using an incremental shift-reduce algorithm with several novel features suited to ASR output texts. Because the gold dependency information is usually annotated only on transcribed texts, we also introduce an alignment-based method for transferring the gold dependency annotation to the ASR output texts to construct training data for our parser. We conducted an experiment on the Switchboard corpus and show that our method outperforms conventional methods in terms of dependency parsing and disÔ¨Çuency detection.  There are a number of studies that address the problem of detecting disÔ¨Çuencies. Some of these studies include dependency parsing (Honnibal and Johnson, 2014; Wu et al., 2015; Rasooli and Tetreault, 2014), whereas others are dedicated systems (Qian and Liu, 2013; Ferguson et al., 2015; Hough and Purver, 2014; Hough and Schlangen, 2015; Liu et al., 2003). Among these studies, Honnibal (2014) and Wu (2015) address this problem by adding a new action to transition-based dependency parsing that removes the disÔ¨Çuent parts of the input sentence from the stack. Using this approach, they achieved high performance in terms of both dependency parsing and disÔ¨Çuency detection on the Switchboard corpus. However, the authors assume that the input texts to parse are transcribed by human annotators, which, in practice, is unrealistic. In real-world applications, in addition to disÔ¨Çuencies, the input texts contain ASR errors; these issues might degrade the parsing performance. For example, proper nouns that are not contained in the ASR system vocabulary may break up into smaller pieces, yielding a difÔ¨Åcult problem for the parsing unit (Cheng et al., 2015):  
In this paper, we describe our approach of enabling an interactive dialogue system to recognize user emotion and sentiment in realtime. These modules allow otherwise conventional dialogue systems to have ‚Äúempathy‚Äù and answer to the user while being aware of their emotion and intent. Emotion recognition from speech previously consists of feature engineering and machine learning where the Ô¨Årst stage causes delay in decoding time. We describe a CNN model to extract emotion from raw speech input without feature engineering. This approach even achieves an impressive average of 65.7% accuracy on six emotion categories, a 4.5% improvement when compared to the conventional feature based SVM classiÔ¨Åcation. A separate, CNN-based sentiment analysis module recognizes sentiments from speech recognition results, with 82.5 Fmeasure on human-machine dialogues when trained with out-of-domain data. 
Even syntactically correct sentences are perceived as awkward if they do not contain correct punctuation. Still, the problem of automatic generation of punctuation marks has been largely neglected for a long time. We present a novel model that introduces punctuation marks into raw text material with transition-based algorithm using LSTMs. Unlike the state-of-the-art approaches, our model is language-independent and also neutral with respect to the intended use of the punctuation. Multilingual experiments show that it achieves high accuracy on the full range of punctuation marks across languages. 
Neural network-based encoder-decoder models are among recent attractive methodologies for tackling natural language generation tasks. This paper investigates the usefulness of structural syntactic and semantic information additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modiÔ¨Åed version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 
Word embedding models learn vectorial word representations that can be used in a variety of NLP applications. When training data is scarce, these models risk losing their generalization abilities due to the complexity of the models and the overÔ¨Åtting to Ô¨Ånite data. We propose a regularized embedding formulation, called Robust Gram (RG), which penalizes overÔ¨Åtting by suppressing the disparity between target and context embeddings. Our experimental analysis shows that the RG model trained on small datasets generalizes better compared to alternatives, is more robust to variations in the training set, and correlates well to human similarities in a set of word similarity tasks. 
Lexical simpliÔ¨Åcation of scientiÔ¨Åc terms represents a unique challenge due to the lack of a standard parallel corpora and fast rate at which vocabulary shift along with research. We introduce SimpleScience, a lexical simpliÔ¨Åcation approach for scientiÔ¨Åc terminology. We use word embeddings to extract simpliÔ¨Åcation rules from a parallel corpora containing scientiÔ¨Åc publications and Wikipedia. To evaluate our system we construct SimpleSciGold, a novel gold standard set for science-related simpliÔ¨Åcations. We Ô¨Ånd that our approach outperforms prior context-aware approaches at generating simpliÔ¨Åcations for scientiÔ¨Åc terms. 
Essay scoring is a complicated processing requiring analyzing, summarizing and judging expertise. Traditional work on essay scoring focused on automatic handcrafted features, which are expensive yet sparse. Neural models offer a way to learn syntactic and semantic features automatically, which can potentially improve upon discrete features. In this paper, we employ convolutional neural network (CNN) for the effect of automatically learning features, and compare the result with the state-of-art discrete baselines. For in-domain and domain-adaptation essay scoring tasks, our neural model empirically outperforms discrete models. 
We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms. 
Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to Ô¨Ånancial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing ‚Äì given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efÔ¨Åcient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in 70% of the cases. In 60% of the time, it also identiÔ¨Åes the correct noun phrase ‚Üí variables mapping, signiÔ¨Åcantly outperforming baselines. We also release a new annotated dataset for task evaluation. 
This paper presents an approach to extract implicit interpretations from modal constructions. Importantly, our approach uses a deterministic procedure to normalize eventualities and generate potential interpretations. An annotation effort demonstrates that these interpretations are intuitive to humans and most modal constructions convey at least one interpretation. Experimental results show that the task is challenging but can be automated. 
This paper presents a two-step procedure to extract positive meaning from verbal negation. We Ô¨Årst generate potential positive interpretations manipulating syntactic dependencies. Then, we score them according to their likelihood. Manual annotations show that positive interpretations are ubiquitous and intuitive to humans. Experimental results show that dependencies are better suited than semantic roles for this task, and automation is possible. 
Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identiÔ¨Åcation and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classiÔ¨Åer for language identiÔ¨Åcation which eliminates this disparity and release a new corpus of tweets containing AAE-like language. Data and software resources are available at: http://slanglab.cs.umass.edu/TwitterAAE 
Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010). Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers. In this paper, we report a study on 430,000 unique tweets from Indian users, speciÔ¨Åcally Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment. To this end, we develop classiÔ¨Åers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments. Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing. As an aside, we explore some common pragmatic functions of codeswitching through sentiment detection. 
SigniÔ¨Åcant events are characterized by interactions between entities (such as countries, organizations, or individuals) that deviate from typical interaction patterns. Analysts, including historians, political scientists, and journalists, commonly read large quantities of text to construct an accurate picture of when and where an event happened, who was involved, and in what ways. In this paper, we present the Capsule model for analyzing documents to detect and characterize events of potential signiÔ¨Åcance. SpeciÔ¨Åcally, we develop a model based on topic modeling that distinguishes between topics that describe ‚Äúbusiness as usual‚Äù and topics that deviate from these patterns. To demonstrate this model, we analyze a corpus of over two million U.S. State Department cables from the 1970s. We provide an open-source implementation of an inference algorithm for the model and a pipeline for exploring its results. 
Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks. Their application to language has received much less attention, and it has mainly focused on static classiÔ¨Åcation tasks, such as sentence classiÔ¨Åcation for Sentiment Analysis or relation extraction. In this work, we study the application of CNNs to language modeling, a dynamic, sequential prediction task that needs models to capture local as well as long-range dependency information. Our contribution is twofold. First, we show that CNNs achieve 11-26% better absolute performance than feed-forward neural language models, demonstrating their potential for language representation even in sequential tasks. As for recurrent models, our model outperforms RNNs but is below state of the art LSTM models. Second, we gain some understanding of the behavior of the model, showing that CNNs in language act as feature detectors at a high level of abstraction, like in Computer Vision, and that the model can profitably use information from as far as 16 words before the target. 
Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be uniÔ¨Åed in a single modeling framework that deÔ¨Ånes a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.1 
We present a model for contrastively describing scenes, in which context-speciÔ¨Åc behavior results from a combination of inferencedriven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple featuredriven architecture (here a pair of neural ‚Äúlistener‚Äù and ‚Äúspeaker‚Äù models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated without demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to 69% using existing techniques.  
We describe Hafez, a program that generates any number of distinct poems on a usersupplied topic. Poems obey rhythmic and rhyme constraints. We describe the poetrygeneration algorithm, give experimental data concerning its parameters, and show its generality with respect to language and poetic form. 
Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their inÔ¨Çuence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity, coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a Ô¨Årst step towards learning a neural conversational model based on the long-term success of dialogues. 
This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. It generates biographical sentences from fact tables on a new dataset of biographies from Wikipedia. This set is an order of magnitude larger than existing resources with over 700k samples and a 400k vocabulary. Our model builds on conditional neural language models for text generation. To deal with the large vocabulary, we extend these models to mix a Ô¨Åxed vocabulary with copy actions that transfer sample-speciÔ¨Åc words from the input database to the generated output sentence. To deal with structured data, we allow the model to embed words differently depending on the data Ô¨Åelds in which they occur. Our neural model significantly outperforms a Templated Kneser-Ney language model by nearly 15 BLEU. 
This article tackles a new challenging task in computational argumentation. Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one. We approach this task in a fully empirical manner by annotating 26k explanations written in natural language. These explanations describe convincingness of arguments in the given argument pair, such as their strengths or Ô¨Çaws. We create a new crowd-sourced corpus containing 9,111 argument pairs, multilabeled with 17 classes, which was cleaned and curated by employing several strict quality measures. We propose two tasks on this data set, namely (1) predicting the full label distribution and (2) classifying types of Ô¨Çaws in less convincing arguments. Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel Ô¨Åne-grained analysis of Web argument convincingness is a very challenging task. We release the new corpus UKPConvArg2 and the accompanying software under permissive licenses to the research community. 
Recognizing implicit discourse relations is a challenging but important task in the Ô¨Åeld of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efÔ¨Åcient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually Ô¨Åx the attention on some speciÔ¨Åc words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-ofart results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words. 
Sluicing is an elliptical process where the majority of a question can go unpronounced as long as there is a salient antecedent in previous discourse. This paper considers the task of antecedent selection: Ô¨Ånding the correct antecedent for a given case of sluicing. We argue that both syntactic and discourse relationships are important in antecedent selection, and we construct linguistically sophisticated features that describe the relevant relationships. We also deÔ¨Åne features that describe the relation of the content of the antecedent and the sluice type. We develop a linear model which achieves accuracy of 72.4%, a substantial improvement over a strong manually constructed baseline. Feature analysis conÔ¨Årms that both syntactic and discourse features are important in antecedent selection. 
This paper proposes a method for intrasentential subject zero anaphora resolution in Japanese. Our proposed method utilizes a Multi-column Convolutional Neural Network (MCNN) for predicting zero anaphoric relations. Motivated by Centering Theory and other previous works, we exploit as clues both the surface word sequence and the dependency tree of a target sentence in our MCNN. Even though the F-score of our method was lower than that of the state-of-the-art method, which achieved relatively high recall and low precision, our method achieved much higher precision (>0.8) in a wide range of recall levels. We believe such high precision is crucial for real-world NLP applications and thus our method is preferable to the state-of-the-art method.  
For many low-resource languages, spoken language resources are more likely to be annotated with translations than with transcriptions. Translated speech data is potentially valuable for documenting endangered languages or for training speech translation systems. A Ô¨Årst step towards making use of such data would be to automatically align spoken words with their translations. We present a model that combines Dyer et al.‚Äôs reparameterization of IBM Model 2 (fast_align) and k-means clustering using Dynamic Time Warping as a distance measure. The two components are trained jointly using expectationmaximization. In an extremely low-resource scenario, our model performs signiÔ¨Åcantly better than both a neural model and a strong baseline. 
Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more Ô¨Åne-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME‚Äôs broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores. 
The state-of-the-art named entity recognition (NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information. However, such a model could still make mistakes if its features favor a wrong entity type. In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-accuracy, highcoverage multilingual Wikipedia entity type mappings. These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages. Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F1 score improvement). 
Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difÔ¨Åculty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classiÔ¨Åcation task. 
Sequence-to-Sequence (seq2seq) modeling has rapidly become an important generalpurpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beamsearch training scheme, based on the work of Daume¬¥ III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and uniÔ¨Åes the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efÔ¨Åcient training approach. We show that our system outperforms a highlyoptimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation. 
We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to Ô¨Ånd the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their Ô¨Åxedlength hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inÔ¨Çection show signiÔ¨Åcant performance gains over the baseline encoder-decoders. 
Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also signiÔ¨Åcantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13√ó fewer parameters than the original teacher model, with a decrease of 0.4 BLEU. 
Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods.1 Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task. 
The Xenotext Experiment implants poetry into an extremophile‚Äôs DNA, and uses that DNA to generate new poetry in a protein form. The molecular machinery of life requires that these two poems encipher each other under a symmetric substitution cipher. We search for ciphers which permit writing under the Xenotext constraints, incorporating ideas from cipher-cracking algorithms, and using n-gram data to assess a cipher‚Äôs ‚Äúwritability‚Äù. Our algorithm, Beam Verse, is a beam search which uses new heuristics to navigate the cipher-space. We Ô¨Ånd thousands of ciphers which score higher than successful ciphers used to write Xenotext constrained texts. 
Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for. Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications. To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semisupervised model, GraLap to label the intensity of references. Experiments with AAN datasets show a signiÔ¨Åcant improvement compared to the baselines to achieve the true labels of the references (46% better correlation). Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications. 
In this paper, we improve microblog users‚Äô demographic prediction by fully utilizing their video related behaviors. First, we collect the describing words of currently popular videos, including video names, actor names and video keywords, from video websites. Secondly, we search these describing words in users‚Äô microblogs, and build the direct relationships between users and the appeared words. After that, to make the sparse relationship denser, we propose a Bayesian method to calculate the probability of connections between users and other video describing words. Lastly, we build two models to predict users‚Äô demographics with the obtained direct and indirect relationships. Based on a large realworld dataset, experiment results show that our method can signiÔ¨Åcantly improve these words‚Äô demographic predictive ability. 
Distant supervision has been widely used in current systems of Ô¨Åne-grained entity typing to automatically assign categories (entity types) to entity mentions. However, the types so obtained from knowledge bases are often incorrect for the entity mention‚Äôs local context. This paper proposes a novel embedding method to separately model ‚Äúclean‚Äù and ‚Äúnoisy‚Äù mentions, and incorporates the given type hierarchy to induce loss functions. We formulate a joint optimization problem to learn embeddings for mentions and typepaths, and develop an iterative algorithm to solve the problem. Experiments on three public datasets demonstrate the effectiveness and robustness of the proposed method, with an average 15% improvement in accuracy over the next best compared method1. 
Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually. Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data. Among a series of methods, Random Walk is believed to be suitable for knowledge graph data. However, a pure random walk without goals still has a poor efÔ¨Åciency of mining useful formulas, and even introduces lots of noise which may mislead inference. Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas. To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the speciÔ¨Åc inference target at each step. The algorithm is more inclined to visit beneÔ¨Åc structures to infer the target, so it can increase efÔ¨Åciency of random walks and avoid noise simultaneously. The experiments on both WordNet and Freebase prove that our approach is has a high efÔ¨Åciency and performs best on the task. 
Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efÔ¨Åciently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of Ô¨Årst-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efÔ¨Åcient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we Ô¨Ånd that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime. 
Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difÔ¨Åculty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WIKIMOVIES, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WIKIQA benchmark. 
We present an unsupervised model for the discovery and clustering of latent ‚Äúpersonas‚Äù (characterizations of entities). Our model simultaneously clusters documents featuring similar collections of personas. We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation. 
When interacting individuals entrain, they begin to speak more like each other. To support research on entrainment in cooperative multi-party dialogues, we have created a corpus where teams of three or four speakers play two rounds of a cooperative board game. We describe the experimental design and technical infrastructure used to collect our corpus, which consists of audio, video, transcriptions, and questionnaire data for 63 teams (47 hours of audio). We illustrate the use of our corpus as a novel resource for studying team entrainment by 1) developing and evaluating teamlevel acoustic-prosodic entrainment measures that extend existing dyad measures, and 2) investigating relationships between team entrainment and participation dominance. 
In this paper, we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals. With this framework, we directly model content selection decisions based on a set of psychologically-motivated domainindependent personal traits including personality (e.g., extraversion) and basic human values (e.g., self-transcendence). We also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation. 
For AI systems to reason about real world situations, they need to recognize which processes are at play and which entities play key roles in them. Our goal is to extract this kind of rolebased knowledge about processes, from multiple sentence-level descriptions. This knowledge is hard to acquire; while semantic role labeling (SRL) systems can extract sentence level role information about individual mentions of a process, their results are often noisy and they do not attempt create a globally consistent characterization of a process. To overcome this, we extend standard within sentence joint inference to inference across multiple sentences. This cross sentence inference promotes role assignments that are compatible across different descriptions of the same process. When formulated as an Integer Linear Program, this leads to improvements over within-sentence inference by nearly 3% in F1. The resulting role-based knowledge is of high quality (with a F1 of nearly 82). 
 Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. This task is especially difÔ¨Åcult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art. 
Named Entity Recognition is a well established information extraction task with many state of the art systems existing for a variety of languages. Most systems rely on language speciÔ¨Åc resources, large annotated corpora, gazetteers and feature engineering to perform well monolingually. In this paper, we introduce an attentional neural model which only uses language universal phonological character representations with word embeddings to achieve state of the art performance in a monolingual setting using supervision and which can quickly adapt to a new language with minimal or no data. We demonstrate that phonological character representations facilitate cross-lingual transfer, outperform orthographic representations and incorporating both attention and phonological features improves statistical efÔ¨Åciency of the model in 0-shot and low data transfer settings with no task speciÔ¨Åc feature engineering in the source or target language. 
The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a signiÔ¨Åcant reduction of the perplexity when compared to state-of-the-art language modeling techniques. 
To enable language-based communication and collaboration with cognitive robots, this paper presents an approach where an agent can learn task models jointly from language instruction and visual demonstration using an And-Or Graph (AoG) representation. The learned AoG captures a hierarchical task structure where linguistic labels (for language communication) are grounded to corresponding state changes from the physical environment (for perception and action). Our empirical results on a cloth-folding domain have shown that, although state detection through visual processing is full of uncertainties and error prone, by a tight integration with language the agent is able to learn an effective AoG for task representation. The learned AoG can be further applied to infer and interpret on-going actions from new visual demonstration using linguistic labels at different levels of granularity. 
We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence ‚ÄúI shot an elephant in my pajamas‚Äù, looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach signiÔ¨Åcantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015). 
We present CHARAGRAM embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that CHARAGRAM embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.1 
Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size. In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model. The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences. For the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models. From a practical point of view, our proposed model also eliminates the need for a beam-search during inference, which reduces to an efÔ¨Åcient dot-product based search in a vector-space. 
We investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A Ô¨Åne-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing. 
Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become a new paradigm. Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT. However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of NMT, especially when the parallel corpus is far from sufÔ¨Åcient. In this paper, we propose two approaches to make full use of the sourceside monolingual data in NMT. The Ô¨Årst approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain signiÔ¨Åcant improvements over the strong attention-based NMT. 
Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efÔ¨Åciently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1 
The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that signiÔ¨Åcantly improves BLEU scores across a range of low-resource languages. Our key idea is to Ô¨Årst train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation. 
Community-driven Question Answering (CQA) systems that crowdsource experiential information in the form of questions and answers and have accumulated valuable reusable knowledge. Clustering of QA datasets from CQA systems provides a means of organizing the content to ease tasks such as manual curation and tagging. In this paper, we present a clustering method that exploits the two-part question-answer structure in QA datasets to improve clustering quality. Our method, MixKMeans, composes question and answer space similarities in a way that the space on which the match is higher is allowed to dominate. This construction is motivated by our observation that semantic similarity between question-answer data (QAs) could get localized in either space. We empirically evaluate our method on a variety of real-world labeled datasets. Our results indicate that our method signiÔ¨Åcantly outperforms stateof-the-art clustering methods for the task of clustering question-answer archives. 
We address the problem of answering new questions in community forums, by selecting suitable answers to already asked questions. We approach the task as an answer ranking problem, adopting a pairwise neural network architecture that selects which of two competing answers is better. We focus on the utility of the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment, which we call relevance, relatedness, and appropriateness. Our proposed neural network models the interactions among all input components using syntactic and semantic embeddings, lexical matching, and domain-speciÔ¨Åc features. It achieves state-of-the-art results, showing that the three similarities are important and need to be modeled together. Our experiments demonstrate that all feature types are relevant, but the most important ones are the lexical similarity features, the domain-speciÔ¨Åc features, and the syntactic and semantic embeddings. 
We show that a character-level encoderdecoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for singlerelation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing. 1 
To learn text understanding models with millions of parameters one needs massive amounts of data. In this work, we argue that generating data can compensate for this need. While deÔ¨Åning generic data generators is difÔ¨Åcult, we propose to allow generators to be ‚Äúweakly‚Äù speciÔ¨Åed in the sense that a set of parameters controls how the data is generated. Consider for example generators where the example templates, grammar, and/or vocabulary is determined by this set of parameters. Instead of manually tuning these parameters, we learn them from the limited training data at our disposal. To achieve this, we derive an efÔ¨Åcient algorithm called GENERE that jointly estimates the parameters of the model and the undetermined generation parameters. We illustrate its beneÔ¨Åts by learning to solve math exam questions using a highly parametrized sequence-to-sequence neural network. 
Texts present coherent stories that have a particular theme or overall setting, for example science Ô¨Åction or western. In this paper, we present a text generation method called rewriting that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a twostage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors deÔ¨Åning aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the Ô¨Ånal stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes. 
Sentiment lexicons have been leveraged as a useful source of features for sentiment analysis models, leading to the state-of-the-art accuracies. On the other hand, most existing methods use sentiment lexicons without considering context, typically taking the count, sum of strength, or maximum sentiment scores over the whole input. We propose a context-sensitive lexicon-based method based on a simple weighted-sum model, using a recurrent neural network to learn the sentiments strength, intensiÔ¨Åcation and negation of lexicon sentiments in composing the sentiment value of sentences. Results show that our model can not only learn such operation details, but also give signiÔ¨Åcant improvements over state-of-the-art recurrent neural network baselines without lexical features, achieving the best results on a Twitter benchmark. 
In this paper, we present our work in emotion cause extraction. Since there is no open dataset available, the lack of annotated resources has limited the research in this area. Thus, we Ô¨Årst present a dataset we built using SINA city news. The annotation is based on the scheme of the W3C Emotion Markup Language. Second, we propose a 7-tuple deÔ¨Ånition to describe emotion cause events. Based on this general deÔ¨Ånition, we propose a new event-driven emotion cause extraction method using multi-kernel SVMs where a syntactical tree based approach is used to represent events in text. A convolution kernel based multikernel SVM are used to extract emotion causes. Because traditional convolution kernels do not use lexical information at the terminal nodes of syntactic trees, we modify the kernel function with a synonym based improvement. Even with very limited training data, we can still extract sufÔ¨Åcient features for the task. Evaluations show that our approach achieves 11.6% higher F-measure compared to referenced methods. The contributions of our work include resource construction, concept deÔ¨Ånition and algorithm development. 
Document-level sentiment classiÔ¨Åcation aims to predict user‚Äôs overall sentiment in a document about a product. However, most of existing methods only focus on local text information and ignore the global user preference and product characteristics. Even though some works take such information into account, they usually suffer from high model complexity and only consider wordlevel preference rather than semantic levels. To address this issue, we propose a hierarchical neural network to incorporate global user and product information into sentiment classiÔ¨Åcation. Our model Ô¨Årst builds a hierarchical LSTM model to generate sentence and document representations. Afterwards, user and product information is considered via attentions over different semantic levels due to its ability of capturing crucial semantic components. The experimental results show that our model achieves signiÔ¨Åcant and consistent improvements compared to all state-of-theart methods. The source code of this paper can be obtained from https://github. com/thunlp/NSC. 
Recently, neural networks have achieved great success on sentiment classiÔ¨Åcation due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classiÔ¨Åcation under a recurrent architecture because of the deÔ¨Åciency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets. 
Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great beneÔ¨Åt for improved accuracy and interpretability. We develop a general framework that enables learning knowledge and its conÔ¨Ådence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts. We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures. Our model substantially enhances the performance using less training data, and shows improved interpretability. The principled framework can also be applied to posterior regularization for regulating other statistical models. 
One major deÔ¨Åciency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conÔ¨Çating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conÔ¨Çating the representations of words based on the deep knowledge that can be derived from a semantic network. Our approach provides multiple advantages in comparison to the previous approaches, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them. 
Distributional models are derived from cooccurrences in a corpus, where only a small proportion of all possible plausible cooccurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjectivenoun, noun-noun and verb-object compositions while being fully interpretable. 
Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. SpeciÔ¨Åcally, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efÔ¨Åcacy of our proposed architectures. 
We present a framework for augmenting data sets from the Universal Dependencies project with Universal Decompositional Semantics. Where the Universal Dependencies project aims to provide a syntactic annotation standard that can be used consistently across many languages as well as a collection of corpora that use that standard, our extension has similar aims for semantic annotation. We describe results from annotating the English Universal Dependencies treebank, dealing with word senses, semantic roles, and event properties. 
We present a probabilistic model of the inÔ¨Çuence of language on the behavior of the U.S. Supreme Court, speciÔ¨Åcally inÔ¨Çuence of amicus briefs on Court decisions and opinions. The approach assumes that amici are rational, utility-maximizing agents who try to win votes or affect the language of court opinions. Our model leads to improved predictions of justices‚Äô votes and perplexity of opinion language. It is amenable to inspection, allowing us to explore inferences about the persuasiveness of different amici and inÔ¨Çuenceability of different justices; these are consistent with earlier Ô¨Åndings. ‚ÄúLanguage is the central tool of our trade.‚Äù John G. Roberts, 2007 (Garner, 2010) 
Verb Phrase Ellipsis (VPE) is an anaphoric construction in which a verb phrase has been elided. It occurs frequently in dialogue and informal conversational settings, but despite its evident impact on event coreference resolution and extraction, there has been relatively little work on computational methods for identifying and resolving VPE. Here, we present a novel approach to detecting and resolving VPE by using supervised discriminative machine learning techniques trained on features extracted from an automatically parsed, publicly available dataset. Our approach yields state-of-the-art results for VPE detection by improving F1 score by over 11%; additionally, we explore an approach to antecedent identiÔ¨Åcation that uses the Margin-Infused-RelaxedAlgorithm, which shows promising results. 
We introduce two Ô¨Årst-order graph-based dependency parsers achieving a new state of the art. The Ô¨Årst is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difÔ¨Åculty or ambiguity. The second parser is a ‚Äúdistillation‚Äù of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment, thereby avoiding the intractable crossentropy computations required by applying standard distillation objectives to problems with structured outputs. The Ô¨Årst-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 
We describe a neural shift-reduce parsing model for CCG, factored into four unidirectional LSTMs and one bidirectional LSTM. This factorization allows the linearization of the complete parsing history, and results in a highly accurate greedy parser that outperforms all previous beam-search shift-reduce parsers for CCG. By further deriving a globally optimized model using a task-based loss, we improve over the state of the art by up to 2.67% labeled F1. 
For many NLP applications that require a parser, the sentences of interest may not be well-formed. If the parser can overlook problems such as grammar mistakes and produce a parse tree that closely resembles the correct analysis for the intended sentence, we say that the parser is robust. This paper compares the performances of eight state-of-the-art dependency parsers on two domains of ungrammatical sentences: learner English and machine translation outputs. We have developed an evaluation metric and conducted a suite of experiments. Our analyses may help practitioners to choose an appropriate parser for their tasks, and help developers to improve parser robustness against ungrammatical sentences.  more robust than others against sentences that are not well-formed? Previous works on parser evaluation that focused on accuracy and speed (Choi et al., 2015; Kummerfeld et al., 2012; McDonald and Nivre, 2011; Kong and Smith, 2014) have not taken ungrammatical sentences into consideration. In this paper, we report a set of empirical analyses of eight leading dependency parsers on two domains of ungrammatical text: English-as-a-Second Language (ESL) learner text and machine translation (MT) outputs. We also vary the types of training sources; the parsers are trained with the Penn Treebank (to be comparable with other studies) and Tweebank, a treebank on tweets (to be a bit more like the test domain) (Kong et al., 2014). The main contributions of the paper are:  
We present a shift-reduce CCG semantic parser. Our parser uses a neural network architecture that balances model capacity and computational cost. We train by transferring a model from a computationally expensive loglinear CKY parser. Our learner addresses two challenges: selecting the best parse for learning when the CKY parser generates multiple correct trees, and learning from partial derivations when the CKY parser fails to parse. We evaluate on AMR parsing. Our parser performs comparably to the CKY parser, while doing signiÔ¨Åcantly fewer operations. We also present results for greedy semantic parsing with a relatively small drop in performance. 
Syntactic parsing of web queries is important for query understanding. However, web queries usually do not observe the grammar of a written language, and no labeled syntactic trees for web queries are available. In this paper, we focus on a query‚Äôs clicked sentence, i.e., a well-formed sentence that i) contains all the tokens of the query, and ii) appears in the query‚Äôs top clicked web pages. We argue such sentences are semantically consistent with the query. We introduce algorithms to derive a query‚Äôs syntactic structure from the dependency trees of its clicked sentences. This gives us a web query treebank without manual labeling. We then train a dependency parser on the treebank. Our model achieves much better UAS (0.86) and LAS (0.80) scores than state-of-the-art parsers on web queries. 
Sequences found at the beginning of TV shows help the audience absorb the essence of previous episodes, and grab their attention with upcoming plots. In this paper, we propose a novel task, text recap extraction. Compared with conventional summarization, text recap extraction captures the duality of summarization and plot contingency between adjacent episodes. We present a new dataset, TVRecap, for text recap extraction on TV shows. We propose an unsupervised model that identiÔ¨Åes text recaps based on plot descriptions. We introduce two contingency factors, concept coverage and sparse reconstruction, that encourage recaps to prompt the upcoming story development. We also propose a multi-view extension of our model which can incorporate dialogues and synopses. We conduct extensive experiments on TVRecap, and conclude that our model outperforms summarization approaches. 
Users prefer natural language software requirements because of their usability and accessibility. When they describe their wishes for software development, they often provide off-topic information. We therefore present REaCT1, an automated approach for identifying and semantically annotating the on-topic parts of requirement descriptions. It is designed to support requirement engineers in the elicitation process on detecting and analyzing requirements in user-generated content. Since no lexical resources with domain-speciÔ¨Åc information about requirements are available, we created a corpus of requirements written in controlled language by instructed users and uncontrolled language by uninstructed users. We annotated these requirements regarding predicate-argument structures, conditions, priorities, motivations and semantic roles and used this information to train classiÔ¨Åers for information extraction purposes. REaCT achieves an accuracy of 92% for the on- and off-topic classiÔ¨Åcation task and an F1measure of 72% for the semantic annotation. 
Existing work on detecting deceptive reviews primarily focuses on feature engineering and applies off-the-shelf supervised classiÔ¨Åcation algorithms to the problem. Then, one real challenge would be to manually recognize plentiful ground truth spam review data for model building, which is rather difÔ¨Åcult and often requires domain expertise in practice. In this paper, we propose to exploit the relatedness of multiple review spam detection tasks and readily available unlabeled data to address the scarcity of labeled opinion spam data. We Ô¨Årst develop a multi-task learning method based on logistic regression (MTL-LR), which can boost the learning for a task by sharing the knowledge contained in the training signals of other related tasks. To leverage the unlabeled data, we introduce a graph Laplacian regularizer into each base model. We then propose a novel semi-supervised multitask learning method via Laplacian regularized logistic regression (SMTL-LLR) to further improve the review spam detection performance. We also develop a stochastic alternating method to cope with the optimization for SMTL-LLR. Experimental results on real-world review data demonstrate the beneÔ¨Åt of SMTL-LLR over several well-established baseline methods. 
Regularization is a critical step in supervised learning to not only address overÔ¨Åtting, but also to take into account any prior knowledge we may have on the features and their dependence. In this paper, we explore stateof-the-art structured regularizers and we propose novel ones based on clusters of words from LSI topics, word2vec embeddings and graph-of-words document representation. We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classiÔ¨Åcation accuracy. Code and data are available online1. 
We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A speciÔ¨Åed number of discussion threads predicted to be popular are recommended, chosen from a Ô¨Åxed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental conÔ¨Ågurations and domains, and it also generalizes well with varying numbers of recommendation requests. 
Text reuse refers to citing, copying or alluding text excerpts from a text resource to a new context. While detecting reuse in contemporary languages is well supported‚Äîgiven extensive research, techniques, and corpora‚Äî automatically detecting historical text reuse is much more difÔ¨Åcult. Corpora of historical languages are less documented and often encompass various genres, linguistic varieties, and topics. In fact, historical text reuse detection is much less understood and empirical studies are necessary to enable and improve its automation. We present a linguistic analysis of text reuse in two ancient data sets. We contribute an automated approach to analyze how an original text was transformed into its reuse, taking linguistic resources into account to understand how they help characterizing the transformation. It is complemented by a manual analysis of a subset of the reuse. Our results show the limitations of approaches focusing on literal reuse detection. Yet, linguistic resources can effectively support understanding the non-literal text reuse transformation process. Our results support practitioners and researchers working on understanding and detecting historical reuse. 
We operate a change of paradigm and hypothesize that keywords are more likely to be found among inÔ¨Çuential nodes of a graph-ofwords rather than among its nodes high on eigenvector-related centrality measures. To test this hypothesis, we introduce unsupervised techniques that capitalize on graph degeneracy. Our methods strongly and signiÔ¨Åcantly outperform all baselines on two datasets (short and medium size documents), and reach best performance on the third one (long documents). 
The problem of accurately predicting relative reading difÔ¨Åculty across a set of sentences arises in a number of important natural language applications, such as Ô¨Ånding and curating effective usage examples for intelligent language tutoring systems. Yet while signiÔ¨Åcant research has explored documentand passage-level reading difÔ¨Åculty, the special challenges involved in assessing aspects of readability for single sentences have received much less attention, particularly when considering the role of surrounding passages. We introduce and evaluate a novel approach for estimating the relative reading difÔ¨Åculty of a set of sentences, with and without surrounding context. Using different sets of lexical and grammatical features, we explore models for predicting pairwise relative difÔ¨Åculty using logistic regression, and examine rankings generated by aggregating pairwise difÔ¨Åculty labels using a Bayesian rating system to form a Ô¨Ånal ranking. We also compare rankings derived for sentences assessed with and without context, and Ô¨Ånd that contextual features can help predict differences in relative difÔ¨Åculty judgments across these two conditions. 
Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering. 
Technical writing in professional environments, such as user manual authoring, requires the use of uniform language. Nonuniform language detection is a novel task, which aims to guarantee the consistency for technical writing by detecting sentences in a document that are intended to have the same meaning within a similar context but use different words or writing style. This paper proposes an approach that utilizes text similarity algorithms at lexical, syntactic, semantic and pragmatic levels. Different features are extracted and integrated by applying a machine learning classiÔ¨Åcation method. We tested our method using smart phone user manuals, and compared its performance against the state-ofthe-art methods in a related area. The experiments demonstrate that our approach achieves the upper bound performance for this task. 
An important aspect for the task of grammatical error correction (GEC) that has not yet been adequately explored is adaptation based on the native language (L1) of writers, despite the marked inÔ¨Çuences of L1 on second language (L2) writing. In this paper, we adapt a neural network joint model (NNJM) using L1-speciÔ¨Åc learner text and integrate it into a statistical machine translation (SMT) based GEC system. SpeciÔ¨Åcally, we train an NNJM on general learner text (not L1-speciÔ¨Åc) and subsequently train on L1-speciÔ¨Åc data using a Kullback-Leibler divergence regularized objective function in order to preserve generalization of the model. We incorporate this adapted NNJM as a feature in an SMT-based English GEC system and show that adaptation achieves signiÔ¨Åcant F0.5 score gains on English texts written by L1 Chinese, Russian, and Spanish writers. 
We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora. 
This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-speciÔ¨Åc crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus1 of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models. 
The problem of noisy and unbalanced training data for supervised keyphrase extraction results from the subjectivity of keyphrase assignment, which we quantify by crowdsourcing keyphrases for news and fashion magazine articles with many annotators per document. We show that annotators exhibit substantial disagreement, meaning that single annotator data could lead to very different training sets for supervised keyphrase extractors. Thus, annotations from single authors or readers lead to noisy training data and poor extraction performance of the resulting supervised extractor. We provide a simple but effective solution to still work with such data by reweighting the importance of unlabeled candidate phrases in a two stage Positive Unlabeled Learning setting. We show that performance of trained keyphrase extractors approximates a classiÔ¨Åer trained on articles labeled by multiple annotators, leading to higher average F1scores and better rankings of keyphrases. We apply this strategy to a variety of test collections from different backgrounds and show improvements over strong baseline models. 
A natural language interface to answers on the Web can help us access information more efÔ¨Åciently. We start with an interesting source of information‚Äîinfoboxes in Wikipedia that summarize factoid knowledge‚Äîand develop a comprehensive approach to answering questions with high precision. We Ô¨Årst build a system to access data in infoboxes in a structured manner. We use our system to construct a crowdsourced dataset of over 15,000 highquality, diverse questions. With these questions, we train a convolutional neural network model that outperforms models that achieve top results in similar answer selection tasks. 
We propose an algorithm that combines supervised and unsupervised methods to ensemble multiple systems for two popular Knowledge Base Population (KBP) tasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and Linking (TEDL). We demonstrate that it outperforms the best system for both tasks in the 2015 competition, several ensembling baselines, as well as a state-of-the-art stacking approach. The success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling. 
We present a neural network architecture to predict a point in color space from the sequence of characters in the color‚Äôs name. Using large scale color‚Äìname pairs obtained from an online color design forum, we evaluate our model on a ‚Äúcolor Turing test‚Äù and Ô¨Ånd that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. Our datasets and demo system are available online at http: //colorlab.us. 
Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a Ô¨Årst step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models ‚Äì with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016. Our behavior analysis reveals that despite recent progress, today‚Äôs VQA models are ‚Äúmyopic‚Äù (tend to fail on sufÔ¨Åciently novel instances), often ‚Äújump to conclusions‚Äù (converge on a predicted answer after ‚Äòlistening‚Äô to just half the question), and are ‚Äústubborn‚Äù (do not change their answers across images). 
This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. SpeciÔ¨Åcally, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing signiÔ¨Åcant improvements in grammaticality while modestly improving descriptive quality. 
Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by deÔ¨Åning verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus. 
We propose a method for learning the structure of variable-order CRFs, a more Ô¨Çexible variant of higher-order linear-chain CRFs. Variableorder CRFs achieve faster inference by including features for only some of the tag ngrams. Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer. An active-set outer loop allows the feature set to grow as far as needed. On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2‚Äì6x speedup over a baseline, with no signiÔ¨Åcant drop in accuracy.  98  97  96  Accuracy  95  1-CRF  94 0-CRF  2-CRF Bulgarian Norwegian  93  Hindi  92  Slovenian Basque  91  0  1000  2000  3000  4000  5000  Number of Tag String Features  Figure 1: Speed-accuracy tradeoff curves on test data  for the 5 languages. Large dark circles represent the kCRFs of ascending orders along x-axis (marked on for  Slovenian). Smaller triangles each represent a VoCRF  discovered by sweeping the speed parameters Œ≥. We Ô¨Ånd faster models at similar accuracy to the best k-CRFs (¬ß5).  
Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks. We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1 
The Dirichlet distribution (Dir) is one of the most widely used prior distributions in statistical approaches to natural language processing. The parameters of Dir are required to be positive, which signiÔ¨Åcantly limits its strength as a sparsity prior. In this paper, we propose a simple modiÔ¨Åcation to the Dirichlet distribution that allows the parameters to be negative. Our modiÔ¨Åed Dirichlet distribution (mDir) not only induces much stronger sparsity, but also simultaneously performs smoothing. mDir is still conjugate to the multinomial distribution, which simpliÔ¨Åes posterior inference. We introduce two simple and efÔ¨Åcient algorithms for Ô¨Ånding the mode of mDir. Our experiments on learning Gaussian mixtures and unsupervised dependency parsing demonstrate the advantage of mDir over Dir.  
We introduce a recurrent neural network language model (RNN-LM) with long shortterm memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively Ô¨Ånds the optimal mixture of the character-level and wordlevel inputs. The gate creates the Ô¨Ånal vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The Ô¨Ånal vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-ofvocabulary words and outperforms word-level language models on several English corpora. 
We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method signiÔ¨Åcantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical signiÔ¨Åcantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 
We adapt the greedy stack LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modiÔ¨Åcations needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 
In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task‚Äôs performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candidate argument belongs to a given predicate, then the other is less likely to belong to the same predicate. However, previous works did not explicitly model argument relationships. We use a simple maximum entropy classiÔ¨Åer to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 
The brain is the locus of our language ability, and so brain images can be used to ground linguistic theories. Here we introduce BrainBench, a lightweight system for testing distributional models of word semantics. We compare the performance of several models, and show that the performance on brain-image tasks differs from the performance on behavioral tasks. We release our benchmark test as part of a web service. 
We compare the effectiveness of four different syntactic CCG parsers for a semantic slotÔ¨Ålling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 
The PPDB is an automatically built database which contains millions of paraphrases in different languages. Paraphrases in this resource are associated with features that serve to their ranking and reÔ¨Çect paraphrase quality. This context-unaware ranking captures the semantic similarity of paraphrases but cannot serve to estimate their adequacy in speciÔ¨Åc contexts. We propose to use vector-space semantic models for selecting PPDB paraphrases that preserve the meaning of speciÔ¨Åc text fragments. This is the Ô¨Årst work that addresses the substitutability of PPDB paraphrases in context. We show that vector-space models of meaning can be successfully applied to this task and increase the beneÔ¨Åt brought by the use of the PPDB resource in applications. 
We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identiÔ¨Åcation of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, Ô¨Årst derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks. 
We investigate whether psychological wellbeing translates across English and Spanish Twitter, by building and comparing source language and automatically translated weighted lexica in English and Spanish. We Ô¨Ånd that the source language models perform substantially better than the machine translated versions. Moreover, manually correcting translation errors does not improve model performance, suggesting that meaningful cultural information is being lost in translation. Further work is needed to clarify when automatic translation of well-being lexica is effective and how it can be improved for crosscultural analysis. 
While much computational work on Ô¨Åction has focused on works in the literary canon, user-created fanÔ¨Åction presents a unique opportunity to study an ecosystem of literary production and consumption, embodying qualities both of large-scale literary data (55 billion tokens) and also a social network (with over 2 million users). We present several empirical analyses of this data in order to illustrate the range of affordances it presents to research in NLP, computational social science and the digital humanities. We Ô¨Ånd that fanÔ¨Åction deprioritizes main protagonists in comparison to canonical texts, has a statistically signiÔ¨Åcant difference in attention allocated to female characters, and offers a framework for developing models of reader reactions to stories. 
Psychological analysis of language has repeatedly shown that an individual‚Äôs rate of mentioning 1st person singular pronouns predicts a wealth of important demographic and psychological factors. However, these analyses are performed out of context ‚Äî syntactic and semantic ‚Äî which may change the magnitude or even direction of such relationships. In this paper, we put ‚Äúpronouns in their context‚Äù, exploring the relationship between self-reference and age, gender, and depression depending on syntactic position and verbal governor. We Ô¨Ånd that pronouns are overall more predictive when taking dependency relations and verb semantic categories into account, and, the direction of the relationship can change depending on the semantic class of the verbal governor. 
In this paper, we experiment with a resource consisting of metaphorically annotated proverbs on the task of word-level metaphor recognition. We observe that existing feature sets do not perform well on this data. We design a novel set of features to better capture the peculiar nature of proverbs and we demonstrate that these new features are significantly more effective on the metaphorically dense proverb data. 
We develop a novel embedding-based model for predicting legislative roll-call votes from bill text. The model introduces multidimensional ideal vectors for legislators as an alternative to single dimensional ideal point models for quantitatively analyzing roll-call data. These vectors are learned to correspond with pre-trained word embeddings which allows us to analyze which features in a bill text are most predictive of political support. Our model is quite simple, while at the same time allowing us to successfully predict legislator votes on speciÔ¨Åc bills with higher accuracy than past methods. 
Natural language understanding is the core of the human computer interactions. However, building new domains and tasks that need a separate set of models is a bottleneck for scaling to a large number of domains and experiences. In this paper, we propose a practical technique that addresses this issue in a web-scale language understanding system: Microsoft‚Äôs personal digital assistant Cortana. The proposed technique uses a constrained decoding method with a universal slot tagging model sharing the same schema as the collection of slot taggers built for each domain. The proposed approach allows reusing of slots across different domains and tasks while achieving virtually the same performance as those slot taggers trained per domain fashion. 
Recurrent Neural Network (RNN) and one of its speciÔ¨Åc architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. Explicitly modeling output label dependencies on top of RNN/LSTM is a widely-studied and effective extension. We propose another extension to incorporate the global information spanning over the whole input sequence. The proposed method, encoder-labeler LSTM, Ô¨Årst encodes the whole input sequence into a Ô¨Åxed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. With this method, we can predict the label sequence while taking the whole input sequence information into consideration. In the experiments of a slot Ô¨Ålling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%. 
In this paper, we investigate case restoration for text without case information. Previous such work operates at the word level. We propose an approach using character-level recurrent neural networks (RNN), which performs competitively compared to language modeling and conditional random Ô¨Åelds (CRF) approaches. We further provide quantitative and qualitative analysis on how RNN helps improve truecasing. 
This study measures the effects of Federal Open Market Committee text content on the direction of short- and medium-term interest rate movements. Because the words relevant to short- and medium-term interest rates differ, we apply a supervised approach to learn distinct sets of topics for each dependent variable being examined. We generate predictions with and without controlling for factors relevant to interest rate movements, and our prediction results average across multiple training-test splits. Using data from 1999-2016, we achieve 93% and 64% accuracy in predicting Target and Effective Federal Funds Rate movements and 38%-40% accuracy in predicting longer term Treasury Rate movements. We obtain lower but comparable accuracies after controlling for other macroeconomic and market factors. 
We propose a text-based recommendation engine that utilizes recurrent neural networks to Ô¨Çexibly map textual input into continuous vector representations tailored to the recommendation task. Here, the text objects are documents such as Wikipedia articles or question and answer pairs. As neural models require substantial training time, we introduce a sequential component so as to quickly adjust the learned metric over objects as additional evidence accrues. We evaluate the approach on recommending Wikipedia descriptions of ingredients to their associated product categories. We also exemplify the sequential metric adjustment on retrieving similar Stack Exchange AskUbuntu questions. 1 
Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics. 
Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes like subjectiÔ¨Åcation. Understanding the evolution of language and culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect two different types of semantic change. The Ô¨Årst measure, which has been used in many previous works, analyzes global shifts in a word‚Äôs distributional semantics; it is sensitive to changes due to regular processes of linguistic drift, such as the semantic generalization of promise (‚ÄúI promise.‚Äù‚Üí‚ÄúIt promised to be exciting.‚Äù). The second measure, which we develop here, focuses on local changes to a word‚Äôs nearest semantic neighbors; it is more sensitive to cultural shifts, such as the change in the meaning of cell (‚Äúprison cell‚Äù ‚Üí ‚Äúcell phone‚Äù). Comparing measurements made by these two methods allows researchers to determine whether changes are more cultural or linguistic in nature, a distinction that is essential for work in the digital humanities and historical linguistics. 
 To create conversational systems working in actual situations, it is crucial to assume that they interact with multiple agents. In this work, we tackle addressee and response selection for multi-party conversation, in which systems are expected to select whom they address as well as what they say. The key challenge of this task is to jointly model who is talking about what in a previous context. For the joint modeling, we propose two modeling frameworks: 1) static modeling and 2) dynamic modeling. To show benchmark results of our frameworks, we created a multi-party conversation corpus. Our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents. 
In this paper, we propose a new generative approach for semantic slot Ô¨Ålling task in spoken language understanding using a nonparametric Bayesian formalism. Slot Ô¨Ålling is typically formulated as a sequential labeling problem, which does not directly deal with the posterior distribution of possible slot values. We present a nonparametric Bayesian model involving the generation of arbitrary natural language phrases, which allows an explicit calculation of the distribution over an inÔ¨Ånite set of slot values. We demonstrate that this approach signiÔ¨Åcantly improves slot estimation accuracy compared to the existing sequential labeling algorithm. 
Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an endto-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate Ô¨Årstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used. 
We consider the task of predicting lexical entailment using distributional vectors. We perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs. We Ô¨Ånd that the model strongly learns to identify hypernyms using Hearst patterns, which are well known to be predictive of lexical relations. We present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to Principal Component Analysis. Our model combines the extracted features with the strengths of other proposed models in the literature, and matches or outperforms prior work on multiple data sets. 
Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together inÔ¨Çuence human understanding of verb meaning. Further, with signiÔ¨Åcantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning. 
Language resources that systematically organize paraphrases for binary relations are of great value for various NLP tasks and have recently been advanced in projects like PATTY, WiseNet and DEFIE. This paper presents a new method for building such a resource and the resource itself, called POLY. Starting with a very large collection of multilingual sentences parsed into triples of phrases, our method clusters relational phrases using probabilistic measures. We judiciously leverage Ô¨Åne-grained semantic typing of relational arguments for identifying synonymous phrases. The evaluation of POLY shows signiÔ¨Åcant improvements in precision and recall over the prior works on PATTY and DEFIE. An extrinsic use case demonstrates the beneÔ¨Åts of POLY for question answering. 
We study the problem of jointly aligning sentence constituents and predicting their similarities. While extensive sentence similarity data exists, manually generating reference alignments and labeling the similarities of the aligned chunks is comparatively onerous. This prompts the natural question of whether we can exploit easy-to-create sentence level data to train better aligners. In this paper, we present a model that learns to jointly align constituents of two sentences and also predict their similarities. By taking advantage of both sentence and constituent level data, we show that our model achieves state-of-the-art performance at predicting alignments and constituent similarities. 
We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards preexisting values. We study the inÔ¨Çuence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with humanbased annotations. Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing. Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for English newswire. We discuss the impact of our Ô¨Åndings on strategies for future annotation efforts and parser evaluations.1 
Implicative verbs (e.g. manage) entail their complement clauses, while non-implicative verbs (e.g. want) do not. For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem. Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization. We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their complements. We show that this yields an effective, data-driven way of capturing this nuanced property in verbs. 
We have constructed a new ‚ÄúWho-did-What‚Äù dataset of over 200,000 Ô¨Åll-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles ‚Äî an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization ‚Äî each choice is a person named entity. Third, the problems have been Ô¨Åltered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.1 
This paper presents a system that compositionally maps outputs of a wide-coverage Japanese CCG parser onto semantic representations and performs automated inference in higher-order logic. The system is evaluated on a textual entailment dataset. It is shown that the system solves inference problems that focus on a variety of complex linguistic phenomena, including those that are difÔ¨Åcult to represent in the standard Ô¨Årst-order logic. 
The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fouriertransformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model‚Äôs output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (‚Äúgreenish‚Äù), bare modiÔ¨Åers (‚Äúbright‚Äù, ‚Äúdull‚Äù), and compositional phrases (‚Äúfaded teal‚Äù) not seen in training. 
We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements. 
Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classiÔ¨Åcation is the bottleneck for discourse parsing. Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred. This paper proposes a stacking neural network model to solve the classiÔ¨Åcation problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems. 
Dependency tree-to-tree translation models are powerful because they can naturally handle long range reorderings which is important for distant language pairs. The translation process is easy if it can be accomplished only by replacing non-terminals in translation rules with other rules. However it is sometimes necessary to adjoin translation rules. Flexible non-terminals have been proposed as a promising solution for this problem. A Ô¨Çexible non-terminal provides several insertion position candidates for the rules to be adjoined, but it increases the computational cost of decoding. In this paper we propose a neural network based insertion position selection model to reduce the computational cost by selecting the appropriate insertion positions. The experimental results show the proposed model can select the appropriate insertion position with a high accuracy. It reduces the decoding time and improves the translation quality owing to reduced search space. 
We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, Ô¨Ånding that a collection of hidden units learns to explicitly implement this functionality. 
In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the ‚Äútrue‚Äù alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities signiÔ¨Åcantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. 
Mapping word embeddings of different languages into a single space has multiple applications. In order to map from a source space into a target space, a common approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary. In this paper, we propose a framework that generalizes previous work, provides an efÔ¨Åcient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task. 
In this paper we discuss a process for quantifying the behavioral impact of a domaincustomized machine translation system deployed on a large-scale e-commerce platform. We discuss several machine translation systems that we trained using aligned text from product listing descriptions written in multiple languages. We document the quality improvements of these systems as measured through automated quality measures and crowdsourced human quality assessments. We then measure the effect of these quality improvements on user behavior using an automated A/B testing framework. Through testing we observed an increase in key ecommerce metrics, including a signiÔ¨Åcant increase in purchases. 
Open information extraction (Open IE) was presented as an unrestricted variant of traditional information extraction. It has been gaining substantial attention, manifested by a large number of automatic Open IE extractors and downstream applications. In spite of this broad attention, the Open IE task deÔ¨Ånition has been lacking ‚Äì there are no formal guidelines and no large scale gold standard annotation. Subsequently, the various implementations of Open IE resorted to small scale posthoc evaluations, inhibiting an objective and reproducible cross-system comparison. In this work, we develop a methodology that leverages the recent QA-SRL annotation to create a Ô¨Årst independent and large scale Open IE annotation,1 and use it to automatically compare the most prominent Open IE systems. 
To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition. These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages. Incorporating these data via a multi-task neural network model achieves signiÔ¨Åcant improvements over baselines, on both the English PDTB and Chinese CDTB data sets. 
We introduce a novel approach to the decoding problem in transition-based parsing: heuristic backtracking. This algorithm uses a series of partial parses on the sentence to locate the best candidate parse, using conÔ¨Ådence estimates of transition decisions as a heuristic to guide the starting points of the search. This allows us to achieve a parse accuracy comparable to beam search, despite using fewer transitions. When used to augment a Stack-LSTM transition-based parser, the parser shows an unlabeled attachment score of up to 93.30% for English and 87.61% for Chinese. 
Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We Ô¨Ånd that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is even more effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time. 
Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield Ô¨Çat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure‚Äî especially in the case of derivational morphology. In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the Ô¨Årst attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.1 
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing ‚Äî 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.  (a)  S  NP  VP  dogs chase NP  cats (b) (S (NP dogs )NP (VP chase (NP cats )NP )VP )S  Figure 1: A tree (a) and its sequential form (b). There is a one-to-one mapping between a tree and its sequential form. (Part-of-speech tags are not used.)  
This paper demonstrates that it is possible for a parser to improve its performance with a human in the loop, by posing simple questions to non-experts. For example, given the Ô¨Årst sentence of this abstract, if the parser is uncertain about the subject of the verb ‚Äúpose,‚Äù it could generate the question What would pose something? with candidate answers this paper and a parser. Any Ô¨Çuent speaker can answer this question, and the correct answer resolves the original uncertainty. We apply the approach to a CCG parser, converting uncertain attachment decisions into natural language questions about the arguments of verbs. Experiments show that crowd workers can answer these questions quickly, accurately and cheaply. Our human-in-the-loop parser improves on the state of the art with less than 2 questions per sentence on average, with a gain of 1.7 F1 on the 10% of sentences whose parses are changed. 
This paper presents a generic approach to content selection for creating timelines from individual history articles for which no external information about the same topic is available. This scenario is in contrast to existing works on timeline generation, which require the presence of a large corpus of news articles. To identify salient events in a given history article, we exploit lexical cues about the article‚Äôs subject area, as well as time expressions that are syntactically attached to an event word. We also test different methods of ensuring timeline coverage of the entire historical time span described. Our best-performing method outperforms a new unsupervised baseline and an improved version of an existing supervised approach. We see our work as a step towards more semantically motivated approaches to single-document summarisation. 
Most existing knowledge base (KB) embedding methods solely learn from time-unknown fact triples but neglect the temporal information in the knowledge base. In this paper, we propose a novel time-aware KB embedding approach taking advantage of the happening time of facts. SpeciÔ¨Åcally, we use temporal order constraints to model transformation between time-sensitive relations and enforce the embeddings to be temporally consistent and more accurate. We empirically evaluate our approach in two tasks of link prediction and triple classiÔ¨Åcation. Experimental results show that our method outperforms other baselines on the two tasks consistently. 
Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufÔ¨Åcient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Qnetwork, trained to optimize a reward function that reÔ¨Çects extraction accuracy while penalizing extra effort. Our experiments on two databases ‚Äì of shooting incidents, and food adulteration cases ‚Äì demonstrate that our system signiÔ¨Åcantly outperforms traditional extractors and a competitive meta-classiÔ¨Åer baseline.1 
We introduce the Ô¨Årst global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efÔ¨Åcient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a novel objective that encourages the parser to search both efÔ¨Åciently and accurately. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser Ô¨Ånds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees. 
Language documentation begins by gathering speech. Manual or automatic transcription at the word level is typically not possible because of the absence of an orthography or prior lexicon, and though manual phonemic transcription is possible, it is prohibitively slow. On the other hand, translations of the minority language into a major language are more easily acquired. We propose a method to harness such translations to improve automatic phoneme recognition. The method assumes no prior lexicon or translation model, instead learning them from phoneme lattices and translations of the speech being transcribed. Experiments demonstrate phoneme error rate improvements against two baselines and the model‚Äôs ability to learn useful bilingual lexical entries. 
We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a signiÔ¨Åcant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com. 
Compositional distributional models of meaning (CDMs) provide a function that produces a vectorial representation for a phrase or a sentence by composing the vectors of its words. Being the natural evolution of the traditional and well-studied distributional models at the word level, CDMs are steadily evolving to a popular and active area of NLP. This COLING 2016 tutorial aims at providing a concise introduction to this emerging field, presenting the different classes of CDMs and the various issues related to them in sufficient detail.
The rapid accumulation of data in social media (in million and billion scales) has imposed great challenges in information extraction, knowledge discovery, and data mining, and texts bearing sentiment and opinions are one of the major categories of user generated data in social media. Sentiment analysis is the main technology to quickly capture what people think from these text data, and is a research direction with immediate practical value in {`}big data{'} era. Learning such techniques will allow data miners to perform advanced mining tasks considering real sentiment and opinions expressed by users in additional to the statistics calculated from the physical actions (such as viewing or purchasing records) user perform, which facilitates the development of real-world applications. However, the situation that most tools are limited to the English language might stop academic or industrial people from doing research or products which cover a wider scope of data, retrieving information from people who speak different languages, or developing applications for worldwide users. More specifically, sentiment analysis determines the polarities and strength of the sentiment-bearing expressions, and it has been an important and attractive research area. In the past decade, resources and tools have been developed for sentiment analysis in order to provide subsequent vital applications, such as product reviews, reputation management, call center robots, automatic public survey, etc. However, most of these resources are for the English language. Being the key to the understanding of business and government issues, sentiment analysis resources and tools are required for other major languages, e.g., Chinese. In this tutorial, audience can learn the skills for retrieving sentiment from texts in another major language, Chinese, to overcome this obstacle. The goal of this tutorial is to introduce the proposed sentiment analysis technologies and datasets in the literature, and give the audience the opportunities to use resources and tools to process Chinese texts from the very basic preprocessing, i.e., word segmentation and part of speech tagging, to sentiment analysis, i.e., applying sentiment dictionaries and obtaining sentiment scores, through step-by-step instructions and a hand-on practice. The basic processing tools are from CKIP Participants can download these resources, use them and solve the problems they encounter in this tutorial. This tutorial will begin from some background knowledge of sentiment analysis, such as how sentiment are categorized, where to find available corpora and which models are commonly applied, especially for the Chinese language. Then a set of basic Chinese text processing tools for word segmentation, tagging and parsing will be introduced for the preparation of mining sentiment and opinions. After bringing the idea of how to pre-process the Chinese language to the audience, I will describe our work on compositional Chinese sentiment analysis from words to sentences, and an application on social media text (Facebook) as an example. All our involved and recently developed related resources, including Chinese Morphological Dataset, Augmented NTU Sentiment Dictionary (aug-NTUSD), E-hownet with sentiment information, Chinese Opinion Treebank, and the CopeOpi Sentiment Scorer, will also be introduced and distributed in this tutorial. The tutorial will end by a hands-on session of how to use these materials and tools to process Chinese sentiment. Content Details, Materials, and Program please refer to the tutorial URL: \url{http://www.lunweiku.com/}
During the last decade the amount of scientific information available on-line increased at an unprecedented rate. As a consequence, nowadays researchers are overwhelmed by an enormous and continuously growing number of articles to consider when they perform research activities like the exploration of advances in specific topics, peer reviewing, writing and evaluation of proposals. Natural Language Processing Technology represents a key enabling factor in providing scientists with intelligent patterns to access to scientific information. Extracting information from scientific papers, for example, can contribute to the development of rich scientific knowledge bases which can be leveraged to support intelligent knowledge access and question answering. Summarization techniques can reduce the size of long papers to their essential content or automatically generate state-of-the-art-reviews. Paraphrase or textual entailment techniques can contribute to the identification of relations across different scientific textual sources. This tutorial provides an overview of the most relevant tasks related to the processing of scientific documents, including but not limited to the in-depth analysis of the structure of the scientific articles, their semantic interpretation, content extraction and summarization.
Quality Estimation (QE) of language output applications is a research area that has been attracting significant attention. The goal of QE is to estimate the quality of language output applications without the need of human references. Instead, machine learning algorithms are used to build supervised models based on a few labelled training instances. Such models are able to generalise over unseen data and thus QE is a robust method applicable to scenarios where human input is not available or possible. One such a scenario where QE is particularly appealing is that of Machine Translation, where a score for predicted quality can help decide whether or not a translation is useful (e.g. for post-editing) or reliable (e.g. for gisting). Other potential applications within Natural Language Processing (NLP) include Text Summarisation and Text Simplification. In this tutorial we present the task of QE and its application in NLP, focusing on Machine Translation. We also introduce QuEst++, a toolkit for QE that encompasses feature extraction and machine learning, and propose a practical activity to extend this toolkit in various ways.
Translated texts, in any language, have unique characteristics that set them apart from texts originally written in the same language. Translation Studies is a research field that focuses on investigating these characteristics. Until recently, research in machine translation (MT) has been entirely divorced from translation studies. The main goal of this tutorial is to introduce some of the findings of translation studies to researchers interested mainly in machine translation, and to demonstrate that awareness to these findings can result in better, more accurate MT systems.
Succinct data structures involve the use of novel data structures, compression technologies, and other mechanisms to allow data to be stored in extremely small memory or disk footprints, while still allowing for efficient access to the underlying data. They have successfully been applied in areas such as Information Retrieval and Bioinformatics to create highly compressible in-memory search indexes which provide efficient search functionality over datasets which traditionally could only be processed using external memory data structures. Modern technologies in this space are not well known within the NLP community, but have the potential to revolutionise NLP, particularly the application to {`}big data{'} in the form of terabyte and larger corpora. This tutorial will present a practical introduction to the most important succinct data structures, tools, and applications with the intent of providing the researchers with a jump-start into this domain. The focus of this tutorial will be efficient text processing utilising space efficient representations of suffix arrays, suffix trees and searchable integer compression schemes with specific applications of succinct data structures to common NLP tasks such as $n$-gram language modelling.
This tutorial examines the characteristics, advantages and limitations of Wikipedia relative to other existing, human-curated resources of knowledge; derivative resources, created by converting semi-structured content in Wikipedia into structured data; the role of Wikipedia and its derivatives in text analysis; and the role of Wikipedia and its derivatives in enhancing information retrieval.
We present an interactive system to provide effective and efficient search capabilities in Community Question Answering (cQA) forums. The system integrates state-of-the-art technology for answer search with a Web-based user interface specifically tailored to support the cQA forum readers. The answer search module automatically finds relevant answers for a new question by exploring related questions and the comments within their threads. The graphical user interface presents the search results and supports the exploration of related information. The system is running live at \url{http://www.qatarliving.com/betasearch/}.
We present a Natural Language Interface (nlmaps.cl.uni-heidelberg.de) to query OpenStreetMap. Natural language questions about geographical facts are parsed into database queries that can be executed against the OpenStreetMap (OSM) database. After parsing the question, the system provides a text based answer as well as an interactive map with all points of interest and their relevant information marked. Additionally, we provide several options for users to give feedback after a question has been parsed.
We present a mobile app that provides a reading environment for learners of Chinese as a foreign language. The app includes a text database that offers over 500K articles from Chinese Wikipedia. These articles have been word-segmented; each word is linked to its entry in a Chinese-English dictionary, and to automatically-generated review exercises. The app estimates the reading proficiency of the user based on a {``}to-learn{''} list of vocabulary items. It automatically constructs and maintains this list by tracking the user{'}s dictionary lookup behavior and performance in review exercises. When a user searches for articles to read, search results are filtered such that the proportion of unknown words does not exceed a user-specified threshold.
Adaptive machine translation (MT) systems are a promising approach for improving the effectiveness of computer-aided translation (CAT) environments. There is, however, virtually only theoretical work that examines how such a system could be implemented. We present an open source post-editing interface for adaptive statistical MT, which has in-depth monitoring capabilities and excellent expandability, and can facilitate practical studies. To this end, we designed text-based and graphical post-editing interfaces. The graphical interface offers means for displaying and editing a rich view of the MT output. Our translation systems may learn from post-edits using several weight, language model and novel translation model adaptation techniques, in part by exploiting the output of the graphical interface. In a user study we show that using the proposed interface and adaptation methods, reductions in technical effort and time can be achieved.
Adult second language learners face the daunting but underappreciated task of mastering patterns of language use that are neither products of fully productive grammar rules nor frozen items to be memorized. Word Midas, a web browser extention, targets this uncharted territory of lexicogrammar by detecting multiword tokens of lexicogrammatical patterning in real time in situ within the noisy digital texts from the user{'}s unscripted web browsing or other digital venues. The language model powering Word Midas is StringNet, a densely cross-indexed navigable network of one billion lexicogrammatical patterns of English. These resources are described and their functionality is illustrated with a detailed scenario.
The National Institute for Japanese Language and Linguistics, Japan (NINJAL) has undertaken a corpus compilation project to construct a web corpus for linguistic research comprising ten billion words. The project is divided into four parts: page collection, linguistic analysis, development of the corpus concordance system, and preservation. This article presents the corpus concordance system named {`}BonTen{'} which enables the ten-billion-scaled corpus to be queried by string, a sequence of morphological information or a subtree of the syntactic dependency structure.
Simultaneous interpretation allows people to communicate spontaneously across language boundaries, but such services are prohibitively expensive for the general public. This paper presents a fully automatic simultaneous interpretation system to address this problem. Though the development is still at an early stage, the system is capable of keeping up with the fastest of the TED speakers while at the same time delivering high-quality translations. We believe that the system will become an effective tool for facilitating cross-lingual communication in the future.
The paper introduces a web-based authoring support system, MuTUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system.
In this paper and the associated system demo, we present an advanced search system that allows to perform a joint search over a (bilingual) valency lexicon and a correspondingly annotated linked parallel corpus. This search tool has been developed on the basis of the Prague Czech-English Dependency Treebank, but its ideas are applicable in principle to any bilingual parallel corpus that is annotated for dependencies and valency (i.e., predicate-argument structure), and where verbs are linked to appropriate entries in an associated valency lexicon. Our online search tool consolidates more search interfaces into one, providing expanded structured search capability and a more efficient advanced way to search, allowing users to search for verb pairs, verbal argument pairs, their surface realization as recorded in the lexicon, or for their surface form actually appearing in the linked parallel corpus. The search system is currently under development, and is replacing our current search tool available at \url{http://lindat.mff.cuni.cz/services/CzEngVallex}, which could search the lexicon but the queries cannot take advantage of the underlying corpus nor use the additional surface form information from the lexicon(s). The system is available as open source.
In this study we develop a system that tags and extracts financial concepts called financial named entities (FNE) along with corresponding numeric values {--} monetary and temporal. We employ machine learning and natural language processing methods to identify financial concepts and dates, and link them to numerical entities.
ChaKi.NET is a corpus management system for dependency structure annotated corpora. After more than 10 years of continuous development, the system is now usable not only for corpus search, but also for visualization, annotation, labelling, and formatting for statistical analysis. This paper describes the various functions included in the current ChaKi.NET system.
Much existing work in text-to-scene generation focuses on generating static scenes. By introducing a focus on motion verbs, we integrate dynamic semantics into a rich formal model of events to generate animations in real time that correlate with human conceptions of the event described. This paper presents a working system that generates these animated scenes over a test set, discussing challenges encountered and describing the solutions implemented.
More and more disciplines require NLP tools for performing automatic text analyses on various levels of linguistic resolution. However, the usage of established NLP frameworks is often hampered for several reasons: in most cases, they require basic to sophisticated programming skills, interfere with interoperability due to using non-standard I/O-formats and often lack tools for visualizing computational results. This makes it difficult especially for humanities scholars to use such frameworks. In order to cope with these challenges, we present TextImager, a UIMA-based framework that offers a range of NLP and visualization tools by means of a user-friendly GUI. Using TextImager requires no programming skills.
This paper presents Disco, a prototype for supporting knowledge workers in exploring, reviewing and sorting collections of textual data. The goal is to facilitate, accelerate and improve the discovery of information. To this end, it combines Semantic Relatedness techniques with a review workflow developed in a tangible environment. Disco uses a semantic model that is leveraged on-line in the course of search sessions, and accessed through natural hand-gesture, in a simple and intuitive way.
In this paper, we describe \textbf{Langforia}, a multilingual processing pipeline to annotate texts with multiple layers: formatting, parts of speech, named entities, dependencies, semantic roles, and entity links. Langforia works as a web service, where the server hosts the language processing components and the client, the input and result visualization. To annotate a text or a Wikipedia page, the user chooses an NLP pipeline and enters the text in the interface or selects the page URL. Once processed, the results are returned to the client, where the user can select the annotation layers s/he wants to visualize. We designed Langforia with a specific focus for Wikipedia, although it can process any type of text. Wikipedia has become an essential encyclopedic corpus used in many NLP projects. However, processing articles and visualizing the annotations are nontrivial tasks that require dealing with multiple markup variants, encodings issues, and tool incompatibilities across the language versions. This motivated the development of a new architecture. A demonstration of Langforia is available for six languages: English, French, German, Spanish, Russian, and Swedish at \url{http://vilde.cs.lth.se:9000/} as well as a web API: \url{http://vilde.cs.lth.se:9000/api}. Langforia is also provided as a standalone library and is compatible with cluster computing.
We introduce Anita: a flexible and intelligent Text Adaptation tool for web content that provides Text Simplification and Text Enhancement modules. Anita{'}s simplification module features a state-of-the-art system that adapts texts according to the needs of individual users, and its enhancement module allows the user to search for a word{'}s definitions, synonyms, translations, and visual cues through related images. These utilities are brought together in an easy-to-use interface of a freely available web browser extension.
Recent years have witnessed significant increase in the number of large scale digital collections of archival documents such as news articles, books, etc. Typically, users access these collections through searching or browsing. In this paper we investigate another way of accessing temporal collections - across-time comparison, i.e., comparing query-relevant information at different periods in the past. We propose an interactive framework called HistoryComparator for contrastively analyzing concepts in archival document collections at different time periods.
In this demo, we present our free on-line multilingual linguistic services which allow to analyze sentences or to extract collocations from a corpus directly on-line, or by uploading a corpus. They are available for 8 European languages (English, French, German, Greek, Italian, Portuguese, Romanian, Spanish) and can also be accessed as web services by programs. While several open systems are available for POS-tagging and dependency parsing or terminology extraction, their integration into an application requires some computational competence. Furthermore, none of the parsers/taggers handles MWEs very satisfactorily, in particular when the two terms of the collocation are distant from each other or in reverse order. Our tools, on the other hand, are specifically designed for users with no particular computational literacy. They do not require from the user any download, installation or adaptation if used on-line, and their integration in an application, using one the scripts described below is quite easy. Furthermore, by default, the parser handles collocations and other MWEs, as well as anaphora resolution (limited to 3rd person personal pronouns). When used in the tagger mode, it can be set to display grammatical functions and collocations.
We present a browser-based editor for simplifying English text. Given an input sentence, the editor performs both syntactic and lexical simplification. It splits a complex sentence into shorter ones, and suggests word substitutions in drop-down lists. The user can choose the best substitution from the list, undo any inappropriate splitting, and further edit the sentence as necessary. A significant novelty is that the system accepts a customized vocabulary list for a target reader population. It identifies all words in the text that do not belong to the list, and attempts to substitute them with words from the list, thus producing a text tailored for the targeted readers.
We present a free web-based CAT tool called CATaLog Online which provides a novel and user-friendly online CAT environment for post-editors/translators. The goal is to support distributed translation, reduce post-editing time and effort, improve the post-editing experience and capture data for incremental MT/APE (automatic post-editing) and translation process research. The tool supports individual as well as batch mode file translation and provides translations from three engines {--} translation memory (TM), MT and APE. TM suggestions are color coded to accelerate the post-editing task. The users can integrate their personal TM/MT outputs. The tool remotely monitors and records post-editing activities generating an extensive range of post-editing logs.
We present INDREX-MM, a main memory database system for interactively executing two interwoven tasks, declarative relation extraction from text and their exploitation with SQL. INDREX-MM simplifies these tasks for the user with powerful SQL extensions for gathering statistical semantics, for executing open information extraction and for integrating relation candidates with domain specific data. We demonstrate these functions on 800k documents from Reuters RCV1 with more than a billion linguistic annotations and report execution times in the order of seconds.
In this paper, we introduce an original Python implementation of datetime resolution in french, which we make available as open-source library. Our approach is based on Frame Semantics and Corpus Pattern Analysis in order to provide a precise semantic interpretation of datetime expressions. This interpretation facilitates the contextual resolution of datetime expressions in timestamp format.
We introduce TASTY (Tag-as-you-type), a novel text editor for interactive entity linking as part of the writing process. Tasty supports the author of a text with complementary information about the mentioned entities shown in a {`}live{'} exploration view. The system is automatically triggered by keystrokes, recognizes mention boundaries and disambiguates the mentioned entities to Wikipedia articles. The author can use seven operators to interact with the editor and refine the results according to his specific intention while writing. Our implementation captures syntactic and semantic context using a robust end-to-end LSTM sequence learner and word embeddings. We demonstrate the applicability of our system in English and German language for encyclopedic or medical text. Tasty is currently being tested in interactive applications for text production, such as scientific research, news editorial, medical anamnesis, help desks and product reviews.
We demonstrate a bilingual robot application, WikiTalk, that can talk fluently in both English and Japanese about almost any topic using information from English and Japanese Wikipedias. The English version of the system has been demonstrated previously, but we now present a live demo with a Nao robot that speaks English and Japanese and switches language on request. The robot supports the verbal interaction with face-tracking, nodding and communicative gesturing. One of the key features of the WikiTalk system is that the robot can switch from the current topic to related topics during the interaction in order to navigate around Wikipedia following the user{'}s individual interests.
The PDTB Annotator is a tool for annotating and adjudicating discourse relations based on the annotation framework of the Penn Discourse TreeBank (PDTB). This demo describes the benefits of using the PDTB Annotator, gives an overview of the PDTB Framework and discusses the tool{'}s features, setup requirements and how it can also be used for adjudication.
Opinion mining is a natural language processing technique which extracts subjective information from natural language text. To estimate an opinion about a query in large data collection, an opinion retrieval system that retrieves subjective and relevant information about the query can be useful. We present an opinion retrieval system that retrieves subjective and query-relevant tweets from Twitter, which is a useful source of obtaining real-time opinions. Our system outperforms previous opinion retrieval systems, and it further provides subjective information about Twitter authors and hashtags to describe their subjective tendencies.
This paper presents TextPro-AL (Active Learning for Text Processing), a platform where human annotators can efficiently work to produce high quality training data for new domains and new languages exploiting Active Learning methodologies. TextPro-AL is a web-based application integrating four components: a machine learning based NLP pipeline, an annotation editor for task definition and text annotations, an incremental re-training procedure based on active learning selection from a large pool of unannotated data, and a graphical visualization of the learning status of the system.
In this paper, we discuss our ongoing efforts to construct a scientific paper browsing system that helps users to read and understand advanced technical content distributed in PDF. Since PDF is a format specifically designed for printing, layout and logical structures of documents are indistinguishably embedded in the file. It requires much effort to extract natural language text from PDF files, and reversely, display semantic annotations produced by NLP tools on the original page layout. In our browsing system, we tackle these issues caused by the gap between printable document and plain text. Our system provides ways to extract natural language sentences from PDF files together with their logical structures, and also to map arbitrary textual spans to their corresponding regions on page images. We setup a demonstration system using papers published in ACL anthology and demonstrate the enhanced search and refined recommendation functions which we plan to make widely available to NLP researchers.
Instant messaging and push notifications play important roles in modern digital life. To enable robust sense-making and rich context awareness in computer mediated communications, we introduce EmotionPush, a system that automatically conveys the emotion of received text with a colored push notification on mobile devices. EmotionPush is powered by state-of-the-art emotion classifiers and is deployed for Facebook Messenger clients on Android. The study showed that the system is able to help users prioritize interactions.
We release a cross-lingual wikification system for all languages in Wikipedia. Given a piece of text in any supported language, the system identifies names of people, locations, organizations, and grounds these names to the corresponding English Wikipedia entries. The system is based on two components: a cross-lingual named entity recognition (NER) model and a cross-lingual mention grounding model. The cross-lingual NER model is a language-independent model which can extract named entity mentions in the text of any language in Wikipedia. The extracted mentions are then grounded to the English Wikipedia using the cross-lingual mention grounding model. The only resources required to train the proposed system are the multilingual Wikipedia dump and existing training data for English NER. The system is online at \url{http://cogcomp.cs.illinois.edu/page/demo_view/xl_wikifier}
This paper presents a meaning-based statistical math word problem (MWP) solver with understanding, reasoning and explanation. It comprises a web user interface and pipelined modules for analysing the text, transforming both body and question parts into their logic forms, and then performing inference on them. The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating the extracted math quantity with its associated syntactic and semantic information (which specifies the physical meaning of that quantity). Those role-tags are then used to identify the desired operands and filter out irrelevant quantities (so that the answer can be obtained precisely). Since the physical meaning of each quantity is explicitly represented with those role-tags and used in the inference process, the proposed approach could explain how the answer is obtained in a human comprehensible way.
This paper introduces Valencer: a RESTful API to search for annotated sentences matching a given combination of syntactic realizations of the arguments of a predicate {--} also called {`}valence pattern{'} {--} in the FrameNet database. The API takes as input an HTTP GET request specifying a valence pattern and outputs a list of exemplifying annotated sentences in JSON format. The API is designed to be modular and language-independent, and can therefore be easily integrated to other (NLP) server-side or client-side applications, as well as non-English FrameNet projects. Valencer is free, open-source, and licensed under the MIT license.
Developing a question answering (QA) system is a task of implementing and integrating modules of different technologies and evaluating an integrated whole system, which inevitably goes with a collaboration among experts of different domains. For supporting a easy collaboration, this demonstration presents the open framework that aims to support developing a QA system in collaborative and intuitive ways. The demonstration also shows the QA system developed by our novel framework.
This paper shows the great potential of incorporating different approaches to help writing. Not only did they solve different kinds of writing problems, but also they complement and reinforce each other to be a complete and effective solution. Despite the extensive and multifaceted feedback and suggestion, writing is not all about syntactically or lexically well-written. It involves contents, structure, the certain understanding of the background, and many other factors to compose a rich, organized and sophisticated text. (e.g., conventional structure and idioms in academic writing). There is still a long way to go to accomplish the ultimate goal. We envision the future of writing to be a joyful experience with the help of instantaneous suggestion and constructive feedback.
We present a text simplification approach that is directed at improving the performance of state-of-the-art Open Relation Extraction (RE) systems. As syntactically complex sentences often pose a challenge for current Open RE approaches, we have developed a simplification framework that performs a pre-processing step by taking a single sentence as input and using a set of syntactic-based transformation rules to create a textual input that is easier to process for subsequently applied Open RE systems.
FrameNet project has begun from Berkeley in 1997, and is now supported in several countries reflecting characteristics of each language. The work for generating Korean FrameNet was already done by converting annotated English sentences into Korean with trained translators. However, high cost of frame-preservation and error revision was a huge burden on further expansion of FrameNet. This study makes use of linguistic similarity between Japanese and Korean to increase Korean FrameNet corpus with low cost. We also suggest adapting PubAnnotation and Korean-friendly valence patterns to FrameNet for increased accessibility.
Any real world events or trends that can affect the company{'}s growth trajectory can be considered as risk. There has been a growing need to automatically identify, extract and analyze risk related statements from news events. In this demonstration, we will present a risk analytics framework that processes enterprise project management reports in the form of textual data and news documents and classify them into valid and invalid risk categories. The framework also extracts information from the text pertaining to the different categories of risks like their possible cause and impacts. Accordingly, we have used machine learning based techniques and studied different linguistic features like n-gram, POS, dependency, future timing, uncertainty factors in texts and their various combinations. A manual annotation study from management experts using risk descriptions collected for a specific organization was conducted to evaluate the framework. The evaluation showed promising results for automated risk analysis and identification.
In this paper, we introduce papago - a translator for mobile device which is equipped with new features that can provide convenience for users. The first feature is word sense disambiguation based on user feedback. By using the feature, users can select one among multiple meanings of a homograph and obtain the corrected translation with the user-selected sense. The second feature is the instant currency conversion of money expressions contained in a translation result with current exchange rate. Users can be quickly and precisely provided the amount of money converted as local currency when they travel abroad.
We demonstrate TopoText, an interactive tool for digital mapping of literary text. TopoText takes as input a literary piece of text such as a novel or a biography article and automatically extracts all place names in the text. The identified places are then geoparsed and displayed on an interactive map. TopoText calculates the number of times a place was mentioned in the text, which is then reflected on the map allowing the end-user to grasp the importance of the different places within the text. It also displays the most frequent words mentioned within a specified proximity of a place name in context or across the entire text. This can also be faceted according to part of speech tags. Finally, TopoText keeps the human in the loop by allowing the end-user to disambiguate places and to provide specific place annotations. All extracted information such as geolocations, place frequencies, as well as all user-provided annotations can be automatically exported as a CSV file that can be imported later by the same user or other users.
We present a system called ACE for Automatic Colloquialism and Errors detection for written Chinese. ACE is based on the combination of N-gram model and rule-base model. Although it focuses on detecting colloquial Cantonese (a dialect of Chinese) at the current stage, it can be extended to detect other dialects. We chose Cantonese becauase it has many interesting properties, such as unique grammar system and huge colloquial terms, that turn the detection task extremely challenging. We conducted experiments using real data and synthetic data. The results indicated that ACE is highly reliable and effective.
We build a tool to assist in content creation by mining the web for information relevant to a given topic. This tool imitates the process of essay writing by humans: searching for topics on the web, selecting content frag-ments from the found document, and then compiling these fragments to obtain a coherent text. The process of writing starts with automated building of a table of content by obtaining the list of key entities for the given topic extracted from web resources such as Wikipedia. Once a table of content is formed, each item forms a seed for web mining. The tool builds a full-featured structured Word document with table of content, section structure, images and captions and web references for all mined text fragments. Two linguistic technologies are employed: for relevance verification, we use similarity computed as a tree similarity between parse trees for a seed and candidate text fragment. For text coherence, we use a measure of agreement between a given and consecutive paragraph by tree kernel learning of their discourse trees. The tool is available at \url{http://animatronica.io/submit.html}.
This demo presents MAGES (multilingual angle-integrated grouping-based entity summarization), an entity summarization system for a large knowledge base such as DBpedia based on a entity-group-bound ranking in a single integrated entity space across multiple language-specific editions. MAGES offers a multilingual angle-integrated space model, which has the advantage of overcoming missing semantic tags (i.e., categories) caused by biases in different language communities, and can contribute to the creation of entity groups that are well-formed and more stable than the monolingual condition within it. MAGES can help people quickly identify the essential points of the entities when they search or browse a large volume of entity-centric data. Evaluation results on the same experimental data demonstrate that our system produces a better summary compared with other representative DBpedia entity summarization methods.
This paper presents BOTTA, the first Arabic dialect chatbot. We explore the challenges of creating a conversational agent that aims to simulate friendly conversations using the Egyptian Arabic dialect. We present a number of solutions and describe the different components of the BOTTA chatbot. The BOTTA database files are publicly available for researchers working on Arabic chatbot technologies. The BOTTA chatbot is also publicly available for any users who want to chat with it online.
Event detection and analysis with respect to public opinions and sentiments in social media is a broad and well-addressed research topic. However, the characteristics and sheer volume of noisy Twitter messages make this a difficult task. This demonstration paper describes a TWItter event Summarizer and Trend detector (TWIST) system for event detection, visualization, textual description, and geo-sentiment analysis of real-life events reported in Twitter.
This paper presents an implementation of the widely used speech analysis tool Praat as a web application with an extended functionality for feature annotation. In particular, Praat on the Web addresses some of the central limitations of the original Praat tool and provides (i) enhanced visualization of annotations in a dedicated window for feature annotation at interval and point segments, (ii) a dynamic scripting composition exemplified with a modular prosody tagger, and (iii) portability and an operational web interface. Speech annotation tools with such a functionality are key for exploring large corpora and designing modular pipelines.
In this paper, we present YAMAMA, a multi-dialect Arabic morphological analyzer and disambiguator. Our system is almost five times faster than the state-of-art MADAMIRA system with a slightly lower quality. In addition to speed, YAMAMA outputs a rich representation which allows for a wider spectrum of use. In this regard, YAMAMA transcends other systems, such as FARASA, which is faster but provides specific outputs catering to specific applications.
In this paper, we present CamelParser, a state-of-the-art system for Arabic syntactic dependency analysis aligned with contextually disambiguated morphological features. CamelParser uses a state-of-the-art morphological disambiguator and improves its results using syntactically driven features. The system offers a number of output formats that include basic dependency with morphological features, two tree visualization modes, and traditional Arabic grammatical analysis.
In this demonstration paper we describe Ambient Search, a system that displays and retrieves documents in real time based on speech input. The system operates continuously in ambient mode, i.e. it generates speech transcriptions and identifies main keywords and keyphrases, while also querying its index to display relevant documents without explicit query. Without user intervention, the results are dynamically updated; users can choose to interact with the system at any time, employing a conversation protocol that is enriched with the ambient information gathered continuously. Our evaluation shows that Ambient Search outperforms another implicit speech-based information retrieval system. Ambient search is available as open source software.
ConFarm is a web service dedicated to extraction of surface representations of verb and noun constructions from dependency annotated corpora of Russian texts. Currently, the extraction of constructions with a specific lemma from SynTagRus and Russian National Corpus is available. The system provides flexible interface that allows users to fine-tune the output. Extracted constructions are grouped by their contents to allow for compact representation, and the groups are visualised as a graph in order to help navigating the extraction results. ConFarm differs from similar existing tools for Russian language in that it offers full constructions, as opposed to extracting separate dependents of search word or working with collocations, and allows users to discover unexpected constructions as opposed to searching for examples of a user-defined construction.
This paper presents ENIAM, the first syntactic and semantic parser that generates semantic representations for sentences in Polish. The parser processes non-annotated data and performs tokenization, lemmatization, dependency recognition, word sense annotation, thematic role annotation, partial disambiguation and computes the semantic representation.
This paper presents a novel high-order dependency parsing framework that targets non-projective treebanks. It imitates how a human parses sentences in an intuitive way. At every step of the parse, it determines which word is the easiest to process among all the remaining words, identifies its head word and then folds it under the head word. Further, this work is flexible enough to be augmented with other parsing techniques.
A script is a type of knowledge representation in artificial intelligence (AI). This paper presents two methods for synthetically using collected scripts for story generation. The first method recursively generates long sequences of events and the second creates script networks. Although related studies generally use one or more scripts for story generation, this research synthetically uses many scripts to flexibly generate a diverse narrative.
Attorneys, judges, and others in the justice system are constantly surrounded by large amounts of legal text, which can be difficult to manage across many cases. We present CaseSummarizer, a tool for automated text summarization of legal documents which uses standard summary methods based on word frequency augmented with additional domain-specific knowledge. Summaries are then provided through an informative interface with abbreviations, significance heat maps, and other flexible controls. It is evaluated using ROUGE and human scoring against several other summarization systems, including summary text and feedback provided by domain experts.
We demonstrate our large-scale NLP systems: WISDOM X, DISAANA, and D-SUMM. WISDOM X provides numerous possible answers including unpredictable ones to widely diverse natural language questions to provide deep insights about a broad range of issues. DISAANA and D-SUMM enable us to assess the damage caused by large-scale disasters in real time using Twitter as an information source.
We present PolyglotIE, a web-based tool for developing extractors that perform Information Extraction (IE) over multilingual data. Our tool has two core features: First, it allows users to develop extractors against a unified abstraction that is shared across a large set of natural languages. This means that an extractor needs only be created once for one language, but will then run on multilingual data without any additional effort or language-specific knowledge on part of the user. Second, it embeds this abstraction as a set of views within a declarative IE system, allowing users to quickly create extractors using a mature IE query language. We present PolyglotIE as a hands-on demo in which users can experiment with creating extractors, execute them on multilingual text and inspect extraction results. Using the UI, we discuss the challenges and potential of using unified, crosslingual semantic abstractions as basis for downstream applications. We demonstrate multilingual IE for 9 languages from 4 different language groups: English, German, French, Spanish, Japanese, Chinese, Arabic, Russian and Hindi.
This paper presents WordForce, a system powered by the state of the art neural network model to visualize the learned user-dependent word embeddings from each post according to the post content and its engaged users. It generates the scatter plots to show the force of a word, i.e., whether the semantics of word embeddings from posts of different stances are clearly separated from the aspect of this controversial word. In addition, WordForce provides the dispersion and the distance of word embeddings from posts of different stance groups, and proposes the most controversial words accordingly to show clues to what people argue about in a debate.
Zara, or {`}Zara the Supergirl{'} is a virtual robot, that can exhibit empathy while interacting with an user, with the aid of its built in facial and emotion recognition, sentiment analysis, and speech module. At the end of the 5-10 minute conversation, Zara can give a personality analysis of the user based on all the user utterances. We have also implemented a real-time emotion recognition, using a CNN model that detects emotion from raw audio without feature extraction, and have achieved an average of 65.7{\%} accuracy on six different emotion classes, which is an impressive 4.5{\%} improvement from the conventional feature based SVM classification. Also, we have described a CNN based sentiment analysis module trained using out-of-domain data, that recognizes sentiment from the speech recognition transcript, which has a 74.8 F-measure when tested on human-machine dialogues.
Words to express relations in natural language (NL) statements may be different from those to represent properties in knowledge bases (KB). The vocabulary gap becomes barriers for knowledge base construction and retrieval. With the demo system called NL2KB in this paper, users can browse which properties in KB side may be mapped to for a given relational pattern in NL side. Besides, they can retrieve the sets of relational patterns in NL side for a given property in KB side. We describe how the mapping is established in detail. Although the mined patterns are used for Chinese knowledge base applications, the methodology can be extended to other languages.
PKUSUMSUM is a Java platform for multilingual document summarization, and it sup-ports multiple languages, integrates 10 automatic summarization methods, and tackles three typical summarization tasks. The summarization platform has been released and users can easily use and update it. In this paper, we make a brief description of the char-acteristics, the summarization methods, and the evaluation results of the platform, and al-so compare PKUSUMSUM with other summarization toolkits.
Kotonush, a system that clarifies people{'}s values on various concepts on the basis of what they write about on social media, is presented. The values are represented by ordering sets of concepts (e.g., London, Berlin, and Rome) in accordance with a common attribute intensity expressed by an adjective (e.g., entertaining). We exploit social media text written by different demographics and at different times in order to induce specific orderings for comparison. The system combines a text-to-ordering module with an interactive querying interface enabled by massive hyponymy relations and provides mechanisms to compare the induced orderings from various viewpoints. We empirically evaluate Kotonush and present some case studies, featuring real-world concept orderings with different domains on Twitter, to demonstrate the usefulness of our system.
We aim at showing that lexical descriptions based on multifactorial and continuous models can be used by linguists and lexicographers (and not only by machines) so long as they are provided with a way to efficiently navigate data collections. We propose to demonstrate such a system.
In this paper, we propose GiveMeExample that ranks example sentences according to their capacity of demonstrating the differences among English and Chinese near-synonyms for language learners. The difficulty of the example sentences is automatically detected. Furthermore, the usage models of the near-synonyms are built by the GMM and Bi-LSTM models to suggest the best elaborative sentences. Experiments show the good performance both in the fill-in-the-blank test and on the manually labeled gold data, that is, the built models can select the appropriate words for the given context and vice versa.
We present Kyoto-NMT, an open-source implementation of the Neural Machine Translation paradigm. This implementation is done in Python and Chainer, an easy-to-use Deep Learning Framework.
We present an efficient model selection method using boosting for transition-based constituency parsing. It is designed for exploring a high-dimensional search space, defined by a large set of feature templates, as for example is typically the case when parsing morphologically rich languages. Our method removes the need to manually define heuristic constraints, which are often imposed in current state-of-the-art selection methods. Our experiments for French show that the method is more efficient and is also capable of producing compact, state-of-the-art models.
Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain common syntactic knowledge that is potential to benefit each other. This paper presents a universal framework for transfer parsing across multi-typed treebanks with deep multi-task learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Knowledge across the source and target treebanks are effectively transferred through multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models.
Grammar induction is the task of learning syntactic structure in a setting where that structure is hidden. Grammar induction from words alone is interesting because it is similiar to the problem that a child learning a language faces. Previous work has typically assumed richer but cognitively implausible input, such as POS tag annotated data, which makes that work less relevant to human language acquisition. We show that grammar induction from words alone is in fact feasible when the model is provided with sufficient training data, and present two new streaming or mini-batch algorithms for PCFG inference that can learn from millions of words of training data. We compare the performance of these algorithms to a batch algorithm that learns from less data. The minibatch algorithms outperform the batch algorithm, showing that cheap inference with more data is better than intensive inference with less data. Additionally, we show that the harmonic initialiser, which previous work identified as essential when learning from small POS-tag annotated corpora (Klein and Manning, 2004), is not superior to a uniform initialisation.
Existing sentence regression methods for extractive summarization usually model sentence importance and redundancy in two separate processes. They first evaluate the importance f(s) of each sentence s and then select sentences to generate a summary based on both the importance scores and redundancy among sentences. In this paper, we propose to model importance and redundancy simultaneously by directly evaluating the relative importance f(s|S) of a sentence s given a set of selected sentences S. Specifically, we present a new framework to conduct regression with respect to the relative gain of s given S calculated by the ROUGE metric. Besides the single sentence features, additional features derived from the sentence relations are incorporated. Experiments on the DUC 2001, 2002 and 2004 multi-document summarization datasets show that the proposed method outperforms state-of-the-art extractive summarization approaches.
Automatic video description generation has recently been getting attention after rapid advancement in image caption generation. Automatically generating description for a video is more challenging than for an image due to its temporal dynamics of frames. Most of the work relied on Recurrent Neural Network (RNN) and recently attentional mechanisms have also been applied to make the model learn to focus on some frames of the video while generating each word in a describing sentence. In this paper, we focus on a sequence-to-sequence approach with temporal attention mechanism. We analyze and compare the results from different attention model configuration. By applying the temporal attention mechanism to the system, we can achieve a METEOR score of 0.310 on Microsoft Video Description dataset, which outperformed the state-of-the-art system so far.
Teaching large classes remains a great challenge, primarily because it is difficult to attend to all the student needs in a timely manner. Automatic text summarization systems can be leveraged to summarize the student feedback, submitted immediately after each lecture, but it is left to be discovered what makes a good summary for student responses. In this work we explore a new methodology that effectively extracts summary phrases from the student responses. Each phrase is tagged with the number of students who raise the issue. The phrases are evaluated along two dimensions: with respect to text content, they should be informative and well-formed, measured by the ROUGE metric; additionally, they shall attend to the most pressing student needs, measured by a newly proposed metric. This work is enabled by a phrase-based annotation and highlighting scheme, which is new to the summarization task. The phrase-based framework allows us to summarize the student responses into a set of bullet points and present to the instructor promptly.
We present CATENA, a sieve-based system to perform temporal and causal relation extraction and classification from English texts, exploiting the interaction between the temporal and the causal model. We evaluate the performance of each sieve, showing that the rule-based, the machine-learned and the reasoning components all contribute to achieving state-of-the-art performance on TempEval-3 and TimeBank-Dense data. Although causal relations are much sparser than temporal ones, the architecture and the selected features are mostly suitable to serve both tasks. The effects of the interaction between the temporal and the causal components, although limited, yield promising results and confirm the tight connection between the temporal and the causal dimension of texts.
Because of the increasing popularity of social media, much information has been shared on the internet, enabling social media users to understand various real world events. Particularly, social media-based infectious disease surveillance has attracted increasing attention. In this work, we specifically examine influenza: a common topic of communication on social media. The fundamental theory of this work is that several words, such as symptom words (fever, headache, etc.), appear in advance of flu epidemic occurrence. Consequently, past word occurrence can contribute to estimation of the number of current patients. To employ such forecasting words, one can first estimate the optimal time lag for each word based on their cross correlation. Then one can build a linear model consisting of word frequencies at different time points for nowcasting and for forecasting influenza epidemics. Experimentally obtained results (using 7.7 million tweets of August 2012 {--} January 2016), the proposed model achieved the best nowcasting performance to date (correlation ratio 0.93) and practically sufficient forecasting performance (correlation ratio 0.91 in 1-week future prediction, and correlation ratio 0.77 in 3-weeks future prediction). This report is the first of the relevant literature to describe a model enabling prediction of future epidemics using Twitter.
Semantic Textual Similarity (STS) is a foundational NLP task and can be used in a wide range of tasks. To determine the STS of two texts, hundreds of different STS systems exist, however, for an NLP system designer, it is hard to decide which system is the best one. To answer this question, an intrinsic evaluation of the STS systems is conducted by comparing the output of the system to human judgments on semantic similarity. The comparison is usually done using Pearson correlation. In this work, we show that relying on intrinsic evaluations with Pearson correlation can be misleading. In three common STS based tasks we could observe that the Pearson correlation was especially ill-suited to detect the best STS system for the task and other evaluation measures were much better suited. In this work we define how the validity of an intrinsic evaluation can be assessed and compare different intrinsic evaluation methods. Understanding of the properties of the targeted task is crucial and we propose a framework for conducting the intrinsic evaluation which takes the properties of the targeted task into account.
Princeton WordNet is one of the most important resources for natural language processing, but is only available for English. While it has been translated using the expand approach to many other languages, this is an expensive manual process. Therefore it would be beneficial to have a high-quality automatic translation approach that would support NLP techniques, which rely on WordNet in new languages. The translation of wordnets is fundamentally complex because of the need to translate all senses of a word including low frequency senses, which is very challenging for current machine translation approaches. For this reason we leverage existing translations of WordNet in other languages to identify contextual information for wordnet senses from a large set of generic parallel corpora. We evaluate our approach using 10 translated wordnets for European languages. Our experiment shows a significant improvement over translation without any contextual information. Furthermore, we evaluate how the choice of pivot languages affects performance of multilingual word sense disambiguation.
Interlingua based Machine Translation (MT) aims to encode multiple languages into a common linguistic representation and then decode sentences in multiple target languages from this representation. In this work we explore this idea in the context of neural encoder decoder architectures, albeit on a smaller scale and without MT as the end goal. Specifically, we consider the case of three languages or modalities X, Z and Y wherein we are interested in generating sequences in Y starting from information available in X. However, there is no parallel training data available between X and Y but, training data is available between X {\&} Z and Z {\&} Y (as is often the case in many real world applications). Z thus acts as a pivot/bridge. An obvious solution, which is perhaps less elegant but works very well in practice is to train a two stage model which first converts from X to Z and then from Z to Y. Instead we explore an interlingua inspired solution which jointly learns to do the following (i) encode X and Z to a common representation and (ii) decode Y from this common representation. We evaluate our model on two tasks: (i) bridge transliteration and (ii) bridge captioning. We report promising results in both these applications and believe that this is a right step towards truly interlingua inspired encoder decoder architectures.
This paper studies cross-lingual transfer for dependency parsing, focusing on very low-resource settings where delexicalized transfer is the only fully automatic option. We show how to boost parsing performance by rewriting the source sentences so as to better match the linguistic regularities of the target language. We contrast a data-driven approach with an approach relying on linguistically motivated rules automatically extracted from the World Atlas of Language Structures. Our findings are backed up by experiments involving 40 languages. They show that both approaches greatly outperform the baseline, the knowledge-driven method yielding the best accuracies, with average improvements of +2.9 UAS, and up to +90 UAS (absolute) on some frequent PoS configurations.
Natural-language processing of historical documents is complicated by the abundance of variant spellings and lack of annotated data. A common approach is to normalize the spelling of historical words to modern forms. We explore the suitability of a deep neural network architecture for this task, particularly a deep bi-LSTM network applied on a character level. Our model compares well to previously established normalization algorithms when evaluated on a diverse set of texts from Early New High German. We show that multi-task learning with additional normalization data can improve our model{'}s performance further.
Deceptive opinion spam detection has attracted significant attention from both business and research communities. Existing approaches are based on manual discrete features, which can capture linguistic and psychological cues. However, such features fail to encode the semantic meaning of a document from the discourse perspective, which limits the performance. In this paper, we empirically explore a neural network model to learn document-level representation for detecting deceptive opinion spam. In particular, given a document, the model learns sentence representations with a convolutional neural network, which are combined using a gated recurrent neural network with attention mechanism to model discourse information and yield a document vector. Finally, the document representation is used directly as features to identify deceptive opinion spam. Experimental results on three domains (Hotel, Restaurant, and Doctor) show that our proposed method outperforms state-of-the-art methods.
Gaussian LDA integrates topic modeling with word embeddings by replacing discrete topic distribution over word types with multivariate Gaussian distribution on the embedding space. This can take semantic information of words into account. However, the Euclidean similarity used in Gaussian topics is not an optimal semantic measure for word embeddings. Acknowledgedly, the cosine similarity better describes the semantic relatedness between word embeddings. To employ the cosine measure and capture complex topic structure, we use von Mises-Fisher (vMF) mixture models to represent topics, and then develop a novel mix-vMF topic model (MvTM). Using public pre-trained word embeddings, we evaluate MvTM on three real-world data sets. Experimental results show that our model can discover more coherent topics than the state-of-the-art baseline models, and achieve competitive classification performance.
This paper describes a Bayesian language model for predicting spontaneous utterances. People sometimes say unexpected words, such as fillers or hesitations, that cause the miss-prediction of words in normal N-gram models. Our proposed model considers mixtures of possible segmental contexts, that is, a kind of context-word selection. It can reduce negative effects caused by unexpected words because it represents conditional occurrence probabilities of a word as weighted mixtures of possible segmental contexts. The tuning of mixture weights is the key issue in this approach as the segment patterns becomes numerous, thus we resolve it by using Bayesian model. The generative process is achieved by combining the stick-breaking process and the process used in the variable order Pitman-Yor language model. Experimental evaluations revealed that our model outperformed contiguous N-gram models in terms of perplexity for noisy text including hesitations.
Named entity typing is the task of detecting the types of a named entity in context. For instance, given {``}Eric is giving a presentation{''}, our goal is to infer that {`}Eric{'} is a speaker or a presenter and a person. Existing approaches to named entity typing cannot work with a growing type set and fails to recognize entity mentions of unseen types. In this paper, we present a label embedding method that incorporates prototypical and hierarchical information to learn pre-trained label embeddings. In addition, we adapt a zero-shot learning framework that can predict both seen and previously unseen entity types. We perform evaluation on three benchmark datasets with two settings: 1) few-shots recognition where all types are covered by the training set; and 2) zero-shot recognition where fine-grained types are assumed absent from training set. Results show that prior knowledge encoded using our label embedding methods can significantly boost the performance of classification for both cases.
Languages with rich morphology often introduce sparsity in language processing tasks. While morphological analyzers can reduce this sparsity by providing morpheme-level analyses for words, they will often introduce ambiguity by returning multiple analyses for the same surface form. The problem of disambiguating between these morphological parses is further complicated by the fact that a correct parse for a word is not only be dependent on the surface form but also on other words in its context. In this paper, we present a language-agnostic approach to morphological disambiguation. We address the problem of using context in morphological disambiguation by presenting several LSTM-based neural architectures that encode long-range surface-level and analysis-level contextual dependencies. We applied our approach to Turkish, Russian, and Arabic to compare effectiveness across languages, matching state-of-the-art results in two of the three languages. Our results also demonstrate that while context plays a role in learning how to disambiguate, the type and amount of context needed varies between languages.
Existing asynchronous parallel learning methods are only for the sparse feature models, and they face new challenges for the dense feature models like neural networks (e.g., LSTM, RNN). The problem for dense features is that asynchronous parallel learning brings gradient errors derived from overwrite actions. We show that gradient errors are very common and inevitable. Nevertheless, our theoretical analysis shows that the learning process with gradient errors can still be convergent towards the optimum of objective functions for many practical applications. Thus, we propose a simple method \textit{AsynGrad} for asynchronous parallel learning with gradient error. Base on various dense feature models (LSTM, dense-CRF) and various NLP tasks, experiments show that \textit{AsynGrad} achieves substantial improvement on training speed, and without any loss on accuracy.
In this paper, we empirically explore the effects of various kinds of skip connections in stacked bidirectional LSTMs for sequential tagging. We investigate three kinds of skip connections connecting to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states and (c) skip connections to the cell outputs. We present comprehensive experiments showing that skip connections to cell outputs outperform the remaining two. Furthermore, we observe that using gated identity functions as skip mappings works pretty well. Based on this novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-of-the-art results on CCG supertagging and comparable results on POS tagging.
Summarization aims to represent source documents by a shortened passage. Existing methods focus on the extraction of key information, but often neglect coherence. Hence the generated summaries suffer from a lack of readability. To address this problem, we have developed a graph-based method by exploring the links between text to produce coherent summaries. Our approach involves finding a sequence of sentences that best represent the key information in a coherent way. In contrast to the previous methods that focus only on salience, the proposed method addresses both coherence and informativeness based on textual linkages. We conduct experiments on the DUC2004 summarization task data set. A performance comparison reveals that the summaries generated by the proposed system achieve comparable results in terms of the ROUGE metric, and show improvements in readability by human evaluation.
A key component in surface realization in natural language generation is to choose concrete syntactic relationships to express a target meaning. We develop a new method for syntactic choice based on learning a stochastic tree grammar in a neural architecture. This framework can exploit state-of-the-art methods for modeling word sequences and generalizing across vocabulary. We also induce embeddings to generalize over elementary tree structures and exploit a tree recurrence over the input structure to model long-distance influences between NLG choices. We evaluate the models on the task of linearizing unannotated dependency trees, documenting the contribution of our modeling techniques to improvements in both accuracy and run time.
This paper studies the abstractive multi-document summarization for event-oriented news texts through event information extraction and abstract representation. Fine-grained event mentions and semantic relations between them are extracted to build a unified and connected event semantic link network, an abstract representation of source texts. A network reduction algorithm is proposed to summarize the most salient and coherent event information. New sentences with good linguistic quality are automatically generated and selected through sentences over-generation and greedy-selection processes. Experimental results on DUC 2006 and DUC 2007 datasets show that our system significantly outperforms the state-of-the-art extractive and abstractive baselines under both pyramid and ROUGE evaluation metrics.
Extracting summaries via integer linear programming and submodularity are popular and successful techniques in extractive multi-document summarization. However, many interesting optimization objectives are neither submodular nor factorizable into an integer linear program. We address this issue and present a general optimization framework where any function of input documents and a system summary can be plugged in. Our framework includes two kinds of summarizers {--} one based on genetic algorithms, the other using a swarm intelligence approach. In our experimental evaluation, we investigate the optimization of two information-theoretic summary evaluation metrics and find that our framework yields competitive results compared to several strong summarization baselines. Our comparative analysis of the genetic and swarm summarizers reveals interesting complementary properties.
This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER).
Predictive incremental parsing produces syntactic representations of sentences as they are produced, e.g. by typing or speaking. In order to generate connected parses for such unfinished sentences, upcoming word types can be hypothesized and structurally integrated with already realized words. For example, the presence of a determiner as the last word of a sentence prefix may indicate that a noun will appear somewhere in the completion of that sentence, and the determiner can be attached to the predicted noun. We combine the forward-looking parser predictions with backward-looking N-gram histories and analyze in a set of experiments the impact on language models, i.e. stronger discriminative power but also higher data sparsity. Conditioning N-gram models, MaxEnt models or RNN-LMs on parser predictions yields perplexity reductions of about 6{\%}. Our method (a) retains online decoding capabilities and (b) incurs relatively little computational overhead which sets it apart from previous approaches that use syntax for language modeling. Our method is particularly attractive for modular systems that make use of a syntax parser anyway, e.g. as part of an understanding pipeline where predictive parsing improves language modeling at no additional cost.
In this paper, we study the problem of disfluency detection using the encoder-decoder framework. We treat disfluency detection as a sequence-to-sequence problem and propose a neural attention-based model which can efficiently model the long-range dependencies between words and make the resulting sentence more likely to be grammatically correct. Our model firstly encode the source sentence with a bidirectional Long Short-Term Memory (BI-LSTM) and then use the neural attention as a pointer to select an ordered sub sequence of the input as the output. Experiments show that our model achieves the state-of-the-art f-score of 86.7{\%} on the commonly used English Switchboard test set. We also evaluate the performance of our model on the in-house annotated Chinese data and achieve a significantly higher f-score compared to the baseline of CRF-based approach.
The paper applies a deep recurrent neural network to the task of sentence boundary detection in Sanskrit, an important, yet underresourced ancient Indian language. The deep learning approach improves the F scores set by a metrical baseline and by a Conditional Random Field classifier by more than 10{\%}.
In this paper, we propose a new annotation approach to Chinese word segmentation, part-of-speech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments.
Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.
Previous studies on Thai Sentence Boundary Detection (SBD) mostly assumed sentence ends at a space disambiguation problem, which classified space either as an indicator for Sentence Boundary (SB) or non-Sentence Boundary (nSB). In this paper, we propose a word labeling approach which treats space as a normal word, and detects SB between any two words. This removes the restriction for SB to be oc-curred only at space and makes our system more robust for modern Thai writing. It is because in modern Thai writing, space is not consistently used to indicate SB. As syntactic information contributes to better SBD, we further propose a joint Part-Of-Speech (POS) tagging and SBD framework based on Factorial Conditional Random Field (FCRF) model. We compare the performance of our proposed ap-proach with reported methods on ORCHID corpus. We also performed experiments of FCRF model on the TaLAPi corpus. The results show that the word labelling approach has better performance than pre-vious space-based classification approaches and FCRF joint model outperforms LCRF model in terms of SBD in all experiments.
We propose a new approach to PoS tagging where in a first step, we assign a coarse-grained tag corresponding to the main syntactic category. Based on this high-precision decision, in the second step we utilize specially trained fine-grained models with heavily reduced decision complexity. By analyzing the system under oracle conditions, we show that there is a quite large potential for significantly outperforming a competitive baseline. When we take error-propagation from the coarse-grained tagging into account, our approach is on par with the state of the art. Our approach also allows tailoring the tagger towards recognizing single word classes which are of interest e.g. for researchers searching for specific phenomena in large corpora. In a case study, we significantly outperform a standard model that also makes use of the same optimizations.
Parsing texts into universal dependencies (UD) in realistic scenarios requires infrastructure for the morphological analysis and disambiguation (MA{\&}D) of typologically different languages as a first tier. MA{\&}D is particularly challenging in morphologically rich languages (MRLs), where the ambiguous space-delimited tokens ought to be disambiguated with respect to their constituent morphemes, each morpheme carrying its own tag and a rich set features. Here we present a novel, language-agnostic, framework for MA{\&}D, based on a transition system with two variants {---} word-based and morpheme-based {---} and a dedicated transition to mitigate the biases of variable-length morpheme sequences. Our experiments on a Modern Hebrew case study show state of the art results, and we show that the morpheme-based MD consistently outperforms our word-based variant. We further illustrate the utility and multilingual coverage of our framework by morphologically analyzing and disambiguating the large set of languages in the UD treebanks.
Development of hand crafted rule for syllabifying words of a language is an expensive task. This paper proposes several data-driven methods for automatic syllabification of words written in Manipuri language. Manipuri is one of the scheduled Indian languages. First, we propose a language-independent rule-based approach formulated using entropy based phonotactic segmentation. Second, we project the syllabification problem as a sequence labeling problem and investigate its effect using various sequence labeling approaches. Third, we combine the effect of sequence labeling and rule-based method and investigate the performance of the hybrid approach. From various experimental observations, it is evident that the proposed methods outperform the baseline rule-based method. The entropy based phonotactic segmentation provides a word accuracy of 96{\%}, CRF (sequence labeling approach) provides 97{\%} and hybrid approach provides 98{\%} word accuracy.
In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some tasks, such as sentiment classification and document summarization. Nevertheless, as far as we are aware, there is only a dearth of research focusing on launching unsupervised paragraph embedding methods. Classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions are twofold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph. We evaluate the proposed EV model on benchmark sentiment classification and multi-document summarization tasks. The experimental results demonstrate the effectiveness and applicability of the proposed embedding method. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector (D-EV) model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition. The utility of the D-EV model is evaluated on a spoken document summarization task, confirming the effectiveness of the proposed embedding method in relation to several well-practiced and state-of-the-art summarization methods.
This paper introduces a continuous system capable of automatically producing the most adequate speaking style to synthesize a desired target text. This is done thanks to a joint modeling of the acoustic and lexical parameters of the speaker models by adapting the CVSM projection of the training texts using MR-HMM techniques. As such, we consider that as long as sufficient variety in the training data is available, we should be able to model a continuous lexical space into a continuous acoustic space. The proposed continuous automatic text to speech system was evaluated by means of a perceptual evaluation in order to compare them with traditional approaches to the task. The system proved to be capable of conveying the correct expressiveness (average adequacy of 3.6) with an expressive strength comparable to oracle traditional expressive speech synthesis (average of 3.6) although with a drop in speech quality mainly due to the semi-continuous nature of the data (average quality of 2.9). This means that the proposed system is capable of improving traditional neutral systems without requiring any additional user interaction.
Speech prosody is known to be central in advanced communication technologies. However, despite the advances of theoretical studies in speech prosody, so far, no large scale prosody annotated resources that would facilitate empirical research and the development of empirical computational approaches are available. This is to a large extent due to the fact that current common prosody annotation conventions offer a descriptive framework of intonation contours and phrasing based on labels. This makes it difficult to reach a satisfactory inter-annotator agreement during the annotation of gold standard annotations and, subsequently, to create consistent large scale annotations. To address this problem, we present an annotation schema for prominence and boundary labeling of prosodic phrases based upon acoustic parameters and a tagger for prosody annotation at the prosodic phrase level. Evaluation proves that inter-annotator agreement reaches satisfactory values, from 0.60 to 0.80 Cohen{'}s kappa, while the prosody tagger achieves acceptable recall and f-measure figures for five spontaneous samples used in the evaluation of monologue and dialogue formats in English and Spanish. The work presented in this paper is a first step towards a semi-automatic acquisition of large corpora for empirical prosodic analysis.
Popular techniques for domain adaptation such as the feature augmentation method of Daum{\'e} III (2009) have mostly been considered for sparse binary-valued features, but not for dense real-valued features such as those used in neural networks. In this paper, we describe simple neural extensions of these techniques. First, we propose a natural generalization of the feature augmentation method that uses K + 1 LSTMs where one model captures global patterns across all K domains and the remaining K models capture domain-specific information. Second, we propose a novel application of the framework for learning shared structures by Ando and Zhang (2005) to domain adaptation, and also provide a neural extension of their approach. In experiments on slot tagging over 17 domains, our methods give clear performance improvement over Daum{\'e} III (2009) applied on feature-rich CRFs.
In Computational Linguistics, Hindi and Urdu are not viewed as a monolithic entity and have received separate attention with respect to their text processing. From part-of-speech tagging to machine translation, models are separately trained for both Hindi and Urdu despite the fact that they represent the same language. The reasons mainly are their divergent literary vocabularies and separate orthographies, and probably also their political status and the social perception that they are two separate languages. In this article, we propose a simple but efficient approach to bridge the lexical and orthographic differences between Hindi and Urdu texts. With respect to text processing, addressing the differences between the Hindi and Urdu texts would be beneficial in the following ways: (a) instead of training separate models, their individual resources can be augmented to train single, unified models for better generalization, and (b) their individual text processing applications can be used interchangeably under varied resource conditions. To remove the script barrier, we learn accurate statistical transliteration models which use sentence-level decoding to resolve word ambiguity. Similarly, we learn cross-register word embeddings from the harmonized Hindi and Urdu corpora to nullify their lexical divergences. As a proof of the concept, we evaluate our approach on the Hindi and Urdu dependency parsing under two scenarios: (a) resource sharing, and (b) resource augmentation. We demonstrate that a neural network-based dependency parser trained on augmented, harmonized Hindi and Urdu resources performs significantly better than the parsing models trained separately on the individual resources. We also show that we can achieve near state-of-the-art results when the parsers are used interchangeably.
Syntax plays an important role in the task of predicting the semantic structure of a sentence. But syntactic phenomena such as alternations, control and raising tend to obfuscate the relation between syntax and semantics. In this paper we predict the semantic structure of a sentence using a deeper syntax than what is usually done. This deep syntactic representation abstracts away from purely syntactic phenomena and proposes a structural organization of the sentence that is closer to the semantic representation. Experiments conducted on a French corpus annotated with semantic frames showed that a semantic parser reaches better performances with such a deep syntactic input.
We present a dependency to constituent tree conversion technique that aims to improve constituent parsing accuracies by leveraging dependency treebanks available in a wide variety in many languages. The technique works in two steps. First, a partial constituent tree is derived from a dependency tree with a very simple deterministic algorithm that is both language and dependency type independent. Second, a complete high accuracy constituent tree is derived with a constraint-based parser, which uses the partial constituent tree as external constraints. Evaluated on Section 22 of the WSJ Treebank, the technique achieves the state-of-the-art conversion F-score 95.6. When applied to English Universal Dependency treebank and German CoNLL2006 treebank, the converted treebanks added to the human-annotated constituent parser training corpus improve parsing F-scores significantly for both languages.
Multiword expressions (MWEs) are pervasive in natural languages and often have both idiomatic and compositional readings, which leads to high syntactic ambiguity. We show that for some MWE types idiomatic readings are usually the correct ones. We propose a heuristic for an A* parser for Tree Adjoining Grammars which benefits from this knowledge by promoting MWE-oriented analyses. This strategy leads to a substantial reduction in the parsing search space in case of true positive MWE occurrences, while avoiding parsing failures in case of false positives.
We present experiments in incrementally learning a dependency parser. The parser will be used in the WordsEye Linguistics Tools (WELT) (Ulinski et al., 2014) which supports field linguists documenting a language{'}s syntax and semantics. Our goal is to make syntactic annotation faster for field linguists. We have created a new parallel corpus of descriptions of spatial relations and motion events, based on pictures and video clips used by field linguists for elicitation of language from native speaker informants. We collected descriptions for each picture and video from native speakers in English, Spanish, German, and Egyptian Arabic. We compare the performance of MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006) when trained on small amounts of this data. We find that MaltParser achieves the best performance. We also present the results of experiments using the parser to assist with annotation. We find that even when the parser is trained on a single sentence from the corpus, annotation time significantly decreases.
This work focuses on the development of linguistic analysis tools for resource-poor languages. We use a parallel corpus to produce a multilingual word representation based only on sentence level alignment. This representation is combined with the annotated source side (resource-rich language) of the parallel corpus to train text analysis tools for resource-poor languages. Our approach is based on Recurrent Neural Networks (RNN) and has the following advantages: (a) it does not use word alignment information, (b) it does not assume any knowledge about foreign languages, which makes it applicable to a wide range of resource-poor languages, (c) it provides truly multilingual taggers. In a previous study, we proposed a method based on Simple RNN to automatically induce a Part-Of-Speech (POS) tagger. In this paper, we propose an improvement of our neural model. We investigate the Bidirectional RNN and the inclusion of external information (for instance low level information from Part-Of-Speech tags) in the RNN to train a more complex tagger (for instance, a multilingual super sense tagger). We demonstrate the validity and genericity of our method by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers.
Annotation projection is a practical method to deal with the low resource problem in incident languages (IL) processing. Previous methods on annotation projection mainly relied on word alignment results without any training process, which led to noise propagation caused by word alignment errors. In this paper, we focus on the named entity recognition (NER) task and propose a weakly-supervised framework to project entity annotations from English to IL through bitexts. Instead of directly relying on word alignment results, this framework combines advantages of rule-based methods and deep learning methods by implementing two steps: First, generates a high-confidence entity annotation set on IL side with strict searching methods; Second, uses this high-confidence set to weakly supervise the model training. The model is finally used to accomplish the projecting process. Experimental results on two low-resource ILs show that the proposed method can generate better annotations projected from English-IL parallel corpora. The performance of IL name tagger can also be improved significantly by training on the newly projected IL annotation set.
Much previous research on multiword expressions (MWEs) has focused on the token- and type-level tasks of MWE identification and extraction, respectively. Such studies typically target known prevalent MWE types in a given language. This paper describes the first attempt to learn the MWE inventory of a {``}surprise{''} language for which we have no explicit prior knowledge of MWE patterns, certainly no annotated MWE data, and not even a parallel corpus. Our proposed model is trained on a treebank with MWE relations of a source language, and can be applied to the monolingual corpus of the surprise language to identify its MWE construction types.
In this paper, we propose a novel hybrid deep learning archtecture which is highly efficient for sentiment analysis in resource-poor languages. We learn sentiment embedded vectors from the Convolutional Neural Network (CNN). These are augmented to a set of optimized features selected through a multi-objective optimization (MOO) framework. The sentiment augmented optimized vector obtained at the end is used for the training of SVM for sentiment classification. We evaluate our proposed approach for coarse-grained (i.e. sentence level) as well as fine-grained (i.e. aspect level) sentiment analysis on four Hindi datasets covering varying domains. In order to show that our proposed method is generic in nature we also evaluate it on two benchmark English datasets. Evaluation shows that the results of the proposed method are consistent across all the datasets and often outperforms the state-of-art systems. To the best of our knowledge, this is the very first attempt where such a deep learning model is used for less-resourced languages such as Hindi.
In Sanskrit, the phonemes at the word boundaries undergo changes to form new phonemes through a process called as sandhi. A fused sentence can be segmented into multiple possible segmentations. We propose a word segmentation approach that predicts the most semantically valid segmentation for a given sentence. We treat the problem as a query expansion problem and use the path-constrained random walks framework to predict the correct segments.
In this paper, we first build a manually annotated named entity corpus of Mongolian. Then, we propose three morphological processing methods and study comprehensive features, including syllable features, lexical features, context features, morphological features and semantic features in Mongolian named entity recognition. Moreover, we also evaluate the influence of word cluster features on the system and combine all features together eventually. The experimental result shows that segmenting each suffix into an individual token achieves better results than deleting suffixes or using the suffixes as feature. The system based on segmenting suffixes with all proposed features yields benchmark result of F-measure=84.65 on this corpus.
When making clinical decisions, practitioners need to rely on the most relevant evidence available. However, accessing a vast body of medical evidence and confronting with the issue of information overload can be challenging and time consuming. This paper proposes an effective summarizer for medical evidence by utilizing both UMLS and WordNet. Given a clinical query and a set of relevant abstracts, our aim is to generate a fluent, well-organized, and compact summary that answers the query. Analysis via ROUGE metrics shows that using WordNet as a general-purpose lexicon helps to capture the concepts not covered by the UMLS Metathesaurus, and hence significantly increases the performance. The effectiveness of our proposed approach is demonstrated by conducting a set of experiments over a specialized evidence-based medicine (EBM) corpus - which has been gathered and annotated for the purpose of biomedical text summarization.
In order to organize the large number of products listed in e-commerce sites, each product is usually assigned to one of the multi-level categories in the taxonomy tree. It is a time-consuming and difficult task for merchants to select proper categories within thousands of options for the products they sell. In this work, we propose an automatic classification tool to predict the matching category for a given product title and description. We used a combination of two different neural models, i.e., deep belief nets and deep autoencoders, for both titles and descriptions. We implemented a selective reconstruction approach for the input layer during the training of the deep neural networks, in order to scale-out for large-sized sparse feature vectors. GPUs are utilized in order to train neural networks in a reasonable time. We have trained our models for around 150 million products with a taxonomy tree with at most 5 levels that contains 28,338 leaf categories. Tests with millions of products show that our first predictions matches 81{\%} of merchants{'} assignments, when {``}others{''} categories are excluded.
Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction.To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilising (with respect to the taxonomy tree) path-wise, node-wise and depth-wise classifiers to reduce error in the final product classification task. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve improved results on various evaluation metrics compared to earlier approaches.
Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. We also observe that the sentences recognized to focus on the query indeed meet the query need.
A news article summary usually consists of 2-3 key sentences that reflect the gist of that news article. In this paper we explore using public posts following a new article to improve automatic summary generation for the news article. We propose different approaches to incorporate information from public posts, including using frequency information from the posts to re-estimate bigram weights in the ILP-based summarization model and to re-weight a dependency tree edge{'}s importance for sentence compression, directly selecting sentences from posts as the final summary, and finally a strategy to combine the summarization results generated from news articles and posts. Our experiments on data collected from Facebook show that relevant public posts provide useful information and can be effectively leveraged to improve news article summarization results.
Abstractive summarisation is not yet common amongst today{'}s deployed and research systems. Most existing systems either extract sentences or compress individual sentences. In this paper, we present a summariser that works by a different paradigm. It is a further development of an existing summariser that has an incremental, proposition-based content selection process but lacks a natural language (NL) generator for the final output. Using an NL generator, we can now produce the summary text to directly reflect the selected propositions. Our evaluation compares textual quality of our system to the earlier preliminary output method, and also uses ROUGE to compare to various summarisers that use the traditional method of sentence extraction, followed by compression. Our results suggest that cutting out the middle-man of sentence extraction can lead to better abstractive summaries.
We propose a method for learning semantic CCG parsers by projecting annotations via a parallel corpus. The method opens an avenue towards cheaply creating multilingual semantic parsers mapping open-domain text to formal meaning representations. A first cross-lingually learned Dutch (from English) semantic parser obtains f-scores ranging from 42.99{\%} to 69.22{\%} depending on the level of label informativity taken into account, compared to 58.40{\%} to 78.88{\%} for the underlying source-language system. These are promising numbers compared to state-of-the-art semantic parsing in open domains.
We propose a dependency parsing pipeline, in which the parsing of long-distance projections and localized dependencies are explicitly decomposed at the input level. A chosen baseline dependency parsing model performs only on {`}carved{'} sequences at the second stage, which are transformed from coarse constituent parsing outputs at the first stage. When k-best constituent parsing outputs are kept, a third-stage is required to search for an optimal combination of the overlapped dependency subtrees. In this sense, our dependency model is subtree-factored. We explore alternative approaches for scoring subtrees, including feature-based models as well as continuous representations. The search for optimal subset to combine is formulated as an ILP problem. This framework especially benefits the models poor on long sentences, generally improving baselines by 0.75-1.28 (UAS) on English, achieving comparable performance with high-order models but faster. For Chinese, the most notable increase is as high as 3.63 (UAS) when the proposed framework is applied to first-order parsing models.
Semantic role labeling (SRL) is the task of identifying and labeling predicate-argument structures in sentences with semantic frame and role labels. A known challenge in SRL is the large number of low-frequency exceptions in training data, which are highly context-specific and difficult to generalize. To overcome this challenge, we propose the use of instance-based learning that performs no explicit generalization, but rather extrapolates predictions from the most similar instances in the training data. We present a variant of k-nearest neighbors (kNN) classification with composite features to identify nearest neighbors for SRL. We show that high-quality predictions can be derived from a very small number of similar instances. In a comparative evaluation we experimentally demonstrate that our instance-based learning approach significantly outperforms current state-of-the-art systems on both in-domain and out-of-domain data, reaching F1-scores of 89,28{\%} and 79.91{\%} respectively.
Keystroke dynamics have been extensively used in psycholinguistic and writing research to gain insights into cognitive processing. But do keystroke logs contain actual signal that can be used to learn better natural language processing models? We postulate that keystroke dynamics contain information about syntactic structure that can inform shallow syntactic parsing. To test this hypothesis, we explore labels derived from keystroke logs as auxiliary task in a multi-task bidirectional Long Short-Term Memory (bi-LSTM). Our results show promising results on two shallow syntactic parsing tasks, chunking and CCG supertagging. Our model is simple, has the advantage that data can come from distinct sources, and produces models that are significantly better than models trained on the text annotations alone.
Current methods for word alignment require considerable amounts of parallel text to deliver accurate results, a requirement which is met only for a small minority of the world{'}s approximately 7,000 languages. We show that by jointly performing word alignment and annotation transfer in a novel Bayesian model, alignment accuracy can be improved for language pairs where annotations are available for only one of the languages{---}a finding which could facilitate the study and processing of a vast number of low-resource languages. We also present an evaluation where our method is used to perform single-source and multi-source part-of-speech transfer with 22 translations of the same text in four different languages. This allows us to quantify the considerable variation in accuracy depending on the specific source text(s) used, even with different translations into the same language.
Compound words with unmarked word boundaries are problematic for many tasks in NLP and computational linguistics, including information extraction, machine translation, and syllabification. This paper introduces a simple, proof-of-concept language modeling approach to automatic compound segmentation, as applied to Finnish. This approach utilizes an off-the-shelf morphological analyzer to split training words into their constituent morphemes. A language model is subsequently trained on ngrams composed of morphemes, morpheme boundaries, and word boundaries. Linguistic constraints are then used to weed out phonotactically ill-formed segmentations, thereby allowing the language model to select the best grammatical segmentation. This approach achieves an accuracy of {\textasciitilde}97{\%}.
Knowledge embedding, which projects triples in a given knowledge base to d-dimensional vectors, has attracted considerable research efforts recently. Most existing approaches treat the given knowledge base as a set of triplets, each of whose representation is then learned separately. However, as a fact, triples are connected and depend on each other. In this paper, we propose a graph aware knowledge embedding method (GAKE), which formulates knowledge base as a directed graph, and learns representations for any vertices or edges by leveraging the graph{'}s structural information. We introduce three types of graph context for embedding: neighbor context, path context, and edge context, each reflects properties of knowledge from different perspectives. We also design an attention mechanism to learn representative power of different vertices or edges. To validate our method, we conduct several experiments on two tasks. Experimental results suggest that our method outperforms several state-of-art knowledge embedding models.
For automatic chatting systems, it is indeed a great challenge to reply the given query considering the conversation history, rather than based on the query only. This paper proposes a deep neural network to address the context-aware response ranking problem by end-to-end learning, so as to help to select conversationally relevant candidate. By combining the multi-column convolutional layer and the recurrent layer, our model is able to model the semantics of the utterance sequence by grasping the semantic clue within the conversation, on the basis of the effective representation for each sentence. Especially, the network utilizes attention pooling to further emphasis the importance of essential words in conversations, thus the representations of contexts tend to be more meaningful and the performance of candidate ranking is notably improved. Meanwhile, due to the adoption of attention pooling, it is possible to visualize the semantic clues. The experimental results on the large amount of conversation data from social media have shown that our approach is promising for quantifying the conversational relevance of responses, and indicated its good potential for building practical IR based chat-bots.
Besides providing the relevant information, amusing users has been an important role of the web. Many web sites provide serendipitous (unexpected but relevant) information to draw user traffic. In this paper, we study the representative scenario of mining an amusing quiz. An existing approach leverages a knowledge base to mine an unexpected property then find quiz questions on such property, based on prototype theory in cognitive science. However, existing deterministic model is vulnerable to noise in the knowledge base. Therefore, we instead propose to leverage probabilistic approach to build a prototype that can overcome noise. Our extensive empirical study shows that our approach not only significantly outperforms baselines by 0.06 in accuracy, and 0.11 in serendipity but also shows higher relevance than the traditional relevance-pursuing baseline using TF-IDF.
Personal writings have inspired researchers in the fields of linguistics and psychology to study the relationship between language and culture to better understand the psychology of people across different cultures. In this paper, we explore this relation by developing cross-cultural word models to identify words with cultural bias {--} i.e., words that are used in significantly different ways by speakers from different cultures. Focusing specifically on two cultures: United States and Australia, we identify a set of words with significant usage differences, and further investigate these words through feature analysis and topic modeling, shedding light on the attributes of language that contribute to these differences.
The Dundee Eyetracking Corpus contains eyetracking data collected while native speakers of English and French read newspaper editorial articles. Similar resources for other languages are still rare, especially for languages in which words are not overtly delimited with spaces. This is a report on a project to build an eyetracking corpus for Japanese. Measurements were collected while 24 native speakers of Japanese read excerpts from the Balanced Corpus of Contemporary Written Japanese Texts were presented with or without segmentation (i.e. with or without space at the boundaries between bunsetsu segmentations) and with two types of methodologies (eyetracking and self-paced reading presentation). Readers{'} background information including vocabulary-size estimation and Japanese reading-span score were also collected. As an example of the possible uses for the corpus, we also report analyses investigating the phenomena of anti-locality.
Cyberbullying statistics are shocking, the number of affected young people is increasing dramatically with the affordability of mobile technology devices combined with a growing number of social networks. This paper proposes a framework to analyse Tweets with the goal to identify cyberharassment in social networks as an important step to protect people from cyberbullying. The proposed framework incorporates latent or hidden variables with supervised learning to determine potential bullying cases resembling short blogging type texts such as Tweets. It uses the LIWC2007 - tool that translates Tweet messages into 67 numeric values, representing 67 word categories. The output vectors are then used as features for four different classifiers implemented in Weka. Tests on all four classifiers delivered encouraging predictive capability of Tweet messages. Overall it was found that the use of numeric psychometric values outperformed the same algorithms using both filtered and unfiltered words as features. The best performing algorithms was Random Forest with an F1-value of 0.947 using psychometric features compared to a value of 0.847 for the same algorithm using words as features.
Learning syntactic categories is a fundamental task in language acquisition. Previous studies show that co-occurrence patterns of preceding and following words are essential to group words into categories. However, the neighboring words, or frames, are rarely repeated exactly in the data. This creates data sparsity and hampers learning for frame based models. In this work, we propose a paradigmatic representation of word context which uses probable substitutes instead of frames. Our experiments on child-directed speech show that models based on probable substitutes learn more accurate categories with fewer examples compared to models based on frames.
We report three user studies in which the Lexical Simplification needs of non-native English speakers are investigated. Our analyses feature valuable new insight on the relationship between the non-natives{'} notion of complexity and various morphological, semantic and lexical word properties. Some of our findings contradict long-standing misconceptions about word simplicity. The data produced in our studies consists of 211,564 annotations made by 1,100 volunteers, which we hope will guide forthcoming research on Text Simplification for non-native speakers of English.
In this paper, we aim to investigate the coordination of interlocutors behavior in different emotional segments. Conversational coordination between the interlocutors is the tendency of speakers to predict and adjust each other accordingly on an ongoing conversation. In order to find such a coordination, we investigated 1) lexical similarities between the speakers in each emotional segments, 2) correlation between the interlocutors using psycholinguistic features, such as linguistic styles, psychological process, personal concerns among others, and 3) relation of interlocutors turn-taking behaviors such as competitiveness. To study the degree of coordination in different emotional segments, we conducted our experiments using real dyadic conversations collected from call centers in which agent{'}s emotional state include empathy and customer{'}s emotional states include anger and frustration. Our findings suggest that the most coordination occurs between the interlocutors inside anger segments, where as, a little coordination was observed when the agent was empathic, even though an increase in the amount of non-competitive overlaps was observed. We found no significant difference between anger and frustration segment in terms of turn-taking behaviors. However, the length of pause significantly decreases in the preceding segment of anger where as it increases in the preceding segment of frustration.
We propose a hierarchical clustering approach designed to group linguistic features for supervised machine learning that is inspired by variationist linguistics. The method makes it possible to abstract away from the individual feature occurrences by grouping features together that behave alike with respect to the target class, thus providing a new, more general perspective on the data. On the one hand, it reduces data sparsity, leading to quantitative performance gains. On the other, it supports the formation and evaluation of hypotheses about individual choices of linguistic structures. We explore the method using features based on verb subcategorization information and evaluate the approach in the context of the Native Language Identification (NLI) task.
Previous linguistic research on scientific writing has shown that language use in the scientific domain varies considerably in register and style over time. In this paper we investigate the introduction of information theory inspired features to study long term diachronic change on three levels: lexis, part-of-speech and syntax. Our approach is based on distinguishing between sentences from 19th and 20th century scientific abstracts using supervised classification models. To the best of our knowledge, the introduction of information theoretic features to this task is novel. We show that these features outperform more traditional features, such as token or character n-grams, while leading to more compact models. We present a detailed analysis of feature informativeness in order to gain a better understanding of diachronic change on different linguistic levels.
Recent work for learning word representations has applied successfully to many NLP applications, such as sentiment analysis and question answering. However, most of these models assume a single vector per word type without considering polysemy and homonymy. In this paper, we present an extension to the CBOW model which not only improves the quality of embeddings but also makes embeddings suitable for polysemy. It differs from most of the related work in that it learns one semantic center embedding and one context bias instead of training multiple embeddings per word type. Different context leads to different bias which is defined as the weighted average embeddings of local context. Experimental results on similarity task and analogy task show that the word representations learned by the proposed method outperform the competitive baselines.
In this work we tackle the challenge of identifying rhythmic patterns in poetry written in English. Although poetry is a literary form that makes use standard meters usually repeated among different authors, we will see in this paper how performing such analyses is a difficult task in machine learning due to the unexpected deviations from such standard patterns. After breaking down some examples of classical poetry, we apply a number of NLP techniques for the scansion of poetry, training and testing our systems against a human-annotated corpus. With these experiments, our purpose is establish a baseline of automatic scansion of poetry using NLP tools in a straightforward manner and to raise awareness of the difficulties of this task.
In order to apply computational linguistic analyses and pass information to downstream applications, transcriptions of speech obtained via automatic speech recognition (ASR) need to be divided into smaller meaningful units, in a task we refer to as {`}speech-unit (SU) delimitation{'}. We closely recreate the automatic delimitation system described by Lee and Glass (2012), {`}Sentence detection using multiple annotations{'}, Proceedings of INTERSPEECH, which combines a prosodic model, language model and speech-unit length model in log-linear fashion. Since state-of-the-art natural language processing (NLP) tools have been developed to deal with written text and its characteristic sentence-like units, SU delimitation helps bridge the gap between ASR and NLP, by normalising spoken data into a more canonical format. Previous work has focused on native speaker recordings; we test the system of Lee and Glass (2012) on non-native speaker (or {`}learner{'}) data, achieving performance above the state-of-the-art. We also consider alternative evaluation metrics which move away from the idea of a single {`}truth{'} in SU delimitation, and frame this work in the context of downstream NLP applications.
Parallelism is an important rhetorical device. We propose a machine learning approach for automated sentence parallelism identification in student essays. We build an essay dataset with sentence level parallelism annotated. We derive features by combining generalized word alignment strategies and the alignment measures between word sequences. The experimental results show that sentence parallelism can be effectively identified with a F1 score of 82{\%} at pair-wise level and 72{\%} at parallelism chunk level.Based on this approach, we automatically identify sentence parallelism in more than 2000 student essays and study the correlation between the use of sentence parallelism and the types and quality of essays.
In this paper we analyze the effectiveness of using linguistic knowledge from coreference and anaphora resolution for improving the performance for supervised keyphrase extraction. In order to verify the impact of these features, we define a baseline keyphrase extraction system and evaluate its performance on a standard dataset using different machine learning algorithms. Then, we consider new sets of features by adding combinations of the linguistic features we propose and we evaluate the new performance of the system. We also use anaphora and coreference resolution to transform the documents, trying to simulate the cohesion process performed by the human mind. We found that our approach has a slightly positive impact on the performance of automatic keyphrase extraction, in particular when considering the ranking of the results.
Finding authentic examples of grammatical constructions is central in constructionist approaches to linguistics, language processing, and second language learning. In this paper, we address this problem as an information retrieval (IR) task. To facilitate research in this area, we built a benchmark collection by annotating the occurrences of six constructions in a Swedish corpus. Furthermore, we implemented a simple and flexible retrieval system for finding construction occurrences, in which the user specifies a ranking function using lexical-semantic similarities (lexicon-based or distributional). The system was evaluated using standard IR metrics on the new benchmark, and we saw that lexical-semantical rerankers improve significantly over a purely surface-oriented system, but must be carefully tailored for each individual construction.
We propose a new method of automatically extracting learner errors from parallel English as a Second Language (ESL) sentences in an effort to regularise annotation formats and reduce inconsistencies. Specifically, given an original and corrected sentence, our method first uses a linguistically enhanced alignment algorithm to determine the most likely mappings between tokens, and secondly employs a rule-based function to decide which alignments should be merged. Our method beats all previous approaches on the tested datasets, achieving state-of-the-art results for automatic error extraction.
Linguistic typology provides features that have a potential of uncovering deep phylogenetic relations among the world{'}s languages. One of the key challenges in using typological features for phylogenetic inference is that horizontal (spatial) transmission obscures vertical (phylogenetic) signals. In this paper, we characterize typological features with respect to the relative strength of vertical and horizontal transmission. To do this, we first construct (1) a spatial neighbor graph of languages and (2) a phylogenetic neighbor graph by collapsing known language families. We then develop an autologistic model that predicts a feature{'}s distribution from these two graphs. In the experiments, we managed to separate vertically and/or horizontally stable features from unstable ones, and the results are largely consistent with previous findings.
The modifications that foreign loanwords undergo when adapted into Japanese have been the subject of much study in linguistics. The scholarly interest of the topic can be attributed to the fact that Japanese loanwords undergo a complex series of phonological adaptations, something which has been puzzling scholars for decades. While previous studies of Japanese loanword accommodation have focused on specific phonological phenomena of limited scope, the current study leverages computational methods to provide a more complete description of all the sound changes that occur when adopting English words into Japanese. To investigate this, we have developed a parallel corpus of 250 English transcriptions and their respective Japanese equivalents. These words were then used to develop a wide-coverage finite state transducer based phonological grammar that mimics the behavior of the Japanese adaption process. By developing rules with the goal of accounting completely for a large number of borrowing and analyzing forms mistakenly generated by the system, we discovered an internal inconsistency inside the loanword phonology of the Japanese language, something arguably underestimated by previous studies. The result of the investigation suggests that there are multiple {`}dimensions{'} that shape the output form of the current Japanese loanwords. These dimensions include orthography, phonetics, and historical changes.
We present a linguistic analysis of a set of English and Spanish verb+noun combinations (VNCs), and a method to use this information to improve VNC identification. Firstly, a sample of frequent VNCs are analysed in-depth and tagged along lexico-semantic and morphosyntactic dimensions, obtaining satisfactory inter-annotator agreement scores. Then, a VNC identification experiment is undertaken, where the analysed linguistic data is combined with chunking information and syntactic dependencies. A comparison between the results of the experiment and the results obtained by a basic detection method shows that VNC identification can be greatly improved by using linguistic information, as a large number of additional occurrences are detected with high precision.
University students in the United States are routinely asked to provide feedback on the quality of the instruction they have received. Such feedback is widely used by university administrators to evaluate teaching ability, despite growing evidence that students assign lower numerical scores to women and people of color, regardless of the actual quality of instruction. In this paper, we analyze students{'} written comments on faculty evaluation forms spanning eight years and five STEM disciplines in order to determine whether open-ended comments reflect these same biases. First, we apply sentiment analysis techniques to the corpus of comments to determine the overall affect of each comment. We then use this information, in combination with other features, to explore whether there is bias in how students describe their instructors. We show that while the gender of the evaluated instructor does not seem to affect students{'} expressed level of overall satisfaction with their instruction, it does strongly influence the language that they use to describe their instructors and their experience in class.
We study the problem of detecting sentences describing adverse drug reactions (ADRs) and frame the problem as binary classification. We investigate different neural network (NN) architectures for ADR classification. In particular, we propose two new neural network models, Convolutional Recurrent Neural Network (CRNN) by concatenating convolutional neural networks with recurrent neural networks, and Convolutional Neural Network with Attention (CNNA) by adding attention weights into convolutional neural networks. We evaluate various NN architectures on a Twitter dataset containing informal language and an Adverse Drug Effects (ADE) dataset constructed by sampling from MEDLINE case reports. Experimental results show that all the NN architectures outperform the traditional maximum entropy classifiers trained from n-grams with different weighting strategies considerably on both datasets. On the Twitter dataset, all the NN architectures perform similarly. But on the ADE dataset, CNN performs better than other more complex CNN variants. Nevertheless, CNNA allows the visualisation of attention weights of words when making classification decisions and hence is more appropriate for the extraction of word subsequences describing ADRs.
Misuse of Chinese prepositions is one of common word usage errors in grammatical error diagnosis. In this paper, we adopt the Chinese Gigaword corpus and HSK corpus as L1 and L2 corpora, respectively. We explore gated recurrent neural network model (GRU), and an ensemble of GRU model and maximum entropy language model (GRU-ME) to select the best preposition from 43 candidates for each test sentence. The experimental results show the advantage of the GRU models over simple RNN and n-gram models. We further analyze the effectiveness of linguistic information such as word boundary and part-of-speech tag in this task.
We investigate using Adaptor Grammars for unsupervised morphological segmentation. Using six development languages, we investigate in detail different grammars, the use of morphological knowledge from outside sources, and the use of a cascaded architecture. Using cross-validation on our development languages, we propose a system which is language-independent. We show that it outperforms two state-of-the-art systems on 5 out of 6 languages.
We describe and evaluate a character-level tagger for language-independent Named Entity Recognition (NER). Instead of words, a sentence is represented as a sequence of characters. The model consists of stacked bidirectional LSTMs which inputs characters and outputs tag probabilities for each character. These probabilities are then converted to consistent word level named entity tags using a Viterbi decoder. We are able to achieve close to state-of-the-art NER performance in seven languages with the same basic model using only labeled NER data and no hand-engineered features or other external resources like syntactic taggers or Gazetteers.
Historical texts are challenging for natural language processing because they differ linguistically from modern texts and because of their lack of orthographical and grammatical standardisation. We use a character-level neural network to build a part-of-speech (POS) tagger that can process historical data directly without requiring a separate spelling normalisation stage. Its performance in a Swedish verb identification and a German POS tagging task is similar to that of a two-stage model. We analyse the performance of this tagger and a more traditional baseline system, discuss some of the remaining problems for tagging historical data and suggest how the flexibility of our neural tagger could be exploited to address diachronic divergences in morphology and syntax in early modern Swedish with the help of data from closely related languages.
The goal of keyphrase extraction is to automatically identify the most salient phrases from documents. The technique has a wide range of applications such as rendering a quick glimpse of a document, or extracting key content for further use. While previous work often assumes keyphrases are a static property of a given documents, in many applications, the appropriate set of keyphrases that should be extracted depends on the set of documents that are being considered together. In particular, good keyphrases should not only accurately describe the content of a document, but also reveal what discriminates it from the other documents. In this paper, we study this problem of extracting discriminative keyphrases. In particularly, we propose to use the hierarchical semantic structure between candidate keyphrases to promote keyphrases that have the right level of specificity to clearly distinguish the target document from others. We show that such knowledge can be used to construct better discriminative keyphrase extraction systems that do not assume a static, fixed set of keyphrases for a document. We show how this helps identify key expertise of authors from their papers, as well as competencies covered by online courses within different domains.
On microblogging services, people usually use hashtags to mark microblogs, which have a specific theme or content, making them easier for users to find. Hence, how to automatically recommend hashtags for microblogs has received much attention in recent years. Previous deep neural network-based hashtag recommendation approaches converted the task into a multi-class classification problem. However, most of these methods only took the microblog itself into consideration. Motivated by the intuition that the history of users should impact the recommendation procedure, in this work, we extend end-to-end memory networks to perform this task. We incorporate the histories of users into the external memory and introduce a hierarchical attention mechanism to select more appropriate histories. To train and evaluate the proposed method, we also construct a dataset based on microblogs collected from Twitter. Experimental results demonstrate that the proposed methods can significantly outperform state-of-the-art methods. By incorporating the hierarchical attention mechanism, the relative improvement in the proposed method over the state-of-the-art method is around 67.9{\%} in the F1-score.
Topics generated by topic models are typically represented as list of terms. To reduce the cognitive overhead of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Comparing to a state-of-the-art topic labelling system, our methodology is simpler, more efficient and finds better topic labels.
This paper presents a new memory-bounded left-corner parsing model for unsupervised raw-text syntax induction, using unsupervised hierarchical hidden Markov models (UHHMM). We deploy this algorithm to shed light on the extent to which human language learners can discover hierarchical syntax through distributional statistics alone, by modeling two widely-accepted features of human language acquisition and sentence processing that have not been simultaneously modeled by any existing grammar induction algorithm: (1) a left-corner parsing strategy and (2) limited working memory capacity. To model realistic input to human language learners, we evaluate our system on a corpus of child-directed speech rather than typical newswire corpora. Results beat or closely match those of three competing systems.
In this paper we discuss three key points related to error detection (ED) in learners{'} English. We focus on content word ED as one of the most challenging tasks in this area, illustrating our claims on adjective{--}noun (AN) combinations. In particular, we (1) investigate the role of context in accurately capturing semantic anomalies and implement a system based on distributional topic coherence, which achieves state-of-the-art accuracy on a standard test set; (2) thoroughly investigate our system{'}s performance across individual adjective classes, concluding that a class-dependent approach is beneficial to the task; (3) discuss the data size bottleneck in this area, and highlight the challenges of automatic error generation for content words.
This paper investigates the effectiveness of 65 cohesion-based variables that are commonly used in the literature as predictive features to assess text readability. We evaluate the efficiency of these variables across narrative and informative texts intended for an audience of L2 French learners. In our experiments, we use a French corpus that has been both manually and automatically annotated as regards to co-reference and anaphoric chains. The efficiency of the 65 variables for readability is analyzed through a correlational analysis and some modelling experiments.
This paper describes our construction of named-entity recognition (NER) systems in two Western Iranian languages, Sorani Kurdish and Tajik, as a part of a pilot study of {``}Linguistic Rapid Response{''} to potential emergency humanitarian relief situations. In the absence of large annotated corpora, parallel corpora, treebanks, bilingual lexica, etc., we found the following to be effective: exploiting distributional regularities in monolingual data, projecting information across closely related languages, and utilizing human linguist judgments. We show promising results on both a four-month exercise in Sorani and a two-day exercise in Tajik, achieved with minimal annotation costs.
In this paper, we investigate the annotation projection of semantic units in a practical setting. Previous approaches have focused on using parallel corpora for semantic transfer. We evaluate an alternative approach using loosely parallel corpora that does not require the corpora to be exact translations of each other. We developed a method that transfers semantic annotations from one language to another using sentences aligned by entities, and we extended it to include alignments by entity-like linguistic units. We conducted our experiments on a large scale using the English, Swedish, and French language editions of Wikipedia. Our results show that the annotation projection using entities in combination with loosely parallel corpora provides a viable approach to extending previous attempts. In addition, it allows the generation of proposition banks upon which semantic parsers can be trained.
In this paper, we present phoneme level Siamese convolutional networks for the task of pair-wise cognate identification. We represent a word as a two-dimensional matrix and employ a siamese convolutional network for learning deep representations. We present siamese architectures that jointly learn phoneme level feature representations and language relatedness from raw words for cognate identification. Compared to previous works, we train and test on larger and realistic datasets; and, show that siamese architectures consistently perform better than traditional linear classifier approach.
This paper investigates differential topic models (dTM) for summarizing the differences among document groups. Starting from a simple probabilistic generative model, we propose dTM-SAGE that explicitly models the deviations on group-specific word distributions to indicate how words are used differen-tially across different document groups from a background word distribution. It is more effective to capture unique characteristics for comparing document groups. To generate dTM-based comparative summaries, we propose two sentence scoring methods for measuring the sentence discriminative capacity. Experimental results on scientific papers dataset show that our dTM-based comparative summari-zation methods significantly outperform the generic baselines and the state-of-the-art comparative summarization methods under ROUGE metrics.
Coherent extracts are a novel type of summary combining the advantages of manually created abstractive summaries, which are fluent but difficult to evaluate, and low-quality automatically created extractive summaries, which lack coherence and structure. We use a corpus of heterogeneous documents to address the issue that information seekers usually face {--} a variety of different types of information sources. We directly extract information from these, but minimally redact and meaningfully order it to form a coherent text. Our qualitative and quantitative evaluations show that quantitative results are not sufficient to judge the quality of a summary and that other quality criteria, such as coherence, should also be taken into account. We find that our manually created corpus is of high quality and that it has the potential to bridge the gap between reference corpora of abstracts and automatic methods producing extracts. Our corpus is available to the research community for further development.
Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user{'}s writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planning-based method can ensure that the generated poem is coherent and semantically consistent with the user{'}s intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets.
We examine the task of aggregation in the context of text-to-text generation. We introduce a new aggregation task which frames the process as grouping input sentence fragments into clusters that are to be expressed as a single output sentence. We extract datasets for this task from a corpus using an automatic extraction process. Based on the results of a user study, we develop two gold-standard clusterings and corresponding evaluation methods for each dataset. We present a hierarchical clustering framework for predicting aggregation decisions on this task, which outperforms several baselines and can serve as a reference in future work.
Recently Wen et al. (2015) have proposed a Recurrent Neural Network (RNN) approach to the generation of utterances from dialog acts, and shown that although their model requires less effort to develop than a rule-based system, it is able to improve certain aspects of the utterances, in particular their naturalness. However their system employs generation at the word-level, which requires one to pre-process the data by substituting named entities with placeholders. This pre-processing prevents the model from handling some contextual effects and from managing multiple occurrences of the same attribute. Our approach uses a character-level model, which unlike the word-level model makes it possible to learn to {``}copy{''} information from the dialog act to the target without having to pre-process the input. In order to avoid generating non-words and inventing information not present in the input, we propose a method for incorporating prior knowledge into the RNN in the form of a weighted finite-state automaton over character sequences. Automatic and human evaluations show improved performance over baselines on several evaluation criteria.
Readers usually rely on abstracts to identify relevant medical information from scientific articles. Abstracts are also essential to advanced information retrieval methods. More than 50 thousand scientific publications in PubMed lack author-generated abstracts, and the relevancy judgements for these papers have to be based on their titles alone. In this paper, we propose a hybrid summarization technique that aims to select the most pertinent sentences from articles to generate an extractive summary in lieu of a missing abstract. We combine i) health outcome detection, ii) keyphrase extraction, and iii) textual entailment recognition between sentences. We evaluate our hybrid approach and analyze the improvements of multi-factor summarization over techniques that rely on a single method, using a collection of 295 manually generated reference summaries. The obtained results show that the hybrid approach outperforms the baseline techniques with an improvement of 13{\%} in recall and 4{\%} in F1 score.
Natural language generation (NLG) is the task of generating natural language from a meaning representation. Current rule-based approaches require domain-specific and manually constructed linguistic resources, while most machine-learning based approaches rely on aligned training data and/or phrase templates. The latter are needed to restrict the search space for the structured prediction task defined by the unaligned datasets. In this work we propose the use of imitation learning for structured prediction which learns an incremental model that handles the large search space by avoiding explicit enumeration of the outputs. We focus on the Locally Optimal Learning to Search framework which allows us to train against non-decomposable loss functions such as the BLEU or ROUGE scores while not assuming gold standard alignments. We evaluate our approach on three datasets using both automatic measures and human judgements and achieve results comparable to the state-of-the-art approaches developed for each of them.
We propose a phrase-based approach for generating product review summaries. The main idea of our method is to leverage phrase properties to choose a subset of optimal phrases for generating the final summary. Specifically, we exploit two phrase properties, popularity and specificity. Popularity describes how popular the phrase is in the original reviews. Specificity describes how descriptive a phrase is in comparison to generic comments. We formalize the phrase selection procedure as an optimization problem and solve it using integer linear programming (ILP). An aspect-based bigram language model is used for generating the final summary with the selected phrases. Experiments show that our summarizer outperforms the other baselines.
We present a novel approach to automated question generation that improves upon prior work both from a technology perspective and from an assessment perspective. Our system is aimed at engaging language learners by generating multiple-choice questions which utilize specific inference steps over multiple sentences, namely coreference resolution and paraphrase detection. The system also generates correct answers and semantically-motivated phrase-level distractors as answer choices. Evaluation by human annotators indicates that our approach requires a larger number of inference steps, which necessitate deeper semantic understanding of texts than a traditional single-sentence approach.
Despite the growing number of Computational Construction Grammar implementations, the field is still lacking evaluation methods to compare grammar fragments across different platforms. Moreover, the hand-crafted nature of most grammars requires profiling tools to understand the complex interactions between constructions of different types. This paper presents a number of evaluation measures, partially based on existing measures in the field of semantic parsing, that are especially relevant for reversible grammar formalisms. The measures are tested on a grammar fragment for European Portuguese clitic placement that is currently under development.
Methods for text simplification using the framework of statistical machine translation have been extensively studied in recent years. However, building the monolingual parallel corpus necessary for training the model requires costly human annotation. Monolingual parallel corpora for text simplification have therefore been built only for a limited number of languages, such as English and Portuguese. To obviate the need for human annotation, we propose an unsupervised method that automatically builds the monolingual parallel corpus for text simplification using sentence similarity based on word embeddings. For any sentence pair comprising a complex sentence and its simple counterpart, we employ a many-to-one method of aligning each word in the complex sentence with the most similar word in the simple sentence and compute sentence similarity by averaging these word similarities. The experimental results demonstrate the excellent performance of the proposed method in a monolingual parallel corpus construction task for English text simplification. The results also demonstrated the superior accuracy in text simplification that use the framework of statistical machine translation trained using the corpus built by the proposed method to that using the existing corpora.
This paper presents an approach combining lexico-semantic resources and distributed representations of words applied to the evaluation in machine translation (MT). This study is made through the enrichment of a well-known MT evaluation metric: METEOR. METEOR enables an approximate match (synonymy or morphological similarity) between an automatic and a reference translation. Our experiments are made in the framework of the Metrics task of WMT 2014. We show that distributed representations are a good alternative to lexico-semanticresources for MT evaluation and they can even bring interesting additional information. The augmented versions of METEOR, using vector representations, are made available on our Github page.
One of the main obstacles, hampering method development and comparative evaluation of named entity recognition in social media, is the lack of a sizeable, diverse, high quality annotated corpus, analogous to the CoNLL{'}2003 news dataset. For instance, the biggest Ritter tweet corpus is only 45,000 tokens {--} a mere 15{\%} the size of CoNLL{'}2003. Another major shortcoming is the lack of temporal, geographic, and author diversity. This paper introduces the Broad Twitter Corpus (BTC), which is not only significantly bigger, but sampled across different regions, temporal periods, and types of Twitter users. The gold-standard named entity annotations are made by a combination of NLP experts and crowd workers, which enables us to harness crowd recall while maintaining high quality. We also measure the entity drift observed in our dataset (i.e. how entity representation varies over time), and compare to newswire. The corpus is released openly, including source text and intermediate annotations.
Semantic text processing faces the challenge of defining the relation between lexical expressions and the world to which they make reference within a period of time. It is unclear whether the current test sets used to evaluate disambiguation tasks are representative for the full complexity considering this time-anchored relation, resulting in semantic overfitting to a specific period and the frequent phenomena within. We conceptualize and formalize a set of metrics which evaluate this complexity of datasets. We provide evidence for their applicability on five different disambiguation tasks. To challenge semantic overfitting of disambiguation systems, we propose a time-based, metric-aware method for developing datasets in a systematic and semi-automated manner, as well as an event-based QA task.
There are growing needs for patent analysis using Natural Language Processing (NLP)-based approaches. Although NLP-based approaches can extract various information from patents, there are very few approaches proposed to extract those parts what inventors regard as novel or having an inventive step compared to all existing works ever. To extract such parts is difficult even for human annotators except for well-trained experts. This causes many difficulties in analyzing patents. We propose a novel approach to automatically extract such keywords that relate to novelties or inventive steps from patent claims using the structure of the claims. In addition, we also propose a new framework of evaluating our approach. The experiments show that our approach outperforms the existing keyword extraction methods significantly in many technical fields.
Event extraction has become one of the most important topics in information extraction, but to date, there is very limited work on leveraging cross-lingual training to boost performance. We propose a new event extraction approach that trains on multiple languages using a combination of both language-dependent and language-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset.
We introduce a generic Language Independent Framework for Linguistic Code Switch Point Detection. The system uses characters level 5-grams and word level unigram language models to train a conditional random fields (CRF) model for classifying input words into various languages. We test our proposed framework and compare it to the state-of-the-art published systems on standard data sets from several language pairs: English-Spanish, Nepali-English, English-Hindi, Arabizi (Refers to Arabic written using the Latin/Roman script)-English, Arabic-Engari (Refers to English written using Arabic script), Modern Standard Arabic(MSA)-Egyptian, Levantine-MSA, Gulf-MSA, one more English-Spanish, and one more MSA-EGY. The overall weighted average F-score of each language pair are 96.4{\%}, 97.3{\%}, 98.0{\%}, 97.0{\%}, 98.9{\%}, 86.3{\%}, 88.2{\%}, 90.6{\%}, 95.2{\%}, and 85.0{\%} respectively. The results show that our approach despite its simplicity, either outperforms or performs at comparable levels to state-of-the-art published systems.
We present in this paper a purely rule-based system for Question Classification which we divide into two parts: The first is the extraction of relevant words from a question by use of its structure, and the second is the classification of questions based on rules that associate these words to Concepts. We achieve an accuracy of 97.2{\%}, close to a 6 point improvement over the previous State of the Art of 91.6{\%}. Additionally, we believe that machine learning algorithms can be applied on top of this method to further improve accuracy.
Identification of Multi-Word Expressions (MWEs) lies at the heart of many natural language processing applications. In this research, we deal with a particular type of Hebrew MWEs, Verb-Noun MWEs (VN-MWEs), which combine a verb and a noun with or without other words. Most prior work on MWEs classification focused on linguistic and statistical information. In this paper, we claim that it is essential to utilize semantic information. To this end, we propose a semantically motivated indicator for classifying VN-MWE and define features that are related to various semantic spaces and combine them as features in a supervised classification framework. We empirically demonstrate that our semantic feature set yields better performance than the common linguistic and statistical feature sets and that combining semantic features contributes to the VN-MWEs identification task.
This paper describes a unified neural architecture for identifying and classifying multi-typed semantic relations between words in a sentence. We investigate two typical and well-studied tasks: semantic role labeling (SRL) which identifies the relations between predicates and arguments, and relation classification (RC) which focuses on the relation between two entities or nominals. While mostly studied separately in prior work, we show that the two tasks can be effectively connected and modeled using a general architecture. Experiments on CoNLL-2009 benchmark datasets show that our SRL models significantly outperform state-of-the-art approaches. Our RC models also yield competitive performance with the best published records. Furthermore, we show that the two tasks can be trained jointly with multi-task learning, resulting in additive significant improvements for SRL.
We present a successful collaboration of word embeddings and co-training to tackle in the most difficult test case of semantic role labeling: predicting out-of-domain and unseen semantic frames. Despite the fact that co-training is a successful traditional semi-supervised method, its application in SRL is very limited especially when a huge amount of labeled data is available. In this work, co-training is used together with word embeddings to improve the performance of a system trained on a large training dataset. We also introduce a semantic role labeling system with a simple learning architecture and effective inference that is easily adaptable to semi-supervised settings with new training data and/or new features. On the out-of-domain testing set of the standard benchmark CoNLL 2009 data our simple approach achieves high performance and improves state-of-the-art results.
Compositional distributional semantic models (CDSMs) have successfully been applied to the task of predicting the meaning of a range of linguistic constructions. Their performance on semi-compositional word formation process of (morphological) derivation, however, has been extremely variable, with no large-scale empirical investigation to date. This paper fills that gap, performing an analysis of CDSM predictions on a large dataset (over 30,000 German derivationally related word pairs). We use linear regression models to analyze CDSM performance and obtain insights into the linguistic factors that influence how predictable the distributional context of a derived word is going to be. We identify various such factors, notably part of speech, argument structure, and semantic regularity.
In recent years linguistic typologies, which classify the world{'}s languages according to their functional and structural properties, have been widely used to support multilingual NLP. While the growing importance of typologies in supporting multilingual tasks has been recognised, no systematic survey of existing typological resources and their use in NLP has been published. This paper provides such a survey as well as discussion which we hope will both inform and inspire future work in the area.
We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning.
Light verb constructions (LVC) in Hindi are highly productive. If we can distinguish a case such as nirnay lenaa {`}decision take; decide{'} from an ordinary verb-argument combination kaagaz lenaa {`}paper take; take (a) paper{'},it has been shown to aid NLP applications such as parsing (Begum et al., 2011) and machine translation (Pal et al., 2011). In this paper, we propose an LVC identification system using language specific features for Hindi which shows an improvement over previous work(Begum et al., 2011). To build our system, we carry out a linguistic analysis of Hindi LVCs using Hindi Treebank annotations and propose two new features that are aimed at capturing the diversity of Hindi LVCs in the corpus. We find that our model performs robustly across a diverse range of LVCs and our results underscore the importance of semantic features, which is in keeping with the findings for English. Our error analysis also demonstrates that our classifier can be used to further refine LVC annotations in the Hindi Treebank and make them more consistent across the board.
Several recent studies have shown that eye movements during reading provide information about grammatical and syntactic processing, which can assist the induction of NLP models. All these studies have been limited to English, however. This study shows that gaze and part of speech (PoS) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models.
Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a two-channel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task.
Hypernym-hyponym ({``}is-a{''}) relations are key components in taxonomies, object hierarchies and knowledge graphs. While there is abundant research on is-a relation extraction in English, it still remains a challenge to identify such relations from Chinese knowledge sources accurately due to the flexibility of language expression. In this paper, we introduce a weakly supervised framework to extract Chinese is-a relations from user generated categories. It employs piecewise linear projection models trained on a Chinese taxonomy and an iterative learning algorithm to update models incrementally. A pattern-based relation selection method is proposed to prevent {``}semantic drift{''} in the learning process using bi-criteria optimization. Experimental results illustrate that the proposed approach outperforms state-of-the-art methods.
As time passes words can acquire meanings they did not previously have, such as the {`}twitter post{'} usage of {`}tweet{'}. We address how this can be detected from time-stamped raw text. We propose a generative model with senses dependent on times and context words dependent on senses but otherwise eternal, and a Gibbs sampler for estimation. We obtain promising parameter estimates for positive (resp. negative) cases of known sense emergence (resp non-emergence) and adapt the {`}pseudo-word{'} technique (Schutze, 1992) to give a novel further evaluation via {`}pseudo-neologisms{'}. The question of ground-truth is also addressed and a technique proposed to locate an emergence date for evaluation purposes.
Determining the intended sense of words in text {--} word sense disambiguation (WSD) {--} is a long-standing problem in natural language processing. Recently, researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms. However, a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text. In this paper, we study WSD with a sequence learning neural net, LSTM, to better capture the sequential and syntactic patterns of the text. To alleviate the lack of training data in all-words WSD, we employ the same LSTM in a semi-supervised label propagation classifier. We demonstrate state-of-the-art results, especially on verbs.
Neural network training has been shown to be advantageous in many natural language processing applications, such as language modelling or machine translation. In this paper, we describe in detail a novel domain adaptation mechanism in neural network training. Instead of learning and adapting the neural network on millions of training sentences {--} which can be very time-consuming or even infeasible in some cases {--} we design a domain adaptation gating mechanism which can be used in recurrent neural networks and quickly learn the out-of-domain knowledge directly from the word vector representations with little speed overhead. In our experiments, we use the recurrent neural network language model (LM) as a case study. We show that the neural LM perplexity can be reduced by 7.395 and 12.011 using the proposed domain adaptation mechanism on the Penn Treebank and News data, respectively. Furthermore, we show that using the domain-adapted neural LM to re-rank the statistical machine translation n-best list on the French-to-English language pair can significantly improve translation quality.
Evaluation of machine translation (MT) into morphologically rich languages (MRL) has not been well studied despite posing many challenges. In this paper, we explore the use of embeddings obtained from different levels of lexical and morpho-syntactic linguistic analysis and show that they improve MT evaluation into an MRL. Specifically we report on Arabic, a language with complex and rich morphology. Our results show that using a neural-network model with different input representations produces results that clearly outperform the state-of-the-art for MT evaluation into Arabic, by almost over 75{\%} increase in correlation with human judgments on pairwise MT evaluation quality task. More importantly, we demonstrate the usefulness of morpho-syntactic representations to model sentence similarity for MT evaluation and address complex linguistic phenomena of Arabic.
In this paper we describe and evaluate methods to perform ensemble prediction in neural machine translation (NMT). We compare two methods of ensemble set induction: sampling parameter initializations for an NMT system, which is a relatively established method in NMT (Sutskever et al., 2014), and NMT systems translating from different source languages into the same target language, i.e., multi-source ensembles, a method recently introduced by Firat et al. (2016). We are motivated by the observation that for different language pairs systems make different types of mistakes. We propose several methods with different degrees of parameterization to combine individual predictions of NMT systems so that they mutually compensate for each other{'}s mistakes and improve overall performance. We find that the biggest improvements can be obtained from a context-dependent weighting scheme for multi-source ensembles. This result offers stronger support for the linguistic motivation of using multi-source ensembles than previous approaches. Evaluation is carried out for German and French into English translation. The best multi-source ensemble method achieves an improvement of up to 2.2 BLEU points over the strongest single-source ensemble baseline, and a 2 BLEU improvement over a multi-source ensemble baseline.
In this paper, we propose a new decoding method for phrase-based statistical machine translation which directly uses multiple preordering candidates as a graph structure. Compared with previous phrase-based decoding methods, our method is based on a simple left-to-right dynamic programming in which no decoding-time reordering is performed. As a result, its runtime is very fast and implementing the algorithm becomes easy. Our system does not depend on specific preordering methods as long as they output multiple preordering candidates, and it is trivial to employ existing preordering methods into our system. In our experiments for translating diverse 11 languages into English, the proposed method outperforms conventional phrase-based decoder in terms of translation qualities under comparable or faster decoding time.
Identifying events of a specific type is a challenging task as events in texts are described in numerous and diverse ways. Aiming to resolve high complexities of event descriptions, previous work (Huang and Riloff, 2013) proposes multi-faceted event recognition and a bootstrapping method to automatically acquire both event facet phrases and event expressions from unannotated texts. However, to ensure high quality of learned phrases, this method is constrained to only learn phrases that match certain syntactic structures. In this paper, we propose a bilingual structure projection algorithm that explores linguistic divergences between two languages (Chinese and English) and mines new phrases with new syntactic structures, which have been ignored in the previous work. Experiments show that our approach can successfully find novel event phrases and structures, e.g., phrases headed by nouns. Furthermore, the newly mined phrases are capable of recognizing additional event descriptions and increasing the recall of event recognition.
Previous studies on temporal relation extraction focus on mining sentence-level information or enforcing coherence on different temporal relation types among various event mentions in the same sentence or neighboring sentences, largely ignoring those discourse-level temporal relations in nonadjacent sentences. In this paper, we propose a discourse-level global inference model to mine those temporal relations between event mentions in document-level, especially in nonadjacent sentences. Moreover, we provide various kinds of discourse-level constraints, which derived from event semantics, to further improve our global inference model. Evaluation on a Chinese corpus justifies the effectiveness of our discourse-level global inference model over two strong baselines.
Distant supervision is an efficient approach that automatically generates labeled data for relation extraction (RE). Traditional distantly supervised RE systems rely heavily on handcrafted features, and hence suffer from error propagation. Recently, a neural network architecture has been proposed to automatically extract features for relation classification. However, this approach follows the traditional expressed-at-least-once assumption, and fails to make full use of information across different sentences. Moreover, it ignores the fact that there can be multiple relations holding between the same entity pair. In this paper, we propose a multi-instance multi-label convolutional neural network for distantly supervised RE. It first relaxes the expressed-at-least-once assumption, and employs cross-sentence max-pooling so as to enable information sharing across different sentences. Then it handles overlapping relations by multi-label learning with a neural network classifier. Experimental results show that our approach performs significantly and consistently better than state-of-the-art methods.
We propose an approach to Named Entity Disambiguation that avoids a problem of standard work on the task (likewise affecting fully supervised, weakly supervised, or distantly supervised machine learning techniques): the treatment of name mentions referring to people with no (or very little) coverage in the textual training data is systematically incorrect. We propose to indirectly take into account the property information for the {``}non-prominent{''} name bearers, such as nationality and profession (e.g., for a Canadian law professor named Michael Jackson, with no Wikipedia article, it is very hard to obtain reliable textual training data). The target property information for the entities is directly available from name authority files, or inferrable, e.g., from listings of sportspeople etc. Our proposed approach employs topic modeling to exploit textual training data based on entities sharing the relevant properties. In experiments with a pilot implementation of the general approach, we show that the approach does indeed work well for name/referent pairs with limited textual coverage in the training data.
In Natural Language Generation (NLG), one important limitation is the lack of common benchmarks on which to train, evaluate and compare data-to-text generators. In this paper, we make one step in that direction and introduce a method for automatically creating an arbitrary large repertoire of data units that could serve as input for generation. Using both automated metrics and a human evaluation, we show that the data units produced by our method are both diverse and coherent.
Sentence compression is a way to perform text simplification and is usually handled in a monolingual setting. In this paper, we study ways to extend sentence compression in a bilingual context, where the goal is to obtain parallel compressions of parallel sentences. This can be beneficial for a series of multilingual natural language processing (NLP) tasks. We compare two ways to take bilingual information into account when compressing parallel sentences. Their efficiency is contrasted on a parallel corpus of News articles.
In the age of information exploding, multi-document summarization is attracting particular attention for the ability to help people get the main ideas in a short time. Traditional extractive methods simply treat the document set as a group of sentences while ignoring the global semantics of the documents. Meanwhile, neural document model is effective on representing the semantic content of documents in low-dimensional vectors. In this paper, we propose a document-level reconstruction framework named DocRebuild, which reconstructs the documents with summary sentences through a neural document model and selects summary sentences to minimize the reconstruction error. We also apply two strategies, sentence filtering and beamsearch, to improve the performance of our method. Experimental results on the benchmark datasets DUC 2006 and DUC 2007 show that DocRebuild is effective and outperforms the state-of-the-art unsupervised algorithms.
The search space in grammar-based natural language generation tasks can get very large, which is particularly problematic when generating long utterances or paragraphs. Using surface realization with OpenCCG as an example, we show that we can effectively detect partial solutions (edges) which cannot ultimately be part of a complete sentence because of their syntactic category. Formulating the completion of an edge into a sentence as finding a solution path in a large state-transition system, we demonstrate a connection to AI Planning which is concerned with this kind of problem. We design a compilation from OpenCCG into AI Planning allowing the detection of infeasible edges via AI Planning dead-end detection methods (proving the absence of a solution to the compilation). Our experiments show that this can filter out large fractions of infeasible edges in, and thus benefit the performance of, complex realization processes.
Research in multi-document summarization has focused on newswire corpora since the early beginnings. However, the newswire genre provides genre-specific features such as sentence position which are easy to exploit in summarization systems. Such easy to exploit genre-specific features are available in other genres as well. We therefore present the new hMDS corpus for multi-document summarization, which contains heterogeneous source documents from multiple text genres, as well as summaries with different lengths. For the construction of the corpus, we developed a novel construction approach which is suited to build large and heterogeneous summarization corpora with little effort. The method reverses the usual process of writing summaries for given source documents: it combines already available summaries with appropriate source documents. In a detailed analysis, we show that our new corpus is significantly different from the homogeneous corpora commonly used, and that it is heterogeneous along several dimensions. Our experimental evaluation using well-known state-of-the-art summarization systems shows that our corpus poses new challenges in the field of multi-document summarization. Last but not least, we make our corpus publicly available to the research community at the corpus web page \url{https://github.com/AIPHES/hMDS}.
In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis {--} that assumes a single entity per document {---} and targeted sentiment analysis {---} that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform,i.e. QA, is used for fine-grained opinion mining. Text coming from QA platforms are far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks
Sentiment polarity lexicons are key resources for sentiment analysis, and researchers have invested a lot of efforts in their manual creation. However, there has been a recent shift towards automatically extracted lexicons, which are orders of magnitude larger and perform much better. These lexicons are typically mined using bootstrapping, starting from very few seed words whose polarity is given, e.g., 50-60 words, and sometimes even just 5-6. Here we demonstrate that much higher-quality lexicons can be built by starting with hundreds of words and phrases as seeds, especially when they are in-domain. Thus, we combine (i) mid-sized high-quality manually crafted lexicons as seeds and (ii) bootstrapping, in order to build large-scale lexicons.
This work investigates whether the development of ideas in writing can be captured by graph properties derived from the text. Focusing on student essays, we represent the essay as a graph, and encode a variety of graph properties including PageRank as features for modeling essay scores related to quality of development. We demonstrate that our approach improves on a state-of-the-art system on the task of holistic scoring of persuasive essays and on the task of scoring narrative essays along the development dimension.
Emotion classification from text typically requires some degree of word-emotion association, either gathered from pre-existing emotion lexicons or calculated using some measure of semantic relatedness. Most emotion lexicons contain a fixed number of emotion categories and provide a rather limited coverage. Current measures of computing semantic relatedness, on the other hand, do not adapt well to the specific task of word-emotion association and therefore, yield average results. In this work, we propose an unsupervised method of learning word-emotion association from large text corpora, called Selective Co-occurrences (SECO), by leveraging the property of mutual exclusivity generally exhibited by emotions. Extensive evaluation, using just one seed word per emotion category, indicates the effectiveness of the proposed approach over three emotion lexicons and two state-of-the-art models of word embeddings on three datasets from different domains.
NBSVM is one of the most popular methods for text classification and has been widely used as baselines for various text representation approaches. It uses Naive Bayes (NB) feature to weight sparse bag-of-n-grams representation. N-gram captures word order in short context and NB feature assigns more weights to those important words. However, NBSVM suffers from sparsity problem and is reported to be exceeded by newly proposed distributed (dense) text representations learned by neural networks. In this paper, we transfer the n-grams and NB weighting to neural models. We train n-gram embeddings and use NB weighting to guide the neural models to focus on important words. In fact, our methods can be viewed as distributed (dense) counterparts of sparse bag-of-n-grams in NBSVM. We discover that n-grams and NB weighting are also effective in distributed representations. As a result, our models achieve new strong baselines on 9 text classification datasets, e.g. on IMDB dataset, we reach performance of 93.5{\%} accuracy, which exceeds previous state-of-the-art results obtained by deep neural models. All source codes are publicly available at \url{https://github.com/zhezhaoa/neural_BOW_toolkit}.
Sarcasm detection is a key task for many natural language processing tasks. In sentiment analysis, for example, sarcasm can flip the polarity of an {``}apparently positive{''} sentence and, hence, negatively affect polarity detection performance. To date, most approaches to sarcasm detection have treated the task primarily as a text categorization problem. Sarcasm, however, can be expressed in very subtle ways and requires a deeper understanding of natural language that standard text categorization techniques cannot grasp. In this work, we develop models based on a pre-trained convolutional neural network for extracting sentiment, emotion and personality features for sarcasm detection. Such features, along with the network{'}s baseline features, allow the proposed models to outperform the state of the art on benchmark datasets. We also address the often ignored generalizability issue of classifying data that have not been seen by the models at learning phase.
Cross-lingual sentiment classification (CLSC) seeks to use resources from a source language in order to detect sentiment and classify text in a target language. Almost all research into CLSC has been carried out at sentence and document level, although this level of granularity is often less useful. This paper explores methods for performing aspect-based cross-lingual sentiment classification (aspect-based CLSC) for under-resourced languages. Given the limited nature of parallel data for many languages, we would like to make the most of this resource for our task. We compare zero-shot learning, bilingual word embeddings, stacked denoising autoencoder representations and machine translation techniques for aspect-based CLSC. Each of these approaches requires differing amounts of parallel data. We show that models based on distributed semantics can achieve comparable results to machine translation on aspect-based CLSC and give an analysis of the errors found for each method.
Emotions in code-switching text can be expressed in either monolingual or bilingual forms. However, relatively little research has emphasized on code-switching text. In this paper, we propose a Bilingual Attention Network (BAN) model to aggregate the monolingual and bilingual informative words to form vectors from the document representation, and integrate the attention vectors to predict the emotion. The experiments show that the effectiveness of the proposed model. Visualization of the attention layers illustrates that the model selects qualitatively informative words.
Most neural network models for document classification on social media focus on text information to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work, showing that UTCNN performs well regardless of language or platform.
Human languages have multiple strategies that allow us to discriminate objects in a vast variety of contexts. Colours have been extensively studied from this point of view. In particular, previous research in artificial language evolution has shown how artificial languages may emerge based on specific strategies to distinguish colours. Still, it has not been shown how several strategies of diverse complexity can be autonomously managed by artificial agents . We propose an intrinsic motivation system that allows agents in a population to create a shared artificial language and progressively increase its expressive power. Our results show that with such a system agents successfully regulate their language development, which indicates a relation between population size and consistency in the emergent communicative systems.
Evocation is a directed yet weighted semantic relationship between lexicalized concepts. Although evocation relations are considered potentially useful in several semantic NLP tasks, the prediction of the evocation relation between an arbitrary pair of concepts remains difficult, since evocation relationships cover a broader range of semantic relations rooted in human perception and experience. This paper presents a supervised learning approach to predict the strength (by regression) and to determine the directionality (by classification) of the evocation relation that might hold between a pair of lexicalized concepts. Empirical results that were obtained by investigating useful features are shown, indicating that a combination of the proposed features largely outperformed individual baselines, and also suggesting that semantic relational vectors computed from existing semantic vectors for lexicalized concepts were indeed effective for both the prediction of strength and the determination of directionality.
Exploring language usage through frequency analysis in large corpora is a defining feature in most recent work in corpus and computational linguistics. From a psycholinguistic perspective, however, the corpora used in these contributions are often not representative of language usage: they are either domain-specific, limited in size, or extracted from unreliable sources. In an effort to address this limitation, we introduce SubIMDB, a corpus of everyday language spoken text we created which contains over 225 million words. The corpus was extracted from 38,102 subtitles of family, comedy and children movies and series, and is the first sizeable structured corpus of subtitles made available. Our experiments show that word frequency norms extracted from this corpus are more effective than those from well-known norms such as Kucera-Francis, HAL and SUBTLEXus in predicting various psycholinguistic properties of words, such as lexical decision times, familiarity, age of acquisition and simplicity. We also provide evidence that contradict the long-standing assumption that the ideal size for a corpus can be determined solely based on how well its word frequencies correlate with lexical decision times.
Argument mining aims to determine the argumentative structure of texts. Although it is said to be crucial for future applications such as writing support systems, the benefit of its output has rarely been evaluated. This paper puts the analysis of the output into the focus. In particular, we investigate to what extent the mined structure can be leveraged to assess the argumentation quality of persuasive essays. We find insightful statistical patterns in the structure of essays. From these, we derive novel features that we evaluate in four argumentation-related essay scoring tasks. Our results reveal the benefit of argument mining for assessing argumentation quality. Among others, we improve the state of the art in scoring an essay{'}s organization and its argument strength.
Language students are most engaged while reading texts at an appropriate difficulty level. However, existing methods of evaluating text difficulty focus mainly on vocabulary and do not prioritize grammatical features, hence they do not work well for language learners with limited knowledge of grammar. In this paper, we introduce grammatical templates, the expert-identified units of grammar that students learn from class, as an important feature of text difficulty evaluation. Experimental classification results show that grammatical template features significantly improve text difficulty prediction accuracy over baseline readability features by 7.4{\%}. Moreover,we build a simple and human-understandable text difficulty evaluation approach with 87.7{\%} accuracy, using only 5 grammatical template features.
We analyze the performance of encoder-decoder neural models and compare them with well-known established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs.
Knowledge graph (KG) completion adds new facts to a KG by making inferences from existing facts. Most existing methods ignore the time information and only learn from time-unknown fact triples. In dynamic environments that evolve over time, it is important and challenging for knowledge graph completion models to take into account the temporal aspects of facts. In this paper, we present a novel time-aware knowledge graph completion model that is able to predict links in a KG using both the existing facts and the temporal information of the facts. To incorporate the happening time of facts, we propose a time-aware KG embedding model using temporal order information among facts. To incorporate the valid time of facts, we propose a joint time-aware inference model based on Integer Linear Programming (ILP) using temporal consistencyinformationasconstraints. Wefurtherintegratetwomodelstomakefulluseofglobal temporal information. We empirically evaluate our models on time-aware KG completion task. Experimental results show that our time-aware models achieve the state-of-the-art on temporal facts consistently.
Ordinal regression which is known with learning to rank has long been used in information retrieval (IR). Learning to rank algorithms, have been tailored in document ranking, information filtering, and building large aligned corpora successfully. In this paper, we propose to use this algorithm for query modeling in cross-language environments. To this end, first we build a query-generated training data using pseudo-relevant documents to the query and all translation candidates. The pseudo-relevant documents are obtained by top-ranked documents in response to a translation of the original query. The class of each candidate in the training data is determined based on presence/absence of the candidate in the pseudo-relevant documents. We learn an ordinal regression model to score the candidates based on their relevance to the context of the query, and after that, we construct a query-dependent translation model using a softmax function. Finally, we re-weight the query based on the obtained model. Experimental results on French, German, Spanish, and Italian CLEF collections demonstrate that the proposed method achieves better results compared to state-of-the-art cross-language information retrieval methods, particularly in long queries with large training data.
This work focuses on answering single-relation factoid questions over Freebase. Each question can acquire the answer from a single fact of form (subject, predicate, object) in Freebase. This task, simple question answering (SimpleQA), can be addressed via a two-step pipeline: entity linking and fact selection. In fact selection, we match the subject entity in a fact candidate with the entity mention in the question by a character-level convolutional neural network (char-CNN), and match the predicate in that fact with the question by a word-level CNN (word-CNN). This work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task.
This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feedforward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most effective modern recurrent network {--} Long Short-Term Memory network. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.
The exchangeability assumption in topic models like Latent Dirichlet Allocation (LDA) often results in inferring inconsistent topics for the words of text spans like noun-phrases, which are usually expected to be topically coherent. We propose copulaLDA, that extends LDA by integrating part of the text structure to the model and relaxes the conditional independence assumption between the word-specific latent topics given the per-document topic distributions. To this end, we assume that the words of text spans like noun-phrases are topically bound and we model this dependence with copulas. We demonstrate empirically the effectiveness of copulaLDA on both intrinsic and extrinsic evaluation tasks on several publicly available corpora.
Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children{'}s Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research.
In modern text annotation projects, crowdsourced annotations are often aggregated using item response models or by majority vote. Recently, item response models enhanced with generative data models have been shown to yield substantial benefits over those with conditional or no data models. However, suitable generative data models do not exist for many tasks, such as semantic labeling tasks. When no generative data model exists, we demonstrate that similar benefits may be derived by conditionally modeling documents that have been previously embedded in a semantic space using recent work in vector space models. We use this approach to show state-of-the-art results on a variety of semantic annotation aggregation tasks.
Interactive-predictive machine translation (IPMT) is a translation mode which combines machine translation technology and human behaviours. In the IPMT system, the utilization of the prefix greatly affects the interaction efficiency. However, state-of-the-art methods filter translation hypotheses mainly according to their matching results with the prefix on character level, and the advantage of the prefix is not fully developed. Focusing on this problem, this paper mines the deep constraints of prefix on syntactic level to improve the performance of IPMT systems. Two syntactic subtree matching rules based on phrase structure grammar are proposed to filter the translation hypotheses more strictly. Experimental results on LDC Chinese-English corpora show that the proposed method outperforms state-of-the-art phrase-based IPMT system while keeping comparable decoding speed.
In recent years, neural machine translation (NMT) has demonstrated state-of-the-art machine translation (MT) performance. It is a new approach to MT, which tries to learn a set of parameters to maximize the conditional probability of target sentences given source sentences. In this paper, we present a novel approach to improve the translation performance in NMT by conveying topic knowledge during translation. The proposed topic-informed NMT can increase the likelihood of selecting words from the same topic and domain for translation. Experimentally, we demonstrate that topic-informed NMT can achieve a 1.15 (3.3{\%} relative) and 1.67 (5.4{\%} relative) absolute improvement in BLEU score on the Chinese-to-English language pair using NIST 2004 and 2005 test sets, respectively, compared to NMT without topic information.
We introduce a distribution based model to learn bilingual word embeddings from monolingual data. It is simple, effective and does not require any parallel data or any seed lexicon. We take advantage of the fact that word embeddings are usually in form of dense real-valued low-dimensional vector and therefore the distribution of them can be accurately estimated. A novel cross-lingual learning objective is proposed which directly matches the distributions of word embeddings in one language with that in the other language. During the joint learning process, we dynamically estimate the distributions of word embeddings in two languages respectively and minimize the dissimilarity between them through standard back propagation algorithm. Our learned bilingual word embeddings allow to group each word and its translations together in the shared vector space. We demonstrate the utility of the learned embeddings on the task of finding word-to-word translations from monolingual corpora. Our model achieved encouraging performance on data in both related languages and substantially different languages.
Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur. When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach. In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation (PBMT) system or a combination of the PBMT output and the source sentence. We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result.
With the success of word embedding methods in various Natural Language Processing tasks, all the field of distributional semantics has experienced a renewed interest. Beside the famous word2vec, recent studies have presented efficient techniques to build distributional thesaurus; in particular, Claveau et al. (2014) have already shown that Information Retrieval (IR) tools and concepts can be successfully used to build a thesaurus. In this paper, we address the problem of the evaluation of such thesauri or embedding models and compare their results. Through several experiments and by evaluating directly the results with reference lexicons, we show that the recent IR-based distributional models outperform state-of-the-art systems such as word2vec. Following the work of Claveau and Kijak (2016), we use IR as an applicative framework to indirectly evaluate the generated thesaurus. Here again, this task-based evaluation validates the IR approach used to build the thesaurus. Moreover, it allows us to compare these results with those from the direct evaluation framework used in the literature. The observed differences bring these evaluation habits into question.
We propose a new word embedding model, inspired by GloVe, which is formulated as a feasible least squares optimization problem. In contrast to existing models, we explicitly represent the uncertainty about the exact definition of each word vector. To this end, we estimate the error that results from using noisy co-occurrence counts in the formulation of the model, and we model the imprecision that results from including uninformative context words. Our experimental results demonstrate that this model compares favourably with existing word embedding models.
Most distributional lexico-semantic models derive their representations based on external language resources such as text corpora. In this study, we propose that internal language models, that are more closely aligned to the mental representations of words could provide important insights into cognitive science, including linguistics. Doing so allows us to reflect upon theoretical questions regarding the structure of the mental lexicon, and also puts into perspective a number of assumptions underlying recently proposed distributional text-based models. In particular, we focus on word-embedding models which have been proposed to learn aspects of word meaning in a manner similar to humans. These are contrasted with internal language models derived from a new extensive data set of word associations. Using relatedness and similarity judgments we evaluate these models and find that the word-association-based internal language models consistently outperform current state-of-the art text-based external language models, often with a large margin. These results are not just a performance improvement; they also have implications for our understanding of how distributional knowledge is used by people.
We propose a novel word embedding-based hypernym generation model that jointly learns clusters of hyponym-hypernym relations, i.e., hypernymy, and projections from hyponym to hypernym embeddings. Most of the recent hypernym detection models focus on a hypernymy classification problem that determines whether a pair of words is in hypernymy or not. These models do not directly deal with a hypernym generation problem in that a model generates hypernyms for a given word. Differently from previous studies, our model jointly learns the clusters and projections with adjusting the number of clusters so that the number of clusters can be determined depending on the learned projections and vice versa. Our model also boosts the performance by incorporating inner product-based similarity measures and negative examples, i.e., sampled non-hypernyms, into our objectives in learning. We evaluated our joint learning models on the task of Japanese and English hypernym generation and showed a significant improvement over an existing pipeline model. Our model also compared favorably to existing distributed hypernym detection models on the English hypernym classification task.
Information status plays an important role in discourse processing. According to the hearer{'}s common sense knowledge and his comprehension of the preceding text, a discourse entity could be old, mediated or new. In this paper, we propose an attention-based LSTM model to address the problem of fine-grained information status classification in an incremental manner. Our approach resembles how human beings process the task, i.e., decide the information status of the current discourse entity based on its preceding context. Experimental results on the ISNotes corpus (Markert et al., 2012) reveal that (1) despite its moderate result, our model with only word embedding features captures the necessary semantic knowledge needed for the task by a large extent; and (2) when incorporating with additional several simple features, our model achieves the competitive results compared to the state-of-the-art approach (Hou et al., 2013) which heavily depends on lots of hand-crafted semantic features.
In this paper, we investigate four important issues together for explicit discourse relation labelling in Chinese texts: (1) discourse connective extraction, (2) linking ambiguity resolution, (3) relation type disambiguation, and (4) argument boundary identification. In a pipelined Chinese discourse parser, we identify potential connective candidates by string matching, eliminate non-discourse usages from them with a binary classifier, resolve linking ambiguities among connective components by ranking, disambiguate relation types by a multiway classifier, and determine the argument boundaries by conditional random fields. The experiments on Chinese Discourse Treebank show that the F1 scores of 0.7506, 0.7693, 0.7458, and 0.3134 are achieved for discourse usage disambiguation, linking disambiguation, relation type disambiguation, and argument boundary identification, respectively, in a pipelined Chinese discourse parser.
We experiment with different ways of training LSTM networks to predict RST discourse trees. The main challenge for RST discourse parsing is the limited amounts of training data. We combat this by regularizing our models using task supervision from related tasks as well as alternative views on discourse structures. We show that a simple LSTM sequential discourse parser takes advantage of this multi-view and multi-task framework with 12-15{\%} error reductions over our baseline (depending on the metric) and results that rival more complex state-of-the-art parsers.
For the task of implicit discourse relation recognition, traditional models utilizing manual features can suffer from data sparsity problem. Neural models provide a solution with distributed representations, which could encode the latent semantic information, and are suitable for recognizing semantic relations between argument pairs. However, conventional vector representations usually adopt embeddings at the word level and cannot well handle the rare word problem without carefully considering morphological information at character level. Moreover, embeddings are assigned to individual words independently, which lacks of the crucial contextual information. This paper proposes a neural model utilizing context-aware character-enhanced embeddings to alleviate the drawbacks of the current word level representation. Our experiments show that the enhanced embeddings work well and the proposed model obtains state-of-the-art results.
This paper introduces a novel method for measuring non-cooperation in dialogue. The key idea is that linguistic non-cooperation can be measured in terms of the extent to which dialogue participants deviate from conventions regarding the proper introduction and discharging of conversational obligations (e.g., the obligation to respond to a question). Previous work on non cooperation has focused mainly on non-linguistic task-related non-cooperation or modelled non-cooperation in terms of special rules describing non-cooperative behaviours. In contrast, we start from rules for normal/correct dialogue behaviour - i.e., a dialogue game - which in principle can be derived from a corpus of cooperative dialogues, and provide a quantitative measure for the degree to which participants comply with these rules. We evaluated the model on a corpus of political interviews, with encouraging results. The model predicts accurately the degree of cooperation for one of the two dialogue game roles (interviewer) and also the relative cooperation for both roles (i.e., which interlocutor in the conversation was most cooperative). Being able to measure cooperation has applications in many areas from the analysis - manual, semi and fully automatic - of natural language interactions to human-like virtual personal assistants, tutoring agents, sophisticated dialogue systems, and role-playing virtual humans.
Determining the relative order of events and times described in text is an important problem in natural language processing. It is also a difficult one: general state-of-the-art performance has been stuck at a relatively low ceiling for years. We investigate the representation of temporal relations, and empirically evaluate the effect that various temporal relation representations have on machine learning performance. While machine learning performance decreases with increased representational expressiveness, not all representation simplifications have equal impact.
Cross document event coreference (CDEC) is an important task that aims at aggregating event-related information across multiple documents. We revisit the evaluation for CDEC, and discover that past works have adopted different, often inconsistent, evaluation settings, which either overlook certain mistakes in coreference decisions, or make assumptions that simplify the coreference task considerably. We suggest a new evaluation methodology which overcomes these limitations, and allows for an accurate assessment of CDEC systems. Our new evaluation setting better reflects the corpus-wide information aggregation ability of CDEC systems by separating event-coreference decisions made across documents from those made within a document. In addition, we suggest a better baseline for the task and semi-automatically identify several inconsistent annotations in the evaluation dataset.
This study proposes a computational model of the discourse segments in lyrics to understand and to model the structure of lyrics. To test our hypothesis that discourse segmentations in lyrics strongly correlate with repeated patterns, we conduct the first large-scale corpus study on discourse segments in lyrics. Next, we propose the task to automatically identify segment boundaries in lyrics and train a logistic regression model for the task with the repeated pattern and textual features. The results of our empirical experiments illustrate the significance of capturing repeated patterns in predicting the boundaries of discourse segments in lyrics.
In this paper we focus on the problem of dialog act (DA) labelling. This problem has recently attracted a lot of attention as it is an important sub-part of an automatic question answering system, which is currently in great demand. Traditional methods tend to see this problem as a sequence labelling task and deals with it by applying classifiers with rich features. Most of the current neural network models still omit the sequential information in the conversation. Henceforth, we apply a novel multi-level gated recurrent neural network (GRNN) with non-textual information to predict the DA tag. Our model not only utilizes textual information, but also makes use of non-textual and contextual information. In comparison, our model has shown significant improvement over previous works on Switchboard Dialog Act (SWDA) task by over 6{\%}.
Music information retrieval has emerged as a mainstream research area in the past two decades. Experiments on music mood classification have been performed mainly on Western music based on audio, lyrics and a combination of both. Unfortunately, due to the scarcity of digitalized resources, Indian music fares poorly in music mood retrieval research. In this paper, we identified the mood taxonomy and prepared multimodal mood annotated datasets for Hindi and Western songs. We identified important audio and lyric features using correlation based feature selection technique. Finally, we developed mood classification systems using Support Vector Machines and Feed Forward Neural Networks based on the features collected from audio, lyrics, and a combination of both. The best performing multimodal systems achieved F-measures of 75.1 and 83.5 for classifying the moods of the Hindi and Western songs respectively using Feed Forward Neural Networks. A comparative analysis indicates that the selected features work well for mood classification of the Western songs and produces better results as compared to the mood classification systems for Hindi songs.
While automatic response generation for building chatbot systems has drawn a lot of attention recently, there is limited understanding on when we need to consider the linguistic context of an input text in the generation process. The task is challenging, as messages in a conversational environment are short and informal, and evidence that can indicate a message is context dependent is scarce. After a study of social conversation data crawled from the web, we observed that some characteristics estimated from the responses of messages are discriminative for identifying context dependent messages. With the characteristics as weak supervision, we propose using a Long Short Term Memory (LSTM) network to learn a classifier. Our method carries out text representation and classifier learning in a unified framework. Experimental results show that the proposed method can significantly outperform baseline methods on accuracy of classification.
Identifying dialogue acts and dialogue modes during tutorial interactions is an extremely crucial sub-step in understanding patterns of effective tutor-tutee interactions. In this work, we develop a novel joint inference method that labels each utterance in a tutoring dialogue session with a dialogue act and a specific mode from a set of pre-defined dialogue acts and modes, respectively. Specifically, we develop our joint model using Markov Logic Networks (MLNs), a framework that combines first-order logic with probabilities, and is thus capable of representing complex, uncertain knowledge. We define first-order formulas in our MLN that encode the inter-dependencies between dialogue modes and more fine-grained dialogue actions. We then use a joint inference to jointly label the modes as well as the dialogue acts in an utterance. We compare our system against a pipeline system based on SVMs on a real-world dataset with tutoring sessions of over 500 students. Our results show that the joint inference system is far more effective than the pipeline system in mode detection, and improves over the performance of the pipeline system by about 6 points in F1 score. The joint inference system also performs much better than the pipeline system in the context of labeling modes that highlight important pedagogical steps in tutoring.
In this study, we applied a deep LSTM structure to classify dialogue acts (DAs) in open-domain conversations. We found that the word embeddings parameters, dropout regularization, decay rate and number of layers are the parameters that have the largest effect on the final system accuracy. Using the findings of these experiments, we trained a deep LSTM network that outperforms the state-of-the-art on the Switchboard corpus by 3.11{\%}, and MRDA by 2.2{\%}.
An interactive Question Answering (QA) system frequently encounters non-sentential (incomplete) questions. These non-sentential questions may not make sense to the system when a user asks them without the context of conversation. The system thus needs to take into account the conversation context to process the question. In this work, we present a recurrent neural network (RNN) based encoder decoder network that can generate a complete (intended) question, given an incomplete question and conversation context. RNN encoder decoder networks have been show to work well when trained on a parallel corpus with millions of sentences, however it is extremely hard to obtain conversation data of this magnitude. We therefore propose to decompose the original problem into two separate simplified problems where each problem focuses on an abstraction. Specifically, we train a semantic sequence model to learn semantic patterns, and a syntactic sequence model to learn linguistic patterns. We further combine syntactic and semantic sequence models to generate an ensemble model. Our model achieves a BLEU score of 30.15 as compared to 18.54 using a standard RNN encoder decoder model.
Natural language generation (NLG) is an important component of question answering(QA) systems which has a significant impact on system quality. Most tranditional QA systems based on templates or rules tend to generate rigid and stylised responses without the natural variation of human language. Furthermore, such methods need an amount of work to generate the templates or rules. To address this problem, we propose a Context-Aware LSTM model for NLG. The model is completely driven by data without manual designed templates or rules. In addition, the context information, including the question to be answered, semantic values to be addressed in the response, and the dialogue act type during interaction, are well approached in the neural network model, which enables the model to produce variant and informative responses. The quantitative evaluation and human evaluation show that CA-LSTM obtains state-of-the-art performance.
This work proposes a new confidence measure for evaluating text-to-speech alignment systems outputs, which is a key component for many applications, such as semi-automatic corpus anonymization, lips syncing, film dubbing, corpus preparation for speech synthesis and speech recognition acoustic models training. This confidence measure exploits deep neural networks that are trained on large corpora without direct supervision. It is evaluated on an open-source spontaneous speech corpus and outperforms a confidence score derived from a state-of-the-art text-to-speech aligner. We further show that this confidence measure can be used to fine-tune the output of this aligner and improve the quality of the resulting alignment.
In many applications such as personal digital assistants, there is a constant need for new domains to increase the system{'}s coverage of user queries. A conventional approach is to learn a separate model every time a new domain is introduced. This approach is slow, inefficient, and a bottleneck for scaling to a large number of domains. In this paper, we introduce a framework that allows us to have a single model that can handle all domains: including unknown domains that may be created in the future as long as they are covered in the master schema. The key idea is to remove the need for distinguishing domains by explicitly predicting the schema of queries. Given permitted schema of a query, we perform constrained decoding on a lattice of slot sequences allowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain.
Training good word embeddings requires large amounts of data. Out-of-vocabulary words will still be encountered at test-time, leaving these words without embeddings. To overcome this lack of embeddings for rare words, existing methods leverage morphological features to generate embeddings. While the existing methods use computationally-intensive rule-based (Soricut and Och, 2015) or tool-based (Botha and Blunsom, 2014) morphological analysis to generate embeddings, our system applies a computationally-simpler sub-word search on words that have existing embeddings. Embeddings of the sub-word search results are then combined using string similarity functions to generate rare word embeddings. We augmented pre-trained word embeddings with these novel embeddings and evaluated on a rare word similarity task, obtaining up to 3 times improvement in correlation over the original set of embeddings. Applying our technique to embeddings trained on larger datasets led to on-par performance with the existing state-of-the-art for this task. Additionally, while analysing augmented embeddings in a log-bilinear language model, we observed up to 50{\%} reduction in rare word perplexity in comparison to other more complex language models.
We introduce the task of detecting cross-lingual marketing blunders, which occur if a trade name resembles an inappropriate or negatively connotated word in a target language. To this end, we suggest a formal task definition and a semi-automatic method based the propagation of pragmatic labels from Wiktionary across sense-disambiguated translations. Our final tool assists users by providing clues for problematic names in any language, which we simulate in two experiments on detecting previously occurred marketing blunders and identifying relevant clues for established international brands. We conclude the paper with a suggested research roadmap for this new task. To initiate further research, we publish our online demo along with the source code and data at \url{http://uby.ukp.informatik.tu-darmstadt.de/blunder/}.
We present Ambient Search, an open source system for displaying and retrieving relevant documents in real time for speech input. The system works ambiently, that is, it unobstructively listens to speech streams in the background, identifies keywords and keyphrases for query construction and continuously serves relevant documents from its index. Query terms are ranked with Word2Vec and TF-IDF and are continuously updated to allow for ongoing querying of a document collection. The retrieved documents, in our case Wikipedia articles, are visualized in real time in a browser interface. Our evaluation shows that Ambient Search compares favorably to another implicit information retrieval system on speech streams. Furthermore, we extrinsically evaluate multiword keyphrase generation, showing positive impact for manual transcriptions.
In gender classification, labeled data is often limited while unlabeled data is ample. This motivates semi-supervised learning for gender classification to improve the performance by exploring the knowledge in both labeled and unlabeled data. In this paper, we propose a semi-supervised approach to gender classification by leveraging textual features and a specific kind of indirect links among the users which we call {``}same-interest{''} links. Specifically, we propose a factor graph, namely Textual and Social Factor Graph (TSFG), to model both the textual and the {``}same-interest{''} link information. Empirical studies demonstrate the effectiveness of the proposed approach to semi-supervised gender classification.
The lack of a sufficient amount of data tailored for a task is a well-recognized problem for many statistical NLP methods. In this paper, we explore whether data sparsity can be successfully tackled when classifying language proficiency levels in the domain of learner-written output texts. We aim at overcoming data sparsity by incorporating knowledge in the trained model from another domain consisting of input texts written by teaching professionals for learners. We compare different domain adaptation techniques and find that a weighted combination of the two types of data performs best, which can even rival systems based on considerably larger amounts of in-domain data. Moreover, we show that normalizing errors in learners{'} texts can substantially improve classification when level-annotated in-domain data is not available.
Textual information is of critical importance for automatic user classification in social media. However, most previous studies model textual features in a single perspective while the text in a user homepage typically possesses different styles of text, such as original message and comment from others. In this paper, we propose a novel approach, namely ensemble LSTM, to user classification by incorporating multiple textual perspectives. Specifically, our approach first learns a LSTM representation with a LSTM recurrent neural network and then presents a joint learning method to integrating all naturally-divided textual perspectives. Empirical studies on two basic user classification tasks, i.e., gender classification and age classification, demonstrate the effectiveness of the proposed approach to user classification with multiple textual perspectives.
We extend classic review mining work by building a binary classifier that predicts whether a review of a documentary film was written by an expert or a layman with 90.70{\%} accuracy (F1 score), and compare the characteristics of the predicted classes. A variety of standard lexical and syntactic features was used for this supervised learning task. Our results suggest that experts write comparatively lengthier and more detailed reviews that feature more complex grammar and a higher diversity in their vocabulary. Layman reviews are more subjective and contextualized in peoples{'} everyday lives. Our error analysis shows that laymen are about twice as likely to be mistaken as experts than vice versa. We argue that the type of author might be a useful new feature for improving the accuracy of predicting the rating, helpfulness and authenticity of reviews. Finally, the outcomes of this work might help researchers and practitioners in the field of impact assessment to gain a more fine-grained understanding of the perception of different types of media consumers and reviewers of a topic, genre or information product.
Representing structured events as vectors in continuous space offers a new way for defining dense features for natural language processing (NLP) applications. Prior work has proposed effective methods to learn event representations that can capture syntactic and semantic information over text corpus, demonstrating their effectiveness for downstream tasks such as event-driven stock prediction. On the other hand, events extracted from raw texts do not contain background knowledge on entities and relations that they are mentioned. To address this issue, this paper proposes to leverage extra information from knowledge graph, which provides ground truth such as attributes and properties of entities and encodes valuable relations between entities. Specifically, we propose a joint model to combine knowledge graph information into the objective function of an event embedding learning model. Experiments on event similarity and stock market prediction show that our model is more capable of obtaining better event embeddings and making more accurate prediction on stock market volatilities.
In this paper, we propose an approach to learn distributed representations of users and items from text comments for recommendation systems. Traditional recommendation algorithms, e.g. collaborative filtering and matrix completion, are not designed to exploit the key information hidden in the text comments, while existing opinion mining methods do not provide direct support to recommendation systems with useful features on users and items. Our approach attempts to construct vectors to represent profiles of users and items under a unified framework to maximize word appearance likelihood. Then, the vector representations are used for a recommendation task in which we predict scores on unobserved user-item pairs without given texts. The recommendation-aware distributed representation approach is fully supported by effective and efficient learning algorithms over massive text archive. Our empirical evaluations on real datasets show that our system outperforms the state-of-the-art baseline systems.
Long-distance semantic dependencies are crucial for lexical choice in statistical machine translation. In this paper, we study semantic dependencies between verbs and their arguments by modeling selectional preferences in the context of machine translation. We incorporate preferences that verbs impose on subjects and objects into translation. In addition, bilingual selectional preferences between source-side verbs and target-side arguments are also investigated. Our experiments on Chinese-to-English translation tasks with large-scale training data demonstrate that statistical machine translation using verbal selectional preferences can achieve statistically significant improvements over a state-of-the-art baseline.
Existing approaches for evaluating word order in machine translation work with metrics computed directly over a permutation of word positions in system output relative to a reference translation. However, every permutation factorizes into a permutation tree (PET) built of primal permutations, i.e., atomic units that do not factorize any further. In this paper we explore the idea that permutations factorizing into (on average) shorter primal permutations should represent simpler ordering as well. Consequently, we contribute Permutation Complexity, a class of metrics over PETs and their extension to forests, and define tight metrics, a sub-class of metrics implementing this idea. Subsequently we define example tight metrics and empirically test them in word order evaluation. Experiments on the WMT13 data sets for ten language pairs show that a tight metric is more often than not better than the baselines.
Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.
Automated short-answer grading is key to help close the automation loop for large-scale, computerised testing in education. A wide range of features on different levels of linguistic processing has been proposed so far. We investigate the relative importance of the different types of features across a range of standard corpora (both from a language skill and content assessment context, in English and in German). We find that features on the lexical, text similarity and dependency level often suffice to approximate full-model performance. Features derived from semantic processing particularly benefit the linguistically more varied answers in content assessment corpora.
Violence is a serious problems for cities like Chicago and has been exacerbated by the use of social media by gang-involved youths for taunting rival gangs. We present a corpus of tweets from a young and powerful female gang member and her communicators, which we have annotated with discourse intention, using a deep read to understand how and what triggered conversations to escalate into aggression. We use this corpus to develop a part-of-speech tagger and phrase table for the variant of English that is used and a classifier for identifying tweets that express grieving and aggression.
Nowadays, social media has become a popular platform for companies to understand their customers. It provides valuable opportunities to gain new insights into how a person{'}s opinion about a product is influenced by his friends. Though various approaches have been proposed to study the opinion formation problem, they all formulate opinions as the derived sentiment values either discrete or continuous without considering the semantic information. In this paper, we propose a Content-based Social Influence Model to study the implicit mechanism underlying the change of opinions. We then apply the learned model to predict users{'} future opinions. The advantages of the proposed model is the ability to handle the semantic information and to learn two influence components including the opinion influence of the content information and the social relation factors. In the experiments conducted on Twitter datasets, our model significantly outperforms other popular opinion formation models.
We propose a non-parametric Bayesian model for learning and weighting symbolically-defined constraints to populate a log-linear model. The model jointly infers a vector of binary constraint values for each candidate output and likely definitions for these constraints, combining observations of the output classes with a (potentially infinite) grammar over potential constraint definitions. We present results on a small morphophonological system, English regular plurals, as a test case. The inferred constraints, based on a grammar of articulatory features, perform as well as theoretically-defined constraints on both observed and novel forms of English regular plurals. The learned constraint values and definitions also closely resemble standard constraints defined within phonological theory.
This paper explores the role of tense information in Chinese causal analysis. Both tasks of causal type classification and causal directionality identification are experimented to show the significant improvement gained from tense features. To automatically extract the tense features, a Chinese tense predictor is proposed. Based on large amount of parallel data, our semi-supervised approach improves the dependency-based convolutional neural network (DCNN) models for Chinese tense labelling and thus the causal analysis.
Latent Dirichlet Allocation (LDA) and its variants have been widely used to discover latent topics in textual documents. However, some of topics generated by LDA may be noisy with irrelevant words scattering across these topics. We name this kind of words as topic-indiscriminate words, which tend to make topics more ambiguous and less interpretable by humans. In our work, we propose a new topic model named TWLDA, which assigns low weights to words with low topic discriminating power (ability). Our experimental results show that the proposed approach, which effectively reduces the number of topic-indiscriminate words in discovered topics, improves the effectiveness of LDA.
Deep learning techniques are increasingly popular in the textual entailment task, overcoming the fragility of traditional discrete models with hard alignments and logics. In particular, the recently proposed attention models (Rockt{\"a
We have released plWordNet 3.0, a very large wordnet for Polish. In addition to what is expected in wordnets {--} richly interrelated synsets {--} it contains sentiment and emotion annotations, a large set of multi-word expressions, and a mapping onto WordNet 3.1. Part of the release is enWordNet 1.0, a substantially enlarged copy of WordNet 3.1, with material added to allow for a more complete mapping. The paper discusses the design principles of plWordNet, its content, its statistical portrait, a comparison with similar resources, and a partial list of applications.
Multiword Expressions (MWEs) are crucial lexico-semantic units in any language. However, most work on MWEs has been focused on standard monolingual corpora. In this work, we examine MWE usage on Twitter - an inherently multilingual medium with an extremely short average text length that is often replete with grammatical errors. In this work we present a new graph based, language agnostic method for automatically extracting MWEs from tweets. We show how our method outperforms standard Association Measures. We also present a novel unsupervised evaluation technique to ascertain the accuracy of MWE extraction.
Event extraction is a difficult information extraction task. Li et al. (2014) explore the benefits of modeling event extraction and two related tasks, entity mention and relation extraction, jointly. This joint system achieves state-of-the-art performance in all tasks. However, as a system operating only at the sentence level, it misses valuable information from other parts of the document. In this paper, we present an incremental easy-first approach to make the global context of the entire document available to the intra-sentential, state-of-the-art event extractor. We show that our method robustly increases performance on two datasets, namely ACE 2005 and TAC 2015.
Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentence-level memory and word-level memory respectively. Then, $k$-max pooling is exploited following reasoning module on the sentence-level memory to sample the $k$ most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks.
Guided by multiple heuristics, a unified taxonomy of entities and categories is distilled from the Wikipedia category network. A comprehensive evaluation, based on the analysis of upward generalization paths, demonstrates that the taxonomy supports generalizations which are more than twice as accurate as the state of the art. The taxonomy is available at \url{http://headstaxonomy.com}.
Previous studies have highlighted the necessity for entity linking systems to capture the local entity-mention similarities and the global topical coherence. We introduce a novel framework based on convolutional neural networks and recurrent neural networks to simultaneously model the local and global features for entity linking. The proposed model benefits from the capacity of convolutional neural networks to induce the underlying representations for local contexts and the advantage of recurrent neural networks to adaptively compress variable length sequences of predictions for global constraints. Our evaluation on multiple datasets demonstrates the effectiveness of the model and yields the state-of-the-art performance on such datasets. In addition, we examine the entity linking systems on the domain adaptation setting that further demonstrates the cross-domain robustness of the proposed model.
Aspect extraction identifies relevant features from a textual description of an entity, e.g., a phone, and is typically targeted to product descriptions, reviews, and other short texts as an enabling task for, e.g., opinion mining and information retrieval. Current aspect extraction methods mostly focus on aspect terms and often neglect interesting modifiers of the term or embed them in the aspect term without proper distinction. Moreover, flat syntactic structures are often assumed, resulting in inaccurate extractions of complex aspects. This paper studies the problem of structured aspect extraction, a variant of traditional aspect extraction aiming at a fine-grained extraction of complex (i.e., hierarchical) aspects. We propose an unsupervised and scalable method for structured aspect extraction consisting of statistical noun phrase clustering, cPMI-based noun phrase segmentation, and hierarchical pattern induction. Our evaluation shows a substantial improvement over existing methods in terms of both quality and computational efficiency.
The conventional solution for handling sparsely labelled data is extensive feature engineering. This is time consuming and task and domain specific. We present a novel approach for learning embedded features that aims to alleviate this problem. Our approach jointly learns embeddings at different levels of granularity (word, sentence and document) along with the class labels. The intuition is that topic semantics represented by embeddings at multiple levels results in better classification. We evaluate this approach in unsupervised and semi-supervised settings on two sparsely labelled classification tasks, outperforming the handcrafted models and several embedding baselines.
We present an approach to mathematical information retrieval (MIR) that exploits a special kind of technical terminology, referred to as a mathematical type. In this paper, we present and evaluate a type detection mechanism and show its positive effect on the retrieval of research-level mathematics. Our best model, which performs query expansion with a type-aware embedding space, strongly outperforms standard IR models with state-of-the-art query expansion (vector space-based and language modelling-based), on a relatively new corpus of research-level queries.
Term co-occurrence in a sentence or paragraph is a powerful and often overlooked feature for text matching in document retrieval. In our experiments with matching email-style query messages to webpages, such term co-occurrence helped greatly to filter and rank documents, compared to matching document-size bags-of-words. The paper presents the results of the experiments as well as a text-matching model where the query shapes the vector space, a document is modelled by two or three vectors in this vector space, and the query-document similarity score depends on the length of the vectors and the relationships between them.
Answer selection is a core component in any question-answering systems. It aims to select correct answer sentences for a given question from a pool of candidate sentences. In recent years, many deep learning methods have been proposed and shown excellent results for this task. However, these methods typically require extensive parameter (and hyper-parameter) tuning, which give rise to efficiency issues for large-scale datasets, and potentially make them less portable across new datasets and domains (as re-tuning is usually required). In this paper, we propose an extremely efficient hybrid model (FastHybrid) that tackles the problem from both an accuracy and scalability point of view. FastHybrid is a light-weight model that requires little tuning and adaptation across different domains. It combines a fast deep model (which will be introduced in the method section) with an initial information retrieval model to effectively and efficiently handle answer selection. We introduce a new efficient attention mechanism in the hybrid model and demonstrate its effectiveness on several QA datasets. Experimental results show that although the hybrid uses no training data, its accuracy is often on-par with supervised deep learning techniques, while significantly reducing training and tuning costs across different domains.
A spatial information extraction system retrieves spatial entities and their relationships for geological searches and reasoning. Spatial information systems have been developed mainly for English text, e.g., through the SpaceEval competition. Some of the techniques are useful but not directly applicable to Korean text, because of linguistic differences and the lack of language resources. In this paper, we propose a Korean spatial entity extraction model and a spatial relation extraction model; the spatial entity extraction model uses word vectors to alleviate the over generation and the spatial relation extraction mod-el uses dependency parse labels to find the proper arguments in relations. Experiments with Korean text show that the two models are effective for spatial information extraction.
Recent trend in question answering (QA) systems focuses on using structured knowledge bases (KBs) to find answers. While these systems are able to provide more precise answers than information retrieval (IR) based QA systems, the natural incompleteness of KB inevitably limits the question scope that the system can answer. In this paper, we present a hybrid question answering (hybrid-QA) system which exploits both structured knowledge base and free text to answer a question. The main challenge is to recognize the meaning of a question using these two resources, i.e., structured KB and free text. To address this, we map relational phrases to KB predicates and textual relations simultaneously, and further develop an integer linear program (ILP) model to infer on these candidates and provide a globally optimal solution. Experiments on benchmark datasets show that our system can benefit from both structured KB and free text, outperforming the state-of-the-art systems.
Distributed word representation is an efficient method for capturing semantic and syntactic word relations. In this work, we introduce an extension to the continuous bag-of-words model for learning word representations efficiently by using implicit structure information. Instead of relying on a syntactic parser which might be noisy and slow to build, we compute weights representing probabilities of syntactic relations based on the Huffman softmax tree in an efficient heuristic. The constructed {``}implicit graphs{''} from these weights show that these weights contain useful implicit structure information. Extensive experiments performed on several word similarity and word analogy tasks show gains compared to the basic continuous bag-of-words model.
With the development and the advancement of social networks, forums, blogs and online sales, a growing number of Arabs are expressing their opinions on the web. In this paper, a scheme of Arabic sentiment classification, which evaluates and detects the sentiment polarity from Arabic reviews and Arabic social media, is studied. We investigated in several architectures to build a quality neural word embeddings using a 3.4 billion words corpus from a collected 10 billion words web-crawled corpus. Moreover, a convolutional neural network trained on top of pre-trained Arabic word embeddings is used for sentiment classification to evaluate the quality of these word embeddings. The simulation results show that the proposed scheme outperforms the existed methods on 4 out of 5 balanced and unbalanced datasets.
Sentiment analysis of short texts is challenging because of the limited contextual information they usually contain. In recent years, deep learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been applied to text sentiment analysis with comparatively remarkable results. In this paper, we describe a jointed CNN and RNN architecture, taking advantage of the coarse-grained local features generated by CNN and long-distance dependencies learned via RNN for sentiment analysis of short texts. Experimental results show an obvious improvement upon the state-of-the-art on three benchmark corpora, MR, SST1 and SST2, with 82.28{\%}, 51.50{\%} and 89.95{\%} accuracy, respectively.
Rumour stance classification, the task that determines if each tweet in a collection discussing a rumour is supporting, denying, questioning or simply commenting on the rumour, has been attracting substantial interest. Here we introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in Twitter. The conversation threads are formed by harvesting users{'} replies to one another, which results in a nested tree-like structure. Previous work addressing the stance classification task has treated each tweet as a separate unit. Here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers, Linear-Chain CRF and Tree CRF, each of which makes different assumptions about the conversational structure. We experiment with eight Twitter datasets, collected during breaking news, and show that exploiting the sequential structure of Twitter conversations achieves significant improvements over the non-sequential methods. Our work is the first to model Twitter conversations as a tree structure in this manner, introducing a novel way of tackling NLP tasks on Twitter conversations.
Sarcasm detection has been modeled as a binary document classification task, with rich features being defined manually over input documents. Traditional models employ discrete manual features to address the task, with much research effect being devoted to the design of effective feature templates. We investigate the use of neural network for tweet sarcasm detection, and compare the effects of the continuous automatic features with discrete manual features. In particular, we use a bi-directional gated recurrent neural network to capture syntactic and semantic information over tweets locally, and a pooling neural network to extract contextual features automatically from history tweets. Results show that neural features give improved accuracies for sarcasm detection, with different error distributions compared with discrete manual features.
The automated comparison of points of view between two politicians is a very challenging task, due not only to the lack of annotated resources, but also to the different dimensions participating to the definition of agreement and disagreement. In order to shed light on this complex task, we first carry out a pilot study to manually annotate the components involved in detecting agreement and disagreement. Then, based on these findings, we implement different features to capture them automatically via supervised classification. We do not focus on debates in dialogical form, but we rather consider sets of documents, in which politicians may express their position with respect to different topics in an implicit or explicit way, like during an electoral campaign. We create and make available three different datasets.
We address the task of targeted sentiment as a means of understanding the sentiment that students hold toward courses and instructors, as expressed by students in their comments. We introduce a new dataset consisting of student comments annotated for targeted sentiment and describe a system that can both identify the courses and instructors mentioned in student comments, as well as label the students{'} sentiment toward those entities. Through several comparative evaluations, we show that our system outperforms previous work on a similar task.
Sentiment analysis (SA) using code-mixed data from social media has several applications in opinion mining ranging from customer satisfaction to social campaign analysis in multilingual societies. Advances in this area are impeded by the lack of a suitable annotated dataset. We introduce a Hindi-English (Hi-En) code-mixed dataset for sentiment analysis and perform empirical analysis comparing the suitability and performance of various state-of-the-art SA methods in social media. In this paper, we introduce learning sub-word level representations in our LSTM (Subword-LSTM) architecture instead of character-level or word-level representations. This linguistic prior in our architecture enables us to learn the information about sentiment value of important morphemes. This also seems to work well in highly noisy text containing misspellings as shown in our experiments which is demonstrated in morpheme-level feature maps learned by our model. Also, we hypothesize that encoding this linguistic prior in the Subword-LSTM architecture leads to the superior performance. Our system attains accuracy 4-5{\%} greater than traditional approaches on our dataset, and also outperforms the available system for sentiment analysis in Hi-En code-mixed text by 18{\%}.
Aspect phrase grouping is an important task in aspect-level sentiment analysis. It is a challenging problem due to polysemy and context dependency. We propose an Attention-based Deep Distance Metric Learning (ADDML) method, by considering aspect phrase representation as well as context representation. First, leveraging the characteristics of the review text, we automatically generate aspect phrase sample pairs for distant supervision. Second, we feed word embeddings of aspect phrases and their contexts into an attention-based neural network to learn feature representation of contexts. Both aspect phrase embedding and context embedding are used to learn a deep feature subspace for measure the distances between aspect phrases for K-means clustering. Experiments on four review datasets show that the proposed method outperforms state-of-the-art strong baseline methods.
WebQuestions and SimpleQuestions are two benchmark data-sets commonly used in recent knowledge-based question answering (KBQA) work. Most questions in them are {`}simple{'} questions which can be answered based on a single relation in the knowledge base. Such data-sets lack the capability of evaluating KBQA systems on complicated questions. Motivated by this issue, we release a new data-set, namely ComplexQuestions, aiming to measure the quality of KBQA systems on {`}multi-constraint{'} questions which require multiple knowledge base relations to get the answer. Beside, we propose a novel systematic KBQA approach to solve multi-constraint questions. Compared to state-of-the-art methods, our approach not only obtains comparable results on the two existing benchmark data-sets, but also achieves significant improvements on the ComplexQuestions.
Community question answering (cQA) websites are focused on users who query questions onto an online forum, expecting for other users to provide them answers or suggestions. Unlike other social media, the length of the posted queries has no limits and queries tend to be multi-sentence elaborations combining context, actual questions, and irrelevant information. We approach the problem of question ranking: given a user{'}s new question, to retrieve those previously-posted questions which could be equivalent, or highly relevant. This could prevent the posting of nearly-duplicate questions and provide the user with instantaneous answers. For the first time in cQA, we address the selection of relevant text {---}both at sentence- and at constituent-level{---} for parse tree-based representations. Our supervised models for text selection boost the performance of a tree kernel-based machine learning model, allowing it to overtake the current state of the art on a recently released cQA evaluation framework.
This paper proposes a novel context-aware joint entity and word-level relation extraction approach through semantic composition of words, introducing a Table Filling Multi-Task Recurrent Neural Network (TF-MTRNN) model that reduces the entity recognition and relation classification tasks to a table-filling problem and models their interdependencies. The proposed neural network architecture is capable of modeling multiple relation instances without knowing the corresponding relation arguments in a sentence. The experimental results show that a simple approach of piggybacking candidate entities to model the label dependencies from relations to entities improves performance. We present state-of-the-art results with improvements of 2.0{\%} and 2.7{\%} for entity recognition and relation classification, respectively on CoNLL04 dataset.
Parallel sentence representations are important for bilingual and cross-lingual tasks in natural language processing. In this paper, we explore a bilingual autoencoder approach to model parallel sentences. We extract sentence-level global descriptors (e.g. min, max) from word embeddings, and construct two monolingual autoencoders over these descriptors on the source and target language. In order to tightly connect the two autoencoders with bilingual correspondences, we force them to share the same decoding parameters and minimize a corpus-level semantic distance between the two languages. Being optimized towards a joint objective function of reconstruction and semantic errors, our bilingual antoencoder is able to learn continuous-valued latent representations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines.
In this paper we combine two strands of machine translation (MT) research: automatic post-editing (APE) and multi-engine (system combination) MT. APE systems learn a target-language-side second stage MT system from the data produced by human corrected output of a first stage MT system, to improve the output of the first stage MT in what is essentially a sequential MT system combination architecture. At the same time, there is a rich research literature on parallel MT system combination where the same input is fed to multiple engines and the best output is selected or smaller sections of the outputs are combined to obtain improved translation output. In the paper we show that parallel system combination in the APE stage of a sequential MT-APE combination yields substantial translation improvements both measured in terms of automatic evaluation metrics as well as in terms of productivity improvements measured in a post-editing experiment. We also show that system combination on the level of APE alignments yields further improvements. Overall our APE system yields statistically significant improvement of 5.9{\%} relative BLEU over a strong baseline (English{--}Italian Google MT) and 21.76{\%} productivity increase in a human post-editing experiment with professional translators.
Research in statistical machine translation (SMT) is largely driven by formal translation tasks, while translating informal text is much more challenging. In this paper we focus on SMT for the informal genre of dialogues, which has rarely been addressed to date. Concretely, we investigate the effect of dialogue acts, speakers, gender, and text register on SMT quality when translating fictional dialogues. We first create and release a corpus of multilingual movie dialogues annotated with these four dialogue-specific aspects. When measuring translation performance for each of these variables, we find that BLEU fluctuations between their categories are often significantly larger than randomly expected. Following this finding, we hypothesize and show that SMT of fictional dialogues benefits from adaptation towards dialogue acts and registers. Finally, we find that male speakers are harder to translate and use more vulgar language than female speakers, and that vulgarity is often not preserved during translation.
The phrase table is considered to be the main bilingual resource for the phrase-based statistical machine translation (PBSMT) model. During translation, a source sentence is decomposed into several phrases. The best match of each source phrase is selected among several target-side counterparts within the phrase table, and processed by the decoder to generate a sentence-level translation. The best match is chosen according to several factors, including a set of bilingual features. PBSMT engines by default provide four probability scores in phrase tables which are considered as the main set of bilingual features. Our goal is to enrich that set of features, as a better feature set should yield better translations. We propose new scores generated by a Convolutional Neural Network (CNN) which indicate the semantic relatedness of phrase pairs. We evaluate our model in different experimental settings with different language pairs. We observe significant improvements when the proposed features are incorporated into the PBSMT pipeline.
We introduce a novel task Anecdote Recognition and Recommendation. An anecdote is a story with a point revealing account of an individual person. Recommending proper anecdotes can be used as evidence to support argumentative writing or as a clue for further reading. We represent an anecdote as a structured tuple {---} {\textless} person, story, implication {\textgreater}. Anecdote recognition runs on archived argumentative essays. We extract narratives containing events of a person as the anecdote story. More importantly, we uncover the anecdote implication, which reveals the meaning and topic of an anecdote. Our approach depends on discourse role identification. Discourse roles such as thesis, main ideas and support help us locate stories and their implications in essays. The experiments show that informative and interpretable anecdotes can be recognized. These anecdotes are used for anecdote recommendation. The anecdote recommender can recommend proper anecdotes in response to given topics. The anecdote implication contributes most for bridging user interested topics and relevant anecdotes.
Discourse parsing is a popular technique widely used in text understanding, sentiment analysis and other NLP tasks. However, for most discourse parsers, the performance varies significantly across different discourse relations. In this paper, we first validate the underfitting hypothesis, i.e., the less frequent a relation is in the training data, the poorer the performance on that relation. We then explore how to increase the number of positive training instances, without resorting to manually creating additional labeled data. We propose a training data enrichment framework that relies on co-training of two different discourse parsers on unlabeled documents. Importantly, we show that co-training alone is not sufficient. The framework requires a filtering step to ensure that only {``}good quality{''} unlabeled documents can be used for enrichment and re-training. We propose and evaluate two ways to perform the filtering. The first is to use an agreement score between the two parsers. The second is to use only the confidence score of the faster parser. Our empirical results show that agreement score can help to boost the performance on infrequent relations, and that the confidence score is a viable approximation of the agreement score for infrequent relations.
Penn Discourse Treebank (PDTB)-style annotation focuses on labeling local discourse relations between text spans and typically ignores larger discourse contexts. In this paper we propose two approaches to infer discourse relations in a paragraph-level context from annotated PDTB labels. We investigate the utility of inferring such discourse information using the task of revision classification. Experimental results demonstrate that the inferred information can significantly improve classification performance compared to baselines, not only when PDTB annotation comes from humans but also from automatic parsers.
We examine the potential of recurrent neural networks for handling pragmatic inferences involving complex contextual cues for the task of article usage prediction. We train and compare several variants of Long Short-Term Memory (LSTM) networks with an attention mechanism. Our model outperforms a previous state-of-the-art system, achieving up to 96.63{\%} accuracy on the WSJ/PTB corpus. In addition, we perform a series of analyses to understand the impact of various model choices. We find that the gain in performance can be attributed to the ability of LSTMs to pick up on contextual cues, both local and further away in distance, and that the model is able to solve cases involving reasoning about coreference and synonymy. We also show how the attention mechanism contributes to the interpretability of the model{'}s effectiveness.
In the literature, various supervised learning approaches have been adopted to address the task of reader emotion classification. However, the classification performance greatly suffers when the size of the labeled data is limited. In this paper, we propose a two-view label propagation approach to semi-supervised reader emotion classification by exploiting two views, namely source text and response text in a label propagation algorithm. Specifically, our approach depends on two word-document bipartite graphs to model the relationship among the samples in the two views respectively. Besides, the two bipartite graphs are integrated by linking each source text sample with its corresponding response text sample via a length-sensitive transition probability. In this way, our two-view label propagation approach to semi-supervised reader emotion classification largely alleviates the reliance on the strong sufficiency and independence assumptions of the two views, as required in co-training. Empirical evaluation demonstrates the effectiveness of our two-view label propagation approach to semi-supervised reader emotion classification.
Classifying the stance expressed in online microblogging social media is an emerging problem in opinion mining. We propose a probabilistic approach to stance classification in tweets, which models stance, target of stance, and sentiment of tweet, jointly. Instead of simply conjoining the sentiment or target variables as extra variables to the feature space, we use a novel formulation to incorporate three-way interactions among sentiment-stance-input variables and three-way interactions among target-stance-input variables. The proposed specification intuitively aims to discriminate sentiment features from target features for stance classification. In addition, regularizing a single stance classifier, which handles all targets, acts as a soft weight-sharing among them. We demonstrate that discriminative training of this model achieves the state-of-the-art results in supervised stance classification, and its generative training obtains competitive results in the weakly supervised setting.
An important difference between traditional AI systems and human intelligence is the human ability to harness commonsense knowledge gleaned from a lifetime of learning and experience to make informed decisions. This allows humans to adapt easily to novel situations where AI fails catastrophically due to a lack of situation-specific rules and generalization capabilities. Commonsense knowledge also provides background information that enables humans to successfully operate in social situations where such knowledge is typically assumed. Since commonsense consists of information that humans take for granted, gathering it is an extremely difficult task. Previous versions of SenticNet were focused on collecting this kind of knowledge for sentiment analysis but they were heavily limited by their inability to generalize. SenticNet 4 overcomes such limitations by leveraging on conceptual primitives automatically generated by means of hierarchical clustering and dimensionality reduction.
Existing work learning distributed representations of knowledge base entities has largely failed to incorporate rich categorical structure, and is unable to induce category representations. We propose a new framework that embeds entities and categories jointly into a semantic space, by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. Our framework enables to compute meaningful semantic relatedness between entities and categories in a principled way, and can handle both single-word and multiple-word concepts. Our method shows significant improvement on the tasks of concept categorization and dataless hierarchical classification.
Topic modeling and word embedding are two important techniques for deriving latent semantics from data. General-purpose topic models typically work in coarse granularity by capturing word co-occurrence at the document/sentence level. In contrast, word embedding models usually work in much finer granularity by modeling word co-occurrence within small sliding windows. With the aim of deriving latent semantics by considering word co-occurrence at different levels of granularity, we propose a novel model named \textit{Latent Topic Embedding} (LTE), which seamlessly integrates topic generation and embedding learning in one unified framework. We further propose an efficient Monte Carlo EM algorithm to estimate the parameters of interest. By retaining the individual advantages of topic modeling and word embedding, LTE results in better latent topics and word embedding. Extensive experiments verify the superiority of LTE over the state-of-the-arts.
Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvements in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings.
Semantic classification of words using distributional features is usually based on the semantic similarity of words. We show on two different datasets that a trained classifier using the distributional features directly gives better results. We use Support Vector Machines (SVM) and Multi-relational Matrix Factorization (MRMF) to train classifiers. Both give similar results. However, MRMF, that was not used for semantic classification with distributional features before, can easily be extended with more matrices containing more information from different sources on the same problem. We demonstrate the effectiveness of the novel approach by including information from WordNet. Thus we show, that MRMF provides an interesting approach for building semantic classifiers that (1) gives better results than unsupervised approaches based on vector similarity, (2) gives similar results as other supervised methods and (3) can naturally be extended with other sources of information in order to improve the results.
Prepositions are very common and very ambiguous, and understanding their sense is critical for understanding the meaning of the sentence. Supervised corpora for the preposition-sense disambiguation task are small, suggesting a semi-supervised approach to the task. We show that signals from unannotated multilingual data can be used to improve supervised preposition-sense disambiguation. Our approach pre-trains an LSTM encoder for predicting the translation of a preposition, and then incorporates the pre-trained encoder as a component in a supervised classification system, and fine-tunes it for the task. The multilingual signals consistently improve results on two preposition-sense datasets.
Recognising and understanding irony is crucial for the improvement natural language processing tasks including sentiment analysis. In this study, we describe the construction of an English Twitter corpus and its annotation for irony based on a newly developed fine-grained annotation scheme. We also explore the feasibility of automatic irony recognition by exploiting a varied set of features including lexical, syntactic, sentiment and semantic (Word2Vec) information. Experiments on a held-out test set show that our irony classifier benefits from this combined information, yielding an F1-score of 67.66{\%}. When explicit hashtag information like {\#}irony is included in the data, the system even obtains an F1-score of 92.77{\%}. A qualitative analysis of the output reveals that recognising irony that results from a polarity clash appears to be (much) more feasible than recognising other forms of ironic utterances (e.g., descriptions of situational irony).
When processing arguments in online user interactive discourse, it is often necessary to determine their bases of support. In this paper, we describe a supervised approach, based on deep neural networks, for classifying the claims made in online arguments. We conduct experiments using convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) on two claim data sets compiled from online user comments. Using different types of distributional word embeddings, but without incorporating any rich, expensive set of features, we achieve a significant improvement over the state of the art for one data set (which categorizes arguments as factual vs. emotional), and performance comparable to the state of the art on the other data set (which categorizes propositions according to their verifiability). Our approach has the advantages of using a generalized, simple, and effective methodology that works for claim categorization on different data sets and tasks.
Some expressions can be ambiguous between idiomatic and literal interpretations depending on the context they occur in, e.g., {`}sales hit the roof{'} vs. {`}hit the roof of the car{'}. We present a novel method of classifying whether a given instance is literal or idiomatic, focusing on verb-noun constructions. We report state-of-the-art results on this task using an approach based on the hypothesis that the distributions of the contexts of the idiomatic phrases will be different from the contexts of the literal usages. We measure contexts by using projections of the words into vector space. For comparison, we implement Fazly et al. (2009){'}s, Sporleder and Li (2009){'}s, and Li and Sporleder (2010b){'}s methods and apply them to our data. We provide experimental results validating the proposed techniques.
Several tasks in argumentation mining and debating, question-answering, and natural language inference involve classifying a sequence in the context of another sequence (referred as bi-sequence classification). For several single sequence classification tasks, the current state-of-the-art approaches are based on recurrent and convolutional neural networks. On the other hand, for bi-sequence classification problems, there is not much understanding as to the best deep learning architecture. In this paper, we attempt to get an understanding of this category of problems by extensive empirical evaluation of 19 different deep learning architectures (specifically on different ways of handling context) for various problems originating in natural language processing like debating, textual entailment and question-answering. Following the empirical evaluation, we offer our insights and conclusions regarding the architectures we have considered. We also establish the first deep learning baselines for three argumentation mining tasks.
The recent proliferation of smart devices necessitates methods to learn small-sized models. This paper demonstrates that if there are $m$ features in total but only $n = o(\sqrt{m})$ features are required to distinguish examples, with $\Omega(\log m)$ training examples and reasonable settings, it is possible to obtain a good model in a \textit{succinct} representation using $n \log_2 \frac{m}{n} + o(m)$ bits, by using a pipeline of existing compression methods: L1-regularized logistic regression, feature hashing, Elias{--}Fano indices, and randomized quantization. An experiment shows that a noun phrase chunking task for which an existing library requires 27 megabytes can be compressed to less than 13 \textit{kilo}bytes without notable loss of accuracy.
We assess the reliability and accuracy of (neural) word embeddings for both modern and historical English and German. Our research provides deeper insights into the empirically justified choice of optimal training methods and parameters. The overall low reliability we observe, nevertheless, casts doubt on the suitability of word neighborhoods in embedding spaces as a basis for qualitative conclusions on synchronic and diachronic lexico-semantic matters, an issue currently high up in the agenda of Digital Humanities.
In this paper, we outline an approach to build graph-based reverse dictionaries using word definitions. A reverse dictionary takes a phrase as an input and outputs a list of words semantically similar to that phrase. It is a solution to the Tip-of-the-Tongue problem. We use a distance-based similarity measure, computed on a graph, to assess the similarity between a word and the input phrase. We compare the performance of our approach with the Onelook Reverse Dictionary and a distributional semantics method based on word2vec, and show that our approach is much better than the distributional semantics method, and as good as Onelook, on a 3k lexicon. This simple approach sets a new performance baseline for reverse dictionaries.
Human concept representations are often grounded with visual information, yet some aspects of meaning cannot be visually represented or are better described with language. Thus, vision and language provide complementary information that, properly combined, can potentially yield more complete concept representations. Recently, state-of-the-art distributional semantic models and convolutional neural networks have achieved great success in representing linguistic and visual knowledge respectively. In this paper, we compare both, visual and linguistic representations in their ability to capture different types of fine-grain semantic knowledge{---}or attributes{---}of concepts. Humans often describe objects using attributes, that is, properties such as shape, color or functionality, which often transcend the linguistic and visual modalities. In our setting, we evaluate how well attributes can be predicted by using the unimodal representations as inputs. We are interested in first, finding out whether attributes are generally better captured by either the vision or by the language modality; and second, if none of them is clearly superior (as we hypothesize), what type of attributes or semantic knowledge are better encoded from each modality. Ultimately, our study sheds light on the potential of combining visual and textual representations.
Temporal relation classification is a challenging task, especially when there are no explicit markers to characterise the relation between temporal entities. This occurs frequently in inter-sentential relations, whose entities are not connected via direct syntactic relations making classification even more difficult. In these cases, resorting to features that focus on the semantic content of the event words may be very beneficial for inferring implicit relations. Specifically, while morpho-syntactic and context features are considered sufficient for classifying event-timex pairs, we believe that exploiting distributional semantic information about event words can benefit supervised classification of other types of pairs. In this work, we assess the impact of using word embeddings as features for event words in classifying temporal relations of event-event pairs and event-DCT (document creation time) pairs.
This paper proposes a novel problem setting of selectional preference (SP) between a predicate and its arguments, called as context-sensitive SP (CSP). CSP models the narrative consistency between the predicate and preceding contexts of its arguments, in addition to the conventional SP based on semantic types. Furthermore, we present a novel CSP model that extends the neural SP model (Van de Cruys, 2014) to incorporate contextual information into the distributed representations of arguments. Experimental results demonstrate that the proposed CSP model successfully learns CSP and outperforms the conventional SP model in coreference cluster ranking.
The paper presents an iterative bidirectional clustering of adjectives and nouns based on a co-occurrence matrix. The clustering method combines a Vector Space Models (VSM) and the results of a Latent Dirichlet Allocation (LDA), whose results are merged in each iterative step. The aim is to derive a clustering of German adjectives that reflects latent semantic classes of adjectives, and that can be used to induce frame-based representations of nouns in a later step. We are able to show that the method induces meaningful groups of adjectives, and that it outperforms a baseline k-means algorithm.
According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing word-based measures, such as Weed{'}s and Clarke{'}s, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures.
In this paper, we propose utilising eye gaze information for estimating parameters of a Japanese predicate argument structure (PAS) analysis model. We employ not only linguistic information in the text, but also the information of annotator eye gaze during their annotation process. We hypothesise that annotator{'}s frequent looks at certain candidates imply their plausibility of being the argument of the predicate. Based on this hypothesis, we consider annotator eye gaze for estimating the model parameters of the PAS analysis. The evaluation experiment showed that introducing eye gaze information increased the accuracy of the PAS analysis by 0.05 compared with the conventional methods.
Recognizing Textual Entailment (RTE) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate deep neural network methods for the RTE task. Previous neural network based methods usually try to encode the two sentences (premise and hypothesis) and send them together into a multi-layer perceptron to get their entailment type, or use LSTM-RNN to link two sentences together while using attention mechanic to enhance the model{'}s ability. In this paper, we propose to use the re-read mechanic, which means to read the premise again and again while reading the hypothesis. After read the premise again, the model can get a better understanding of the premise, which can also affect the understanding of the hypothesis. On the contrary, a better understanding of the hypothesis can also affect the understanding of the premise. With the alternative re-read process, the model can {``}think{''} of a better decision of entailment type. We designed a new LSTM unit called re-read LSTM (rLSTM) to implement this {``}thinking{''} process. Experiments show that we achieve results better than current state-of-the-art equivalents.
Sentence intersection captures the semantic overlap of two texts, generalizing over paradigms such as textual entailment and semantic text similarity. Despite its modeling power, it has received little attention because it is difficult for non-experts to annotate. We analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. We leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks, facilitating the construction of a high quality dataset via crowdsourcing. We implement this approach and provide an annotated dataset of 1,764 sentence intersections.
Inference rule discovery aims to identify entailment relations between predicates, e.g., {`}X acquire Y {--}{\textgreater} X purchase Y{'} and {`}X is author of Y {--}{\textgreater} X write Y{'}. Traditional methods dis-cover inference rules by computing distributional similarities between predicates, with each predicate is represented as one or more feature vectors of its instantiations. These methods, however, have two main drawbacks. Firstly, these methods are mostly context-insensitive, cannot accurately measure the similarity between two predicates in a specific context. Secondly, traditional methods usually model predicates independently, ignore the rich inter-dependencies between predicates. To address the above two issues, this pa-per proposes a graph-based method, which can discover inference rules by effectively modelling and exploiting both the context and the inter-dependencies between predicates. Specifically, we propose a graph-based representation{---}Predicate Graph, which can capture the semantic relevance between predicates using both the predicate-feature co-occurrence statistics and the inter-dependencies between predicates. Based on the predicate graph, we propose a context-sensitive random walk algorithm, which can learn con-text-specific predicate representations by distinguishing context-relevant information from context-irrelevant information. Experimental results show that our method significantly outperforms traditional inference rule discovery methods.
We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks.
In this paper, we propose a novel neural approach for paraphrase generation. Conventional paraphrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers, and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based, and bi-directional LSTM models on BLEU, METEOR, TER, and an embedding-based sentence similarity metric.
Knowledge base (KB) such as Freebase plays an important role for many natural language processing tasks. English knowledge base is obviously larger and of higher quality than low resource language like Chinese. To expand Chinese KB by leveraging English KB resources, an effective way is to translate English KB (source) into Chinese (target). In this direction, two major challenges are to model triple semantics and to build a robust KB translator. We address these challenges by presenting a neural network approach, which learns continuous triple representation with a gated neural network. Accordingly, source triples and target triples are mapped in the same semantic vector space. We build a new dataset for English-Chinese KB translation from Freebase, and compare with several baselines on it. Experimental results show that the proposed method improves translation accuracy compared with baseline methods. We show that adaptive composition model improves standard solution such as neural tensor network in terms of translation accuracy.
Keyphrase annotation is the task of identifying textual units that represent the main content of a document. Keyphrase annotation is either carried out by extracting the most important phrases from a document, keyphrase extraction, or by assigning entries from a controlled domain-specific vocabulary, keyphrase assignment. Assignment methods are generally more reliable. They provide better-formed keyphrases, as well as keyphrases that do not occur in the document. But they are often silent on the contrary of extraction methods that do not depend on manually built resources. This paper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods.
QA systems have been making steady advances in the challenging elementary science exam domain. In this work, we develop an explanation-based analysis of knowledge and inference requirements, which supports a fine-grained characterization of the challenges. In particular, we model the requirements based on appropriate sources of evidence to be used for the QA task. We create requirements by first identifying suitable sentences in a knowledge base that support the correct answer, then use these to build explanations, filling in any necessary missing information. These explanations are used to create a fine-grained categorization of the requirements. Using these requirements, we compare a retrieval and an inference solver on 212 questions. The analysis validates the gains of the inference solver, demonstrating that it answers more questions requiring complex inference, while also providing insights into the relative strengths of the solvers and knowledge sources. We release the annotated questions and explanations as a resource with broad utility for science exam QA, including determining knowledge base construction targets, as well as supporting information aggregation in automated inference.
During the 2016 United States presidential election, politicians have increasingly used Twitter to express their beliefs, stances on current political issues, and reactions concerning national and international events. Given the limited length of tweets and the scrutiny politicians face for what they choose or neglect to say, they must craft and time their tweets carefully. The content and delivery of these tweets is therefore highly indicative of a politician{'}s stances. We present a weakly supervised method for extracting how issues are framed and temporal activity patterns on Twitter for popular politicians and issues of the 2016 election. These behavioral components are combined into a global model which collectively infers the most likely stance and agreement patterns among politicians, with respective accuracies of 86.44{\%} and 84.6{\%} on average.
Sentiment classification becomes more and more important with the rapid growth of user generated content. However, sentiment classification task usually comes with two challenges: first, sentiment classification is highly domain-dependent and training sentiment classifier for every domain is inefficient and often impractical; second, since the quantity of labeled data is important for assessing the quality of classifier, it is hard to evaluate classifiers when labeled data is limited for certain domains. To address the challenges mentioned above, we focus on learning high-level features that are able to generalize across domains, so a global classifier can benefit with a simple combination of documents from multiple domains. In this paper, the proposed model incorporates both sentiment polarity and unlabeled data from multiple domains and learns new feature representations. Our model doesn{'}t require labels from every domain, which means the learned feature representation can be generalized for sentiment domain adaptation. In addition, the learned feature representation can be used as classifier since our model defines the meaning of feature value and arranges high-level features in a prefixed order, so it is not necessary to train another classifier on top of the new features. Empirical evaluations demonstrate our model outperforms baselines and yields competitive results to other state-of-the-art works on benchmark datasets.
This paper presents classification results for the analysis of sentiment in political news articles. The domain of political news is particularly challenging, as journalists are presumably objective, whilst at the same time opinions can be subtly expressed. To deal with this challenge, in this work we conduct a two-step classification model, distinguishing first subjective and second positive and negative sentiment texts. More specifically, we propose a shallow machine learning approach where only minimal features are needed to train the classifier, including sentiment-bearing Co-Occurring Terms (COTs) and negation words. This approach yields close to state-of-the-art results. Contrary to results in other domains, the use of negations as features does not have a positive impact in the evaluation results. This method is particularly suited for languages that suffer from a lack of resources, such as sentiment lexicons or parsers, and for those systems that need to function in real-time.
Probabilistic models are a useful means for analyzing large text corpora. Integrating such models with human interaction enables many new use cases. However, adding human interaction to probabilistic models requires inference algorithms which are both fast and accurate. We explore the use of Iterated Conditional Modes as a fast alternative to Gibbs sampling or variational EM. We demonstrate superior performance both in run time and model quality on three different models of text including a DP Mixture of Multinomials for web search result clustering, the Interactive Topic Model, and M OM R ESP , a multinomial crowdsourcing model.
In this paper we address a new problem of predicting affect and well-being scales in a real-world setting of heterogeneous, longitudinal and non-synchronous textual as well as non-linguistic data that can be harvested from on-line media and mobile phones. We describe the method for collecting the heterogeneous longitudinal data, how features are extracted to address missing information and differences in temporal alignment, and how the latter are combined to yield promising predictions of affect and well-being on the basis of widely used psychological scales. We achieve a coefficient of determination ($R^2$) of 0.71-0.76 and a correlation coefficient of 0.68-0.87 which is higher than the state-of-the art in equivalent multi-modal tasks for affect.
Microblogging services allow users to create hashtags to categorize their posts. In recent years, the task of recommending hashtags for microblogs has been given increasing attention. However, most of existing methods depend on hand-crafted features. Motivated by the successful use of long short-term memory (LSTM) for many natural language processing tasks, in this paper, we adopt LSTM to learn the representation of a microblog post. Observing that hashtags indicate the primary topics of microblog posts, we propose a novel attention-based LSTM model which incorporates topic modeling into the LSTM architecture through an attention mechanism. We evaluate our model using a large real-world dataset. Experimental results show that our model significantly outperforms various competitive baseline methods. Furthermore, the incorporation of topical attention mechanism gives more than 7.4{\%} improvement in F1 score compared with standard LSTM method.
We present a novel way for designing complex joint inference and learning models using Saul (Kordjamshidi et al., 2015), a recently-introduced declarative learning-based programming language (DeLBP). We enrich Saul with components that are necessary for a broad range of learning based Natural Language Processing tasks at various levels of granularity. We illustrate these advances using three different, well-known NLP problems, and show how these generic learning and inference modules can directly exploit Saul{'}s graph-based data representation. These properties allow the programmer to easily switch between different model formulations and configurations, and consider various kinds of dependencies and correlations among variables of interest with minimal programming effort. We argue that Saul provides an extremely useful paradigm both for the design of advanced NLP systems and for supporting advanced research in NLP.
This article presents the results we obtained on a complex annotation task (that of dependency syntax) using a specifically designed Game with a Purpose, ZombiLingo. We show that with suitable mechanisms (decomposition of the task, training of the players and regular control of the annotation quality during the game), it is possible to obtain annotations whose quality is significantly higher than that obtainable with a parser, provided that enough players participate. The source code of the game and the resulting annotated corpora (for French) are freely available.
In this paper, we provide a solution to multilingual sentiment classification using deep learning. Given input text in a language, we use word translation into English and then the embeddings of these English words to train a classifier. This projection into the English space plus word embeddings gives a simple and uniform framework for multilingual sentiment analysis. A novel idea is augmentation of the training data with polar words, appearing in these sentences, along with their polarities. This approach leads to a performance gain of 7-10{\%} over traditional classifiers on many languages, irrespective of text genre, despite the scarcity of resources in most languages.
This article proposes a novel character-aware neural machine translation (NMT) model that views the input sequences as sequences of characters rather than words. On the use of row convolution (Amodei et al., 2015), the encoder of the proposed model composes word-level information from the input sequences of characters automatically. Since our model doesn{'}t rely on the boundaries between each word (as the whitespace boundaries in English), it is also applied to languages without explicit word segmentations (like Chinese). Experimental results on Chinese-English translation tasks show that the proposed character-aware NMT model can achieve comparable translation performance with the traditional word based NMT models. Despite the target side is still word based, the proposed model is able to generate much less unknown words.
Estimating similarities at different levels of linguistic units, such as words, sub-phrases and phrases, is helpful for measuring semantic similarity of an entire bilingual phrase. In this paper, we propose a convolution-enhanced bilingual recursive neural network (ConvBRNN), which not only exploits word alignments to guide the generation of phrase structures but also integrates multiple-level information of the generated phrase structures into bilingual semantic modeling. In order to accurately learn the semantic hierarchy of a bilingual phrase, we develop a recursive neural network to constrain the learned bilingual phrase structures to be consistent with word alignments. Upon the generated source and target phrase structures, we stack a convolutional neural network to integrate vector representations of linguistic units on the structures into bilingual phrase embeddings. After that, we fully incorporate information of different linguistic units into a bilinear semantic similarity model. We introduce two max-margin losses to train the ConvBRNN model: one for the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation.
In neural machine translation, the attention mechanism facilitates the translation process by producing a soft alignment between the source sentence and the target sentence. However, without dedicated distortion and fertility models seen in traditional SMT systems, the learned alignment may not be accurate, which can lead to low translation quality. In this paper, we propose two novel models to improve attention-based neural machine translation. We propose a recurrent attention mechanism as an implicit distortion model, and a fertility conditioned decoder as an implicit fertility model. We conduct experiments on large-scale Chinese{--}English translation tasks. The results show that our models significantly improve both the alignment and translation quality compared to the original attention mechanism and several other variations.
The attention mechanism is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in alignment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.
Evaluating the quality of output from language processing systems such as machine translation or speech recognition is an essential step in ensuring that they are sufficient for practical use. However, depending on the practical requirements, evaluation approaches can differ strongly. Often, reference-based evaluation measures (such as BLEU or WER) are appealing because they are cheap and allow rapid quantitative comparison. On the other hand, practitioners often focus on manual evaluation because they must deal with frequently changing domains and quality standards requested by customers, for which reference-based evaluation is insufficient or not possible due to missing in-domain reference data (Harris et al., 2016). In this paper, we attempt to bridge this gap by proposing a framework for lightly supervised quality estimation. We collect manually annotated scores for a small number of segments in a test corpus or document, and combine them with automatically predicted quality scores for the remaining segments to predict an overall quality estimate. An evaluation shows that our framework estimates quality more reliably than using fully automatic quality estimation approaches, while keeping annotation effort low by not requiring full references to be available for the particular domain.
Selecting appropriate translations for source words with multiple meanings still remains a challenge for statistical machine translation (SMT). One reason for this is that most SMT systems are not good at detecting the proper sense for a polysemic word when it appears in different contexts. In this paper, we adopt a supersense tagging method to annotate source words with coarse-grained ontological concepts. In order to enable the system to choose an appropriate translation for a word or phrase according to the annotated supersense of the word or phrase, we propose two translation models with supersense knowledge: a maximum entropy based model and a supersense embedding model. The effectiveness of our proposed models is validated on a large-scale English-to-Spanish translation task. Results indicate that our method can significantly improve translation quality via correctly conveying the meaning of the source language to the target language.
Human-targeted metrics provide a compromise between human evaluation of machine translation, where high inter-annotator agreement is difficult to achieve, and fully automatic metrics, such as BLEU or TER, that lack the validity of human assessment. Human-targeted translation edit rate (HTER) is by far the most widely employed human-targeted metric in machine translation, commonly employed, for example, as a gold standard in evaluation of quality estimation. Original experiments justifying the design of HTER, as opposed to other possible formulations, were limited to a small sample of translations and a single language pair, however, and this motivates our re-evaluation of a range of human-targeted metrics on a substantially larger scale. Results show significantly stronger correlation with human judgment for HBLEU over HTER for two of the nine language pairs we include and no significant difference between correlations achieved by HTER and HBLEU for the remaining language pairs. Finally, we evaluate a range of quality estimation systems employing HTER and direct assessment (DA) of translation adequacy as gold labels, resulting in a divergence in system rankings, and propose employment of DA for future quality estimation evaluations.
Although more additional corpora are now available for Statistical Machine Translation (SMT), only the ones which belong to the same or similar domains of the original corpus can indeed enhance SMT performance directly. A series of SMT adaptation methods have been proposed to select these similar-domain data, and most of them focus on sentence selection. In comparison, phrase is a smaller and more fine grained unit for data selection, therefore we propose a straightforward and efficient connecting phrase based adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram adaptation. The proposed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performances are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods).
We present a new Bayesian HMM word alignment model for statistical machine translation. The model is a mixture of an alignment model and a language model. The alignment component is a Bayesian extension of the standard HMM. The language model component is responsible for the generation of words needed for source fluency reasons from source language context. This allows for untranslatable source words to remain unaligned and at the same time avoids the introduction of artificial NULL words which introduces unusually long alignment jumps. Existing Bayesian word alignment models are unpractically slow because they consider each target position when resampling a given alignment link. The sampling complexity therefore grows linearly in the target sentence length. In order to make our model useful in practice, we devise an auxiliary variable Gibbs sampler that allows us to resample alignment links in constant time independently of the target sentence length. This leads to considerable speed improvements. Experimental results show that our model performs as well as existing word alignment toolkits in terms of resulting BLEU score.
We present an approach for learning to translate by exploiting cross-lingual link structure in multilingual document collections. We propose a new learning objective based on structured ramp loss, which learns from graded relevance, explicitly including negative relevance information. Our results on English German translation of Wikipedia entries show small, but significant, improvements of our method over an unadapted baseline, even when only a weak relevance signal is used. We also compare our method to monolingual language model adaptation and automatic pseudo-parallel data extraction and find small improvements even over these strong baselines.
In this paper we explore the novel idea of building a single universal reordering model from English to a large number of target languages. To build this model we exploit typological features of word order for a large number of target languages together with source (English) syntactic features and we train this model on a single combined parallel corpus representing all (22) involved language pairs. We contribute experimental evidence for the usefulness of linguistically defined typological features for building such a model. When the universal reordering model is used for preordering followed by monotone translation (no reordering inside the decoder), our experiments show that this pipeline gives comparable or improved translation performance with a phrase-based baseline for a large number of language pairs (12 out of 22) from diverse language families.
We present a novel fusion model for domain adaptation in Statistical Machine Translation. Our model is based on the joint source-target neural network Devlin et al., 2014, and is learned by fusing in- and out-domain models. The adaptation is performed by backpropagating errors from the output layer to the word embedding layer of each model, subsequently adjusting parameters of the composite model towards the in-domain data. On the standard tasks of translating English-to-German and Arabic-to-English TED talks, we observed average improvements of +0.9 and +0.7 BLEU points, respectively over a competition grade phrase-based system. We also demonstrate improvements over existing adaptation methods.
Being able to induce word translations from non-parallel data is often a prerequisite for cross-lingual processing in resource-scarce languages and domains. Previous endeavors typically simplify this task by imposing the one-to-one translation assumption, which is too strong to hold for natural languages. We remove this constraint by introducing the Earth Mover{'}s Distance into the training of bilingual word embeddings. In this way, we take advantage of its capability to handle multiple alternative word translations in a natural form of regularization. Our approach shows significant and consistent improvements across four language pairs. We also demonstrate that our approach is particularly preferable in resource-scarce settings as it only requires a minimal seed lexicon.
Traditional machine translation systems often require heavy feature engineering and the combination of multiple techniques for solving different subproblems. In recent years, several end-to-end learning architectures based on recurrent neural networks have been proposed. Unlike traditional systems, Neural Machine Translation (NMT) systems learn the parameters of the model and require only minimal preprocessing. Memory and time constraints allow to take only a fixed number of words into account, which leads to the out-of-vocabulary (OOV) problem. In this work, we analyze why the OOV problem arises and why it is considered a serious problem in German. We study the effectiveness of compound word splitters for alleviating the OOV problem, resulting in a 2.5+ BLEU points improvement over a baseline on the WMT{'}14 German-to-English translation task. For English-to-German translation, we use target-side compound splitting through a special syntax during training that allows the model to merge compound words and gain 0.2 BLEU points.
We address the problem of inducing word alignment for language pairs by developing an unsupervised model with the capability of getting applied to other generative alignment models. We approach the task by: i)proposing a new alignment model based on the IBM alignment model 1 that uses vector representation of words, and ii)examining the use of similar source words to overcome the problem of rare source words and improving the alignments. We apply our method to English-French corpora and run the experiments with different sizes of sentence pairs. Our results show competitive performance against the baseline and in some cases improve the results up to 6.9{\%} in terms of precision.
Measuring the information content of news text is useful for decision makers in their investments since news information can influence the intrinsic values of companies. We propose a model to automatically measure the information content given news text, trained using news and corresponding cumulative abnormal returns of listed companies. Existing methods in finance literature exploit sentiment signal features, which are limited by not considering factors such as events. We address this issue by leveraging deep neural models to extract rich semantic features from news text. In particular, a novel tree-structured LSTM is used to find target-specific representations of news text given syntax structures. Empirical results show that the neural models can outperform sentiment-based models, demonstrating the effectiveness of recent NLP technology advances for computational finance.
Truly effective and practical educational systems will only be achievable when they have the ability to fully recognize deep relationships between a learner{'}s interpretation of a subject and the desired conceptual understanding. In this paper, we take important steps in this direction by introducing a new representation of sentences {--} Minimal Meaningful Propositions (MMPs), which will allow us to significantly improve the mapping between a learner{'}s answer and the ideal response. Using this technique, we make significant progress towards highly scalable and domain independent educational systems, that will be able to operate without human intervention. Even though this is a new task, we show very good results both for the extraction of MMPs and for classification with respect to their importance.
News portals, such as Yahoo News or Google News, collect large amounts of documents from a variety of sources on a daily basis. Only a small portion of these documents can be selected and displayed on the homepage. Thus, there is a strong preference for major, recent events. In this work, we propose a scalable and accurate First Story Detection (FSD) pipeline that identifies fresh news. In comparison to other FSD systems, our method relies on relation extraction methods exploiting entities and their relations. We evaluate our pipeline using two distinct datasets from Yahoo News and Google News. Experimental results demonstrate that our method improves over the state-of-the-art systems on both datasets with constant space and time requirements.
In this paper we explore to what extent the difficulty of listening items in an English language proficiency test can be predicted by the textual properties of the prompt. We show that a system based on multiple text complexity features can predict item difficulty for several different item types and for some items achieves higher accuracy than human estimates of item difficulty.
Event coreference resolution is a challenging problem since it relies on several components of the information extraction pipeline that typically yield noisy outputs. We hypothesize that exploiting the inter-dependencies between these components can significantly improve the performance of an event coreference resolver, and subsequently propose a novel joint inference based event coreference resolver using Markov Logic Networks (MLNs). However, the rich features that are important for this task are typically very hard to explicitly encode as MLN formulas since they significantly increase the size of the MLN, thereby making joint inference and learning infeasible. To address this problem, we propose a novel solution where we implicitly encode rich features into our model by augmenting the MLN distribution with low dimensional unit clauses. Our approach achieves state-of-the-art results on two standard evaluation corpora.
Retrospective event detection is an important task for discovering previously unidentified events in a text stream. In this paper, we propose two fast centroid-aware event detection models based on a novel text stream representation {--} Burst Information Networks (BINets) for addressing the challenge. The BINets are time-aware, efficient and can be easily analyzed for identifying key information (centroids). These advantages allow the BINet-based approaches to achieve the state-of-the-art performance on multiple datasets, demonstrating the efficacy of BINets for the task of event detection.
Machine learning-based methods have obtained great progress on emotion classification. However, in most previous studies, the models are learned based on a single corpus which often suffers from insufficient labeled data. In this paper, we propose a corpus fusion approach to address emotion classification across two corpora which use different emotion taxonomies. The objective of this approach is to utilize the annotated data from one corpus to help the emotion classification on another corpus. An Integer Linear Programming (ILP) optimization is proposed to refine the classification results. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification.
Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.
For analyzing argumentative text, we propose to study the {`}depth{'} of argumentation as one important component, which we distinguish from argument quality. In a pilot study with German newspaper commentary texts, we asked students to rate the degree of argumentativeness, and then looked for correlations with features of the annotated argumentation structure and the rhetorical structure (in terms of RST). The results indicate that the human judgements correlate with our operationalization of depth and with certain structural features of RST trees.
Video event detection is a challenging problem in information and multimedia retrieval. Different from single action detection, event detection requires a richer level of semantic information from video. In order to overcome this challenge, existing solutions often represent videos using high level features such as concepts. However, concept-based representation can be confusing because it does not encode the relationship between concepts. This issue can be addressed by exploiting the co-occurrences of the concepts, however, it often leads to a very huge number of possible combinations. In this paper, we propose a new approach to obtain the relationship between concepts by exploiting the syntactic dependencies between words in the image captions. The main advantage of this approach is that it significantly reduces the number of informative combinations between concepts. We conduct extensive experiments to analyze the effectiveness of using the new dependency representation for event detection on two large-scale TRECVID Multimedia Event Detection 2013 and 2014 datasets. Experimental results show that i) Dependency features are more discriminative than concept-based features. ii) Dependency features can be combined with our current event detection system to further improve the performance. For instance, the relative improvement can be as far as 8.6{\%} on the MEDTEST14 10Ex setting.
Accurate prediction of user attributes from social media is valuable for both social science analysis and consumer targeting. In this paper, we propose a systematic method to leverage user online social media content for predicting offline restaurant consumption level. We utilize the social login as a bridge and construct a dataset of 8,844 users who have been linked across Dianping (similar to Yelp) and Sina Weibo. More specifically, we construct consumption level ground truth based on user self report spending. We build predictive models using both raw features and, especially, latent features, such as topic distributions and celebrities clusters. The employed methods demonstrate that online social media content has strong predictive power for offline spending. Finally, combined with qualitative feature analysis, we present the differences in words usage, topic interests and following behavior between different consumption level groups.
Recently, topic modeling has been widely applied in data mining due to its powerful ability. A common, major challenge in applying such topic models to other tasks is to accurately interpret the meaning of each topic. Topic labeling, as a major interpreting method, has attracted significant attention recently. However, most of previous works only focus on the effectiveness of topic labeling, and less attention has been paid to quickly creating good topic descriptors; meanwhile, it{'}s hard to assign labels for new emerging topics by using most of existing methods. To solve the problems above, in this paper, we propose a novel fast topic labeling framework that casts the labeling problem as a k-nearest neighbor (KNN) search problem in a probability vector set. Our experimental results show that the proposed sequential interleaving method based on locality sensitive hashing (LSH) technology is efficient in boosting the comparison speed among probability distributions, and the proposed framework can generate meaningful labels to interpret topics, including new emerging topics.
Using neural networks to generate replies in human-computer dialogue systems is attracting increasing attention over the past few years. However, the performance is not satisfactory: the neural network tends to generate safe, universally relevant replies which carry little meaning. In this paper, we propose a content-introducing approach to neural network-based generative dialogue systems. We first use pointwise mutual information (PMI) to predict a noun as a keyword, reflecting the main gist of the reply. We then propose seq2BF, a {``}sequence to backward and forward sequences{''} model, which generates a reply containing the given keyword. Experimental results show that our approach significantly outperforms traditional sequence-to-sequence models in terms of human evaluation and the entropy measure, and that the predicted keyword can appear at an appropriate position in the reply.
Situated dialogue systems that interact with humans as part of a team (e.g., robot teammates) need to be able to use information from communication channels to gauge the coordination level and effectiveness of the team. Currently, the feasibility of this end goal is limited by several gaps in both the empirical and computational literature. The purpose of this paper is to address those gaps in the following ways: (1) investigate which properties of task-oriented discourse correspond with effective performance in human teams, and (2) discuss how and to what extent these properties can be utilized in spoken dialogue systems. To this end, we analyzed natural language data from a unique corpus of spontaneous, task-oriented dialogue (CReST corpus), which was annotated for disfluencies and conversational moves. We found that effective teams made more self-repair disfluencies and used specific communication strategies to facilitate grounding and coordination. Our results indicate that truly robust and natural dialogue systems will need to interpret highly disfluent utterances and also utilize specific collaborative mechanisms to facilitate grounding. These data shed light on effective communication in performance scenarios and directly inform the development of robust dialogue systems for situated artificial agents.
We present a novel response generation system. The system assumes the hypothesis that participants in a conversation base their response not only on previous dialog utterances but also on their background knowledge. Our model is based on a Recurrent Neural Network (RNN) that is trained over concatenated sequences of comments, a Convolution Neural Network that is trained over Wikipedia sentences and a formulation that couples the two trained embeddings in a multimodal space. We create a dataset of aligned Wikipedia sentences and sequences of Reddit utterances, which we we use to train our model. Given a sequence of past utterances and a set of sentences that represent the background knowledge, our end-to-end learnable model is able to generate context-sensitive and knowledge-driven responses by leveraging the alignment of two different data sources. Our approach achieves up to 55{\%} improvement in perplexity compared to purely sequential models based on RNNs that are trained only on sequences of utterances.
Named-Entity Recognition (NER) is still a challenging task for languages with low digital resources. The main difficulties arise from the scarcity of annotated corpora and the consequent problematic training of an effective NER pipeline. To abridge this gap, in this paper we target the Persian language that is spoken by a population of over a hundred million people world-wide. We first present and provide ArmanPerosNERCorpus, the first manually-annotated Persian NER corpus. Then, we introduce PersoNER, an NER pipeline for Persian that leverages a word embedding and a sequential max-margin classifier. The experimental results show that the proposed approach is capable of achieving interesting MUC7 and CoNNL scores while outperforming two alternatives based on a CRF and a recurrent neural network.
This paper proposes OCR++, an open-source framework designed for a variety of information extraction tasks from scholarly articles including metadata (title, author names, affiliation and e-mail), structure (section headings and body text, table and figure headings, URLs and footnotes) and bibliography (citation instances and references). We analyze a diverse set of scientific articles written in English to understand generic writing patterns and formulate rules to develop this hybrid framework. Extensive evaluations show that the proposed framework outperforms the existing state-of-the-art tools by a large margin in structural information extraction along with improved performance in metadata and bibliography extraction tasks, both in terms of accuracy (around 50{\%} improvement) and processing time (around 52{\%} improvement). A user experience study conducted with the help of 30 researchers reveals that the researchers found this system to be very helpful. As an additional objective, we discuss two novel use cases including automatically extracting links to public datasets from the proceedings, which would further accelerate the advancement in digital libraries. The result of the framework can be exported as a whole into structured TEI-encoded documents. Our framework is accessible online at \url{http://www.cnergres.iitkgp.ac.in/OCR++/home/}.
Comparable corpora are the main alternative to the use of parallel corpora to extract bilingual lexicons. Although it is easier to build comparable corpora, specialized comparable corpora are often of modest size in comparison with corpora issued from the general domain. Consequently, the observations of word co-occurrences which are the basis of context-based methods are unreliable. We propose in this article to improve word co-occurrences of specialized comparable corpora and thus context representation by using general-domain data. This idea, which has been already used in machine translation task for more than a decade, is not straightforward for the task of bilingual lexicon extraction from specific-domain comparable corpora. We go against the mainstream of this task where many studies support the idea that adding out-of-domain documents decreases the quality of lexicons. Our empirical evaluation shows the advantages of this approach which induces a significant gain in the accuracy of extracted lexicons.
In this paper we present a newly developed tool that enables researchers interested in spatial variation of language to define a geographic perimeter of interest, collect data from the Twitter streaming API published in that perimeter, filter the obtained data by language and country, define and extract variables of interest and analyse the extracted variables by one spatial statistic and two spatial visualisations. We showcase the tool on the area and a selection of languages spoken in former Yugoslavia. By defining the perimeter, languages and a series of linguistic variables of interest we demonstrate the data collection, processing and analysis capabilities of the tool.
WordNet is probably the best known lexical resource in Natural Language Processing. While it is widely regarded as a high quality repository of concepts and semantic relations, updating and extending it manually is costly. One important type of relation which could potentially add enormous value to WordNet is the inclusion of collocational information, which is paramount in tasks such as Machine Translation, Natural Language Generation and Second Language Learning. In this paper, we present ColWordNet (CWN), an extended WordNet version with fine-grained collocational information, automatically introduced thanks to a method exploiting linear relations between analogous sense-level embeddings spaces. We perform both intrinsic and extrinsic evaluations, and release CWN for the use and scrutiny of the community.
Many argumentative texts, and news editorials in particular, follow a specific strategy to persuade their readers of some opinion or attitude. This includes decisions such as when to tell an anecdote or where to support an assumption with statistics, which is reflected by the composition of different types of argumentative discourse units in a text. While several argument mining corpora have recently been published, they do not allow the study of argumentation strategies due to incomplete or coarse-grained unit annotations. This paper presents a novel corpus with 300 editorials from three diverse news portals that provides the basis for mining argumentation strategies. Each unit in all editorials has been assigned one of six types by three annotators with a high Fleiss{'} Kappa agreement of 0.56. We investigate various challenges of the annotation process and we conduct a first corpus analysis. Our results reveal different strategies across the news portals, exemplifying the benefit of studying editorials{---}a so far underresourced text genre in argument mining.
The Universal Dependencies (UD) project was conceived after the substantial recent interest in unifying annotation schemes across languages. With its own annotation principles and abstract inventory for parts of speech, morphosyntactic features and dependency relations, UD aims to facilitate multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. This paper presents the Turkish IMST-UD Treebank, the first Turkish treebank to be in a UD release. The IMST-UD Treebank was automatically converted from the IMST Treebank, which was also recently released. We describe this conversion procedure in detail, complete with mapping tables. We also present our evaluation of the parsing performances of both versions of the IMST Treebank. Our findings suggest that the UD framework is at least as viable for Turkish as the original annotation framework of the IMST Treebank.
Arabic dialects present a special problem for natural language processing because there are few resources, they have no standard orthography, and have not been studied much. However, as more and more written dialectal Arabic is found in social media, NLP for Arabic dialects becomes an important goal. We present a methodology for creating a morphological analyzer and a morphological tagger for dialectal Arabic, and we illustrate it on Egyptian and Levantine Arabic. To our knowledge, these are the first analyzer and tagger for Levantine.
Semantic Role Labeling (SRL) is the task of identifying the predicate-argument structure in sentences with semantic frame and role labels. For the English language, the Proposition Bank provides both a lexicon of all possible semantic frames and large amounts of labeled training data. In order to expand SRL beyond English, previous work investigated automatic approaches based on parallel corpora to automatically generate Proposition Banks for new target languages (TLs). However, this approach heuristically produces the frame lexicon from word alignments, leading to a range of lexicon-level errors and inconsistencies. To address these issues, we propose to manually alias TL verbs to existing English frames. For instance, the German verb drehen may evoke several meanings, including {``}turn something{''} and {``}film something{''}. Accordingly, we alias the former to the frame TURN.01 and the latter to a group of frames that includes FILM.01 and SHOOT.03. We execute a large-scale manual aliasing effort for three target languages and apply the new lexicons to automatically generate large Proposition Banks for Chinese, French and German with manually curated frames. We present a detailed evaluation in which we find that our proposed approach significantly increases the quality and consistency of the generated Proposition Banks. We release these resources to the research community.
This paper contributes to a growing body of evidence that{---}when coupled with appropriate machine-learning techniques{--}linguistically motivated, information-rich representations can outperform one-hot encodings of linguistic data. In particular, we show that phonological features outperform character-based models. PanPhon is a database relating over 5,000 IPA segments to 21 subsegmental articulatory features. We show that this database boosts performance in various NER-related tasks. Phonologically aware, neural CRF models built on PanPhon features are able to perform better on monolingual Spanish and Turkish NER tasks that character-based models. They have also been shown to work well in transfer models (as between Uzbek and Turkish). PanPhon features also contribute measurably to Orthography-to-IPA conversion tasks.
Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variable-length text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.
We study the role of the second language in bilingual word embeddings in monolingual semantic evaluation tasks. We find strongly and weakly positive correlations between down-stream task performance and second language similarity to the target language. Additionally, we show how bilingual word embeddings can be employed for the task of semantic language classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact.
Solving word analogies became one of the most popular benchmarks for word embeddings on the assumption that linear relations between word pairs (such as \textit{king}:\textit{man} :: \textit{woman}:\textit{queen}) are indicative of the quality of the embedding. We question this assumption by showing that the information not detected by linear offset may still be recoverable by a more sophisticated search method, and thus is actually encoded in the embedding. The general problem with linear offset is its sensitivity to the idiosyncrasies of individual words. We show that simple averaging over multiple word pairs improves over the state-of-the-art. A further improvement in accuracy (up to 30{\%} for some embeddings and relations) is achieved by combining cosine similarity with an estimation of the extent to which a candidate answer belongs to the correct word class. In addition to this practical contribution, this work highlights the problem of the interaction between word embeddings and analogy retrieval algorithms, and its implications for the evaluation of word embeddings and the use of analogies in extrinsic tasks.
We propose a novel semantic tagging task, semtagging, tailored for the purpose of multilingual semantic parsing, and present the first tagger using deep residual networks (ResNets). Our tagger uses both word and character representations, and includes a novel residual bypass architecture. We evaluate the tagset both intrinsically on the new task of semantic tagging, as well as on Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an auxiliary loss function predicting our semantic tags, significantly outperforms prior results on English Universal Dependencies POS tagging (95.71{\%} accuracy on UD v1.2 and 95.67{\%} accuracy on UD v1.3).
Frame semantics is a theory of linguistic meanings, and is considered to be a useful framework for shallow semantic analysis of natural language. FrameNet, which is based on frame semantics, is a popular lexical semantic resource. In addition to providing a set of core semantic frames and their frame elements, FrameNet also provides relations between those frames (hence providing a network of frames i.e. FrameNet). We address here the limited coverage of the network of conceptual relations between frames in FrameNet, which has previously been pointed out by others. We present a supervised model using rich features from three different sources: structural features from the existing FrameNet network, information from the WordNet relations between synsets projected into semantic frames, and corpus-collected lexical associations. We show large improvements over baselines consisting of each of the three groups of features in isolation. We then use this model to select frame pairs as candidate relations, and perform evaluation on a sample with good precision.
We study the network structure underlying dictionaries. We systematize the properties of such networks and show their relevance for linguistics. As case of study, we apply this technique to identify the graph structure of Ogden{'}s Basic English. We show that it constitutes a strong core of the English language network and that classic centrality measures fail to capture this set of words.
Part 1: Theory. Although the economics of the business preclude large-scale investment in terminology, I believe that an iterative approach to collecting and improving terminological data can pay off. The quality and value of terminology are discussed from an LSP‚Äôs viewpoint and defined for an LSP. The features of an optimal terminology process and the process‚Äô relationship to the ISO17100 translation process are identified. The interests of the other parties in the translation process are reviewed and best practices for terminology work are identified for the different parties involved. The objectives of a terminology process are formulated and discussed. The features of two standard terminology modules are compared and my choice of terminology server is explained. A standard terminological record structure for termbases is introduced. Part 2: Practice. The second part of the workshop will present an implementation of termbases using this term record structure. This will include the ways in which TransForm is dealing with the strengths and weaknesses of the terminology server used and an iterative process for improving the value of terminological records. Different approaches to automatic term matching will be evaluated, with particular attention paid to the problem of false positive results in QA checks. 
This paper describes the QT21 project from the perspective of the International Federation of Translators (FIT) in three main parts. Firstly, six of the ways that humans currently relate with machine translation (MT) systems will be outlined, leading up to a seventh way that will be discussed in more detail. Huge volumes of texts need to be translated in different sectors of the economy globally. A feasible approach to meeting this need is to employ both raw MT and humans, including translators, in addressing the world's translation needs. Secondly, analytic evaluation of MT quality by human translators will be introduced, focusing on the MQM framework. This seventh way involves annotation, by humans, of specific errors in the raw MT using standardized error categories, rather than only generating a single number indicating overall quality. Lastly, the potential impact of QT21 on MT and professional translators will be reflected on. Through FIT, human translators will be able to participate in the development of improved MT systems. This will help them give objective advice to clients and to guide the developers of next generation translation tools. FIT‚Äôs position is there will be enough work for translators who do not feel threatened by MT. 
The migration to the revamped, modernised and upgraded InterActive Terminology for Europe, the EU‚Äôs inter-institutional terminology database, is going through a thorough IT development process designed to produce a brand new tool built around some major requirements as detailed in this paper. Keeping in mind all improvements required, as defined by a dedicated task force reporting to the IATE Management Group (‚ÄòIMG‚Äô), or needed, due to the obsolescence of some technologies used over the last 12 years or to the availability of new technologies that could better serve users‚Äô needs. Taking into account the current state of a tool which had undergone corrective and evolutive maintenance over time with an increasing number of technical limitations, it was proposed and accepted to go for a brand new tool. The rebirth of IATE was announced. Over the last two years the interinstitutional cooperation took a new rise: all Task Forces of the IMG brought ideas and expressed needs, while engaging in complementary activities, such as a vast cleaning of IATE entries. The IATE 2 project deliverable will provide enhanced and new features to further improve terminology management. In conjunction with its collaborative platform, EurTerm, it will also strengthen collaborative working with stakeholders. Keywords: Making life easier for users, responsive web design, integration with Computer Assisted Translation and Terminology (CATT) tools, improved collaborative working, improved return on investment. 
This paper presents InterpretBank, a computer-assisted interpreting tool developed to support conference interpreters during all phases of the interpreting process. The overall aim of the tool is to create an interpreter‚Äôs workstation which allows conference interpreters to optimize the workÔ¨Çow before, during and after the event they are called upon to interpret. The tool takes into consideration the speciÔ¨Åc needs of conference interpreters, such as the way they prepare for a conference, the modality of terminology access, and so forth. It also exploits the latest advances in computational linguistics, especially in the Ô¨Åeld of information retrieval and text mining, making use of the abundance of information available on the Web to provide interpreters with specialized information which can be used to increase the quality of interpreter performance. The paper also introduces some theoretical principles of the use of terminology tools in interpretation and the results of initial empirical experiments conducted with this software.  
This is to inform the business and decision making communities among the ASLING audience about the high level benefits of bitext and XLIFF 2. Translator and Engineering communities will also benefit, as they need the high level arguments to make the call for XLIFF 2 adoption in their organizations. We start with a conceptual outline what bitext is, what different sorts of bitext exist and how they are useful at various stages in various industry processes, such as translation, localisation, terminology management, quality and sanity assurance projects etc. Examples of projects NOT based on bitext are given, benefits and drawbacks compared on a practical level of tasks performed. The following is demonstrated: That bitext management is a core process for efficient multilingual content value chains; That usage of an open standard bitext creates a greater sum of good than usage of proprietary bitext formats; and finally: That XLIFF 2 is the core format and data model to base bitext management on. 
This paper describes an experiment conducted by the author in November 2015 with 69 MSc Translation students at CenTraS @ UCL (covering 14 target languages) and in August 2016 with 30 professional translators in Saudi Arabia (covering English to Arabic). The experiment was inspired by Lynne Bowker‚Äôs pilot study Productivity vs Quality? A pilot study on the impact of translation memory systems (published in Localisation Focus in March 2005). The author of this paper wanted to find out whether translators who are fairly new to translation technology would ‚Äúblindly‚Äù trust the content of a TM or whether they would still check the content thoroughly and make any necessary changes to the translation. Students and professional translators were asked to translate a short text consisting of 14 sentences and a total of 217 words in Wordfast Anywhere/SDL Trados Studio 2015. They also received a translation memory (TM) for their respective language combination. All TMs contained mistakes, which the author did not mention to the students and the professional translators. Interestingly, while the professional translators fared better at editing fuzzy matches than the students, they did not pick up on incorrect 100% matches as well as the student translators, tended to lack attention to detail by, for example, introducing double spaces into sentences, and not all professional translators translated the new sentences given for translation. 
 This work describes a teaching method to enrich and improve the translators‚Äô and interpreters‚Äô multilingual terminology to translate specialized texts from a single link. This method consists of using controlled vocabularies such as thesauri, classification schemes, subject heading systems and taxonomies that employ Linked Open Data (LOD) technology in the framework of Semantic Web.  
After years of development of corpus technologies, it has become obvious that translators can benefit directly from the achievements of this field. However, it seems that corpus advancement has not been deployed accordingly by translators to aid their translation. As a corpus linguist and translator myself, I believe that when corpus technologies are made attractive and simple enough and when they do feel a strong need and burning desire to make their own corpus to assist their translation, then application of such technology will gradually become part of a translator's life, just as other computer-assisted translation (CAT) tools have done over the past ten years or so. This paper attempts to make a demonstration as to how easy it can be to DIY a corpus by building a small domain-specific corpus between English and Chinese in the field of financial services. The making of such a corpus has been summarised into three simple steps: 1) Collection of raw parallel language data; 2) Alignment; 3) Segmentation and Annotation. It is hoped that other users of corpora including translation trainers, language teachers and students will also find this presentation informative and beneficial. 1. Introduction At the Fourth International Conference on Corpus Use and Learning to Translate held in Alicante in 2015, there was a strongly felt concern that even though corpus technologies are available and abundant for translators to use, surveys had shown that a very small number of translators are currently deploying this new technology to aid their work in translation (Fr√©rot, 2015). Actually, this phenomenon was addressed several year ago by Bernardini and Castagnoli (2008). In my opinion, there might be at least two reasons that have contributed to this undesirable outcome. One reason is that translators do not find it absolutely necessary to take the trouble to build and use a corpus of their own because they are short of time in learning the method of making a corpus and putting their hands on it while they can use the Google search engine instead (also see Bernardini, 2015). And the other reason is that the corpus technology has not been presented as a user-friendly and efficient means of assistance to translators yet, most probably due to its seemingly daunting complexity involved in it. Even though awareness has been raised of the usefulness of corpus technology for translators (i.e. Bowker, 2002; Quah, 2006), it seems that there is a need to simplify and systemise the process of construction with clearer instructions and perhaps more importantly examples while the technology details are presented to translators. As a corpus linguist and translator myself, I believe that when corpus technologies are made attractive and simple enough and when they do feel a strong need to make their own corpus to assist their translation, then they will gradually integrate the application of corpus technology into their translators work, just as other computer-assisted translation (CAT) tools have been in the past few decades. This paper attempts to make a demonstration through DIYing a corpus by building a small domainspecific corpus in the broad field of financial services, with English as the source and Chinese as the translation. The significance of this demonstration should be applicable to other domains. The paper is to introduce the necessary stages to take if a small domain specific corpus is to be build, i.e. a) collection of raw parallel texts, b) alignment of the collected parallel texts, and c) segmentation to the C8h8inese texts and annotation to both English and Proceedings of the 38th Conference Translating and the Computer, pages 88‚Äì99, London, UK, November 17-18, 2016. c 2016 AsLing  Chinese. The issues of translation quality control and copyright in building a corpus by using texts collected from the web are addressed in brief. At the centre of my target readers are professional translators who are new to corpora, but I also hope that others such as translation trainees and teachers, language learners and teachers could find this paper informative and relevant to their interests.  2. Collection of raw parallel language data  The making of this corpus is primarily to demonstrate how it can aid translators in a specific domain, therefore, the size of the corpus does not have be very big. This section describes my method in collecting English-Chinese parallel texts in financial services. English is the source language and simplified Chinese (as mainly used in the mainland of China) is the translation. Collection of raw language data is carried out through a combination of pure manual collection and semi-automatic collection assisted by a web-crawling programme called Wget1.  2.1 Pure manual collection  Unlike collecting monolingual texts, starting to collect parallel texts can be somewhat tricky. One way to be adopted is to find out some websites containing texts of the relevant languages in a searching engine such as Google. In order for Google to search from the internet some candidate websites, a few key words can be tried in both the source language English (in this case) and the translation language Chinese (in this case) in Google. For example, some of the English terms and their translation in Chinese can be typed into the Google search engine for a preliminary search such as financial services, foreign exchange, trading, platform, risks, terms and conditions, and the equivalent Chinese translation of these terms. When you have the retrieved websites by Google, you may select and open some potential websites for detailed look for possible parallel texts. Sometimes a few different sets of key words need to be fed into Google before serious candidate websites can be captured. Double quotation marks can be used to search multiple key words in a string so that Google concentrates on the exact phrasing instead of a combination of the individual words in the string. Sometimes you need to try different sets of key words for several times before there appear some websites containing the right information needed. Some other detailed advice for this purpose is available online for users reference2. Translators who find it necessary to build a domain specific corpus of their own normally would have known some websites containing potential parallel texts while they are working on their translation tasks. Therefore, these websites could serve as a starting point for the collection of raw parallel texts. Manually collected parallel texts from the internet through keying bilingual key words or terms in a search engine are normally mixed in languages in one document and cannot be directly passed for alignment programmes to carry out alignment because most current alignment programmes only accept the input of the parallel data in the way that the source language is in one file and the translation in another (see Section 3 for alignment). Separating Chinese from English and saving the two texts originally in one document into two individual files takes two stages. The first stage is to separate the mixed texts with a uniform marker called delimiter to facilitate the recognition of the boundary of the two differently coded languages by the next software. In this process, regular expressions can be used to separate the two languages properly. It may take several rounds to conduct the separation completely due to the various situations of the mixture of the Chinese language. Other programmes using  
This paper evaluates the usefulness of Moses, an open source statistical machine translation (SMT) engine, for professional translators and post editors. It takes a look behind the scenes at the workings of Moses and reports on experiments to investigate how translators can contribute to advances in the use of SMT as a tool. In particular the difference in quality of output was compared as the amount of training data was increased using four SMT engines. This small study works with the German-English language pair to investigate the difficulty of building a personal SMT engine on a PC with no connection to the Internet to overcome the problems of confidentiality and security that prevent the use of online tools. The paper reports on the ease of installing Moses on an Ubuntu PC using Moses for Mere Mortals. Translations were compared using the Bleu metric and human evaluation. Introduction Pym (2012) considers that translators are destined to become post-editors because the amalgamation of statistical machine translation (SMT) into translation memory (TM) suites will cause changes to the skills required by translators. He believes that machine translation (MT) systems are improving with use and a virtuous circle should result. However, free online MT, for example Google Translate (GT), could lead to a vicious circle caused by the recycling of poor unedited translations. Technologists have a blind faith in the quality of translations used as 'gold standards' and Bowker (2005) found that TM users tend to accept matches without any critical checks. Further, the re-use of short sentences leads to inconsistent terminology, lexical anaphora, deictic errors and instances where the meaning is not as foreseen in the original. Pym (2010, p.127) suggests that this could be avoided if each organisation has its own in-house SMT system. There is another compelling reason for in-house SMT. Achim Klabunde (2014), Data Protection Supervisor at the EU warns against using free translation services on the Internet. He asserts that someone is paying for what appears to be a free service. It is likely that users pay by providing their personal data. Translators using these services may however be in breach of confidentiality agreements because the data could be harvested by others and recycled in their translations. Googles terms and conditions are clear that they could use any content submitted to their services (Google, 2014). The increased volume of translation caused by localisation does, however, call for automation (Carson-Berndsen et al, 2010, p.53). Advances in computer power have enhanced MT and good quality full post editing can be included in TM for segments (sentences, paragraphs or sentence-like units eg. headings, titles or elements in a list) where no match or fuzzy match is available (Garcia, 2011, p.218). TM developers now offer the facility to generate MT matches to freelance translators, eg. Google Translator Toolkit, Smartcat, and Lilt. This technology will increase the rate of production and according to Garcia (2011, p.228) industry expects that post-editing, with experienced post-editors and in-domain trained engines, for publication should be able to process 5000 words a day. Pym (2012, p.2) maintains that post-editors will require ex1c0e0llent target language (TL) skills, good subject Proceedings of the 38th Conference Translating and the Computer, pages 100‚Äì112, London, UK, November 17-18, 2016. c 2016 AsLing  knowledge but only weak source language (SL) skills. For this reason I elected to work from German, my weaker foreign language, into English my native language. I have used Lilt to post-edit translated segments to help with evaluation of the SMT as will be explained later. This paper reports a case study that used Moses for Mere Mortals (MMM) to investigate how difficult it might be for a freelance translator to incorporate an in-house SMT engine into a single workstation by building four distinct Moses engines with different amounts of data. Following an overview of the project the method followed to install, create, train and use the Moses engines using MMM is explained. Then an explanation of how the raw MT output was obtained, processed and evaluated will be given before presenting the results and drawing a conclusion. Overview A study carried out by Machado and Fontes (2011) for the Directorate General for Translation at the European Commission forms the basis for the methods adopted in the experiments. The aim was to explore integrating a personal SMT engine into a translator‚Äôs workbench whereby the MT system was to be within a single computer with no connection to the Internet. It is trained with data that the user owns or has permission to use. The MT output is post-edited by the user to a level that Taus (2014) defines as being comprehensible (i.e. the content is perfectly understandable), accurate (i.e. it communicates the ST meaning) and the style is good but probably not that of a L1 human translator. Punctuation should be correct and syntax and grammar should be normal as follows: ‚Ä¢ The post-edited machine translation (PEMT) should be grammatically, semantically and syntactically correct. ‚Ä¢ Key terminology should be correctly translated. ‚Ä¢ No information should be accidentally added or omitted. ‚Ä¢ Offensive, inappropriate or culturally unacceptable content should be edited. ‚Ä¢ As much raw MT output as possible should be used. ‚Ä¢ Basic rules of spelling, punctuation and hyphenation should apply. ‚Ä¢ Formatting should be correct. McElhany and Vasconellos (1988, p.147) warn that because editing is not rewriting corrections should be minimal. To carry out this study, I installed Moses on a desktop computer using MT software MMM that claims to be user-friendly and able to be understood by users who are not computational linguists or computer scientists. Such users are referred to as ‚Äòmere mortals‚Äô (Machado and Fontes, 2014, p. 2). A large parallel corpus is required for training Moses. TM is ideal for this because it produces aligned bi-texts that can be used with minimal changes. The Canadian Parliament‚Äôs Hansard, which is bilingual, was the source of data for early work on SMT (Brown et al, 1988, p. 71.) A data source created and often used to promote the progress of SMT development is the Europarl corpus produced from the European Parliament‚Äôs multilingual proceedings, which are published on the EU website. Koehn (2005) arranged them into the corpus. He confirmed that it can be used freely (personal comm10u1nication, 25 January 2016). It was chosen to  simulate a TM for this project because it was used in the study made by Machado and Fontes (2011). When aligned with German it has 1,011,476 sentences. I used MMM to build four MT systems with different amounts of data and tested them with a test document of 1000 isolated sentences extracted, together with their translations from the corpus. Moses‚Äô developers suggest that by varying the tuning weights it is possible to tune the system for a particular language pair and corpus (Koehn, 2015, p.62). MMM facilitates some adjustments and the effect of these was studied using the largest training. Before explaining how the experiments were conducted I will describe how I installed MMM and built the Moses engines. . Equipment, and software installation MMM (Machado, and Fontes 2014, p.2) is intended to make the SMT system Moses available to many users and is distributed under a GNU General Public Licence (p.10). There is a very comprehensive MMM tutorial (Machado, and Fontes 2014) giving a step-by-step guide to SMT for newcomers like myself. The tutorial recommends a PC with at least 8GB of RAM, an 8 core processor and 0.5 TB hard disk (p.14) but no less than 4 GB of RAM and a 2 core processor. I used a machine with 8 GB of ram, 4 processors but only a 148GB hard disk. There is a ‚Äòtransfer-to-anotherlocation‚Äô script that can be used to transfer training to another machine with a much lower specification for translating/decoding only. I tried this using a 1GB laptop but would not recommend it. It was able to complete the translation but it took hours rather than the minutes taken by the 8GB machine. MMM consists of a series of scripts that automate installation, test file creation, training, translation and automatic evaluation or scoring. Following the tutorial, I installed Ubuntu on the computer, choosing 14.04(LTS)(64 bits), although MMM will also run on 12.04 (LTS). The next step was to download a zipped archive of MMM files and unpack them onto the computer. The MMM tutorial explains how to prepare the corpus for training and build the system. Training the full Europarl corpus took 30 hours. Although Ubuntu has a Graphical User Interface (GUI), I preferred the Command Line Interface (CLI) (see figure 2). The script ‚ÄòInstall‚Äô, was run next to install all the files required onto the computer. MMM includes all the files necessary to run Moses but the script downloads, any Ubuntu files that are needed but not present in the computer from the Internet With MMM installed running the ‚ÄòCreate‚Äô script completes installation of Moses. There is a demo corpus that translates from English into Portuguese included with MMM for trying out Moses. I used this to experience preparing the corpora, extracting test data, translating and scoring before doing it with the German and English parts of the Europarl corpus. 102  Figure 1 Installing Ubuntu packages Preparation of the corpora. The 'Make-test-files' script was used to extract a 1000 segment test file from the Europarl corpus before using it for training. Training With MMM it is possible to build multiple translation engines on one computer. Where possible files generated by earlier ‚Äòtrainings‚Äô are re-used. The training to be used for a particular translation is selected from the ‚Äòtranslation‚Äô script. A monolingual TL corpus is used to build the language model. I used the English side of the bilingual corpus in all trainings. The aligned texts are placed in a folder named ‚Äòcorporafor- training‚Äô and the train script is run. When the training is complete a report is generated that is required by the ‚Äòtranslation‚Äô script to select the correct training. Four basic trainings were built and tested. The first used the whole corpus. Then a second engine was built by splitting out the first 200,000 segments. This was repeated for 400,000 and 800,000 segments. The 1,000 segment test document was translated by each of the engines and Bleu scores obtained using the MMM ‚ÄòScore‚Äô script. A sample of 50 segments from each translation was post-edited and evaluated by me. Translation The tests were divided into two parts. Before studying the difference in translation quality using the different sized corpora, the effect of the tuning weights was examined with the whole corpus. With the German ST part of the test document in the ‚Äòtranslation-in‚Äô folder and the required data entered into the ‚ÄòTranslate‚Äô script, translation was initiated by running the script from the CLI. According to Koehn (2015, p.62) a good phrase translation table is the key to good performance. He goes on to explain how some tuning can be done using the decoder. Significantly, the weighting of the four Moses models can be adjusted. They are combined by a log linear model (Koehn, 2010, p.137), which is well known in MT circles. The four models or features are: 103  ‚Ä¢ The phrase translation table contains English and German phrases that are good translations. A phrase in SMT is one or more contiguous words. It is not a grammatical unit. ‚Ä¢ The language model contributes by keeping the output in fluent English. ‚Ä¢ The distortion model permits the input sentence to be reordered but at a price: The translation costs more the more reordering there is. ‚Ä¢ The word penalty prevents the translations from getting too long or too short. There are three weights that can be adjusted in the ‚ÄòTranslation‚Äô script. These are Wl, Wd and Ww. They have default values of 1,1 and 0. The tuning weights were adjusted in turn using the translation script. With all the weights left at their default levels I produced the first translation. The reference translation was placed in the MMM ‚Äòreference-translation‚Äô folder and a Bleu score was obtained by running the 'score' script. I then post-edited 50 segments and evaluated the MT as explained below. This was repeated with Wd set to 0.5 and then 0.1. Then with Wd set back at 1, and with Wl set to 0.5 and then 0.1 further MTs were gathered and evaluated. Similar experiments were conducted with Ww set to ‚Äì3 and then 3 and with Minimum Bayes Risk (MBR) decoding (MBR decoding outputs the translation that is most similar to the most likely translation). Having explained how the system was built and the MTs obtained, the methods used to evaluate the results will be described. Evaluation A total of 8 measurement points generated translations that were evaluated by both automatic and manual techniques. Metrics Machado and Fontes (2011, p.4) utilised the automatic evaluation metric Bleu (bilingual evaluation understudy), which compares the closeness of the MT to a professional translation relying on there being at least one good quality human reference translation available (Papineni et al, 2001, p.1). It is measured with an n-gram algorithm developed by IBM. The algorithm tabulates the number of n-grams in the test MT that are also present in the reference translation(s) and scores quality as a weighted sum of the counts of matching n-grams. In computing the n-gram overlap of the MT output and the reference translation the IBM algorithm penalises translations that are significantly longer or shorter than the reference. For computational linguists Bleu is a cheap quick language independent method of evaluation and correlates well with human techniques (Papineni et al, 2001). In many cases this correlation has been shown to be correct (Doddington, 2002, p.138-145) and a study by Coughlin (2003, p.6) claims that Bleu correlates with the ranking of the TM and also 'provides a rough but reliable indication of the magnitude of the difference between the systems'. However, Callison-Burch et al (2006) take the view that higher Bleu scores do not necessarily indicate improved translation quality and focused manual evaluation may be preferable for some research projects. They conclude that for systems with similar translation structures Bleu is appropriate. 104  Manual Evaluation Machado and Fontes (2011) had a team of translators performing human translations of a sample of segments even though human evaluations of MT output are extensive, expensive and take weeks or months to complete (Papineni, Roukos, Ward and Zhu, 2001, p.1). White (2003, p.213) points out that they are very subjective because there is no ‚Äòright‚Äô translation, as there is never any agreement on which is the best. Newmark (1982, p.140) is convinced that the perfect translation does not exist, but if it does Biguenet and Schulte (1989, p.12) are sure that it will never be found. Evaluators are always biased (White, 2003, p.219). For example, seeing a really bad segment might make the next one seem relatively better than it is and viceversa. Another example is where a mistake such as a trivial software bug is forgiven. An evaluator may also become bored or tired, resulting in segments graded early in the cycle receiving a more favourable treatment to those graded later. 'Fluency' and 'adequacy' are commonly used for evaluating raw MT output. Two scores are combined, averaged, and written as a percentage. Machado and Fontes (2011, p.7) did not follow this method and use what they call 'real life conditions' by post-editing and classifying the effort required for each segment on a scale of 1 to 5. Their scale was adopted in this study: 1. Bad: Many changes for an acceptable translation; no time saved. 2. So So: Quite a number of changes, but some time saved. 3. Good: Few changes; time saved. 4. Very Good: Only minor changes, a lot of time saved. 5. Fully correct: Could be used without any change, even if I would still change it if it were my own translation. Machado and Fontes do not mention whether or not time saved was measured but they do say that their objective was classifying segments by translation quality. Only a translation that can be used without change scores 5. A segment that is understandable and correct apart from one or maybe two errors receives a score of 4. One that should be translated from scratch scores 1. Scores were recorded segment-by-segment on a spreadsheet and averaged for the fifty segments. Since I was the only post editor 50 different segments were post-edited for each MT. This avoided previous knowledge influencing the scoring and permitted a PEMT version of the test text to be gradually produced. For consistency with the Bleu scores the averages were divided by 5 to express them on a scale of 0 to 1. There were eight measurement points in the first part of the experiments. A further four measurement points were made for the second part. These were for the 200000, 400000, 800000 and, for comparison, GT. 105  Figure 2 from text-compare.com  For the second part a third method of evaluation was introduced. This was based on Translation Edit Rate (TER), as a less subjective technique to check the quality of assessment, which should be quantitative. TER is defined as the lowest number of edits needed to change a MT segment so that it matches the reference, which is the post-edited segment, normalised by the length of the reference (Snover et al, 2006, p.3). TER = number of edits / number of words in the PEMT sentence. The raw MT was compared with the post-edited text using text-compare.com as shown in figure 3. The number of edits and the number of words in the PEMT were counted manually. I termed this hter because of the human involvement. I used a spread sheet to obtain a score for each segment, which were then ranked on a scale of 1 to 5 as follows:  TER  Hter  0  5  0 to 0.25  4  0.25 to 0.5  3  0.5 to 0.75  2  >0.75  
Two of the main challenges of translation are comprehension and terminology: understanding the text, and knowing what to call things in the target language. The focus of this topic is the comprehension component, not so much "how to help translators in understanding what is meant by a text string", but more, "how to deploy the solution to translation queries so that all translators have access to the solutions when they highlight a string in the translation (CAT) tool". The CAT tools we use do have partial solutions, but we did not find these viable for different reasons. We needed a way of annotating source files once and for all. However, it was vital that we did not leave a footprint in our source files. Any footprint would lead to a breakdown at build (compilation) time. The challenge: How do you create an external annotation that will always find its target string in the source files. We opted for a methodology borrowed from the terminology paradigm. Our source string was like a term, and the annotation like a term comment. The termbank became a stringbank, and the term dictionary an annotation dictionary. 
This paper describes the development of a language independent process for identifying proper-names in a text. The process is derived from a machine originally designed to analyse non-concatenative morphologies in natural languages. The particular context for this work is the task of managing the 5,000 or so proper-names found in a Bible, including the identification of close cognates and reporting instances where a related form does not appear to be present. The need for such a system is explained and the process by which the machine is able to identify names in the target text is described. The problems posed by disparate orthographies are noted. Results obtained from Eurasian, South American and African languages are discussed, common problems for the process identified and its possible use in the context of technical vocabulary suggested. Commonalities between the task of identifying morphology templates, ordered phoneme sets and syntax patterns are noted. 
In today‚Äôs digital and connected environment, it has become much easier to dissect services like interpretation into very small units. In some cases, interpreters working in micro units, i.e. within a limited space of information, may have a business case, in others, they operate in less restricted and predictable ‚Äúmacro‚Äù information space, having to recur to a wide range of secondary context, background and linguistic knowledge. Accordingly, payment can occur on micro and macro level, i.e. taking account or not of the secondary knowledge work involved in an assignment. Small payment units (minutes, or words) are not the most useful way of remunerating ‚Äúinformed‚Äù macro level knowledge work, but don‚Äôt necessarily have to exclude it. Software and digital platforms might not only be the catalyst of small-piece contracting, it could also serve as a means to make interpreters‚Äô knowledge work more efficient and profitable, thus provide optimum quality and value for money to the customer. 
In this paper we outline easily implementable procedures to leverage multilingual Linked Open Data (LOD) resources such as the DBpedia in open-source Statistical Machine Translation (SMT) systems such as Moses. Using open standards such as RDF (Resource Description Framework) Schema, NIF (Natural language processing Interchange Format), and SPARQL (SPARQL Protocol and RDF Query Language) queries, we demonstrate the efÔ¨Åcacy of translating named entities and thereby improving the quality and consistency of SMT outputs. We also give a brief overview of two funded projects that are actively working on this topic. These are the (1) BMBF funded project DKT (Digitale Kuratierungstechnologien) on digital curation technologies, and (2) EU Horizon 2020 funded project FREME (Open Framework of e-services for Multilingual and Semantic Enrichment of Digital Content). This is a step towards designing a Semantic Web-aware Machine Translation (MT) system and keeping SMT algorithms up-to-date with the current stage of web development (Web 3.0).  
Current technologies may lead to a revolution to Computer-Aided Translation (CAT) tools. Most of these technologies, which are behind the Machine Translation (MT) comeback, come from the field of Machine Learning. When these technologies are incorporated as extra supports to the tools used by translators, this new generation of tools may be renamed as Knowledge-Assisted Translation (KAT) tools. We will describe our experience with some of the features that are available in some implementations, but this paper will concentrate on suggesting ‚ÄúRecommended Specifications‚Äù for such tools, by resorting to the capacities of Machine Learning methods, complemented by Artificial Intelligence and Augmented Intelligence, to deal with huge volumes of data. Our starting point is the tasks that translators perform in an interconnected world ‚Äì clients, and human and machine resources. We will then present some of the Machine Learning features that may be used as supports to the work of translators and post-editors. From domain identification to resource management, there are several areas to study. At the end, zooming into the simpler editing tasks, there are complex theoretical and technological issues that are worth discussing, because they are at the centre of the adaptation that these tools should undergo. 
This is a study to combine a number of existing technologies with newly developed tools to create an automatic tool to assist with corpus collection for machine translation. This study aims to combine technologies for domain classification, domain source identification, and comparable file alignment into a unified tool. The unified tool will be used to make the corpora collection process more focused and efficient and enable a wider variety of sources to be used. 
Up to rather recently Natural Language Processing has not given much attention to modality. As long as the main task was to determined what a text was about (Information Retrieval) or who the participants in an eventuality were (Information Extraction), this neglect was understandable. With the focus moving to questions of natural language understanding and inferencing as well as to sentiment and opinion analysis, it becomes necessary to distinguish between actual and envisioned eventualities and to draw conclusions about the attitude of the writer or speaker towards the eventualities referred to. This means, i.a., to be able to distinguish {`}John went to Paris{'} and {`}John wanted to go to Paris{'}. To do this one has to calculate the effect of different linguistic operators on the eventuality predication.
Classical theories of discourse semantics, such as Discourse Representation Theory (DRT), Dynamic Predicate Logic (DPL), predict that an indefinite noun phrase cannot serve as antecedent for an anaphor if the noun phrase is, but the anaphor is not, in the scope of a modal expression. However, this prediction meets with counterexamples. The phenomenon modal subordination is one of them. In general, modal subordination is concerned with more than two modalities, where the modality in subsequent sentences is interpreted in a context {`}subordinate{'} to the one created by the first modal expression. In other words, subsequent sentences are interpreted as being conditional on the scenario introduced in the first sentence. One consequence is that the anaphoric potential of indefinites may extend beyond the standard limits of accessibility constraints. This paper aims to give a formal interpretation on modal subordination. The theoretical backbone of the current work is Type Theoretic Dynamic Logic (TTDL), which is a Montagovian account of discourse semantics. Different from other dynamic theories, TTDL was built on classical mathematical and logical tools, such as Œª-calculus and Church{'}s theory of types. Hence it is completely compositional and does not suffer from the destructive assignment problem. We will review the basic set-up of TTDL and then present Kratzer{'}s theory on natural language modality. After that, by integrating the notion of conversation background, in particular, the modal base usage, we offer an extension of TTDL (called Modal-TTDL, or M-TTDL in short) which properly deals with anaphora across modality. The formal relation between Modal-TTDL and TTDL will be discussed as well. We uncover the difficulty of specific sense distinctions by investigating distributional bias and reducing the sparsity of existing small-scale corpora used in prior work. We build a semantically enriched model for modal sense classification by designing novel features related to lexical, proposition-level and discourse-level semantic factors. Besides improved classification performance, closer examination of interpretable feature sets unveils relevant semantic and contextual factors in modal sense classification. Finally, we investigate genre effects on modal sense distribution and how they affect classification performance. Our investigations uncover the difficulty of specific sense distinctions and how they are affected by training set size and distributional bias. Our large-scale experiments confirm that semantically enriched models outperform models built on shallow feature sets. Cross-genre experiments shed light on differences in sense distributions across genres and confirm that semantically enriched models have high generalization capacity, especially in unstable distributional settings.
Modal verbs have different interpretations depending on their context. Their sense categories {--} epistemic, deontic and dynamic {--} provide important dimensions of meaning for the interpretation of discourse. Previous work on modal sense classification achieved relatively high performance using shallow lexical and syntactic features drawn from small-size annotated corpora. Due to the restricted empirical basis, it is difficult to assess the particular difficulties of modal sense classification and the generalization capacity of the proposed models. In this work we create large-scale, high-quality annotated corpora for modal sense classification using an automatic paraphrase-driven projection approach. Using the acquired corpora, we investigate the modal sense classification task from different perspectives.
In this paper we present current work on the design and validation of a linguistically-motivated annotation model of modality in English and Spanish in the context of the MULTINOT project. Our annotation model captures four basic modal meanings and their subtypes, on the one hand, and provides a fine-grained characterisation of the syntactic realisations of those meanings in English and Spanish, on the other. We validate the modal tagset proposed through an agreement study performed on a bilingual sample of four hundred sentences extracted from original texts of the MULTINOT corpus, and discuss the difficult cases encountered in the annotation experiment. We also describe current steps in the implementation of the proposed scheme for the large-scale annotation of the bilingual corpus using both automatic and manual procedures.
We investigate modality in Portuguese and we combine a linguistic perspective with an application-oriented perspective on modality. We design an annotation scheme reflecting theoretical linguistic concepts and apply this schema to a small corpus sample to show how the scheme deals with real world language usage. We present two schemas for Portuguese, one for spoken Brazilian Portuguese and one for written European Portuguese. Furthermore, we use the annotated data not only to study the linguistic phenomena of modality, but also to train a practical text mining tool to detect modality in text automatically. The modality tagger uses a machine learning classifier trained on automatically extracted features from a syntactic parser. As we only have a small annotated sample available, the tagger was evaluated on 11 modal verbs that are frequent in our corpus and that denote more than one modal meaning. Finally, we discuss several valuable insights into the complexity of the semantic concept of modality that derive from the process of manual annotation of the corpus and from the analysis of the results of the automatic labeling: ambiguity and the semantic and syntactic properties typically associated to one modal meaning in context, and also the interaction of modality with negation and focus. The knowledge gained from the manual annotation task leads us to propose a new unified scheme for modality that applies to the two Portuguese varieties and covers both written and spoken data.
Modal auxiliaries have different readings, depending on the context in which they occur (Kratzer, 1981). Several projects have attempted to classify uses of modal auxiliaries in corpora according to their reading using supervised machine learning techniques (e.g., Rubinstein et al., 2013, Ruppenhofer {\&} Rehbein, 2012). In each study, traditional taxonomic labels, such as {`}epistemic{'} and {`}deontic{'} are used by human annotators to label instances of modal auxiliaries in a corpus. In order to achieve higher agreement among annotators, results in these previous studies are reported after collapsing some of the initial categories. The results show that human annotators have fairly good agreement on some of the categories, such as whether or not a use is epistemic, but poor agreement on others. They also show that annotators agree more on modals such as might than on modals such as could. In this study, we used traditional taxonomic categories on sentences containing modal auxiliary verbs that were randomly extracted from the English Gigaword 4th edition corpus (Parker et al., 2009). The lowest inner-annotator agreement using traditional taxonomic labels occurred with uses of could, with raw agreements of 42{\%}‚àí48{\%} (Œ∫ = 0.196‚àí0.259), compared to might, for instance, with raw agreement of 98{\%}. In response to the low numbers, rather than collapsing traditional categories, we tried a new method of classifying uses of could with respect to where the reading situates the eventuality being described relative to the speech time. For example, the sentence {`}Jess could swim.{'} is about a swimming eventuality in the past leading up to the time of speech, if it is read as being an ability. The sentence is about a swimming eventuality in the future, if it is read as being a statement about a possibility. The classification labels we propose are crucial in separating uses of could that have actuality inferences (Bhatt, 1999, Hacquard, 2006) from uses that do not. For the temporal location of the event described by a use of could, using four category labels, we achieved 73{\%}‚àí90{\%} raw agreement (Œ∫ = 0.614‚àí0.744). Sequence of tense contexts (Abusch, 1997) present a major factor in the difficulty of determining the temporal properties present in uses of could. Among three annotators, we achieved raw agreement scores of 89{\%}‚àí96{\%}(Œ∫ =0.779‚àí0.919{\%}) on identification of sequence of tense contexts. We discuss the role of our findings with respect to textual entailment.
Verbal irony, or sarcasm, presents a significant technical and conceptual challenge when it comes to automatic detection. Moreover, it can be a disruptive factor in sentiment analysis and opinion mining, because it changes the polarity of a message implicitly. Extant methods for automatic detection are mostly based on overt clues to ironic intent such as hashtags, also known as irony markers. In this paper, we investigate whether people who know each other make use of irony markers less often than people who do not know each other. We trained a machinelearning classifier to detect sarcasm in Twitter messages (tweets) that were addressed to specific users, and in tweets that were not addressed to a particular user. Human coders analyzed the top-1000 features found to be most discriminative into ten categories of irony markers. The classifier was also tested within and across the two categories. We find that tweets with a user mention contain fewer irony markers than tweets not addressed to a particular user. Classification experiments confirm that the irony in the two types of tweets is signaled differently. The within-category performance of the classifier is about 91{\%} for both categories, while cross-category experiments yield substantially lower generalization performance scores of 75{\%} and 71{\%}. We conclude that irony markers are used more often when there is less mutual knowledge between sender and receiver. Senders addressing other Twitter users less often use irony markers, relying on mutual knowledge which should lead the receiver to infer ironic intent from more implicit clues. With regard to automatic detection, we conclude that our classifier is able to detect ironic tweets addressed at another user as reliably as tweets that are not addressed at at a particular person.
Verb phrase (VP) ellipsis is the omission of a verb phrase whose meaning can be reconstructed from the linguistic or real-world context. It is licensed in English by auxiliary verbs, often modal auxiliaries: She can go to Hawaii but he can{'}t [e]. This paper describes a system called ViPER (VP Ellipsis Resolver) that detects and resolves VP ellipsis, relying on linguistic principles such as syntactic parallelism, modality correlations, and the delineation of core vs. peripheral sentence constituents. The key insight guiding the work is that not all cases of ellipsis are equally difficult: some can be detected and resolved with high confidence even before we are able to build systems with human-level semantic and pragmatic understanding of text.
Quantification (see e.g. Peters and Westerst Ãäahl, 2006) is probably one of the most extensively studied phenomena in formal semantics. But because of the specific representation of meaning assumed by modeltheoretic semantics (one where a true model of the world is a priori available), research in the area has primarily focused on one question: what is the relation of a quantifier to the truth value of a sentence? In contrast, relatively little has been said about the way the underlying model comes about, and its relation to individual speakers{'} conceptual knowledge. In this paper, we make a first step in investigating how native speakers of English model relations between non-grounded sets, by observing how they quantify simple statements. We first give some motivation for our task, from both a theoretical linguistic and computational semantic point of view ({\S}2). We then describe our annotation setup ({\S}3) and follow on with an analysis of the produced dataset, conducting a quantitative evaluation which includes inter-annotator agreement for different classes of predicates ({\S}4). We observe that there is significant agreement between speakers but also noticeable variations. We posit that in settheoretic terms, there are as many worlds as there are speakers ({\S}5), but the overwhelming use of underspecified quantification in ordinary language covers up the individual differences that might otherwise be observed.
This paper proposes a method for improving the results of a statistical Machine Translation system using boundedness, a pragmatic component of the verbal phrase{'}s lexical aspect. First, the paper presents manual and automatic annotation experiments for lexical aspect in EnglishFrench parallel corpora. It will be shown that this aspectual property is identified and classified with ease both by humans and by automatic systems. Second, Statistical Machine Translation experiments using the boundedness annotations are presented. These experiments show that the information regarding lexical aspect is useful to improve the output of a Machine Translation system in terms of better choices of verbal tenses in the target language, as well as better lexical choices. Ultimately, this work aims at providing a method for the automatic annotation of data with boundedness information and at contributing to Machine Translation by taking into account linguistic data.
Abstract syntax is a semantic tree representation that lies between parse trees and logical forms. It abstracts away from word order and lexical items, but contains enough information to generate both surface strings and logical forms. Abstract syntax is commonly used in compilers as an intermediate between source and target languages. Grammatical Framework (GF) is a grammar formalism that generalizes the idea to natural languages, to capture cross-lingual generalizations and perform interlingual translation. As one of the main results, the GF Resource Grammar Library (GF-RGL) has implemented a shared abstract syntax for over 30 languages. Each language has its own set of concrete syntax rules (morphology and syntax), by which it can be generated from the abstract syntax and parsed into it. This paper presents a conversion method from abstract syntax trees to dependency trees. The method is applied for converting GF-RGL trees to Universal Dependencies (UD), which uses a common set of labels for different languages. The correspondence between GF-RGL and UD turns out to be good, and the relatively few discrepancies give rise to interesting questions about universality. The conversion also has potential for practical applications: (1) it makes the GF parser usable as a rule-based dependency parser; (2) it enables bootstrapping UD treebanks from GF treebanks; (3) it defines formal criteria to assess the informal annotation schemes of UD; (4) it gives a method to check the consistency of manually annotated UD trees with respect to the annotation schemes; (5) it makes information from UD treebanks available.
Hypernym extraction from Wikip{\'e}dia The volume of available documents on the Web continues to increase, the texts contained in these documents are rich information describing concepts and relationships between concepts specific to a particular field. In this paper, we propose and exploit an hypernymy extractor based on lexico-syntactic patterns designed for Wikipedia semi-structured pages, especially the disambiguation pages, to enrich a knowledge base as BabelNet and DBPedia. The results show a precision of 0.68 and a recall of 0.75 for the patterns that we have defined, and an enrichment rate up to 33{\%} for both BabelNet and DBP{\'e}dia semantic resources.
Approximate summary optimisation for selections of ROUGE It is standard to measure automatic summariser performance using the ROUGE metric. Unfortunately, ROUGE is not appropriate for unsupervised summarisation approaches. On the other hand, we show that it is possible to optimise approximately for ROUGE-n by using a document-weighted ROUGE objective. Doing so results in state-of-the-art summariser performance for single and multiple document summaries for both English and French. This is despite a non-correlation of the documentweighted ROUGE metric with human judgments, unlike the original ROUGE metric. These findings suggest a theoretical approximation link between the two metrics.
Comparing Named-Entity Recognizers in a Targeted Domain : Handcrafted Rules vs. Machine Learning Named-Entity Recognition concerns the classification of textual objects in a predefined set of categories such as persons, organizations, and localizations. While Named-Entity Recognition is well studied since 20 years, the application to specialized domains still poses challenges for current systems. We developed a rule-based system and two machine learning approaches to tackle the same task : recognition of product names, brand names, etc., in the domain of Cosmetics, for French. Our systems can thus be compared under ideal conditions. In this paper, we introduce both systems and we compare them.
In this paper we investigate the impact of the integration of context into dialogue translation. We present a new contextual parallel corpus of television subtitles and show how taking into account speaker gender can significantly improve machine translation quality in terms of B LEU and M ETEOR scores. We perform a manual analysis, which suggests that these improvements are not necessary related to the morphological consequences of speaker gender, but to more general linguistic divergences.
This paper investigates self trained cross-show speaker diarization applied to collections of French TV archives, based on an \textit{i-vector/PLDA} framework. The parameters used for i-vectors extraction and PLDA scoring are trained in a unsupervised way, using the data of the collection itself. Performances are compared, using combinations of target data and external data for training. The experimental results on two distinct target corpora show that using data from the corpora themselves to perform unsupervised iterative training and domain adaptation of PLDA parameters can improve an existing system, trained on external annotated data. Such results indicate that performing speaker indexation on small collections of unlabeled audio archives should only rely on the availability of a sufficient external corpus, which can be specifically adapted to every target collection. We show that a minimum collection size is required to exclude the use of such an external bootstrap.
Linguistic Linked Open Data (LLOD) is a technology and a movement in several disciplines working with language resources, including Natural Language Processing, general linguistics, computational lexicography and the localization industry. This talk describes basic principles of Linguistic Linked Open Data and their application to linguistically annotated corpora, it summarizes the current status of the Linguistic Linked Open Data cloud and gives an overview over selected LLOD vocabularies and their uses.
Thirty years ago, in order to get past roadblocks in Machine Translation and Automatic Speech Recognition, DARPA invented a new way to organize and manage technological R{\&}D : a {``}common task{''} is defined by a formal quantitative evaluation metric and a body of shared training data, and researchers join an open competition to compare approaches. Over the past three decades, this method has produced steadily improving technologies, with many practical applications now possible. And Moore{'}s law has created a sort of digital shadow universe, which increasingly mirrors the real world in flows and stores of bits, while the same improvements in digital hardware and software make it increasingly easy to pull content out of the these rivers and oceans of information. It{'}s natural to be excited about these technologies, where we can see an open road to rapid improvements beyond the current state of the art, and an explosion of near-term commercial applications. But there are some important opportunities in a less obvious direction. Several areas of scientific and humanistic research are being revolutionized by the application of Human Language Technology. At a minimum, orders of magnitude more data can be addressed with orders of magnitude less effort - but this change also transforms old theoretical questions, and poses new ones. And eventually, new modes of research organization and funding are likely to emerge..
LNE-Visu : a tool to explore and visualize multimedia data LNE-Visu is a tool to explore and visualize multimedia data created for the LNE evaluation campaigns. 3 functionalities are available: explore and select data, visualize and listen data, apply significance tests
The wordnet contains part-of-speech categories such as noun, verb, adjective and adverb. In Sanskrit, there is no formal distinction among nouns, adjectives and adverbs. This poses the question, is an adverb a separate category in Sanskrit? If not, then how do we accommodate it in a lexical resource? To investigate the issue, we attempt to study the complex nature of adverbs in Sanskrit and the policies adopted by Sanskrit lexicographers that would guide us in storing them in the Sanskrit wordnet.
Russian Language is currently poorly supported with WordNet-like resources. One of the new efforts for building Russian WordNet involves mining the monolingual dictionaries. While most steps of the building process are straightforward, word sense disambiguation (WSD) is a source of problems. Due to limited word context specific WSD mechanism is required for each kind of relations mined. This paper describes the WSD method used for mining hypernym relations. First part of the paper explains the main reasons for choosing monolingual dictionaries as the primary source of information for Russian language WordNet and states some problems faced during the information extraction. The second part defines algorithm used to extract hyponym-hypernym pair. The third part describes the algorithm used for WSD.
This paper describes an electronic variant of popular word game Alias where people have to guess words according to their associations via synonyms, opposites, hyperonyms etc. Lexical data comes from the Estonian Wordnet. The computer game Alias which draws information from Estonian Wordnet is useful at least for two reasons: it creates an opportunity to learn language through play, and it helps to evaluate and improve the quality of Estonian Wordnet.
Since the inception of the SENSEVAL evaluation exercises there has been a great deal of recent research into Word Sense Disambiguation (WSD). Over the years, various supervised, unsupervised and knowledge based WSD systems have been proposed. Beating the first sense heuristics is a challenging task for these systems. In this paper, we present our work on Most Frequent Sense (MFS) detection using Word Embeddings and BabelNet features. The semantic features from BabelNet viz., synsets, gloss, relations, etc. are used for generating sense embeddings. We compare word embedding of a word with its sense embeddings to obtain the MFS with the highest similarity. The MFS is detected for six languages viz., English, Spanish, Russian, German, French and Italian. However, this approach can be applied to any language provided that word embeddings are available for that language.
The data compiled through many Wordnet projects can be a rich source of seed information for a multilingual dictionary. However, the original Princeton WordNet was not intended as a dictionary per se, and spawning other languages from it introduces inherent ambiguity that confounds precise inter-lingual linking. This paper discusses a new presentation of existing Wordnet data that displays joints (distance between predicted links) and substitution (degree of equivalence between confirmed pairs) as a two-tiered horizontal ontology. Improvements to make Wordnet data function as lexicography include term-specific English definitions where the topical synset glosses are inadequate, validation of mappings between each member of an English synset and each member of the synsets from other languages, removal of erroneous translation terms, creation of own-language definitions for the many languages where those are absent, and validation of predicted links between non-English pairs. The paper describes the current state and future directions of a system to crowdsource human review and expansion of Wordnet data, using gamification to build consensus validated, dictionary caliber data for languages now in the Global WordNet as well as new languages that do not have formal Wordnet projects of their own.
The Ancient Greek WordNet (AGWN) and the Dynamic Lexicon (DL) are multilingual resources to study the lexicon of Ancient Greek texts and their translations. Both AGWN and DL are works in progress that need accuracy improvement and manual validation. After a detailed description of the current state of each work, this paper illustrates a methodology to cross AGWN and DL data, in order to mutually score the items of each resource according to the evidence provided by the other resource. The training data is based on the corpus of the Digital Fragmenta Historicorum Graecorum (DFHG), which includes ancient Greek texts with Latin translations.
Semantic similarity and relatedness measures play an important role in natural language processing applications. In this paper, we present the IndoWordNet::Similarity tool and interface, designed for computing the semantic similarity and relatedness between two words in IndoWordNet. A java based tool and a web interface have been developed to compute this semantic similarity and relatedness. Also, Java API has been developed for this purpose. This tool, web interface and the API are made available for the research purpose.
Supervised methods for Word Sense Disambiguation (WSD) benefit from high-quality sense-annotated resources, which are lacking for many languages less common than English. There are, however, several multilingual parallel corpora that can be inexpensively annotated with senses through cross-lingual methods. We test the effectiveness of such an approach by attempting to disambiguate English texts through their translations in Italian, Romanian and Japanese. Specifically, we try to find the appropriate word senses for the English words by comparison with all the word senses associated to their translations. The main advantage of this approach is in that it can be applied to any parallel corpus, as long as large, high-quality inter-linked sense inventories exist for all the languages considered.
This paper introduces the motivation for and design of the Collaborative InterLingual Index (CILI). It is designed to make possible coordination between multiple loosely coupled wordnet projects. The structure of the CILI is based on the Interlingual index first proposed in the EuroWordNet project with several pragmatic extensions: an explicit open license, definitions in English and links to wordnets in the Global Wordnet Grid.
YARN (Yet Another RussNet), a project started in 2013, aims at creating a large open WordNet-like thesaurus for Russian by means of crowdsourcing. The first stage of the project was to create noun synsets. Currently, the resource comprises 48K+ word entries and 44K+ synsets. More than 200 people have taken part in assembling synsets throughout the project. The paper describes the linguistic, technical, and organizational principles of the project, as well as the evaluation results, lessons learned, and the future plans.
We describe the implementation of a short answer extraction system. It consists of a simple sentence selection front-end and a two phase approach to answer extraction from a sentence. In the first phase sentence classification is performed with a classifier trained with the passive aggressive algorithm utilizing the UIUC dataset and taxonomy and a feature set including word vectors. This phase outperforms the current best published results on that dataset. In the second phase, a sieve algorithm consisting of a series of increasingly general extraction rules is applied, using WordNet to find word types aligned with the UIUC classifications determined in the first phase. Some very preliminary performance metrics are presented.
Semantic relations between words are key to building systems that aim to understand and manipulate language. For English, the {``}de facto{''} standard for representing this kind of knowledge is Princeton{'}s WordNet. Here, we describe the wordnet-like resources currently available for Portuguese: their origins, methods of creation, sizes, and usage restrictions. We start tackling the problem of comparing them, but only in quantitative terms. Finally, we sketch ideas for potential collaboration between some of the projects that produce Portuguese wordnets.
In the context of a student software project we are investigating the use of WordNet for improving the automatic detection and classification of actors (or characters) mentioned in folktales. Our starting point is the book {``}Classification of International Folktales{''}, out of which we extract text segments that name the different actors involved in tales, taking advantage of patterns used by its author, Hans-Jo Ãàrg Uther. We apply on those text segments functions that are implemented in the NLTK interface to WordNet in order to obtain lexical semantic information to enrich the original naming of characters proposed in the {``}Classification of International Folktales{''} and to support their translation in other languages.
In this paper, we present methods of extraction of multi-word lexical units (MWLUs) from large text corpora and their description in plWordNet 3.0. MWLUs are filtered from collocations of the structural type Noun+Adjective (NA).
This paper aims at a morpho-semantic analysis of 2461 Persian derived nouns, documented in FarsNet addressing computational codification via formulating specific morpho-semantic relations between classes of derived nouns and their bases. Considering the ultimate aim of the study, FarsNet derived nouns included 12 most productive suffixes have been analysed and as a consequence 45 morpho-semantic patterns were distinguished leading to creation of 17 morpho-semantic relations. The approach includes a close examination of beginners, grammatical category and part of speech shifts of bases undergoing the derivation process. In this research the morpho-semantic relations are considered at the word level and not at the synset level which will represent a cross-lingual validity, even if the morphological aspect of the relation is not the same in the studied languages. The resulting morpho-semantic formulations notably increase linguistic and operative competence and performance of FarsNet while is considered an achievement in Persian descriptive morphology and its codification.
We present a methodology for building lexical sets for argument slots of Italian verbs. We start from an inventory of semantically typed Italian verb frames and through a mapping to WordNet we automatically annotate the sets of fillers for the argument positions in a corpus of sentences. We evaluate both a baseline algorithm and a syntax driven algorithm and show that the latter performs significantly better in terms of precision.
WordNet represents polysemous terms by capturing the different meanings of these terms at the lexical level, but without giving emphasis on the polysemy types such terms belong to. The state of the art polysemy approaches identify several polysemy types in WordNet but they do not explain how to classify and organize them. In this paper, we present a novel approach for classifying the polysemy types which exploits taxonomic principles which in turn, allow us to discover a set of polysemy structural patterns.
Although there are currently several versions of Princeton WordNet for different languages, the lack of development of some of these versions does not make it possible to use them in different Natural Language Processing applications. So is the case of the Spanish Wordnet contained in the Multilingual Central Repository (MCR), which we tried unsuccessfully to incorporate into an anaphora resolution application and also in search terms expansion. In this situation, different strategies to improve MCR Spanish WordNet coverage were put forward and tested, obtaining encouraging results. A specific process was conducted to increase the number of adverbs, and a few simple processes were applied which made it possible to increase, at a very low cost, the number of terms in the Spanish WordNet. Finally, a more complex method based on distributional semantics was proposed, using the relations between English Wordnet synsets, also returning positive results.
While gender identities in the Western world are typically regarded as binary, our previous work (Hicks et al., 2015) shows that there is more lexical variety of gender identity and the way people identify their gender. There is also a growing need to lexically represent this variety of gender identities. In our previous work, we developed a set of tools and approaches for analyzing Twitter data as a basis for generating hypotheses on language used to identify gender and discuss gender-related issues across geographic regions and population groups in the U.S.A. In this paper we analyze the coverage and relative frequency of the word forms in our Twitter analysis with respect to the National Transgender Discrimination Survey data set, one of the most comprehensive data sets on transgender, gender non-conforming, and gender variant people in the U.S.A. We then analyze the coverage of WordNet, a widely used lexical database, with respect to these identities and discuss some key considerations and next steps for adding gender identity words and their meanings to WordNet.
Here we report the construction of a wordnet for Mansi, an endangered minority language spoken in Russia. We will pay special attention to challenges that we encountered during the building process, among which the most important ones are the low number of native speakers, the lack of thesauri and the bear language. We will discuss our solutions to these issues, which might have some theoretical implications for the methodology of wordnet building in general.
This paper presents a standalone spell corrector, WNSpell, based on and written for WordNet. It is aimed at generating the best possible suggestion for a mistyped query but can also serve as an all-purpose spell corrector. The spell corrector consists of a standard initial correction system, which evaluates word entries using a multifaceted approach to achieve the best results, and a semantic recognition system, wherein given a related word input, the system will adjust the spelling suggestions accordingly. Both feature significant performance improvements over current context-free spell correctors.
India is a country with 22 officially recognized languages and 17 of these have WordNets, a crucial resource. Web browser based interfaces are available for these WordNets, but are not suited for mobile devices which deters people from effectively using this resource. We present our initial work on developing mobile applications and browser extensions to access WordNets for Indian Languages. Our contribution is two fold: (1) We develop mobile applications for the Android, iOS and Windows Phone OS platforms for Hindi, Marathi and Sanskrit WordNets which allow users to search for words and obtain more information along with their translations in English and other Indian languages. (2) We also develop browser extensions for English, Hindi, Marathi, and Sanskrit WordNets, for both Mozilla Firefox, and Google Chrome. We believe that such applications can be quite helpful in a classroom scenario, where students would be able to access the WordNets as dictionaries as well as lexical knowledge bases. This can help in overcoming the language barrier along with furthering language understanding.
WordNet has proved to be immensely useful for Word Sense Disambiguation, and thence Machine translation, Information Retrieval and Question Answering. It can also be used as a dictionary for educational purposes. The semantic nature of concepts in a WordNet motivates one to try to express this meaning in a more visual way. In this paper, we describe our work of enriching IndoWordNet with image acquisitions from the OpenClipArt library. We describe an approach used to enrich WordNets for eighteen Indian languages. Our contribution is three fold: (1) We develop a system, which, given a synset in English, finds an appropriate image for the synset. The system uses the OpenclipArt library (OCAL) to retrieve images and ranks them. (2) After retrieving the images, we map the results along with the linkages between Princeton WordNet and Hindi WordNet, to link several synsets to corresponding images. We choose and sort top three images based on our ranking heuristic per synset. (3) We develop a tool that allows a lexicographer to manually evaluate these images. The top images are shown to a lexicographer by the evaluation tool for the task of choosing the best image representation. The lexicographer also selects the number of relevant images. Using our system, we obtain an Average Precision (P @ 3) score of 0.30.
We propose the use of WordNet synsets in a syntax-based reordering model for hierarchical statistical machine translation (HPB-SMT) to enable the model to generalize to phrases not seen in the training data but that have equivalent meaning. We detail our methodology to incorporate synsets{'} knowledge in the reordering model and evaluate the resulting WordNet-enhanced SMT systems on the English-to-Farsi language direction. The inclusion of synsets leads to the best BLEU score, outperforming the baseline (standard HPB-SMT) by 0.6 points absolute.
Collaboratively created lexical resources is a trending approach to creating high quality thesauri in a short time span at a remarkably low price. The key idea is to invite non-expert participants to express and share their knowledge with the aim of constructing a resource. However, this approach tends to be noisy and error-prone, thus making data cleansing a highly topical task to perform. In this paper, we study different techniques for synset deduplication including machine- and crowd-based ones. Eventually, we put forward an approach that can solve the deduplication problem fully automatically, with the quality comparable to the expert-based approach.
This paper presents a machine learning method for automatic identification and classification of morphosemantic relations (MSRs) between verb and noun synset pairs in the Bulgarian WordNet (BulNet). The core training data comprise 6,641 morphosemantically related verb{--}noun literal pairs from BulNet. The core dataset were preprocessed quality-wise by applying validation and reorganisation procedures. Further, the data were supplemented with negative examples of literal pairs not linked by an MSR. The designed supervised machine learning method uses the RandomTree algorithm and is implemented in Java with the Weka package. A set of experiments were performed to test various approaches to the task. Future work on improving the classifier includes adding more training data, employing more features, and fine-tuning. Apart from the language specific information about derivational processes, the proposed method is language independent.
Many new wordnets in the world are constantly created and most take the original Princeton WordNet (PWN) as their starting point. This arguably central position imposes a responsibility on PWN to ensure that its structure is clean and consistent. To validate PWN hierarchical structures we propose the application of a system of test patterns. In this paper, we report on how to validate the PWN hierarchies using the system of test patterns. In sum, test patterns provide lexicographers with a very powerful tool, which we hope will be adopted by the global wordnet community.
New concepts and semantic relations are constantly added to Estonian Wordnet (EstWN) to increase its size. In addition to this, with the use of test patterns, the validation of EstWN hierarchies is also performed. This parallel work was carried out over the past four years (2011-2014) with 10 different EstWN versions (60-70). This has been a collaboration between the creators of test patterns and the lexicographers currently working on EstWN. This paper describes the usage of test patterns from the points of views of information scientists (the creators of test patterns) as well as the users (lexicographers). Using EstWN as an example, we illustrate how the continuous use of test patterns has led to significant improvement of the semantic hierarchies in EstWN.
In promoting a multilingual South Africa, the government is encouraging people to speak more than one language. In order to comply with this initiative, people choose to learn the languages which they do not speak as home language. The African languages are mostly chosen because they are spoken by the majority of the country{'}s population. Most words in these languages have many possible senses. This phenomenon tends to pose problems to people who want to learn these languages. This article argues that the African WordNet may the best tool to address the problem of sense discrimination. The focus of the argument will be on the primary sense of the word {`}hand{'}, which is part of the body, as lexicalized in three indigenous languages spoken in South Africa, namely, Tshiven·∏ìa, Sesotho sa Leboa and isiZulu. A brief historical background of the African WordNet will be provided, followed by the definition of the word {`}hand{'} in the three languages and the analysis of the word in context. Lastly, the primary sense of the word {`}hand{'} across the three languages will be discussed.
In this article we present an expansion of the supersense inventory. All new super-senses are extensions of members of the current inventory, which we postulate by identifying semantically coherent groups of synsets. We cover the expansion of the already-established supernsense inventory for nouns and verbs, the addition of coarse supersenses for adjectives in absence of a canonical supersense inventory, and super-senses for verbal satellites. We evaluate the viability of the new senses examining the annotation agreement, frequency and co-ocurrence patterns.
Adverbs are seldom well represented in wordnets. Princeton WordNet, for example, derives from adjectives practically all its adverbs and whatever involvement they have. GermaNet stays away from this part of speech. Adverbs in plWordNet will be emphatically present in all their semantic and syntactic distinctness. We briefly discuss the linguistic background of the lexical system of Polish adverbs. We describe an automated generator of accurate candidate adverbs, and introduce the lexicographic procedures which will ensure high consistency of wordnet editors{'} decisions about adverbs.
The aim of this paper is to show a language-independent process of creating a new semantic relation between adjectives and nouns in wordnets. The existence of such a relation is expected to improve the detection of figurative language and sentiment analysis (SA). The proposed method uses an annotated corpus to explore the semantic knowledge contained in linguistic constructs performing as the rhetorical figure Simile. Based on the frequency of occurrence of similes in an annotated corpus, we propose a new relation, which connects the noun synset with the synset of an adjective representing that noun{'}s specific attribute. We elaborate on adding this new relation in the case of the Serbian WordNet (SWN). The proposed method is evaluated by human judgement in order to determine the relevance of automatically selected relation items. The evaluation has shown that 84{\%} of the automatically selected and the most frequent linguistic constructs, whose frequency threshold was equal to 3, were also selected by humans.
This paper describes our attempts to add Indonesian definitions to synsets in the Wordnet Bahasa (Nurril Hirfana Mohamed Noor et al., 2011; Bond et al., 2014), to extract semantic relations between lemmas and definitions for nouns and verbs, such as synonym, hyponym, hypernym and instance hypernym, and to generally improve Wordnet. The original, somewhat noisy, definitions for Indonesian came from the Asian Wordnet project (Riza et al., 2010). The basic method of extracting the relations is based on Bond et al. (2004). Before the relations can be extracted, the definitions were cleaned up and tokenized. We found that the definitions cannot be completely cleaned up because of many misspellings and bad translations. However, we could identify four semantic relations in 57.10{\%} of noun and verb definitions. For the remaining 42.90{\%}, we propose to add 149 new Indonesian lemmas and make some improvements to Wordnet Bahasa and Wordnet in general.
This paper presents a linguistic account of the lexical semantics of body parts in African WordNet, with special reference to Northern Sotho. It focuses on external human body parts synsets in Northern Sotho. The paper seeks to support the effectiveness of African WordNet as a resource for services such as in the healthcare and medical field in South Africa. It transpired from this exploration that there is either a one-to-one correspondence or some form of misalignment of lexicalisation with regard to the sample of examined synsets. The paper concludes by making suggestions on how African WordNet can deal with such semantic misalignments in order to improve its efficiency as a resource for the targeted purpose.
In order to overcome the lack of medical corpora, we have developed a WordNet for Medical Events (WME) for identifying medical terms and their sense related information using a seed list. The initial WME resource contains 1654 medical terms or concepts. In the present research, we have reported the enhancement of WME with 6415 number of medical concepts along with their conceptual features viz. Parts-of-Speech (POS), gloss, semantics, polarity, sense and affinity. Several polarity lexicons viz. SentiWordNet, SenticNet, Bing Liu{'}s subjectivity list and Taboda{'}s adjective list were introduced with WordNet synonyms and hyponyms for expansion. The semantics feature guided us to build a semantic co-reference relation based network between the related medical concepts. These features help to prepare a medical concept network for better sense relation based visualization. Finally, we evaluated with respect to Adaptive Lesk Algorithm and conducted an agreement analysis for validating the expanded WME resource.
In languages such as Chinese, classifiers (CLs) play a central role in the quantification of noun-phrases. This can be a problem when generating text from input that does not specify the classifier, as in machine translation (MT) from English to Chinese. Many solutions to this problem rely on dictionaries of noun-CL pairs. However, there is no open large-scale machine-tractable dictionary of noun-CL associations. Many published resources exist, but they tend to focus on how a CL is used (e.g. what kinds of nouns can be used with it, or what features seem to be selected by each CL). In fact, since nouns are open class words, producing an exhaustive definite list of noun-CL associations is not possible, since it would quickly get out of date. Our work tries to address this problem by providing an algorithm for automatic building of a frequency based dictionary of noun-CL pairs, mapped to concepts in the Chinese Open Wordnet (Wang and Bond, 2013), an open machine-tractable dictionary for Chinese. All results will released under an open license.
WordNet plays a significant role in Linked Open Data (LOD) cloud. It has numerous application ranging from ontology annotation to ontology mapping. IndoWordNet is a linked WordNet connecting 18 Indian language WordNets with Hindi as a source WordNet. The Hindi WordNet was initially developed by linking it to English WordNet. In this paper, we present a data representation of IndoWordNet in Web Ontology Language (OWL). The schema of Princeton WordNet has been enhanced to support the representation of IndoWordNet. This IndoWordNet representation in OWL format is now available to link other web resources. This representation is implemented for eight Indian languages.
Wordnets play an important role not only in linguistics but also in natural language processing (NLP). This paper reports major results of a project which aims to construct a wordnet for Vietnamese language. We propose a two-phase approach to the construction of Vietnamese WordNet employing available language resources and ensuring Vietnamese specific linguistic and cultural characteristics. We also give statistical results and analyses to show characteristics of the wordnet.
In this paper we present an extension of the dictionary-based strategy for wordnet construction implemented in the WN-Toolkit. This strategy allows the extraction of information for polysemous English words if definitions and/or semantic relations are present in the dictionary. The WN-Toolkit is a freely available set of programs for the creation and expansion of wordnets using dictionary-based and parallel-corpus based strategies. In previous versions of the toolkit the dictionary-based strategy was only used for translating monosemous English variants. In the experiments we have used Omegawiki and Wiktionary and we present automatic evaluation results for 24 languages that have wordnets in the Open Multilingual Wordnet project. We have used these existing versions of the wordnet to perform an automatic evaluation.
This paper describes a language- independent LESK based approach to Word Sense Disambiguation (WSD), involving also Vector Space Models applied to the Distributional Semantics Hypotesis. In particular this approach tries to solve some issues that come up with less-resourced languages. The approach also addresses the inadequacy of the Most Frequent Sense (MFS) heuristics to fit specific domain corpora.
The paper explores the application of plWordNet, a very large wordnet of Polish, in weakly supervised Word Sense Disambiguation (WSD). Because plWordNet provides only partial descriptions by glosses and usage examples, and does not include sense-disambiguated glosses, PageRank-based WSD methods perform slightly worse than for English. However, we show that the use of weights for the relation types and the order in which lexical units have been added for sense re-ranking can significantly improve WSD precision. The evaluation was done on two Polish corpora (KPWr and Sk{\l}adnica) including manual WSD. We discuss the fundamental difference in the construction of both corpora and very different test results.
It took us nearly ten years to get from no wordnet for Polish to the largest wordnet ever built. We started small but quickly learned to dream big. Now we are about to release plWordNet 3.0-emo {--} complete with sentiment and emotions annotated {--} and a domestic version of Princeton WordNet, larger than WordNet 3.1 by nearly ten thousand newly added words. The paper retraces the road we travelled and talks a little about the future.
We describe Open Dutch WordNet, which has been derived from the Cornetto database, the Princeton WordNet and open source resources. We exploited existing equivalence relations between Cornetto synsets and WordNet synsets in order to move the open source content from Cornetto into WordNet synsets. Currently, Open Dutch Wordnet contains 117,914 synsets, of which 51,588 synsets contain at least one Dutch synonym, which leaves 66,326 synsets still to obtain a Dutch synonym. The average polysemy is 1.5. The resource is currently delivered in XML under the CC BY-SA 4.0 license1 and it has been linked to the Global Wordnet Grid. In order to use the resource, we refer to: https: //github.com/MartenPostma/OpenDutchWordnet.
This paper presents our first attempt at verifying integrity constraints of our openWordnet-PT against the ontology for Wordnets encoding. Our wordnet is distributed in Resource Description Format (RDF) and we want to guarantee not only the syntax correctness but also its semantics soundness.
The semantic network editor DEBVisDic has been used by different development teams to create more than 20 national wordnets. The editor was recently re-developed as a multi-platform web-based application for general semantic networks editing. One of the main advantages, when compared to the previous implementation, lies in the fact that no client-side installation is needed now. Following the successful first phase in building the Open Dutch Wordnet, DEBVisDic was extended with features that allow users to easily create, edit, and share a new (usually national) wordnet without the need of any complicated configuration or advanced technical skills. The DEBVisDic editor provides advanced features for wordnet browsing, editing, and visualization. Apart from the user-friendly web-based application, DEBVisDic also provides an API interface to integrate the semantic network data into external applications.
Sam{\=a}sa or compounds are a regular feature of Indian Languages. They are also found in other languages like German, Italian, French, Russian, Spanish, etc. Compound word is constructed from two or more words to form a single word. The meaning of this word is derived from each of the individual words of the compound. To develop a system to generate, identify and interpret compounds, is an important task in Natural Language Processing. This paper introduces a web based tool - Sam{\=a}sa-Kart{\=a} for producing compound words. Here, the focus is on Sanskrit language due to its richness in usage of compounds; however, this approach can be applied to any Indian language as well as other languages. IndoWordNet is used as a resource for words to be compounded. The motivation behind creating compound words is to create, to improve the vocabulary, to reduce sense ambiguity, etc. in order to enrich the WordNet. The Sam{\=a}sa-Kart{\=a} can be used for various applications viz., compound categorization, sandhi creation, morphological analysis, paraphrasing, synset creation, etc.
The Arabic WordNet project has provided the Arabic Natural Language Processing (NLP) community with the first WordNet-compliant resource. It allowed new possibilities in terms of building sophisticated NLP applications related to this Semitic language. In this paper, we present the new content added to this resource, using semi-automatic techniques, and validated by Arabic native-speaker lexicographers. We also present how this content helps in the implementation of new Arabic NLP applications, especially for Question Answering (QA) systems. The obtained results show the contribution of the added content. The resource, fully transformed into the standard Lexical Markup Framework (LMF), is made available for the community.
This paper presents a web interface for wordnets named Hydra for Web which is built on top of Hydra {--} an open source tool for wordnet development {--} by means of modern web technologies. It is a Single Page Application with simple but powerful and convenient GUI. It has two modes for visualisation of the language correspondences of searched (and found) wordnet synsets {--} single and parallel modes. Hydra for web is available at: http://dcl.bas.bg/bulnet/.
This paper presents the results of large-scale noun synset mapping between plWordNet, the wordnet of Polish, and Princeton WordNet, the wordnet of English, which have shown high predominance of inter-lingual hyponymy relation over inter-synonymy relation. Two main sources of such effect are identified in the paper: differences in the methodologies of construction of plWN and PWN and cross-linguistic differences in lexicalization of concepts and grammatical categories between English and Polish. Next, we propose a typology of specific gaps and mismatches across wordnets and a rule-based system of filters developed specifically to scan all I(inter-lingual)-hyponymy links between plWN and PWN. The proposed system, it should be stressed, also enables one to pinpoint the frequencies of the identified gaps and mismatches.
This paper presents a method to compute similarity of folktales based on conceptual overlap at various levels of abstraction as defined in Dutch WordNet. The method is applied on a corpus of Dutch folktales and evaluated using a comparison to traditional folktale similarity analysis based on the Aarne{--}Thompson{--}Uther (ATU) classification system. Document similarity computed by the presented method is in agreement with traditional analysis for a certain amount of folktale pairs, but differs for other pairs. However, it can be argued that the current approach computes an alternative, data-driven type of similarity. Using WordNet instead of a domain-specific ontology or classification system ensures applicability of the method outside of the folktale domain.
This paper presents the Event and Implied Situation Ontology (ESO), a resource which formalizes the pre and post situations of events and the roles of the entities affected by an event. The ontology reuses and maps across existing resources such as WordNet, SUMO, VerbNet, PropBank and FrameNet. We describe how ESO is injected into a new version of the Predicate Matrix and illustrate how these resources are used to detect information in large document collections that otherwise would have remained implicit. The model targets interpretations of situations rather than the semantics of verbs per se. The event is interpreted as a situation using RDF taking all event components into account. Hence, the ontology and the linked resources need to be considered from the perspective of this interpretation model.
We present preliminary work on the mapping of WordNet 3.0 to the Basic Formal Ontology (BFO 2.0). WordNet is a large, widely used semantic network. BFO is a domain-neutral upper-level ontology that represents the types of things that exist in the world and relations between them. BFO serves as an integration hub for more specific ontologies, such as the Ontology for Biomedical Investigations (OBI) and Ontology for Biobanking (OBIB). This work aims at creating a lexico-semantic resource that can be used in NLP tools to perform ontology-related text manipulation tasks. Our semi-automatic mapping method consists in using existing mappings between WordNet and the KYOTO Ontology. The latter allows machines to reason over texts by providing interpretations of the words in ontological terms. Our working hypothesis is that a large portion of WordNet synsets can be semi-automatically mapped to BFO using simple mapping rules from KYOTO to BFO. We evaluate the method on a randomized subset of synsets, examine preliminary results, and discuss challenges related to the method. We conclude with suggestions for future work.
This paper discusses the semantic augmentation of FarsNet - the Persian WordNet - with new relations and structures for verbs. FarsNet1.0, the first Persian WordNet obeys the Structure of Princeton WordNet 2.1. In this paper we discuss FarsNet 2.0 in which new inter-POS relations and verb frames are added. In fact FarsNet2.0 is a combination of WordNet and VerbNet for Persian. It includes more than 30,000 lexical entries arranged in about 20,000 synsets with about 18000 mappings to Princeton WordNet synsets. There ae about 43000 relations between synsets and senses in FarsNet 2.0. It includes verb frames in two levels (syntactic and thematic) for about 200 simple Persian verbs.
For fine-grained sentiment analysis, we need to go beyond zero-one polarity and find a way to compare adjectives (synonyms) that share the same sense. Choice of a word from a set of synonyms, provides a way to select the exact polarity-intensity. For example, choosing to describe a person as benevolent rather than kind1 changes the intensity of the expression. In this paper, we present a sense based lexical resource, where synonyms are assigned intensity levels, viz., high, medium and low. We show that the measure P (s|w) (probability of a sense s given the word w) can derive the intensity of a word within the sense. We observe a statistically significant positive correlation between P(s|w) and intensity of synonyms for three languages, viz., English, Marathi and Hindi. The average correlation scores are 0.47 for English, 0.56 for Marathi and 0.58 for Hindi.
In this paper we present an analysis of different semantic relations extracted from WordNet, Extended WordNet and SemCor, with respect to their role in the task of knowledge-based word sense disambiguation. The experiments use the same algorithm and the same test sets, but different variants of the knowledge graph. The results show that different sets of relations have different impact on the results: positive or negative. The beneficial ones are discussed with respect to the combination of relations and with respect to the test set. The inclusion of inference has only a modest impact on accuracy, while the addition of syntactic relations produces stable improvement over the baselines.
Detection of MultiWord Expressions (MWEs) is one of the fundamental problems in Natural Language Processing. In this paper, we focus on two categories of MWEs - Compound Nouns and Light Verb Constructions. These two categories can be tackled using knowledge bases, rather than pure statistics. We investigate usability of IndoWordNet for the detection of MWEs. Our IndoWordNet based approach uses semantic and ontological features of words that can be extracted from IndoWordNet. This approach has been tested on Indian languages viz., Assamese, Bengali, Hindi, Konkani, Marathi, Odia and Punjabi. Results show that ontological features are found to be very useful for the detection of light verb constructions, while use of semantic properties for the detection of compound nouns is found to be satisfactory. This approach can be easily adapted by other Indian languages. Detected MWEs can be interpolated into WordNets as they help in representing semantic knowledge.
This paper reports the work of creating bilingual mappings in English for certain synsets of Hindi wordnet, the need for doing this, the methods adopted and the tools created for the task. Hindi wordnet, which forms the foundation for other Indian language wordnets, has been linked to the English WordNet. To maximize linkages, an important strategy of using direct and hypernymy linkages has been followed. However, the hypernymy linkages were found to be inadequate in certain cases and posed a challenge due to sense granularity of language. Thus, the idea of creating bilingual mappings was adopted as a solution. A bilingual mapping means a linkage between a concept in two different languages, with the help of translation and/or transliteration. Such mappings retain meaningful representations, while capturing semantic similarity at the same time. This has also proven to be a great enhancement of Hindi wordnet and can be a crucial resource for multilingual applications in natural language processing, including machine translation and cross language information retrieval.
Le and Fokkens (2015) recently showed that taxonomy-based approaches are more reliable than corpus-based approaches in estimating human similarity ratings. On the other hand, distributional models provide much better coverage. The lack of an established similarity metric for adjectives in WordNet is a case in point. I present initial work to establish such a metric, and propose ways to move forward by looking at extensions to WordNet. I show that the shortest path distance between derivationally related forms provides a reliable estimate of adjective similarity. Furthermore, I find that a hybrid method combining this measure with vector-based similarity estimations gives us the best of both worlds: more reliable similarity estimations than vectors alone, but with the same coverage as corpus-based methods.
In this paper, we describe a new and improved Global Wordnet Grid that takes advantage of the Collaborative InterLingual Index (CILI). Currently, the Open Multilingal Wordnet has made many wordnets accessible as a single linked wordnet, but as it used the Princeton Wordnet of English (PWN) as a pivot, it loses concepts that are not part of PWN. The technical solution to this, a central registry of concepts, as proposed in the EuroWordnet project through the InterLingual Index, has been known for many years. However, the practical issues of how to host this index and who decides what goes in remained unsolved. Inspired by current practice in the Semantic Web and the Linked Open Data community, we propose a way to solve this issue. In this paper we define the principles and protocols for contributing to the Grid. We tested them on two use cases, adding version 3.1 of the Princeton WordNet to a CILI based on 3.0 and adding the Open Dutch Wordnet, to validate the current set up. This paper aims to be a call for action that we hope will be further discussed and ultimately taken up by the whole wordnet community.
Writing intended to inform frequently contains references to document entities (DEs), a mixed class that includes orthographically structured items (e.g., illustrations, sections, lists) and discourse entities (arguments, suggestions, points). Such references are vital to the interpretation of documents, but they often eschew identifiers such as {``}Figure 1{''} for inexplicit phrases like {``}in this figure{''} or {``}from these premises{''}. We examine inexplicit references to DEs, termed DE references, and recast the problem of their automatic detection into the determination of relevant word senses. We then show the feasibility of machine learning for the detection of DE-relevant word senses, using a corpus of human-labeled synsets from WordNet. We test cross-domain performance by gathering lemmas and synsets from three corpora: website privacy policies, Wikipedia articles, and Wikibooks textbooks. Identifying DE references will enable language technologies to use the information encoded by them, permitting the automatic generation of finely-tuned descriptions of DEs and the presentation of richly-structured information to readers.
For humans the main functions of a dictionary is to store information concerning words and to reveal it when needed. While readers are interested in the meaning of words, writers look for answers concerning usage, spelling, grammar or word forms (lemma). We will focus here on this latter task: help authors to find the word they are looking for, word they may know but whose form is eluding them. Put differently, we try to build a resource helping authors to overcome the tip-of-the-tongue problem (ToT). Obviously, in order to access a word, it must be stored somewhere (brain, resource). Yet this is by no means sufficient. We will illustrate this here by comparing WordNet (WN) to an equivalent lexical resource bootstrapped from Wikipedia (WiPi). Both may contain a given word, but ease and success of access may be different depending on other factors like quality of the query, proximity, type of connections, etc. Next we will show under what conditions WN is suitable for word access, and finally we will present a roadmap showing the obstacles to be overcome to build a resource allowing the text producer to find the word s/he is looking for.
TectoMT (the deep-linguistic core of Chimera) is an open-source MT system based on the Treex platform for general natural-language processing. TectoMT uses a combination of rule-based and statistical (trained) modules (‚Äúblocks‚Äù in Treex terminology), with a statistical transfer based on HMTM (Hidden Markov Tree Model) at the level of a deep, so-called tectogrammatical representation of sentence structure. In the Chimera combination, TectoMT is complemented by a Moses PB-SMT system (factored setup with additional language models over morphological tags) and optionally also by an automatic postprocessing (correction) component called Depfix. Chimera can be thus characterized as a hybrid system that combines statistical MT with deep linguistic analysis and automatic post-correction system, which is useful especially for translation into inflectionally rich languages. The three systems are combined serially: TectoMT runs first, then an additional Moses phrase table is extracted from TectoMT‚Äôs input and output. The additional table is then used in a weighted combination with a large Moses translation table to produce pre-final output. Depfix then re-parses the output (as well as input) and generates the final output based on rules reflecting morphosyntactic properties of the target language. Chimera was transferred from English‚ÄìCzech to additional three language pairs (English to Dutch, Portuguese and Spanish) within the QTLeap 7th EU project. References Du≈°ek, O., Gomes, L., Nov√°k, M., Popel, M., Rosa, R. (2015). New Language Pairs in TectoMT. Proceedings of the 10th Workshop on Machine Translation, ISBN 978-1-941643-32-7, ACL, Stroudsburg, PA, USA, 98‚Äì104. Rosa, R., Du≈°ek, O., Nov√°k, M., Popel. M. (2015). Translation Model Interpolation for Domain Adaptation in TectoMT. Proceedings of the 1st Deep Machine Translation Workshop, ISBN 978-80-904571-7-1, Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics, Prague, Czech Republic, 89‚Äì96. Bojar, O., Tamchyna, A. (2015). CUNI in WMT15: Chimera Strikes Again. Proceedings of the 10th Workshop on Machine Translation, ISBN 978-1-941643-32-7, ACL, Stroudsburg, PA, USA, 79‚Äì83.  378  Proceedings of the 19th Annual Conference of the EAMT: Projects/Products  WiTKoM - Virtual Sign Language Translator Project Katarzyna BARCZEWSKA1, Jakub GA≈ÅKA1,2, Filip MALAWSKI1, Mariusz MƒÑSIOR1, Dorota SZULC1, Tomasz WILCZY≈ÉSKI1,2, Krzysztof WR√ìBEL1 1AGH University of Science and Technology, Department of Electronics, Poland 2VoicePIN.com Sp. z o. o., Poland jgalka@agh.edu.pl Abstract. WiTKoM (Virtual Sign Language Translator) is an interdisciplinary research project carried out by AGH University of Science and Technology and VoicePIN.com, which aims to create a Polish Sign Language (PJM, pl. Polski Jƒôzyk Migowy) translator. This work was supported by the Polish National Centre for Research and Development ‚Äì Applied Research Program under Grant PBS2/B3/21/2013 tilted Virtual sign language translator. Website: www.witkom.info. Description WiTKoM is intended to promote the social inclusion. Hearing-impaired people constitute a considerable language minority in Poland. PJM is currently experiencing a renaissance, having 50,000‚Äì100,000 users, according to recent statistics. The practical goal of the project is to develop the technology and solutions for communication support for hearing-impaired. Project WiTKoM consists of various separate elements, responsible for the particular stage of Polish to PJM translation, and statement creation by the signing avatar. Within the project, several developments have been made: know-how, technologies, and automatic PJM translation software. The developed technology includes: ÔÇ∑ real-time user-independent gesture recognition system; ÔÇ∑ PL‚ÄìPJM translator, from Polish sentence analysis, through machine translation methods, to avatar transcription system employing HamNoSys; ÔÇ∑ mobile application prototype for gesture recognition; ÔÇ∑ accelerometer-based sensor glove for gesture motion acquisition; ÔÇ∑ computer application for conducting automatic dialogues, module for building and managing dialogue; ÔÇ∑ computer application for multi-stream data acquisition and management; ÔÇ∑ a rich set of developing tools for conducting researches in the fields of image processing and gesture recognition; ÔÇ∑ annotated PJM gesture corpus acquired with RGB and depth cameras; ÔÇ∑ Polish-PJM parallel corpus for machine translation research. WiTKoM project is still in progress. The main focus is currently on the development of continuous statement recognition algorithms and practical use-case deployments.  Proceedings of the 19th Annual Conference of the EAMT: Projects/Products  379  Multi-Level Quality Prediction with QuEst++ Gustavo H. PAETZOLD, Lucia SPECIA University of Sheffield, Department of Computer Science Western Bank, Sheffield, South Yorkshire S10 2TN, United Kingdom ghpaetzold1@sheffield.ac.uk, l.specia@sheffield.ac.uk Abstract. We introduce QuEst++: an open-source multi-level Machine Translation Quality Estimation framework. The core of the framework is implemented in Java, and wrappers for a Machine Learning module implemented in Python are provided. QuEst++'s development was funded by the EAMT, as well as by the QT21 and EXPERT projects. The framework is distributed under the BSD license and can be downloaded from: https://github.com/ghpaetzold/questplusplus. Description QuEst++ is an extended and improved version of QuEst, a framework for Machine Translation Quality Estimation (Specia et. al., 2013). While the original QuEst framework provides only sentence-level Quality Estimation, QuEst++ is a multi-level framework that combines solutions to word-, sentence- and document-level Quality Estimation in the same pipeline. QuEst++ is composed of two modules: Feature Extraction and Machine Learning. The Feature Extraction module provides access to 43 word-level features, 148 sentencelevel features and 67 document-level features. If certain resources needed for feature extraction are not provided, QuEst++ employs its Automatic Resource Generation routines to produce the resources. The Machine Learning module offers all the utilities previously included in QuEst, and also an easy-to-use interface to CRFSuite (Okazaki, 2007), which allows for stateof-the-art Conditional Random Field models to be trained. As reported in the WMT 2015 Quality Estimation tasks‚Äô results (Bojar et al., 2015), QuEst++ itself performs well as compared to other systems. In addition, it has been used as the basis to create more advanced approaches. More details on QuEst++ can be found in Specia et al. (2015). References Specia, L., Paetzold, G., Scarton, C. (2015). Multi-level Translation Quality Prediction with QuEst++. In ACL-IJCNLP 2015 System Demonstrations, 115‚Äì120, Beijing. Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C. Koehn, P., Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, S., Specia, L., Turchi, M. (2015). Findings of the 2015 Workshop on Statistical Machine Translation. In WMT , 1‚Äì46, Lisbon. Specia, L., Shah, K., De Souza, J. G., & Cohn, T. (2013). QuEst-A Translation Quality Estimation Framework. In ACL 2013, 79-84, Sofia. Okazaki, N. (2007). CRFsuite: A Fast Implementation of Conditional Random Fields (CRFs). URL http://www. chokkan. org/software/crfsuite.  380  Proceedings of the 19th Annual Conference of the EAMT: Projects/Products  Apertium: A Free/Open-Source Platform for Machine Translation and Basic Language Technology  Mikel L. FORCADA1 and Francis M. TYERS2 
This paper evaluates the level of acceptance of Machine Translation (MT) among translators as of Summer, 2016. Translators are notoriously shy about MT, and their level of acceptance has a lot to say about the level reached by MT. 1. Credits This text format is derived from the AMTA guidelines for documents submitted for inclusion in AMTA proceedings. 2. MT Among Translators: the Early Days Translators have been aware of Machine Translation (MT) ever since it first began: the era of automated weather bulletin translation by robots after World War II. But they remained outsiders for a few decades. The Nineties can be seen as the first foray of MT in the world of translators, when the first off-the-shelf solutions (Systran, PowerTranslator, ProMT, etc.) appeared at affordable prices. Still, computer savviness was a prerequisite, which greatly limited the spread of MT use among translators. 3. The Real Start The nineties witnessed a few earth-shaking changes that redesigned the life of translators for ever, like the widespread acceptance and use of wordprocessors. The spread of the internet is another key factor. While the Internet (and online sales) are credited for the downfall of middlemen like travel agencies, the same factor can be credited for the opposite in translation: the sharp increase in the number of translation agencies. With the ability to team up hundreds of translators and quickly form ad hoc translation task forces for large translation projects, the translation agency, as we know it today, was born. One collateral effect of agencies attracting the bulk of translation orders was the relentless push toward lower costs. Human translation was considered a relative luxury before the nineties. With the age of luxury for the masses (from smoked salmon to ocean cruises to oversea vacations) came translation for the masses, at least in the business world. Businesses and companies, however small they are, can today call an agency and get 100 pages translated into 20 languages in a week: that was a surhuman feat in the past, available only to very large organizations.  Proceedings of AMTA 2016, vol. 2: MT Users' Track  Austin, Oct 28 - Nov 1, 2016 | p. 82  4. Was MT Here to Stay? The nineties taught us that one low-costing any human activity is how addictive that particular activity becomes. Spend one vacation abroad on an exotic island and presto, you must get back there every year. The addiction of the masses to luxury inexorably leads to a price war, where every trick is authorized. The translation industry is not immune to that effect. Much to the contrary: it is a textbook example of that trend. As of 2016, we see the maturity of the translation-for-the-masses model. Little more can be added to the existing array of tricks and techniques that agencies use to satisfy a translation-addicted business world. However, just as air transport or ocean cruises, the translation industry has reached a pricing floor below which it cannot plunge. Or can it? 5. MT's Age of Reason Machine Translation is one factor that can prolong the downward spiral of prices in the translation industry. With a twist. In other industries, when a price war is declared and low-cost is the order of the day, quality is sacrificed first. Summer fruits and vegetables are now available at dirt-cheap prices even in Winter - but they taste like wet cardboard. Senior members of the audience remember a glorious and distant past when tomatoes where had only in season, but Lord did they taste good. On the contrary, MT's salvation lies in the increase of its quality - because unlike tomatoes, quantity is no issue at all. As hinted earlier, MT is one of the factors, perhaps the only one, that the translation industry can use to prolong the price war, to keep lowering costs. The human factor cannot be compressed any further - the price paid to translators cannot be significantly lowered at this stage. The acceptance of MT by linguists lies at the heart of the equation. 6. MT Acceptance Among Translators: Where we Come From In the 2000's, ten years ago, MT usage was still between low and non-existent among translators, remaining at the level it was in the nineties. A comprehensive research made at London's Imperial College (Elina Lagoudakis, 20061) surveyed Translation Memory usage (MT) among translators, in short, technology used in the translation process, as TM was the prevalent technology back then. The expression "Machine Translation" only appears once in 36 pages - and actually does not refer to MT as we discuss it. Those were the days! Machine... what? That does not mean MT was not used before 2010. But it was rare among independent freelance translators. However, translation agencies would sometimes inject MT into pre-processed translation jobs, but that was not a widespread practice, and remained undercover. Linguists raised eyebrows at MT usage, and many would outright refuse; there was still as strong stigma associated with artificial translation. Another survey on translation technology for individual translators (Luciano Monteiro, 20092) was published by 
¬è¬É¬î¬Ö¬ã¬ï«§¬í¬ã¬ê¬ê¬ã¬ïÃ∑¬ñ¬ã¬é¬Ü¬á«§¬Ö¬ë¬è ¬î¬ã¬ä¬É¬î¬Ü¬ï«§¬ç¬É¬é¬ê¬ã¬ê¬ïÃ∑¬ñ¬ã¬é¬Ü¬á«§¬Ö¬ë¬è ¬î¬É¬ã¬ò¬ã¬ï«§¬ï¬ç¬É¬Ü¬ã¬ê¬ïÃ∑¬ñ¬ã¬é¬Ü¬á«§¬Ö¬ë¬è ¬ã¬ê¬â¬ó¬ê¬É«§¬ï¬ç¬É¬Ü¬ã¬ê¬ÉÃ∑¬ñ¬ã¬é¬Ü¬á«§¬Ö¬ë¬è  $EVWUDFW 7KLVSDSHUGHVFULEHVWKHILQGLQJVRIDODUJHSRVWHGLWLQJSURMHFWLQWKHPHGLFDOGRPDLQFDUULHG RXW E\ 7LOGH ,W DQDO\]HV WKH HIILFDF\ RI SRVWHGLWLQJ KLJKO\ WHFKQLFDO WH[WV LQ D VSHFLDOL]HG GRPDLQ DQG SURYLGHV DQVZHUV WR TXHVWLRQV LPSRUWDQW WR ORFDOL]DWLRQ VHUYLFH SURYLGHUV WKDW DUHFRQVLGHULQJWKHLQWURGXFWLRQRI SRVWHGLWLQJLQWKHLU WUDQVODWLRQZRUNIORZV 7KHUHVXOWV VKRZ WKDW E\ FDUHIXOO\ DQDO\]LQJ SRVWHGLWLQJ SURMHFWV PDFKLQH WUDQVODWLRQ SURYLGHUV DQG ODQJXDJHVHUYLFHSURYLGHUVFDQOHDUQKRZWRERRVWSURGXFWLYLW\LQORFDOL]DWLRQVDYHWLPHDQG RSWLPL]HUHVRXUFHVLQWKHODQJXDJHHGLWLQJSURFHVVDVZHOODVOHYHUDJHSRVWHGLWVWRLPSURYH PDFKLQHWUDQVODWLRQHQJLQHVWKURXJKG\QDPLFOHDUQLQJ   ,QWURGXFWLRQ  ,QRUGHUWRDQDO\]HWKHHIILFDF\RISRVWHGLWLQJKLJKO\WHFKQLFDOWH[WVLQDVSHFLDOL]HGGRPDLQ 7LOGHHPEDUNHGRQDSURMHFWWRDQDO\]HDODUJHSRVWHGLWLQJHIIRUWLQWKHPHGLFDOGRPDLQ'XULQJ WKH SURMHFW 7LOGH KDG WKH XQLTXH RSSRUWXQLW\ WR WDNH GHWDLOHG ORJV RI HDFK DFWLYLW\ SHUIRUPHG E\SRVWHGLWRUV7LOGHWKHQDQDO\]HGWKHSRVWHGLWLQJUHVXOWVDOORZLQJXVWRDQVZHULPSRUWDQW TXHVWLRQVOLNH+RZHIIHFWLYHO\GRSRVWHGLWRUVUHDOO\ZRUNZLWKPDFKLQHWUDQVODWLRQ07" 'RSRVWHGLWRUVH[SHQGWKHLUHIIRUWVXVHIXOO\RQHGLWLQJ07UHVXOWV" +RZFDQ07EH LPSURYHG WR PHHW WKH QHHGV RI ORFDOL]DWLRQ FRPSDQLHV WKDW XWLOL]H SRVWHGLWLQJ WR ERRVW WUDQVODWLRQSURGXFWLYLW\"+RZGRHVWKH07TXDOLW\DIIHFWSRVWHGLWLQJSHUIRUPDQFH"   076\VWHP  'XULQJWKHFRXUVHRIWKHSURMHFWSRVWHGLWRUVXVHGDVWDWLVWLFDO07607V\VWHPWKDWZDVEDVHG RQ WKH SKUDVHEDVHG 0RVHV 607 V\VWHP .RHKQ HW DO  7KH V\VWHP ZDV WUDLQHG RQ WKH (XURSHDQ0HGLFLQHV$JHQF\(0($SDUDOOHOFRUSXVIURP2386FRUSXV7LHGHPDQQ DQG ODWHVW GRFXPHQWV IURP (0($ ZHEVLWH \HDUV  FROOHFWHG E\ 7LOGH RQ WKH 7LOGH 07 SODWIRUP 9DVL∆∫MHYV HW DO  7KH VWDWLVWLFV RI WKH WUDLQLQJ FRUSXV EHIRUH DQG DIWHU ILOWHULQJDUHJLYHQLQ7DEOH7KHV\VWHP¬∂VDXWRPDWLFHYDOXDWLRQUHVXOWVDUHJLYHQLQ7DEOH  &RUSXV 3DUDOOHO 0RQROLQJXDO  6HQWHQFHVEHIRUHILOWHULQJ    6HQWHQFHVDIWHUILOWHULQJ    Table 1: Statistics of the training corpora used to train the SMT system  Proceedings of AMTA 2016, vol. 2: MT Users' Track  Austin, Oct 28 - Nov 1, 2016 | p. 86  B. MT usage from free sources Free MT sources are numerous; we may quote WordLingo3 and MyMemory. The stats here is a whopping 45%. The figure is evenly distributed among all regions. The figure means that nearly half of all translators regularly use a free MT provider. 
Abstract In recent years Statistical Machine Translation (SMT) has established a dominant position among the variety of machine translation paradigms. Industrial Machine Translation computer systems, such as KantanMT, deliver fast and of high performance SMT solutions to the end user. KantanMT is a cloud-based platform that allows its users to build custom SMT engines and use them for translation via a batch or an online mode. In order to employ the full potential of the cloud we have developed an efÔ¨Åcient method for asynchronous online translation. This method implements a producer-consumer technique that uses multiple queues as intermediate data storage units. Furthermore, each queue is associated with a priority that deÔ¨Ånes how quickly the queue can be consumed. That gives our users the control on the Ô¨Çow of translation requests, especially when it comes to large amounts of data. In this paper we describe the design and the implementation of the new method and compare it to others. We then assess the improvement in the quality of service of our platform by empirical evaluation. 
Abstract In this paper we discuss the motivation and process for planning, building, and monitoring a translation memory (TM) that serves both human- and machine-translated text segments in a production e-commerce environment. We consider the quality improvements associated with serving human translations for commonly used and mis-translated strings, and the cost beneÔ¨Åts of avoiding multiple re-translations of the same source text segments. We cover the technical considerations and architecture for each stage of the TM pipeline, and review the results of using and monitoring the TM in a production setting. 
In a typical TM-MT environment, translations for segments are provided from TMs based on matching criteria while the remaining segments are translated completely by MT. However, this binary approach does not always produce desirable translations. For example, even though a contiguous portion of a sentence to be translated may exactly match a TM entry or a frequently occurring sub-segment in many TM entries, if the match for the entire sentence does not exceed some arbitrary threshold, the smaller matches will not be used, and the entire sentence will be machine translated, resulting in a less than perfect translation, even for those portions that could have matched perfectly. In this report, we describe our approach to Ô¨Çexibly combine the capability of MT and TMs, applying exact TM matches to sub-segments of sentences and allowing MT to handle the remaining portions of the sentences. We speciÔ¨Åcally focus on the scenario where the matched phrases, clauses, and/or sentences are quotations in the text to be translated. 
Abstract In recent years, statistical machine translation (SMT) has been widely deployed in translators‚Äô workÔ¨Çow with signiÔ¨Åcant improvement of productivity. However, prior to invoking an SMT system to translate an unknown text, an SMT engine needs to be built. As such, building speed of the engine is essential for the translation workÔ¨Çow, i.e., the sooner an engine is built, the sooner it will be exploited. With the increase of the computational capabilities of recent technology the building time for an SMT engine has decreased substantially. For example, cloud-based SMT providers, such as KantanMT, can built high-quality, ready-to-use, custom SMT engines in less than a couple of days. To speed-up furthermore this process we look into optimizing the word alignment process that takes place during building the SMT engine. Namely, we substitute the word alignment tool used by KantanMT pipeline ‚Äì Giza++ ‚Äì with a more efÔ¨Åcient one, i.e., fast_align. In this work we present the design and the implementation of the KantanMT pipeline that uses fast_align in place of Giza++. We also conduct a comparison between the two word alignment tools with industry data and report on our Ô¨Åndings. Up to our knowledge, such extensive empirical evaluation of the two tools has not been done before. 
In recent years, several end-to-end online translation systems have been proposed to successfully incorporate human post-editing feedback in the translation workflow. The performance of these systems in a multi-domain translation environment (involving different text genres, post-editing styles, machine translation systems) within the automatic post-editing (APE) task has not been thoroughly investigated yet. In this work, we show that when used in the APE framework the existing online systems are not robust towards domain changes in the incoming data stream. In particular, these systems lack in the capability to learn and use domain-specific post-editing rules from a pool of multi-domain data sets. To cope with this problem, we propose an online learning framework that generates more reliable translations with significantly better quality as compared with the existing online and batch systems. Our framework includes: i) an instance selection technique based on information retrieval that helps to build domain-specific APE systems, and ii) an optimization procedure to tune the feature weights of the log-linear model that allows the decoder to improve the post-editing quality.
We assessed how different machine translation (MT) systems affect the post-editing (PE) process and product of professional English{--}Spanish translators. Our model found that for each 1-point increase in BLEU, there is a PE time decrease of 0.16 seconds per word, about 3-4{\%}. The MT system with the lowest BLEU score produced the output that was post-edited to the lowest quality and with the highest PE effort, measured both in HTER and actual PE operations.
Computer-aided translation (CAT) tools often use a translation memory (TM) as the key resource to assist translators. A TM contains translation units (TU) which are made up of source and target language segments; translators use the target segments in the TU suggested by the CAT tool by converting them into the desired translation. Proposals from TMs could be made more useful by using techniques such as fuzzy-match repair (FMR) which modify words in the target segment corresponding to mismatches identified in the source segment. Modifications in the target segment are done by translating the mismatched source sub-segments using an external source of bilingual information (SBI) and applying the translations to the corresponding positions in the target segment. Several combinations of translated sub-segments can be applied to the target segment which can produce multiple repair candidates. We provide a formal algorithmic description of a method that is capable of using any SBI to generate all possible fuzzy-match repairs and perform an oracle evaluation on three different language pairs to ascertain the potential of the method to improve translation productivity. Using DGT-TM translation memories and the machine system Apertium as the single source to build repair operators in three different language pairs, we show that the best repaired fuzzy matches are consistently closer to reference translations than either machine-translated segments or unrepaired fuzzy matches.
The utilization of statistical machine translation (SMT) has grown enormously over the last decade, many using open-source software developed by the NLP community. As commercial use has increased, there is need for software that is optimized for commercial requirements, in particular, fast phrase-based decoding and more efficient utilization of modern multicore servers. In this paper we re-examine the major components of phrase-based decoding and decoder implementation with particular emphasis on speed and scalability on multicore machines. The result is a drop-in replacement for the Moses decoder which is up to fifteen times faster and scales monotonically with the number of cores.
The objective of interactive translation prediction (ITP), a paradigm of computer-aided translation, is to assist professional translators by offering context-based computer-generated suggestions as they type. While most state-of-the-art ITP systems are tightly coupled to a machine translation (MT) system (often created ad-hoc for this purpose), our proposal follows a resourceagnostic approach, one that does not need access to the inner workings of the bilingual resources (MT systems or any other bilingual resources) used to generate the suggestions, thus allowing to include new resources almost seamlessly. As we do not expect the user to tolerate more than a few proposals each time, the set of potential suggestions need to be filtered and ranked; the resource-agnostic approach has been evaluated before using a set of intuitive length-based and position-based heuristics designed to determine which suggestions to show, achieving promising results. In this paper, we propose a more principled suggestion ranking approach using a regressor (a multilayer perceptron) that achieves significantly better results.
Domain adaptation is a major challenge when applying machine translation to practical tasks. In this paper, we present domain adaptation methods for machine translation that assume multiple domains. The proposed methods combine two model types: a corpus-concatenated model covering multiple domains and single-domain models that are accurate but sparse in specific domains. We combine the advantages of both models using feature augmentation for domain adaptation in machine learning. Our experimental results show that the BLEU scores of the proposed method clearly surpass those of single-domain models for low-resource domains. For high-resource domains, the scores of the proposed method were superior to those of both single-domain and corpusconcatenated models. Even in domains having a million bilingual sentences, the translation quality was at least preserved and even improved in some domains. These results demonstrate that state-of-the-art domain adaptation can be realized with appropriate settings, even when using standard log-linear models.
In this paper, we propose a new data selection method which uses semi-supervised convolutional neural networks based on bitokens (Bi-SSCNNs) for training machine translation systems from a large bilingual corpus. In earlier work, we devised a data selection method based on semi-supervised convolutional neural networks (SSCNNs). The new method, Bi-SSCNN, is based on bitokens, which use bilingual information. When the new methods are tested on two translation tasks (Chinese-to-English and Arabic-to-English), they significantly outperform the other three data selection methods in the experiments. We also show that the BiSSCNN method is much more effective than other methods in preventing noisy sentence pairs from being chosen for training. More interestingly, this method only needs a tiny amount of in-domain data to train the selection model, which makes fine-grained topic-dependent translation adaptation possible. In the follow-up experiments, we find that neural machine translation (NMT) is more sensitive to noisy data than statistical machine translation (SMT). Therefore, Bi-SSCNN which can effectively screen out noisy sentence pairs, can benefit NMT much more than SMT.We observed a BLEU improvement over 3 points on an English-to-French WMT task when Bi-SSCNNs were used.
We present an interactive translation prediction method based on neural machine translation. Even with the same translation quality of the underlying machine translation systems, the neural prediction method yields much higher word prediction accuracy (61.6{\%} vs. 43.3{\%}) than the traditional method based on search graphs, mainly due to better recovery from errors. We also develop efficient means to enable practical deployment.
In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3{\%}. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1{\%} BLEU absolute.
In the recent years interest in Deep Neural Networks (DNN) has grown in the field of Natural Language Processing, as new training methods have been proposed. The usage of DNN has achieved state-of-the-art performance in various areas. Neural Machine Translation (NMT) described by Bahdanau et al. (2014) and its successive variations have shown promising results. DNN, however, tend to over-fit on small data-sets, which makes this method impracticable for resource-limited language pairs. This article combines three different ideas (splitting words into smaller units, using an extra dataset of a related language pair and using monolingual data) for improving the performance of NMT models on language pairs with limited data. Our experiments show that, in some cases, our proposed approach to subword-units performs better than BPE (Byte pair encoding) and that auxiliary language-pairs and monolingual data can help improve the performance of languages with limited resources.
Lexicalized and hierarchical reordering models use relative frequencies of fully lexicalized phrase pairs to learn phrase reordering distributions. This results in unreliable estimation for infrequent phrase pairs which also tend to be longer phrases. There are some smoothing techniques used to smooth the distributions in these models. But these techniques are unable to address the similarities between phrase pairs and their reordering distributions. We propose two models to use shorter sub-phrase pairs of an original phrase pair to smooth the phrase reordering distributions. In the first model we follow the classic idea of backing off to shorter histories commonly used in language model smoothing. In the second model, we use syntactic dependencies to identify the most relevant words in a phrase to back off to. We show how these models can be easily applied to existing lexicalized and hierarchical reordering models. Our models achieve improvements of up to 0.40 BLEU points in Chinese-English translation compared to a baseline which uses a regular lexicalized reordering model and a hierarchical reordering model. The results show that not all the words inside a phrase pair are equally important in defining phrase reordering behavior and shortening towards important words will decrease the sparsity problem for long phrase pairs.
Statistical Machine Translation (SMT) of highly inflected, low-resource languages suffers from the problem of low bitext availability, which is exacerbated by large inflectional paradigms. When translating into English, rich source inflections have a high chance of being poorly estimated or out-of-vocabulary (OOV). We present a source language-agnostic system for automatically constructing phrase pairs from foreign-language inflections and their morphological analyses using manually constructed datasets, including Wiktionary. We then demonstrate the utility of these phrase tables in improving translation into English from Finnish, Czech, and Turkish in simulated low-resource settings, finding substantial gains in translation quality. We report up to +2.58 BLEU in a simulated low-resource setting and +1.65 BLEU in a moderateresource setting. We release our morphologically-motivated translation models, with tens of thousands of inflections in each of 8 languages.
Most diacritics in Arabic represent short vowels. In Arabic orthography, such diacritics are considered optional. The absence of these diacritics naturally leads to significant word ambiguity to top the inherent ambiguity present in fully diacritized words. Word ambiguity is a significant impediment for machine translation. Despite the ambiguity presented by lack of diacritization, context helps ameliorate the situation. Identifying the appropriate amount of diacritic restoration to reduce word sense ambiguity in the context of machine translation is the object of this paper. Diacritic marks help reduce the number of possible lexical word choices assigned to a source word which leads to better quality translated sentences. We investigate a variety of (linguistically motivated) partial diacritization schemes that preserve some of the semantics that in essence complement the implicit contextual information present in the sentences. We also study the effect of training data size and report results on three standard test sets that represent a combination of different genres. The results show statistically significant improvements for some schemes compared to two baselines: text with no diacritics (the typical writing system adopted for Arabic) and text that is fully diacritized.
